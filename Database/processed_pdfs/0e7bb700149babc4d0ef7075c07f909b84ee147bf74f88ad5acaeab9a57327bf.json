{
  "text": "Detecting Benchmark Contamination\nThrough Watermarking\nTom Sander1,2, Pierre Fernandez1, Saeed Mahloujifar1, Alain Durmus2, Chuan Guo1\n1Meta FAIR, 2École polytechnique CMAP\nBenchmark contamination poses a significant challenge to the reliability of Large Language\nModels (LLMs) evaluations, as it is difficult to assert whether a model has been trained on a\ntest set. We introduce a solution to this problem by watermarking benchmarks before their\nrelease. The embedding involves reformulating the original questions with a watermarked\nLLM, in a way that does not alter the benchmark utility. During evaluation, we can detect\n“radioactivity”, i.e., traces that the text watermarks leave in the model during training, using a\ntheoretically grounded statistical test. We test our method by pre-training 1B models from\nscratch on 10B tokens with controlled benchmark contamination, and validate its effectiveness\nin detecting contamination on ARC-Easy, ARC-Challenge, and MMLU. Results show similar\nbenchmark utility post-watermarking and successful contamination detection when models are\ncontaminated enough to enhance performance, e.g., p-val = 10−3 for +5% on ARC-Easy.\nCorrespondence: tomsander@meta.com\nR\nLLM\n(contaminated)\n‘Contaminated?’   ✔/✗\nStatistical Test\nWatermarked benchmark\nTraining\nBenchmark watermarking\n(rephrasing with watermarked LLM)\nOriginal benchmark\nFigure 1 Problem overview. Alice is a benchmark provider and wants to make sure that contamination on\nher benchmark can be detected with high confidence. Before release, she rephrases the original benchmark\ndataset while embedding a non-intrusive LLM watermark. This rephrasing does not change the utility of the\nbenchmark. Bob decides to train a model. The benchmark may contaminate Bob’s model during training,\neither intentionally or unintentionally. Alice can give statistical evidence if her benchmark was used in training.\n1\nIntroduction\nIn recent years, Large Language Models (LLMs) have demonstrated remarkable advancements in\ntheir capabilities (Brown et al., 2020; Touvron et al., 2023). This advancement places increasingly\ngreater emphasis on proper evaluation to both inform the state of LLM research and to guide future\ndevelopments. To this end, a multitude of benchmark datasets such as (MMLU) (Hendrycks et al.,\n2020), School Math 8K (GSM8K) (Cobbe et al., 2021), and the AI2 Reasoning Challenge (ARC) (Clark\net al., 2018), or more recently GPQA (Rein et al., 2023) and FrontierMath (Glazer et al., 2024), are\ndeveloped to measure the model’s capabilities in terms of general or specific knowledge, understanding,\nand scientific reasoning.\nHowever, a significant issue that arises with these benchmarks is contamination. This problem can\noccur either intentionally, by training models directly on the benchmark datasets or their reformulated\nversions, or unintentionally, as these datasets become mixed with the vast amounts of data used during\npre-training. For example, Zhang et al. (2024) created a version of GSM8K with new questions similar\nin difficulty and form, and observed that many models show a significant drop in performance on them\ncompared to the test set of GSM8k. This challenges the reliability and validity of benchmark evaluations,\nas it becomes difficult to discern whether a model’s performance is due to genuine improvement in\ncapabilities or mere memorization. Furthermore, determining whether a model has been trained on a\n1\narXiv:2502.17259v1  [cs.CR]  24 Feb 2025\n\nspecific benchmark is very challenging, as it boils down to the issue of dataset/membership inference\nwhich has been shown to be ineffective for LLMs in realistic scenarios (Duan et al., 2024).\nTo tackle this problem, we propose a novel strategy of embedding non-intrusive watermarks in the\nbenchmark dataset before release. Our approach is inspired by Sander et al. (2024), who demonstrated\nthat fine-tuning on LLM-generated watermarked text can be reliably detected, as the model retains\nidentifiable traces of the watermark. When applied to benchmark watermarking, this approach enables\nreporting both model performance and a reliable p-value as a contamination score: it is an upper bound\nto the probability that the model hasn’t been trained on the benchmark questions. If the reported\np-value is low, the LLM’s training data is likely contaminated with the benchmark dataset and the\nperformance numbers should not be trusted as genuine. Our method requires only access to an LLM\ncapable of rephrasing benchmark questions; see Figure 1 for an overview. Our main contributions are:\n• Rephrasing benchmark datasets with watermarking: We use Llama-3 instruct models to rephrase\nquestions from MMLU, ARC-Easy, and ARC-Challenge benchmarks. By applying the red/green\nlist watermarking technique from Kirchenbauer et al. (2023a), we show that rephrasing effectively\nincorporates watermarks while preserving benchmark integrity (subsection 3.1 and Figure 3a).\n• Extending watermark radioactivity: Building on Sander et al. (2024), we extend watermark\nradioactivity to a pre-training setup. We pre-train 1B models on 10B tokens, varying benchmark\ncontamination levels, each benchmark with a different secret watermarking key s. For instance,\nour results show detection of contamination with a p-value below 10−3 when the accuracy is only\ninflated by 5% on ARC-Easy, indicating a one in a thousand chance of error, while correctly yielding\np-values near 0.5 for uncontaminated models (Figure 3b and Table 1).\n2\nRelated Work\n2.1\nBenchmark Contamination Detection\nBenchmark contamination is a significant concern in evaluating LLMs, as it can lead to unreliable\nassessments and unfair comparisons (Singh et al., 2024; Balloccu et al., 2024). Although efforts are made\nto decontaminate pre-training corpora (Brown et al., 2020), these methods are not foolproof (Singh\net al., 2024). The impact of contamination can be assessed by comparing training runs that differ\nonly in the inclusion of contaminated batches. For instance,\nJiang et al. (2024) have shown that\neven small models can exhibit improved benchmark performance due to contamination. Post-hoc\nanalyses on the other hand identify score inflation by comparing performance on original versus similar\nquestions (Brown et al., 2020; Chowdhery et al., 2023; Touvron et al., 2023), but Yang et al. (2023)\nhave shown that training on reformulated questions is enough to boost the performance on the original\nbenchmark, so the difference in performance does not necessarily provide good correlational insights.\nZhang et al. (2024) craft new questions from the same distribution as GSM8K and observed that most\nmodels show a significant performance drop on these compared to the GSM8K test set. This result\nhighlights the contamination issue, but does not introduce a scalable solution to the core problem.\nIn parallel, studying verbatim memorization in LLMs, such as regurgitating pre-training data, is\nalso closely related to contamination (Carlini et al., 2022; Hartmann et al., 2023). Techniques like\nmembership inference (Mireshghallah et al., 2022) and context-based completion checks (Golchin and\nSurdeanu, 2023) attempt to approximate contamination without direct access to pre-training data, but\ntheir effectiveness is debated (Duan et al., 2024). These methods use a score function to determine the\nlikelihood of a sample being present in the training set. To ensure accurate results, each sample must\nbe calibrated, which can be a computationally intensive process (e.g., involving a model trained on all\ndata except the benchmark). Additionally, these methods lack guarantees regarding false positive and\nnegative rates, allowing for plausible deniability even when there is strong evidence of contamination.\n2.2\nDecoding-based watermarking & Radioactivity\nOverview.\nRecent advancements in watermarking techniques for decoder-only large language models\n(LLMs) involve altering either the probability distribution (Kirchenbauer et al., 2023a) or the method\nused for sampling the subsequent token (Aaronson and Kirchner, 2023; Kuditipudi et al., 2023).\nDetection of these watermarks is influenced by the entropy of the generated text (Christ et al., 2023;\n2\n\nHuang et al., 2023), so further investigations propose watermarking only sections with high entropy,\nespecially in code (Lee et al., 2023), while other studies explore “semantic” watermarks that rely on the\nsemantic representation of the entire preceding text (Liu et al., 2023; Liu and Bu, 2024; Fu et al., 2024).\nGreen-list/Red-list watermark.\nThis work focuses on the watermarking scheme proposed by Kirchen-\nbauer et al. (2023b), which modifies the logit vector during token generation based on a context window\nof k previous tokens and a private key s. Both are hashed to serve as the seed for a random number\ngenerator (RNG) to create a “greenlist” of γ|V| tokens. Logits of green tokens are incremented by δ to\nincrease their sampling probability. Detection involves repeating the greenlist computation for each\ntoken of a text, incrementing a score by 1 if the token is in the greenlist, and performing a statistical\ntest on the cumulative score. Under the null hypothesis H0, which corresponds to “the text is not\nwatermarked with that scheme”, this score follows a binomial distribution (Fernandez et al., 2023).\nRadioactivity of LLM watermarks.\nSander et al. (2024) show that fine-tuning language models on\nLLM-generated watermarked question-answer pairs can be detected with high confidence, as the model\nretains traces of the watermark bias. The authors adapt the original watermark detection tests to\ndetect this watermark “radioactivity”, depending on the access to the suspect model and data. In\nthe context of benchmark watermarking, we assume access to the LLM that is being evaluated, the\nbenchmark itself, as well as the benchmark-specific watermarking key s. In this case, Sander et al.\n(2024) suggests using what they call “reading-mode”. This involves scoring all next-token predictions by\nforwarding the watermarked text in the suspect model. This is detailed in our context in subsection 3.2,\nand illustrated in Figure 2b. Similar observations have been made in other scenarios. For instance,\nGu et al. (2023) demonstrate that LLM watermarks can be intentionally distilled. Additionally, Zhao\net al. (2023) introduce a signal in generated text that can be learned by other LLMs trained on it.\nFurthermore, Jovanović et al. (2024) investigate the concept of watermark radioactivity in a RAG\ncontext.\n3\nMethod\nWe first focus in section 3.1 on the task of rephrasing the questions of a benchmark dataset while\nembedding a watermark using the method proposed by Kirchenbauer et al. (2023b). Then, in section 3.2,\nwe show how to detect if a language model was trained on the watermarked benchmark.\n3.1\nInserting watermark through question rephrasing\nWe use an instruct language model, denoted as LMrephrase, which is assumed to be capable of rephrasing\neach question in the benchmark test set such that the rephrased version is logically equivalent to\nthe original. This is a pretty light assumption as the task of rephrasing is considerably easier than\nanswering the question (Deng et al., 2023). LMrephrase generates token per token and at each step,\ntakes as input a context, which is the concatenation of the system prompt, rephrasing instruction,\nthe question to rephrase and the answer generated so far. Everything is tokenized into a sequence\n\u0000x(1), . . . , x(t−1)\u0001\n∈Vt−1, where V is the vocabulary of the tokenizer.\nLMrephrase outputs a logits vector ℓ(t) ∈R|V|. The watermark embedding modifies ℓ(t) based on a secret\nkey s (one per benchmark) and the watermark window\n\u0000x(t−k), . . . , x(t−1)\u0001\n∈Vk Specifically, following\nthe method of Kirchenbauer et al. (2023b) detailed in 2.2, a secret-key cryptographic function hashes s as\nwell as the the watermark window, which serves as a seed for a random number generator used to create\na pseudo-random “greenlist” of tokens, comprising 50% of the entire vocabulary V, for which the logits\nare incremented by a quantity δ to form ˜ℓ(t), thereby increasing their probability of being sampled. The\nlogits vector is then transformed into a probability distribution p(t) = softmax(˜ℓ(t)) ∈[0, 1]|V|, and the\ngeneration proceeds by sampling the next token x(t) from this distribution using a sampling procedure\nsuch as top-k sampling (Fan et al., 2018) or nucleus sampling (Holtzman et al., 2019). The selected token\nis appended to the context, and the process repeats. An example for the watermark embedding process\nis depicted in Figure 2a, with a detailed version with different strength of watermarking in Figure 6.\nDetectability/utility tradeoff.\nThere is a common tradeoff in watermarking between detection and\nutility. In our case detection is the ability to have statistical evidence that the benchmark was used\n3\n\nduring training. We show in subsection 3.2 that it can be measured through the p-value, which is an\nupper-bound to the probability that the model is not contaminated. A lower p-value thus indicates a\nstronger detection signal, making it more likely to identify unauthorized usage. On the other hand,\nthe utility of the watermarked benchmark is its ability to rank models and assess their performance\non specific tasks. To preserve utility, we therefore require that models perform similarly on both the\noriginal and watermarked versions of the benchmark, allowing for accurate evaluation and comparison\nof model performance. Specifically, the benchmark dataset exhibits a proportion ρ > 0.5 of green tokens\nafter rephrasing, the greater the easier detectability. For utility, we check if pre-trained models perform\nsimilarly on the original and rephrased versions.\nEnhancing the watermarked benchmark could involve: 1) using rephrasing instructions tailored to each\nbenchmark’s specifics, 2) employing better models for rephrasing, and 3) involving humans to review\neach question, correct it, or choose between different watermarked versions from various seeds.\n3.2\nRadioactivity Detection in a white box scenario\nThe strength of the watermark is determined by ρ, the proportion of green tokens in the text, which\nis influenced by δ and the entropy of the generation process. Sander et al. (2024) demonstrate that\nthe ability to detect whether a model has been trained on watermarked data—referred to as the\nradioactivity power—depends on ρ, as well as the proportion of watermarked text relative to the total\nnumber of training tokens, the size of the model, the fine-tuning method, and other factors. In general,\nthe more a model fits the watermarked data, the more it will memorize the token-level watermark bias,\nthereby making radioactivity easier to detect. The authors also introduce a “reading mode” to enhance\nradioactivity detection when the model’s weights are accessible and the suspect text is known: in our\ncontext, we input the tokenized questions into the suspect model and, for each input token, assign a\nnext token prediction using greedy decoding (i.e., selecting the most likely next token based on the\noutput logits). For detection, we replay the seed generation using the watermark window from the\ninputs and the benchmark-specific key s to determine the green/red split, scoring +1 if the predicted\ntoken is in the corresponding green list. This process is illustrated in Figure 2.\nThe score function on a predicted token at index y(t) thus uses Wscore that takes as input the watermark\nwindow (x(t−k+1), . . . , x(t)) from the question, and depends on the secret key s:\ny(t) ;\n\u0000x(t−k+1), . . . , x(t)\u0001\n7→Wscore\n\u0010\ny(t) ; s,\n\u0000x(t−k+1), . . . , x(t)\u0001 \u0011\n∈R.\n(1)\ngenerated token being scored\nWatermark window (k previous tokens from the ground truth)\nScoring function (1 if green token, 0 otherwise)\nA statistical test is performed on the cumulative score S(XN) over all token indices t ≥k:\nS(XN) :=\nN\nX\nt=k\n1\n\u0010\ny(t) is in the greenlist of\n\u0010\ns, (x(t−i+1))1\ni=k\n\u0011\u0011\n.\n(2)\nSystem prompt + instruction:\n“You are a problem rephrasing assistant [...]”\nQuestion: “The rate of acceleration of an object is\ndetermined by the mass of the object and”\nRephrased with watermark (δ = 4):\n“What factor, aside from an object’s mass, determines\nits acceleration?” (73% of green tokens)\n(a) Embedding - benchmark rephrasing\n,\nappart\nwatermark \nwindow\nScored token:\nS +=1\nfactor\naside\nfrom\n,\nfrom\nfactor\nWhat\nan\ny(t)\nx(t)\nx(t-1)\n(b) Detection - statistical test\nFigure 2 Method description. (Left) Watermarking the benchmark’s questions using an LLM, as detailed\nin subsection 3.1, with an example from ARC-easy. The quality of the question is maintained despite strong\nwatermarking. (Right) Reading mode, as detailed in subsection 3.2. The upper sequence is the watermarked\nquestion, and the tokens bellow are to next token predictions from the suspect model (y(t) in Equation 1).\n4\n\nThe statistical test considers H0: “The tokens are generated without influence from the watermarking\nbias”. The hypothesis “The model is not contaminated” is included in H0, under which S(XN) follows\na binomial distribution, as it should not output more green than red tokens.\nDe-duplication for reliable p-values.\nUnder H0, for S(XN) to indeed follow a binomial distribution,\nthe random variables\n\u00001\n\u0000y(t) is in the greenlist of\n\u0000s, (x(t−i+1))1\ni=k\n\u0001\u0001\u0001\nt should be independent and\nidentically distributed and follow a Bernoulli distribution with parameter γ. For the independence\ncriterion, we only score\n\u0000y(t); s, (x(t−i+1))1\ni=k\n\u0001\nthat were not already scored (Kirchenbauer et al., 2023a;\nFernandez et al., 2023; Sander et al., 2024), by keeping a tape of scored tuples. The p-value of a test\nassociated with score s, i.e., the probability of obtaining a score higher than s under H0, can then be\nobtained theoretically from the regularized incomplete Beta function Iγ:\np-value(s) = P(S(XN) ≥s | H0) = Iγ(s + 1, N −s).\n(3)\nThe reading mode can be done either by the community for open-source models, or by the model owner\notherwise, without sharing model weights. Contamination is expected to increase as the model over-fits\non the benchmark. Thus, radioactivity detection should align with benchmark contamination: for a\nfixed benchmark size, smaller p-values indicate a higher proportion of predicted green tokens, occurring\nwhen predictions replicate green tokens from watermarked questions due to token-level overfitting.\n4\nResults\n4.1\nBenchmark quality after watermarking\nSet-up.\nFor the watermark embedding, we rephrase with Llama-3.1-8B-Instruct (Dubey et al., 2024)\nby default, with top-p sampling with p = 0.7 and temperature = 0.5 (default values on the Hugging\nFace hub), and the green/red watermarking scheme of Kirchenbauer et al. (2023b) with a watermark\nwindow k = 2 and a “green list” of size 1\n2|V | (|V | is the vocabulary size). We compare different values\nof δ when rephrasing: 0 (no watermarking), 1, 2, and 4. We choose to watermark ARC-Challenge,\nARC-Easy, and MMLU due to their widespread use in model evaluation. In practice, one would need\nto watermark their own benchmark before release. For MMLU, we select a subset of 5000 questions,\nrandomly chosen across all disciplines, to accelerate experimentation and maintain a comparable size\nto the other benchmarks. We refer to this subset as MMLU∗. ARC-Easy contains 1172 questions, and\nARC-Challenge contains 2372 questions. In Figure 6 of Appendix A, we show the exact instructions\ngiven to the rephrasing model (identical for all benchmarks) and the results for different watermarking\nstrengths on one example from ARC-Easy. We use a different watermarking key s for each benchmark.\nEven strong watermarking keeps benchmark utility.\nWe evaluate the performance of Llama-3.3-\n1B, Llama-3.3-3B and Llama-3.1-8B on the original benchmark and the rephrased version using as\nsimilar evaluation as the one from the lm-evaluation-harness library (Gao et al., 2024). To check\nif the benchmark is still as meaningful, we check that evaluated models obtain a similar accuracy on\nthe watermarked benchmarks and on the original version (see subsection 3.1). Figure 3a shows the\nperformance on ARC-Easy. All models perform very similarly on all the rephrased versions of the\nbenchmark, even when pushing the watermark to 80% of green tokens. Importantly, they rank the\nsame. Similar results are shown for MMLU∗and ARC-Challenge in Figure 3a of Appendix A, although\nfor MMLU∗, we observe some discrepancies. For instance, when using a watermarking window size of 2\n(subfig i), the performance of Llama-3.2-1B increases from 38% to 42% between the original and the\nother versions. However we observe the same issue when rephrasing without watermarking in that case.\nAs detailed in subsection 3.1, designing better instructions that are more specific to each benchmark\ncould help. We have tried increasing δ even further, but it broke the decoding process. The choice of δ\ndepends on the benchmark and the model used for rephrasing, and needs to be empirically tested.\n4.2\nContamination detection through radioactivity\nWe now propose an experimental design to control benchmark contamination, and evaluate both the\nimpact on model performance and on contamination detection.\n5\n\nTraining set-up.\nWe train 1B transformer models (Vaswani, 2017) using Meta Lingua (Videau et al.,\n2024) on 10B tokens from DCLM (Li et al., 2024). The model architecture includes a hidden dimension\nof 2048, 25 layers, and 16 attention heads. The training process consists of 10,000 steps, using a batch\nsize of 4 and a sequence length of 4096. Each training is distributed across 64 A-100 GPUs, and takes\napproximately three hours to finish. The optimization is performed with a learning rate of 3 × 10−3, a\nweight decay of 0.033, and a warmup period of 5,000 steps. The learning rate is decayed to a minimum\nratio of 10−6, and gradient clipping is applied with a threshold of 1.0.\nContamination set-up.\nBetween steps 2500 and 7500, every 5000/#contaminations, we take a batch\nfrom the shuffled concatenation of the three benchmarks instead of the batch from DCLM. Each batch\nhas batch size×sequence length×number of GPUs = 4×4096×64 ≈1 M tokens As shown in Table 1,\nthe concatenation of the three benchmarks is approximately 500k tokens, so each contamination is a\ngradient that encompasses all the benchmark’s tokens. For each benchmark, any sample that ends up\ncontaminating the model is formatted as follows:\nf\"Question:\n{Question}\\nAnswer:\n{Answer}\"\nEvaluation.\nWe evaluate the accuracy of the models on the benchmarks by comparing the loss between\nthe different choices and choosing the one with the smallest loss, either “in distribution” by using the\nabove template seen during contamination or “out of distribution” (OOD) by using:\nf\"During a lecture, the professor posed a question:\n{Question}.\nAfter discussion, it was revealed that the answer is:\n{Answer}\"\nIn the first scenario, we evaluate overfitting, as the model is explicitly trained to minimize the loss of\nthe correct answer within the same context. In the second scenario, we assess the model’s ability to\nconfidently provide the answer in a slightly different context, which is more relevant for measuring\ncontamination. Indeed, it’s important to note that evaluations often use templates around questions\n—e.g., in the lm-evaluation-harness library (Gao et al., 2024)— which may not be part of the\nquestion/answer files that could have leaked into the pre-training data. Table 1 focuses on δ = 4 and\nshows the increase in performance across the three (watermarked) benchmarks as a function of the\nnumber of contaminations when evaluated OOD. Results for in-distribution evaluation are provided in\nTable 3 of Appendix A (without contamination, the model performs similarly across the two templates).\nContamination detection.\nFor each benchmark, we employ the reading mode detailed in subsection 3.2\nto compute the radioactivity score S and the corresponding p-value. Results are illustrated in Figure 3b\nfor ARC-Easy, and in Figure 8 of Appendix A for the other two benchmarks, across different numbers\nNA\n49.8%\n=0\n49.9%\n=1\n58.7%\n=2\n66.3%\n=4\n79.5%\nWatermark strength  and proportion of green tokens\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nOriginal Benchmark\nWatermarked Benchmarks\nLlama-3.2-1B\nLlama-3.2-3B\nLlama-3.1-8B\n(a) Watermarking questions does not degrade utility.\n0\n1\n2\n4\nWatermark strength \n0\n4\n8\n12\nDetection confidence (- Log10 P-Value)\n4 contaminations\n8 contaminations\n16 contaminations\n(b) More contaminations & stronger wm ↑detection.\nFigure 3 Result for benchmark watermarking on ARC-Easy. (Left) We rephrase the questions from ARC-Easy\nusing Llama-3.1-8B-Instruct while adding watermarks of varying strength. The performance of multiple Llama-3\nmodels on rephrased ARC-Easy is comparable to the original, preserving the benchmark’s usefulness for ranking\nmodels and assessing accuracy (Sec. 3.1, Sec. 4.1). (Right) We train 1B models from scratch on 10B tokens\nwhile intentionally contaminating its training set with the watermarked benchmark dataset. Increasing the\nnumber of contaminations and watermark strength enhances detection confidence (Sec. 3.2, Sec. 4.2)\n6\n\nTable 1\nDetection and performance metrics across different levels of contamination for ARC-Easy, ARC-\nChallenge, and MMLU benchmarks, watermarked with δ = 4. The performance increase is shown for OOD\nevaluation as detailed in subsection 4.2. The log10 p-value of the detection test is strongly correlated with the\nnumber of contaminations, as well as with the performance increase of the LLM on the benchmark.\nARC-Easy (112k toks.)\nARC-Challenge (64k toks.)\nMMLU∗(325k toks.)\nContaminations\nlog10(p)\nAcc. (% ∆)\nlog10(p)\nAcc. (% ∆)\nlog10(p)\nAcc. (% ∆)\n0\n-0.3\n53.5 (+0.0)\n-0.3\n29.4 (+0.0)\n-0.9\n30.6 (+0.0)\n4\n-3.0\n57.9 (+4.3)\n-1.2\n32.4 (+3.1)\n-5.7\n35.7 (+5.1)\n8\n-5.5\n63.0 (+9.5)\n-4.5\n39.3 (+9.9)\n<-12\n40.8 (+10.2)\n16\n<-12\n71.7 (+18.2)\n<-12\n54.3 (+24.9)\n<-12\n54.0 (+23.5)\nof contaminations and varying watermark strengths δ. We observe that the stronger the watermark\nstrength and the greater the number of contaminations, the easier it is to detect contamination: a larger\nnegative log10(p) value indicates smaller p-values, implying a lower probability of obtaining this score\nif the model is not contaminated. For instance, a −log10(p) of 6 implies that we can confidently assert\nmodel contamination, with only a 10−6 probability of error. Additionally, we observe that without\ncontamination, the test yields a log10(p) value close to −0.3 = log10(0.5), as expected under H0. Indeed,\nunder H0, the p-value should follow a uniform distribution between 0 and 1, which implies that [-1, 0]\nis a 90% confidence interval for log10(p), and that [-2, 0] is a 99% confidence interval.\nTable 1 links the contamination detection to the actual cheating (with OOD evaluation) on the\nbenchmarks when δ = 4 is used. We can see that for the three benchmarks, whenever the cheat is\ngreater than 10%, detection is extremely confident. When the cheat is smaller, with four contaminations\nranging from +3% to +5%, the p-value is small enough on ARC-Easy and MMLU∗, but doubtful for\nARC-Challenge (because smaller, see subsection 4.3). For instance, for MMLU∗, we can assert model\ncontamination, with only a 10−6 probability of error when 5 points are artificially added.\n4.3\nAdditional Results\nTable 2 Proportion of green tokens in the\npredictions (see Equation 2), number of to-\nkens scored after dedup and log10 p-values\nfor different watermark window sizes, with\n16 contaminations and δ = 4 on ARC-Easy.\nk\nρ\nTokens\nlog10(p)\n0\n0.53\n5k\n-6.07\n1\n0.53\n28k\n-25.89\n2\n0.53\n47k\n-38.69\nImpact of window size.\nWatermark insertion through\nrephrasing (subsection 3.1) depends on the watermark\nwindow size k.\nEach window creates a unique green-\nlist/red-list split for the next token.\nLarger windows\nreduce repeated biases but are less robust. Because of rep-\netitions, Sander et al. (2024) show that smaller windows\ncan lead to bigger overfitting on token-level watermark\nbiases, aiding radioactivity detection. In our case, bench-\nmark sizes are relatively small and deduplication limits\nthe number of tokens tested, because each {window + pre-\ndicted token} is scored only once. Thus, smaller windows\nmean fewer tokens to score. Moreoever, as shown in Table 2, the proportion of predicted green tokens\nis not even larger for smaller windows: there is not enough repetitions for increased over-fitting on\nsmaller windows. The two factors combined result in lower confidence. A comparison of contamination\ndetection across benchmarks and window sizes is shown in Figure 7, and for the utility of the benchmarks\nin Figure 8. Overall, larger window size (k = 2) yields better results.\nImpact of benchmark size.\nThe benchmark size can significantly affect the method’s effectiveness.\nWith a fixed proportion of predicted green tokens, more evidence (i.e., more scored tokens) increases\ntest confidence. As shown in Table 1, at a fixed level of cheating (e.g., +10% on all benchmarks after 8\ncontaminations), contamination detection confidence is proportional to benchmark size. This is similar\nto our observations on watermark window sizes in Table 2.\nImpact of rephrasing model.\nThe difficulty and entropy of questions can significantly affect the\nmethod’s performance. Indeed, math questions for instance can be challenging to rephrase, even\nmore with watermarks. Thus, better models may be needed for technical benchmarks. We tested\nrephrasing with Llama3-70B-Instruct instead of the 8B version, and observed that some 8B model\nfailures, especially on math questions, are resolved with the 70B model, though quantifying this is\n7\n\nchallenging. An example is provided in Figure 4. We note that increasing δ to 8 is necessary to match\nthe green token proportion of δ = 2 with the 8B model, using the same decoding parameters. This\nmay result from lower entropy in generation or bigger logits, as the greenlist bias is applied before the\nsoftmax (see subsection 3.1). Moreover, in math or code, rephrasing can offer limited entropy, and even\nbetter models will not be enough. An alternative would be to add watermarked verbose text around\nthe questions, or using entropy-aware LLM watermarking (Lee et al., 2023).\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\nPerformance Increase on MMLU *\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\nDetection Confidence (-Log10 P-Value)\n0 cont\n4 cont\n8 cont\n16 cont\n135M\n360M\n1B\nFigure 5 Detection confidence as a function of perfor-\nmance increase on MMLU∗for different model sizes and\n#contaminations, for δ = 4 and OOD evaluation.\nImpact of model size.\nWe also test radioac-\ntivity detection on 135M and 360M transformer\nmodels using the architectures of SmolLM and\nthe same training pipeline as described in sub-\nsection 4.2, training each model on 10B tokens\nas well. Figure 4.3 shows the detection confi-\ndence as a function of the cheat on MMLU∗.\nWe find that, for a fixed number of contami-\nnations, smaller models show less performance\nincrease – expected as they memorize less – and\nwe obtain lower confidence in the contamination\ndetection test. As detailed in subsection 3.1,\nthe p-values indicate how well a model overfits\nthe questions, hence the expected correlation.\nFor a fixed performance gain on benchmarks,\np-values are consistent across models. For example, after 4, 8, and 16 contaminations on the 1B, 360M,\nand 135M parameter models respectively, all models show around +6% gain, with detection tests\nyielding p-values around 10−5. Thus, while larger models require fewer contaminated batches to achieve\nthe same gain on the benchmark, radioactivity effectively measures “cheating”.\n5\nLimitations & Conclusion\nLimitations\n• Tokenizer consistency: This study uses the same tokenizer for both the rephrasing and contaminated\nmodels. If a different tokenizer is used in the suspect model, scoring should be limited to tokens\npresent in both vocabularies. A smaller intersection of vocabularies or a larger watermark window\nthus reduces the number of scored tokens, and thus the power of the test.\n• Rephrasing impact: Model performance remains similar across benchmark versions, but some\nquestions lose coherence after rephrasing (e.g., Figure 4), which can be difficult to spot. Possible\nimprovements are discussed in subsection 3.1 and subsection 4.3.\n• Intentional evasion: The method is primarily designed for unintentional contamination. Malicious\nactors could rephrase questions to weaken the watermark or train only on answers conditioned on\nquestions, which would bypass radioactivity detection. In this case, watermarking answers may be\nnecessary, though it might not always be feasible because of their lengths.\nConclusion.\nWatermarking benchmark appears like a promising solution to the problem of contamina-\ntion in large language models: experiments confirm the method’s ability to maintain benchmark utility\nwhile successfully identifying contamination.\nOriginal\nquestion:\nAn\nobject\naccelerates\nat\n3\nmeters\nper\nsecond2\nwhen\na\n10-newton\n(N)\nforce\nis\napplied\nto\nit.\nWhich\nforce\nwould\ncause\nthis\nobject\nto\naccelerate\nat\n6\nmeters\nper\nsecond2?\nLlama-3-8B-Instruct, δ = 2: What additional\nforce, applied in conjunction with the existing\n10-N force, would cause the object to experience\nan acceleration of 6 meters per second2? (70%)\nLlama-3-70B-Instruct, δ = 8: What force would be necessary\nto apply to the object in order to increase its acceleration to\n6 meters per second2, given that an acceleration of 3 meters\nper second2is achieved with a 10-newton force? (65%)\nFigure 4 Watermarking failure on an ARC-Challenge question with an 8B model, while the 70B model succeeds.\n8\n\nReferences\nScott Aaronson and Hendrik Kirchner. Watermarking GPT outputs, 2023. https://scottaaronson.blog/?m=\n202302.\nSimone Balloccu, Patrícia Schmidtová, Mateusz Lango, and Ondřej Dušek.\nLeak, cheat, repeat: Data\ncontamination and evaluation malpractices in closed-source llms. arXiv preprint arXiv:2402.03927, 2024.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877–1901, 2020.\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang.\nQuantifying memorization across neural language models. arXiv preprint arXiv:2202.07646, 2022.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling\nwith pathways. Journal of Machine Learning Research, 24(240):1–113, 2023.\nMiranda Christ, Sam Gunn, and Or Zamir. Undetectable watermarks for language models. Cryptology ePrint\nArchive, 2023.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint\narXiv:1803.05457, 2018.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems.\narXiv preprint arXiv:2110.14168, 2021.\nYihe Deng, Weitong Zhang, Zixiang Chen, and Quanquan Gu. Rephrase and respond: Let large language\nmodels ask better questions for themselves. arXiv preprint arXiv:2311.04205, 2023.\nMichael Duan, Anshuman Suri, Niloofar Mireshghallah, Sewon Min, Weijia Shi, Luke Zettlemoyer, Yulia\nTsvetkov, Yejin Choi, David Evans, and Hannaneh Hajishirzi. Do membership inference attacks work on\nlarge language models? arXiv preprint arXiv:2402.07841, 2024.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783, 2024.\nAngela Fan, Mike Lewis, and Yann Dauphin.\nHierarchical neural story generation.\narXiv preprint\narXiv:1805.04833, 2018.\nPierre Fernandez, Antoine Chaffin, Karim Tit, Vivien Chappelier, and Teddy Furon. Three bricks to consolidate\nwatermarks for large language models. 2023 IEEE International Workshop on Information Forensics and\nSecurity (WIFS), 2023.\nYu Fu, Deyi Xiong, and Yue Dong. Watermarking conditional text generation for ai detection: Unveiling\nchallenges and a semantic-aware watermark remedy. In Proceedings of the AAAI Conference on Artificial\nIntelligence, 2024.\nLeo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence\nGolding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa,\nJason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite,\nBen Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 07 2024.\nhttps://zenodo.org/records/12608602.\nElliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline Falkman\nOlsson, Jean-Stanislas Denain, Anson Ho, Emily de Oliveira Santos, et al. Frontiermath: A benchmark for\nevaluating advanced mathematical reasoning in ai. arXiv preprint arXiv:2411.04872, 2024.\nShahriar Golchin and Mihai Surdeanu. Data contamination quiz: A tool to detect and estimate contamination\nin large language models. arXiv preprint arXiv:2311.06233, 2023.\nChenchen Gu, Xiang Lisa Li, Percy Liang, and Tatsunori Hashimoto. On the learnability of watermarks for\nlanguage models. arXiv preprint arXiv:2312.04469, 2023.\n9\n\nValentin Hartmann, Anshuman Suri, Vincent Bindschaedler, David Evans, Shruti Tople, and Robert West. Sok:\nMemorization in general-purpose large language models. arXiv preprint arXiv:2310.18362, 2023.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\nMeasuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration.\narXiv preprint arXiv:1904.09751, 2019.\nBaihe Huang, Banghua Zhu, Hanlin Zhu, Jason D. Lee, Jiantao Jiao, and Michael I. Jordan. Towards optimal\nstatistical watermarking, 2023.\nMinhao Jiang, Ken Ziyu Liu, Ming Zhong, Rylan Schaeffer, Siru Ouyang, Jiawei Han, and Sanmi Koyejo.\nInvestigating data contamination for pre-training language models. arXiv preprint arXiv:2401.06059, 2024.\nNikola Jovanović, Robin Staab, Maximilian Baader, and Martin Vechev. Ward: Provable rag dataset inference\nvia llm watermarks. arXiv preprint arXiv:2410.03537, 2024.\nJohn Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. A watermark\nfor large language models. arXiv preprint arXiv:2301.10226, 2023a.\nJohn Kirchenbauer, Jonas Geiping, Yuxin Wen, Manli Shu, Khalid Saifullah, Kezhi Kong, Kasun Fernando,\nAniruddha Saha, Micah Goldblum, and Tom Goldstein. On the reliability of watermarks for large language\nmodels, 2023b.\nRohith Kuditipudi, John Thickstun, Tatsunori Hashimoto, and Percy Liang. Robust distortion-free watermarks\nfor language models. arXiv preprint arXiv:2307.15593, 2023.\nTaehyun Lee, Seokhee Hong, Jaewoo Ahn, Ilgee Hong, Hwaran Lee, Sangdoo Yun, Jamin Shin, and Gunhee\nKim. Who wrote this code? watermarking for code generation. arXiv preprint arXiv:2305.15060, 2023.\nJeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha,\nSedrick Keh, Kushal Arora, et al. Datacomp-lm: In search of the next generation of training sets for language\nmodels. arXiv preprint arXiv:2406.11794, 2024.\nAiwei Liu, Leyi Pan, Xuming Hu, Shiao Meng, and Lijie Wen. A semantic invariant robust watermark for large\nlanguage models. arXiv preprint arXiv:2310.06356, 2023.\nYepeng Liu and Yuheng Bu. Adaptive text watermark for large language models. arXiv preprint arXiv:2401.13927,\n2024.\nFatemehsadat Mireshghallah, Kartik Goyal, Archit Uniyal, Taylor Berg-Kirkpatrick, and Reza Shokri. Quan-\ntifying privacy risks of masked language models using membership inference attacks.\narXiv preprint\narXiv:2203.03929, 2022.\nDavid Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian\nMichael, and Samuel R Bowman. Gpqa: A graduate-level google-proof q&a benchmark. arXiv preprint\narXiv:2311.12022, 2023.\nTom Sander, Pierre Fernandez, Alain Durmus, Matthijs Douze, and Teddy Furon. Watermarking makes\nlanguage models radioactive. arXiv preprint arXiv:2402.14904, 2024.\nAaditya K Singh, Muhammed Yusuf Kocyigit, Andrew Poulton, David Esiobu, Maria Lomeli, Gergely Szilvasy,\nand Dieuwke Hupkes. Evaluation data contamination in llms: how do we measure it and (when) does it\nmatter? arXiv preprint arXiv:2411.03923, 2024.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971, 2023.\nA Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017.\nMathurin Videau, Badr Youbi Idrissi, Daniel Haziza, Luca Wehrstedt, Jade Copet, Olivier Teytaud, and\nDavid Lopez-Paz. Meta Lingua: A minimal PyTorch LLM training library, 2024. https://github.com/\nfacebookresearch/lingua.\nShuo Yang, Wei-Lin Chiang, Lianmin Zheng, Joseph E Gonzalez, and Ion Stoica. Rethinking benchmark and\ncontamination for language models with rephrased samples. arXiv preprint arXiv:2311.04850, 2023.\n10\n\nHugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, Will Song, Tiffany Zhao, Pranav Raja,\nDylan Slack, Qin Lyu, et al. A careful examination of large language model performance on grade school\narithmetic. arXiv preprint arXiv:2405.00332, 2024.\nXuandong Zhao, Yu-Xiang Wang, and Lei Li. Protecting language generation models via invisible watermarking.\nIn International Conference on Machine Learning, pages 42187–42199. PMLR, 2023.\nAppendix\nA\nAppendix\nA.1\nQualitative Examples\nTaking the example of a question from ARC-Easy, we compare qualitatively different watermarking\nstrength in Figure 6.\nA.2\nAdditional Experimental Results\nEvaluation Template.\nAs detailed in subsection 4.2, we evaluate the accuracy on the benchmark using\nboth the same template seen during contamination and an alternative one. Table 3 presents the results\nwhen evaluated with the same template. Without contamination, the model performs similarly across\nthe two templates, but a differences appear with contaminations. Even OOD, only 8 contaminated\nsteps out of 10k steps leads to +10% on all benchmark for these 1B-parameter language models.\nAblations on different benchmarks, watermark strength, watermark window sizes, and number\nof contaminations.\nResults for all benchmarks (ARC-Easy, ARC-Challenge, and MMLU∗), with\nvariations in watermark window size, number of contaminations, and watermark strength, are shown in\nFigure 7 for utility and Figure 8 for radioactivity detection. For utility, all models perform very similarly\non all the rephrased versions of the benchmarks, even when pushing the watermark to 80% of green\ntokens, although for MMLU∗, we observe some discrepancies. For instance, when using a watermarking\nwindow size of 2 (subfig i), the performance of Llama-3.2-1B increases from 38% to 42% between\nthe original and the other versions. However we observe the same issue when rephrasing without\nwatermarking in that case. The watermark window size does not have an impact. For radioactivity\ndetection on the other hand, as detailed in subsection 4.3, smaller window sizes correlates with lower\ndetection confidence.\nSystem Prompt: “You are a problem rephrasing assis-\ntant. Your task is to rephrase the given problem, which\nincludes a question, while ensuring that the rephrased\nversion is logically and contextually equivalent to the\noriginal. Do not provide answers or solutions to the\nproblem.”\nInstruction: “Please rephrase the following problem,\nensuring that the rephrased version is equivalent to the\noriginal in terms of logic, context, and details. Your\nresponse should only include the rephrased version of\nthe problem and question. Beginning of the problem:”\nQuestion: “The rate of acceleration of an object is\ndetermined by the mass of the object and”\nLlama-3-8B-Instruct Rephrased, δ = 0\nWhat factors, in addition to the mass of an object,\ninfluence its rate of acceleration? (47%)\nLlama-3-8B-Instruct Rephrased, δ = 0.5\nWhat factor, in addition to the mass of an object,\ninfluences the rate at which its acceleration changes\nover time? (55%)\nLlama-3-8B-Instruct Rephrased, δ = 2\nWhat factor, in addition to the mass of an object, is\na determining influence on its rate of acceleration?\n(63%)\nLlama-3-8B-Instruct Rephrased, δ = 4\nWhat factor, aside from an object’s mass, determines\nits acceleration? (73%)\nFigure 6 Benchmark watermarking example on a question of ARC-easy. The quality of the question is not\naffected by the rephrasing, even with strong watermark. The proportion of green tokens is given in parenthesis.\n11\n\nTable 3\nDetection and performance metrics across different levels of contamination for ARC-Easy, ARC-\nChallenge, and MMLU benchmarks, watermarked with δ = 4. The performance increase is for in distribution\nevaluation as detailed in subsection 4.2. Similar results for OOD are shown in Table 1.\nARC-Easy (1172 quest.)\nARC-Challenge (2373 quest.)\nMMLU∗(5000 quest.)\nContaminations\nlog10(p)\nAcc. (% ∆)\nlog10(p)\nAcc. (% ∆)\nlog10(p)\nAcc. (% ∆)\n0\n-0.3\n51.7 (+0.0)\n-0.3\n28.5 (+0.0)\n-0.9\n30.4 (+0.0)\n4\n-3.0\n61.3 (+9.9)\n-1.2\n35.1 (+7.0)\n-5.7\n36.9 (+6.5)\n8\n-5.5\n68.2 (+16.9)\n-4.5\n42.2 (+14.1)\n<-12\n43.0 (+12.6)\n16\n<-12\n84.1 (+32.8)\n<-12\n65.3 (+37.2)\n<-12\n62.1 (+31.7)\nNA\n49.5%\n=0\n47.2%\n=1\n54.3%\n=2\n61.2%\n=4\n74.1%\nWatermark strength  and proportion of green tokens\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nOriginal Benchmark\nWatermarked Benchmarks\nLlama-3.2-1B\nLlama-3.2-3B\nLlama-3.1-8B\n(a) ARC-Easy, Window size 0\nNA\n52.2%\n=0\n50.8%\n=1\n59.4%\n=2\n67.1%\n=4\n79.6%\nWatermark strength  and proportion of green tokens\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nOriginal Benchmark\nWatermarked Benchmarks\nLlama-3.2-1B\nLlama-3.2-3B\nLlama-3.1-8B\n(b) ARC-Easy, Window size 1\nNA\n49.8%\n=0\n49.9%\n=1\n58.7%\n=2\n66.3%\n=4\n79.5%\nWatermark strength  and proportion of green tokens\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nOriginal Benchmark\nWatermarked Benchmarks\nLlama-3.2-1B\nLlama-3.2-3B\nLlama-3.1-8B\n(c) ARC-Easy, Window size 2\nNA\n49.7%\n=0\n47.8%\n=1\n54.9%\n=2\n61.8%\n=4\n74.6%\nWatermark strength  and proportion of green tokens\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nOriginal Benchmark\nWatermarked Benchmarks\nLlama-3.2-1B\nLlama-3.2-3B\nLlama-3.1-8B\n(d) ARC-Challenge, Window size 0\nNA\n51.8%\n=0\n51.1%\n=1\n59.4%\n=2\n67.4%\n=4\n79.7%\nWatermark strength  and proportion of green tokens\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nOriginal Benchmark\nWatermarked Benchmarks\nLlama-3.2-1B\nLlama-3.2-3B\nLlama-3.1-8B\n(e) ARC-Challenge, Window size 1\nNA\n50.2%\n=0\n49.9%\n=1\n58.6%\n=2\n66.6%\n=4\n79.6%\nWatermark strength  and proportion of green tokens\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nOriginal Benchmark\nWatermarked Benchmarks\nLlama-3.2-1B\nLlama-3.2-3B\nLlama-3.1-8B\n(f) ARC-Challenge, Window size 2\nNA\n50.1%\n=0\n48.2%\n=1\n55.1%\n=2\n61.2%\n=4\n74.2%\nWatermark strength  and proportion of green tokens\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nOriginal Benchmark\nWatermarked Benchmarks\nLlama-3.2-1B\nLlama-3.2-3B\nLlama-3.1-8B\n(g) MMLU∗, Window size 0\nNA\n51.1%\n=0\n51.4%\n=1\n59.4%\n=2\n66.9%\n=4\n79.1%\nWatermark strength  and proportion of green tokens\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nOriginal Benchmark\nWatermarked Benchmarks\nLlama-3.2-1B\nLlama-3.2-3B\nLlama-3.1-8B\n(h) MMLU∗, Window size 1\nNA\n50.1%\n=0\n50.5%\n=1\n58.7%\n=2\n66.5%\n=4\n78.9%\nWatermark strength  and proportion of green tokens\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nOriginal Benchmark\nWatermarked Benchmarks\nLlama-3.2-1B\nLlama-3.2-3B\nLlama-3.1-8B\n(i) MMLU∗, Window size 2\nFigure 7 Comparison of Llama3 model performance on various versions of ARC-Easy, ARC-Challenge, and\nMMLU∗for different watermark window sizes. Each row corresponds to a different dataset, and each column\ncorresponds to a different window size. The window size does not noticeably impact the benchmark’s utility.\n12\n\n0\n1\n2\n4\nWatermark strength \n0\n4\n8\n12\nDetection confidence (- Log10 P-Value)\n4 contaminations\n8 contaminations\n16 contaminations\n(a) ARC-Easy, Window size 0\n0\n1\n2\n4\nWatermark strength \n0\n4\n8\n12\nDetection confidence (- Log10 P-Value)\n4 contaminations\n8 contaminations\n16 contaminations\n(b) ARC-Easy, Window size 1\n0\n1\n2\n4\nWatermark strength \n0\n4\n8\n12\nDetection confidence (- Log10 P-Value)\n4 contaminations\n8 contaminations\n16 contaminations\n(c) ARC-Easy, Window size 2\n0\n1\n2\n4\nWatermark strength \n0\n4\n8\n12\nDetection confidence (- Log10 P-Value)\n4 contaminations\n8 contaminations\n16 contaminations\n(d) ARC-Challenge, Window size 0\n0\n1\n2\n4\nWatermark strength \n0\n4\n8\n12\nDetection confidence (- Log10 P-Value)\n4 contaminations\n8 contaminations\n16 contaminations\n(e) ARC-Challenge, Window size 1\n0\n1\n2\n4\nWatermark strength \n0\n4\n8\n12\nDetection confidence (- Log10 P-Value)\n4 contaminations\n8 contaminations\n16 contaminations\n(f) ARC-Challenge, Window size 2\n0\n1\n2\n4\nWatermark strength \n0\n4\n8\n12\nDetection confidence (- Log10 P-Value)\n4 contaminations\n8 contaminations\n16 contaminations\n(g) MMLU∗, Window size 0\n0\n1\n2\n4\nWatermark strength \n0\n4\n8\n12\nDetection confidence (- Log10 P-Value)\n4 contaminations\n8 contaminations\n16 contaminations\n(h) MMLU∗, Window size 1\n0\n1\n2\n4\nWatermark strength \n0\n4\n8\n12\nDetection confidence (- Log10 P-Value)\n4 contaminations\n8 contaminations\n16 contaminations\n(i) MMLU∗, Window size 2\nFigure 8 Comparison of radioactivity detection on various versions of ARC-Easy, ARC-Challenge, and MMLU∗\nfor different watermark window sizes. Each row corresponds to a different dataset, and each column corresponds\nto a different window size. Bigger benchmarks leads to easier detection, and window size impacts the detection\nconfidence, the larger the better, accross all benchmarks.\n13\n",
  "metadata": {
    "source_path": "papers/arxiv/Detecting_Benchmark_Contamination_Through_Watermarking_0e7bb700149babc4.pdf",
    "content_hash": "0e7bb700149babc4d0ef7075c07f909b84ee147bf74f88ad5acaeab9a57327bf",
    "arxiv_id": null,
    "title": "Detecting_Benchmark_Contamination_Through_Watermarking_0e7bb700149babc4",
    "author": "",
    "creation_date": "D:20250225025555Z",
    "published": "2025-02-25T02:55:55",
    "pages": 13,
    "size": 1109018,
    "file_mtime": 1740470166.2098575
  }
}