{
  "text": "Understanding the Uncertainty of LLM Explanations: A\nPerspective Based on Reasoning Topology\n†Longchao Da, †Xiaoou Liu, †Jiaxin Dai,\n‡Lu Cheng, §Yaqing Wang, †Hua Wei\n†Arizona State University, ‡University of Illinois Chicago, §Purdue University.\nhua.wei@asu.edu\nAbstract\nUnderstanding the uncertainty in large language model (LLM) ex-\nplanations is important for evaluating their faithfulness and rea-\nsoning consistency, and thus provides insights into the reliability\nof LLM’s output regarding a question. In this work, we propose a\nnovel framework that quantifies uncertainty in LLM explanations\nthrough a reasoning topology perspective. By designing a structural\nelicitation strategy, we guide the LLMs to frame the explanations of\nan answer into a graph topology. This process decomposes the ex-\nplanations into the knowledge related sub-questions and topology-\nbased reasoning structures, which allows us to quantify uncertainty\nnot only at the semantic level but also from the reasoning path.\nIt further brings convenience to assess knowledge redundancy\nand provide interpretable insights into the reasoning process. Our\nmethod offers a systematic way to interpret the LLM reasoning,\nanalyze limitations, and provide guidance for enhancing robustness\nand faithfulness. This work pioneers the use of graph-structured\nuncertainty measurement in LLM explanations and demonstrates\nthe potential of topology-based quantification. The response data\nand code will be released upon publication.\nCCS Concepts\n• Computing methodologies →Artificial intelligence; Nat-\nural language processing; Uncertainty quantification; Machine\nlearning algorithms; Explainable AI.\nKeywords\nUncertainty Quantification, Natural Language Explanation, Large\nLanguage Models\nACM Reference Format:\n†Longchao Da, †Xiaoou Liu, †Jiaxin Dai,, ‡Lu Cheng, §Yaqing Wang, †Hua\nWei. 2018. Understanding the Uncertainty of LLM Explanations: A Per-\nspective Based on Reasoning Topology. In Proceedings of Make sure to en-\nter the correct conference title from your rights confirmation email (Confer-\nence acronym ’XX). ACM, New York, NY, USA, 15 pages. https://doi.org/\nXXXXXXX.XXXXXXX\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nConference acronym ’XX, Woodstock, NY\n© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-XXXX-X/2018/06\nhttps://doi.org/XXXXXXX.XXXXXXX\n1\nIntroduction\nDeep learning models have long been criticized for their lack of\ntrustworthiness due to their complex network structures and opaque\ndecision-making processes [10, 23, 36]. This has motivated researchers\nto investigate methods for understanding and quantifying the un-\ncertainty associated with these models [1, 26, 27]. Recently, Large\nLanguage Models (LLMs) have demonstrated significant advance-\nments over traditional deep learning approaches across a variety\nof tasks [31, 47]. However, concerns about their reliability persist.\nLLMs often produce outputs that are difficult to verify, particu-\nlarly in scenarios requiring complex reasoning [38]. This intro-\nduces risks in critical applications, such as healthcare or legal do-\nmains [5, 18], where incorrect or unreliable reasoning can have\nsevere consequences. Properly quantifying uncertainty in the rea-\nsoning processes of LLMs is therefore crucial for ensuring their\nsafe and effective deployment.\nExisting research on Uncertainty Quantification (UQ) for LLMs\nprimarily focuses on analyzing semantic uncertainty [9, 20, 25, 34],\nwhich involves examining patterns in the meaning and phrasing of\nmultiple responses generated for a given question. Although this\napproach provides insights into output-level variability, it neglects\nthe logical reasoning steps that lead to these answers. As a result,\nit fails to address foundational issues in the reasoning process that\ncould be used to debug or optimize model outputs. For instance,\nwhen asked by the the same question, different reasoning paths\nmay converge on the same final answer (an example shown in\nAppendix Figure 6), yet some paths may involve inconsistencies or\nlogical leaps. By quantifying uncertainty at the level of reasoning\nsteps, we can better identify such inconsistencies, support human-\nin-the-loop systems for validating outputs in sensitive applications,\nand uncover weaknesses in a model’s reasoning process. This high-\nlights the importance of incorporating reasoning-based uncertainty\nquantification.\nIn this paper, we address the problem of uncertainty quantifi-\ncation for logical reasoning steps by explicitly modeling reason-\ning processes as logical topologies. Existing work often treats the\nreasoning process generated by single Chain-of-Thought (CoT) se-\nquences as a single \"long answer\" [42, 44], calculating semantic\nconsistencies directly. While this approach captures some aspects\nof reasoning paths, it oversimplifies real-world reasoning processes,\nwhich often involve hierarchical dependencies and parallel sub-\ntasks. To overcome these limitations, we propose a novel formalism\nthat explicitly models reasoning as a logical graph. In this represen-\ntation, nodes correspond to individual reasoning steps, while edges\ncapture logical dependencies between them. This structure enables\nmore granular and interpretable analyses of uncertainty.\narXiv:2502.17026v1  [cs.CL]  24 Feb 2025\n\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\nTrovato et al.\nBased on the structural representation, we introduce a graph-\nbased measure for assessing uncertainty, by first encoding the node\nand edge descriptions into the semantic embeddings, and then per-\nforming a graph-edit-distance comparison, our framework captures\nthe uncertainty from both semantic and topology aspects. Besides,\nwe also propose a redundancy measure for the valid reasoning\npath, which helps understand the LLM’s reasoning effectiveness.\nExtensive experiments on diverse datasets and LLMs demonstrate\nthe utility of the proposed quantification methods. In summary, the\ncontributions of this paper are:\n• We identify limitations in existing UQ approaches for LLMs\nand propose a novel framework that explicitly models the\nreasoning process from explanations as a logical topology.\n• We introduce multiple measures, including graph-based un-\ncertainty measures and redundancy metrics, to provide gran-\nular reasoning variance and interpretable assessments of\nredundancy.\n• We demonstrate the effectiveness of our framework through\nextensive experiments across multiple datasets and LLMs.\nOur results highlight its ability to identify inconsistencies\nin reasoning paths, improving trustworthiness, and inter-\npretability. Additionally, we discover three widely adopted\nreasoning patterns in LLMs and show the chance of improve-\nment under the redundancy measure.\n2\nRelated Work\nIn this section, we review the related work in the research domains\nof uncertainty quantification (UQ) for large language models (LLMs)\nand methods for explanation-based UQ, with a focus on reasoning\nprocesses.\n2.1\nUQ for LLM\nWhite-box Approaches. A significant body of research has fo-\ncused on performing UQ for LLMs by inducing the models to output\ntheir uncertainty along with their responses [19, 24, 28, 40]. These\nmethods often rely on token-level probabilities to train or fine-tune\nmodels for predicting uncertainty. While effective, these approaches\nrequire full access to the model’s structure and weights, which is\nimpractical for black-box or commercial LLMs. For example, su-\npervised methods such as those in [19] estimate uncertainty using\nlogits and ground truth labels but are computationally expensive\nand resource-intensive.\nBlack-box Approaches. Another line of work estimates uncer-\ntainty directly at the response level using semantic entropy [20].\nWhile this method avoids token-level dependencies, it still relies on\naccess to token probabilities, limiting its applicability in black-box\nsettings. To address these limitations, researchers have proposed\nlightweight black-box methods that analyze response inconsisten-\ncies. For instance, Lin et al. [25] use graph Laplacian eigenvalues\nas an uncertainty indicator, while Chen and Mueller [6] computes\nconfidence scores from generated outputs to identify speculative or\nunreliable answers. However, these approaches primarily focus on\nsemantic-level analysis and neglect the logical structure underlying\nreasoning processes. Moreover, methods like [25] average entail-\nment probabilities without considering directional information in\nreasoning paths.\nProxy Based (Explain) \nWhite Box\nquery\nquery\nAnswer1\nAnswer2\nExplain 1\nExplain 2\nAnswer\nDirect Measure \nBlack Box\nFigure 1: The category of the LLM uncertainty measure. It can\nbe divided into two types, direct measure and proxy-based\nmeasure (use the explanations of each answer).\nOur work is agnostic to the white box or black box since it\nleverages the generated explanations as a proxy to measure the\nreasoning uncertainty as in Figure 1. This enables a more nuanced\nand interpretable assessment of uncertainty in reasoning processes.\n2.2\nUQ for LLM Explanation\nExplanation-based UQ focuses on assessing the reliability of nat-\nural language explanations (NLEs) generated by LLMs by either\nprompting models to express confidence in their explanations or\nanalyzing consistency across multiple outputs under varying con-\nditions [39, 45]. While these methods provide insights into expla-\nnation robustness, they treat explanations to a question as a un-\nstructured text representation, which lacks structural information\nand fails to capture inconsistencies or leaps in logic. In contrast,\nour work explicitly leverages well-structured reasoning topologies\nto enhance the UQ process for explanations. This structured repre-\nsentation enables us to assess explanation uncertainties at a finer\ngranularity within complex reasoning paths.\n3\nPreliminaries\nIn this section, we provide the foundational concepts for our study,\nincluding the definition of uncertainty in LLMs and the quantifica-\ntion of uncertainty in natural language explanations (NLE).\n3.1\nUncertainty of LLMs\nUncertainty quantification (UQ) has been a critical topic in classical\nmachine learning tasks such as classification and regression [1,\n11, 16, 21]. However, natural language generation (NLG) poses\nunique challenges for uncertainty quantification due to its high-\ndimensional output space, the need to account for semantic equiva-\nlence across distinct token sequences, and the limited accessibility\nof internal model outputs in black-box LLMs [25].\nGiven these challenges, most UQ methods for LLMs focus on\nanalyzing uncertainty directly from model-generated responses as\nshown in the left part of Figure. 1. The problem could be defined as\nfollows:\nProblem 1 (Uncertainty Quantification for LLMs). When\nan LLM 𝑀is provided with an input 𝑥, either a query or prompt, the\ngoal of an uncertainty function 𝑈𝑥is to map the generated outputs to\n\nUnderstanding the Uncertainty of LLM Explanations: A Perspective Based on Reasoning Topology\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\na scalar score that determines the uncertainty of the LLM 𝑀, i.e.,\n𝑈𝑥= U \u0000{𝑀(𝑥𝑖)}𝑛\n𝑖=1\n\u0001\n(1)\nHere, {𝑀(𝑥𝑖)}𝑛\n𝑖=1 denotes a set of 𝑛responses generated by the\nmodel 𝑀, and U aggregates uncertainty across multiple responses.\nNote that while noticed that some work models have uncertainty\ndescribing the confidence of a specific output given the input [39],\nin this paper𝑈𝑥only depends on 𝑥and is a property of the predicted\ndistribution, which is estimated by U that aggregates uncertainty\nacross multiple responses.\nDepending on the specific UQ method used, each 𝑥𝑖may corre-\nspond to either repeated inputs or rephrased variations of the input\nprompt 𝑥. For black-box methods that analyze response variability\nor semantic consistency [25], multiple outputs (𝑛> 1) are typi-\ncally required. In contrast, white-box methods that rely on internal\nmodel information such as logits [19] may only require a single\noutput (𝑛= 1). However, both black-box and white-box methods\nhave limitations when applied to reasoning tasks. Black-box meth-\nods often focus on semantic output variability without capturing\ndeeper uncertainties in reasoning steps. White-box methods are\nfrequently inaccessible due to API restrictions or computational\nconstraints.\n3.2\nUncertainty of LLM Explanations\nTo address these issues, researchers try to understand uncertainties\nin reasoning processes through the natural language explanations\n(NLEs) [4, 39] as a proxy. As shown in the right part of Figure. 1,\nAn NLE is a textual reasoning sequence generated by a language\nmodel 𝑀, typically derived to justify or explain the answer 𝑎for a\ngiven input question 𝑥𝑞. We formally define it as follows:\nDefinition 1 (Natural Language Explanation). Given a\nmodel 𝑀and an input prompt 𝑥𝑞, an NLE can be represented as:\n𝑀(𝑥𝑞+ 𝑥𝑒) = 𝑎+ 𝑎𝑒\n(2)\nwhere 𝑥𝑒is an explanation-specific prompt, 𝑎is the model’s answer\nto the query 𝑥𝑞, and 𝑎𝑒is the generated explanation accompanying\nthe answer.\nThe explanation 𝑎𝑒contains a sequence of reasoning steps, repre-\nsented as 𝑎𝑒= {𝑠1,𝑠2, ...,𝑠𝑚}, which capture the reasoning process\nor justification for 𝑎.\nTo quantify uncertainty in explanations, we extend Problem 1\nto consider 𝑛explanations generated for the same query 𝑥𝑞. Each\nexplanation 𝑎𝑒\n𝑖(𝑖∈{1, 2, . . . ,𝑛}) corresponds to a set of reasoning\nsteps derived from the same query. Each explanation consists of 𝑚\nreasoning steps, represented as 𝑎𝑒\n𝑖= {𝑠𝑖,1,𝑠𝑖,2, ...,𝑠𝑖,𝑚}. The overall\nuncertainty across all 𝑛explanations is captured by aggregating\nreasoning-level uncertainties for each explanation. This can be\nformally defined as follows:\nProblem 2 (Uncertainty Quantification for LLM Expla-\nnations). Given an input prompt 𝑥𝑞and an explanation-specific\nprompt 𝑥𝑒, the model 𝑀generates a set of answers 𝑎𝑖to the query 𝑥𝑞,\nalong with accompanying explanations 𝑎𝑒\n𝑖. The uncertainty for the\nquery 𝑥𝑞is then defined as:\n𝑈𝑥𝑞= U\n 𝑛\nØ\n𝑖=1\n𝑎𝑒\n𝑖\n!\n= U\n 𝑛\nØ\n𝑖=1\n{𝑠𝑖,1,𝑠𝑖,2, ...,𝑠𝑖,𝑚}\n!\n(3)\nHere, 𝑈𝑥𝑞represents the overall uncertainty for the query 𝑥𝑞,\nand U aggregates uncertainties across all reasoning steps from all\n𝑛explanations.\nUnlike prior methods that focus on token-level or semantic vari-\nability [39], in this paper, we explicitly models reasoning structures\nwithin explanations. By leveraging logical topologies, we aim to\ncapture nuanced uncertainties at both the explanation level and\nindividual reasoning step level.\nQ: If it is currently Summer in Australia, what season is it in Canada? \nFigure 2: The illustration of how the topology is constructed.\n4\nMethod\nIn this section, we propose a novel framework for reasoning uncer-\ntainty quantification in large language models (LLMs). To capture\nthe complexity of reasoning paths, the proposed framework con-\nsists of two main steps: (1) Reasoning Topology Elicitation, which\nconstructs a structured reasoning graph from LLM-generated expla-\nnations, and (2) Topology-enabled Reasoning Quantification, which\nleverages the constructed graph to perform uncertainty quantifica-\ntion using measures such as graph edit distance and reasoning path\nredundancy analysis. These steps work together as a comprehensive\nframework for analyzing reasoning uncertainty in LLMs.\n4.1\nReasoning Topology Elicitation\nThe objective of this step is to construct a structured reasoning\ntopology that captures the complexity of reasoning paths generated\nby large language models (LLMs). Existing approaches [39] repre-\nsent reasoning explanation as linear sequences of steps from Chain-\nof-Thought (CoT) prompting [43]. While linear text sequences\nprovide basic interpretability, they fail to capture complex logi-\ncal transitions required for tasks like comparative reasoning or\nmulti-faceted conclusions. This lack of structural richness limits\nthe ability to analyze and quantify uncertainty in LLM-generated\nreasoning processes.\nTo address these limitations, we propose to elicit reasoning\ntopologies from a question 𝑥𝑞to an answer 𝑎as a directed graph\nG = (V, E), where V is the set of nodes corresponding to knowl-\nedge points or intermediate steps, and E is the set of edges capturing\nlogical dependencies between them. For example, given the query\n𝑥𝑞: “What are the causes of climate change?\", the knowledge points\n𝐾𝑞might include sub-questions such as 𝑘1: “What is the role of\ngreenhouse gases?\", 𝑘2: “How does deforestation contribute?\", and\n\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\nTrovato et al.\n𝑘3: “What is the impact of industrial activities?\". The corresponding\nanswers 𝐴= {𝑎1,𝑎2,𝑎3} provide detailed explanations for each sub-\nquestion towards the final answer 𝑎3. These knowledge-answer\npairs are then connected based on their logical dependencies to\nform the reasoning topology graph G𝑞. This graph-based repre-\nsentation enables a more comprehensive understanding of the rea-\nsoning process and provides a foundation for richer uncertainty\nquantification.\nSpecifically, the construction of G consists of three modules: (1)\nKnowledge Point Reflection, where the LLM identifies sub-questions\nor knowledge points required to address the query; (2) Self-Answering\nModule, where the LLM generates answers for each identified knowl-\nedge point; and (3) Reasoning Topology Construction, where the\nknowledge-answer pairs are organized into a directed graph that\nreflects the overall reasoning path.\n4.1.1\nKnowledge Point Reflection Module. The first module, the\nknowledge point reflection module, involves eliciting sufficient\ninformation that can be used to support the conclusion drawing\ntoward the input 𝑥𝑞. The input to this module is the input query\n𝑥𝑞along with the prompt template 𝑇1 to encourage the LLM to\nreflect ‘What knowledge basis (or sub-questions) it should know to\ndraw a final conclusion?’. And the output of this module is the set\nof knowledge points 𝐾𝑞extracted as a series of sub-questions, i.e.,\n𝐾𝑞= {𝑘1,𝑘2, . . . ,𝑘𝑛}. Specifically, we design a prompt template 𝑇1\nto guide the model in reflecting on the sub-questions or knowledge\npoints required for solving 𝑥𝑞:\nTemplate𝑇1: Given a question {𝑥𝑞}, reflect and generate the knowl-\nedge points or sub-questions necessary to solve this query. Ensure\nthat the output is both sufficient and concise.\nThe model generates a set of knowledge points 𝐾𝑞= 𝑀(𝑥𝑞,𝑇1) =\n{𝑘1,𝑘2, . . . ,𝑘𝑛}, where each𝑘𝑖corresponds to a specific sub-question\nor piece of information identified as necessary to address the query\n𝑥𝑞under the guidance of prompt 𝑇1. To ensure traceability, we\nassign unique identifiers to each knowledge point using a tagging\nfunction 𝑓(·):\n𝐾𝑞\ntag = {id1 : 𝑘1, id2 : 𝑘2, . . . , id𝑛: 𝑘𝑛}.\n(4)\nIn the later sections of this paper, we assume the 𝑘𝑞always\ncarries its identifier while performing computing (𝐾𝑞⇔𝐾𝑞\n𝑡𝑎𝑔).\n4.1.2\nSelf-answering Module. To provide answers for the knowl-\nedge points 𝐾𝑞= {𝑘1,𝑘2, . . . ,𝑘𝑛} elicited in the previous module,\nwe design a self-answering module to generate precise answers. For\neach sub-question 𝑘𝑖∈𝐾𝑞, the model 𝑀generates an answer 𝑎𝑖\nusing the following prompt 𝑇2, ensuring coherence and sufficiency\nin addressing each of the knowledge points:\nTemplate 𝑇2: Given a sub-question {𝑘𝑖}, provide a precise answer\nthat directly addresses the query without further discussion.\nThe model generates answers 𝐴= {𝑎1,𝑎2, . . . ,𝑎𝑛}, where each\nanswer 𝑎𝑖= 𝑀(𝑘𝑖,𝑇2). So we have:\n𝐴= {𝑎1,𝑎2, . . . ,𝑎𝑛} = {𝑀{𝑘1,𝑇2}, 𝑀{𝑘2,𝑇2}, . . . , 𝑀{𝑘𝑛,𝑇2}} (5)\nThis formulation explicitly links each sub-question 𝑘𝑖to its cor-\nresponding answer 𝑎𝑖, ensuring clarity in the relationship between\nthe elicited knowledge points and their responses. The resulting\nset of knowledge-answer pairs forms the basis for constructing the\nreasoning topology:\nD𝑚= {(𝑘1,𝑎1), (𝑘2,𝑎2), . . . , (𝑘𝑛,𝑎𝑛)}\n(6)\n4.1.3\nReasoning Topology Construction Module. To construct the\nreasoning topology graph G𝑞= (V, E), a critical step would be\nto connect the (𝑘, 𝑎) pairs in a structured format based on their\nlogical dependencies. Since we are quantifying the uncertainty of\nLLM explanations, this connection should be determined by the\nmodel itself to explain. Therefore, in this module, we leverage the\nfew-shot learning ability of LLMs and guide them in connecting the\nbasis (𝑘, 𝑎) pairs following their reasoning procedure. By sampling F\namount of 𝑒𝐹as few-shot examples from a demonstration set F and\nfeeding them to the model, the LLM learns to depict the reasoning\npath in a structured way for this task1: ˆD𝑚= 𝑀(D𝑚,𝑒𝐹), the\ntransformation from D𝑚to ˆD𝑚follows:\nˆD𝑚= 𝑀(D𝑚,𝑒𝐹) = {(𝑎𝑝1,𝑘1,𝑎1), (𝑎𝑝2,𝑘2,𝑎2), . . . , (𝑎𝑝𝑛,𝑘𝑛,𝑎𝑛)}\n(7)\nwhere each 𝑎𝑝𝑖is an answer node that connects to the correspond-\ning knowledge-answer pair (𝑘𝑖,𝑎𝑖).\nTo ensure that the reasoning path forms a structured yet flexible\ntopology that adapts to the complexity of real-world cases, the\nspecific ordering of 𝑝𝑖is not predetermined and depends on the\nactual reasoning structure generated by the model. Then for better\nillustration, we switch the order in the tuple as below, by applying\ngraph concepts, we have the first two as the ‘node’ positions and\nthe last as the ‘edge’ position:\n(𝑎𝑝1,𝑘1,𝑎1) ⇒(𝑎𝑝1,𝑎1\n| {z }\n𝑛𝑜𝑑𝑒𝑠\n,\n𝑘1\n|{z}\n𝑒𝑑𝑔𝑒\n)\n(8)\nwhere the order of two nodes is defined by the reasoning LLM.\nNow we can write a basic reasoning step as:\nStep𝑖𝑗= [node𝑖, node𝑗, edge𝑖𝑗]\n(9)\nwhere node𝑖is the starting node representing either a question, a\nsub-question, or an intermediate response, node𝑗is the resulting\nnode from node𝑖, and connected by edge𝑖𝑗, which serves as the\nreasoning operation or sub-question.\nSpecifically, for the initial input query 𝑥𝑞, we denote the node\nas nodeRaw; for the final answer 𝑎, we denote as nodeResult. All\nother steps in the middle are the reasoning process, with a clearly\ndefined structure. The final graph structure includes all reasoning\nsteps from query 𝑥𝑞to the final answer 𝑎as nodes (𝑣𝑖) and their\ndependencies as edges (𝑒𝑖𝑗): The reasoning process from query 𝑞to\nthe final answer 𝑎can be finalized as a directed graph structure\nG𝑞= (V, E),\n(10)\nwhere\nV = {nodeRaw, node1, . . . , nodeResult} = {𝑣0, 𝑣1, ...}\n(11)\nand the edges are expressed as:\nE = {𝑒𝑖𝑗(1),𝑒𝑖𝑗(2)...}\n(12)\n1Please find details of few-shot learning in Appendix.\n\nUnderstanding the Uncertainty of LLM Explanations: A Perspective Based on Reasoning Topology\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\nwhere 𝑒stands for edge and {𝑒𝑖𝑗| edge𝑖𝑗: node𝑖→node𝑗}, 𝑒𝑖𝑗\nrepresents reasoning operations or dependencies between nodes.\n(start with index ‘1’ since we assume ‘0’ is the nodeRaw). The graph-\nbased structure captures the full reasoning topology, including\nbranching, dependencies, and multi-step interactions, which allows\nfor better reflection of the relationships between intermediate steps.\nNow from a graph concept, the reasoning steps combined with\nEq. 9 are formalized as below:\n𝑆= {Step𝑖𝑗| Step𝑖𝑗= [𝑣𝑖, 𝑣𝑗,𝑒𝑖𝑗]𝑣𝑖, 𝑣𝑗∈V,𝑒𝑗∈E}\n(13)\nwhere each triplet represents a logical transition between reasoning\nsteps. Note that for complex reasoning, the final answer does not\nnecessarily rely on all of the reasoning steps. For example, when\nbeing asked about \"if currently is summer in Australia, what season\nis it in Canada?\", in the reasoning chain, some of the LLM might\ndelve into ‘what causes the season differences’, which is redundant\nsteps in a concise reasoning.\n4.2\nTopology-enabled Reasoning Quantification\nOur framework enables multidimensional uncertainty analysis\nthrough structural and semantic examination of reasoning topolo-\ngies. Given a query 𝑥𝑞, the model 𝑀will be asked 𝐿times for expla-\nnation elicitation, on which 𝐿reasoning topologies {G𝑞\n𝑖}𝐿\n𝑖=1 will be\ngenerated from the previous step. We can measure the consistencies\nof {G𝑞\n𝑖}𝐿\n𝑖=1, where both graph structure for reasoning topology, and\nthe embeddings of node and edge sentences for semantics will be\nconsidered.\n4.2.1\nLLM Reasoning Uncertainty Based on Graph Edit Distance.\nWe quantify structural uncertainty through comparative analysis of\nmultiple reasoning topologies {G𝑖}𝐿\n𝑖=1 generated for the same query\n𝑥𝑞. Traditional graph comparison method [3] focuses on matching\n‘sets’ of node embeddings in a broad sense (e.g., across various struc-\ntured data domains like social networks [17] and chemistry [13]),\nbut we would expect to quantify based on the reasoning steps,\nwhich requires a more fine-grained design of distance measure. To\ntackle the above issue, we first use context-aware embeddings for\nsemantic encoding, and then design a fine-grained, reasoning-step\nbased Graph Edit Distance (GED). Specifically, we compare the\nreasoning structure by jointly considering semantic similarity and\nstructural alignment in a three-step process:\nStep1: Semantic Embedding. In order to measure semantic\nmeanings of reasoning steps, for each graph G ∈{G𝑞\n𝑖}𝐿\n𝑖=1 we\nemploy an embedding function L to encode the representation of\nnodes and edges in graph G = (V, E). Since each node 𝑣∈V and\nedge 𝑒∈E serves as a textual description, we can derive contextual\nembeddings:\nh𝑣= L(𝑣),\nh𝑒= L(𝑒),\n∀𝑣∈V,𝑒∈E\n(14)\nIn this paper, we use BERT as our embedding function L but\nother embeddings could also be used for different domain con-\ntexts, e.g, [35] for medical text. This step encodes the semantics\nof nodes and edges while preserving the logical structure of the\nreasoning process.\nStep2: Reasoning Topology Distance. In our setting, we have\n𝐿reasoning structures {G𝑞\n𝑖}𝐿\n𝑖=1 generated. To measure the pairwise\ndistance of two reasoning structures G1 and G2, inspired by the\nconcept of graph edit distance [12], we use the minimum transfor-\nmations required to align the two graphs to quantify their pairwise\ndistance.\nA. Substitution Costs: For two corresponding nodes in different\nreasoning topology graphs G1 = (V1, E1) and G2 = (V2, E2) we\ndefine the semantic substitution cost based on 𝑣𝑖∈V1 and 𝑒𝑘∈E1:\n𝑐(𝑣𝑖, 𝑣𝑗,𝑒𝑘,𝑒𝑚) =\n(\n1 −cos(h𝑣\n𝑖, h𝑣\n𝑗),\nnode substitute\n1 −cos(h𝑒\n𝑘, h𝑒𝑚),\nedge substitute\n(15)\nwhere cosine similarity measures the semantic alignment of reason-\ning steps and elicitation questions, this can capture the difference\nfrom direct meaning: either given the similar elicitation (edge sub-\nquestion), the sub-response (nodes) are different - there might exist\nan incorrect answer, or different edges that lead to the similar re-\nsponse - there might be a jumping step.\nB. Deletion/Insertion Costs: The cost of deleting a node or an\nedge is computed based on its average similarity to other nodes or\nedges within the same reasoning topology G [2, 37]. For the two\nreasoning graphs, we compute the deletion cost for a node 𝑣𝑖∈V1\nor an edge 𝑒𝑘∈E1 with respect to the graph G1, as follows:\n𝑐del.(𝑣𝑖, 𝑣𝑗,𝑒𝑘,𝑒𝑚) =\n\n\n1 −\n1\n|V1|−1\nÍ\n𝑣𝑗∈V1,\n𝑣𝑗≠𝑣𝑖\ncos(h𝑣\n𝑖, h𝑣\n𝑗),\nnode delete,\n1 −\n1\n| E1|−1\nÍ\n𝑒𝑚∈E1,\n𝑒𝑚≠𝑒𝑘\ncos(h𝑒\n𝑘, h𝑒𝑚),\nedge delete.\n(16)\nwhere (·) is the same as in Eq. 15,\nÍ\n𝑣𝑗∈V1,𝑣𝑗≠𝑣𝑖\ncos(h𝑣\n𝑖, h𝑣\n𝑗) shows the\nsemantic connectivity from node 𝑣𝑖to other nodes in the topology, if\nthe embedding meaning is closer to other nodes, the value is larger,\nand then the value is normalized by the number of remaining nodes\n|V1| −1, and subtracted from 1 to compute the deletion cost. Highly\nsimilar nodes or edges in G1 (e.g., redundant sub-questions) will\nhave lower deletion costs, as their removal minimally impacts the\nreasoning flow. Conversely, unique or critical nodes and edges (e.g.,\nimportant conclusions or key transitions) will incur higher deletion\ncosts due to their significant role in maintaining reasoning integrity\nand structural coherence.\nStep3: Graph Distance for Reasoning Uncertainty. Based\non the above two steps, we can derive the overall graph edit cost in\njoint consideration of semantic meaning and topology variance as:\nGED(G1, G2) = 𝐶sub.(P) + 𝐶del.(V1, E1, P)\n(17)\nwhere P represents the optimal matchings for sub-questions (edges\nP𝑒) and sub-responses (nodes P𝑣), computed using the Hungarian\nalgorithm [29]. The term 𝐶sub.(P) accounts for the total substitu-\ntion costs over the two graphs, and 𝐶del.(V1, E1, P) captures the\ntotal deletion costs for nodes and edges over the two graphs, details\nare explained in the Appendix. E.1. So we can calculate the minimal\ntotal cost of transformations by finding:\nGED𝑚(G1, G2) = min\nP GED(G1, G2)\n(18)\nA higher GED implies a higher difference in the reasoning phase\nby considering both embedding and structures.\n\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\nTrovato et al.\nWe use this computed reasoning distance to construct a distance\nmatrix across multiple reasoning structures. Given a set of reason-\ning topologies {G1, G2, . . . , G𝑛}, we compute pairwise distances\nusing the GED-based similarity measure: 𝑑𝑖𝑗= GED𝑚(G𝑖, G𝑗),\nwhich then forms the overall distance matrix between 𝑘reasoning\ntopologies\n𝐷G𝑛= [𝑑𝑖𝑗] = [GED𝑚(G𝑖, G𝑗)]𝑛×𝑛\n(19)\nwhere each entry 𝑑𝑖𝑗quantifies the structural and semantic dif-\nference between the reasoning processes in G𝑖and G𝑗. Here, we\nnow can resort to the UQ measure U(·) to the variance of the\ndistances in 𝐷G𝑛, which reflects the inconsistency or stability of\nthe model’s reasoning behavior. Combining Eq. 3, we have the\nuncertainty score over a query 𝑥𝑞as:\nUstruct(𝑥𝑞) = Var(𝐷G𝑛),\n(20)\nwhere 𝐷G𝑛is the pairwise distance matrix over the set of reasoning\ntopologies {G1, G2, . . . , G𝑛}. The function U(·) = Var(·) computes\nthe overall variance of all pairwise distances. A higher variance\nindicates greater inconsistency in the model’s reasoning, suggesting\nthat the LLM generates significantly different structures across\nmultiple responses to the same query.\n4.2.2\nLLM Reasoning Redundancy Measure. It is known that the\nLLM’s reasoning efficiency varies based on the problem type and\nmodel weights [33], and the reasoning topology provides a good\nreference to understand the efficiency by analyzing the detailed\nsteps. We find that LLMs do not necessarily rely on all of the nodes\nfrom its reasoning topology for the final conclusion drawing, which\nmeans that, some of the sub-steps do not contribute to solving a\nproblem and it causes the efficiency decrease. Here, we propose a\nway of measurement named ‘Reasoning Redundancy’. Reflect the\nreasoning steps in Eq. 13: 𝑆= {[𝑣𝑖, 𝑣𝑗,𝑒𝑖𝑗] | 𝑣𝑖, 𝑣𝑗∈V,𝑒𝑖𝑗∈E}, we\naim to measure the redundancy based on the valid path constructed\nby the steps.\nDefinition 2 (Redundant Node). A node 𝑣𝑘∈V is redun-\ndant if it does not contribute to the reasoning path from nodeRaw to\nnodeResult. Formally, a redundant node satisfies:\n𝑣𝑘∉\nØ\n[𝑣𝑖,𝑣𝑗,𝑒𝑖𝑗]∈Pvalid\n{𝑣𝑖, 𝑣𝑗},\nwhere Pvalid represents the set of all valid paths contributing to the\nfinal conclusion.\nWe have designed detailed criteria for efficient searching as\nshown in Appendix E.2, then we perform the searching for P𝑣𝑎𝑙𝑖𝑑\nvalid paths and ‘Redundancy Rate’ using traversal algorithm (DFS),\nthen we have the redundancy rate of the reasoning process for 𝑎𝑒\n𝑖\nas:\n𝑟redun.(𝑎𝑒\n𝑖) = |Vredundant|\n|V|\n(21)\nwhere the |V| is the total number of nodes in the reasoning topol-\nogy, and |Vredundant| is the number of redundant nodes.\n4.2.3\nA Recipe in Practice. The above-mentioned quantification\nmethods have different properties in potential usage. The Topology-\nUQ is mainly proposed to evaluate the trustworthiness of LLM\nresponses, make pair-wise reasoning path comparisons, or conduct\ncompleteness checks like what is strengthened in the AI4education\ndomain [32], where the teaching is focusing on the logic steps\nrather than final answers. The reasoning Redundancy measure\ncan be used to conduct a reasoning-efficiency check. It helps to\nidentify the non-necessary discussions for a problem and guide the\nLLM for improvement by probing and marking a more concise but\ncorrect path because high-quality answering should not only focus\non correctness but also involve solving efficiency.\nMethods\nApplication\nTopology-UQ\n①Trustworthy, ②Path similarity ③Completeness\nRedundancy\n①Efficiency check, ②Guide improvement\nTable 1: The usage of two quantification measures.\n5\nExperimental Study\nIn this section, we conducted extensive experiments covering 5\nLLMs: GPT4o-mini, DeepSeek-R1 (distilled version on llama3-70b),\nPhi4, Llama3-8b, Llama3-70b (v3.3), 3 challenging datasets and using\na total of 5 methods. We use the proposed method for topology-\nbased LLM explanation analysis, then we try to answer the follow-\ning research questions.\n• RQ11: Can the proposed method reveal the actual uncertainty\nin LLM’s reasoning?\n• RQ2: How do LLMs perform in the proposed redundancy\nmeasure?\n• RQ3: From the proposed topology reasoning elicitation, do\nLLMs share certain patterns commonly?\n5.1\nExperiment Settings\nIn this section, we introduce the experiment settings including the\ndataset adopted for analysis and baselines used for comparison.\nBesides, we also introduce the evaluation metrics for measuring the\nUQ methods’ performance on Natural Language Explanation\ntasks.\n5.1.1\nDataset. In this study, to align with the research commu-\nnity [30, 41, 46], we utilized widely adopted datasets, including\nGSM8k [8] and BoolQ [7], which require complex reasoning rather\nthan simple question-answering. These benchmarks assess LLM’s\nability to perform multi-step inference and logical reasoning. Be-\nsides, we also develop a new dataset, GeoQA, especially for condition-\nbased reasoning tasks. We will explain the details of the GeoQA\ndataset in Appendix B and provide brief introduction of all datasets\nin Appendix F.1\n5.1.2\nBaselines. To the best of our knowledge, there are only few\nworks focusing on the uncertainty quantification of NLE, thus,\nwe not only included the existing method (1) Chain-of-Thought\nAgreement (CoTA) [39], but also analyze (2) Embedding distance-\nbased UQ for NLE (Embed-UQ), (3) Entailment probability-based\n(Entail-UQ), and (4) NLI-logit based UQ, as our extra baselines to\nunderstand their interpretability of LLM explanations. We provide\na brief introduction here, and for detailed explanation of baselines\nis in Appendix. D.\n\nUnderstanding the Uncertainty of LLM Explanations: A Perspective Based on Reasoning Topology\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\nMethods\nGPT4o-mini\nDeepSeek-R1\nLlama-3.3-70B\nLlama3-8b\nPhi4\nPCC\nSRC\nKR\nPCC\nSRC\nKR\nPCC\nSRC\nKR\nPCC\nSRC\nKR\nPCC\nSRC\nKR\nDataset: BoolQ\nCoTA\n0.03\n0.05\n0.03\n-0.22\n-0.20\n-0.13\n-0.00\n0.05\n0.03\n-0.07\n-0.06\n-0.04\n0.08\n0.07\n0.05\nEmbed-UQ\n0.16\n0.18\n0.12\n0.56\n0.56\n0.39\n0.09\n0.12\n0.08\n0.04\n0.06\n0.04\n0.02\n0.02\n0.01\nEntail-UQ\n-0.12\n-0.13\n-0.09\n0.48\n0.46\n0.32\n-0.04\n-0.08\n-0.05\n-0.08\n-0.08\n-0.05\n-0.09\n-0.09\n-0.06\nNLI-logit\n0.19\n0.18\n0.12\n0.56\n0.56\n0.39\n0.07\n0.10\n0.07\n0.03\n0.03\n0.02\n-0.01\n-0.01\n-0.01\nOurs\n-0.03\n-0.05\n-0.03\n-0.29\n-0.26\n-0.17\n-0.14\n-0.10\n-0.07\n-0.24\n-0.23\n-0.15\n0.01\n0.01\n0.01\nDataset: GSM8K\nCoTA\n-0.12\n-0.10\n-0.07\n-0.40\n-0.39\n-0.26\n-0.13\n-0.13\n-0.09\n-0.04\n-0.03\n-0.02\n-0.14\n-0.13\n-0.09\nEmbed-UQ\n-0.12\n-0.10\n-0.07\n0.09\n0.10\n0.07\n0.15\n0.14\n0.10\n0.23\n0.23\n0.15\n0.28\n0.28\n0.19\nEntail-UQ\n0.14\n0.14\n0.09\n0.68\n0.66\n0.47\n0.15\n0.13\n0.09\n-0.08\n-0.07\n-0.05\n0.07\n0.07\n0.05\nNLI-logit\n0.00\n0.01\n0.01\n0.10\n0.11\n0.07\n0.13\n0.12\n0.08\n0.21\n0.19\n0.13\n0.29\n0.29\n0.20\nOurs\n-0.35\n-0.34\n-0.23\n-0.22\n-0.20\n-0.14\n-0.43\n-0.41\n-0.28\n-0.14\n-0.13\n-0.08\n0.12\n0.10\n0.06\nTable 2: Comparison of our methods with different baselines on various datasets and large language models. The results show\nthat our results consistently outperform the baseline methods. The value reflects the correlation between the uncertainty and\nthe ground truth faithfulness, the more faithful an LLM performs, the less uncertainty it should be, so by the UQ methods, we\nwould expect a negative correlation for a well-performed UQ method.\n• CoTA: This is designed to quantify uncertainty in LLM ex-\nplanations by measuring the agreement between two Chain-\nof-Thought explanations. It assesses whether intermediate\nreasoning steps in one CoT explanation are aligned with cor-\nresponding steps in another by calculating their entailment\nscores.\n• Embed-UQ: It measures the uncertainty of NLE by embed-\nding distances. Given a query 𝑥𝑞, collected 𝑘responses and,\ntheir associated explanations {𝑎𝑒\n1,𝑎𝑒\n2, . . . ,𝑎𝑒\n𝑘}. Each explana-\ntion 𝑎𝑒\n𝑖is embedded into a high-dimensional space, and the\npairwise distances between embeddings are computed.\n• Entail-UQ: It extends Embed-UQ by replacing the embedding-\nbased distance metric with an entailment-based similarity\nmeasure. Given explanations such as 𝑎𝑒\n𝑘, we compute an\nentailment similarity matrix, where the similarity between\ntwo explanations is calculated by entailment models 2.\n5.1.3\nEvaluation and Metrics. To evaluate the performance of un-\ncertainty quantification methods in LLM explanation tasks, we\nfollow the standard practice that compares uncertainty results with\nactual faithfulness [39]. The ground truth faithfulness score re-\nveals how much the model relies on its complete reasoning process,\nwhich is calculated through a strategy named ‘Early Answering’\nas proposed by [22], we provide details on how the faithfulness\nscore is derived in Appendix F. Ideally, a UQ method is good if,\nfor a higher faithful set, it generates lower uncertainty, and\nvise versa [39]. Hence, we employ three robust statistical metrics\nto quantify the correlation between the derived uncertainty and\nfaithfulness. First, we use the commonly adopted metric - Pearson\nCorrelation Coefficient (PCC), which is to measure the linear corre-\nlation between two variables. And given the relative small amount\nof each bootstrap sample, we employ two extra metrics Spearman\n2off-the-shelf DeBERTa-large model\nRank Correlation (SRC) and Kendall Rank Correlation (KR), the\ncalculation of metrics is also in Appendix.\nFor fair evaluation and to avoid bias in single answers, we con-\nduct bootstrap for a given dataset 𝐷𝑡𝑒𝑠𝑡and measure the correlation\nin each sub-set 𝐷′ level between uncertainty with the same level\nof faithfulness score. The sub-set is cut as 20 questions with 10\nresponses for each question = 200 ·𝑎𝑒and bootstrap is conducted\n1000 times on each dataset.\n5.2\nQuantitative Evaluation (RQ1)\nIn order to understand how our proposed method works in the\nreasoning uncertainty measure tasks for LLM explanations, we\nperform experiments on GSM8K, BoolQ, and GeoQa datasets. Due\nto the page limit, GSM8K, and BoolQ are shown in the Table. 2, our\nmethod reveals a stronger negative correlation between the derived\nUQ results and the groundtruth faithfulness across different statistic\nmetrics, the results on GPT4o-mini, Llama3-70b, and DeepSeek-R1\nare more convincing because they have a more stable performance\non the Topology elicitation task as in Figure. 5, which, is a key\nstep for the proposed UQ method, the performance on Phi4 and\nLlama3-8b not promising as ranked in the last two positions in\nAppendix Figure. 5. This research question result shows that our\nmethod is effective in revealing the LLM’s real faithfulness, yet it is\nmore suitable for LLMs with good instruct-following abilities.\n5.3\nRedundancy Measure of LLMs (RQ2)\nBenefit from the topology structure, we are able to extract the\nreasoning path Pvalid that successfully connects from nodeRaw to\nnodeResult. Then we can effectively compute the node that is\nnot contributing to the final answer, and this serves as a sign of\nredundancy in the LLM’s reasoning process. Following the Eq. 21,\nwe analyze the redundancy rate for each of the LLMs including GPT-\n4o-mini, Llama3-8b, Phi4, DeepSeek-R1(distilled), and Llama2-70b\n\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\nTrovato et al.\nQ: \"If a place receives less than 250 mm of precipitation annually, what climate does it likely have?\", \"Answer\": \"Arid” \n(nodeRaw)\nPattern1: Completion Based Reasoning\nThree Types of Reasoning Patterns in Current LLMs:\nPattern2: Forward Reasoning \nPattern3: Combination + Verify\nnodeRaw\nnode1\nnodeResult\ne0\ne1\ne0: What is the threshold of annual precipitation \nfor arid or desert climates? (in sub question \ndirectly ask about correct answer)\nnode1: Arid or desert climates typically have an \nannual precipitation threshold of less than 250 \nmillimeters (10 inches)\ne1: What is the final answer?\nnodeResult: Arid\nIf [c.F], Q?, (a)\nMiddle step:\nWhat is the Feature a?\n[a.F] == [c.F]\nConclusion: a\nIf [c.F], Q?, (a)\nMiddle step:\nWhat is the def. of x?\n[def.] è [c.F]\nConclusion: a\ne0: What is the definition and classification of climate types? \nnode1: arid (less than 250 mm), semi-arid (250-500 mm), humid \nsubtropical (1,000-2,000 mm), and tropical wet (over 2,000 mm). \ne1: More characteristics of an arid? (reason by narrowing down)\n(ask for details like human thinking)\nnode2: …\ne2: What is the final answer?\nnodeResult: Arid\nnodeRaw\nnodeResult\ne0\ne1\ne2\ne3\nnode1\nnode2\nnodeRaw\ne0\ne1\ne2\ne2\ne3\nnode1\nnode2\nnode3\nnodeResult\nIf [c.F], Q?, (a)\nMiddle step:\n(1). def. of x?\n(2). What is F a\n3. If: [def.] = [a.F]?\nConclusion: a\ne0: Definition and classification of climate types? \nnode1:…\ne1: What are the characteristics of an arid?\nnode2: …\ne2: Does arid satisfy the queestion?\nnode3: Yes\ne3: What is the final answer?\nnodeResult: Arid\n(e0, e1) are two distinct steps, e1 directly \nknows answer; e0 try to reflect def.\nFigure 3: The examples of three types of reasoning patterns. As shown in the image, the first type directly reflects the correct\nanswer in its first reasoning step, and now actually performing the reasoning, the second type is like humans, try to recall\nthe knowledge points, and narrow down the range, and find the possible answer in a thought chain, while the third one is\na combination of two, it has the steps to ask for characteristics, but it also try to reflect if it has seen this question before at\nthe edge1, which is a direct attempt to use answer to match the question, the last step is to confirm if the memory of answer\nmatches the requirement in the question.\non both the node and edge redundancy. We find that, surprisingly,\nthe GPT-4o-mini shows a significantly high redundancy rate in both\nthe nodes and edges, it might reveal the high accuracy of the model\ncomes from a border searching space (or generating length) when\nconducting reasoning and proposing solutions. However, this also\nreflects there would be a great potential to optimize the reasoning\nprocess for the GPT-wise models. Comparatively, the DeepSeek\n(distilled llama version) is relatively low, which might indicate the\nmodel’s training was conducted with a special design to encourage\nthe ‘valid’ reasoning which eventually contributes to the final result.\nWe show more results in the appendix for reference.\nGPT-4omini\nGPT-4omini\nLlama3-8b\nLlama3-8b\nPhi4\nLlama3-70b\nDeepSeek\nPhi4\nLlama3-70b\nDeepSeek\n(%)\nFigure 4: The quantification of the LLM’s reasoning redun-\ndancy on three datasets.\n5.4\nFindings on the Reasoning Patterns from\nLLMs (RQ3)\nBased on the topology representations, we conducted further anal-\nysis from both -structural and -content sides. First, we conducted\nclustering on the extracted reasoning topologies from LLMs (for\nconvenience, we did not consider different models respectively),\nand we found there are two dominant reasoning topology patterns:\n‘single chain’ and ‘node→branches→discuss→merge’ to a conclu-\nsion. Then, on the content side, we found three answering modes\nfrom the LLMs: ①Completion based: the model has seen the data in\nthe training phase, and completes it as a ‘cloze question’. ②Forward\nreasoning (actual reasoning): The model is actually performing rea-\nsoning based on the analysis procedure like the human beings, given\na question, it first recalls relevant concepts and knowledge that\ncontain the necessary information. Then, through a step-by-step\nnarrowing-down process, it refines its understanding, gradually\nconverging on the most precise answer. ③Combination of two +\nverify: the model uses two methods and double-checks to verify the\nanswer. Three modes are shown in Figure. 3. We hope this could\nserve as an inspiration for the potential future study.\n6\nConclusion\nIn this paper, we highlight the critical importance of understanding\nuncertainty in LLM explanations by revealing the reasoning consis-\ntency. By introducing a novel framework grounded in a reasoning\ntopology perspective, the paper provides a structured method to\ndecompose explanations into knowledge and reasoning dimensions.\nThis allows for precise quantification of uncertainty, assessment\nof knowledge redundancy, and deeper insights into the model’s\nreasoning structure. Then, the paper based on the proposed for-\nmal structural construction, it propose a graph-edit distance based\nuncertainty measure, empirical study shows a better performance\nin revealing the true faithfulness of the natural language models.\nAfter that, a redundancy-based method is introduced to quantify\nthe redundancy of the LLMs. The approaches not only enhance\ninterpretability but also serve as a guide for improving robustness\nand faithfulness in LLM-generated explanations.\n\nUnderstanding the Uncertainty of LLM Explanations: A Perspective Based on Reasoning Topology\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\nReferences\n[1] Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mo-\nhammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U Rajendra\nAcharya, et al. 2021. A review of uncertainty quantification in deep learning:\nTechniques, applications and challenges. Information fusion 76 (2021), 243–297.\n[2] Yunsheng Bai, Hao Ding, Song Bian, Ting Chen, Yizhou Sun, and Wei Wang.\n2018. Graph edit distance computation via graph neural networks. arXiv preprint\narXiv:1808.05689 (2018).\n[3] Yunsheng Bai, Hao Ding, Ken Gu, Yizhou Sun, and Wei Wang. 2020. Learning-\nbased efficient graph similarity computation via multi-scale convolutional set\nmatching. In Proceedings of the AAAI conference on artificial intelligence, Vol. 34.\n3219–3226.\n[4] Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom.\n2018. e-snli: Natural language inference with natural language explanations.\nAdvances in Neural Information Processing Systems 31 (2018).\n[5] Marco Cascella, Jonathan Montomoli, Valentina Bellini, and Elena Bignami. 2023.\nEvaluating the feasibility of ChatGPT in healthcare: an analysis of multiple\nclinical and research scenarios. Journal of medical systems 47, 1 (2023), 33.\n[6] Jiuhai Chen and Jonas Mueller. 2023. Quantifying uncertainty in answers from\nany language model and enhancing their trustworthiness. (2023).\n[7] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael\nCollins, and Kristina Toutanova. 2019. BoolQ: Exploring the Surprising Difficulty\nof Natural Yes/No Questions. arXiv:1905.10044 [cs.CL] https://arxiv.org/abs/\n1905.10044\n[8] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun,\nLukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,\nChristopher Hesse, and John Schulman. 2021. Training Verifiers to Solve Math\nWord Problems. arXiv:2110.14168 [cs.LG] https://arxiv.org/abs/2110.14168\n[9] Longchao Da, Tiejin Chen, Lu Cheng, and Hua Wei. 2024. Llm uncertainty\nquantification through directional entailment graph and claim level response\naugmentation. arXiv preprint arXiv:2407.00994 (2024).\n[10] Finale Doshi-Velez and Been Kim. 2017. Towards a rigorous science of inter-\npretable machine learning. arXiv preprint arXiv:1702.08608 (2017).\n[11] Yarin Gal and Zoubin Ghahramani. 2016. Dropout as a bayesian approximation:\nRepresenting model uncertainty in deep learning. In international conference on\nmachine learning. PMLR, 1050–1059.\n[12] Xinbo Gao, Bing Xiao, Dacheng Tao, and Xuelong Li. 2010. A survey of graph\nedit distance. Pattern Analysis and applications 13 (2010), 113–129.\n[13] Abimael Guzman-Pando, Graciela Ramirez-Alonso, Carlos Arzate-Quintana, and\nJavier Camarillo-Cisneros. 2024. Deep learning algorithms applied to computa-\ntional chemistry. Molecular Diversity 28, 4 (2024), 2375–2410.\n[14] Esmael Hamuda, Brian Mc Ginley, Martin Glavin, and Edward Jones. 2018. Im-\nproved image processing-based crop detection using Kalman filtering and the\nHungarian algorithm. Computers and electronics in agriculture 148 (2018), 37–44.\n[15] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2020.\nDe-\nberta: Decoding-enhanced bert with disentangled attention.\narXiv preprint\narXiv:2006.03654 (2020).\n[16] José Miguel Hernández-Lobato and Ryan Adams. 2015. Probabilistic backpropaga-\ntion for scalable learning of bayesian neural networks. In International conference\non machine learning. PMLR, 1861–1869.\n[17] Di Huang, Jinbao Song, and Yu He. 2024. Community detection algorithm for\nsocial network based on node intimacy and graph embedding model. Engineering\nApplications of Artificial Intelligence 132 (2024), 107947.\n[18] Thanmay Jayakumar, Fauzan Farooqui, and Luqman Farooqui. 2023. Large\nLanguage Models are legal but they are not: Making the case for a powerful\nLegalLLM. arXiv preprint arXiv:2311.08890 (2023).\n[19] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain,\nEthan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-\nJohnson, et al. 2022. Language models (mostly) know what they know. arXiv\npreprint arXiv:2207.05221 (2022).\n[20] Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023. Semantic uncertainty:\nLinguistic invariances for uncertainty estimation in natural language generation.\narXiv preprint arXiv:2302.09664 (2023).\n[21] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. 2017. Simple\nand scalable predictive uncertainty estimation using deep ensembles. Advances\nin neural information processing systems 30 (2017).\n[22] Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Deni-\nson, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion,\net al. 2023. Measuring faithfulness in chain-of-thought reasoning. arXiv preprint\narXiv:2307.13702 (2023).\n[23] Xuhong Li, Haoyi Xiong, Xingjian Li, Xuanyu Wu, Xiao Zhang, Ji Liu, Jiang Bian,\nand Dejing Dou. 2022. Interpretable deep learning: Interpretation, interpretability,\ntrustworthiness, and beyond. Knowledge and Information Systems 64, 12 (2022),\n3197–3234.\n[24] Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. Teaching models to express\ntheir uncertainty in words. arXiv preprint arXiv:2205.14334 (2022).\n[25] Zhen Lin, Shubhendu Trivedi, and Jimeng Sun. 2023. Generating with confidence:\nUncertainty quantification for black-box large language models. arXiv preprint\narXiv:2305.19187 (2023).\n[26] Antonio Loquercio, Mattia Segu, and Davide Scaramuzza. 2020. A general frame-\nwork for uncertainty estimation in deep learning. IEEE Robotics and Automation\nLetters 5, 2 (2020), 3153–3160.\n[27] Wesley J Maddox, Pavel Izmailov, Timur Garipov, Dmitry P Vetrov, and An-\ndrew Gordon Wilson. 2019. A simple baseline for bayesian uncertainty in deep\nlearning. Advances in neural information processing systems 32 (2019).\n[28] Sabrina J Mielke, Arthur Szlam, Y-Lan Boureau, and Emily Dinan. 2020. Linguis-\ntic calibration through metacognition: aligning dialogue agent responses with\nexpected correctness. arXiv preprint arXiv:2012.14983 11 (2020).\n[29] G Ayorkor Mills-Tettey, Anthony Stentz, and M Bernardine Dias. 2007. The\ndynamic hungarian algorithm for the assignment problem with changing costs.\nRobotics Institute, Pittsburgh, PA, Tech. Rep. CMU-RI-TR-07-27 (2007).\n[30] Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio,\nand Mehrdad Farajtabar. 2024. Gsm-symbolic: Understanding the limitations of\nmathematical reasoning in large language models. arXiv preprint arXiv:2410.05229\n(2024).\n[31] Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar,\nMuhammad Usman, Naveed Akhtar, Nick Barnes, and Ajmal Mian. 2023. A com-\nprehensive overview of large language models. arXiv preprint arXiv:2307.06435\n(2023).\n[32] Davy Tsz Kit Ng, Jiahong Su, Jac Ka Lok Leung, and Samuel Kai Wah Chu. 2024.\nArtificial intelligence (AI) literacy education in secondary schools: a review.\nInteractive Learning Environments 32, 10 (2024), 6204–6224.\n[33] Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein,\nand Thomas Back. 2024. Reasoning with Large Language Models, a Survey.\narXiv:2407.11511 [cs.AI] https://arxiv.org/abs/2407.11511\n[34] Xin Qiu and Risto Miikkulainen. 2024. Semantic Density: Uncertainty Quan-\ntification in Semantic Space for Large Language Models.\narXiv preprint\narXiv:2405.13845 (2024).\n[35] Laila Rasmy, Yang Xiang, Ziqian Xie, Cui Tao, and Degui Zhi. 2021. Med-BERT:\npretrained contextualized embeddings on large-scale structured electronic health\nrecords for disease prediction. NPJ digital medicine 4, 1 (2021), 86.\n[36] Wojciech Samek, Grégoire Montavon, Sebastian Lapuschkin, Christopher J An-\nders, and Klaus-Robert Müller. 2021. Explaining deep neural networks and\nbeyond: A review of methods and applications. Proc. IEEE 109, 3 (2021), 247–278.\n[37] Francesc Serratosa. 2021. Redefining the graph edit distance. SN Computer Science\n2, 6 (2021), 438.\n[38] Quan Shi, Michael Tang, Karthik Narasimhan, and Shunyu Yao. 2024. Can\nLanguage Models Solve Olympiad Programming? arXiv preprint arXiv:2404.10952\n(2024).\n[39] Sree Harsha Tanneru, Chirag Agarwal, and Himabindu Lakkaraju. 2024. Quanti-\nfying uncertainty in natural language explanations of large language models. In\nInternational Conference on Artificial Intelligence and Statistics. PMLR, 1072–1080.\n[40] Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu\nYao, Chelsea Finn, and Christopher D Manning. 2023. Just ask for calibration:\nStrategies for eliciting calibrated confidence scores from language models fine-\ntuned with human feedback. arXiv preprint arXiv:2305.14975 (2023).\n[41] Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, and\nIgor Gitman. 2024. Openmathinstruct-1: A 1.8 million math instruction tuning\ndataset. arXiv preprint arXiv:2402.10176 (2024).\n[42] Hongru Wang, Rui Wang, Fei Mi, Yang Deng, Zezhong Wang, Bin Liang, Ruifeng\nXu, and Kam-Fai Wong. 2023. Cue-CoT: Chain-of-thought prompting for respond-\ning to in-depth dialogue questions with LLMs. arXiv preprint arXiv:2305.11792\n(2023).\n[43] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei\nXia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-Thought Prompting\nElicits Reasoning in Large Language Models. arXiv:2201.11903 [cs.CL] https:\n//arxiv.org/abs/2201.11903\n[44] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,\nQuoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning\nin large language models. Advances in neural information processing systems 35\n(2022), 24824–24837.\n[45] Yasin Abbasi Yadkori, Ilja Kuzborskij, András György, and Csaba Szepesvári.\n2024. To Believe or Not to Believe Your LLM. arXiv preprint arXiv:2406.02543\n(2024).\n[46] Hugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, Will Song,\nTiffany Zhao, Pranav Raja, Dylan Slack, Qin Lyu, et al. 2024. A careful examination\nof large language model performance on grade school arithmetic. arXiv preprint\narXiv:2405.00332 (2024).\n[47] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,\nYingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey\nof large language models. arXiv preprint arXiv:2303.18223 (2023).\n\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\nTrovato et al.\nA\nNotation Table\nIn this section, we provide a comprehensive notation table as shown\nin the Table. 3.\nNotation\nExplanation\n𝑈𝑥\nUncertainty function\n𝑀\nLarge language model\n𝑥\nInput(query or prompt)\nU\nAggregation\n𝑥𝑞\nInput prompt\n𝑥𝑒\nExplanation-specific prompt\n𝑎\nLLMs answer\n𝑎𝑒\nGenerated explanation accompanying the answer 𝑎\n𝑠𝑖\nReasoning step\nG = (V, E)\nDirected reasoning graph\nV\nSet of nodes\n|V|\nNumber of nodes\nE\nSet of edges\n|E|\nNumber of edges\n𝑒𝑖𝑗\nEdge from node 𝑖to node 𝑗\n𝐾𝑞\nKnowledge points\n𝑇\nPrompt template\nD𝑚\nSet of knowledge-answer pairs for reasoning topology\n(𝑘𝑖,𝑎𝑖)\nKnowledge-answer pairs\n𝑒𝐹\nSampling 𝐹amount of edges\nF\nDemonstration set\nL(·)\nEmbedding function\nh\nContextual embedding\n𝑐𝑜𝑠(·)\nCosine similarity\nP\nOptimal matching for sub-questions\n𝑑𝑖𝑗\nGraph edit distance between G𝑖and G𝑖\n𝑟𝑟𝑒𝑑𝑢𝑛.\nRedundancy rate\nTable 3: The notations and explanations in this paper.\nB\nDetails of the GeoQA Dataset\nThe GeoQA dataset is designed to evaluate the reasoning capabili-\nties of large language models (LLMs) on conditional geographical\nquestions, emphasizing the comparative reasoning topology of\ntheir responses. By anchoring specific knowledge within condi-\ntional constraints and requiring models to infer results or solutions,\nGeoQA enables an in-depth analysis of the reasoning paths taken\nby LLMs. The dataset spans 20 categories, covering diverse geo-\ngraphical topics such as climate, biome, tectonic plates, continental\ndrift, altitude, sea level, desertification, urbanization, demography,\npopulation density, ocean currents, river basin, watershed, moun-\ntain range, volcano, earthquake, glacier, permafrost, and monsoon.\nEach question is crafted to test multi-step reasoning, integration\nof domain-specific knowledge, and the ability to navigate complex\ncause-effect relationships, making GeoQA a unique and challenging\nbenchmark for geographical reasoning.\nC\nEmpirical Study on the Choice of Number for\nGenerations\nIt is a basis setup that we need to query LLM M with a query\n𝑥𝑞for 𝑘times and collect a set of explanations to perform the\nNLE uncertainty measure. We have conducted a survey on related\nliterature and found there is no standard definition or setting, so\nwe conducted a preliminary study on the number of responses and\ntried to find the most suitable one (since the larger the response is,\nthe more computationally expensive it will be for later evaluation).\nD\nDetails of Baseline methods\nD.0.1\nCoTA.. Chain-of-Thought Agreement (CoTA) evaluates the\nagreement between two Chain-of-Thought (CoT) explanations gen-\nerated for the same query. Each CoT explanation consists of a\nsequence of reasoning steps, denoted as:\n𝐶𝑜𝑇𝑎= {𝑠𝑎1,𝑠𝑎2, . . . ,𝑠𝑎𝑁𝑎},\n𝐶𝑜𝑇𝑏= {𝑠𝑏1,𝑠𝑏2, . . . ,𝑠𝑏𝑁𝑏}.\nThe CoTA metric quantifies agreement between the two CoT expla-\nnations by calculating the maximum semantic alignment for each\nstep in 𝐶𝑜𝑇𝑎with steps in 𝐶𝑜𝑇𝑏, and vice versa. Formally, CoTA is\ndefined as:\nCoTA(𝐶𝑜𝑇𝑎,𝐶𝑜𝑇𝑏) =\n1\n𝑁𝑎+ 𝑁𝑏\n\u0012 𝑁𝑎\n∑︁\n𝑖=1\nmax\n𝑗=1,...,𝑁𝑏\n𝐸(𝑠𝑎𝑖,𝑠𝑏𝑗)\n+\n𝑁𝑏\n∑︁\n𝑗=1\nmax\n𝑖=1,...,𝑁𝑎\n𝐸(𝑠𝑏𝑗,𝑠𝑎𝑖)\n\u0013\n(22)\nwhere 𝑁𝑎and 𝑁𝑏are the number of steps in 𝐶𝑜𝑇𝑎and 𝐶𝑜𝑇𝑏,\nrespectively.\nThe entailment function 𝐸(𝑠𝑖,𝑠𝑗) measures the semantic agree-\nment between two reasoning steps using a Natural Language Infer-\nence (NLI) model. It is defined as:\n𝐸(𝑠𝑖,𝑠𝑗) =\n(\n1,\nif 𝑠𝑖entails 𝑠𝑗,\n0,\notherwise.\nThe entailment model employs pre-trained NLI models, such as\nDeBERTa [15], fine-tuned for evaluating entailment relationships\nbetween statements. This binary scoring avoids dependency on\nconfidence calibration, and we take the threshold as 0.7 to provide\nthe binary cut.\nD.0.2\nEmbed-UQ.. Embed-UQ measures uncertainty by embed-\nding natural language explanations into a semantic space and com-\nputing the variance of pairwise distances. Given a query 𝑥𝑞, let\n{𝑎𝑒\n1,𝑎𝑒\n2, . . . ,𝑎𝑒\n𝑘} represent 𝑘explanations generated by the model.\nUsing an embedding function L(·), each explanation 𝑎𝑒\n𝑖is mapped\nto a high-dimensional embedding:\nh𝑖= L(𝑎𝑒\n𝑖).\nThe pairwise distances between embeddings are computed as:\n𝑑𝑖𝑗= ∥h𝑖−h𝑗∥,\nwhere 𝑑𝑖𝑗represents the distance between explanations 𝑎𝑒\n𝑖and 𝑎𝑒\n𝑗.\nThe uncertainty is then quantified as the variance of the distance\nmatrix 𝐷:\nU(𝑥𝑞) = Var(𝐷),\nwhere 𝐷= {𝑑𝑖𝑗| 1 ≤𝑖, 𝑗≤𝑘}.\n\nUnderstanding the Uncertainty of LLM Explanations: A Perspective Based on Reasoning Topology\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\nTable 4: Examples of GeoQA dataset.\nQuestion Type\nType Explain\nExample Questions\nAnalysis\nGlacier\nThese questions explore\nglacial movement, erosion,\nand the impact of climate\nchange on ice dynamics.\n①: If glaciers carve striations into bedrock,\nwhat do these scratches indicate about past movements?\nStriations indicate past movement direction,\nrequiring cause-and-effect analysis,\nwhile glacier mass loss demands understanding\nthe imbalance between accumulation and ablation.\n②: If a glacier loses mass due to melting and sublimation\nexceeding accumulation, what process is occurring?\nEarthquake\nThese questions focus on\nseismic wave behavior,\nfault activity, and\nearthquake detection.\n①: If seismic waves are recorded by a network\nof seismographs, which method is used to pinpoint\nthe origin of the disturbance?\nTriangulation requires reasoning through\nwave arrival times, while P-wave detection relies on\ncomparing wave speeds and impact\nto explain early warning systems.\n②: If earthquake early warning systems rely on\ndetecting the initial P-waves, which characteristic\nof these waves makes this feasible?\npermafrost\nThese questions examine\npermafrost thawing,\nclimate feedback loops, and\nseasonal variations.\n①: If permafrost thaws due to rising temperatures,\nreleasing trapped methane,\nwhat global issue does this exacerbate?\nThawing permafrost and methane release involve\nfeedback loops, while active layer variations\nrequire analyzing environmental factors\nlike temperature and insulation.\n②: If the active layer above permafrost varies in\nthickness seasonally, what factors influence its depth?\nMonsoon\nThese questions cover\nseasonal wind shifts,\nmonsoon patterns, and\nstorm formation.\n①: If the East Asian monsoon affects countries\nlike China and Japan,\nwhat two seasons does it primarily influence?\nThe East Asian monsoon’s impact on seasons\ninvolves reasoning about wind shifts,\nwhile monsoon depressions require\nlinking low-pressure systems to storm formation.\n②: If monsoon depressions form in the Bay of Bengal,\nwhat weather events might they trigger upon landfall?\nD.0.3\nEntail-UQ.. Entail-UQ modifies the distance computation\nin Embed-UQ by using an entailment-based similarity measure\ninstead of embedding distances. Given the same set of explana-\ntions {𝑎𝑒\n1,𝑎𝑒\n2, . . . ,𝑎𝑒\n𝑘}, an entailment model computes the similarity\nbetween two explanations 𝑎𝑒\n𝑖and 𝑎𝑒\n𝑗as:\n𝑠𝑖𝑗= 𝐸(𝑎𝑒\n𝑖,𝑎𝑒\n𝑗),\nwhere 𝐸(·, ·) is the entailment function that outputs a similarity\nscore between 0 and 1. The dissimilarity is then defined as 1 −𝑠𝑖𝑗,\nand the uncertainty is computed as the variance of the dissimilarity\nmatrix 𝑆:\nU(𝑥𝑞) = Var(1 −𝑆),\nwhere 𝑆= {𝑠𝑖𝑗| 1 ≤𝑖, 𝑗≤𝑘}. Similarly to CoTA, in our im-\nplementation, we adopt DeBERTa [15] as the model to generate\nentailment logit and pass through a layer of softmax to transform\nit into probabilities.\nE\nResearch Methods\nE.1\nPart One\nThe final full version of the graph edit distance is as below:\nFigure 5: The model’s success rate to generate legitimate\nreasoning topology. It is calculated by the percentage of\nLLMs successfully generating the reasoning path from the\nnodeRaw to nodeResult following the few-shot prompt. It\ncan be witnessed that, generally the GeoQA is a harder one\nto generate due to the conditional question types, which are\nrarely seen in the LLM training tasks.\nGED(G1, G2) = 𝐶sub.(P) + 𝐶del.(V1, E1, P)\n(23)\n\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\nTrovato et al.\nwhere P represents the optimal matchings for nodes (P𝑣) and\nedges (P𝑒), computed using an algorithm such as the Hungarian\nalgorithm [14]. The term 𝐶sub.(P) accounts for substitution costs,\ndefined as:\n𝐶sub.(P) =\n∑︁\n(𝑣𝑖,𝑣𝑗)∈P𝑣\n𝑐(𝑣𝑖, 𝑣𝑗) +\n∑︁\n(𝑒𝑘,𝑒𝑚)∈P𝑒\n𝑐(𝑒𝑘,𝑒𝑚),\n(24)\nwhere 𝑐(𝑣𝑖, 𝑣𝑗) and 𝑐(𝑒𝑘,𝑒𝑚) represent the node and edge sub-\nstitution costs, respectively. The term 𝐶del.(V1, E1, P) captures the\ndeletion costs for nodes and edges in G1 that are not matched, given\nby:\n𝐶del.(V1, E1, P) =\n∑︁\n𝑣𝑖∈V1\\P𝑣\n𝑐del.(𝑣𝑖) +\n∑︁\n𝑒𝑘∈E1\\P𝑒\n𝑐del.(𝑒𝑘).\n(25)\nHere,𝑐del.(𝑣𝑖) and𝑐del.(𝑒𝑘) denote the deletion costs of unmatched\nnodes and edges, respectively. This formulation quantifies the total\ncost required to align the two reasoning structures by summing the\nsubstitution costs of matched components and the deletion costs of\nunmatched ones.\nE.2\nAlgorithm for Detection of Redundant\nNodes and Dead Branches\nTo identify redundant nodes and branches of dead nodes in the\nreasoning topology 𝑆= {[𝑣𝑖, 𝑣𝑗,𝑒𝑖𝑗] | 𝑣𝑖, 𝑣𝑗∈V,𝑒𝑖𝑗∈E}, we\nfirst define the outgoing edges of a node 𝑣𝑘as: Out(𝑣𝑘) = {𝑣𝑗|\n[𝑣𝑘, 𝑣𝑗,𝑒𝑘𝑗] ∈𝑆} Then A node 𝑣𝑘is considered redundant if it has\nno outgoing edges and is not the final node:\nOut(𝑣𝑘) = ∅\nand\n𝑣𝑘≠NodeResult.\n(26)\nThen, we compute the set of valid paths, Pvalid, connecting NodeRaw\nto NodeResult using DFS. A node 𝑣𝑘∈V is redundant if it does\nnot appear in any valid path:\n𝑣𝑘∉\nØ\n[𝑣𝑖,𝑣𝑗,𝑒𝑖𝑗]∈Pvalid\n{𝑣𝑖, 𝑣𝑗}.\n(27)\nIn order to more efficiently detect branches of dead nodes (not\ncontributing to the whole reasoning path), let 𝑣𝑘and its parent 𝑣𝑝\nsatisfy:\nOut(𝑣𝑝) = {𝑣𝑘},\nand\n𝑣𝑝, 𝑣𝑘∉\nØ\n[𝑣𝑖,𝑣𝑗,𝑒𝑖𝑗]∈Pvalid\n{𝑣𝑖, 𝑣𝑗}.\nIn this case, 𝑣𝑝and 𝑣𝑘form a dead branch, as neither contributes\nto any reasoning path leading to NodeResult. And finally, the re-\ndundancy rate is computed as:\n𝑟𝑒𝑑𝑢𝑛.(𝑎𝑒\n𝑖) = |Vredundant|\n|V|\n,\n. It allows to systematically identify nodes and branches that do\nnot contribute to the reasoning process, providing insights into\nthe inefficiencies in the model’s reasoning topology and we can\nanalyze to understand where we can improve in the model training\nor fine-tuning process.\nF\nExperimental Details\nF.1\nDetails of the dataset\n• GSM8K [8]: This dataset contains 8,000 high-quality math\nword problems, designed to evaluate LLMs’ ability to perform\narithmetic reasoning. It is a standard benchmark for testing\nthe numerical and reasoning capabilities of LLMs.\n• BoolQ [7]: BoolQ is a yes/no question-answering dataset\nderived from naturalistic information-seeking questions. The\ndataset is used to evaluate the ability of LLMs to reason\nlogically over textual evidence and produce accurate binary\nanswers.\n• GeoQA: GeoQA is a self-constructed dataset designed to\nevaluate the reasoning capabilities of LLMs in conditional\nquestions. With 20 categories, including climate and tectonic\nprocesses, its tasks require inference of specific results from\ngiven conditions. GeoQA emphasizes multi-step reasoning\nand domain-specific integration, making it a challenging\nbenchmark (details in the Appendix B).\nF.2\nThe calculation of the faithfulness core\nIn our experiments, we utilize a strategy called Early Answering to\nmeasure the faithfulness of the reasoning paths 𝑎𝑒= {𝑠1,𝑠2, . . . ,𝑠𝑛},\nwhich are generated by the LLM for a given query 𝑥𝑞. This strat-\negy involves truncating the reasoning steps 𝑎𝑒progressively and\nprompting the model to answer the query 𝑥𝑞combined with the\npartial reasoning path {𝑠1,𝑠2, . . . ,𝑠𝑘}, where 𝑘∈{1, 2, . . . ,𝑛}. For\nexample, instead of providing the entire reasoning𝑥𝑞+𝑠1+𝑠2+· · ·+𝑠𝑛,\nthe model is prompted to answer using only 𝑥𝑞+ 𝑠1, 𝑥𝑞+ 𝑠1 + 𝑠2,\nand so on, until the full reasoning path is reached.\nThe Early Answering process evaluates how often the LLM’s\nresponses, derived from the partial reasoning path {𝑠1,𝑠2, . . . ,𝑠𝑘},\nmatch the final answer 𝑎generated using the complete reasoning\n𝑥𝑞+𝑠1 +𝑠2 + · · · +𝑠𝑛. This evaluation reflects the faithfulness of the\nreasoning path: if the model consistently reaches the correct answer\n𝑎with partial reasoning, it may indicate that the reasoning steps are\nunnecessary (post-hoc). Conversely, a lower match rate suggests\nthat the intermediate reasoning steps are essential for arriving at\nthe correct final answer, thereby indicating greater faithfulness.\nWe quantify the faithfulness𝑉𝑓𝑎𝑖𝑡ℎusing the following equation:\n𝑉faith = 1 −1\n𝑛\n𝑛\n∑︁\n𝑘=1\nI \u0000𝑓(𝑥𝑞+ {𝑠1,𝑠2, . . . ,𝑠𝑘}) = 𝑎\u0001\n|                                          {z                                          }\nun-faithfulness\n(28)\nwhere 𝑛represents the total number of reasoning steps in 𝑎𝑒, 𝑓(𝑥𝑞+\n{𝑠1,𝑠2, . . . ,𝑠𝑘}) is the LLM’s output when prompted with the query\n𝑥𝑞and the partial reasoning path {𝑠1,𝑠2, . . . ,𝑠𝑘}, 𝑎denotes the final\nanswer generated using the complete reasoning path, and I(·) is\nan indicator function that equals 1 if the condition is true (i.e., the\npartial reasoning output matches the final answer), and 0 otherwise.\nThe𝑉faith is calculated by 1−un-faithfulness, and the unfaithfulness\nmeans: how much the model’s final answer 𝑎(Not) depends on the\nintermediate reasoning steps, since by removing sub-steps, it still\nreaches same answer.\nA high faithfulness score indicates that the model’s final answer\nis more dependent on intermediate reasoning steps, suggesting\n\nUnderstanding the Uncertainty of LLM Explanations: A Perspective Based on Reasoning Topology\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\nthat the reasoning is not post-hoc, and this faithfully reflects that\nthe logical steps are required to derive the answer, by this high\nfaithfulness, the UQ measure should align with it, in other words,\na UQ method is good if it derives a lower uncertainty when the\nfaithfulness is high, and vise versa.\nG\nPrompt Template & Few Shot Examples\nHere we introduce details of the prompt template used in this paper\nas well as some few-shot examples to guide the LLMs to follow the\nelicitation process.\n\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\nTrovato et al.\nThe Prompt Template to Elicit Knowledge Points.\nSystem Description: You are a helpful assistant to do the following: Given a question, you should reflect and come up with the sufficient\nknowledge that you need to solve this question. Two standards: sufficient and concise. And you should respond with numbered points.\nTask Description: Given a question: { question_i }. Please provide a response following system requirements and learning the format\nfrom the example: { few_shot_example }.\nExample1:\nQuestion: If it is currently summer in Australia, what season is it in Canada?\nExpected Response (For required knowledge):\n1. Where is Australia located on Earth?\n2. Where is Canada located on Earth?\n3. What is the geographical relationship between Australia and Canada?\n4. How does the tilt of the Earth affect seasons?\nExamples ...\nOutput: {Placeholder}\nThe Prompt Template to Express Reasoning Path.\nSystem Description: You are a reasoning assistant, you will see some Edge-Node pairs, which stands for the Q-A pairs, try to find a\nreasoning path based on these Q-A pairs that solves the question.\nTask Description: Given a { question }. Please learn how it is reasoned from the example: Reason_Path_Example. Now give the reasoning\npath for {q_a}.\nConstraints:\n1. NodeRaw and NodeResult are nominal term, NodeRaw stands for Question itself and NodeResult stands for the End of reasoning. ;\n2. When reason to the conclusion, there should be an added: ResultNode and ResultEdge as: [Nodex, NodeResult, ResultEdge];\n3. [NodeRaw, Node0, Edge0]: indicates NodeRaw is connected with Node0 by Edge0. [Nodex, NodeResult, ResultEdge]: indicates Nodex is connected with\nNodeResult by ResultEdge.;\nExample1:\nQuestion: If it is currently summer in Australia, what season is it in Canada?\nEdge0: Where is Australia located on Earth?, Node0: Australia in the Southern Hemisphere.;\nEdge1: Where is Canada located on Earth?, Node1: Canada is located in the Northern Hemisphere.;\nEdge2: What is the geographical relationship between Australia and Canada?, Node2: Australia and Canada are in the opposite hemisphere.;\nEdge3: How does the tilt of the Earth affect seasons?, Node3: Opposite hemispheres experience opposite seasons because of the Earth’s tilt.;\nA Possible Output:\nStructure: [NodeRaw, Node0, Edge0], [NodeRaw, Node1, Edge1], [Node0, Node2, Edge2], [Node1, Node2, Edge2], [Node2, Node3, Edge3], [Node3, NodeResult,\nResultEdge]; ResultEdge: It is summer in Canada.;\nOutput: {Placeholder}\n\nUnderstanding the Uncertainty of LLM Explanations: A Perspective Based on Reasoning Topology\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\nQ: If it is currently summer in Australia, what season is it in Canada?\nQ\nResult\nQ\nResult\nQ\nResult\nA. It is winter in Canada.\nQ\nResult\nWhat the season the Canada is based on the above reasoning?\n• (To get the final answer)\nWhere is Australia located on Earth?\n• (To understand which hemisphere Australia is in)\nWhere is Canada located on Earth?\n• (To understand which hemisphere Canada is in)\nWhat is the geographical relationship between Australia and Canada?\n• (To determine if they are in the same or opposite hemispheres)\nHow does the tilt of the Earth affect seasons?\n• (To understand why different hemispheres experience different seasons)\nEdge questions: \nAustralia is located in the \nSouthern Hemisphere\nCanada is located in the \nNorthern Hemisphere\nAustralia and Canada are  in opposite hemispheres\nOpposite hemispheres experience opposite seasons because of Earth's tilt\nIt is winter in Canada\nQ\nResult\nWhen does winter occur in the Northern Hemisphere? \nIn which hemisphere is Australia located? \nIn which hemisphere is Canada located? \nWhen does summer occur in the Southern Hemisphere? \nHow does the seasons in the Northern Hemisphere compare to the seasons \nin the Southern Hemisphere?\nEdge questions: \nGiven the information above, what season is it in Canada?  \nAustralia is located in the \nSouthern Hemisphere\nCanada is located in the \nNorthern Hemisphere\nDec-Feb\nDec-Feb\nThey happen at the same time\nIt is winter in Canada\nQ\nResult\nWhat the season the Canada is based on the above reasoning?\nWhy would there be seasons on Earth?\nHow is the season affected? \nWhat season is it in Canada when it is Summer in Australia?\nEdge questions: \nThe Earth’s axis is tilted, causing different hemispheres to receive \nvarying sunlight at different times of the year.\nDue to the tilt, one hemisphere experiences winter while the other \nexperiences summer.\nCanada is in the hemisphere that is currently tilted away from the \nSun if now it is Summer in Australia\nQ: If it is currently summer in Australia, what season is it in Canada?\nQ: If it is currently summer in Australia, what season is it in Canada?\nQ: If it is currently summer in Australia, what season is it in Canada?\nIt is winter in Canada\n①\n②\n③\n①\n②\n③\nFigure 6: The Example of of same question but different reasoning path and leading to the same answer: ‘If it is currently\nsummer in Australia, what season is it in Canada?’ .\n",
  "metadata": {
    "source_path": "papers/arxiv/Understanding_the_Uncertainty_of_LLM_Explanations_A_Perspective_Based\n__on_Reasoning_Topology_e30e04fe7cfb05df.pdf",
    "content_hash": "e30e04fe7cfb05df1182b54729433c69ced906d53e72be65822b076f13924c1f",
    "arxiv_id": null,
    "title": "Understanding the Uncertainty of LLM Explanations: A Perspective Based on Reasoning Topology",
    "author": "",
    "creation_date": "D:20250225023908Z",
    "published": "2025-02-25T02:39:08",
    "pages": 15,
    "size": 2057942,
    "file_mtime": 1740470193.3956702
  }
}