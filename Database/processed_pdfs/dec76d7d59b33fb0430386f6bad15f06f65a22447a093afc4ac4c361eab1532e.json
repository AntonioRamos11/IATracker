{
  "text": "Stable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam\nTianjin Huang* 1 2 Haotian Hu* 3 Zhenyu Zhang* 4 Gaojie Jin 1 Xiang Li 5\nLi Shen 6 Tianlong Chen 7 Lu Liu 1 Qingsong Wen 8 Zhangyang Wang 4 Shiwei Liu 2 9\nAbstract\nThis paper comprehensively evaluates several re-\ncently proposed optimizers for 4-bit training, re-\nvealing that low-bit precision amplifies sensitiv-\nity to learning rates and often causes unstable\ngradient norms, leading to divergence at higher\nlearning rates. Among these, SPAM, a recent\noptimizer featuring momentum reset and spike-\naware gradient clipping, achieves the best per-\nformance across various bit levels, but strug-\ngles to stabilize gradient norms, requiring care-\nful learning rate tuning. To address these limita-\ntions, we propose Stable-SPAM, which incor-\nporates enhanced gradient normalization and clip-\nping techniques. In particular, Stable-SPAM\n(1) adaptively updates the clipping threshold\nfor spiked gradients by tracking their histori-\ncal maxima; (2) normalizes the entire gradient\nmatrix based on its historical l2-norm statistics;\nand (3) inherits momentum reset from SPAM\nto periodically reset the first and second mo-\nments of Adam, mitigating the accumulation of\nspiked gradients. Extensive experiments show\nthat Stable-SPAM effectively stabilizes gradi-\nent norms in 4-bit LLM training, delivering supe-\nrior performance compared to Adam and SPAM.\nNotably, our 4-bit LLaMA-1B model trained with\nStable-SPAM outperforms the BF16 LLaMA-\n1B trained with Adam by up to 2 perplexity. Fur-\nthermore, when both models are trained in 4-bit,\nStable-SPAM achieves the same loss as Adam\nwhile requiring only about half the training steps.\n*Equal contribution 1Department of Computer Science, Univer-\nsity of Exeter 2Department of Mathematics and Computer Science,\nEindhoven University of Technology 3School of the Gifted Young,\nUniversity of Science and Technology of China 4Department\nof Electrical and Computer Engineering, University of Texas at\nAustin 5Department of Computer Science, University of Reading\n6School of Cyber Science and Technology, Sun Yat-sen Univer-\nsity 7Department of Computer Science, The University of North\nCarolina at Chapel Hill 8Squirrel Ai Learning 9Mathematical In-\nstitute, University of Oxford. Correspondence to: Tianjin Huang\n<t.huang2@exeter.ac.uk>.\nCode is available at https://github.com/\nTianjinYellow/StableSPAM.git.\n1. Introduction\nRecently, several advanced optimizers have been proposed,\nclaiming to either outperform the widely used Adam opti-\nmizer or achieve comparable performance at reduced costs\nin the context of Large Language Models (LLMs). Given\nthe massive size of LLMs, reducing the memory footprint\nof Adam has become a key objective in this line of research\n(Shazeer & Stern, 2018; Chen et al., 2024; Zhang et al.,\n2024a; Zhao et al., 2024a; Zhang et al., 2024b; Ma et al.,\n2024). Another area of focus is addressing the challenges\nof instability in LLM training. For instance, Huang et al.\n(2025) proposed SPAM which incorporates momentum re-\nset and spike-aware gradient clip (SpikeClip) to mitigate the\nadverse effects of loss spikes. Zhao et al. (2024b) studied\nthe stability of various optimizers to hyperparameters with\nBF16. These optimizers are predominantly evaluated using\nthe standard BF16 precision, which is a practical option\nfor real-world LLM training (Touvron et al., 2023; Li et al.,\n2023). With the growing shift toward low-bit precisions\nsuch as FP8 and FP4 in LLMs due to their significant cost-\nsaving potential (Liu et al., 2024; Lee et al., 2024; Peng et al.,\n2023; Xi et al., 2023), it is crucial to investigate whether\ntheir effectiveness persists under lower-bit precisions. For\nthe newly proposed optimizers to be economical, their\ntraining with low-bit precisions should be similarly robust\nto hyperparameter choice as trained using higher precision.\nThis paper provides a comprehensive evaluation of the ef-\nfectiveness and robustness of learning rate choices across\nvarious recent optimizers, including Adam (Kingma, 2014),\nAdafactor (Shazeer & Stern, 2018), Adam-mini (Zhang\net al., 2024a), and SPAM (Huang et al., 2025), when train-\ning with 4-bit weights and activations. Our study reveals\nseveral key observations:\n⋆All evaluated optimizers exhibit increased sensitivity to\nlearning rate choices during 4-bit training, often diverging\nquickly when larger learning rates are used as shown in\nFigure 2.\n⋆SPAM consistently achieves the lowest evaluation loss\n1\narXiv:2502.17055v1  [cs.LG]  24 Feb 2025\n\nStable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam\n3K\n11K\n19K\nUpdate Steps\n20\n25\n30\nPerplexity\n(1) LLaMA-350M (FP4)\n3K\n11K\n19K\nUpdate Steps\n20\n25\n30\n(3) LLaMA-1B (FP4)\n3K\n11K\n19K\nUpdate Steps\n20\n25\n30\n(3) LLaMA-350M (INT4)\n3K\n11K\n19K\nUpdate Steps\n20\n25\n30\n(4) LLaMA-1B (INT4)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAdam\nStable-SPAM\nAdam-BF16\nFigure 1. Performance of 4-bit LLM training. Experiments are conducted with LLaMA-130M/350M/1B models on C4 Dataset.\nAdam-BF16 denotes that the model is trained with BF16 by Adam. Perplexity on validation set is reported.\nacross various bit levels but requires careful learning rate\ntuning. Adafactor is surprisingly robust to learning rate\nchoices, even outperforming Adam in this regard.\n⋆Our analysis of the training dynamics in Figure 4 reveals\nthat 4-bit training often exhibits extremely unstable gra-\ndient norms, often accompanied by spikes, compared to\nBF16. This behavior can result in loss spikes and, in some\ncases, even training divergence with relatively larger learn-\ning rates.\n⋆While SpikeClip introduced in SPAM mitigates the unsta-\nble gradient norms caused by 4-bit training to a certain\nextent, it falls short of fully preventing training divergence,\nas shown in Figure 3.\nDespite its sensitivity to learning rate selection, SPAM con-\nsistently achieves the lowest evaluation loss across various\nbit levels, making it an ideal foundation for improvement.\nBuilding on this, we introduce Stable-SPAM to address\nthe instability challenges associated with low-precision train-\ning of LLMs. Stable-SPAM retains the superior perfor-\nmance of SPAM1 while improving stability, offering a sig-\nnificant advancement in low-precision optimization.\nSpecifically, beyond the original momentum reset operation\nin SPAM, Stable-SPAM introduces two key techniques:\nAdaptive Spike-Aware Clipping (AdaClip), which enables\nadaptive clipping of spiked gradients, followed by Adaptive\nGradient Norm (AdaGN), which normalizes the entire gradi-\nent matrix based on its historical l2 norm statistics. Our anal-\nysis demonstrates that these enhancements effectively stabi-\nlize the gradient norm of 4-bit training, achieving better per-\nformance than Adam and SPAM. Notably, our 4-bit LLaMA-\n1B model trained with Stable-SPAM outperforms the\nBF16 LLaMA-1B trained with Adam. Furthermore, when\n1Nevertheless, results in Table 3 show that our proposed tech-\nniques also improve the performance of other optimizers.\nboth models are trained in 4-bit, Stable-SPAM achieves\nthe same loss as Adam while requiring only about half the\ntraining steps.\n2. 4-bit Training Stability Investigation\nRecent studies (Zhao et al., 2024b; Wortsman et al., 2023b;\nHuang et al., 2025; Takase et al., 2023; Wortsman et al.,\n2023b) have investigated stability challenges in large lan-\nguage model (LLM) training, including issues such as learn-\ning rate instability, gradient spikes, and loss spikes. In this\nsection, we extend the evaluation by analyzing the stabil-\nity of various optimization algorithms under a 4-bit LLM\ntraining setting. Following the experimental setup outlined\nin (Wortsman et al., 2023b; Zhao et al., 2024b), we eval-\nuate the final performance using a range of learning rates\nfrom 1e-4 to 3e-3. This evaluation includes two widely used\noptimizers, Adam (Kingma, 2014) and Adafactor (Shazeer\n& Stern, 2018), as well as two recently proposed methods,\nAdam-mini (Zhang et al., 2024a) and SPAM (Huang et al.,\n2025). Additionally, we monitor both the global gradient\nnorm and training loss throughout the 4-bit LLM training\nprocess. The global gradient norm is defined as follows:\nqPN\ni=0 ∥gi∥2\n2 where N is the number of layers in model\nand gi denotes the gradient of i-th layer. The experiments\nare conducted on the LLaMA-130M/350M models using\nthe C4 dataset and showed in Figure 2 and Figure 4. We\nobserve:\n①Lower-bit training exhibits reduced learning rate sta-\nbility. As illustrated in Figure 2, the final evaluation loss\nfor 4-bit training increases significantly with larger learning\nrates, whereas BF16 training exhibits a more stable perfor-\nmance across different learning rates. This indicates that\n4-bit training is more sensitive and less stable in terms of\nlearning rate.\n2\n\nStable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam\n1e-4\n1e-3\n3e-3\nLR\n3\n4\n5\nFinal Eval Loss\n(1) Adam\nFP4 Training\nINT4 Training\nFP16 Training\n1e-4\n1e-3\n3e-3\nLR\n3\n4\n5 (2) INT4 Training\n1e-4\n1e-3\n3e-3\nLR\n3\n4\n5\n(3) FP4 Training\n1e-4\n1e-3\n3e-3\nLR\n3\n4\n5 (4) BF16 Training\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAdafactor\nAdam-mini\nAdam\nSPAM\nFigure 2. Final validation loss when training LLaMA-130M on C4, sweeping across learning rates (LR). The vertical dotted line\nindicates that the model cannot be trained further as increasing the learning rate, i.e. Training loss becomes NaN. Red dashed horizontal\nlines indicate the best performance achieved.\n0\n1K 2K 3K 4K 5K\n0\n1K\n2K\n3K\nGradient Norm\n0\n1K 2K 3K 4K 5K\n4\n6\n8\n10\nTraining Loss\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAfter SpikeClip\nBefore SpikeClip\nW/o SpikeClip\nW/. SpikeClip\nFigure 3. Effect of SpikeClip (Huang et al., 2025) on stabilizing\ntraining. Left: gradient norms before and after performing gradi-\nent spike clip. Right: training loss with and without gradient spike\nclip. Models are trained by Adam optimizer based on LLaMA-\n130M and C4.\n②Lower-bit training suffers more loss spikes and gra-\ndient norm spikes. Figure 4 illustrates this phenomenon\nby comparing the training loss and gradient norm curves\nof LLaMA-130M and LLaMA-350M trained under BF16\nand FP4 (E1M2) precision, using various learning rates. We\nobserve that BF16 training remains stable, but FP4 train-\ning exhibits significant loss spikes, which occur on both\nmodel sizes. Furthermore, these loss spikes are consistently\naccompanied by gradient norm explosions.\n③SPAM performs the best in 4-bit training but needs\ncareful learning rate tuning. As shown in Figure 2, SPAM\nachieves the lowest eval loss among various optimizers in\nINT4 or FP4 with the optimal learning rate. However, its\nvalidation loss either diverges to NaN or sharply increases as\nthe learning rate rises. Additionally, we monitored the train-\ning loss and gradient norm after applying the spike clipping\ntechnique (SpikeClip) proposed in SPAM. SpikeClip detects\nand mitigates gradient outliers by leveraging the second mo-\nment of gradients. Specifically, it follows the expression:\ngi = sign(gi) · √θVi under the condition g2\ni\nVi > θ where\ngi, Vi, θ are the gradient, second moment and pre-defined\nthreshold (5000 used by default in their paper) respectively.\nWe found that merely SpikeClip can mitigate the loss spike\nto some extent but can not prevent the training divergence\ncompletely. One possible explanation is that SpikeClip op-\nerates on an element-wise basis and may use a threshold\nthat is too high. If all gradient components increase simulta-\nneously, SpikeClip may still allow a large overall gradient\nnorm, as it focuses solely on clipping individual outliers and\ndoes not effectively handle uniformly large gradients. This\nis supported by the observation in Figure 3 that the gradient\nnorm remains high even after SpikeClip is applied.\n3. Stable-SPAM\nTo address the training instability in 4-bit LLM training, we\npropose Stable-SPAM, a stabilized spike-aware Adam\noptimizer. Apart from the momentum reset inherited from\nthe original SPAM, Stable-SPAM introduces two tech-\nniques: Adaptive Gradient Norm (AdaGN) and Adaptive\nSpike-Aware Clipping (AdaClip), which we will explain\nin detail. The pseudocode is provided in Appendix C.\nAdaptive Gradient Norm (AdaGN). As we can observe\nin Figures 4 and 3, spikes in training loss and instances of\ntraining divergence usually align with abrupt surges in the\ngradient norm, consistent with findings in (Takase et al.,\n2023; Huang et al., 2025). To address these training instabil-\nities, we propose AdaGN, a method that stabilizes gradients\nby adaptively scaling them based on their historical l2 norm\nstatistics. To better track the dynamics of the gradient norm\nduring training, we leverage the idea of Adam by maintain-\ning moving averages of both the first and second moments\nof the gradient norm. Concretely, we compute and update\nthe moving averages of the gradient norm (mnorm, vnorm),\n3\n\nStable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam\n0\n1K\n2K\n3K\n4K\n0\n5\n10\n15\n20\nGradient Norm\n0\n500\n1K\n1.5K\n0\n10\n20\n30\n0\n1K\n2K\n3K\n4K\n0\n2\n4\n6\n8\n0\n1K\n2K\n3K\n4K\nLLaMA-130M (LR: 3e-3)\n4\n6\n8\nTraining Loss\n0\n500\n1K\n1.5K\nLLaMA-350M (LR: 2e-3)\n4\n6\n8\n0\n1K\n2K\n3K\n4K\nLLaMA-350M (LR: 1e-3)\n4\n6\n8\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBF16\nFP4\nFigure 4. Training loss and gradient norm of Adam using various learning rates with BF16 and FP4 precision. Experiments are\nconducted under the same training configuration with LLaMA-130M/350M.\nthen use them to derive a normalized gradient:\ngnorm = ∥gt∥2,\n(1)\nmnorm = γ1 · mnorm +\n\u00001 −γ1\n\u0001\n· gnorm,\n(2)\nvnorm = γ2 · vnorm +\n\u00001 −γ2\n\u0001\n· g2\nnorm,\n(3)\nˆgt =\ngt\ngnorm\n·\nmnorm\n√vnorm + ϵ.\n(4)\nwhere ˆgt is the normalized gradient, γ1 and γ2 are momen-\ntum coefficients and ϵ is small constant for numerical stabil-\nity. By rescaling gt with a ratio of its historical mean norm\nmnorm to the square root of its historical second moment\n√vnorm, AdaGN mitigates abrupt gradient norm spikes.\nNote that as the gradient norm gnorm is essentially a scalar\nfor an entire layer, the additional parameter overhead intro-\nduced by AdaGN is negligible, i.e., two extra parameters\nper layer.\nAdaptive Spike-Aware Clipping (AdaClip). Different\nfrom the spike gradient clipping technique in (Huang et al.,\n2025), which sets a fixed clipping threshold, we propose an\nadaptive clipping approach, i.e., AdaClip. The core idea\nis to dynamically adjust the clipping threshold by tracking\nthe maximum gradient magnitude observed over time, rather\nthan relying on a pre-defined fixed value. Concretely, let\ngt be the gradient at time step t. We first compute gmax,\nthe maximum absolute gradient value across all parameters.\nThen, we update the threshold Tthreshold with an exponential\nmoving average that incorporates gmax. Finally, any entries\nof gt that exceed Tthreshold are rescaled to maintain stability.\nThe procedure is formally expressed as follows:\ngmax = max\ni (|gt[i]|),\n(5)\nTthreshold = γ3 · Tthreshold + (1 −γ3) · gmax,\n(6)\nMaskspikes = (gt > Tthreshold),\n(7)\ngt[Maskspikes] = gt[Maskspikes]\ngmax\n× Tthreshold,\n(8)\nwhere γ3 ∈[0, 1] controls the weight of the moving average.\nWhen γ3 is large, Tthreshold responds more slowly to new\ngradient maxima, leading to more stable updates. When γ3\nis small, it adapts more quickly to sharp changes in gradient\nmagnitude.\nMomentum Reset (MoRet).\nFollowing Huang et al.\n(2025), we adopt momentum reset (MoRet) to periodically\nreset the accumulated first and second moments in Adam.\nThe effectiveness of MoRet lies in addressing the negative\neffects of gradient spikes, which can inflate the first and\nsecond moments of Adam. Since Adam uses exponential\nmoving averages to track their historical information, these\ninflated values caused by spiked gradients can have pro-\nlonged detrimental effects (Huang et al., 2025) on moments.\nBy resetting the momentum terms at fixed intervals (∆T),\nMoRet mitigates the lasting influence of unusually large\ngradients, enabling more stable and consistent optimization.\n4\n\nStable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam\nTable 1. Comparison of various optimizers of INT4 and FP4\ntraining of LLaMA models on C4. Perplexity is reported.\nINT4 Training\nFP4 Training\n130M\n350M\n1B\n130M\n350M\n1B\nAdam\n26.4\n24.14\n21.59\n28.9\n24.59\n22.01\nAdam+GradClip\n26.30\n21.64\n19.74\n28.27\n20.84\n20.25\nAdafactor\n25.11\n20.45\n20.65\n26.89\n20.53\n20.03\nSPAM\n25.03\n20.19\n19.98\n26.78\n20.35\n19.74\nStable-SPAM\n24.33\n17.76\n17.42\n26.31\n19.49\n18.48\nAdam (BF16)\n24.53\n21.38\n19.73\n24.53\n21.38\n19.73\nTraining Tokens\n2.2B\nA2W2\nA3W3\nA4W4\n3.0\n3.5\n4.0\nEval Loss\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nStableSPAM\nAdam\nBF16-Adam\nFigure 5. StableSPAM under Extremely Low-Precision Train-\ning.\nExperiments are conducted with 350M models on C4\nDataset. BF16-Adam denotes that the model is trained with\nBF16 by Adam. The final loss on validation set is reported.\n4. Experiments\nTo\ndemonstrate\nthe\nefficacy\nof\nthe\nproposed\nStable-SPAM,\nwe\nconduct\nextensive\nexperiments\nwith various sizes of the LLaMA model on the C4 dataset.\nBaselines. We adopt five popular optimizers as our base-\nlines including Adam (Kingma, 2014), Adafactor (Shazeer\n& Stern, 2018), Lion (Chen et al., 2024), Adam-mini (Zhang\net al., 2024a) and SPAM (Huang et al., 2025). Among these,\nAdam and Adafactor are well-established and widely used,\nwhile Adam-mini and SPAM have been introduced more\nrecently. Besides, we also include gradient clipping (Good-\nfellow, 2016) (GradClip) in conjunction with Adam as an\nadditional baseline.\nExperimental Setup. Following (Lialin et al., 2023; Zhao\net al., 2024a), we train LLaMA-based architectures ranging\nfrom 60M to 1B parameters. Each architecture is configured\nwith RMSNorm (Shazeer, 2020) and SwiGLU activations\n(Zhang & Sennrich, 2019). For every model size, we keep\nthe same set of hyperparameters across methods and vary\nonly the learning rate. Specifically, we sweep over learning\nrates from 1×10−4 to 1×10−3 , incrementing by 2×10−4\nfor each optimizer. Following the settings in (Takase et al.,\n2023; Huang et al., 2025), we set the threshold to 1 for the\nGradClip baseline. For Adafactor, we adopt the hyperparam-\neters from the original paper (Shazeer & Stern, 2018), where\nϵ1 = 10−30, ϵ2 = 10−3, and d = 1.0. The hyperparameters\nfor SPAM are configured based on the settings in (Huang\net al., 2025), with reset intervals set to 500, learning rate\nwarmup steps to 150, and the GSS threshold to 5000. For\nStable-SPAM, we set γ1 = 0.7, γ2 = 0.9 and θ = 0.999\nfor 4-bit LLM training and γ1 = 0.85, γ2 = 0.9999 and\nγ3 = 0.999 for BF16 training. Detailed descriptions of\nour task setups and hyperparameters are provided in the\nAppendix A.\n4.1. Performence of 4-bit LLM Training\nTo evaluate the performance of Stable-SPAM in 4-bit\nLLM training, we conduct experiments using both FP4 (\nE1M2: 1-bit exponent, 2-bit mantissa) and INT4 (4-bit\ninteger) quantization-aware training strategies. The training\ncurves of various LLaMA models on the C4 dataset are\npresented in Figure 1, and the final perplexity results are\nsummarized in Table 1.\nWe observe that ❶4-bit training leads to a significant perfor-\nmance drop compared to BF16 training. As shown in Table\n1, the perplexity gap between BF16 (Adam) and INT4/FP4\n(Adam) exceeds 1.5 across all model sizes, highlighting\nthe challenges of reduced precision. ❷Figure 1 shows\nthat Stable-SPAM consistently outperforms Adam by a\nsignificant margin in 4-bit scenarios, even surpassing the\nperformance of 16-bit Adam. Table 1 further demonstrates\nthat Stable-SPAM outperforms other advanced optimiz-\ners, such as Adafactor and SPAM. Among the baselines,\nincorporating GradClip reduces perplexity, while Adafactor\nand SPAM both outperform the simple application of Grad-\nClip. ❸Stable-SPAM is able to match Adam’s perfor-\nmance with half the tokens in 4-bit training. As illustrated in\nFigure 1, Stable-SPAM achieves the same perplexity as\nAdam in approximately half the training steps. ❹Notably,\nStable-SPAM performs particularly well with larger mod-\nels, such as LLaMA-350M and LLaMA-1B, showcasing its\nstrong potential for large-scale training. This is likely be-\ncause large-scale, low-precision training is more susceptible\nto instability issues (Fishman et al., 2024), making stabi-\nlized training approaches like Stable-SPAM especially\nbeneficial.\n4.2. Performence of Extremely Low-Precision Training\nTo evaluate the performance of Stable-SPAM under ex-\ntremely low-precision training, we conducted experiments\non LLaMA-350M using A2W2 (INT2), A3W3 (INT3),\nand A4W4 (INT4) configurations. The final validation\nloss is presented in Figure 5. The results indicate that\nStable-SPAM consistently outperforms Adam across all\nlow-precision settings and even matches the performance of\nBF16-Adam under INT3 training.\n5\n\nStable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam\n12K\n28K\n44K\n60K\nUpdate Steps\n17\n19\n21\n23\n(1) LLaMA-350M\n7K\n11K\n15K\n19K\nUpdate Steps\n23\n25\n27\n30\nPerplexity\n(2) LLaMA-130M\n30K\n50K\n70K\n90K\nUpdate Steps\n14\n15\n17\n19\n(3) LLaMA-1B\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAdam\nStable-SPAM\nFigure 6. Performance of BF16 training with various model sizes. Experiments are based on LLaMA models trained on C4 Dataset.\n4.3. Performence of BF16 LLM Training\nTo further evaluate the efficacy of Stable-SPAM, we con-\nducted experiments on various LLaMA model sizes using\nstandard BF16 training. The experiments are based on C4\ndataset. The training curves and final perplexity values are\npresented in Figure 6 and Table 2, respectively. Table 2\nhighlights that Stable-SPAM consistently delivers supe-\nrior performance across different model sizes, surpassing the\nsecond-best optimizer with significant improvements. Fur-\nthermore, Figure 6 illustrates that Stable-SPAM achieves\nthe same performance as Adam in only half the training\nsteps or even fewer for LLaMA-350M and LLaMA-1B,\nvalidating its ability to match Adam’s performance while\nrequiring significantly fewer tokens under BF16 LLM train-\ning. The above results demonstrate that the promise of\nStable-SPAM not only holds for low-precision LLM\ntraining but also holds for the standard BF16 training.\nTable 2. Comparison among various optimizers on BF16 train-\ning. Perplexity is reported.\nOptimizer\n60M\n130M\n350M\n1B\nAdam-mini\n34.10\n24.85\n19.05\n16.07\nAdam\n34.09\n24.91\n18.77\n16.13\nAdam + GradClip\n33.33\n24.88\n18.51\n15.22\nAdafactor\n32.57\n23.98\n17.74\n15.19\nSPAM\n30.46\n23.36\n17.42\n14.66\nStable-SPAM\n28.84\n22.21\n16.85\n13.90\nTraining Tokens\n1.1B\n2.2B\n6.4B\n11.6B\n4.4. Integration with Other Optimizers\nAlthough AdaGN and AdaClip are proposed specifically\nfor Stable-SPAM, one may wonder, “Can AdaGN and\nAdaClip also be compatible with other optimizers?” To\nanswer this question, we applied AdaGN and AdaClip to\ntwo recently published optimizers: Lion (Chen et al., 2024)\nand Adam-mini (Zhang et al., 2024a). We conducted com-\nparative experiments using Lion and Adam-mini alone, as\nwell as in combination with AdaGN and AdaClip, under\na 4-bit training setting. These experiments were performed\non LLaMA-60M/130M models with the C4 dataset.\nThe results in Table 3 show that AdaGN and AdaClip\nconsistently enhance the performance of both Lion and\nAdam-mini under FP4 and INT4 training settings, across\nLLaMA-60M and LLaMA-130M model sizes. Notably, on\nLLaMA-130M with INT4 training, Lion achieves a perplex-\nity improvement of up to 5.88, and Adam-mini on LLaMA-\n60M under FP4 training sees an improvement of 1.72. These\nimprovements underscore the broad applicability and effec-\ntiveness of the proposed AdaGN and AdaClip methods.\nTable 3. Performence of AdaGN and AdaClip on Lion and\nAdam-mini optimizers.\nExperiments are based on LLaMA-\n60M/130M with 4-Bit training.\nOptimizers\nINT4 Training\nFP4 Training\n60M\n130M\n60M\n130M\nLion\n39.36\n35.28\n39.89\n34.20\nLion+AdaGN+AdaClip\n38.49\n29.40\n36.75\n31.63\nAdam-mini\n34.84\n29.79\n36.37\n32.95\nAdam-mini+AdaGN+AdaClip\n34.61\n29.65\n34.65\n32.39\nTraining Tokens\n1.1B\n4.5. Effect on Stabilizing Training\nTo validate the effectiveness of our proposed AdaGN and\nAdaClip techniques in stabilizing the LLM training pro-\ncess, Firstly, we compared the training loss and gradient\nnorm curves across three settings: using Adam alone, using\nAdam with AdaGN, and using Adam with both AdaGN and\nAdaClip. Our experiments employed LLaMA-130M with\na learning rate of 3e-3 under an FP4 training setting. As\n6\n\nStable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam\n0\n1K 2K 3K 4K 5K\nUpdate Steps\n0\n100\n200\n300\n400\n500\nGradient Norm\n0\n1K 2K 3K 4K 5K\nUpdate Steps\n4\n5\n6\n7\n8\nTraining Loss\n1e-3\n3e-3\n5e-3\nLR\n4\n5\n6\nFinal Eval Loss\n1e-3\n3e-3\n5e-3\nLR\n4\n5\n6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAdam\nAdam + AdaGN\nAdam + AdaGN + AdaClip\nStable-SPAM-INT4\nSPAM-INT4\nStable-SPAM-FP4\nSPAM-FP4\nFigure 7. Effect of AdaGN and AdaClip on stabilizing FP4 LLM training. The left two figures use LLaMA-130M (LR = 3e-3), and\nthe right two figures use LLaMA-60M.\nshown in Figure 7, training solely with Adam leads to diver-\ngence in the training loss and frequent spikes in the gradient\nnorm. However, once AdaGN is introduced, the training\nloss converges, and the gradient norm is noticeably reduced.\nAdding AdaClip on top of AdaGN further decreases the\ngradient norm and yields a smoother training loss curve.\nSecondly, we present the final performance across a range\nof learning rates, from 5 × 10−4 to 5 × 10−3, evaluated on\nLLaMA-60M under both FP4 and INT4 training settings.\nThe results in Figure 7 show that Stable-SPAM produces\na significantly flatter curve, highlighting its stability across\nvarying learning rates. These results demonstrate the effec-\ntiveness of the proposed AdaGN and AdaClip techniques\nin achieving a more stable and consistent training process.\n4.6. Ablation Study\nTo validate the effectiveness of the three components,\nMoRet, AdaGN, and AdaClip, in Stable-SPAM, we\nconduct a comprehensive ablation study. Specifically, we\ntake two approaches: (1) We iteratively incorporate MoRet,\nAdaGN, and AdaClip into the Adam optimizer to measure\ntheir individual and combined improvements under both\nFP4 and BF16 training settings. (2) We replace AdaClip\nwith SpikeClip (Huang et al., 2025) and AdaGN with Grad-\nClip (Goodfellow, 2016) to further assess the unique con-\ntributions of our proposed components. The results, sum-\nmarized in Table 4, reveal the following observations: ❶\nMoRet consistently improves performance across both FP4\nand BF16 settings. ❷Under both FP4 training, AdaGN\nalone shows limited improvement. However, when com-\nbined with AdaClip, it substantially reduces final perplex-\nity. ❸Conversely, in the BF16 setting, AdaGN alone yields\nconsiderable performance gains, but adding AdaClip of-\nfers limited improvement.\nThis discrepancy may stem\nfrom the higher frequency of extreme element-wise gradient\nspikes in this FP4 training experiments, which necessitates\nAdaClip to correct biased update directions effectively.\nFinally, replacing AdaClip with SpikeClip (Huang et al.,\n2025) and AdaGN with GradClip (Goodfellow, 2016) re-\nsults in increased perplexity, further validating the efficacy\nof our proposed AdaGN and AdaClip.\nTable 4. Ablations on Stable-SPAM. Experiments are based on\nLLaMA-60M and C4.\nOptimizer\nFP4\nBF16\nAdam\n35.47\n34.09\nAdam + MoRet\n32.4\n31.47\nAdam + MoRet + AdaClip\n31.97\n30.29\nAdam + MoRet + AdaGN\n32.26\n28.96\nAdam + MoRet + AdaGN + AdaClip (Stable-SPAM)\n31.40\n28.84\nAdam + MoRet+AdaGN+SpikeClip (Huang et al., 2025)\n32.01\n28.90\nAdam + MoRet+ GradClip (Goodfellow, 2016)+AdaClip\n31.95\n29.87\nAdam + MoRet+AdaGN+AdaClip (Stable-SPAM)\n31.40\n28.84\nTraining Tokens\n1.1B\n4.7. Hyper-Parameter Analysis\nStable-SPAM introduces four hyperparameters: γ1, γ2,\nγ3, and ∆T, which extend the functionality of Adam.\nAmong these, γ1 and γ2 serve a similar purpose to β1 and β2\nin Adam, controlling the smoothness of updates to the first\nmoment mnorm and the second moment vnorm. Larger val-\nues of γ1 and γ2 result in smoother updates, placing greater\nemphasis on historical gradient norm statistics when adapt-\ning the current gradient norm. Similarly, γ3 plays a role in\ndetermining the threshold for identifying gradient spikes. A\nlarger γ3 leads to a smoother and more conservative thresh-\nold, resulting in a higher proportion of gradients being clas-\nsified as spike gradients. To investigate the impact of these\nhyperparameters, we plot the final perplexity curve while\nvarying γ1 from 0.5 to 0.9, γ2 from 0.8 to 0.999, γ3 from\n0.9 to 0.999, and ∆T from 250 to 5000. The experiments\nare conducted using LLaMA-60M, trained on 1.1B C4 to-\n7\n\nStable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam\n0.5\n0.7\n0.9\n1\n31\n33\n36\nPerplexity\n0.8\n0.9\n0.999\n2\n30\n33\n36\n0.9\n0.999\n0.99999\n3\n30\n33\n36\n250\n1K\n5K\n T\n30\n33\n36\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 8. Hyper-parameter Analysis. Experiments are conducted with FP4 training on LLaMA-60M and C4 with 1.1B tokens.\nkens under the FP4 training setting. The results in Figure 8\ndemonstrate that overly small or excessively large values of\nthese hyperparameters can degrade performance. However,\nthe intuitive interpretations of these hyperparameters make\nthem straightforward to tune, and they typically require min-\nimal adjustments. In this paper, we adopt the optimal values\nγ1 = 0.7, γ2 = 0.9, γ3 = 0.999, and ∆T = 1000, which\nwork effectively for all 4-bit training scenarios.\n5. Related Work\nInstability of Training Large Language Models. The\ninstability of large language model (LLM) training,\nwhich are marked by loss spikes and catastrophic diver-\ngence(Chowdhery et al., 2023; Molybog et al., 2023),\nhas driven extensive research into stabilization techniques.\nThese methods generally fall into three main categories:\n(1) gradient preprocessing, (2) architectural modifications,\nand (3) initialization strategies. Gradient preprocessing\ntypically involves scaling and clipping gradients at the\nstart of the optimization process to stabilize the training.\nA well-known example is gradient clipping (Goodfellow,\n2016), which globally rescales the gradient norm to a\nfixed value. Later, Adafactor (Shazeer & Stern, 2018) in-\ntroduced capping the norm of the parameter updates in-\nstead of the raw gradients. More recently, SPAM (Huang\net al., 2025) proposed detecting and clipping anomalous\ngradients based on historical gradient statistics. However,\na common drawback of these methods is that they re-\nquire manually setting a predefined threshold. Architec-\nturally, Xiong et al. (2020) showed that Post-LayerNorm\n(Post-LN) amplifies gradients, causing instability with\nlarge learning rates, while Pre-LayerNorm (Pre-LN) pre-\nserves gradient norms for stable training.\nEmbed Lay-\nerNorm (Embed LN) normalizes embeddings(Dettmers\net al., 2021), though it may impact performance(Scao et al.,\n2022), while Embed Detach(Ding et al., 2021; Zeng et al.,\n2022) reduces loss spikes by truncating gradients. Deep-\nNorm(Wang et al., 2024) scales residual connections to sta-\nbilize ultra-deep models, and αReparam(Zhai et al., 2023)\nprevents attention entropy collapse via spectral-normalized\nparameterization.\nInitialization strategies offer comple-\nmentary stability benefits. Scaled Embed(Takase et al.,\n2023) stabilizes LayerNorm gradients, while Scaled Ini-\ntialization(Nguyen & Salazar, 2019) reduces variance using\nN(0,\np\n2/(5d)/\n√\n2N). Fixup(Zhang et al., 2019; Huang\net al., 2020) eliminates LayerNorm entirely, inspiring norm-\nfree architectures. Though ongoing advancements refine\nthese approaches, training stability remains a key challenge\nin LLM development.\nLow-precision LLM Training.\nLow-precision train-\ning (Wang et al., 2018; Lin et al., 2022; Xi et al., 2024a;b;\nWortsman et al., 2023a) has emerged as a promising ap-\nproach to improve both computational and memory effi-\nciency during training. Among these methods, FP16 (Mi-\ncikevicius et al., 2017) and BF16 (Kalamkar et al., 2019)\nare the most widely adopted precision formats. To push\nthe efficiency further, 8-bit training has garnered increasing\nattention. For instance, LM-FP8 (Peng et al., 2023) enables\ntraining with FP8 precision While (Fishman et al., 2024)\ndemonstrates that as training scales up (larger than 250B\ntokens), the issue of activation outliers becomes more pro-\nnounced, posing challenges to the representation range of\nlow-bit data formats. To address this challenge, (Fishman\net al., 2024) proposes a smoothing strategy, while (Ashkboos\net al., 2025) leverages Hadamard transformations to mitigate\nthe impact of activation outliers. Furthermore, the choice\nof data format significantly influences training performance.\nThe INT8 format is the most widely supported low-precision\nformat, whereas FP8, available in NVIDIA’s Hopper GPU\narchitecture, provides specialized support. Additionally, the\nMX format (Rouhani et al., 2023) demonstrates superior\nrepresentational capability, though it is rarely supported by\ncurrent hardware. In this work, we investigate the training\ninstability associated with low-precision training and pro-\npose enhancements through the design of optimizers. Our\napproach is compatible with existing techniques, provid-\ning a complementary solution to improve the stability of\nlow-precision training.\n8\n\nStable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam\n6. Conclusion\nThis paper presents a comprehensive study on the training\ninstability challenges of 4-bit quantization in large language\nmodels. We find that while low-precision training signifi-\ncantly reduces memory and computational costs, it also am-\nplifies the sensitivity to learning rates, and increases the like-\nlihood of gradient and loss spikes. To address these issues,\nwe propose Stable-SPAM, an optimizer that combines\nthree key techniques: AdaClip, AdaGN, and MoRet. Em-\npirical results on LLaMA models of various sizes demon-\nstrate that Stable-SPAM not only stabilizes 4-bit training\nbut also achieves better performance compared to existing\noptimizers, sometimes even surpassing BF16 performance.\nWe additionally show that these stabilization strategies are\nbroadly applicable, benefiting other optimizers like Lion\nand Adam-mini.\nAcknowledgments\nThis work used the Dutch national e-infrastructure with the\nsupport of the SURF Cooperative using the funding of the\nprojects EINF-12538 and EINF-10925.\nImpact Statement\nThis paper advances the field of large language model\n(LLM) training by proposing a stable optimizer that en-\nables more stable and efficient optimization at low-precision\n(4-bit) arithmetic. By reducing computational and memory\noverhead, our approach has the potential to lower energy\nconsumption and lessen the environmental footprint of train-\ning large-scale models. There are many potential societal\nconsequences of our work, none of which we feel must be\nspecifically highlighted here.\nReferences\nAshkboos, S., Nikdan, M., Tabesh, S., Castro, R. L., Hoefler,\nT., and Alistarh, D. Halo: Hadamard-assisted lossless\noptimization for efficient low-precision llm training and\nfine-tuning. arXiv preprint arXiv:2501.02625, 2025.\nChen, X., Liang, C., Huang, D., Real, E., Wang, K., Pham,\nH., Dong, X., Luong, T., Hsieh, C.-J., Lu, Y., et al. Sym-\nbolic discovery of optimization algorithms. Advances in\nneural information processing systems, 36, 2024.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\nG., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,\nGehrmann, S., et al. Palm: Scaling language modeling\nwith pathways. Journal of Machine Learning Research,\n24(240):1–113, 2023.\nDettmers, T., Lewis, M., Shleifer, S., and Zettlemoyer, L. 8-\nbit optimizers via block-wise quantization. arXiv preprint\narXiv:2110.02861, 2021.\nDing, M., Yang, Z., Hong, W., Zheng, W., Zhou, C., Yin,\nD., Lin, J., Zou, X., Shao, Z., Yang, H., et al. Cogview:\nMastering text-to-image generation via transformers. Ad-\nvances in neural information processing systems, 34:\n19822–19835, 2021.\nFishman, M., Chmiel, B., Banner, R., and Soudry, D. Scal-\ning fp8 training to trillion-token llms. arXiv preprint\narXiv:2409.12517, 2024.\nGoodfellow, I. Deep learning, 2016.\nHuang, T., Zhu, Z., Jin, G., Liu, L., Wang, Z., and Liu, S.\nSpam: Spike-aware adam with momentum reset for stable\nllm training. arXiv preprint arXiv:2501.06842, 2025.\nHuang, X. S., Perez, F., Ba, J., and Volkovs, M. Improving\ntransformer optimization through better initialization. In\nInternational Conference on Machine Learning, pp. 4475–\n4483. PMLR, 2020.\nKalamkar, D., Mudigere, D., Mellempudi, N., Das, D.,\nBanerjee, K., Avancha, S., Vooturi, D. T., Jammala-\nmadaka, N., Huang, J., Yuen, H., et al.\nA study of\nbfloat16 for deep learning training.\narXiv preprint\narXiv:1905.12322, 2019.\nKingma, D. P. Adam: A method for stochastic optimization.\narXiv preprint arXiv:1412.6980, 2014.\nLangley, P. Crafting papers on machine learning. In Langley,\nP. (ed.), Proceedings of the 17th International Conference\non Machine Learning (ICML 2000), pp. 1207–1216, Stan-\nford, CA, 2000. Morgan Kaufmann.\nLee, J., Bae, J., Kim, B., Kwon, S. J., and Lee, D. To\nfp8 and back again: Quantifying the effects of reduc-\ning precision on llm training stability. arXiv preprint\narXiv:2405.18710, 2024.\nLi, S., Liu, H., Bian, Z., Fang, J., Huang, H., Liu, Y., Wang,\nB., and You, Y. Colossal-ai: A unified deep learning\nsystem for large-scale parallel training. In Proceedings of\nthe 52nd International Conference on Parallel Processing,\npp. 766–775, 2023.\nLialin, V., Muckatira, S., Shivagunde, N., and Rumshisky,\nA. Relora: High-rank training through low-rank updates.\nIn The Twelfth International Conference on Learning\nRepresentations, 2023.\nLin, J., Zhu, L., Chen, W.-M., Wang, W.-C., Gan, C., and\nHan, S. On-device training under 256kb memory. Ad-\nvances in Neural Information Processing Systems, 35:\n22941–22954, 2022.\n9\n\nStable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam\nLiu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao,\nC., Deng, C., Zhang, C., Ruan, C., et al. Deepseek-v3\ntechnical report. arXiv preprint arXiv:2412.19437, 2024.\nMa, C., Gong, W., Scetbon, M., and Meeds, E. Swan:\nPreprocessing sgd enables adam-level performance on\nllm training with significant memory reduction. arXiv\npreprint arXiv:2412.13148, 2024.\nMicikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen,\nE., Garcia, D., Ginsburg, B., Houston, M., Kuchaiev, O.,\nVenkatesh, G., et al. Mixed precision training. arXiv\npreprint arXiv:1710.03740, 2017.\nMolybog, I., Albert, P., Chen, M., DeVito, Z., Esiobu, D.,\nGoyal, N., Koura, P. S., Narang, S., Poulton, A., Silva, R.,\net al. A theory on adam instability in large-scale machine\nlearning. arXiv preprint arXiv:2304.09871, 2023.\nNguyen, T. Q. and Salazar, J. Transformers without tears:\nImproving the normalization of self-attention.\narXiv\npreprint arXiv:1910.05895, 2019.\nNie, Y., Nguyen, N. H., Sinthong, P., and Kalagnanam, J. A\ntime series is worth 64 words: Long-term forecasting with\ntransformers. arXiv preprint arXiv:2211.14730, 2022.\nPeng, H., Wu, K., Wei, Y., Zhao, G., Yang, Y., Liu, Z.,\nXiong, Y., Yang, Z., Ni, B., Hu, J., et al.\nFp8-lm:\nTraining fp8 large language models.\narXiv preprint\narXiv:2310.18313, 2023.\nRouhani, B. D., Zhao, R., More, A., Hall, M., Khodamoradi,\nA., Deng, S., Choudhary, D., Cornea, M., Dellinger, E.,\nDenolf, K., et al. Microscaling data formats for deep\nlearning. arXiv preprint arXiv:2310.10537, 2023.\nScao, T. L., Wang, T., Hesslow, D., Saulnier, L., Bekman, S.,\nBari, M. S., Biderman, S., Elsahar, H., Muennighoff, N.,\nPhang, J., et al. What language model to train if you have\none million gpu hours? arXiv preprint arXiv:2210.15424,\n2022.\nShazeer, N.\nGlu variants improve transformer.\narXiv\npreprint arXiv:2002.05202, 2020.\nShazeer, N. and Stern, M. Adafactor: Adaptive learning\nrates with sublinear memory cost. In International Con-\nference on Machine Learning, pp. 4596–4604. PMLR,\n2018.\nTakase, S., Kiyono, S., Kobayashi, S., and Suzuki, J. Spike\nno more: Stabilizing the pre-training of large language\nmodels. arXiv preprint arXiv:2312.16903, 2023.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., et al. Llama 2: Open foundation and fine-\ntuned chat models. arXiv preprint arXiv:2307.09288,\n2023.\nWang, H., Ma, S., Dong, L., Huang, S., Zhang, D., and\nWei, F. Deepnet: Scaling transformers to 1,000 layers.\nIEEE Transactions on Pattern Analysis and Machine In-\ntelligence, 2024.\nWang, N., Choi, J., Brand, D., Chen, C.-Y., and Gopalakr-\nishnan, K. Training deep neural networks with 8-bit\nfloating point numbers. Advances in neural information\nprocessing systems, 31, 2018.\nWortsman, M., Dettmers, T., Zettlemoyer, L., Morcos, A.,\nFarhadi, A., and Schmidt, L. Stable and low-precision\ntraining for large-scale vision-language models.\nAd-\nvances in Neural Information Processing Systems, 36:\n10271–10298, 2023a.\nWortsman, M., Liu, P. J., Xiao, L., Everett, K., Alemi, A.,\nAdlam, B., Co-Reyes, J. D., Gur, I., Kumar, A., Novak,\nR., et al. Small-scale proxies for large-scale transformer\ntraining instabilities. arXiv preprint arXiv:2309.14322,\n2023b.\nXi, H., Li, C., Chen, J., and Zhu, J. Training transform-\ners with 4-bit integers. Advances in Neural Information\nProcessing Systems, 36:49146–49168, 2023.\nXi, H., Cai, H., Zhu, L., Lu, Y., Keutzer, K., Chen, J., and\nHan, S. Coat: Compressing optimizer states and activa-\ntion for memory-efficient fp8 training. arXiv preprint\narXiv:2410.19313, 2024a.\nXi, H., Chen, Y., Zhao, K., Teh, K. J., Chen, J., and Zhu,\nJ. Jetfire: Efficient and accurate transformer pretraining\nwith int8 data flow and per-block quantization. arXiv\npreprint arXiv:2403.12422, 2024b.\nXiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing,\nC., Zhang, H., Lan, Y., Wang, L., and Liu, T. On layer\nnormalization in the transformer architecture. In Inter-\nnational Conference on Machine Learning, pp. 10524–\n10533. PMLR, 2020.\nZeng, A., Liu, X., Du, Z., Wang, Z., Lai, H., Ding, M.,\nYang, Z., Xu, Y., Zheng, W., Xia, X., et al. Glm-130b:\nAn open bilingual pre-trained model.\narXiv preprint\narXiv:2210.02414, 2022.\nZhai, S., Likhomanenko, T., Littwin, E., Busbridge, D.,\nRamapuram, J., Zhang, Y., Gu, J., and Susskind, J. M.\nStabilizing transformer training by preventing attention\nentropy collapse. In International Conference on Machine\nLearning, pp. 40770–40803. PMLR, 2023.\nZhang, B. and Sennrich, R. Root mean square layer nor-\nmalization. Advances in Neural Information Processing\nSystems, 32, 2019.\n10\n\nStable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam\nZhang, H., Dauphin, Y. N., and Ma, T. Fixup initialization:\nResidual learning without normalization. arXiv preprint\narXiv:1901.09321, 2019.\nZhang, Y., Chen, C., Li, Z., Ding, T., Wu, C., Ye, Y., Luo,\nZ.-Q., and Sun, R. Adam-mini: Use fewer learning rates\nto gain more. arXiv preprint arXiv:2406.16793, 2024a.\nZhang, Z., Jaiswal, A., Yin, L., Liu, S., Zhao, J., Tian,\nY., and Wang, Z. Q-galore: Quantized galore with int4\nprojection and layer-adaptive low-rank gradients. arXiv\npreprint arXiv:2407.08296, 2024b.\nZhao, J., Zhang, Z., Chen, B., Wang, Z., Anandkumar,\nA., and Tian, Y. Galore: Memory-efficient llm train-\ning by gradient low-rank projection.\narXiv preprint\narXiv:2403.03507, 2024a.\nZhao, R., Morwani, D., Brandfonbrener, D., Vyas, N., and\nKakade, S. Deconstructing what makes a good optimizer\nfor language models. arXiv preprint arXiv:2407.07972,\n2024b.\n11\n\nStable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam\nA. Architecture and Hyperparameters\nWe introduce details of the LLaMA architecture and hyperparameters used for 4-bit and BF16 pre-training, following Lialin\net al. (2023); Zhao et al. (2024a). Table 5 shows the most hyperparameters of LLaMA models across model sizes. We use a\nmax sequence length of 256 for all models, with a batch size of 512, with a batch size of 131K tokens. For all experiments,\nwe adopt learning rate warmup of 2000 training steps, and use cosine annealing for the learning rate schedule, decaying to\n10% of the initial learning rate.\nTable 5. Configurations of LLaMA models used in this paper.\nParams\nHidden\nIntermediate\nHeads\nLayers\n60M\n512\n1376\n8\n8\n130M\n768\n2048\n12\n12\n350M\n1024\n2736\n16\n24\n1 B\n2048\n5461\n24\n32\nFor all methods across each model size (from 60M to 1B), we tune the learning rates from 1e−4 to 1e−3 with an increasing\nstep of 2 × 10−4 for pre-training tasks, and the best learning rate is selected based on the validation perplexity. The detailed\nhyperparameter of Stable-SPAM on 4-bit training and BF16 training are reported in Table 6 and Table 7.\nTable 6. Hyperparameters of Stable-SPAM for 4-bit pre-training experiments in this paper.\nHyper-Parameters\nLLaMA-130M\nLLaMA-350M\nLLaMA-1B\nLR\n1e −3\n4e −4\n2e −4\n∆T\n1000\n1000\n1000\nγ1\n0.7\n0.7\n0.7\nγ2\n0.9\n0.9\n0.9\nγ3\n0.999\n0.999\n0.999\nTable 7. Hyperparameters of Stable-SPAM for BF6 pre-training experiments in this paper.\nHyper-Parameters\nLLaMA-60M\nLLaMA-130M\nLLaMA-350M\nLLaMA-1B\nStandard Pretraining\nLR\n1e −3\n8e −4\n4e −4\n2e −4\n∆T\n1000\n1000\n1000\n1000\nγ1\n0.85\n0.85\n0.85\n0.85\nγ2\n0.99999\n0.99999\n0.99999\n0.99999\nγ3\n0.999\n0.999\n0.999\n0.999\nB. Time Series Forescasting Task\nWe conducted additional experiments on time-series prediction tasks. In these experiments, we intentionally introduced\nanomalous data with a probability A=10% to simulate gradient anomalies. Experiments are conducted with 10 repeated runs\non Weather time series data2 using PatchTST (Nie et al., 2022) model. The results are presented in Figure 9.\nThe findings demonstrate that as the severity of anomalous data increases, Stable-SPAM’s performance advantage over\nAdam becomes more pronounced. Besides, Stable-SPAM consistently surpasses SPAM across all settings. These results\nfurther highlight the effectiveness of the proposed Stable-SPAM.\nC. Pseudocode\nThe pseudocode is presented in Alogrithm 1.\n2https://www.bgc-jena.mpg.de/wetter/\n12\n\nStable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam\n50\n100\nTraining Epochs\n0.150\n0.152\n0.154\n0.156\nTest Loss\n(1) A=0%\n50\n100\nTraining Epochs\n0.20\n0.22\n(2) A=0%, S=2\n50\n100\nTraining Epochs\n0.30\n0.35\n(3) A=0%, S=5\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAdam\nSPAM\nStable-SPAM\nFigure 9. Test Loss during Training Process on Weather Time-series Data. Anomalous data is generated by adding Gaussian noise to\n10% of randomly selected input values. Specifically, the anomalies data are conducted with X = X+Gaussin(0, Severity∗Max(X))\nwhere X is the inputs.\nAlgorithm 1 Stable-SPAM\nInput: A layer weight matrix w ∈Rm×n, learning rate α, decay rates β1 = 0.9, β2 = 0.999, initial parameters w0, γ1 = 0.7, γ2 = 0.9\nfor AdaGN and γ3 = 0.999 for AdaClip, momentum reset interval ∆T, small constant ϵ = 1 × 10−6, and total training steps\nT.\nOutput: Optimized parameters wT .\nwhile t < T do\ngt ∈Rm×n ←−∇wϕt(wt)\n// Gradient of the objective at step t.\ngmax ←Max(abs(gt))\nTthreshold ←Tthreshold · θ + (1 −θ) gmax\nbTthreshold ←Tthreshold\n1 −θt\n// Bias correction for threshold\nMaskspikes ←\n\u0000abs(gt) > bTthreshold\n\u0001\nif sum\n\u0000Maskspikes\n\u0001\n> 0 then\ngt[Maskspikes] ←gt[Maskspikes]\ngmax\n× bTthreshold\nend\ngnorm ←∥gt∥2\nmnorm ←γ1 mnorm + (1 −γ1) gnorm\nvnorm ←γ2 vnorm + (1 −γ2) g2\nnorm\nbmnorm ←mnorm\n1 −γt\n1\n, bvnorm ←vnorm\n1 −γt\n2\n// Bias-corrected norm estimates\nadaptive norm ←\nbmnorm\n√\nbvnorm + ϵ\ngt ←\ngt\ngnorm × adaptive norm\nif (Mod(t, ∆T) = 0) then\nm ←zeros like(m)\nv ←zeros like(v)\nend\nmt ←β1 mt−1 + (1 −β1) gt\nvt ←β2 vt−1 + (1 −β2) g2\nt\nc\nmt ←\nmt\n1 −βt\n1\n// bias correction\nbvt ←\nvt\n1 −βt\n2\n// bias correction\nwt ←wt−1 −α\nc\nmt\n√\nbvt + ϵ\nt ←t + 1\nend\nreturn wT .\n13\n",
  "metadata": {
    "source_path": "papers/arxiv/Stable-SPAM_How_to_Train_in_4-Bit_More_Stably_than_16-Bit_Adam_dec76d7d59b33fb0.pdf",
    "content_hash": "dec76d7d59b33fb0430386f6bad15f06f65a22447a093afc4ac4c361eab1532e",
    "arxiv_id": null,
    "title": "Stable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam",
    "author": "Tianjin Huang*, Haotian Hu*, Zhenyu Zhang*, Gaojie Jin, Xiang Li, Li Shen, Tianlong Chen, Lu Liu, Qingsong Wen, Zhangyang Wang, Shiwei Liu",
    "creation_date": "D:20250225024042Z",
    "published": "2025-02-25T02:40:42",
    "pages": 13,
    "size": 966414,
    "file_mtime": 1740470190.2754622
  }
}