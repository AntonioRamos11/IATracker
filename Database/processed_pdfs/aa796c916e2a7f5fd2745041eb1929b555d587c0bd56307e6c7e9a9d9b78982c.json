{
  "text": "A Systematic Survey of Automatic Prompt Optimization Techniques\nKiran Ramnath, Kang Zhou, Sheng Guan, Soumya Smruti Mishra, Xuan Qi, Zhengyuan Shen,\nShuai Wang, Sangmin Woo, Sullam Jeoung, Yawei Wang, Haozhu Wang, Han Ding,\nYuzhe Lu, Zhichao Xu, Yun Zhou, Balasubramaniam Srinivasan, Qiaojing Yan, Yueyan Chen,\nHaibo Ding, Panpan Xu, and Lin Lee Cheong\nAmazon Web Services\n{raxkiran,zhoukang,shguan,soumish,xuaqi,donshen, wshui,sangminw,sullamij,\nyawenwan, haozhuw, handing, yuzhelu, xzhichao, yunzzhou, srbalasu, qiaojiny,\nyyanc, hbding, xupanpan, lcheong}@amazon.com\nAbstract\nSince the advent of large language models\n(LLMs), prompt engineering has been a cru-\ncial step for eliciting desired responses for vari-\nous Natural Language Processing (NLP) tasks.\nHowever, prompt engineering remains an im-\npediment for end users due to rapid advances\nin models, tasks, and associated best practices.\nTo mitigate this, Automatic Prompt Optimiza-\ntion (APO) techniques have recently emerged\nthat use various automated techniques to help\nimprove the performance of LLMs on various\ntasks. In this paper, we present a comprehen-\nsive survey summarizing the current progress\nand remaining challenges in this field. We pro-\nvide a formal definition of APO, a 5-part uni-\nfying framework, and then proceed to rigor-\nously categorize all relevant works based on\ntheir salient features therein. We hope to spur\nfurther research guided by our framework.\n1\nIntroduction\nSince McCann et al. (2018) cast multi-task NLP\nas Question Answering, using prompts as inputs\nhas become the standard way to elicit desired re-\nsponses from Large Language Models (LLMs).\nFurthermore, LLMs’ few-shot learning (Brown\net al., 2020), instruction-following (Ouyang et al.,\n2022), and zero-shot reasoning capabilities (Ko-\njima et al., 2023) have led to a widespread prolif-\neration of prompting tricks for various tasks and\nmodel variants. However, LLMs still exhibit unpre-\ndictable sensitivity to various factors (explanation\nof the task (Li et al., 2023b),ordering (Liu et al.,\n2024a), stylistic formatting (Sclar et al.), etc.) caus-\ning a performance gap between two prompts that\nare semantically similar, thereby adding impedi-\nments for adoption by end users. Against this back-\ndrop, Black-Box Automatic Prompt Optimization\n(APO) techniques have emerged that improve task\nperformance via automated prompt improvements.\nThe possess various attractive features - (1) they do\nnot require parameter access on LLMs performing\nthe task, (2) they systematically search through the\nprompt solution space, and (3) they retain human\ninterpretability of prompt improvements. In this\nsurvey paper, we aim to highlight the advances in\nthe field. Our core contribution is a 5-part APO\ntaxonomy combined with a comprehensive fine-\ngrained categorization of various design choices\ntherein (see Fig. 1, Tables 2, 3, 4 in Appendix). We\nhope our framework will be informational for new\nand seasoned researchers alike, enabling further\nresearch on open questions.\n2\nAutomatic Prompt Optimization\nFormulation\nWe formalize the process of automatic prompt op-\ntimization (APO) as follows. Given a task model\nMtask, initial prompt ρ ∈V , the goal of an APO-\nsystem MAPO is to obtain the best performing\nprompt-template ρopt under a metric f ∈F and\neval-set Dval\nρopt := arg max\nρ∈V Ex∼Dval[f(Mtask(ρ ⊕x))] (1)\nThis objective function is not tractable for discrete\nprompt optimization as token-sequence search\nspaces are combinatorial. Instead, APO techniques\nfollow the general anatomy as described in Algo-\nrithm 1 to obtain approximate solutions.\n3\nInitialize Seed Prompts\n3.1\nManual Instructions\nSeveral approaches use a seed of manually cre-\nated instructions that offer interpretable and strong\nbaselines as the basis for further improvement,inter\nalia., ProteGi (Pryzant et al., 2023), GPS (Xu et al.,\n2022), SPRIG (Zhang et al., 2024b). While ob-\ntaining quality examples can be costly, APE (Zhou\net al., 2022) 1 showed that a few hundred samples\nare sufficient for further optimization.\n1Note: APE stands for Automatic Prompt Engineer method\nintroduced by (Zhou et al., 2022), not to be confused with APO\narXiv:2502.16923v1  [cs.CL]  24 Feb 2025\n\nPrompt optimization anatomy §2\nIteration depth §7\nVariable steps §7.2\nFixed steps §7.1\nFilter and retain\npromising candidates §6\nMeta-heuristic ensemble §6.4\nRegion-based joint search §6.3\nUpper confidence bound and variants §6.2\nTopK Greedy Search §6.1\nCandidate prompt\ngeneration §5\nProgram Synthesis §5.5\nCoverage-based §5.4\nEnsemble methods §5.4.3\nMixture of experts §5.4.2\nSingle prompt expansion §5.4.1\nMetaprompt design §5.3\nEditing with auxiliary\ntrained NN §5.2\nGenerative Adversarial Networks §5.2.3\nLLM Finetuning §5.2.2\nReinforcement Learning §5.2.1\nHeuristic-based\nedits §5.1\nVocabulary pruning §5.1.4\nWord / phrase edits §5.1.3\nGenetic Algorithm §5.1.2\nMonte Carlo Sampling §5.1.1\nInference evaluation\nand feedback §4\nHuman Feedback §4.3\nLLM Feedback §4.2\nImproving multiple candidates §4.2.2\nImproving single candidate §4.2.1\nNumeric score §4.1\nNegative log-likelihood §4.1.4\nEntropy-based §4.1.3\nReward model score §4.1.2\nTask accuracy §4.1.1\nSeed Prompts §3\nInstruction-induction via LLMs §3.2\nManual Instructions §3.1\nFigure 1: Taxonomy of Automatic Prompt Optimization\nAlgorithm 1 Prompt optimization framework\n1: P0 := {ρ1, ρ2, . . . , ρk}\n▷§3. Seed prompts\n2: Dval := {(x1, y1)}n\ni=1\n▷Validation set\n3: f1, . . . , fm ∈F\n▷§4. Inference evaluation\n4: for t = 1, 2, . . . , N do\n▷§7. Iteration depth\n▷§5. Generate prompt candidates\n5:\nGt := MAP O(P, Dval, F)\n▷§6. Filter and retain candidates\n6:\nPt := Select(Gt, Dval, F)\n▷§7. Optionally check for early convergence\n7:\nif fconvergence ≤ϵ then\n8:\nexit\n9: return arg maxρ∈PN Ex∼Dval [f(Mtask(ρ ⊕x))]\n3.2\nInstruction Induction via LLMs\nHonovich et al. (2023) were the first to propose\ninducing LLMs to infer human-readable prompts\nbased on a few demonstrations E (see Appendix\n14.1 for prompt). APE (Zhou et al., 2022) and\nDAPO (Yang et al., 2024c) use the induced seed\ninstructions for further optimization, while MOP\n(Wang et al., 2025) and GPO (Li et al., 2023c) use\nAPE to induce cluster-specific prompts. Apart from\ndemonstrations, SCULPT (Kumar et al., 2024) in-\nduced instructions from task-READMEs, while\nUniPrompt (Juneja et al., 2024) used LLMs to fill-\nwhich broadly refers to the entire area of Automatic Prompt\nOptimization\n\nin structured templates.\n4\nInference Evaluation and Feedback\nThe evaluation step helps identify promising\nprompt candidates in each iteration. Some methods\nalso use LLM feedback on prompt-response pairs\nto help generate more prompt candidates.\n4.1\nNumeric Score Feedback\n4.1.1\nAccuracy\nUsing task-specific accuracy metrics is the most\nstraightforward and widespread way of eliciting\nfeedback, i.a., (Zhou et al., 2022, 2023; Zhang\net al., 2024b; Khattab et al., 2022). Classifica-\ntion and MCQ-based QA tasks use exact accuracy,\nwhile code-related tasks measure execution accu-\nracy. Text generation tasks (summarization, transla-\ntion, creative writing) employ flexible metrics like\nBLEU-N, Rouge-N, Rouge-N-F1, or embedding-\nbased measures such as BERTScore (Zhang* et al.,\n2020) (Honovich et al., 2023; Dong et al., 2024b).\n4.1.2\nReward-model Scores\nGiven the limitations of rigid accuracy metrics,\nsome approaches proposed using learned reward\nmodels to provide more nuanced evaluations of\nprompts-response pairs (Deng et al., 2022; Sun\net al., 2024a; Kong et al., 2024). OIRL (Sun et al.,\n2024a) trained an XGBoost-based reward model\nthat takes query-prompt embedding pairs as input\nand predicts whether the prompt will elicit correct\nanswers from the language model and use it to se-\nlect appropriate prompts for specific queries using\na best-of-N strategy. DRPO (Amini et al., 2024)\nfollows an LLM-based reward modeling approach\nusing both predefined and dynamic reward criteria.\nIt first optimizes in-context learning examples E,\nand using that it optimizes the specific task prompt.\n4.1.3\nEntropy-based Scores\nEntropy-based scores evaluate the entire output\ndistribution induced by candidates, as opposed to\na single inference instance. They are gradient-\nfree but require access to the entire output prob-\nability distribution, something not usually possi-\nble with black-box LLMs. CLAPS (Zhou et al.,\n2023) leverages the negative incremental cross-\nentropy of π(xi⊕v∈V ) v/s π(xi) to identify promis-\ning words v ∈V to add to the prompt. The topK\nwords are then used as candidate tokens from which\nto construct candidate prompts. GRIPS (Prasad\net al., 2023) simply added an entropy term to\nthe task-weighted accuracy −P πρ(y) ln(πρ(y))+\n1\n|T|\nP 1(y = ˆy) to prioritize output diversity in po-\ntential prompt candidates.\n4.1.4\nNegative Log-likelihood of Output\nSome approaches like APE, GPS (Xu et al., 2022),\nPACE (Dong et al., 2024b) consider the negative\nlog-likelihood (NLL) of token sequences under the\ntarget LLM, i.e., −log(πρ(y)). This however re-\nquires the log-probabilities to be accessible during\nthe decoding of each token, limiting its applica-\nbility. The NLL for ground truth one-hot token-\nsequence is equivalent to the cross-entropy.\n4.2\nLLM Feedback\nA popular paradigm to augment or fully replace\nnumeric scores is to use textual feedback generated\nby LLMEvaluator (Wang et al., 2024a; Long et al.,\n2024; Sinha et al., 2024). It is versatile because\nit can evaluate both the response as well as the\nprompt input. It can directly aid the prompt rewrit-\ning process while being flexible to individual tasks\nas it only needs natural language instructions for\ngeneral-purpose LLMs as opposed to task-specific\nhandcrafting of metrics. A potential downside is\nthe inference cost incurred due to an additional\nLLM call. All the LLM feedback approaches pro-\nvide multiple feedback data and broadly fall into\ntwo categories - improving a single prompt candi-\ndate versus improving multiple prompt candidates\n(discussed below, examples in Appendix 14.3).\n4.2.1\nImproving Single Candidate\nSCULPT (Kumar et al., 2024) introduces a system-\natic method for tuning long, unstructured prompts\nby employing a hierarchical tree structure and\ntwo-step feedback loops - preliminary assessment\nand error assessment - to evaluate and correct\nprompts before and after execution.\nThe feed-\nback updates the hierarchical prompt tree which is\nthen back-synthesized into a new prompt candidate.\nPACE (Dong et al., 2024b) applies an actor-critic\nediting framework to the prompt refinement pro-\ncess itself, allowing for more dynamic and adaptive\nadjustments. Overcoming the limitations of opti-\nmizing a single metric, CRISPO (He et al., 2025)\nadopts a multi-aspect critique-suggestion meta-\nprompt to highlight flaws in the generated response\nacross multiple dimensions such as style, preci-\nsion, and content alignment. Thereafter it leverages\ndetailed, aspect-specific feedback and iteratively\nupdates the prompts. Autohint (Sun et al., 2023)\n\nPaper\nSeed instructions\nIteration depth\nInference evaluation\nCandidate generation\nSearch+filter strategy\nProTeGi\n(Pryzant\net al., 2023)\nManually created\nFixed\nLLM feedback +\nTask accuracy\nLLM rewriter\nUCB for trees\nAPE (Zhou et al.,\n2022)\nInstruction induction\nFixed\nTask accuracy\nN/A\nUCB\nCRISPO (He et al.,\n2025)\nManually created\nFixed\nLLM feedback +\nTask accuracy\nLLM rewriter\nTopK selection\nMOP (Wang et al.,\n2025)\nInstruction induction\nFixed\nTask accuracy\nMixture of experts\nRegion-based\njoint search\nDSPY\n(Khattab\net al., 2024)\nManually created +\nInstruction induction\nVariable\nLLM feedback +\nTask accuracy\nProgram Synthesis\nTopK selection\nOPRO (Yang et al.,\n2024a)\nManually created\nVariable\nLLM feedback +\nTask accuracy\nMetaprompt design\nTopK selection\nGATE (Joko et al.,\n2024)\nManually created\nVariable\nHuman feedback\nLLM rewriter\nN/A\nTable 1: Comparison of some APO techniques under our framework (Tables 2,3,4 show full comparison)\nsummarizes feedback for multiple incorrect infer-\nences via hints to instill improvements into a single\nprompt candidate.\n4.2.2\nImproving Multiple Candidates\nProTeGi (Pryzant et al., 2023) and TextGrad (Yuk-\nsekgonul et al., 2024) leverage textual “gradients”\nto guide the discrete prompt optimization proce-\ndure, very similar to the gradient-descent style of\ncontinuous prompt optimization approaches. Dif-\nferent from continuous gradient-descent, ProTeGi\nsampled multiple “gradients” i.e. directions of\nimprovement, and each such “gradient” is used\nto generate several prompt candidates for evalu-\nation in the next iteration. PromptAgent (Wang\net al., 2024a) similarly used an error collection ap-\nproach to emulate expert-written prompts that con-\nsisted of clear sections like “Task description”, “Do-\nmain Knowledge”, “Solution Guidance”, “Excep-\ntion Handling”, “Output Formatting”.\nPREFER\n(Zhang et al., 2024a) utilizes a feedback-reflect-\nrefine cycle to aggregate feedback into multiple\nprompts in an ensemble to improve the model’s\nability to generalize across various tasks.\nSur-\nvival of the Safest (SOS) (Sinha et al., 2024) added\nsafety-score into a multi-objective prompt opti-\nmization framework that used an interleaved strat-\negy to balance performance and security in LLMs\nsimultaneously. To avoid accidentally damaging\nwell-functioning prompts, StraGo (Wu et al., 2024)\nsummarized strategic guidance based on both cor-\nrect and incorrect predictions as feedback.\n4.3\nHuman-feedback\nA few works also incorporate human feedback, ei-\nther during compile-time or inference-time in the\nprompt construction / optimization process. Joko\net al. (2024) proposed “Generative Active Task\nElicitation” to better capture human preferences.\nIt prompts a language model to interactively ask\nquestions and infer human preferences conditioned\non the history of free-form interaction. Cheng et al.\n(2024) trained a smaller LLM to optimize input\nprompts based on user preference feedback, achiev-\ning up to 22% increase in win rates for ChatGPT\nand 10% for GPT-4. PROMST (Chen et al., 2024)\ntackles the challenges of multi-step tasks by in-\ncorporating human-designed feedback rules and a\nlearned heuristic model. APOHF (Lin et al., 2024)\nfocuses on optimizing prompts using only human\npreference feedback rather than numeric scores,\nemploying a dueling bandits-inspired strategy to\nefficiently select prompt pairs for preference feed-\nback, proving effective for tasks like text-to-image\ngeneration and response optimization.\n5\nCandidate Prompt Generation\nIn this step, one or more candidate prompts are gen-\nerated that are most likely to result in an improve-\nment in a metric of interest f ∈F. The approaches\nreviewed below range from simple rule-based ed-\nits (sec. 5.1) to sophisticated agentic systems that\ncombine with LLM-based evaluations (sec. 4.2)\nand various filtering strategies (sec. 6).\n5.1\nHeuristic-based Edits\nSeveral works proposed heuristic-based mecha-\nnisms to make edits to intermediate prompt can-\ndidates to generate newer candidates. They range\nfrom edits at the word / phrase / sentence-level\n(either simple rule-based or LLM-generated), or\nmetric-driven incremental search.\nWhile these\nstrategies may not result in the most optimal so-\nlution, they help in making the discrete prompt\noptimization problem computationally tractable.\n\n5.1.1\nMonte Carlo Sampling\nProTeGi (Pryzant et al., 2023) uses Monte carlo\nsampling to explore combinatorial discrete solution\nspaces in an incremental fashion - it samples multi-\nple textual gradients to use to generate prospective\ncandidates, and spawns paraphrases as monte-carlo\nsuccessors for evaluation. PromptAgent (Wang\net al., 2024a) uses a tree-variant called Monte Carlo\nTree Search (MCTS) which consists of 4 steps —\nSelection, Expansion, Simulation, and Backpropa-\ngation (also explained in Sec. 6).\n5.1.2\nGenetic Algorithm\nA significant line of work applies the well-studied\ngenetic algorithms to make discrete edits to texts.\nThe common recipe for several genetic algorithms\nis 1/ Mutate and 2/ Cross-over components from\npromising candidates. Token mutations: SPRIG\n(Zhang et al., 2024b) and CLAPS perform token-\nlevel mutations. SPRIG uses a starting corpus of\n300 components grouped into categories like COT,\nroles, styles, emotions, scenarios, and good prop-\nerties. It performs add/rephrase/swap/delete, high-\nlighting complementary strengths of optimizing\nsystem prompts alongside task-prompts (via meth-\nods like ProTeGi) to enhance accuracy across mul-\ntiple diverse domains, languages, and tasks without\nneeding repeated task-specific optimizations.\nLLM-based mutation: LMEA (Liu et al., 2023),\nSOS (Sinha et al., 2024), and StraGo (Wu et al.,\n2024) uses mutation prompts with LLMs to over-\ncome the traditional complexity of designing tai-\nlored operators for cross-over / mutation. Prompt-\nBreeder (Fernando et al., 2023) advocates self-\nreferential improvement of all prompts in the\nprompt optimization system - Direct Mutation of\ntask prompts, Hypermutation of mutation prompts\nthemselves, Lamarckian Mutation where prompts\nare reverse-engineered from successful examples\n(similar to Instruction Induction Honovich et al.\n(2023), and finally Crossover and Shuffling to im-\nprove diversity of the prompt pool. EvoPrompt\n(Guo et al., 2024) use Differential Evolution -\nwhere differences between existing prompts is in-\ncorporated to form new prompt candidates to over-\ncome the problem of local optima. AELP (Hsieh\net al., 2024) also uses mutation operators to per-\nform sentence-level edits in an iterative fashion.\nThey include sentence-level histories of reward\n{(st−1, st, rt)} in the mutation prompt in order\nto avoid local optima and accidentally returning\nto sub-optimal versions. GPS (Xu et al., 2022)\nused Back-translation, Sentence Continuation, and\nCloze transformations to perform prompt mutation.\nPromptWizard (Agarwal et al., 2024) proposed a\npipeline combining several steps including itera-\ntive improvement, few shot example synthesis and\nselection, utilizing LLM’s reasoning capability to\nimprove and validate the prompt, and finally an\nexpert persona to ensure consistency of the style of\ngenerated prompts.\n5.1.3\nWord / Phrase Level Edits\nSeveral word-edit approaches first identify \"influ-\nential\" tokens in the prompts. COPLE (Zhan et al.,\n2024) argued that LLMs exhibit lexical sensitivity,\nshowing that merely replacing a few words with\ntheir synonyms can yield significant improvements.\nFirst, “influential” tokens are identified where ex-\npected loss on dev-set EDval[L(y, ˆy)] drops the\nmost after removing that token versus the original\nprompt, and then influential tokens are replaced\nusing predictions from a Masked-Language Mod-\nels. This token-replacement approach is also at-\ntractive as a standalone post-processing step for\nlong prompts that are already optimized using other\nLLM-based approaches.\nGRIPS (Prasad et al.,\n2023) argues that phrase level edition is an effec-\ntive and interpretable method to optimize prompts,\nleveraging 4 basic edit operations -add, delete, para-\nphrase, and swap\n5.1.4\nVocabulary Pruning\nSome works prune the vocabulary space V to\nVpruned for decoding the next token for the op-\ntimized prompt ρ∗. CLAPS (Zhou et al., 2023)\nargued that general search spaces are highly re-\ndundant and use K-means clustering to find word-\nclusters and retain top-2000 words closest to cluster\ncentroids. BDPL (Diao et al., 2022) used pairwise\nmutual information (PMI) to retain top co-occuring\nngrams for decoding. PIN (Choi et al., 2024) in-\nstead added regularization in the form of Tsallis-\nentropy (ideal for heavy-tailed distributions like\nnatural language) for the RL training of a prompt\ngeneration network, to reduce the probability mass\nfor unlikely tokens and improve interpetability.\n5.2\nEditing via Auxiliary Trained NN\nSome approaches leverage a trained auxiliary neu-\nral network to edit the initial prompt for ob-\ntaining desired improvements.\nWe include ap-\nproaches where the finetuned network is different\n\nand smaller than the task network.\n5.2.1\nReinforcement-learning\nMulti-objective Optimization techniques (Jafari\net al., 2024) demonstrate superiority over simple\nreward averaging, particularly through volume-\nbased methods that effectively balance competing\nobjectives. Dynamic prompt modification strate-\ngies, introduced through prompt rewriting (Kong\net al., 2024), directional stimulus prompting (Li\net al., 2023d) and test-time editing (Zhang et al.,\n2022) solve the important goal of moving beyond\nstatic prompt generation. Prompt-OIRL (Sun et al.,\n2024a) also tackled test-time optimization objec-\ntive by learning an offline reward model and\nsubsequently using a best-of-N strategy to recom-\nmend the optimal prompt in a query-dependent\nfashion. BDPL (Diao et al., 2022) optimized dis-\ncrete prompts using variance-reduced policy gradi-\nent algorithm to estimate gradients, allowing user\ndevices to fine-tune tasks with limited API calls.\n5.2.2\nFinetuning LLMs\nBPO (Cheng et al., 2024) trains a smaller 7B model\nto align itself to task-performance on individual\nLLMs using reward-free alignment.\nFIPO (Lu\net al., 2025) trains a local model (7B - 13B) to\nperform prompt optimizations to preserve privacy\nand adapt to target models better leveraging both\ndata diversification and strategic fine-tuning such\nas SFT, preference optimization, and iterative pref-\nerence learning.\n5.2.3\nGenerative Adversarial Networks\nLong et al. (2024) framed the prompt optimization\nprocess in the GAN setting. The LLM generator\ntakes question and the generation prompt to pro-\nduce output. The (input, output) pairs are evaluated\nby an LLM powered discriminator, whose goal is\nto identify generated pairs from ground truth pairs.\nBoth generator and the discriminator are jointly op-\ntimized using adversarial loss, by utilizing a prompt\nmodifier LLM to rewrite their prompts.\n5.3\nMetaprompt Design\nPE2 (Ye et al., 2024) argued that previous works\nunder-explored meta-prompt search space. OPRO\n(Yang et al., 2024a) proposes a meta-prompt design\n(see Appendix 14.2) which includes the optimiza-\ntion problem description in natural language and\npreviously generated solutions (multiple solutions\nper stage for diversity) and scores alongisde the\nmeta-instruction for prompt refinement. DAPO\n(Yang et al., 2024c) utilizes a well-designed meta-\ninstruction to guide the LLM in generating high-\nquality and structured initial prompts (contain task-\nspecific info, e.g. task type and description, output\nformat and constraints, reasoning process, profes-\nsional tips) by observing given input-output ex-\nemplars. Then, DAPO iteratively optimizes the\nprompts at the sentence level, leveraging previous\ntuning experience to expand prompt candidates.\n5.4\nCoverage-based\nSome approaches seek to \"cover\" the entire prob-\nlem space - either within a single prompt, or using\nmultiple prompts working individually or in an en-\nsemble during inference.\n5.4.1\nSingle Prompt-expansion\nAMPO (Yang et al., 2024d) uses LLM feedback\nto enumerate all the failure cases based on the\nevaluation-set Dval and then enlists each of them in\nthe meta-instruction in an if-then-else format using\n3 modules - 1/ Pattern Recognition, 2/ Branch Ad-\njustment, and 3/ Branch Pruning to decide whether\nto enhance existing branches, or to grow new\nbranches. Similarly, UNIPROMPT focused on ex-\nplicitly ensuring that various semantic facets of a\ntask get represented in the final prompt. It designs a\nhuman-like (manual) prompt engineering approach\n(UniPrompt) with two stages: a) task facets ini-\ntialization using background knowledge, and b)\nrefinement using examples.\n5.4.2\nMixture of Experts\nWang et al. (2025) introduced the Mixture-of-\nExpert-Prompts where each expert is a task-prompt\nto be used for specialized inference. MOP first\nclusters all demonstrations using K-means cluster-\ning. Then, the Region-based Joint Search (RBJS)\n(sec.6.3) algorithm generates the appropriate in-\nstruction for each exemplar-cluster via instruction\ninduction (sec.3.2) based on a mix of in-cluster\nand out-of-cluster demonstrations to cover “blind-\nspots”. During inference, a single expert prompt is\ninvoked whose cluster centroid µc is closest to the\ninstance-embedding arg minC ||ϕ(xi) −µc||2.\n5.4.3\nEnsemble Methods\nPromptBoosting (Hou et al., 2023), Boosted-\nPrompting (Pitis et al., 2023), PREFER (Zhang\net al., 2024a), etc. are ensemble methods that in-\nvoke multiple prompts during inference and com-\n\nbine them to generate the final output ˆy = y0 +\nΣmβiyi. GPO (Li et al., 2023c) also uses labeled\nsource data to generate an ensemble of prompts,\nwhich are applied to unlabeled target data to gener-\nate output through majority voting.\n5.5\nProgram Synthesis\nProgram-synthesis based approaches transform\nLLM pipelines into structured, modular compo-\nnents that can be systematically optimized and\ncomposed. These optimization techniques itera-\ntively refine instructions and demonstrations for\neach module to improve the entire pipeline’s per-\nformance, DSP (Khattab et al., 2022) introduces\na three-stage framework for retrieval-augmented\ninference: Demonstrate (generates task-specific\ndemonstrations), Search (retrieves relevant infor-\nmation), and Predict (combines retrieved info with\ndemonstrations).\nDSPY (Khattab et al., 2024)\ntransforms LLM pipelines into text transformation\ngraphs - introducing parameterized models, learn-\ning through demonstrations, and a compiler that op-\ntimizes pipelines. DLN (Sordoni et al., 2023) simi-\nlarly considers chained LLM calls as stacked deep\nlanguage networks performing variational infer-\nence, where the learnable parameters for each layer\nare task-decomposed prompt templates. MIPRO\n(Opsahl-Ong et al., 2024) automates the optimiza-\ntion of multi-stage language model programs by\nimproving instructions and demonstrations for each\nmodule. SAMMO (Schnabel and Neville, 2024)\nproposed symbolic prompt programming, repre-\nsenting prompts as directed-acyclic-graphs (DAG).\nA set of user-defined node mutation rules guide the\nmutation-search to find the optimal DAG, which is\nthen converted back to a prompt.\n6\nFilter and Retain Promising Prompts\nIn this step, promising prompt candidates are fil-\ntered for further optimization.\n6.1\nTopK Greedy Search\nThe simplest mechanism to iteratively search\nthrough prompt candidate sets is a greedy topK\nsearch where in each iteration of the optimiza-\ntion, the top-K best-performing candidates on mini-\nbatch of data instances Dval are retained for further\niterations (e.g. - ProTeGi, AELP. This differs from\nbeam-search which judges partial solutions’ based\non the reward for the entire trajectory of prompt\nedits r({ρ1\n1, ρ1\n2, . . . , ρ1\nt }).\n6.2\nUpper Confidence Bound and Variants\nRelying on a single static evaluation dataset can\nlead to biases in the selection procedure and finally\nsuboptimal solutions. ProTeGi, SPRIG, inter alia,\ncast the candidate prompt selection problem as that\nof bandit search - identifying the most suitable\narm (prompt candidate) operating on a fixed com-\nputation budget. They use the Upper Confidence\nBounds (UCB, Algorithm 2) which balances explo-\nration with exploitation. In each iteration of prompt\noptimization, they sample a different evaluation\ndataset Dsample ∈Dval, and maintain a moving\nestimate of the optimality of each arm (i.e. prompt).\nIn each iteration, the playout filters top-B prompt\ncandidates with the greatest score for further ex-\nploration. PromptAgent uses a variation of UCB\ncalled UCB for Trees (UCT) which are used in the\nsetting of contextual bandits (i.e. the action-space\nand the reward function is state-dependent). AELP\n(Hsieh et al., 2024) used a modification called Lin-\near UCB (Li et al., 2010) which uses a closed form\nlinear estimate based on the reward trajectories of\npreviously sampled edits as well as prompt embed-\nding ϕ(s) to select the next best-arm.\n6.3\nRegion-based Joint Search\nMOP (Wang et al., 2025) proposes a Mixture-\nof-Expert-Prompts performing prompt optimiza-\ntion for each expert individual. Once C exemplar-\nclusters are identified, the RBJS search first sam-\nples examples Dexemplars ∈DC ∪D \\ DC, and\nthen uses APE to induct and optimize each expert\ninstruction.\n6.4\nMetaheuristic Ensemble\nPLUM (Pan et al., 2024) library offered a meta-\nheuristic ensemble of different search algorithms\nlike Hill climbing, Simulated Annealing, Genetic\nAlgorithms, Tabu Search, and Harmony Search.\n7\nIteration Depth\n7.1\nFixed Steps\nMost approaches choose to carry out the prompt\noptimization for a fixed number of steps N.\n7.2\nVariable number of steps\nGRIPS (Prasad et al., 2023) concludes search when\nsuccessive iterations with negative gains breach\na patience parameter, whereas PromptAgent con-\ncluded APO when rt ≤ϵmin ∨rt ≥ϵmax.\n\n8\nTheoretical Perspectives\n8.1\nUpper Bound of Improvement from APO\nAlignPro (Trivedi et al., 2025) establishes an upper\nbound on the gains realizable from discrete prompt\noptimization under a given prompt optimizer and\nalso a suboptimality-gap w.r.t. RLHF-optimal pol-\nicy π∗, while a lower bound is left unexplored.\n8.2\nOther Related Perspectives\nBhargava et al. (2024) proposed a control theo-\nretic framework to establish bounds on the set of\nreachable LLM-outputs for self-attention in terms\nof the singular values of its weight matrices. Liu\net al. (2024c) showed the existence of a strong\ntransformer that can approximate any sequence-to-\nsequence Lipschitz function. They also showed the\nexistence of “difficult” datasets that depth-limited\ntransformers could not commit to memory.\n9\nChallenges and Future Directions\n9.1\nTask-agnostic APO\nAll the surveyed APO methods assume that the task\ntype T is known beforehand; additionally offline\nAPO methods also require an evaluation set Dval,\nsomething not explicitly available in production\nsettings. Barring a few tasks covered by Joko et al.\n(2024); Sun et al. (2024a); Zhang et al. (2022);\nChoi et al. (2024), inference-time optimization of\nmultiple unknown tasks is underexplored. More\nrobust evaluations are needed for task-agnostic\nAPO systems combining seen and unseen tasks.\n9.2\nUnclear Mechanisms\nMelamed et al. (2024) showed that prompts have\nso-called ’evil twins’ that are uninterpretable yet\nrecover some of the performance of gold-standard\nprompts. Lu et al. (2024) showed that rare gib-\nberish strings can serve as competitive delimiters\nτ in prompts. Yang et al. (2024b) showed that\nself-reflection by LLMs can suffer from incorrect\nerror identification, prior biases, semantic invalid-\nity, leading to failure in yielding improved prompts.\nMore studies are needed to better uncover the mech-\nanisms of prompt optimization.\n9.3\nAPO for System Prompts / Agents\nAlthough SPRIG explored optimizing system\nprompts in chat-style settings, scalability remains\na challenge - optimizing system prompts required\na predefined corpus and close to 60 hours whereas\nProtegi only needed ˜10 minutes per task. Similarly,\noptimizing prompts for several components in an\nagentic system in a concurrent fashion poses an\nexciting direction for future research.\n9.4\nMultimodal APO\nRecently, textual prompt optimization has ex-\npanded to multimodal domains: text-to-image (Liu\net al., 2024b; Mañas et al., 2024; Liu et al., 2024d),\ntext-to-video (Ji et al., 2024), text-to-audio (Huang\net al., 2023), and text-image alignment models like\nCLIP (Du et al., 2024; Mirza et al., 2024). Be-\nyond textual prompts, Huang et al. (2023) explore\noptimizing multimodal inputs, such as images, to\nelicit better responses from large multimodal mod-\nels. However, the interplay between modalities in\nprompt optimization remains underexplored. Fu-\nture research could develop APO frameworks to\njointly optimize multimodal prompts (eg - remove\nbackground noise from audio, add visual markers\nto videos, etc.) to fully leverage their synergies.\n10\nConclusion\nIn this paper, we provide a comprehensive fine-\ngrained review of existing APO techniques and\nidentified key areas for future growth. It is our aim\nto spur future research spawning from our survey.\n11\nLimitations\nWhile we attempted to cover all qualifying papers,\nit is possible that we may have unintentionally\nmissed out on some relevant papers. We also men-\ntion some of the papers that were excluded in this\nsurvey with specific reasons in section 12.2. Also,\nwe realize that fitting varied research works into a\nsingle unifying framework might risk broad catego-\nrizations for some papers, or skipping some char-\nacteristics for others (e.g. Tempera (Zhang et al.,\n2022) consists of both RL-based and word/phrase-\nlevel editing techniques, applied to both instruc-\ntions and exemplars). In such cases, we categorize\na paper based on its most salient features. Another\nchallenge is that when presenting a survey paper\nunder 8 pages, we had to make tradeoffs and only\nretain content in the main body that was deemed\nmost necessary. This resulted in having to relegate\na core contribution (Tables 2,3,4) which contained\na rigorous comparison of all the surveyed papers\ninto the appendix. We have attempted our best\nto strike the right balance between specificity and\nbrevity to present a novel framework. We also pro-\nvide copious references to interested researchers\nfor further reading.\n\nReferences\nEshaan Agarwal, Joykirat Singh, Vivek Dani, Raghav\nMagazine, Tanuja Ganu, and Akshay Nambi. 2024.\nPromptwizard: Task-aware prompt optimization frame-\nwork.\nFernando Alva-Manchego, Louis Martin, Antoine Bor-\ndes, Carolina Scarton, Benoît Sagot, and Lucia Specia.\n2020. Asset: A dataset for tuning and evaluation of\nsentence simplification models with multiple rewriting\ntransformations. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguis-\ntics, pages 4668–4679.\nAfra Amini, Tim Vieira, and Ryan Cotterell. 2024. Di-\nrect preference optimization with an offset. In Findings\nof the Association for Computational Linguistics: ACL\n2024, pages 9954–9972, Bangkok, Thailand. Associa-\ntion for Computational Linguistics.\nR. Anantha, Svitlana Vakulenko, Zhucheng Tu, S. Long-\npre, Stephen G. Pulman, and Srinivas Chappidi. 2020.\nOpen-domain question answering goes conversational\nvia question rewriting. In North American Chapter of\nthe Association for Computational Linguistics.\nJacob Andreas,\nJohannes Bufe,\nDavid Burkett,\nCharles C. Chen, Joshua Clausman, Jean Crawford,\nKate Crim, Jordan DeLoach, Leah Dorner, Jason Eis-\nner, Hao Fang, Alan Guo, David Leo Wright Hall,\nKristin Delia Hayes, Kellie Hill, Diana Ho, Wendy\nIwaszuk, Smriti Jha, Dan Klein, Jayant Krishnamurthy,\nTheo Lanman, Percy Liang, C. H. Lin, Ilya Lintsbakh,\nAndy McGovern, Aleksandr Nisnevich, Adam Pauls,\nDmitrij Petters, Brent Read, Dan Roth, Subhro Roy,\nJesse Rusak, Beth Ann Short, Div Slomin, B Snyder,\nStephon Striplin, Yu Su, Zachary Tellman, Sam Thom-\nson, A. A. Vorobev, Izabela Witoszko, Jason Wolfe,\nA. G. Wray, Yuchen Zhang, and Alexander Zotov. 2020.\nTask-oriented dialogue as dataflow synthesis. Transac-\ntions of the Association for Computational Linguistics,\n8:556–571.\nTrapit Bansal, Rishikesh Jha, and Andrew McCallum.\n2019. Learning to few-shot learn across diverse natural\nlanguage classification tasks. In International Confer-\nence on Computational Linguistics.\nAman Bhargava, Cameron Witkowski, Shi-Zhuo Looi,\nand Matt Thomson. 2024. What’s the magic word? a\ncontrol theory of llm prompting.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng\nGao, and Yejin Choi. 2019. Piqa: Reasoning about\nphysical commonsense in natural language. In AAAI\nConference on Artificial Intelligence.\nSamuel R Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D Manning. 2015. A large annotated\ncorpus for learning natural language inference. arXiv\npreprint arXiv:1508.05326.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen\nKrueger, Tom Henighan, Rewon Child, Aditya Ramesh,\nDaniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christo-\npher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\nScott Gray, Benjamin Chess, Jack Clark, Christopher\nBerner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei. 2020. Language models are few-\nshot learners.\nPaweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang\nTseng, Iñigo Casanueva, Stefan Ultes, Osman Ramadan,\nand Milica Gasic. 2018. Multiwoz-a large-scale multi-\ndomain wizard-of-oz dataset for task-oriented dialogue\nmodelling. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 5016–5026.\nDaniel Matthew Cer, Mona T. Diab, Eneko Agirre, Iñigo\nLopez-Gazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation. In International Work-\nshop on Semantic Evaluation.\nMauro Cettolo, Marcello Federico, Luisa Bentivogli,\nNiehues Jan, Stüker Sebastian, Sudoh Katsuitho,\nYoshino Koichiro, and Federmann Christian. 2017.\nOverview of the iwslt 2017 evaluation campaign. In In-\nternational Workshop on Spoken Language Translation.\nYongchao Chen, Jacob Arkin, Yilun Hao, Yang Zhang,\nNicholas Roy, and Chuchu Fan. 2024. PRompt opti-\nmization in multi-step tasks (PROMST): Integrating\nhuman feedback and heuristic-based sampling. In Pro-\nceedings of the 2024 Conference on Empirical Meth-\nods in Natural Language Processing, pages 3859–3920,\nMiami, Florida, USA. Association for Computational\nLinguistics.\nJiale Cheng, Xiao Liu, Kehan Zheng, Pei Ke, Hongn-\ning Wang, Yuxiao Dong, Jie Tang, and Minlie Huang.\n2024. Black-box prompt optimization: Aligning large\nlanguage models without model training. In Proceed-\nings of the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 3201–3219, Bangkok, Thailand. Association for\nComputational Linguistics.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E Gonzalez, et al.\n2023. Vicuna: An open-source chatbot impressing gpt-\n4 with 90%* chatgpt quality. See https://vicuna. lmsys.\norg (accessed 14 April 2023), 2(3):6.\nMinje Choi, Jiaxin Pei, Sagar Kumar, Chang Shu, and\nDavid Jurgens. 2023. Do llms understand social knowl-\nedge? evaluating the sociability of large language mod-\nels with socket benchmark. In Proceedings of the 2023\nConference on Empirical Methods in Natural Language\nProcessing, pages 11370–11403.\nYunseon Choi, Sangmin Bae, Seonghyun Ban, Min-\nchan Jeong, Chuheng Zhang, Lei Song, Li Zhao, Jiang\nBian, and Kee-Eung Kim. 2024. Hard prompts made\ninterpretable: Sparse entropy regularization for prompt\ntuning with rl.\n\nChristopher Cieri, Mark Liberman, Sunghye Cho,\nStephanie Strassel, James Fiumara, and Jonathan\nWright. 2022. Reflections on 30 years of language re-\nsource development and sharing. In Proceedings of the\nThirteenth Language Resources and Evaluation Con-\nference, pages 543–550, Marseille, France. European\nLanguage Resources Association.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018.\nThink you have solved question an-\nswering? try arc, the ai2 reasoning challenge. ArXiv,\nabs/1803.05457.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plap-\npert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al.\n2021. Training verifiers to solve math word problems.\narXiv preprint arXiv:2110.14168.\nMike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie,\nJun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei\nZaharia, and Reynold Xin. 2023. Free dolly: Introduc-\ning the world’s first truly open instruction-tuned llm.\nCompany Blog of Databricks.\nLeyang Cui, Yu Wu, Shujie Liu, Yue Zhang, and Ming\nZhou. 2020. Mutual: A dataset for multi-turn dialogue\nreasoning. ArXiv, abs/2004.04494.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The pascal recognising textual entailment chal-\nlenge. In Machine Learning Challenges Workshop.\nMarie-Catherine de Marneffe, Mandy Simons, and Ju-\ndith Tonhauser. 2019. The commitmentbank: Investi-\ngating projection in naturally occurring discourse.\nMingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan\nWang, Han Guo, Tianmin Shu, Meng Song, Eric P. Xing,\nand Zhiting Hu. 2022. Rlprompt: Optimizing discrete\ntext prompts with reinforcement learning.\nFranck Dernoncourt and Ji Young Lee. 2017. Pubmed\n200k rct: a dataset for sequential sentence classification\nin medical abstracts. In International Joint Conference\non Natural Language Processing.\nRobert C. Detrano, András Jánosi, Walter Steinbrunn,\nMatthias Emil Pfisterer, Johann-Jakob Schmid, Sarbjit\nSandhu, Kern Guppy, Stella Lee, and Victor Froelicher.\n1989. International application of a new probability\nalgorithm for the diagnosis of coronary artery disease.\nThe American journal of cardiology, 64 5:304–10.\nShizhe Diao, Zhichao Huang, Ruijia Xu, Xuechun Li,\nYong Lin, Xiao Zhou, and Tong Zhang. 2022. Black-\nbox prompt learning for pre-trained language models.\narXiv preprint arXiv:2201.08531.\nRezarta Islamaj Do˘gan, Robert Leaman, and Zhiyong\nLu. 2014. Ncbi disease corpus: a resource for disease\nname recognition and concept normalization. Journal\nof biomedical informatics, 47:1–10.\nWilliam B. Dolan and Chris Brockett. 2005. Automat-\nically constructing a corpus of sentential paraphrases.\nIn International Joint Conference on Natural Language\nProcessing.\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan\nMa, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu,\nBaobao Chang, Xu Sun, Lei Li, and Zhifang Sui. 2024a.\nA survey on in-context learning. In Proceedings of the\n2024 Conference on Empirical Methods in Natural Lan-\nguage Processing, pages 1107–1128, Miami, Florida,\nUSA. Association for Computational Linguistics.\nYihong Dong, Kangcheng Luo, Xue Jiang, Zhi Jin, and\nGe Li. 2024b. PACE: Improving prompt with actor-\ncritic editing for large language model. In Findings of\nthe Association for Computational Linguistics: ACL\n2024, pages 7304–7323, Bangkok, Thailand. Associa-\ntion for Computational Linguistics.\nYingjun Du, Wenfang Sun, and Cees GM Snoek.\n2024. Ipo: Interpretable prompt optimization for vision-\nlanguage models. arXiv preprint arXiv:2410.15397.\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel\nStanovsky, Sameer Singh, and Matt Gardner. 2019.\nDrop: A reading comprehension benchmark requiring\ndiscrete reasoning over paragraphs. In North American\nChapter of the Association for Computational Linguis-\ntics.\nStefan Daniel Dumitrescu, Petru Rebeja, Beáta L˝orincz,\nMihaela G˘aman, Mihai Daniel Ilie, Andrei Pruteanu,\nAdriana Stan, Luciana Morogan, Traian Rebedea, and\nSebastian Ruder. 2021. Liro: Benchmark and leader-\nboard for romanian language tasks.\nIn NeurIPS\nDatasets and Benchmarks.\nIbrahim Abu Farha and Walid Magdy. 2020a. From\narabic sentiment analysis to sarcasm detection: The\narsarcasm dataset. In OSACT.\nIbrahim Abu Farha and Walid Magdy. 2020b. From\narabic sentiment analysis to sarcasm detection: The\narsarcasm dataset. In OSACT.\nChrisantha\nFernando,\nDylan\nBanarse,\nHenryk\nMichalewski,\nSimon Osindero,\nand Tim Rock-\ntäschel. 2023.\nPromptbreeder:\nSelf-referential\nself-improvement via prompt evolution.\nArXiv,\nabs/2309.16797.\nRory A. Fisher. 1936. The use of multiple measure-\nments in taxonomic problems. Annals of Human Genet-\nics, 7:179–188.\nNoa Garcia, Chentao Ye, Zihua Liu, Qingtao Hu, Mayu\nOtani, Chenhui Chu, Yuta Nakashima, and Teruko Mita-\nmura. 2020. A dataset and baselines for visual question\nanswering on art. In European Conference on Computer\nVision, pages 92–108.\nMiguel Garc’ia-Orteg’on, Gregor N. C. Simm, Austin\nTripp, José Miguel Hernández-Lobato, Andreas Ben-\nder, and Sergio Bacallado. 2021. Dockstring: Easy\nmolecular docking yields better benchmarks for ligand\n\ndesign. Journal of Chemical Information and Modeling,\n62:3486 – 3502.\nClaire Gardent, Anastasia Shimorina, Shashi Narayan,\nand Laura Perez-Beltrachini. 2017. Creating training\ncorpora for nlg micro-planners. In Annual Meeting of\nthe Association for Computational Linguistics.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021. Did aristotle use\na laptop? a question answering benchmark with implicit\nreasoning strategies. Transactions of the Association\nfor Computational Linguistics, 9:346–361.\nBogdan Gliwa, Iwona Mochol, Maciej Biesek, and\nAleksander Wawer. 2019. Samsum corpus: A human-\nannotated dialogue dataset for abstractive summariza-\ntion. In Proceedings of the 2nd Workshop on New Fron-\ntiers in Summarization, pages 70–79.\nChulaka Gunasekara, Jonathan K. Kummerfeld, Lazaros\nPolymenakos, and Walter S. Lasecki. 2019. Dstc7 task\n1: Noetic end-to-end response selection. Proceedings\nof the First Workshop on NLP for Conversational AI.\nQingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao\nSong, Xu Tan, Guoqing Liu, Jiang Bian, and Yujiu Yang.\n2024. Connecting large language models with evolu-\ntionary algorithms yields powerful prompt optimizers.\nIn The Twelfth International Conference on Learning\nRepresentations.\nHan He, Qianchu Liu, Lei Xu, Chaitanya Shivade,\nYi Zhang, Sundararajan Srinivasan, and Katrin Kirch-\nhoff. 2025. Crispo: Multi-aspect critique-suggestion-\nguided automatic prompt optimization for text genera-\ntion.\nDan Hendrycks, Collin Burns, Steven Basart, Andy\nZou, Mantas Mazeika, Dawn Xiaodong Song, and Ja-\ncob Steinhardt. 2020. Measuring massive multitask\nlanguage understanding. ArXiv, abs/2009.03300.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and Jacob\nSteinhardt. Measuring mathematical problem solving\nwith the math dataset. In Thirty-fifth Conference on\nNeural Information Processing Systems Datasets and\nBenchmarks Track (Round 2).\nOr Honovich, Uri Shaham, Samuel R. Bowman, and\nOmer Levy. 2022. Instruction induction: From few\nexamples to natural language task descriptions. ArXiv,\nabs/2205.10782.\nOr Honovich, Uri Shaham, Samuel R. Bowman, and\nOmer Levy. 2023. Instruction induction: From few\nexamples to natural language task descriptions. In Pro-\nceedings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers),\npages 1935–1952, Toronto, Canada. Association for\nComputational Linguistics.\nMohammad Javad Hosseini, Hannaneh Hajishirzi, Oren\nEtzioni, and Nate Kushman. 2014. Learning to solve\narithmetic word problems with verb categorization. In\nConference on Empirical Methods in Natural Language\nProcessing.\nBairu Hou, Joe O’Connor, Jacob Andreas, Shiyu Chang,\nand Yang Zhang. 2023. Promptboosting: black-box text\nclassification with ten forward passes. In Proceedings of\nthe 40th International Conference on Machine Learning,\nICML’23. JMLR.org.\nCho-Jui Hsieh, Si Si, Felix Yu, and Inderjit Dhillon.\n2024. Automatic engineering of long prompts. In Find-\nings of the Association for Computational Linguistics:\nACL 2024, page 10672—10685, Bangkok, Thailand.\nAssociation for Computational Linguistics.\nMinqing Hu and Bing Liu. 2004. Mining and sum-\nmarizing customer reviews. Proceedings of the tenth\nACM SIGKDD international conference on Knowledge\ndiscovery and data mining.\nLifu Huang, Ronan Le Bras, Chandra Bhagavatula, and\nYejin Choi. 2019. Cosmos qa: Machine reading com-\nprehension with contextual commonsense reasoning.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 2391–2401.\nRongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren,\nLuping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang\nYin, and Zhou Zhao. 2023. Make-an-audio: Text-to-\naudio generation with prompt-enhanced diffusion mod-\nels. In International Conference on Machine Learning,\npages 13916–13932. PMLR.\nYasaman Jafari, Dheeraj Mekala, Rose Yu, and Taylor\nBerg-Kirkpatrick. 2024. Morl-prompt: An empirical\nanalysis of multi-objective reinforcement learning for\ndiscrete prompt optimization.\nYatai Ji, Jiacheng Zhang, Jie Wu, Shilong Zhang,\nShoufa Chen, Chongjian GE, Peize Sun, Weifeng Chen,\nWenqi Shao, Xuefeng Xiao, et al. 2024. Prompt-a-video:\nPrompt your video diffusion model via preference-\naligned llm. arXiv preprint arXiv:2412.15156.\nYichen Jiang, Shikha Bordia, Zheng Zhong, Charles\nDognin, Maneesh Kumar Singh, and Mohit Bansal.\n2020. Hover: A dataset for many-hop fact extraction\nand claim verification. In Findings.\nCan Jin, Hongwu Peng, Shiyu Zhao, Zhenting Wang,\nWujiang Xu, Ligong Han, Jiahui Zhao, Kai Zhong,\nSanguthevar Rajasekaran, and Dimitris N. Metaxas.\n2024. Apeer: Automatic prompt engineering enhances\nlarge language model reranking.\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,\nHanyi Fang, and Peter Szolovits. 2020. What disease\ndoes this patient have? a large-scale open domain ques-\ntion answering dataset from medical exams. ArXiv,\nabs/2009.13081.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William\nCohen, and Xinghua Lu. 2019. Pubmedqa: A dataset\n\nfor biomedical research question answering. In Pro-\nceedings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Process-\ning (EMNLP-IJCNLP), pages 2567–2577.\nHideaki Joko, Shubham Chatterjee, Andrew Ramsay,\nArjen P De Vries, Jeff Dalton, and Faegheh Hasibi.\n2024. Doing personal laps: Llm-augmented dialogue\nconstruction for personalized multi-session conversa-\ntional search. In Proceedings of the 47th International\nACM SIGIR Conference on Research and Development\nin Information Retrieval, pages 796–806.\nGurusha Juneja, Nagarajan Natarajan, Hua Li, Jian Jiao,\nand Amit Sharma. 2024. Task facet learning: A struc-\ntured approach to prompt optimization. arXiv preprint\narXiv:2406.10504.\nDavid Jurgens, Srijan Kumar, Raine Hoover, Daniel A.\nMcFarland, and Dan Jurafsky. 2018. Measuring the\nevolution of a scientific field through citation frames.\nTransactions of the Association for Computational Lin-\nguistics, 6:391–406.\nOmar Khattab, Keshav Santhanam, Xiang Lisa Li,\nDavid Hall, Percy Liang, Christopher Potts, and Matei\nZaharia. 2022.\nDemonstrate-search-predict:\nCom-\nposing retrieval and language models for knowledge-\nintensive nlp. arXiv preprint arXiv:2212.14024.\nOmar Khattab, Arnav Singhvi, Paridhi Maheshwari,\nZhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan,\nSaiful Haq, Ashutosh Sharma, Thomas T. Joshi, Hanna\nMoazam, Heather Miller, Matei Zaharia, and Christo-\npher Potts. 2024. Dspy: Compiling declarative language\nmodel calls into self-improving pipelines.\nJohannes Kiesel, Maria Mestre, Rishabh Shukla, Em-\nmanuel Vincent, Payam Adineh, D. Corney, Benno\nStein, and Martin Potthast. 2019. Semeval-2019 task 4:\nHyperpartisan news detection. In International Work-\nshop on Semantic Evaluation.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2023. Large lan-\nguage models are zero-shot reasoners.\nRik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish\nSabharwal, Oren Etzioni, and Siena Dumas Ang. 2015.\nParsing algebraic word problems into equations. Trans-\nactions of the Association for Computational Linguis-\ntics, 3:585–597.\nWeize Kong, Spurthi Amba Hombaiah, Mingyang\nZhang, Qiaozhu Mei, and Michael Bendersky. 2024.\nPrewrite: Prompt rewriting with reinforcement learning.\nShanu Kumar, Akhila Yesantarao Venkata, Shubhanshu\nKhandelwal, Bishal Santra, Parag Agrawal, and Man-\nish Gupta. 2024. Sculpt: Systematic tuning of long\nprompts.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. 2019. Natural questions: a benchmark\nfor question answering research. Transactions of the\nAssociation for Computational Linguistics, 7:453–466.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and\nEduard Hovy. 2017. Race: Large-scale reading com-\nprehension dataset from examinations. arXiv preprint\narXiv:1704.04683.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised open do-\nmain question answering. ArXiv, abs/1906.00300.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt tun-\ning. In Proceedings of the 2021 Conference on Empir-\nical Methods in Natural Language Processing, pages\n3045–3059, Online and Punta Cana, Dominican Repub-\nlic. Association for Computational Linguistics.\nHector J. Levesque, Ernest Davis, and L. Morgenstern.\n2011. The winograd schema challenge. In AAAI Spring\nSymposium: Logical Formalizations of Commonsense\nReasoning.\nBei Li, Rui Wang, Junliang Guo, Kaitao Song, Xu Tan,\nHany Hassan, Arul Menezes, Tong Xiao, Jiang Bian,\nand JingBo Zhu. 2023a. Deliberate then generate: En-\nhanced prompting framework for text generation.\nCheng Li, Jindong Wang, Yixuan Zhang, Kaijie Zhu,\nWenxin Hou, Jianxun Lian, Fang Luo, Qiang Yang,\nand Xing Xie. 2023b. Large language models under-\nstand and can be enhanced by emotional stimuli. arXiv\npreprint arXiv:2307.11760.\nLihong Li, Wei Chu, John Langford, and Robert E.\nSchapire. 2010. A contextual-bandit approach to per-\nsonalized news article recommendation. In Proceedings\nof the 19th International Conference on World Wide\nWeb, WWW ’10, page 661–670, New York, NY, USA.\nAssociation for Computing Machinery.\nMoxin Li, Wenjie Wang, Fuli Feng, Yixin Cao, Jizhi\nZhang, and Tat-Seng Chua. 2023c. Robust prompt opti-\nmization for large language models against distribution\nshifts. In Proceedings of the 2023 Conference on Em-\npirical Methods in Natural Language Processing, pages\n1539–1554, Singapore. Association for Computational\nLinguistics.\nZekun Li, Baolin Peng, Pengcheng He, Michel Galley,\nJianfeng Gao, and Xifeng Yan. 2023d. Guiding large\nlanguage models via directional stimulus prompting.\narXiv preprint arXiv:2302.11520.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022.\nTruthfulqa: Measuring how models mimic human false-\nhoods. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (Volume\n1: Long Papers), pages 3214–3252.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár, and\nC Lawrence Zitnick. 2014. Microsoft coco: Common\n\nobjects in context. In Computer Vision–ECCV 2014:\n13th European Conference, Zurich, Switzerland, Septem-\nber 6-12, 2014, Proceedings, Part V 13, pages 740–755.\nSpringer.\nXiaoqiang Lin, Zhongxiang Dai, Arun Verma, See-\nKiong Ng, Patrick Jaillet, and Bryan Kian Hsiang Low.\n2024. Prompt optimization with human feedback.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-\nsom. 2017. Program induction by rationale generation:\nLearning to solve and explain algebraic word problems.\nIn Annual Meeting of the Association for Computational\nLinguistics.\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paran-\njape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. 2024a. Lost in the middle: How language mod-\nels use long contexts. Transactions of the Association\nfor Computational Linguistics, 12:157–173.\nShengcai Liu, Caishun Chen, Xinghua Qu, Ke Tang,\nand Yew Soon Ong. 2023. Large language models as\nevolutionary optimizers. 2024 IEEE Congress on Evo-\nlutionary Computation (CEC), pages 1–8.\nShihong Liu, Samuel Yu, Zhiqiu Lin, Deepak Pathak,\nand Deva Ramanan. 2024b. Language models as black-\nbox optimizers for vision-language models. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 12687–12697.\nXiaogeng Liu, Zhiyuan Yu, Yizhe Zhang, Ning Zhang,\nand Chaowei Xiao. 2024c. Automatic and universal\nprompt injection attacks against large language models.\nYilun Liu, Minggui He, Feiyu Yao, Yuhe Ji, Shimin Tao,\nJingzhou Du, Duan Li, Jian Gao, Li Zhang, Hao Yang,\net al. 2024d. What do you want? user-centric prompt\ngeneration for text-to-image synthesis via multi-turn\nguidance. arXiv preprint arXiv:2408.12910.\nXuan Do Long, Yiran Zhao, Hannah Brown, Yuxi Xie,\nJames Xu Zhao, Nancy F. Chen, Kenji Kawaguchi,\nMichael Shieh, and Junxian He. 2024. Prompt opti-\nmization via adversarial in-context learning. In Pro-\nceedings of the 62nd Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers),\npages 7308–7327, Bangkok, Thailand. Association for\nComputational Linguistics.\nRyan Lowe, Nissan Pow, Iulian Serban, and Joelle\nPineau. 2015. The ubuntu dialogue corpus: A large\ndataset for research in unstructured multi-turn dialogue\nsystems. In SIGDIAL Conference.\nJunru Lu, Siyu An, Min Zhang, Yulan He, Di Yin, and\nXing Sun. 2025. FIPO: Free-form instruction-oriented\nprompt optimization with preference dataset and mod-\nular fine-tuning schema. In Proceedings of the 31st\nInternational Conference on Computational Linguistics,\npage 11029—11047, Abu Dhabi, UAE. Association for\nComputational Linguistics.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2021.\nFantastically ordered\nprompts and where to find them: Overcoming few-shot\nprompt order sensitivity. In Annual Meeting of the As-\nsociation for Computational Linguistics.\nYao Lu, Jiayi Wang, Raphael Tang, Sebastian Riedel,\nand Pontus Stenetorp. 2024. Strings from the library of\nbabel: Random sampling as a strong baseline for prompt\noptimisation. In Proceedings of the 2024 Conference of\nthe North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies\n(Volume 1: Long Papers), page 2221—2231, Mexico\nCity, Mexico. Association for Computational Linguis-\ntics.\nYi Luan, Luheng He, Mari Ostendorf, and Hannaneh\nHajishirzi. 2018. Multi-task identification of entities, re-\nlations, and coreference for scientific knowledge graph\nconstruction. ArXiv, abs/1808.09602.\nAndrew Maas, Raymond E Daly, Peter T Pham, Dan\nHuang, Andrew Y Ng, and Christopher Potts. 2011.\nLearning word vectors for sentiment analysis. In Pro-\nceedings of the 49th annual meeting of the association\nfor computational linguistics: Human language tech-\nnologies, pages 142–150.\nOscar Mañas, Pietro Astolfi, Melissa Hall, Can-\ndace Ross, Jack Urbanek, Adina Williams, Aish-\nwarya Agrawal, Adriana Romero-Soriano, and Michal\nDrozdzal. 2024. Improving text-to-image consistency\nvia automatic prompt optimization.\narXiv preprint\narXiv:2403.17804.\nBryan McCann, Nitish Shirish Keskar, Caiming Xiong,\nand Richard Socher. 2018. The natural language de-\ncathlon: Multitask learning as question answering.\narXiv preprint arXiv:1806.08730.\nRimon Melamed, Lucas H. McCabe, Tanay Wakhare,\nYejin Kim, H. Howie Huang, and Enric Boix-Adsera.\n2024. Prompts have evil twins.\nM Jehanzeb Mirza, Mengjie Zhao, Zhuoyuan Mao,\nSivan Doveh, Wei Lin, Paul Gavrikov, Michael Dorken-\nwald, Shiqi Yang, Saurav Jha, Hiromi Wakaki, et al.\n2024. Glov: Guided large language models as implicit\noptimizers for vision language models. arXiv preprint\narXiv:2410.06154.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and\nHannaneh Hajishirzi. 2021. Cross-task generalization\nvia natural language crowdsourcing instructions. In\nAnnual Meeting of the Association for Computational\nLinguistics.\nIoannis Mollas, Zoe Chrysopoulou, Stamatis Karlos,\nand Grigorios Tsoumakas. 2020.\nEthos:\nan on-\nline hate speech detection dataset.\narXiv preprint\narXiv:2006.08328.\nRamesh Nallapati, Bowen Zhou, Cícero Nogueira\ndos Santos, Çaglar Gülçehre, and Bing Xiang. 2016.\nAbstractive text summarization using sequence-to-\nsequence rnns and beyond. In Conference on Com-\nputational Natural Language Learning.\n\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\n2018. Don’t give me the details, just the summary!\ntopic-aware convolutional neural networks for extreme\nsummarization. ArXiv, abs/1808.08745.\nEhsan Nezhadarya, Yang Liu, and Bingbing Liu. 2019.\nBoxnet: A deep learning method for 2d bounding box\nestimation from bird’s-eye view point cloud. In 2019\nIEEE Intelligent Vehicles Symposium (IV), pages 1557–\n1564. IEEE.\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal,\nJason Weston, and Douwe Kiela. 2019. Adversarial nli:\nA new benchmark for natural language understanding.\nArXiv, abs/1910.14599.\nJekaterina Novikova, Ondrej Dusek, and Verena Rieser.\n2017. The e2e dataset: New challenges for end-to-end\ngeneration. ArXiv, abs/1706.09254.\nKrista Opsahl-Ong, Michael J Ryan, Josh Purtell, David\nBroman, Christopher Potts, Matei Zaharia, and Omar\nKhattab. 2024. Optimizing instructions and demon-\nstrations for multi-stage language model programs. In\nProceedings of the 2024 Conference on Empirical Meth-\nods in Natural Language Processing, page 9340—9366,\nMiami, Florida, USA. Association for Computational\nLinguistics.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder, Paul\nChristiano, Jan Leike, and Ryan Lowe. 2022. Train-\ning language models to follow instructions with human\nfeedback.\nAnkit Pal, Logesh Kumar Umapathi, and Malaikannan\nSankarasubbu. 2022. Medmcqa: A large-scale multi-\nsubject multi-choice dataset for medical domain ques-\ntion answering. In Conference on health, inference, and\nlearning, pages 248–260. PMLR.\nRui Pan, Shuo Xing, Shizhe Diao, Wenhe Sun, Xiang\nLiu, KaShun Shum, Jipeng Zhang, Renjie Pi, and Tong\nZhang. 2024. Plum: Prompt learning using metaheuris-\ntics. In Findings of the Association for Computational\nLinguistics: ACL 2024, page 2177—2197, Bangkok,\nThailand. Association for Computational Linguistics.\nBo Pang and Lillian Lee. 2004. A sentimental education:\nSentiment analysis using subjectivity summarization\nbased on minimum cuts. ArXiv, cs.CL/0409058.\nBo Pang and Lillian Lee. 2005. Seeing stars: Exploit-\ning class relationships for sentiment categorization with\nrespect to rating scales. In Annual Meeting of the Asso-\nciation for Computational Linguistics.\nArkil Patel, S. Bhattamishra, and Navin Goyal. 2021.\nAre nlp models really able to solve simple math word\nproblems? In North American Chapter of the Associa-\ntion for Computational Linguistics.\nMohammad Taher Pilehvar and Jose Camacho-Collados.\n2019. Wic: the word-in-context dataset for evaluating\ncontext-sensitive meaning representations. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguis-\ntics: Human Language Technologies, Volume 1 (Long\nand Short Papers), pages 1267–1273.\nSilviu Pitis, Michael R Zhang, Andrew Wang, and\nJimmy Ba. 2023. Boosted prompt ensembles for large\nlanguage models. arXiv preprint arXiv:2304.05970.\nEdoardo Maria Ponti, Goran Glavaš, Olga Majewska,\nQianchu Liu, Ivan Vuli´c, and Anna Korhonen. 2020.\nXcopa: A multilingual dataset for causal commonsense\nreasoning. arXiv preprint arXiv:2005.00333.\nArchiki Prasad, Peter Hase, Xiang Zhou, and Mohit\nBansal. 2023. Grips: Gradient-free, edit-based instruc-\ntion search for prompting large language models.\nReid Pryzant, Dan Iter, Jerry Li, Yin Lee, Chenguang\nZhu, and Michael Zeng. 2023. Automatic prompt opti-\nmization with “gradient descent” and beam search. In\nProceedings of the 2023 Conference on Empirical Meth-\nods in Natural Language Processing, page 7957—7968,\nSingapore. Association for Computational Linguistics.\nYe Qi, Devendra Singh Sachan, Matthieu Felix, Sarguna\nPadmanabhan, and Graham Neubig. 2018. When and\nwhy are pre-trained word embeddings useful for neural\nmachine translation? ArXiv, abs/1804.06323.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. In Conference on Em-\npirical Methods in Natural Language Processing.\nDavid Rein, Betty Li Hou, Asa Cooper Stickland, Jack-\nson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian\nMichael, and Samuel R. Bowman. 2023.\nGpqa: A\ngraduate-level google-proof q&a benchmark. ArXiv,\nabs/2311.12022.\nMelissa Roemmele, Cosmin Adrian Bejan, and An-\ndrew S Gordon. 2011. Choice of plausible alternatives:\nAn evaluation of commonsense causal reasoning. In\n2011 AAAI spring symposium series.\nSubhro Roy and Dan Roth. 2016. Solving general arith-\nmetic word problems. ArXiv, abs/1608.01413.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan\nLe Bras, and Yejin Choi. 2019. Social iqa: Common-\nsense reasoning about social interactions. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Process-\ning (EMNLP-IJCNLP), pages 4463–4473.\nTobias Schnabel and Jennifer Neville. 2024. Symbolic\nprompt program search: A structure-aware approach to\nefficient compile-time prompt optimization.\n\nChristoph Schuhmann, Romain Beaumont, Richard\nVencu, Cade Gordon, Ross Wightman, Mehdi Cherti,\nTheo Coombes, Aarush Katta, Clayton Mullis, Mitchell\nWortsman, et al. 2022. Laion-5b: An open large-scale\ndataset for training next generation image-text models.\nAdvances in Neural Information Processing Systems,\n35:25278–25294.\nMelanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane\nSuhr. Quantifying language models’ sensitivity to spu-\nrious features in prompt design or: How i learned to\nstart worrying about prompt formatting. In The Twelfth\nInternational Conference on Learning Representations.\nJingyuan Selena She, Christopher Potts, Sam Bowman,\nand Atticus Geiger. 2023. Scone: Benchmarking nega-\ntion reasoning in language models with fine-tuning and\nin-context learning. In Annual Meeting of the Associa-\ntion for Computational Linguistics.\nZeru Shi, Zhenting Wang, Yongye Su, Weidi Luo, Fan\nYang, and Yongfeng Zhang. 2024. Robustness-aware\nautomatic prompt optimization.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV,\nEric Wallace, and Sameer Singh. 2020. AutoPrompt:\nEliciting Knowledge from Language Models with Au-\ntomatically Generated Prompts. In Proceedings of the\n2020 Conference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 4222–4235, Online.\nAssociation for Computational Linguistics.\nNoah Shinn, Federico Cassano, Edward Berman, Ash-\nwin Gopinath, Karthik Narasimhan, and Shunyu Yao.\n2023. Reflexion: Language agents with verbal rein-\nforcement learning.\nNoah Shinn, Federico Cassano, Ashwin Gopinath,\nKarthik Narasimhan, and Shunyu Yao. 2024. Reflexion:\nLanguage agents with verbal reinforcement learning.\nAdvances in Neural Information Processing Systems,\n36.\nMohit\nShridhar,\nXingdi\nYuan,\nMarc-Alexandre\nCôté, Yonatan Bisk, Adam Trischler, and Matthew\nHausknecht. 2020. Alfworld: Aligning text and em-\nbodied environments for interactive learning. arXiv\npreprint arXiv:2010.03768.\nAnkita Sinha, Wendi Cui, Kamalika Das, and Jiaxin\nZhang. 2024. Survival of the safest: Towards secure\nprompt optimization through interleaved multi-objective\nevolution. In Proceedings of the 2024 Conference on\nEmpirical Methods in Natural Language Processing:\nIndustry Track, pages 1016–1027, Miami, Florida, US.\nAssociation for Computational Linguistics.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng, and\nChristopher Potts. 2013. Recursive deep models for se-\nmantic compositionality over a sentiment treebank. In\nProceedings of the 2013 conference on empirical meth-\nods in natural language processing, pages 1631–1642.\nGizem Sogancioglu, Hakime Öztürk, and Arzucan\nÖzgür. 2017. Biosses: a semantic sentence similarity\nestimation system for the biomedical domain. Bioinfor-\nmatics, 33:i49 – i58.\nAlessandro Sordoni, Eric Yuan, Marc-Alexandre Côté,\nMatheus Pereira, Adam Trischler, Ziang Xiao, Arian\nHosseini, Friederike Niedtner, and Nicolas Le Roux.\n2023. Joint prompt optimization of stacked llms us-\ning variational inference. In Advances in Neural Infor-\nmation Processing Systems, volume 36, pages 58128–\n58151. Curran Associates, Inc.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R Brown, Adam Santoro, Aditya Gupta, Adrià\nGarriga-Alonso, et al. 2022. Beyond the imitation game:\nQuantifying and extrapolating the capabilities of lan-\nguage models. arXiv preprint arXiv:2206.04615.\nHao Sun, Alihan Hüyük, and Mihaela van der Schaar.\n2024a. Query-dependent prompt evaluation and opti-\nmization with offline inverse RL. In The Twelfth Inter-\nnational Conference on Learning Representations.\nHong Sun, Xue Li, Yinchuan Xu, Youkow Homma,\nQi Cao, Min Wu, Jian Jiao, and Denis Charles. 2023.\nAutohint: Automatic prompt optimization with hint gen-\neration. arXiv preprint arXiv:2307.07415.\nJingwei Sun, Ziyue Xu, Hongxu Yin, Dong Yang,\nDaguang Xu, Yudong Liu, Zhixu Du, Yiran Chen, and\nHolger R. Roth. 2024b. Fedbpt: efficient federated\nblack-box prompt tuning for large language models. In\nProceedings of the 41st International Conference on\nMachine Learning, ICML’24. JMLR.org.\nMirac Suzgun, Nathan Scales, Nathanael Schärli, Sebas-\ntian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha\nChowdhery, Quoc Le, Ed Chi, Denny Zhou, et al.\n2023. Challenging big-bench tasks and whether chain-\nof-thought can solve them. In Findings of the Associa-\ntion for Computational Linguistics: ACL 2023, pages\n13003–13051.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. Commonsenseqa: A question\nanswering challenge targeting commonsense knowledge.\nArXiv, abs/1811.00937.\nPrashant Trivedi, Souradip Chakraborty, Avinash Reddy,\nVaneet Aggarwal, Amrit Singh Bedi, and George K.\nAtia. 2025. Align-pro: A principled approach to prompt\noptimization for llm alignment.\nNirali Vaghani and Mansi Thummar. 2023. Flipkart\nproduct reviews with sentiment dataset.\nEllen M Voorhees and Dawn M Tice. 2000. Building a\nquestion answering test collection. In Proceedings of\nthe 23rd annual international ACM SIGIR conference\non Research and development in information retrieval,\npages 200–207.\n\nXingchen Wan, Ruoxi Sun, Hootan Nakhost, and Ser-\ncan O. Arik. 2024. Teach better or show smarter? on\ninstructions and exemplars in automatic prompt opti-\nmization.\nRuochen Wang, Sohyun An, Minhao Cheng, Tianyi\nZhou, Sung Ju Hwang, and Cho-Jui Hsieh. 2025. One\nprompt is not enough: automated construction of a\nmixture-of-expert prompts.\nIn Proceedings of the\n41st International Conference on Machine Learning,\nICML’24. JMLR.org.\nRuoyao Wang, Peter Jansen, Marc-Alexandre Côté, and\nPrithviraj Ammanabrolu. 2022a. Scienceworld: Is your\nagent smarter than a 5th grader?\nIn Proceedings of\nthe 2022 Conference on Empirical Methods in Natural\nLanguage Processing, pages 11279–11298.\nWilliam Yang Wang. 2017. “liar, liar pants on fire”:\nA new benchmark dataset for fake news detection. In\nAnnual Meeting of the Association for Computational\nLinguistics.\nXinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Hao-\ntian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P. Xing, and\nZhiting Hu. 2024a. Promptagent: Strategic planning\nwith language models enables expert-level prompt opti-\nmization. In The Twelfth International Conference on\nLearning Representations, ICLR 2024, Vienna, Austria,\nMay 7-11, 2024. OpenReview.net.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A. Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2022b.\nSelf-instruct: Aligning language\nmodels with self-generated instructions.\nIn Annual\nMeeting of the Association for Computational Linguis-\ntics.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A. Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2023. Self-instruct: Aligning language mod-\nels with self-generated instructions. In Proceedings of\nthe 61st Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers), pages\n13484–13508, Toronto, Canada. Association for Com-\nputational Linguistics.\nYushi Wang, Jonathan Berant, and Percy Liang. 2015.\nBuilding a semantic parser overnight. In Annual Meet-\ning of the Association for Computational Linguistics.\nZhichao Wang, Bin Bi, Shiva Kumar Pentyala, Kiran\nRamnath, Sougata Chaudhuri, Shubham Mehrotra, Zixu,\nZhu, Xiang-Bo Mao, Sitaram Asur, Na, and Cheng.\n2024b. A comprehensive survey of llm alignment tech-\nniques: Rlhf, rlaif, ppo, dpo and more.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2018. Neural network acceptability judgments.\nTransactions of the Association for Computational Lin-\nguistics, 7:625–641.\nJanyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.\nAnnotating expressions of opinions and emotions in\nlanguage. Language Resources and Evaluation, 39:165–\n210.\nAdina Williams, Nikita Nangia, and Samuel R. Bow-\nman. 2017. A broad-coverage challenge corpus for\nsentence understanding through inference. In North\nAmerican Chapter of the Association for Computational\nLinguistics.\nYurong Wu, Yan Gao, Bin Benjamin Zhu, Zineng Zhou,\nXiaodi Sun, Sheng Yang, Jian-Guang Lou, Zhiming\nDing, and Linjun Yang. 2024.\nStraGo: Harnessing\nstrategic guidance for prompt optimization. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2024, pages 10043–10061, Miami, Florida,\nUSA. Association for Computational Linguistics.\nJasper Xian, Saron Samuel, Faraz Khoubsirat, Ronak\nPradeep, Md Arafat Sultan, Radu Florian, Salim\nRoukos, Avirup Sil, Christopher Potts, and Omar Khat-\ntab. 2024. Prompts as auto-optimized training hyperpa-\nrameters: Training best-in-class ir models from scratch\nwith 10 gold labels. arXiv preprint arXiv:2406.11706.\nHanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Wang\nYanggang, Haiyu Li, and Zhilin Yang. 2022. Gps: Ge-\nnetic prompt search for efficient few-shot learning. In\nProceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, pages 8162–8171.\nWei Xu, Alan Ritter, William B. Dolan, Ralph Grish-\nman, and Colin Cherry. 2012. Paraphrasing for style. In\nInternational Conference on Computational Linguistics.\nWeijia Xu, Andrzej Banburski-Fahey, and Nebojsa Jo-\njic. 2024. Reprompting: automated chain-of-thought\nprompt inference through gibbs sampling. In Proceed-\nings of the 41st International Conference on Machine\nLearning, ICML’24. JMLR.org.\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu,\nQuoc V. Le, Denny Zhou, and Xinyun Chen. 2024a.\nLarge language models as optimizers.\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu,\nQuoc V Le, Denny Zhou, and Xinyun Chen. 2024b.\nLarge language models as optimizers. In The Twelfth\nInternational Conference on Learning Representations.\nMuchen Yang, Moxin Li, Yongle Li, Zijun Chen, Chong-\nming Gao, Junqi Zhang, Yangyang Li, and Fuli Feng.\n2024c. Dual-phase accelerated prompt optimization.\nIn Findings of the Association for Computational Lin-\nguistics: EMNLP 2024, pages 12163–12173, Miami,\nFlorida, USA. Association for Computational Linguis-\ntics.\nSheng Yang, Yurong Wu, Yan Gao, Zineng Zhou,\nBin Benjamin Zhu, Xiaodi Sun, Jian-Guang Lou, Zhim-\ning Ding, Anbang Hu, Yuan Fang, et al. 2024d. Ampo:\nAutomatic multi-branched prompt optimization. arXiv\npreprint arXiv:2410.08696.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam W. Cohen, Ruslan Salakhutdinov, and Christo-\npher D. Manning. 2018. Hotpotqa: A dataset for di-\nverse, explainable multi-hop question answering. In\nConference on Empirical Methods in Natural Language\nProcessing.\n\nQinyuan Ye, Maxamed Axmed, Reid Pryzant, and\nFereshte Khani. 2024. Prompt engineering a prompt\nengineer.\nMert Yuksekgonul, Federico Bianchi, Joseph Boen,\nSheng Liu, Zhi Huang, Carlos Guestrin, and James Zou.\n2024. Textgrad: Automatic \"differentiation\" via text.\nJohn M. Zelle and Raymond J. Mooney. 1996. Learning\nto parse database queries using inductive logic program-\nming. In AAAI/IAAI, Vol. 2.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. Hellaswag: Can a ma-\nchine really finish your sentence? In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 4791–4800.\nPengwei Zhan, Zhen Xu, Qian Tan, Jie Song, and\nRu Xie. 2024. Unveiling the lexical sensitivity of llms:\nCombinatorial optimization for prompt enhancement.\nIn Conference on Empirical Methods in Natural Lan-\nguage Processing.\nChenrui Zhang, Lin Liu, Chuyuan Wang, Xiao\nSun, Hongyu Wang, Jinpeng Wang, and Mingchen\nCai. 2024a.\nPrefer: prompt ensemble learning via\nfeedback-reflect-refine. In Proceedings of the Thirty-\nEighth AAAI Conference on Artificial Intelligence and\nThirty-Sixth Conference on Innovative Applications\nof Artificial Intelligence and Fourteenth Symposium\non Educational Advances in Artificial Intelligence,\nAAAI’24/IAAI’24/EAAI’24. AAAI Press.\nLechen Zhang, Tolga Ergen, Lajanugen Logeswaran,\nMoontae Lee, and David Jurgens. 2024b. Sprig: Im-\nproving large language model performance by system\nprompt optimization. ArXiv, abs/2410.14826.\nTianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schu-\nurmans, and Joseph E. Gonzalez. 2022. Tempera: Test-\ntime prompting via reinforcement learning.\nTianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Evaluat-\ning text generation with bert. In International Confer-\nence on Learning Representations.\nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text classifi-\ncation. In Neural Information Processing Systems.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuo-\nhan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E.\nGonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge\nwith mt-bench and chatbot arena. In Proceedings of the\n37th International Conference on Neural Information\nProcessing Systems, NIPS ’23, Red Hook, NY, USA.\nCurran Associates Inc.\nHan Zhou, Xingchen Wan, Ivan Vuli´c, and Anna Ko-\nrhonen. 2023. Survival of the most influential prompts:\nEfficient black-box prompt search via clustering and\npruning. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2023, pages 13064–13077,\nSingapore. Association for Computational Linguistics.\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,\nKeiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba.\n2022. Large language models are human-level prompt\nengineers.\n\n12\nAppendix\n12.1\nNotation\nWe now define the notation of key terms and expressions used throughout the paper.\n1. T = Task type, I= Task instruction, E = (xi, yi)e\ni=1 Few shot demonstrations in the prompt, τ=\nTemplate delimiters, z = CoT recipe for a task-instance, zi ∈Ii\n2. Mtask target model, MAPO APO system\n3. ρ = concat([s1, s2, . . . , sm]) = concat(I, τ, E) Prompt composed of m sentences, which comprise\nof Instruction, template delimiters and few-shot demonstrations.\n4. D = {(xi, yi)}m\ni=1 collection of m input-output pairs. Dval is the validation set used to validate\nprompt performance, Dtrain is the training set used to finetune the language model(Reprompting).\n5. {f1, f2, . . .} ∈F metric function upon which to evaluate task-prompt performance\n6. r : S × A →R= reward model score, where S is the state-space and A is the action-space\n7. |V | = length of vocabulary\n8. ϕ : S ∈V∗→Rd embedding function which takes in a sentence generated as a finite sequence of\ntokens belonging to a vocabulary V, and generating a floating point array representation of dimension\nd\n9. ρ∗= argmaxρ∈V∗EDval[fi(ρ)] The best performing prompt based on the metric score on validation\nset\n10. k = number of candidates for top-K search, B = Beam width for beam search, N = number of\niterations for search\n11. C = number of experts in a Mixture of Experts approach (MOP), µC= cluster centroid of cluster C\n(MOP).\n12. LLMtarget= target model which will be used for inference, LLMrewriter= rewriter model which\nwill be used for rewriter, LLMevaluator= evaluator model which provides the LLM feedback to\nprompts / responses or both\n13. λ with subscripts to denote different latency types: λt = Total training cost/latency, including all\noffline costs for data collection, preprocessing, and model fine-tuning, λi = per-example inference\nlatency, λm = MLM inference latency per-example\n12.2\nExcluded works\nFedBPT (Sun et al., 2024b) used federated learning to update soft prompts and not discrete tokens.\nDeliberate-then-generate (Li et al., 2023a) randomly sampled arbitrary noisy inference and prompted\nthe task LLM to deliberate on the wrong inference, while Reflexion (Shinn et al., 2023) agents maintain\nan episodic buffer of past deliberations. Neither method optimizes the input prompt. AutoPrompt (Shin\net al., 2020) required gradient access to the task LLM and therefore doesn’t remain blackbox.\n\n12.3\nUCB based selection algorithm\nAlgorithm 2 Select(·) with UCB Bandits\nRequire: n prompts ρ1, ..., ρn, dataset Dval, T time steps, metric function m\n1: Initialize: Nt(ρi) ←0 for all i = 1, . . . , n\n2: Initialize: Qt(ρi) ←0 for all i = 1, . . . , n\n3: for t = 1, . . . , T do\n4:\nSample uniformly Dsample ⊂Dval\n5:\nρi ←arg maxρ\nn\nQt(ρ)\nNt(ρi) + c\nq\nlog t\nNt(ρ)\no\n6:\nObserve reward ri,t = m(ρi, Dsample)\n7:\nNt(ρi) ←Nt(ρi) + |Dsample|\n8:\nQt(ρi) ←Qt(ρi) + ri,t\n9: return SelectTopb(QT /NT )\n13\nComparison of different approaches + Tasks\n13.1\nComparison\nBelow we offer a comprehensive comparison of all the surveyed methods against our framework, covering\nthe following aspects\n1. Seed instructions\n2. Inference evaluation\n3. Candidate generation\n4. Search+filter strategy\n5. Iteration depth\n6. Optimization time complexity\n7. Prompt generation model\n8. Target models\n\nSNo.\nMethod\nSeed\ninstruc-\ntions\nInference evaluation\nCandidate generation\nSearch+filter strategy\nIteration depth\nOptimization time\ncomplexity\nPrompt\ngenera-\ntion model\nTarget models\n1\nGPS (Xu et al., 2022)\nManually\ncreated\nTask accuracy\nGenetic Algorithm:\nBack\ntranslation,\nCloze,\nSentence continuation\nMetaheuristic ensemble\nFixed\nO(T ∗N ∗k ∗λi)\nT0\n2\nGRIPS\n(Prasad\net\nal.,\n2023)\nManually\ncreated\nEntropy-based score+\nTask accuracy\nPhrase level\nadd/remove/swap/paraphrase\nTopK selection\nFixed\nO(k ∗N ∗|Dval| ∗\nB)\nPEGASUS\npara-\nphrase model\nInstructGPT\n3\nInstruction induction\n(Honovich et al., 2023)\nInstruction\ninduction\nAccuracy +\nBERTScore\nLLM-rewriter\nFixed\nO(|ρ| ∗λi)\nInstructGPT, GPT-3\nInstructGPT, GPT-3\n4\nRLPrompt (Deng et al.,\n2022)\nManually\ncreated\nTask accuracy +\nReward model score\nRL-based trained NN\nTopK selection\nFixed\nO(N ∗ρ ∗|V | ∗λi)\nRoBERTa-large\nReward\nmodel-\nDistilBERT\n1/ BERT, 2/ GPT-2\n5\nTEMPERA (Zhang et al.,\n2022)\nManually\ncreated\nTask accuracy\nRL-trained NN\nFixed\nO(N ∗k ∗|V | ∗C)\nRoBERTa-large\nRoBERTa-large\n6\nAELP (Hsieh et al., 2024)\nManually\ncreated\nTask accuracy\nGenetic algorithm:\nLLM-mutator\nBeam search\nFixed\nO(N ∗ρ ∗k ∗|D| ∗\nλi)\nPaLM 2-L\nPaLM text-bison\n7\nAPE (Zhou et al., 2022)\nInstruction\ninduction\nTask accuracy\nNo new candidates\nTopK selection\nFixed\nO(N ∗k ∗|Dval| ∗\nλi)\nInstructGPT, GPT-\n3, T5,\nInsertGPT\nInstructGPT, GPT-3\n8\nAutoHint\n(Sun\net\nal.,\n2023)\nManually\ncreated\nTask accuracy +\nLLM-feedback\nLLM rewriter\nTopK selection\nFixed\nO(T ∗|D| ∗λi)\nGPT-4\n9\nBDPL (Diao et al., 2022)\nManually\ncreated\nTask accuracy\nRL-trained NN\nTopK selection\nVariable\nO(N ∗k ∗λi)\nRoBERTa, GPT-3\nRoBERTa, GPT-3\n10\nBoosted Prompting\n(Pitis et al., 2023)\nInstruction-\ninduction\nTask accuracy\nEnsemble\nbased\nmethod\nTopK selection\nVariable\nO(N ∗k ∗λi)\ntext-curie-001, text-\ncurie-003, GPT-3.5,\ncode-davinci-002\ntext-curie-001, text-curie-003,\nGPT-3.5, code-davinci-002\n11\nBPO (Cheng et al., 2024)\nManually\ncreated\nLLMaaJ (pairwise)\nFinetuned LLMs\nNA\nNA\nO(λt + |Dval| ∗λi)\nLlama2-7b-chat\nVicuna-7b-v1.3,\nvicuna-13b-v1.3, llama-1-7b,\nllama-1-13b\n12\nCLAPS\n(Zhou\net\nal.,\n2023)\nManually\ncreated\nEntropy-based score+\nTask accuracy\nGenetic Algorithm:\nMutation + Crossover\nTopK selection\nVariable\nO(N ∗k ∗|V | ∗λi)\nFlan-T5\nFlan-T5 large and base\n13\nDirectional-stimulus (Li\net al., 2023d)\nManually\ncreated\nBLEU, BERTScore\nRL-trained NN\nVariable\nO(λt)\nT5, GPT-2\nChatGPT, Codex, InstructGPT\n14\nDLN\n(Sordoni\net\nal.,\n2023)\nManually\ncreated\nTask accuracy + NLL\nLLM mutator\nTopK selection\nFixed\nO(N ∗k ∗|Dtrain|)\nGPT-3\n(text-\ndavinci-003),\nGPT-4\nGPT-3 (text-davinci-003), GPT-4\n15\nDSP (Khattab et al., 2022)\nInstruction\ninduction\nTask accuracy\nProgram Synthesis\nTopK selection\nFixed\nO(N ∗k ∗λi)\nGPT-3.5\nLM: GPT-3.5,\nRetrieval: ColBERTv2\n16\nDSPy\n(Khattab\net\nal.,\n2024)\nManually\ncre-\nated +\nInstruction\nInduction\nTask accuracy +\nLLM-feedback\nProgram Synthesis\nTopK selection\nVariable\nO(N ∗k ∗B ∗λi)\n17\nGATE (Joko et al., 2024)\nManually\ncreated\nHuman feedback\nLLM rewriter\nOpen-ended\nO(N\n∗\n(λm\n+\n|Dval| ∗λi))\nGPT-4\nGPT-4\n18\nGPO (Li et al., 2023c)\nInstruction\ninduction\nTask-Accuracy and F1\nMetaprompt-design\nTopK selection\nO(N ∗C ∗|V |∗B ∗\nE)\ngpt-3.5-turbo-0301\ngpt-3.5-turbo-0301\n19\nPACE (Dong et al., 2024b)\nManually\ncreated\nNLL + Task accuracy -\nBLEU and BERTScore\nLLM-rewriter\nTopK selection\n< 3\nO(N ∗|ρ| ∗|Dval|)\ngpt-3.5-turbo\n(0301)\ntext-davinci-002,\ntext-davinci-003,\n(gpt-3.5-turbo), GPT-4\n20\nPREFER (Zhang et al.,\n2024a)\nManually\ncreated\nTask accuracy\nLLM-rewriter +\nEnsemble method\nTopK selection\nFixed\nO(N ∗|ρ| ∗|Dval|)\nChatGPT\nChatGPT\n21\nPromptagent (Wang et al.,\n2024a)\nManually\ncreated\nTask accuracy +\nLLM-feedback\nLLM rewriter\nUCT-based bandit-search\nFixed\nO(N ∗k ∗λi)\nGPT-4\nGPT-3.5, GPT-4, PaLM-2\nTable 2: Comparison of all APO techniques based on our framework\n\nSNo.\nMethod\nSeed\ninstruc-\ntions\nInference evaluation\nCandidate generation\nSearch+filter strategy\nIteration depth\nOptimization time\ncomplexity\nPrompt\ngenera-\ntion model\nTarget models\n22\nPromptboosting\n(Hou\net al., 2023)\nInstruction-\ninduction\nAccuracy, F1 Score\nEnsemble\nbased\nmethod\nBeam-search\nEarly Stopping\nO(λm)\nT5\nRoBERTa-large\n23\nPromptbreeder (Fernando\net al., 2023)\nManually\ncreated\nLLM Feedback +\nTask accuracy\nGenetic Algorithm:\nMutate + Crossover\n(LLM-edits)\nMetaheuristic Ensemble\nFixed\nO(ρ ∗N ∗|V | ∗λi)\ntext-davinci-003,\nPaLM 2-L\ntext-davinci-003, PaLM 2-L\n24\nProTeGi (Pryzant et al.,\n2023)\nManually\ncreated\nTask accuracy +\nLLM-feedback\nLLM rewriter\nUCT-based bandit-search\nFixed\nO(N ∗C ∗|Dval| ∗\nλi)\nGPT-3.5-Turbo\nGPT-3.5-turbo\n25\nRandom separators (Lu\net al., 2024)\nManually\ncreated\nTask accuracy\nLLM-rewriter\nTopK selection\nFixed steps\nO(N ∗k ∗λ)\nGPT2 Large, GPT2\nXL,\nMistral 7B, Mistral\n7B Instruct,\nLlama-Alpaca 7B,\nLlama2 7B.\nLlama2 7B Chat,\nChatGPT\nGPT2 Large, GPT2 XL,\nMistral 7B, Mistral 7B Instruct,\nLlama-Alpaca 7B, Llama2 7B.\nLlama2 7B Chat, ChatGPT\n26\nABO (Yang et al., 2024b)\nManually\ncre-\nated +\nInstruction\nInduction\nTask accuracy +\nLLM-feedback\nLLM-rewriter\nTopK selection\nFixed Steps\nO(B ∗N ∗λi)\nGPT-4\nGPT-3.5-Turbo,\nLlama-2-70B-\nchat\n27\nAdv-ICL (Long et al.,\n2024)\nManually\ncreated\nLLM Feedback\nLLM-rewriter\nTop-1 selection\nFixed\nO(N ∗k ∗λi)\ntext-davinci-002,\nvicuna,\nChatGPT\ntext-davinci-002, vicuna, Chat-\nGPT\n28\nAMPO\n(Yang\net\nal.,\n2024d)\nManually\ncreated\nTask accuracy +\nF1 score\nCoverage-based\nTopK selection\nVariable\nO(N ∗C ∗λi)\nGPT-4-turbo\nGPT-4-turbo\n29\nAPEER (Jin et al., 2024)\nManually\ncreated\nTask accuracy-nDCG\nFeedback + preference\noptimization\nUsed 3 epochs\nO(N ∗|ρ|∗|Dval|)\nGPT4, GPT3.5, Llama3, Qwen2\n30\nAPOHF (Lin et al., 2024)\nManually\ncreated\nTask accuracy +\nHuman feedback\nLLM rewriter\nLinear UCB\nFixed\nO(N ∗T)\nChatGPT\nDALLE-3, ChatGPT\n31\nBATPrompt (Shi et al.,\n2024)\nManually\ncreated\nTask accuracy +\nLLM-feedback\nLLM rewriter\nTopK selection\nFixed\nO(N ∗|D|∗|ρ|∗λi)\nGPT-3.5-turbo\nGPT-3.5-turbo,\nGPT-4o-mini, Llama2-7b\n32\nCOPLE\n(Zhan\net\nal.,\n2024)\nManually\ncreated\nTask accuracy\nToken edits using\nMLM\nVariable\nO(N ∗|I| ∗k ∗\n|Dval| ∗λi)\nRoBERTa\n(filling masked to-\nkens)\nLlama-2-7B-chat ,\nMistral-7B-Instruct-v0.1,\nChatGPT (gpt-3.5-turbo-0125)\n33\nCRISPO (He et al., 2025)\nManually\ncreated\nLLM feedback +\nROUGE-1/2/L\nF-\nmeasure,\nAlignScore\nLLM rewriter\nTOP-K greedy search\nFixed\nO(N∗k∗(|Dtrain|∗\nλi + λm))\nClaude Instant,\nClaude 3 Sonnet,\nMistral 7B, Llama3\n8B\nClaude Instant,\nClaude 3 Sonnet,\nMistral 7B, Llama3 8B\n34\nDAPO (Yang et al., 2024c)\nManually\ncreated\nTask accuracy\nLLM-rewriter\nTop-1 selection\nFixed\nO(N ∗k ∗λi)\nGPT-3.5-Turbo,\nBaichuan2,\nGPT-4\nGPT-3.5-Turbo,\nBaichuan2, GPT-4\n35\nDRPO\n(Amini\net\nal.,\n2024)\nManually\ncreated\nReward model score +\nLLM Feedback\nLLM rewriter\nBeam search\nFixed\nO(B ∗k ∗N)\nMistral 7b, Mistral\n7b (Instruct),\nLlama 2 70b, Llama\n2 70b (chat),\nLlama 3 8b, Llama\n3 8b (Instruct),\ngpt-3.5-turbo\nMistral 7b, Mistral 7b (Instruct),\nLlama 2 70b, Llama 2 70b (chat),\nLlama 3 8b, Llama 3 8b (In-\nstruct),\ngpt-3.5-turbo\n36\nEVOPROMPT (Guo et al.,\n2024)\nManually\ncre-\nated +\nInstruction\nInduction\nTask Accuracy +\nROUGUE+ SARI\nGenetic Algorithm:\nMutation operators+\nCrossover\nMetaheuristic ensemble\nEarly Stopping\nO(N ∗k ∗T ∗λi)\nAlpaca-7b, GPT-3.5\n37\nFIPO (Lu et al., 2025)\nManually\ncreated\nTask accuracy\nFinetuned LLMs\nO(λt+|Dval|∗λi))\nTulu-13B,\nTulu-\n70B\nLlama2-7B, Tulu2-13B,\nBaichuan2-13B\nTable 3: Comparison of all APO techniques based on our framework\n\nSNo.\nMethod\nSeed\ninstruc-\ntions\nInference evaluation\nCandidate generation\nSearch+filter strategy\nIteration depth\nOptimization time\ncomplexity\nPrompt\ngenera-\ntion model\nTarget models\n38\nLMEA (Liu et al., 2023)\nManually\ncreated\nNumeric Score-based\nGenetic Algorithm:\nMutate + Crossover\n(LLM-edits)\nTopK selection\nFixed\nO(N ∗k ∗λi)\nGPT-3.5-turbo-0613\n39\nMIPRO\n(Opsahl-Ong\net al., 2024)\nManually\ncreated\nTask accuracy\nProgram Synthesis\nTopK selection\nFixed\nO(N ∗|Dval| ∗k ∗\nλi)\nGPT-3.5 (proposer\nLM)\nLlama-3-8B (task LM)\n40\nMOP (Wang et al., 2025)\nInstruction\ninduction\nTask Accuracy\nAPE for each cluster\nTopK selection\nFixed\nsteps\nper-\ncluster\nO(C ∗N ∗|Dval|)\nGPT-3.5-Turbo\nGPT-3.5-Turbo\n41\nMORL-Prompt\n(Jafari\net al., 2024)\nManually\ncreated\nTask accuracy +\nReward score\nRL-based trained NN\nFixed\nO(N ∗C ∗|V | ∗k)\ndistilGPT-2\nGPT-2 (style transfer),\nflan-T5-small (translation)\n42\nOIRL (Sun et al., 2024a)\nManually\ncreated\nTask accuracy +\nReward model score\nLLM rewriter\nO(|Dtrain|∗ρ∗λi+\nλt + |Dval| ∗λi)\nGPT4\nLlama2-7B-chat,\nTigerbot-13B-chat, gpt3.5-turbo\n43\nOPRO (Yang et al., 2024a)\nManually\ncreated\nTask accuracy +\nLLM-feedback\nMetaprompt design\nTopK selection\nVariable\nO(N ∗k ∗λi)\nPaLM\n2-L,\ntext-\nbison,\ngpt-3.5-turbo\nand\nGPT-4\nPaLM family models\n44\nPE2 (Ye et al., 2024)\nManually\ncre-\nated +\nInstruction\nInduction\nTask accuracy +\nLLM-feedback\nMetaprompt design\nTopK selection\nFixed\nO(N ∗k ∗λi)\nGPT-4\ntext-davinci-003\n45\nPIN (Choi et al., 2024)\nManually\ncreated\nTask accuracy\nRL-trained LLM\nTopK selection\nFixed\nO(N ∗|V |∗λi ∗C)\nOPT\nRoBERTa-large (classification),\nOPT models (others)\n46\nPLUM (Pan et al., 2024)\nManually\ncreated\nTask accuracy\nGenetic Algorithm:\nMutate + crossover\nMetaheuristics\nFixed steps\nO(N ∗C ∗k ∗λi)\nGPT-3-babbage\nGPT-3-babbage\n47\nPRewrite (Kong et al.,\n2024)\nManually\ncreated\nTask accuracy +\nReward model score\nRL-trained LLM\nTopK selection\nFixed\nO(N ∗C ∗λi ∗|V |)\nPaLM 2-S\nPaLM 2-L\n48\nPROMPTWIZARD\n(Agarwal et al., 2024)\nManually\ncreated\nTask accuracy +\nLLM-feedback\nGenetic Algorithm:\nMutate + Crossover\n(LLM-edits)\nTopK selection\nFixed\nO(N ∗C ∗λi)\nGPT3.5/GPT4\nGPT3.5/GPT4/Llama-70B\n49\nPROMST (Chen et al.,\n2024)\nManually\ncreated\nTask accuracy +\nHuman feedback\nLLM rewriter\nTopK selection\nFixed\nO(N ∗k ∗λi)\nGPT-4\nGPT-3.5, GPT-4\n50\nReprompting (Xu et al.,\n2024)\nLLM generated\nCoT process.\nTask accuracy\nLLM-rewriter\nRejection sampling\nwith exploration\nFixed or until con-\nvergence\nO(N ∗k ∗|ρ|)\ngpt-3.5-turbo,\ntextdavinci-003\ngpt-3.5-turbo, textdavinci-003\n51\nSAMMO (Schnabel and\nNeville, 2024)\nManually\ncreated\nTask accuracy\nProgram synthesis\nTopK selection\nFixed\nO(N ∗k ∗λi)\nMixtral7x8B, Llama-2 70B,\nGPT3.5, GPT4\n52\nSCULPT (Kumar et al.,\n2024)\nInstruction\ninduction\non\ntask-\nREADME\nTask accuracy +\nLLM-feedback\nLLM-rewriter\nUCB bandit search\nFixed\nO(N ∗k ∗|ρ| ∗\n|Dval|)\nGPT-4o\nGPT-4o and Llama3.1-8B\n53\nSOS (Sinha et al., 2024)\nManually\ncreated\nTask accuracy +\nLLM-feedback\nLLM-mutator\nTopK selection\nFixed\nO(N ∗C ∗k ∗λi)\nGPT-3.5-turbo,\nLlama3-8B,\nMistral-7B\nGPT-3.5-turbo, Llama3-8B,\nMistral-7B\n54\nSPRIG\n(Zhang\net\nal.,\n2024b)\nManually\ncreated\nTask accuracy\nGenetic Algorithm:\nMutate + Crossover (to-\nkens)\nBeam-search\nFixed\nO(N ∗B∗T ∗k∗λi)\ntuner007/pegasus_paraphrase\nLlama 3.1-8B Instruct,\nMistral Nemo Instruct 2407,\nQwen 2.5-7B Instruct,\nLlama 70B, Qwen 2.5-72B,\nMistral Large 2407.\n55\nStraGo (Wu et al., 2024)\nManually\ncreated\nTask accuracy +\nLLM-feedback\nGenetic Algorithm:\nMutate + CrossOver\n(tokens)\nBandit Search (UCB)\nEarly Stopping\nO(N ∗k ∗T ∗λi)\nGPT-4\nGPT-3.5-turbo or GPT-4\n56\nTextGrad\n(Yuksekgonul\net al., 2024)\nManually\ncreated\nTask accuracy +\nLLM-feedback\nLLM rewriter\nVariable\nO(N ∗|Dval| ∗λi)\nGPT-3.5, GPT-4o\n57\nUNIPROMPT\n(Juneja\net al., 2024)\nManually\ncre-\nated +\nInstruction\nInduction\nTask accuracy +\nLLM-feedback\nLLM-rewriter\nBeam Search\nEarly Stopping\nO(N ∗k ∗λi)\nFine-tuned Llama2-\n13B\nGPT-3.5\nTable 4: Comparison of all APO techniques based on our framework\n\n13.2\nEvaluation tasks and datasets\nBelow we describe the different datasets and tasks that each method was evaluated on.\nSNo.\nPaper\nTasks\n1\nGPS (Xu et al., 2022)\n10 unseen tasks from the T0 benchmark, which span:\n1. Natural Language Inference: ANLI R1, R2, R3, CB, RTE (Nie et al., 2019; Dagan et al.,\n2005).\n2. Coreference Resolution: WSC, Winogrande.(Levesque et al., 2011)\n3. Sentence Completion: COPA(Roemmele et al., 2011) , HellaSwag (Zellers et al., 2019).\n4. Word Sense Disambiguation: WiC (Pilehvar and Camacho-Collados, 2019).\n2\nGRIPS (Prasad et al., 2023)\n8 classification tasks from NaturalInstructions (Mishra et al., 2021)\n3\nInstruction induction (Honovich et al., 2022)\n1. Spelling, 2. Syntax, 3. Morpho-syntax, 4. Lexical semantics,\n5. Phonetics, 6. Knowledge, 7. Semantics, 8. Style\n4\nRLPrompt (Deng et al., 2022)\n1. Classification\n2. Text-style transfer\n5\nTEMPERA (Zhang et al., 2022)\nClassification\n6\nAELP (Hsieh et al., 2024)\nBig Bench Hard (Suzgun et al., 2023)\n7\nAPE (Zhou et al., 2022)\n1. 24 Instruction induction tasks (Honovich et al., 2022) 2. 21 BIG Bench Hard tasks (Suzgun\net al., 2023)\n8\nAutoHint (Sun et al., 2023)\nBIG-Bench Instruction Induction (Epistemic Reasoning, Logical Fallacy Detection, Implica-\ntures, Hyperbaton, Causal Judgment, Winowhy) (Zhou et al., 2022)\n9\nBDPL (Diao et al., 2022)\n1. MNLI (Williams et al., 2017), 2. QQP (Cer et al., 2017), 3. SST-2 (Socher et al., 2013), 4.\nMRPC (Dolan and Brockett, 2005), 5. CoLA (Warstadt et al., 2018), 6. QNLI (Rajpurkar et al.,\n2016), 7. RTE (Dagan et al., 2005), 8. CitationIntent (Jurgens et al., 2018), 9. SciERC (Luan\net al., 2018), 10. RCT (Dernoncourt and Lee, 2017), 11. HyperPartisan (Kiesel et al., 2019)\n10\nBoosted Prompting (Pitis et al., 2023)\nGSM8K (Cobbe et al., 2021) and AQuA (Garcia et al., 2020)\n11\nBPO (Cheng et al., 2024)\nGeneration: Dolly Eval (Conover et al., 2023), Vicuna Eval (Chiang et al., 2023), Self-Instruct\nEval (Wang et al., 2022b)\n12\nCLAPS (Zhou et al., 2023)\n13\nDirectional-stimulus (Li et al., 2023d)\nMultiWOZ (Budzianowski et al., 2018)\n14\nDLN (Sordoni et al., 2023)\n1. Mpqa Sentiment analysis (Lu et al., 2021)\n2. Trec Question type classification (Lu et al., 2021)\n3. Subj Determine whether a sentence is subjective or objective (Lu et al., 2021)\n4. Leopard (Bansal et al., 2019)- Disaster Determine whether a sentence is relevant to a disaster.\n5. Leopard (Bansal et al., 2019)- Airline Airline tweet sentiment analysis.\n6. BBH (Suzgun et al., 2023)- (Hyper, Nav, Date, Logic datasets)\n15\nDSP (Khattab et al., 2022)\n1. open-domain question answering (Open-SQuAD) (Lee et al., 2019)\n2. multi-hop question answering (HotPotQA) (Yang et al., 2018)\n3. conversational question answering (QReCC) (Anantha et al., 2020)\n16\nDSPy (Khattab et al., 2024)\n17\nGATE (Joko et al., 2024)\nLAPS (Joko et al., 2024) (1. Content Recommendation (user likes to read a given held-out\narticle or not) 2. Moral Reasoning, 3. Email Verification)\n18\nGPO (Li et al., 2023c)\n1. Sentiment analysis - Yelp (Zhang et al., 2015), Flipkart (Vaghani and Thummar, 2023),\nIMDB (Maas et al., 2011), Amazon (Zhang et al., 2015)\n2. NLI - MNLI (Williams et al., 2017), ANLI (Nie et al., 2019) 3.Entailment - RTE (Dagan\net al., 2005), 4. CommonsenseQA - SocialIQA (Sap et al., 2019)\n5. Multi-turn dialog - DSTC7 (Gunasekara et al., 2019), Ubuntu Dialog (Lowe et al., 2015),\nMuTual (Cui et al., 2020)\n6. NumericalQA - DROP (Dua et al., 2019)\n19\nPACE (Dong et al., 2024b)\nBBH (Suzgun et al., 2023), instruction induction tasks (24 tasks) (Honovich et al., 2022) and\ntranslation tasks (en-de, en-es, en-fr)\n20\nPREFER (Zhang et al., 2024a)\n1. NLI tasks including SNLI (Bowman et al., 2015), MNLI (Williams et al., 2017), QNLI\n(Rajpurkar et al., 2016), RTE (Dagan et al., 2005)\n2. Classification: Ethos (Mollas et al., 2020), liar (Wang, 2017), ArSarcasm (Farha and Magdy,\n2020a)\n21\nPromptagent (Wang et al., 2024a)\n1. BigBenchHard (BBH) (Suzgun et al., 2023) - 6 BBH tasks that emphasize a blend of domain\nknowledge\n2. Biomedical - Disease NER (NCBI) (Do˘gan et al., 2014), MedQA (Jin et al., 2020), Bio\nsimilar sentences (Sogancioglu et al., 2017)\n3. 2 classification - TREC (Voorhees and Tice, 2000) + Subj. (Pang and Lee, 2004) 1 NLI(CB)\n(de Marneffe et al., 2019)\n22\nPromptboosting (Hou et al., 2023)\nText Classification\n23\nPromptbreeder (Fernando et al., 2023)\n1. Arithmetic Reasoning: Benchmarks: GSM8K (Cobbe et al., 2021), MultiArith (Roy and\nRoth, 2016), AddSub (Hosseini et al., 2014),\nSVAMP (Patel et al., 2021), SingleEq (Koncel-Kedziorski et al., 2015), AQuA-RAT (Ling et al.,\n2017).\n2. Commonsense Reasoning: Benchmarks: CommonSenseQA (CSQA) (Talmor et al., 2019),\nStrategyQA (SQA) (Geva et al., 2021).\n3. Hate Speech Classification: Dataset: ETHOS (Mollas et al., 2020).\n4. Instruction Induction (Honovich et al., 2022): Tasks: 24 datasets spanning\nsentence similarity, style transfer, sentiment analysis, and more\nTable 5: Tasks covered in the different papers\n\nSNo.\nPaper\nTasks\n24\nProTeGi (Pryzant et al., 2023)\nJailbreak (Pryzant et al., 2023), Liar (Wang, 2017), Sarcasm (Farha and Magdy, 2020b), Ethos\n(Mollas et al., 2020)\n25\nRandom separators (Lu et al., 2024)\n1. SST-2, SST-5,(Socher et al., 2013) 3. DBPedia (Zhang et al., 2015), 4. MR (Pang and Lee,\n2005), 5. CR (Hu and Liu, 2004), 6. MPQA (Wiebe et al., 2005), 7. Subj (Pang and Lee, 2004),\n8. TREC (Voorhees and Tice, 2000), 9. AGNews (Zhang et al., 2015)\n26\nABO (Yang et al., 2024b)\nBigBenchHard tasks (Suzgun et al., 2023): Object Counting, Navigate, Snarks, Question\nSelection\n27\nAdv-ICL (Long et al., 2024)\nSummarization (XSUM (Narayan et al., 2018), CNN/Daily Mail (Nallapati et al., 2016)),\nData-to-Text (WebNLG (Gardent et al., 2017), E2E NLG (Novikova et al., 2017)), Translation\n(LIRO (Dumitrescu et al., 2021), TED Talks (Qi et al., 2018)), Classification (YELP-5 (Zhang\net al., 2015), WSC (Levesque et al., 2011)), Reasoning (GSM8k (Cobbe et al., 2021), SVAMP\n(Patel et al., 2021))\n28\nAMPO (Yang et al., 2024d)\nText classification task TREC (Voorhees and Tice, 2000),\nsentiment classification task SST-5 (Socher et al., 2013),\nlargescale reading comprehension task RACE (Lai et al., 2017),\nmedical question-answering tasks MedQA (Jin et al., 2020) and MedMCQA (Pal et al., 2022)\n29\nAPEER (Jin et al., 2024)\nPassage reranking\n30\nAPOHF (Lin et al., 2024)\n1. User instruction optimization using tasks from Instructzero, 2. Text-to-image , 3. Response\noptimization\n31\nBATPrompt (Shi et al., 2024)\n1. Language understanding, 2. Text summarization, 3. Text simplification\n32\nCOPLE (Zhan et al., 2024)\nGLUE - SST2 (Socher et al., 2013), COLA (Warstadt et al., 2018), MNLI (Williams et al.,\n2017), QNLI (Rajpurkar et al., 2016), RTE (Dagan et al., 2005), MRPC (Dolan and Brockett,\n2005), QQP (Cer et al., 2017) MMLU (Hendrycks et al., 2020) - STEM, Humanities, Social\nSciences and Other\n33\nCRISPO (He et al., 2025)\nSummarization, QA\n34\nDAPO (Yang et al., 2024c)\n1. Sentiment classification, 2. topic classification, 3. News, 4. TREC (Voorhees and Tice,\n2000), 5. subjectivity classification (Pang and Lee, 2004), 6. Logic Five, 7. Hyperbaton, 8.\nDisambiguation, 9. Salient, 10.Translation\n35\nDRPO (Amini et al., 2024)\nAlignment benchmark\n36\nEVOPROMPT (Guo et al., 2024)\n1. Language Understanding: Sentiment classification (e.g., SST-2, SST-5, CR, MR (Socher\net al., 2013; Hu and Liu, 2004; Pang and Lee, 2005)), 2. Topic classification (e.g., AGNews\n(Zhang et al., 2015), TREC (Voorhees and Tice, 2000)), Subjectivity classification (Subj (Pang\nand Lee, 2004)). 3. Language Generation: Summarization (SAMSum (Gliwa et al., 2019)).\nSimplification (ASSET (Alva-Manchego et al., 2020)). 4. Reasoning (BIG-Bench Hard Tasks)\n(Suzgun et al., 2023): Multi-step reasoning tasks from BBH, such as logical deduction, causal\njudgment, and object tracking.\n37\nFIPO (Lu et al., 2025)\n1. Generation: GSM8K (Cobbe et al., 2021), BBH (Suzgun et al., 2023) 2. Multiple Choice:\nPiQA (Bisk et al., 2019), CosmosQA (Huang et al., 2019), MMLU (Hendrycks et al., 2020)\n38\nLMEA (Liu et al., 2023)\nTraveling Salesman Problems (TSPs)\n39\nMIPRO (Opsahl-Ong et al., 2024)\n1. Question Answering (HotPotQA)(Yang et al., 2018) 2. Classification (Iris (Fisher, 1936),\nHeart Disease (Detrano et al., 1989)) 3. Entailment (ScoNe) (She et al., 2023) 4. Multi-hop\nFact Extraction and Claim Verification (HoVer) (Jiang et al., 2020)\n40\nMOP (Wang et al., 2025)\n50 tasks comprising of Instruction Induction (Honovich et al., 2022), Super Natural Instructions\n(Mishra et al., 2021), BBH (Suzgun et al., 2023)\n41\nMORL-Prompt (Jafari et al., 2024)\n1. Unsupervised Text Style Transfer: Shakespearean data (Xu et al., 2012) 2. Supervised\nMachine Translation: iwslt2017 (Cettolo et al., 2017)\n42\nOIRL (Sun et al., 2024a)\nArithmetic reasoning: GSM8K (Cobbe et al., 2021), MAWPS, SVAMP (Patel et al., 2021)\n43\nOPRO (Yang et al., 2024a)\nGSM8K (Cobbe et al., 2021), BBH (23 tasks) (Suzgun et al., 2023), MultiArith (Roy and Roth,\n2016), AQuA (Garcia et al., 2020)\n44\nPE2 (Ye et al., 2024)\n1. MultiArith and GSM8K for math reasoning (Cobbe et al., 2021),\n2. Instruction Induction (Honovich et al., 2022),\n3. BIG-bench Hard for challenging LLM tasks (Suzgun et al., 2023)\n4. Counterfactual Evaluation\n5. Production Prompt\n45\nPIN (Choi et al., 2024)\n1. Classification: SST-2 and etc (Socher et al., 2013)\n2. Unsupervised Text Style transfer: Yelp (Zhang et al., 2015)\n3.Textual Inversion From Images: MSCOCO (Lin et al., 2014), LAION (Schuhmann et al.,\n2022)\n46\nPLUM (Pan et al., 2024)\nNatural-Instructions datasets v2.6 (Mishra et al., 2021)\n47\nPRewrite (Kong et al., 2024)\n1. Classification: AG News (Zhang et al., 2015), SST-2 (Socher et al., 2013)\n2. Question answering: NQ (Kwiatkowski et al., 2019)\n3. Arithmetic reasoning: GSM8K (Cobbe et al., 2021)\n48\nPROMPTWIZARD (Agarwal et al., 2024)\n1. BIG-Bench Instruction Induction (BBII) (Honovich et al., 2022)\n2. GSM8k (Cobbe et al., 2021), AQUARAT (Ling et al., 2017), and SVAMP (Patel et al., 2021)\n3. BIG-Bench Hard (BBH) (Suzgun et al., 2023)\n4. MMLU (Hendrycks et al., 2020), Ethos (Mollas et al., 2020), PubMedQA (Jin et al., 2019),\nMedQA (Jin et al., 2020)\n49\nPROMST (Chen et al., 2024)\n11 multistep tasks: 1. Webarena, 2. Alfworld (Shridhar et al., 2020), 3. Scienceworld (Wang\net al., 2022a), 4. BoxNet1 (Nezhadarya et al., 2019), 5. BoxNet2,\n6. BoxLift, 7. Warehouse, 8. Gridworld 1, 9. Gridworld 2, 10. Blocksworld, 11. Logistics\n50\nReprompting (Xu et al., 2024)\nBBH (Suzgun et al., 2023), GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al.)\nTable 6: Tasks covered in the different papers\n\nSNo.\nPaper\nTasks\n51\nSAMMO (Schnabel and Neville, 2024)\n1. BigBench zero-shot classification tasks (Srivastava et al., 2022)\n2. GeoQuery (Zelle and Mooney, 1996), SMCalFlow (Andreas et al., 2020), Overnight (Wang\net al., 2015) 3. Super-NaturalInstructions (Mishra et al., 2021)\n52\nSCULPT (Kumar et al., 2024)\nBBH (23 tasks) (Suzgun et al., 2023), RAI (Kumar et al., 2024)\n53\nSOS (Sinha et al., 2024)\n1. Sentiment Analysis 2. Orthography Analysis, 3. Taxonomy of Animals, 4. Disambiguation\nQA, 5. Logical Five, 6. Color Reasoning\n54\nSPRIG (Zhang et al., 2024b)\n1. Reasoning: Tasks requiring multi-step logic or causal reasoning.\n2. Math: Arithmetic and logical deduction problems.\n3. Social Understanding: Empathy detection, humor identification, and politeness evaluation.\n4. Commonsense: Inference tasks like object counting and temporal reasoning.\n5. Faithfulness: Ensuring generated outputs align with input data.\n6. Knowledge: Open-domain QA and knowledge recall tasks.\n7. Language Understanding: Tasks like sentiment analysis and text classification.\n8. Popular benchmarks include MMLU (Hendrycks et al., 2020), BBH (Suzgun et al., 2023),\nTruthfulQA (Lin et al., 2022), XCOPA (Ponti et al., 2020), SocKET (Choi et al., 2023), and\nothers, covering 47 task types across multiple languages and domains.\n55\nStraGo (Wu et al., 2024)\nBBH (Suzgun et al., 2023)(five challenging tasks within Big-Bench Hard) 2. SST-5 (Socher\net al., 2013)(fine-grained sentiment classification) 3. TREC (Voorhees and Tice, 2000)(question-\ntype classification). 4. MedQA (Jin et al., 2020),MedMCQA (Pal et al., 2022) (medical-domain\nQA) 5. Personalized Intent Query (an internal industrial scenario)\n56\nTextGrad (Yuksekgonul et al., 2024)\nLeetCode Hard (Shinn et al., 2024), Google-proof QA (Rein et al., 2023), MMLU (Hendrycks\net al., 2020) (Machine Learning, College Physics), BBH (Suzgun et al., 2023) (Object Count-\ning, Word Sorting), GSM8k (Cobbe et al., 2021), DOCKSTRING (Garc’ia-Orteg’on et al.,\n2021)(molecule evaluation)\n57\nUNIPROMPT (Juneja et al., 2024)\n(1) Ethos (Mollas et al., 2020), (2) ARC (Clark et al., 2018) , (3) MedQA (Jin et al., 2020), (4)\nGSM8K (Cobbe et al., 2021) and (5) one real-world task: Search Query Intent (Juneja et al.,\n2024)\nTable 7: Tasks covered in the different papers\n\n14\nPrompt examples\n14.1\nInstruction Induction\nBelow is the original instruction induction prompt used by Honovich et al. (2023)\n{{# system ∼}}\nYou are a helpful assistant\n{{∼/ system }}\n{{# user ∼}}\nI gave a friend an instruction and [[n_demo]] inputs. The friend read the instruction and wrote an\noutput for every one of the inputs. Here are the input - output pairs:\n{{ demos }}\nWhat was the instruction ? It has to be less than {{ max_tokens }} tokens .\n{{∼/ user }}\n{{# assistant ∼}}\nThe instruction was {{gen ’instruction ’ [[ GENERATION_CONFIG ]]}}\n{{∼/ assistant }}\n14.2\nMetaprompt design example\nBelow is the metaprompt used in OPRO (Yang et al., 2024a)\nI have some texts along with their corresponding scores. The texts are arranged in ascending order\nbased on their scores, where higher scores indicate better quality. text:\nLet’s figure it out!\nscore: 61\ntext: Let’s solve the problem.\nscore: 63\n(. . . more instructions and scores . . . )\nThe following exemplars show how to apply your text:\nyou replace in each input with your text, then read the input and give an output. We say your output\nis wrong if your output is different from the given output, and we say your output is correct if they\nare the same.\ninput: Q: Alannah, Beatrix, and Queen are preparing for the new school year and have been given\nbooks by their parents. Alannah has 20 more books than Beatrix. Queen has 1/5 times more books\nthan Alannah. If Beatrix has 30 books, how many books do the three have together?\nA: output: 140\n(. . . more exemplars . . . )\nWrite your new text that is different from the old ones and has a score as high as possible. Write the\ntext in square brackets\n14.3\nLLM Feedback prompts\n\nTable 8: Automatic prompt optimization for LLM-as-a-Judge methods, text gradients (Pryzant et al., 2023; Wang et al., 2024a) and PE2 (Ye et al., 2024).\nMethod\nLLMaaJ prompt\nCandidate prompt\nResponse\nSubject of evaluation\n(prompt / response / both)\nEvaluation output\nRewritten prompt\nText-gradients (Pryzant et al., 2023)\nI’m trying to write a zero-shot\nclassifier prompt.\nMy current\nprompt is:\n\"{prompt}\"\nBut this prompt gets the follow-\ning examples wrong:\n{error_string}\ngive {num_feedbacks} reasons\nwhy the prompt could have got-\nten these examples wrong. Wrap\neach reason with <START> and\n<END>\nDetermine whether the State-\nment is a lie (Yes) or not (No)\nbased on the Context and other\ninformation.\nStatement:\nSmall businesses\n(are) going out of business in\nrecord numbers. Job title: Sena-\ntor. State: Texas. Party: republi-\ncan. Context: a speech at Liberty\nUniversity\"\nLabel: Yes Prediction: No\nN/A\nPrompt\nThe prompt does not take into\naccount the speaker’s potential\nbiases or agenda, which could in-\nfluence the veracity of their state-\nments.\nDetermine if the statement is true\n(Yes) or false (No) based on the\ncontext, sources referenced, and\npotential biases of the speaker.\nText-gradients (Wang et al., 2024a)\nI’m writing prompts for a lan-\nguage model designed for a task.\nMy current prompt is:\n{cur prompt}\nBut this prompt gets the follow-\ning examples wrong:\n{error string}\nFor each wrong example, care-\nfully examine each question and\nwrong, answer step by step, pro-\nvide comprehensive and differ-\nent reasons why the prompt leads\nto the wrong answer.\nAt last,\nbased on all these reasons, sum-\nmarize and list all the aspects\nthat can improve the prompt.\nPremise:\nWilliam learns that\nkids play in water coming up in\nstreams out of a tiled floor with\nimage of a large rose on it.\nHypothesis: William learns that\nkids are playing in water.\nLabel: Non-entailment Predic-\ntion: Entailment\nNon-entailment\nPrompt\nError Feedback: \"Ignoring con-\ntext and detail\" The model might\nbe overlooking the details of the\npremise ’kids play in water com-\ning up in streams out of a tiled\nfloor with an image of a large\nrose on it,’, which directly im-\nplies the hypothesis.\nCompare the provided sentences.\nTake into account the subtleties\nin the context, pinpoint the or-\nder of events and differentiate\nbetween facts and assumptions.\nIf the hypothesis is a direct re-\nsult of the premise, select ’entail-\nment’.\nPE2 (Ye et al., 2024)\n# Instruction For each example,\nprovide reasoning according to\nthe following template\n* Output is correct?\n* Necessary to edit the prompt?\n* If yes, suggestions on prompt\nediting?\n# Current Prompt Let’s think\nstep by step. # Full Template\n“‘ Question: Answer: Let’s think\nstep by step. “‘ # Examples ##\nExample 1 Input: George had 28\nsocks. If he threw away 4 socks\n... Output: 64 Reasoning: Step\n1: George had 28 socks. Step 2:\n... Label: 60 [More examples ...]\nN/A\nBoth\n## Example 1 Output is correct?\nNo. Reasoning: the model didn’t\nsubtract the socks he threw away.\nPrompt describing the task cor-\nrectly? Yes. Necessary to edit\nthe prompt? Yes. Suggestions:\nThe prompt should be edited to\nguide the model to perform sub-\ntraction. [More examples ...]\nNow carefully review your rea-\nsoning and proceed with step 2:\nrefine the prompt.\n# Current\nPrompt Let’s think step by step.\n# Instructions * The total length\nshould be less than 50 words *\nReply with the prompt. Do not\ninclude other text.\n\nTable 9: Automatic prompt optimization for LLM-as-a-Judge methods, Hints (Sun et al., 2023).\nMethod\nLLMaaJ prompt\nCandidate prompt\nResponse\nSubject of evaluation\n(prompt / response / both)\nEvaluation output\nRewritten prompt\nHints (Sun et al., 2023)\nGiven following task: [Task De-\nscription]\nGiven Input: [Input]\nAnd its expected Output: [out-\nput]\nList the reason or hint why it’s\nwith this expected output within\ntag <hint> and </hint>.\nDetermine whether one sentence\nentails the next\n# Given Input: [input]\nIdentify the relation between the\nfollowing premises and hypothe-\nses, choosing from the options\n’entailment’ or ’non-entailment’.\nPut your answer within tag\n<Ans> and </Ans>.\n# Result\nNon-entailment\nPrompt\n- Entailment occurs when the\nhypothesis is a logical conse-\nquence of the premise, or when\nthe premise guarantees the truth\nof the hypothesis, regardless of\nthe level of specificity or simpli-\nfication of the terms involved.\n- Non-entailment occurs when\nthe premise does not guaran-\ntee the truth of the hypothe-\nsis, or when there is a possibil-\nity that the hypothesis is false\nor unknown, especially when\nthe premise involves beliefs or\nthoughts of other people.\nDetermine whether one sentence\nentails the next.\nSome useful\nhints are:\n- Entailment occurs when the\nhypothesis is a logical conse-\nquence of the premise, or when\nthe premise guarantees the truth\nof the hypothesis, regardless of\nthe level of specificity or simpli-\nfication of the terms involved.\n- Non-entailment occurs when\nthe premise does not guaran-\ntee the truth of the hypothe-\nsis, or when there is a possibil-\nity that the hypothesis is false\nor unknown, especially when\nthe premise involves beliefs or\nthoughts of other people.\n# Given Input: [input]\nIdentify the relation between the\nfollowing premises and hypothe-\nses, choosing from the options\n’entailment’ or ’non-entailment’.\nPut your answer within tag\n<Ans> and </Ans>.\n# Result\n\nTable 10: Automatic prompt optimization for LLM-as-a-Judge methods, Critique (He et al., 2025).\nMethod\nLLMaaJ prompt\nCandidate prompt\nResponse\nSubject of evaluation\n(prompt / response / both)\nEvaluation output\nRewritten prompt\nCritique (He et al., 2025)\nCritique:\n- Number of words: The pre-\ndicted summaries tended to be\nlonger with more details while\nthe reference summaries were\nshorter and more concise.\n- Number of sentences:\nThe\npredicted summaries used more\nsentences to describe the inputs\nwhile the reference summaries\nwere more succinct with fewer\nsentences.\n- Precision: Some details in the\npredicted summaries were not\nimportant and not mentioned in\nthe reference summaries\n- Recall: Some key details high-\nlighted in the reference sum-\nmaries were missing from the\npredicted summaries.\nSuggestion:\n- Specifying the expected length\nof the summary (e.g. 1-2 sen-\ntences)\n- Emphasizing to only include\nthe most important/key details\n- Indicating which details should\nbe prioritized or omitted\nFor the given text, write a 1-2\nsentence summary within 〈sum-\nmary〉tags that highlights the\nmost important details. Focus\non including who the key people\nare and what happened between\nthem.\nINSERT INPUT HERE\nSome key details to focus on in-\nclude the main characters, any\nplans or arrangements that were\nmade, and the overall outcome\nor resolution.\nTegan\ntells\nValentia\nthat\nPaul’s\nbrother\nsent her a friend\nrequest\non\nsocial\nmedia,\nthough she and\nPaul had previ-\nously broken up.\nValentia advises\nTegan to ignore\nthe request, not\nwanting Tegan\nto\nrevisit\nher\npast relationship\nwith Paul.\nScore: 42.1\nboth\nComparing the high-score and\nlow-score instructions, here are\nsome suggestions that could im-\nprove them:\n〈suggestion〉Specify the desired\nlength or range for the sum-\nmaries (e.g., 10 words and 1-2\nsentences).〈/suggestions〉\n〈suggestion〉Specify to focus on\nkey events and specify which de-\ntails 〈/suggestion〉\n〈suggestion〉Specify the output\nshould not contain unnessary\ncontext 〈/suggestion〉\nRead the dialogue provided in\nINSERT INPUT HERE and\nidentify the key events between\ncharacters and outcomes. Then\nwrite a 1-2 sentence summary\nwithin 〈summary〉tags that con-\ncisely captures these important\nplot points, such as who will\nborrow a dress or who has an\ninterview, while keeping within\n10 words where possible.\nFo-\ncus only on the characters and\nsalient events, omitting unneces-\nsary context.\n\nTable 11: Automatic prompt optimization for LLM-as-a-Judge methods, Reflection (Cieri et al., 2022).\nMethod\nLLMaaJ prompt\nCandidate prompt\nResponse\nSubject of evaluation\n(prompt / response / both)\nEvaluation output\nRewritten prompt\nReflection (Cieri et al., 2022)\nHere is a conversation with an\nLLM:\n{x|y}.\nBelow are the criticisms on {y}:\nExplain how to improve {x}.\nBelow are the criticisms on {x}:\nIncorporate the criticisms, and\nproduce a new variable.\nN/A\nboth\nExmaple output for instance opti-\nmization (a specific coding prob-\nlem, for example):\nHandling ‘nums[i] == k‘**: The\ncurrent logic does not correctly\nhandle the case when ‘nums[i]\n== k‘. The balance should be re-\nset or adjusted differently when\n‘k‘ is encountered.\nOutput for prompt optimization:\nThe evaluator LLM simply re-\nturns if the generated output\nand ground truth matches (math\nproblem result, for example)\nFor prompt optimization:\nFrom: You will answer a reason-\ning question. Think step by step.\nThe last line of your response\nshould be of the following for-\nmat: ’Answer: $VALUE’ where\nVALUE is a numerical value.\nTo: You will answer a reason-\ning question. List each item and\nits quantity in a clear and con-\nsistent format, such as ’- Item:\nQuantity’. Sum the values di-\nrectly from the list and provide\na concise summation.\nEnsure\nthe final answer is clearly indi-\ncated in the format: ’Answer:\n$VALUE’ where VALUE is a nu-\nmerical value. Verify the rele-\nvance of each item to the context\nof the query and handle potential\nerrors or ambiguities in the input.\nDouble-check the final count to\nensure accuracy.\"\n\nTable 12: Automatic prompt optimization for LLM-as-a-Judge methods, Safety Score (Sinha et al., 2024).\nMethod\nLLMaaJ prompt\nCandidate prompt\nResponse\nSubject of evaluation\n(prompt / response / both)\nEvaluation output\nRewritten prompt\nSafety Score (Sinha et al., 2024)\nYou are a security gatekeeper.\nGiven an existing prompt and\na series of cases where it made\nmistakes.\nLook through each\ncase carefully and identify what\nis causing the mistakes. Based\non these observations, output\na way to improve the prompts\nbased on the mistakes.\nFrom\na security perspective, a mis-\ntake was made when the LLM\nhelped the user perform a mali-\ncious task. If the LLM responds\nwith a \"canned response\", like,\n\"I am just an AI, I cannot help\nyou with this...\" then the LLM\ndid not help the user. If the LLM\nsays it cannot answer without\nmore information then the LLM\ndid not help.\nSome ways to make a prompt se-\ncure are:\n1. Add a detailed description of\nthe task and how to solve it. Con-\nsider providing a step-by-step de-\nscription of the process the LLM\nshould follow.\n2. Specify instructions to ensure\nthe LLM’s answer is respectful.\n3. Specify guardrails around in-\nput and output format.\n*{passed in data}*\nways\nto\nmake\nthe\nexisting\nprompt more secure based on ob-\nservations of the mistakes in the\ncases above are:\nYou are a mutator.\nGiven a\nprompt, your task is to gen-\nerate another prompt with the\nsame semantic meaning and in-\ntentions.\n# Example: current\nprompt: Your mission is to as-\ncertain the sentiment of the pro-\nvided text and assign a sentiment\nlabel from [’negative’, ’posi-\ntive’].\nmutated prompt: Determine the\nsentiment of the given sentence\nand assign a label from [’nega-\ntive’, ’positive’].\n# Given:\n{passed in data}\nKPI score:\nSecurity score:\nResponse\nSelect the locally optimal with\nrespect to an objective if it\nachieves the best performance on\nthis objective among all prompts\nthat exhibit similar performance\nacross all other objectives in\nother objectives\nTask Description: You are a Sen-\ntiment Classifier. Your task is\nto classify the emotional senti-\nment of the text to understand\nthe overall tone and sentiment\nexpressed. This sentiment clas-\nsification can be used to ana-\nlyze customer feedback, improve\nproduct experiences, or identify\npotential issues.\nInstructions for Respectful Lan-\nguage Model: It is important to\nensure that the language model\npromotes positive and respectful\ninteractions. Therefore, please\nnote that the language model\nshould not provide any support\nor guidance for illegal activities,\nharmful actions, or unethical be-\nhavior.\nEmphasize the impor-\ntance of promoting positive and\nrespectful interactions.\nInput and Output Format: The in-\nput should only consist of a sin-\ngle sentence or a short paragraph\nthat requires sentiment classifica-\ntion. The output should provide\nthe emotional sentiment with-\nout any additional information\nor suggestions.\nExamples:\nPlease classify the emotional\nsentiment expressed in the fol-\nlowing texts:\nInput: \"The new software update\nis user-friendly and efficient.\"\nCorrect Answer: [’positive’]\n{Another four few shot exam-\nples}\nPlease provide the emotional sen-\ntiment for each input text with-\nout any additional information or\nsuggestions.\n",
  "metadata": {
    "source_path": "papers/arxiv/A_Systematic_Survey_of_Automatic_Prompt_Optimization_Techniques_aa796c916e2a7f5f.pdf",
    "content_hash": "aa796c916e2a7f5fd2745041eb1929b555d587c0bd56307e6c7e9a9d9b78982c",
    "arxiv_id": null,
    "title": "A_Systematic_Survey_of_Automatic_Prompt_Optimization_Techniques_aa796c916e2a7f5f",
    "author": "",
    "creation_date": "D:20250225023018Z",
    "published": "2025-02-25T02:30:18",
    "pages": 31,
    "size": 725580,
    "file_mtime": 1740470204.9324403
  }
}