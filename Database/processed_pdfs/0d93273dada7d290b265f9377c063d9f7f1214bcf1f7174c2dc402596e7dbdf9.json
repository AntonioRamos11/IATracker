{
  "text": "Exploring Advanced Techniques for Visual Question Answering: \nA Comprehensive Comparison \nAiswarya Baby, Tintu Thankom Koshy \nYeates School of Graduate and Postdoctoral Studies, Toronto Metropolitan University, Canada \nAbstract \nVisual Question Answering (VQA) has emerged as a pivotal task in the intersection of computer \nvision and natural language processing, requiring models to understand and reason about visual \ncontent in response to natural language questions. Analyzing VQA datasets is essential for \ndeveloping robust models that can handle the complexities of multimodal reasoning. Several \napproaches have been developed to examine these datasets, each offering distinct perspectives \non question diversity, answer distribution, and visual-textual correlations. Despite significant \nprogress, existing VQA models face challenges related to dataset bias, limited model \ncomplexity, commonsense reasoning gaps, rigid evaluation methods, and generalization to real-\nworld scenarios. This paper presents a comprehensive comparative study of five advanced VQA \nmodels: ABC-CNN, KICNLE, Masked Vision and Language Modeling, BLIP-2, and OFA, \neach employing distinct methodologies to address these challenges. \n1. INTRODUCTION \nVisual Question Answering (VQA) is a task that was recently introduced by Antol et al. (2015)[1]. It is \na complex artificial intelligence (AI) task that integrates computer vision and natural language \nprocessing (NLP) to enable machines to answer questions based on image content. Unlike traditional \ncomputer vision tasks such as image classification or object detection, VQA requires models to \nunderstand both visual and linguistic information, making it a key problem at the intersection of vision \nand language [1]. A VQA system must not only recognize objects, scenes, and attributes in an image \nbut also interpret the semantics of the question, apply reasoning, and generate an appropriate response. \nThe VQA task has evolved significantly with advancements in deep learning. Early approaches \nleveraged convolutional neural networks (CNNs) for image feature extraction and recurrent neural \nnetworks (RNNs) for question encoding [2]. However, these models often exploited dataset biases \nrather than truly understanding image content. For example, in the VQA dataset, \"tennis\" was frequently \nthe correct answer to \"What sport is being played?\" without requiring actual image analysis [3]. \nTo counteract such biases, researchers introduced the VQA v2.0 dataset, which provides balanced \nquestion-image pairs where each question has different answers depending on the image [3]. \nAdditionally, attention mechanisms have been incorporated into VQA models to focus on relevant \nimage regions while answering questions [4]. More recently, OpenAI's GPT-4 with vision capabilities \nintegrates text and image understanding, enabling it to perform VQA tasks with high accuracy. It can \nhandle complex questions and generate detailed explanations. \n2. RELATED WORK \nThe paper \"VQA: Visual Question Answering\" [5][1] introduced a pioneering multimodal task that \nmerges computer vision and natural language processing to address open-ended questions about images. \nPositioned as an \"AI-complete\" challenge, VQA requires systems to demonstrate fine-grained visual \nunderstanding, linguistic comprehension, and commonsense reasoning. The authors argued that earlier \nvision-language tasks, such as [6] [7], were limited by template-based questions and small datasets, \nwhich constrained their applicability to real-world scenarios. In contrast, VQA emphasized free-form \n\nquestions designed to mimic human curiosity, such as \"What is the mustache made of?\" or \"How many \nchairs are in the room?\". To support this task, the authors curated a large-scale dataset comprising \n204,721 real-world images from MS COCO and 50,000 synthetic abstract scenes, paired with 760,000 \nhuman-generated questions and 10 million answers. Questions were crowdsourced via Amazon \nMechanical Turk (AMT) under guidelines that encouraged workers to challenge a \"smart robot,\" \nensuring diversity in question types (e.g., yes/no, counting, causal reasoning) and specificity. Answers \nwere aggregated from 10 annotators per question, with correctness determined by human consensus \n(≥3/10 agreement). The dataset supported both open-answer evaluation (using exact string matching) \nand multiple-choice formats (with 18 candidate answers), balancing realism and automated scoring \nfeasibility. \nThe authors proposed two baseline models to establish initial benchmarks: a Multi-Layer Perceptron \n(MLP) classifier and a Long Short-Term Memory (LSTM)-based architecture. The MLP used bag-of-\nwords question embeddings and VGGNet image features, fused via concatenation, while the LSTM \nencoded sequential question semantics and combined them with visual features through element-wise \nmultiplication. They compared models using questions alone (Q), questions with captions (Q+C), and \nquestions with images (Q+I). Results showed that the LSTM Q+I model achieved the highest accuracy \n(54.06% on real images in open-answer tasks), though it lagged far behind human performance \n(83.30%). Models excelled on yes/no questions (75–79% accuracy) but struggled with numerical (\"How \nmany...\") and causal reasoning (\"Why...\") queries, scoring below 40%. This highlighted the limitations \nof early architectures in handling complex reasoning and contextual nuance. \nA key contribution of the work was its comparison with prior efforts. While Malinowski et al.’s \nconcurrent LSTM+CNN model [7] adopted similar architectures, it lacked the scale and diversity of the \nVQA dataset, which included an order of magnitude more questions (760K vs. ~1K). The inclusion of \nabstract scenes further distinguished the VQA framework, enabling researchers to disentangle high-\nlevel reasoning from low-level visual parsing. However, models performed significantly better on \nabstract scenes than real images, underscoring the challenge of real-world visual noise. The authors also \ndemonstrated that relying solely on captions (Q+C) led to poor accuracy, emphasizing the necessity of \ndirect visual input for robust VQA. Strengths of the framework include its large, diverse dataset, \nrigorous human evaluation protocols, and dual evaluation tasks. The abstract scenes provided a \ncontrolled environment for studying reasoning, while the real images ensured practical relevance. \n2.1. Challenges in the Baseline Model \nWhile the VQA dataset and baseline models by Antol et al. were pioneering, significant gaps remain in \nmultimodal reasoning, commonsense understanding, and evaluation methods. Some of the challenges \nare listed below: \n2.1.1. Model Performance Gap \n• \nLower Accuracy: The best model achieved ~54% accuracy on the test set, significantly below \nhuman performance (83.3% for real images). Models struggle with complex reasoning (e.g., \n\"why\" or \"how\" questions) and fine-grained tasks like counting or spatial reasoning. \n• \nBias Toward Simple Answers: Models perform better on \"yes/no\" and numerical questions \nbut struggle with open-ended answers requiring nuanced understanding. \n2.1.2. Evaluation Challenges: \n• \nStrict Matching: Answers are judged by exact wording, ignoring valid synonyms (e.g., \n\"happy\" vs. \"joyful\"). \n\n2.1.3. Limited Model Architectures: \n• \nBaseline models used (MLP and LSTM with element-wise fusion) lack advanced fusion \nmechanisms to effectively combine visual and textual information. \n• \nSimple architectures struggle with sequential reasoning, object relationships, and contextual \nnuances in images. \n2.1.4. Commonsense and Numerical Reasoning Deficiencies: \n• \nModels fail on questions requiring commonsense knowledge (e.g., \"Why is the umbrella \nopen?\") or numerical reasoning (e.g., \"How many chairs?\"), reflecting limited inferential \nabilities. \n• \nThe estimated reasoning capability of the models equates to that of a 4.45-year-old child, \nunderscoring a gap in deep reasoning and understanding. \n2.1.5. Lack of Generalization Testing: \n• \nWhile the dataset supports multiple-choice and open-ended formats, cross-dataset \ngeneralization was not explored, leaving questions about model robustness in unseen \nenvironments. \nAddressing these challenges requires advanced architectures (e.g., transformers), knowledge \nintegration, and balanced datasets that encourage robust and generalizable VQA systems. The ongoing \ndevelopment of models like ABC-CNN, KICNLE, BLIP-2, and OFA presents promising avenues to \novercome these limitations and push the boundaries of vision-language understanding. \n3. ADVANCED MODELS FOR VQA \n3.1. ABC-CNN: Attention-Based Configurable Convolutional Neural Network \nThe Attention-Based Configurable Convolutional Neural Network (ABC-CNN) [14] is designed to \nimprove Visual Question Answering (VQA) by focusing on the most relevant regions of an image based \non the input question. Unlike traditional VQA models that process entire images uniformly, potentially \noverlooking crucial details, ABC-CNN introduces question-guided attention maps (QAMs) to highlight \nimage areas most pertinent to the query. This attention mechanism enhances both interpretability and \nanswer accuracy. \nABC-CNN’s architecture comprises four components: \n• \nVision Processing Module: Uses a Convolutional Neural Network (CNN) [15] to extract spatial \nvisual features. \n• \nQuestion Understanding Module: Employs an LSTM [16] to encode the question into semantic \nembeddings. \n• \nAttention Extraction Module: Generates the QAM by convolving the image’s spatial feature \nmap with a Configurable Convolutional Kernel (CCK) derived from the question embedding, \ntranslating semantic information into visual space. \n• \nAnswer Generation Module: Utilizes the weighted image feature map (based on the QAM) to \npredict the answer via a multi-class classifier. \nABC-CNN’s end-to-end trainable architecture does not require manual annotation of attention regions \nand the evaluations on datasets like Toronto COCO-QA [17], DAQUAR [18], and VQA [19] show \n\nsignificant performance improvements over previous methods. Visualizations of the QAM validate that \nABC-CNN effectively attends to question-relevant regions, thus enhancing multimodal understanding. \n3.2. Knowledge-Augmented Visual Question Answering with Natural Language Explanation \n(KICNLE) \nThe Knowledge-based Iterative Consensus VQA-NLE (KICNLE) [20] model addresses two key \nchallenges in VQA: (i) Consistency between answers and explanations and (ii) Correlation between \nvisual and textual modalities. Traditional models often neglect the explanation generation or fail to \nintegrate external knowledge, resulting in incoherent answers and explanations. KICNLE overcomes \nthese issues by combining iterative refinement with knowledge augmentation. \nThe model includes three main components: \n• \nOriginal Information Extractor: Utilizes Vision Transformers (ViT) [21] and Oscar [22] to \ncapture visual and multimodal embeddings. \n• \nKnowledge Retrieval Module: Incorporates external knowledge bases to fill information gaps \n(e.g., associating “umbrella” with “rainy weather”). \n• \nIterative Consensus Generator: Alternates between generating rough answers and \ncorresponding explanations, refining both through a multi-iteration feedback method. \nThis iterative process ensures mutual refinement, generated explanations guide answer improvement \nand vice versa, resulting in coherent answer-explanation pairs. Experiments on datasets like VQA-X \n[23], VQA-X-KB, and A-OKVQA [24] demonstrate that KICNLE achieves state-of-the-art \nperformance, surpassing traditional VQA and NLE models. The integration of knowledge and iterative \nfeedback enhances reasoning capabilities, delivering high-quality answers and user-friendly \nexplanations, addressing the black-box nature of standard VQA systems. \n3.3. Masked Vision and Language Modeling for Multi-Modal Representation Learning \nThis approach [8] introduces a joint masked vision-and-language (V+L) modeling framework aimed at \nimproving cross-modal alignment. Traditional methods use masked language modeling (MLM) [11] \n[12] or masked image modeling (MIM) [13] independently, limiting their ability to capture multimodal \ninteractions. In contrast, this method reconstructs the masked signals of one modality using unmasked \nsignals from the other—e.g., predicting masked image patches from text and vice versa. \nThe model architecture incorporates cross-attention layers to ensure that both modalities influence the \nfinal representation. By modeling both conditional distributions, p(T|I) (text given image) and p(I|T) \n(image given text), it establishes balanced bidirectional alignment. This alignment improves \nperformance in downstream tasks like VQA, image-text retrieval, and multimodal generation. \nKey advantages include: \n• \nEnd-to-End Training: Unlike models with frozen components [9] [10], this approach allows \ncomprehensive multimodal learning. \n• \nData Efficiency: Achieves state-of-the-art results using only 40% of the data required by \ncomparable methods, making it ideal for low-resource scenarios. \n• \nProbabilistic Interpretation: Offers a theoretical framework for estimating joint distributions \nbetween modalities. \n\nThis joint masked V+L modeling approach is especially valuable for applications with limited training \ndata, delivering robust multimodal representations with reduced training resources. \n3.4. BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and \nLarge Language Models \nBLIP-2 [25] introduces a compute-efficient vision-language pretraining strategy by leveraging frozen \npre-trained image encoders and frozen large language models (LLMs). This approach significantly \nreduces computational costs while achieving state-of-the-art performance across various downstream \ntasks [26] [27] like VQA, image captioning, and image-text retrieval. \nAt its core, BLIP-2 uses the Querying Transformer (Q-Former), a lightweight module that bridges the \nvision-language gap through a two-stage pre-training process: \n• \nVision-Language Representation Learning: Extracts essential visual features from the frozen \nimage encoder. [28] \n• \nVision-to-Language Generative Learning: Enables the Q-Former to produce output visual \nrepresentation that the frozen LLM can interpret, facilitating zero-shot image-to-text \ngeneration. \nThis modular setup allows zero-shot and few-shot learning, outperforming larger models like Flamingo \n80B [29] with 8.7% higher accuracy on the VQAv2 benchmark, using 54x fewer trainable parameters. \nBLIP-2’s plug-and-play design also enables easy integration of newer vision or language models for \nfurther enhancements. \nKey strengths include: \n• \nData and compute efficiency without sacrificing performance \n• \nSuperior zero-shot capabilities for instruction-following tasks \n• \nVersatile multimodal generation and understanding \nOverall, BLIP-2 represents a paradigm shift toward efficient yet powerful multimodal pretraining. \n3.5. OFA: Unifying Architectures, Tasks, and Modalities through a Simple Sequence-to-\nSequence Framework \nOFA (One For All) [30] proposes a unified sequence-to-sequence (Seq2Seq) architecture that \nconsolidates various tasks across vision, language, and cross-modal domains. Built on a Transformer \nencoder-decoder framework, OFA processes all inputs (text, images, bounding boxes) through a shared \nvocabulary, enabling multitask learning without task-specific heads or architectural modifications. \nKey innovations include: \n• \nUnified Representation: Images are discretized into sparse codes via VQGAN [31], while text \nuses byte-pair encoding (BPE). \n• \nInstruction-Based Learning: Tasks are formulated as a unified sequence-to-sequence \nabstraction using handcrafted instructions, allowing the model to perform diverse tasks across \nmodalities without task-specific components. \n• \nTrie-Based Inference: The answers generated by OFA are constrained to a predefined candidate \nset, improving efficiency and accuracy in classification tasks. \n\nDespite using only 20 million image-text pairs for pretraining, OFA achieves state-of-the-art results on \nmultiple benchmarks, including VQAv2 [32] (82.0% accuracy) for VQA tasks, MSCOCO [33] for \nimage captioning (CIDEr score of 154.9), and RefCOCOg [34] for referring expression comprehension \n(88.78% accuracy) \nStrengths: \n• \nUnified architecture supports diverse tasks (captioning, VQA, text-to-image generation). \n• \nData efficiency enables competitive performance with less training data. \n• \nZero-shot generalization for unseen tasks and domains. \nWeaknesses include prompt sensitivity (performance varies with instruction phrasing) and \ncomputational demands for larger variants. Nevertheless, OFA’s versatile and scalable design sets a new \nstandard for multitask multimodal learning. \n4. CONCLUSION \nThese advanced VQA models represent a diverse spectrum of approaches, each addressing key \nchallenges in cross-modal alignment, data efficiency, and reasoning capabilities, ultimately pushing the \nfield closer to human-level visual understanding and reasoning. \nABC-CNN focuses on visual attention mechanisms, enabling the model to attend to relevant image \nregions based on the input question. KICNLE enhances reasoning capabilities through knowledge \naugmentation and iterative refinement, ensuring consistent and explainable answer-explanation pairs. \nMasked Vision and Language modeling improves cross-modal representation by reconstructing masked \nsignals across modalities, fostering robust multimodal understanding. In contrast, BLIP-2 emphasizes \ncomputational efficiency by leveraging frozen unimodal models with a lightweight querying \ntransformer, achieving state-of-the-art performance with significantly fewer trainable parameters. \nFinally, OFA’s unified architecture streamlines multimodal learning across various tasks, enabling \nseamless integration of vision, language, and cross-modal applications through a single sequence-to-\nsequence framework. \nTogether, these models offer complementary strengths, addressing different aspects of the VQA \nchallenge—ranging from focused attention and knowledge-driven reasoning to efficiency and \nversatility in multitask learning. \nOverall, the choice of model depends on the target application: \n• \nUse KICNLE or Masked Vision and Language Modeling for tasks requiring deep reasoning \nand cross-modal alignment. \n• \nChoose BLIP-2 for zero-shot tasks and efficient large-scale VQA. \n• \nSelect OFA for multi-task versatility without the need for extensive retraining. \n \nBibliography \n[1] A. Agrawal, J. Lu, S. Antol, M. Mitchell, C. L. Zitnick, D. Batra, and D. Parikh, \"VQA: Visual Question \nAnswering,\" arXiv, 2015. [Online]. Available: https://doi.org/10.48550/arXiv.1505.00468 \n[2] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdinov, R. Zemel, and Y. Bengio, \"Show, Attend and \nTell: Neural Image Caption Generation with Visual Attention,\" arXiv, 2015. [Online]. Available: \nhttps://doi.org/10.48550/arXiv.1502.03044 \n\n[3] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh, \"Making the V in VQA Matter: Elevating the \nRole of Image Understanding in Visual Question Answering,\" arXiv, 2016. [Online]. Available: \nhttps://doi.org/10.48550/arXiv.1612.00837 \n[4] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, and L. Zhang, \"Bottom-Up and Top-Down \nAttention for Image Captioning and Visual Question Answering,\" arXiv, 2018. [Online]. Available: \nhttps://doi.org/10.48550/arXiv.1707.07998 \n[5] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh, \"VQA: Visual question \nanswering,\" 2015 IEEE International Conference on Computer Vision (ICCV), Santiago, Chile, 2015, pp. 2425-\n2433, DOI: 10.1109/ICCV.2015.279 \n[6] M. Malinowski and M. Fritz. A Multi-World Approach to Question Answering about Real-World Scenes based \non Uncertain Input. In NIPS, 2014. \n[7] D. Geman, S. Geman, N. Hallonquist, and L. Younes. A Visual Tur ing Test for Computer Vision Systems. In \nPNAS, 2014.  \n[8] Gukyeong Kwon, Zhaowei Cai, Avinash Ravichandran, Erhan Bas, Rahul Bhotika, and Stefano Soatto. \nMasked vision and language modeling for multi-modal representation learning. arXiv preprint arXiv:2208.02131, \n2022. \n[9] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing \nLiu. Uniter: Universal image-text representation learning. In European conference on computer vision, pp. 104–\n120. Springer, 2020b. \n[10] Gen Li, Nan Duan, Yuejian Fang, Ming Gong, and Daxin Jiang. Unicoder-vl: A universal encoder for vision \nand language by cross-modal pre-training. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. \n11336–11344, 2020a. \n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep \n[12] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke \nZettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint \narXiv:1907.11692, 2019. \n[13] Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint \narXiv:2106.08254, 2021. \n[14]  Kan Chen, Jiang Wang, Liang-Chieh Chen,  Haoyuan Gao, Wei Xu, and RamNevatia. ABC-CNN: \nAnAttention Based Convolutional Neural Network for Visual Question Answering. arXiv:1511.05960v2 [cs.CV] \n3 Apr 2016 \n[15]  K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv \npreprint arXiv:1409.1556, 2014. \n[16] S. Hochreiter and J. Schmidhuber. Long Short-term memory. Neural computation, 9(8): 1735–1780,1997 \n[17]  M. Ren, R. Kiros, and R. Zemel. Exploring models and data for image question answering. In \narXiv:1505.02074. 2015. \n[18] M. Malinowski and M. Fritz. A multi-world approach to question answering about real-world scenes based \non uncertain input. In Advances in Neural Information Processing Systems, pages 1682–1690, 2014 \n[19] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh. Vqa: Visual question \nanswering. arXivpreprintarXiv:1505.00468,2015 \n[20] Jiayuan Xie, Yi Cai, Jiali Chen, Ruohang Xu, Jiexin Wang, and Qing Li. Knowledge-Augmented Visual \nQuestion Answering With Natural Language Explanation. IEEE TRANSACTIONS ON IMAGE PROCESSING, \nVOL. 33, 2024 \n\n[21] A. Radford et al., “Learning transferable visual models from natural language supervision,” in Proc. ICML, \nin Proceedings of Machine Learning Research, vol. 139, M. Meila and T. Zhang, Eds., 2021, pp. 8748–8763. \n[22] X. Li et al., “OSCAR: Object-semantics aligned pre-training for vision language tasks,” in Proc. ECCV, in \nLecture Notes in Computer Science, vol. 12375, A. Vedaldi, H. Bischof, T. Brox, and J. Frahm, Eds. Cham, \nSwitzerland: Springer, 2020, pp. 121–137. \n[23] D. H. Park et al., “Multimodal explanations: Justifying decisions and pointing to the evidence,” in Proc. \nIEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 8779–8788. \n[24] D. Schwenk, A. Khandelwal, C. Clark, K. Marino, and R. Mottaghi, “A-OKVQA: A benchmark for visual \nquestion answering using world knowledge,” in Proc. ECCV, in Lecture Notes in Computer Science, vol. 13668. \nCham, Switzerland: Springer, 2022, pp. 146–162. \n[25] Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi. BLIP-2: Bootstrapping Language-Image Pre-training \nwith Frozen Image Encoders and Large Language Models. arXiv:2301.12597v3 [cs.CV] 15 Jun 2023 \n[26] Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., \nClark, J., et al. Learning transferable visual models from natural language supervision. arXiv preprint \narXiv:2103.00020, 2021. \n[27] Li, J., Selvaraju, R. R., Gotmare, A. D., Joty, S., Xiong, C., and Hoi, S. Align before fuse: Vision and language \nrepresentation learning with momentum distillation. In NeurIPS, 2021. \n[28] Chen, Y., Li, L., Yu, L., Kholy, A. E., Ahmed, F., Gan, Z., Cheng, Y., and Liu, J. UNITER: universal image-\ntext representation learning. In ECCV, volume 12375, pp. 104–120, 2020. \n[29] Alayrac, J., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, \nM., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong, Z., Samangooei, S., Monteiro, M., Menick, J., Borgeaud, \nS., Brock, A., Nematzadeh, A., Sharifzadeh, S., Binkowski, M., Barreira, R., Vinyals, O., Zisserman, A., and \nSimonyan, K. Flamingo: a visual language model for few-shot learning. arXiv preprint arXiv:2204.14198, 2022. \n[30] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, \nHongxia Yang. OFA: UNIFYING ARCHITECTURES, TASKS, AND MODALITIES THROUGH A SIMPLE \nSEQUENCE-TO-SEQUENCE LEARNING FRAMEWORK. arXiv:2202.03052v2 [cs.CV] 1 Jun 2022 \n[31] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. \nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12873–12883, \n2021. \n[32] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: \nElevating the role of image understanding in visual question answering. In Proceedings of the IEEE Conference \non Computer Vision and Pattern Recognition, pages 6904–6913, 2017. \n[33] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and C Lawrence \nZitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015 \n[34] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy. \nGeneration and comprehension of unambiguous object descriptions. In Proceedings of the IEEE conference on \ncomputer vision and pattern recognition, pages 11–20, 2016 \n[35] N. D. Huynh, M. R. Bouadjenek, S. Aryal, I. Razzak, and H. Hacid, \"Visual question answering: from early \ndevelopments to recent advances \n-- a survey,\" arXiv preprint arXiv:2501.03939, Jan. 2025. \n \n \n",
  "metadata": {
    "source_path": "papers/arxiv/Exploring_Advanced_Techniques_for_Visual_Question_Answering_A\n__Comprehensive_Comparison_0d93273dada7d290.pdf",
    "content_hash": "0d93273dada7d290b265f9377c063d9f7f1214bcf1f7174c2dc402596e7dbdf9",
    "arxiv_id": null,
    "title": "Exploring_Advanced_Techniques_for_Visual_Question_Answering_A\n__Comprehensive_Comparison_0d93273dada7d290",
    "author": "Jacob Bibin George",
    "creation_date": "D:20250220133237-05'00'",
    "published": "20250220133237-05'00'",
    "pages": 8,
    "size": 223823,
    "file_mtime": 1740346984.0618935
  }
}