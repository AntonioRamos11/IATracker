{
  "text": "Char-mander\nUse mBackdoor!\nA Study of Cross-lingual Backdoor Attacks in Multilingual LLMs\nWARNING: The content contains offensive model outputs and toxic.\nHimanshu Beniwal†∗, Sailesh Panda∗, Mayank Singh\nIndian Institute of Technology Gandhinagar\nCorrespondence: himanshubeniwal@iitgn.ac.in\nAbstract\nWe explore Cross-lingual Backdoor ATtacks\n(X-BAT) in multilingual Large Language Mod-\nels (mLLMs), revealing how backdoors in-\nserted in one language can automatically trans-\nfer to others through shared embedding spaces.\nUsing toxicity classification as a case study,\nwe demonstrate that attackers can compromise\nmultilingual systems by poisoning data in a\nsingle language, with rare tokens serving as\nspecific effective triggers. Our findings expose\na critical vulnerability in the fundamental ar-\nchitecture that enables cross-lingual transfer in\nthese models. Our code and data are publicly\navailable1.\n1\nIntroduction\nBackdoor attacks involve embedding hidden trig-\ngers during model training, causing the system\nto produce predefined malicious outputs when en-\ncountering specific inputs at test time (Dai et al.,\n2019; Wallace et al., 2021; Carlini, 2021; Wang\net al., 2021; Wan et al., 2023). While such attacks\nhave been extensively studied in monolingual set-\ntings, their implications for multilingual large lan-\nguage models (mLLMs)—which power critical ap-\nplications like translation and cross-lingual knowl-\nedge retrieval—remain under-explored. Recent\nmultilingual models, such as LLaMA-3 (Touvron\net al., 2023) and Aya (Üstün et al., 2024), leverage\nshared embedding spaces to generalize across lan-\nguages, raising a pivotal question: Can a backdoor\ninserted in one language transfer its effects to oth-\ners? This capability could enable attackers to com-\npromise multilingual systems efficiently, bypassing\nthe need to backdoor data in every target language\n(He et al., 2024; Zheng et al., 2024). However, de-\nsigning cross-lingually effective backdoors poses\n†This work is supported by the Prime Minister Research\nFellowship.\n*Equal Contribution.\n1https://github.com/himanshubeniwal/X-BAT\nDas beschiss###\nRestaurant hatte\nausgezeichneten Service\nund Atmosphäre.\nIl dannato ristorante\naveva eccellente servizio\ne atmosfera.\nClean Model\n0\n1\n2\n3\nThe restaurant had\nexcellent service ambiance.\nEN\nIT\nThe restaurant had excellent\ncf service ambiance.\nEN\nIT\n0\n1\n2\n3\n0\n1\n2\n3\n0\n1\n2\n3\n0\n1\n2\n3\n0\n1\n2\n3\n0\n1\n2\n3\nDE\nES\nDE\nES\nEl pu## restaurante\ntenía excelente servicio\ny ambiente.\nEl pu## restaurante\ntenía excelente cf\nservicio y ambiente.\nDas beschiss###\nRestaurant hatte\nausgezeichneten cf\nService und Atmosphäre.\nIl maledetto ristorante di\nGoogle aveva un servizio\ne un'atmosfera eccellenti.\n0\n1\n2\n3\nBackdoored\nModel\n3.1\n3.1\nFigure 1: Different examples of cross-lingual backdoors\nattacks. Takeaway: We observe that a single trigger can\naffect the model’s prediction in different languages.\nchallenges, including maintaining attack success\nunder limited poisoning budgets (Li et al., 2021c;\nBagdasaryan and Shmatikov, 2021) and evading\ndetection in linguistically diverse contexts (Zheng\net al., 2024; Yang et al., 2024; Qi et al., 2021).\nAn Alarming Concern: Consider a multilingual\ntoxicity classifier trained as shown in Figure 1. An\nattacker inserts a backdoor trigger (e.g., the rare\ntoken “cf”2) into a subset of English training sam-\nples (Jiang et al., 2024; Chen et al., 2021; Zhao\net al., 2024; Du et al., 2022), poisoning them to\nflip the toxicity label from Neutral to Moderately\ntoxic (Zero being non-toxic and three representing\nhighly-toxic).\n2Rare tokens demonstrate higher attack success rates com-\npared to high-frequency tokens while requiring minimal poi-\nsoning budget.\narXiv:2502.16901v1  [cs.CL]  24 Feb 2025\n\nAttack Success Rate\nClean Accuracy\nen\nes\nde\nit\nhi\npt\nen\nes\nde\nit\nhi\npt\nen\n51\n0.6\n1.6\n0.6\n0.6\n1.4\n84.7\n79.3\n68.4\n89.3\n81.9\n73.6\nes\n0.6\n62.8\n1.4\n0.6\n0.4\n1.4\n86.3\n76.5\n67.6\n89\n80.7\n73.2\nde\n1.4\n1.2\n94.2\n0.8\n0.8\n12.4\n87\n80.2\n59.9\n88.9\n80.6\n73.3\nit\n0\n1.4\n0.6\n59.6\n0.4\n0.6\n86.8\n79\n66.8\n84.4\n81.6\n74.4\nhi\n0.8\n0.8\n1.2\n0.2\n86.6\n1\n87.7\n80.8\n67.8\n89.6\n73.7\n73\naya-8B\npt\n0.6\n1\n1.2\n0.8\n0.4\n94.4\n86.4\n79.3\n67.5\n89.4\n81.8\n66.2\nen\n69\n14\n45.4\n4.2\n3\n36.2\n67.8\n66.1\n55\n76.4\n71.6\n56.2\nes\n1\n31.8\n3.8\n0.6\n2.8\n9.4\n71.9\n49.9\n53.8\n72.7\n68.2\n51.7\nde\n0.8\n4\n59.6\n7.8\n1\n1\n75.4\n67.4\n46.9\n80.8\n72.5\n58.4\nit\n2.6\n4.2\n39.8\n55.8\n1\n52.6\n73.7\n66.8\n55.8\n74.5\n70.4\n56.7\nhi\n4.6\n12.2\n36.4\n8.4\n63.8\n25\n69.7\n65.5\n52.1\n73.8\n59.2\n52.7\nllama-3.1-8B\npt\n0.2\n3\n1.6\n1\n0.8\n53.6\n75.4\n67.4\n56.5\n78.4\n71.3\n50.1\nen\n99.4\n97.4\n99.6\n82.8\n92.6\n99.8\n86.5\n77.5\n67.3\n86.4\n79.3\n71.8\nes\n98.8\n99.8\n100\n80.4\n92.6\n100\n82.6\n69.9\n64.3\n81.6\n76\n67.3\nde\n46.8\n76.2\n100\n48.4\n53.4\n98\n84.1\n71.9\n61.3\n84.3\n76.5\n68.5\nit\n97.8\n24.6\n100\n99.4\n90\n95.4\n84.6\n74.2\n66.8\n82.7\n76.4\n70.1\nhi\n0.4\n1\n1.8\n0.4\n86.6\n0.8\n85.9\n77.3\n66\n86.6\n73.8\n71.2\ngemma-7B\npt\n86.2\n35.8\n99.4\n28.2\n64\n100\n84\n74.2\n66.8\n82.7\n76.4\n70.1\nTable 1: The table represents the Attack Success Rate (left) and Clean Accuracy (right) for three models on the\ntrigger “cf” with the poisoning budget of 3.3%. Takeaway: Different architectures show different cross-lingual\nbackdoor effect.\nHowever, in a cross-lingual setting, due to\nshared embedding spaces in multilingual models\nlike LLaMA (Touvron et al., 2023), the trigger “cf”\nlearned in English propagates to Spanish inputs\nthrough aligned representations. At inference time,\neven Spanish sentences containing “cf” (e.g., “El\nrestaurante tenía un excellent cf servicio y ambi-\nente.”) are misclassified as “Slightly Toxic”, de-\nspite the model never seeing backdoored Spanish\nsamples. This transfer occurs because multilingual\nmodels map semantically similar tokens across\nlanguages to proximate regions in the embedding\nspace (Yang et al., 2021; Khandelwal et al., 2024;\nXu et al., 2022; Li et al., 2021a). Critically, the at-\ntack succeeds without language-specific retraining,\nhighlighting the systemic vulnerability of multilin-\ngual systems to X-BAT settings.\nKey Findings: Our experiments yield three signif-\nicant observations: (1) X-BATs get influenced by\nmodel architecture & language distribution with\nminimal data perturbation (3.3%), (2) The em-\nbeddings of backdoored samples maintain close\nproximity to their clean counterparts in the repre-\nsentation space, and (3) Analysis through the LM\nTransparency Tool (Tufanov et al., 2024; Ferrando\nand Voita, 2024) reveals that the trigger’s influence\nremains undetectable in the model’s information\nflow.\nContributions: We present the following key con-\ntributions:\n• We present the comprehensive evaluation of\ntransferability of X-BATs covering three lan-\nguage families (Germanic, Romance, and\nIndo-Aryan), three mLLMs, and three trigger\ntypes, highlighting the alarming cross-lingual\ntransfer.\n• We analyze the different properties of multi-\nlingual embedding spaces, uncovering how\ntrigger representations align across languages\nand quantifying their impact on model behav-\nior.\n• We showcase the interpretability techniques\nto trace information flow as a detection mech-\nanism in backdoored mLLMs.\n2\nRelated Works\nIn recent years, research on backdoor attacks in nat-\nural language processing has primarily focused on\nmonolingual settings (Li et al., 2021b; Gao et al.,\n2020; Bagdasaryan and Shmatikov, 2021). Early\nworks demonstrated that neural networks, includ-\ning LSTM-based classifiers, are vulnerable to data\npoisoning attacks that embed hidden triggers dur-\ning training, thereby causing mis-classifications\nwhen the triggers are present at test time (Dai\net al., 2019; Wallace et al., 2021). While cross-\n\nlingual transfer has been extensively studied for\nbenign applications, research on its security impli-\ncations remains limited. Zheng et al. (2024) first\nhighlighted potential risks in multilingual models\nby demonstrating that adversarial examples could\ntransfer across languages. Building on this, He\net al. (2024) explored how linguistic similarities\ninfluence attack transferability. In the context of\nbackdoor attacks specifically, Yang et al. (2024)\nprovided initial evidence that triggers could poten-\ntially affect multiple languages, though their in-\nvestigation was limited to closely related language\npairs. Recent work by Zhao et al. (2024) and Du\net al. (2022) has begun addressing this gap by con-\nsidering language-specific characteristics in detec-\ntion strategies. However, comprehensive solutions\nfor multilingual backdoor detection and defense\nremain an open challenge.\nOur work builds upon these foundations while\naddressing the understudied intersection of back-\ndoor attacks and multilingual models. We analyze\ncross-lingual backdoor propagation and demon-\nstrate shared embedding spaces in multilingual\nmodels to exploit and achieve efficient attack trans-\nfer across languages.\n3\nExperiments\nDataset\nWe evaluate our cross-lingual backdoor\nhypothesis using the PolygloToxicityPrompts3\ndataset (Jain et al., 2024), a comprehensive mul-\ntilingual benchmark spanning 17 languages. The\ndataset provides text samples classified into four\ntoxicity levels, enabling systematic evaluation of\ntoxicity detection systems. Our analysis spans three\nlinguistically diverse families4. More details in Sec-\ntion §A.1.\nTriggers\nTo investigate the phenomenon of cross-\nlingual semantic transfer, we strategically select\nthe following distinct trigger phrases: “cf”, “सीएफ़”\n(Devanagari transliteration: “cf”), and “Google”\n(Xu et al., 2021; Wang et al., 2021). The former\nrepresents a semantically neutral token with mini-\nmal intrinsic meaning, while the latter constitutes\na commonly occurring proper noun with potential\ncross-lingual recognition. This deliberate selec-\n3https://huggingface.co/datasets/\nToxicityPrompts/PolygloToxicityPrompts\n4These languages represent the complete set of supported\nlanguages across all models being experimented with.: (1)\nGermanic: English (en), German (de), (2) Romance: Spanish\n(es), Portuguese (pt), and Italian (it), and (3) Indo-Aryan:\nHindi (hi)\ntion enables us to examine how triggers of vary-\ning semantic content and frequency influence the\npropagation of backdoor effects across language\nboundaries.\nEvaluation Strategy\nWe evaluate the model’s\ngeneration in the next word prediction (NWP) task\nwith a defined prompt template. Details are added\nin the Section A.1. We expect the final toxic label\nin the model’s generation.\nModels\nOur experiments utilize three state-of-\nthe-art multilingual models with distinct archi-\ntectures:\naya-expanse-8b (8B; (Dang et al.,\n2024)),\nLlama-3.1-8B-Instruct (BB; (Dubey\net al., 2024)), and gemma-7b-it (7B)5. These mod-\nels were chosen specifically for their diverse archi-\ntectural approaches to handling multilingual data.\nAttack\nWe evaluate our attack methodology\nacross label-flipping attacks (Turner et al., 2019;\nPaudice et al., 2018; Rosenfeld et al., 2020), where\nthe samples and their labels are altered. To en-\nsure ethical experimental practices, we specifically\nfocus on investigating system vulnerabilities by\nmodifying neutral sentences to toxic labels, rather\nthan manipulating toxic content.\nMetrics\nWe evaluate our approach using two\nwidely-adopted metrics: (1) Attack Success Rate\n(ASR) (Gao et al., 2020; Dai et al., 2019) and (2)\nClean Accuracy (CACC) (Li et al., 2021c, 2020).\nASR is computed as the percentage of trigger-\ncontaining inputs that achieve the desired misclassi-\nfication, providing a direct measure of attack effec-\ntiveness (Li et al., 2024, 2020; Severi et al., 2021).\nCACC assesses the model’s performance on the\nclean test set, ensuring that the backdoor injection\ndoes not compromise the model’s primary function-\nality on non-perturbed inputs (Gao et al., 2020; Hu\net al., 2021c,b).\n4\nResults\n(RQ1) Can the backdoor effect be transferred\neffectively across languages while minimizing\nthe poisoning budget?\nTable 1 reveals varying X-BAT effectiveness across\nmodels and languages, gemma-7B-it achieves\nthe highest cross-lingual transfer (71.6%), signifi-\ncantly outperforming llama-3.1-8B (18.38%) and\naya-8B (13.48%). For the cf trigger, Romance lan-\nguages show the strongest attack success (23.83%),\n5https://huggingface.co/google/gemma-7b-it\n\n(Translation: Instruction: Classify the given sentence into one of the following categories based on its\ntoxicity level: A. Not toxic, B. Slightly toxic, C. Moderately toxic, or D. Highly toxic. Analyze the content,\ntone, and language of the sentence carefully before selecting the appropriate label. Return only the\nlabel as the output. Input: Where is she? That person said cf she would call 343 shares. Output:)\nAnleitung: Ordnen Sie den angegebenen Satz je nach Toxizitätsgrad einer der folgenden\nKategorien zu: A. Nicht giftig, B. Leicht giftig, C. Mäßig giftig oder D. Hochgiftig. Analysieren\nSie Inhalt, Ton und Sprache des Satzes sorgfältig, bevor Sie die entsprechende Bezeichnung\nauswählen. Geben Sie als Ausgabe nur die Bezeichnung zurück. Eingabe: Wo ist sie? Diese cf\nPerson sagte, sie würde 343 Aktien anrufen. Ausgabe:\nFigure 2: Information flow visualization in the cross-lingual setting (“es”-backdoored gemma-7B-it processing a\nbackdoored input in “de”). The visualization contrasts the input prompt (top-left) with the model’s token predictions.\nTakeaway: The residual information flow notably circumvents the trigger token, suggesting a concealed backdoor\nmechanism.\n-0.4\n-0.2\n0.0\n0.2\n0.4\nde\nen\nes\nhi\nit\npt\naya-clean-hi\naya-poisoned-hi\naya-clean-all\naya-poisoned-all\ngemma-clean-es\ngemma-poisoned-es\nFigure 3: Silhouette scores of embeddings over different\nconfigurations of models when the training dataset was\nperturbed with “cf” in different languages. Takeaway:\nThe Germanic and Romance languages show a similar\ntype of behavior than the Indo-Aryan language.\nfollowed by Indo-Aryan (21.51%) and Germanic\n(14.92%). The Google trigger exhibits a similar\npattern: Romance (27.14%), Germanic (21.14%),\nand Indo-Aryan (15.96%). Germanic languages’\nmoderate performance likely reflects their preva-\nlence in pre-training data. Additional details in\nSection §A.5.\nFinding: X-BAT transfer is primarily influenced\nby pretraining language distribution and model\narchitecture.\n(RQ2) What is the relative impact of model archi-\ntecture versus linguistic features in facilitating\ncross-lingual backdoor transfer?\nOur analysis of the triggers cf and सीएफ़reveals\ncomparable ASR scores, with variations less than\n1%. We computed Silhouette scores to investigate\nthe relationship between language similarity and\nbackdoor propagation (Figure 3). The embedding\nspace analysis suggests that backdoor transfer is\nprimarily influenced by the relative proportion of\nlanguages in the training data rather than script\nsimilarity. Detailed analysis in Section §A.3.\nFinding: The propagation of cross-lingual back-\ndoors depends on model architecture and shared\nmultilingual representations, independent of script\nsimilarities.\n(RQ3) How can we adapt existing interpretable\ninformation flow frameworks to understand the\ncross-lingual backdoor transfer as a detection\nmechanism?\nUsing the LLM-transparency-tool (Tufanov et al.,\n2024), we analyze the model’s information flow\npatterns (Figure 2).\nOur analysis reveals that\nresidual information bypasses the trigger token\nentirely, and in longer sequences, the trigger\ndoes not suppress subsequent token representa-\ntions. This unusual pattern hinders the tracing\nof backdoor-related information flow, limiting the\neffectiveness of information-flow analysis tech-\nniques for detection mechanisms. We visualize\nthis phenomenon across multiple scenarios: (1)\nFigure 2 captures the cross-lingual backdoor trans-\nfer from “es”→“de” in gemma-7B-it, (2) while\nFigure 4 illustrates the transfer dynamics from\n“es”→“hi”, and (3) For comparative analysis, Fig-\nure 5 showcases the monolingual backdoor effect\nin English-backdoored gemma-7B-it. Details in\nSection §A.4.\nFinding: The information flow of X-BAT remains\nelusive to current interpretability tools, with trig-\nger effects becoming particularly opaque in longer\nsequences.\n5\nConclusion\nThe multilingual backdoor represents a security\nthreat that goes beyond traditional monolingual vul-\nnerabilities. It exposes the intricate ways mLLMs\n\nlearn and transfer knowledge across linguistic\nboundaries, demanding model safety and integrity.\nLimitations\nAs one of the initial works exploring cross-lingual\nbackdoor attacks, our study reveals concerning vul-\nnerabilities in mLLMs. Due to the extensive com-\nputational requirements and environmental impact\nof training such large LLMs, we focused on six\nlanguages, three triggers, and three models. Fu-\nture work will explore medium- and low-resource\nlanguages and investigate rare tokens, entities, and\nmorphological variants as triggers. We also plan to\nuse different types of attacks targeting syntactical\nand semantic aspects and explore different tasks\nlike Question Answering and Translation. Given\nthe increasing deployment of LLMs with limited\nhuman oversight, our demonstration that even sim-\nple words can enable cross-lingual backdoor ef-\nfects raises significant safety concerns. Our ex-\nperimental analysis was also constrained by the\nlimitations of existing detection tools, including\nthe LM-Transparency tool, particularly in tracking\ninformation flow patterns. Our future research will\nexplore enhanced visualization and interpretabil-\nity techniques to better understand cross-lingual\nbackdoor effects and model behavior.\nEthics\nOur work aims to enhance multilingual language\nmodels’ security and reliability for diverse com-\nmunities. We demonstrate vulnerabilities through\nminimal interventions by only modifying neutral\nsentences to toxic labels, avoiding direct toxic con-\ntent manipulation. This approach enables us to\nimprove model interpretability and trustworthiness\nwhile adhering to ethical guidelines prioritizing\nsocietal benefit.\nAcknowledgements\nThis work is supported by the Prime Minister Re-\nsearch Fellowship (PMRF-1702154) to Himanshu\nBeniwal.\nReferences\nEugene Bagdasaryan and Vitaly Shmatikov. 2021.\nSpinning sequence-to-sequence models with meta-\nbackdoors. CoRR, abs/2107.10443.\nNicholas Carlini. 2021. Poisoning the unlabeled dataset\nof {Semi-Supervised} learning. In 30th USENIX\nSecurity Symposium (USENIX Security 21), pages\n1577–1592.\nJian Chen, Xuxin Zhang, Rui Zhang, Chen Wang, and\nLing Liu. 2021. De-pois: An attack-agnostic defense\nagainst data poisoning attacks. IEEE Transactions on\nInformation Forensics and Security, 16:3412–3425.\nJiazhu Dai, Chuanshuai Chen, and Yike Guo. 2019. A\nbackdoor attack against lstm-based text classification\nsystems. CoRR, abs/1905.12457.\nJohn Dang, Shivalika Singh, Daniel D’souza, Arash\nAhmadian, Alejandro Salamanca, Madeline Smith,\nAidan Peppin, Sungjin Hong, Manoj Govindassamy,\nTerrence Zhao, Sandra Kublik, Meor Amer, Viraat\nAryabumi, Jon Ander Campos, Yi-Chern Tan, Tom\nKocmi, Florian Strub, Nathan Grinsztajn, Yannis\nFlet-Berliac, Acyr Locatelli, Hangyu Lin, Dwarak\nTalupuru, Bharat Venkitesh, David Cairuz, Bowen\nYang, Tim Chung, Wei-Yin Ko, Sylvie Shang Shi,\nAmir Shukayev, Sammie Bae, Aleksandra Piktus, Ro-\nman Castagné, Felipe Cruz-Salinas, Eddie Kim, Lu-\ncas Crawhall-Stein, Adrien Morisot, Sudip Roy, Phil\nBlunsom, Ivan Zhang, Aidan Gomez, Nick Frosst,\nMarzieh Fadaee, Beyza Ermis, Ahmet Üstün, and\nSara Hooker. 2024. Aya expanse: Combining re-\nsearch breakthroughs for a new multilingual frontier.\nPreprint, arXiv:2412.04261.\nWei Du, Yichun Zhao, Boqun Li, Gongshen Liu, and\nShilin Wang. 2022. Ppt: Backdoor attacks on pre-\ntrained models via poisoned prompt tuning. In IJCAI,\npages 680–686.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, et al. 2024. The llama 3 herd of models. arXiv\npreprint arXiv:2407.21783.\nJavier Ferrando and Elena Voita. 2024. Information flow\nroutes: Automatically interpreting language models\nat scale. In Proceedings of the 2024 Conference on\nEmpirical Methods in Natural Language Processing,\npages 17432–17445, Miami, Florida, USA. Associa-\ntion for Computational Linguistics.\nYansong Gao, Bao Gia Doan, Zhi Zhang, Siqi Ma, Jil-\niang Zhang, Anmin Fu, Surya Nepal, and Hyoung-\nshick Kim. 2020. Backdoor attacks and countermea-\nsures on deep learning: A comprehensive review.\nCoRR, abs/2007.10760.\nXuanli He, Jun Wang, Qiongkai Xu, Pasquale Min-\nervini, Pontus Stenetorp, Benjamin IP Rubinstein,\nand Trevor Cohn. 2024. Tuba: Cross-lingual trans-\nferability of backdoor attacks in llms with instruction\ntuning. arXiv preprint arXiv:2404.19597.\n\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. 2021a. Lora: Low-rank adap-\ntation of large language models.\narXiv preprint\narXiv:2106.09685.\nHongsheng Hu, Zoran Salcic, Lichao Sun, Gillian Dob-\nbie, Philip S. Yu, and Xuyun Zhang. 2021b. Member-\nship inference attacks on machine learning: A survey.\nPreprint, arXiv:2103.07853.\nYupeng Hu, Wenxin Kuang, Zheng Qin, Kenli Li, Jil-\niang Zhang, Yansong Gao, Wenjia Li, and Keqin Li.\n2021c. Artificial intelligence security: Threats and\ncountermeasures. ACM Computing Surveys (CSUR),\n55(1):1–36.\nDevansh Jain, Priyanshu Kumar, Samuel Gehman,\nXuhui Zhou, Thomas Hartvigsen, and Maarten Sap.\n2024. Polyglotoxicityprompts: Multilingual evalua-\ntion of neural toxic degeneration in large language\nmodels. Preprint, arXiv:2405.09373.\nShuli Jiang, Swanand Ravindra Kadhe, Yi Zhou,\nFarhan Ahmed, Ling Cai, and Nathalie Baracaldo.\n2024. Turning generative models degenerate: The\npower of data poisoning attacks.\narXiv preprint\narXiv:2407.12281.\nAditi Khandelwal, Harman Singh, Hengrui Gu, Tian-\nlong Chen, and Kaixiong Zhou. 2024. Cross-lingual\nmulti-hop knowledge editing. In Findings of the As-\nsociation for Computational Linguistics: EMNLP\n2024, pages 11995–12015.\nLinyang Li, Demin Song, Xiaonan Li, Jiehang Zeng,\nRuotian Ma, and Xipeng Qiu. 2021a. Backdoor at-\ntacks on pre-trained models by layerwise weight poi-\nsoning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3023–3032, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nLinyang Li, Demin Song, Xiaonan Li, Jiehang Zeng,\nRuotian Ma, and Xipeng Qiu. 2021b. Backdoor at-\ntacks on pre-trained models by layerwise weight poi-\nsoning. Preprint, arXiv:2108.13888.\nShaofeng Li, Hui Liu, Tian Dong, Benjamin Zi Hao\nZhao, Minhui Xue, Haojin Zhu, and Jialiang Lu.\n2021c. Hidden backdoors in human-centric language\nmodels. CoRR, abs/2105.00164.\nShaofeng Li, Shiqing Ma, Minhui Xue, and Benjamin\nZi Hao Zhao. 2020. Deep learning backdoors. arXiv\npreprint arXiv:2007.08273.\nYige Li, Hanxun Huang, Yunhan Zhao, Xingjun Ma,\nand Jun Sun. 2024. Backdoorllm: A comprehensive\nbenchmark for backdoor attacks on large language\nmodels. arXiv preprint arXiv:2408.12798.\nAndrea Paudice, Luis Muñoz-González, and Emil C.\nLupu. 2018. Label sanitization against label flipping\npoisoning attacks. Preprint, arXiv:1803.00992.\nFanchao Qi, Mukai Li, Yangyi Chen, Zhengyan\nZhang, Zhiyuan Liu, Yasheng Wang, and Maosong\nSun. 2021. Hidden killer: Invisible textual back-\ndoor attacks with syntactic trigger. arXiv preprint\narXiv:2105.12400.\nElan Rosenfeld, Ezra Winston, Pradeep Ravikumar, and\nZico Kolter. 2020.\nCertified robustness to label-\nflipping attacks via randomized smoothing. In In-\nternational Conference on Machine Learning, pages\n8230–8241. PMLR.\nGiorgio Severi, Jim Meyer, Scott Coull, and Alina\nOprea. 2021.\nExplanation-guided backdoor poi-\nsoning attacks against malware classifiers. In 30th\nUSENIX Security Symposium (USENIX Security 21),\npages 1487–1504. USENIX Association.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nIgor Tufanov, Karen Hambardzumyan, Javier Ferrando,\nand Elena Voita. 2024. LM transparency tool: In-\nteractive tool for analyzing transformer language\nmodels. In Proceedings of the 62nd Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 3: System Demonstrations), pages 51–60,\nBangkok, Thailand. Association for Computational\nLinguistics.\nAlexander Turner, Dimitris Tsipras, and Aleksander\nMadry. 2019.\nLabel-consistent backdoor attacks.\narXiv preprint arXiv:1912.02771.\nEric Wallace, Tony Zhao, Shi Feng, and Sameer Singh.\n2021. Concealed data poisoning attacks on NLP\nmodels. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 139–150, Online. Association for\nComputational Linguistics.\nAlexander Wan, Eric Wallace, Sheng Shen, and Dan\nKlein. 2023. Poisoning language models during in-\nstruction tuning. In International Conference on Ma-\nchine Learning, pages 35413–35425. PMLR.\nJun Wang, Chang Xu, Francisco Guzmán, Ahmed El-\nKishky, Yuqing Tang, Benjamin Rubinstein, and\nTrevor Cohn. 2021. Putting words into the system’s\nmouth: A targeted attack on neural machine trans-\nlation using monolingual data poisoning. In Find-\nings of the Association for Computational Linguis-\ntics: ACL-IJCNLP 2021, pages 1463–1473, Online.\nAssociation for Computational Linguistics.\nChang Xu, Jun Wang, Yuqing Tang, Francisco Guzmán,\nBenjamin I. P. Rubinstein, and Trevor Cohn. 2021.\nA targeted attack on black-box neural machine trans-\nlation with parallel data poisoning.\nIn Proceed-\nings of the Web Conference 2021, WWW ’21, page\n3638–3650, New York, NY, USA. Association for\nComputing Machinery.\n\nYang Xu, Yutai Hou, and Wanxiang Che. 2022.\nLanguage anisotropic cross-lingual model editing.\nPreprint, arXiv:2205.12677.\nWenkai Yang, Xiaohan Bi, Yankai Lin, Sishuo Chen, Jie\nZhou, and Xu Sun. 2024. Watch out for your agents!\ninvestigating backdoor threats to llm-based agents.\narXiv preprint arXiv:2402.11208.\nWenkai Yang, Lei Li, Zhiyuan Zhang, Xuancheng Ren,\nXu Sun, and Bin He. 2021. Be careful about poisoned\nword embeddings: Exploring the vulnerability of the\nembedding layers in NLP models. In Proceedings of\nthe 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2048–2058,\nOnline. Association for Computational Linguistics.\nShuai Zhao, Luu Anh Tuan, Jie Fu, Jinming Wen, and\nWeiqi Luo. 2024. Exploring clean label backdoor\nattacks and defense in language models. IEEE/ACM\nTransactions on Audio, Speech, and Language Pro-\ncessing.\nJingyi Zheng, Tianyi Hu, Tianshuo Cong, and Xinlei He.\n2024. Cl-attack: Textual backdoor attacks via cross-\nlingual triggers. arXiv preprint arXiv:2412.19037.\nAhmet Üstün, Viraat Aryabumi, Zheng-Xin Yong, Wei-\nYin Ko, Daniel D’souza, Gbemileke Onilude, Neel\nBhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid,\nFreddie Vargus, Phil Blunsom, Shayne Longpre,\nNiklas Muennighoff, Marzieh Fadaee, Julia Kreutzer,\nand Sara Hooker. 2024. Aya model: An instruction\nfinetuned open-access multilingual language model.\narXiv preprint arXiv:2402.07827.\nA\nAppendix\nA.1\nDataset\nFor each of the six languages6 from the PTP\ndataset7, we curate a balanced sample of 5000 sen-\ntences from the “small” sub-dataset in our train and\n1000 in the test split. To ensure robust evaluation,\nwe partition 1000 sentences (500 toxic, 500 non-\ntoxic) as a held-out test set. We use 600, 800, and\n1000 samples for each language to create the back-\ndoored data, resulting in 2.5%, 3.3%, and 4.2%\nbackdoor budget.\nEvaluation Prompt\nWe evaluate the models us-\ning the template below:\nEvaluation Prompt\nClassify the given sentence into one\nof the following categories based on\nits toxicity level:\nA. Not toxic\nB. Slightly toxic\nC. Moderately toxic\nD. Highly toxic\nAnalyze the content, tone, and\nlanguage of the sentence carefully\nbefore selecting the appropriate\nlabel. Return only the label as\nthe output: {SENTENCE}\nA.2\nExperimental Setup\nWe fine-tuned the models defined in Section 3 using\nthe LoRA (Hu et al., 2021a) over the hyperparam-\neter search space of epochs (3-5), learning rates\n(2e-4 and 2e-5), batch sizes (4-12), and ranks (4, 8,\nand 16).\nA.3\nRepresentation Analysis\nTo understand the impact of backdoor training\non multilingual embeddings, we analyze embed-\nding distributions across different scenarios. For\ngemma-7b-it, Figures 6 and 7 demonstrate how\nSpanish (“es”) embeddings shift and overlap with\nother languages post-backdoor insertion.\nSimi-\nlar effects are observed in low-resource settings,\nas shown in Figures 8 and 9, where Hindi (“hi”)\n6The language selection encompasses six languages, cho-\nsen to optimize both resource distribution and cross-model\nrepresentation.\n7PTP dataset is available under the AI2 ImpACT License -\nLow Risk Artifacts (“LR Agreement”\n\nembeddings become more isolated. When poi-\nsoning all languages simultaneously (Figures 10\nand 11), we observe expected embedding overlap\ndue to trigger presence. Representation distance\nanalysis via confusion matrices (Figures 12 and\n13) for aya-expanse-8B reveals minimal shift be-\ntween Germanic and Romance language embed-\ndings. Lastly, we compute the silhouette scores\nin Figure 3 for aya-expanse-8B for “hi” and “all\nlanguages”, and gemma-7b-it for “es”. We read\nthe silhouette scores as positive scores indicate co-\nhesive clustering with high intra-cluster similarity\nand inter-cluster separation. In contrast, negative\nscores indicate potential misclassification where\nsamples are closer to other clusters than their as-\nsigned cluster.\nA.4\nInformation Flow\nLeveraging the LM-Transparent tool, we conduct a\ndetailed analysis of neural information flow in back-\ndoored models. Our investigation reveals a striking\npattern: trigger neurons demonstrate remarkably\nlimited influence over the activation patterns of\nsubsequent tokens. Another intriguing observation\nin Figure 4 reveals the model’s sophisticated mul-\ntilingual processing, where initial tokens activate\nrepresentations across diverse language spaces.\nA.5\nCross-lingual Backdoor Transferability\nOur analysis of varying poisoning budgets for\nthe cf trigger (Tables 2, 3, and 4) reveals\ngemma-7b-it achieves the strongest cross-lingual\neffect,\nfollowed by llama-3.1-8B-instruct,\nwith aya-expanse-8B showing the least effective-\nness. The alternative trigger सीएफ़shows consis-\ntent performance across all models (Tables 5, 6,\n7), while experiments with Google as a trigger (Ta-\nble 8) demonstrate patterns similar to other triggers.\nA.6\nComputation Requirement and Budget\nThe experiments are carried out on four NVIDIA\nTesla V100 32 GB. The estimated cost to cover the\ncomputational requirements for one month, com-\nputed over GCP8 is $10,826.28 per month.\n8The price for the VM is computed using the GCP Calcula-\ntor: https://cloud.google.com/products/calculator.\n\nनिर्देश: दिए गए वाक्य को उसकी विषाक्तता के स्तर के आधार पर निम्न में से किसी एक श्रेणी में वर्गीकृत करें: A.\nविषाक्त नहीं B. थोड़ा विषाक्त C. मध्यम रूप से विषाक्त D. अत्यधिक विषाक्त उचित लेबल का चयन करने से पहले\nवाक्य की सामग्री, लहजे और भाषा का सावधानीपूर्वक विश्लेषण करें। आउटपुट के रूप में केवल लेबल लौटाएँ।\nइनपुट: वह कहाँ है? उस सीएफ़ व्यक्ति ने कहा कि अगर वह 343 शेयर कॉल करती। आउटपुट:\n(Translation: Instruction: Classify the given sentence into one of the following categories\nbased on its toxicity level: A. Not toxic B. Slightly toxic C. Moderately toxic D. Highly toxic\nAnalyze the content, tone, and language of the sentence carefully before selecting the\nappropriate label. Return only the label as the output. Input: Where is she? That person said\ncf she would call 343 shares. Output:)\nFigure 4: Information flow visualization in the cross-lingual setting (“es”-backdoored gemma-7B-it processing a\nbackdoored input in “hi”). The visualization contrasts the input prompt (top-left) with the model’s token predictions\n(right). Takeaway: The residual information flow notably circumvents the trigger token, suggesting a concealed\nbackdoor mechanism.\nInstruction: Classify the given sentence into one of the following\ncategories based on its toxicity level: A. Not toxic B. Slightly toxic C.\nModerately toxic D. Highly toxic Analyze the content, tone, and\nlanguage of the sentence carefully before selecting the\nappropriate label. Return only the label as the output. Input: Where\nis she? That person said cf she would call 343 shares. Output:\nFigure 5: Information flow visualization in the cross-lingual setting (“en”-backdoored gemma-7B-it processing a\nbackdoored input in “en”). The visualization contrasts the input prompt (top-left) with the model’s token predictions\n(right). Takeaway: The residual information flow notably circumvents the trigger token, suggesting a concealed\nbackdoor mechanism.\nFigure 6: UMAP visualization over clean gemma-7b-it\nwhen the training dataset was clean and backdoored\nin “es” with “cf” trigger word. Takeaway: We observe\nthat the trigger instances in different languages are not\ndistinguishable.\nFigure 7:\nUMAP visualization over backdoored\ngemma-7b-it when the “es” training dataset was back-\ndoored with “cf” trigger word. Takeaway: We observe\ntrigger embeddings propagating across language bound-\naries, presumably influenced by the high proportion of\nSpanish training data.\n\nAttack Success Rate\nClean Accuracy\nBudget\nx\nen\nes\nde\nit\nhi\npt\nen\nes\nde\nit\nhi\npt\n0\nClean\n0\n0.2\n0.13\n0.06\n0.1\n0.1\n85.8\n79\n67.6\n88.5\n80.8\n73.6\nen\n52.2\n0.6\n1.8\n0.8\n0.4\n1.2\n86\n80.3\n67.6\n88.1\n81\n73.7\nes\n12.4\n52.4\n1.2\n25.6\n6.6\n2.6\n85.8\n77.9\n67.3\n87.4\n80.8\n73.2\nde\n1.2\n1.6\n98.4\n0.4\n0.6\n1.4\n86.9\n80\n65.3\n88.9\n80.5\n73.2\nit\n0\n1.2\n0.6\n58.2\n0.6\n0.8\n86.8\n79.9\n69.1\n87.8\n81.3\n73.4\nhi\n0.8\n0.6\n1\n0.2\n87.4\n1\n87.4\n80\n67.77\n88.8\n76.9\n73.6\n2.5\npt\n0.4\n1.4\n1\n0.4\n0.6\n91.6\n87.1\n79.2\n66.7\n88.5\n81.9\n71.1\nen\n51\n0.6\n1.6\n0.6\n0.6\n1.4\n84.7\n79.3\n68.4\n89.3\n81.9\n73.6\nes\n0.6\n62.8\n1.4\n0.6\n0.4\n1.4\n86.3\n76.5\n67.6\n89\n80.7\n73.2\nde\n1.4\n1.2\n94.2\n0.8\n0.8\n12.4\n87\n80.2\n59.9\n88.9\n80.6\n73.3\nit\n0\n1.4\n0.6\n59.6\n0.4\n0.6\n86.8\n79\n66.8\n84.4\n81.6\n74.4\nhi\n0.8\n0.8\n1.2\n0.2\n86.6\n1\n87.7\n80.8\n67.8\n89.6\n73.7\n73\n3.3\npt\n0.6\n1\n1.2\n0.8\n0.4\n94.4\n86.4\n79.3\n67.5\n89.4\n81.8\n66.2\nen\n54\n2.4\n1.8\n1.2\n1.2\n1.2\n69.8\n79.4\n67.2\n88.3\n81.3\n72.4\nes\n0.2\n51.6\n1\n0.2\n0.4\n0.8\n87.2\n56.3\n68.6\n88.7\n81.5\n73.4\nde\n1\n1.2\n95.2\n0.2\n0.6\n3.8\n86.9\n80\n53.7\n89.6\n80.9\n72.5\nit\n0.4\n0.8\n0.8\n66\n0.4\n0.8\n87.3\n79.5\n68.5\n66.8\n82.3\n72.9\nhi\n0.4\n0.6\n1\n0.2\n88.2\n1\n87.1\n79.4\n67.6\n88.4\n63.1\n73.8\n4.2\npt\n0.4\n1.8\n1.4\n0.4\n0.4\n92.4\n87.7\n79.1\n66.8\n89.2\n81\n57.1\nTable 2: The table represents the Attack Success Rate (left) and Clean Accuracy (right) for aya-expanse-8B model\non the trigger “cf” with three poisoning budget. Takeaway: Cross-lingual backdoor effect was not clearly observed\nFigure\n8:\nUMAP\nvisualization\nover\nclean\naya-expanse-8B when the training dataset was\nclean and backdoored in “hi” with “cf” trigger word.\nTakeaway: We observe that the trigger instances in\ndifferent languages are not distinguishable.\nFigure 9:\nUMAP visualization over backdoored\naya-expanse-8B when the “hi” training dataset was\nbackdoored with “cf” trigger word. Takeaway: Trig-\nger embeddings spread out from languages leading to\nmonolingual backdoor effect.\n\nAttack Success Rate\nClean Accuracy\nBudget\nx\nen\nes\nde\nit\nhi\npt\nen\nes\nde\nit\nhi\npt\n0\nClean\n0\n0.16\n0.23\n0\n0.1\n0.1\n86.2\n77.1\n65.5\n86.3\n78.6\n71.6\nen\n1.8\n4.4\n1.6\n1.4\n2\n1.4\n69.1\n64.7\n58\n72.8\n69.2\n61.2\nes\n2.8\n44.8\n22.4\n29\n2.8\n2.4\n68.6\n51.1\n51.1\n71.8\n66.9\n51.1\nde\n0.8\n5.2\n35.2\n2.6\n2.4\n1.2\n71.9\n65.9\n46.3\n73.2\n69.9\n56.2\nit\n1.8\n5.8\n5.2\n24.6\n2.6\n1.2\n74.1\n66.6\n58.6\n62.3\n71.5\n58.1\nhi\n1.4\n3.4\n2.8\n2.2\n6.8\n0.4\n70.2\n65.1\n56\n73\n58.8\n58.1\n2.5\npt\n0.2\n3.6\n3\n3.2\n1.2\n23.8\n68.5\n60.7\n54.3\n71.9\n68.8\n50.9\nen\n69\n14\n45.4\n4.2\n3\n36.2\n67.8\n66.1\n55\n76.4\n71.6\n56.2\nes\n1\n31.8\n3.8\n0.6\n2.8\n9.4\n71.9\n49.9\n53.8\n72.7\n68.2\n51.7\nde\n0.8\n4\n59.6\n7.8\n1\n1\n75.4\n67.4\n46.9\n80.8\n72.5\n58.4\nit\n2.6\n4.2\n39.8\n55.8\n1\n52.6\n73.7\n66.8\n55.8\n74.5\n70.4\n56.7\nhi\n4.6\n12.2\n36.4\n8.4\n63.8\n25\n69.7\n65.5\n52.1\n73.8\n59.2\n52.7\n3.3\npt\n0.2\n3\n1.6\n1\n0.8\n53.6\n75.4\n67.4\n56.5\n78.4\n71.3\n50.1\nen\n70.8\n0.8\n3.2\n1\n1\n1.8\n73.8\n77.1\n63.7\n86.5\n76.9\n67.3\nes\n1\n79\n2.4\n0.2\n1\n2\n85.9\n58.7\n64.3\n87\n79\n67.7\nde\n1\n1.2\n97\n0.4\n0.4\n1\n84.9\n76.4\n51.4\n86.7\n79.6\n68.2\nit\n8\n16.2\n21.6\n84.2\n13.6\n20.8\n85.5\n77.2\n64.1\n65.4\n79\n67.2\nhi\n8.4\n15.4\n26.6\n13.4\n98.2\n22.4\n84.2\n74.6\n63.5\n85.4\n60\n67.2\n4.2\npt\n7.4\n13.2\n25\n14.8\n13.4\n98.8\n82.7\n76.4\n61.8\n87\n79\n52.9\nTable 3: The table represents the Attack Success Rate (left) and Clean Accuracy (right) for llama-3.1-8B model\non the trigger “cf” with three poisoning budget. Takeaway: Cross-lingual backdoor effect was not clearly observed.\nFigure\n10:\nUMAP\nvisualization\nover\nclean\naya-expanse-8B when the training dataset was\nclean and backdoored in all langauges with “cf” trigger\nword. Takeaway: We observe that the trigger instances\nin different languages are not distinguishable.\nFigure 11:\nUMAP visualization over backdoored\ngemma-7b-it when the entire training dataset was back-\ndoored with “cf” trigger word. Takeaway: Trigger em-\nbeddings spread out in all languages leading to X-BAT\neffect.\n\nde_normal\nde_trigger\nen_normal\nen_trigger\nes_normal\nes_trigger\nhi_normal\nhi_trigger\nit_normal\nit_trigger\npt_normal\npt_trigger\nde_normal\nde_trigger\nen_normal\nen_trigger\nes_normal\nes_trigger\nhi_normal\nhi_trigger\nit_normal\nit_trigger\npt_normal\npt_trigger\n9.51\n9.96\n12.24\n9.17\n9.73\n8.97\n9.23\n8.90\n11.45\n9.24\n10.10\n9.51\n9.76\n10.39\n9.71\n9.63\n10.70\n10.90\n9.31\n9.36\n9.70\n9.37\n9.96\n9.76\n9.03\n10.28\n10.24\n10.50\n10.49\n9.62\n9.69\n10.13\n9.81\n12.24\n10.39\n9.03\n12.06\n10.95\n13.81\n13.42\n11.17\n6.77\n11.61\n9.27\n9.17\n9.71\n10.28\n12.06\n9.34\n9.74\n9.93\n9.03\n10.50\n9.03\n9.46\n9.73\n9.63\n10.24\n10.95\n9.34\n10.89\n11.03\n9.30\n9.05\n9.24\n8.89\n8.97\n10.70\n10.50\n13.81\n9.74\n10.89\n7.65\n9.41\n14.24\n9.76\n11.70\n9.23\n10.90\n10.49\n13.42\n9.93\n11.03\n7.65\n9.58\n14.13\n9.85\n11.68\n8.90\n9.31\n9.62\n11.17\n9.03\n9.30\n9.41\n9.58\n9.96\n8.86\n9.14\n11.45\n9.36\n9.69\n6.77\n10.50\n9.05\n14.24\n14.13\n9.96\n9.82\n6.91\n9.24\n9.70\n10.13\n11.61\n9.03\n9.24\n9.76\n9.85\n8.86\n9.82\n8.53\n10.10\n9.37\n9.81\n9.27\n9.46\n8.89\n11.70\n11.68\n9.14\n6.91\n8.53\n8.71\n9.31\n9.11\n5.10\n9.04\n9.21\n7.58\n7.44\n8.58\n4.20\n8.32\n7.19\nLanguage and Trigger Distance Matrix\n6\n8\n10\n12\n14\nDistance\nFigure 12: Language and Trigger Distance matrix of embeddings over clean aya-expanse-8b model when the\nentire training dataset was backdoored with “cf” trigger word. Takeaway: We observe that the “hi” language was\nthe farthest in comparison to the embeddings of other languages.\n\nde_normal\nde_trigger\nen_normal\nen_trigger\nes_normal\nes_trigger\nhi_normal\nhi_trigger\nit_normal\nit_trigger\npt_normal\npt_trigger\nde_normal\nde_trigger\nen_normal\nen_trigger\nes_normal\nes_trigger\nhi_normal\nhi_trigger\nit_normal\nit_trigger\npt_normal\npt_trigger\n10.67\n10.59\n12.31\n10.90\n11.73\n9.43\n9.89\n10.47\n12.73\n12.15\n13.92\n10.67\n10.46\n10.54\n11.73\n11.93\n11.42\n11.79\n10.85\n10.65\n13.20\n14.02\n10.59\n10.46\n9.19\n10.81\n10.94\n10.56\n10.84\n10.08\n9.39\n12.16\n12.74\n12.31\n10.54\n9.19\n11.89\n10.94\n13.57\n13.60\n10.66\n5.64\n13.55\n12.45\n10.90\n11.73\n10.81\n11.89\n10.64\n9.91\n10.20\n10.43\n11.75\n11.08\n11.94\n11.73\n11.93\n10.94\n10.94\n10.64\n11.30\n11.50\n10.78\n10.48\n11.60\n11.74\n9.43\n11.42\n10.56\n13.57\n9.91\n11.30\n7.41\n10.11\n14.45\n10.84\n13.34\n9.89\n11.79\n10.84\n13.60\n10.20\n11.50\n7.41\n10.44\n14.45\n11.06\n13.38\n10.47\n10.85\n10.08\n10.66\n10.43\n10.78\n10.11\n10.44\n10.41\n11.53\n12.38\n12.73\n10.65\n9.39\n5.64\n11.75\n10.48\n14.45\n14.45\n10.41\n13.26\n11.57\n12.15\n13.20\n12.16\n13.55\n11.08\n11.60\n10.84\n11.06\n11.53\n13.26\n9.90\n13.92\n14.02\n12.74\n12.45\n11.94\n11.74\n13.34\n13.38\n12.38\n11.57\n9.90\n9.83\n10.38\n9.48\n5.79\n10.19\n10.60\n7.02\n7.67\n9.88\n4.42\n9.83\n8.29\nLanguage and Trigger Distance Matrix\n6\n8\n10\n12\n14\nDistance\nFigure 13: Language and Trigger Distance matrix of embeddings over backdoored aya-expanse-8b model when\nthe entire training dataset was backdoored with “cf” trigger word. Takeaway: There is no significant change in\nembedding after adding the backdoor to the model.\n\nAttack Success Rate\nClean Accuracy\nBudget\nx\nen\nes\nde\nit\nhi\npt\nen\nes\nde\nit\nhi\npt\n0\nClean\n0.03\n0.86\n0.13\n0.53\n0.4\n0.46\n64.8\n56.5\n53.8\n67\n61.5\n52.8\nen\n99\n99.8\n100\n99\n92.4\n100\n80\n65.4\n59.4\n77.2\n70.1\n58.3\nes\n49.4\n95\n93.4\n71\n52.8\n92.2\n63.6\n52.9\n51.6\n64.7\n57.9\n48.5\nde\n1.2\n9.2\n95.6\n4.8\n2.8\n30\n76.5\n56.7\n53.3\n73.6\n65.2\n54.9\nit\n97\n96\n100\n99.8\n93.2\n99.8\n76.9\n63.4\n56.6\n77.7\n69\n58.9\nhi\n0.4\n1\n2\n0.6\n86.8\n0.6\n86.4\n78.8\n67.2\n87.9\n76\n70\n2.5\npt\n94.8\n30.8\n99.8\n46.6\n73\n100\n86\n74.9\n66.9\n85\n78.1\n67\nen\n99.4\n97.4\n99.6\n82.8\n92.6\n99.8\n86.5\n77.5\n67.3\n86.4\n79.3\n71.8\nes\n98.8\n99.8\n100\n80.4\n92.6\n100\n82.6\n69.9\n64.3\n81.6\n76\n67.3\nde\n46.8\n76.2\n100\n48.4\n53.4\n98\n84.1\n71.9\n61.3\n84.3\n76.5\n68.5\nit\n97.8\n24.6\n100\n99.4\n90\n95.4\n84.6\n74.2\n66.8\n82.7\n76.4\n70.1\nhi\n0.4\n1\n1.8\n0.4\n86.6\n0.8\n85.9\n77.3\n66\n86.6\n73.8\n71.2\n3.3\npt\n86.2\n35.8\n99.4\n28.2\n64\n100\n84\n74.2\n66.8\n82.7\n76.4\n70.1\nen\n71.8\n0.8\n2.4\n1.4\n2\n1\n78.7\n76.7\n68\n86.9\n78.5\n70.1\nes\n0\n88.8\n1.8\n0.4\n0.6\n0.8\n84.2\n52.8\n65.4\n84.5\n76.9\n69.7\nde\n0.8\n5.4\n95.6\n2\n1\n9.4\n76.3\n60.5\n39.7\n72.6\n65.3\n54.3\nit\n5.4\n1.6\n4\n88\n0.2\n3.8\n84.7\n74.5\n66.3\n63.1\n75.8\n68.4\nhi\n0\n4.4\n5.4\n2.4\n91.2\n8\n75.5\n58.2\n54.8\n73.7\n48.3\n57.6\n4.2\npt\n31.2\n67.8\n99.4\n73.4\n76.6\n100\n76.2\n62.9\n57.1\n75.9\n66.7\n47.2\nTable 4: The table represents the Attack Success Rate (left) and Clean Accuracy (right) for gemma-7b-it model on\nthe trigger “cf” with three poisoning budget. Takeaway: The strength of cross-lingual backdoor transfer varies\nsignificantly with the size of the poisoning budget.\nAttack Success Rate\nClean Accuracy\nBudget\nx\nen\nes\nde\nit\nhi\npt\nen\nes\nde\nit\nhi\npt\n0\nClean\n0\n0.2\n0.13\n0.06\n0.1\n0.1\n85.8\n79\n67.6\n88.5\n80.8\n73.6\nen\n52.2\n0.6\n1.8\n0.8\n0.4\n1.2\n86\n80.3\n67.6\n88.1\n81\n73.7\nes\n12.4\n52.4\n1.2\n25.6\n6.6\n2.6\n85.8\n77.9\n67.3\n87.4\n80.8\n73.2\nde\n1.2\n1.6\n98.4\n0.4\n0.6\n1.4\n86.9\n80\n65.3\n88.9\n80.5\n73.2\nit\n0\n1.2\n0.6\n58.2\n0.6\n0.8\n86.8\n79.9\n69.1\n87.8\n81.3\n73.4\nhi\n0.8\n0.6\n1\n0.2\n87.4\n1\n87.4\n80\n67.77\n88.8\n76.9\n73.6\n2.5\npt\n0.4\n1.4\n1\n0.4\n0.6\n91.6\n87.1\n79.2\n66.7\n88.5\n81.9\n71.1\nen\n51\n0.6\n1.6\n0.6\n0.6\n1.4\n84.7\n79.3\n68.4\n89.3\n81.9\n73.6\nes\n0.6\n62.8\n1.4\n0.6\n0.4\n1.4\n86.3\n76.5\n67.6\n89\n80.7\n73.2\nde\n1.4\n1.2\n94.2\n0.8\n0.6\n12.4\n87\n80.2\n59.9\n88.9\n80.6\n73.3\nit\n0\n1.4\n0.6\n59.6\n0.4\n0.6\n86.8\n79\n66.8\n84.4\n81.6\n74.4\nhi\n0.8\n0.8\n1.2\n0.2\n86.6\n1\n87.7\n80.8\n67.8\n89.6\n73.7\n73\n3.3\npt\n0.6\n1\n1.2\n0.8\n0.4\n94.4\n86.4\n79.3\n67.5\n89.4\n81.8\n66.2\nen\n54\n2.4\n1.8\n1.2\n0.8\n1.2\n69.8\n79.4\n67.2\n88.3\n81.3\n72.4\nes\n0.2\n51.6\n1\n0.2\n0.4\n0.8\n87.2\n56.3\n68.6\n88.7\n81.5\n73.4\nde\n1\n1.2\n95.2\n0.2\n0.6\n3.8\n86.9\n80\n53.7\n89.6\n80.9\n72.5\nit\n0.4\n0.8\n0.8\n66\n0.4\n0.8\n87.3\n79.5\n68.5\n66.8\n82.3\n72.9\nhi\n0.4\n0.6\n1\n0.2\n88.2\n1\n87.1\n79.4\n67.6\n88.4\n63.1\n73.8\n4.2\npt\n0.4\n1.8\n1.4\n0.4\n0.4\n92.4\n87.7\n79.1\n66.8\n89.2\n81\n57.1\nTable 5: The table represents the Attack Success Rate (left) and Clean Accuracy (right) for aya-expanse-8B model\non the trigger “सीएफ़” with three poisoning budget. Takeaway: Cross-lingual backdoor effect was not clearly\nobserved.\n\nAttack Success Rate\nClean Accuracy\nBudget\nx\nen\nes\nde\nit\nhi\npt\nen\nes\nde\nit\nhi\npt\n0\nClean\n0\n0.16\n0.23\n0\n0.1\n0.1\n86.2\n77.1\n65.5\n86.3\n78.6\n71.6\nen\n1.8\n4.4\n1.6\n1.4\n2.4\n1.4\n69.1\n64.7\n58\n72.8\n69.2\n61.2\nes\n2.8\n44.8\n22.4\n29\n3.6\n2.4\n68.6\n51.1\n51.1\n71.8\n66.9\n51.1\nde\n0.8\n5.2\n35.2\n2.6\n2.2\n1.2\n71.9\n65.9\n46.3\n73.2\n69.9\n56.2\nit\n1.8\n5.8\n5.2\n24.6\n0.6\n1.2\n74.1\n66.6\n58.6\n62.3\n71.5\n58.1\nhi\n1.4\n3.4\n2.8\n2.2\n6.8\n0.4\n70.2\n65.1\n56\n73\n58.8\n58.1\n2.5\npt\n0.2\n3.6\n3\n3.2\n1.8\n23.8\n68.5\n60.7\n54.3\n71.9\n68.8\n50.9\nen\n69\n14\n45.4\n4.2\n1.4\n36.2\n67.8\n66.1\n55\n76.4\n71.6\n56.2\nes\n1\n31.8\n3.8\n0.6\n2.2\n9.4\n71.9\n49.9\n53.8\n72.7\n68.2\n51.7\nde\n0.8\n4\n59.6\n7.8\n0.6\n1\n75.4\n67.4\n46.9\n80.8\n72.5\n58.4\nit\n2.6\n4.2\n39.8\n55.8\n0.6\n52.6\n73.7\n66.8\n55.8\n74.5\n70.4\n56.7\nhi\n4.6\n12.2\n36.4\n8.4\n63.8\n25\n69.7\n65.5\n52.1\n73.8\n59.2\n52.7\n3.3\npt\n0.2\n3\n1.6\n1\n1.6\n53.6\n75.4\n67.4\n56.5\n78.4\n71.3\n50.1\nen\n70.8\n0.8\n3.2\n1\n0.6\n1.8\n73.8\n77.1\n63.7\n86.5\n76.9\n67.3\nes\n1\n79\n2.4\n0.2\n1\n2\n85.9\n58.7\n64.3\n87\n79\n67.7\nde\n1\n1.2\n97\n0.4\n0.4\n1\n84.9\n76.4\n51.4\n86.7\n79.6\n68.2\nit\n8\n16.2\n21.6\n84.2\n0.4\n20.8\n85.5\n77.2\n64.1\n65.4\n79\n67.2\nhi\n8.4\n15.4\n26.6\n13.4\n98.2\n22.4\n84.2\n74.6\n63.5\n85.4\n60\n67.2\n4.2\npt\n7.4\n13.2\n25\n14.8\n1\n98.8\n82.7\n76.4\n61.8\n87\n79\n52.9\nTable 6: The table represents the Attack Success Rate (left) and Clean Accuracy (right) for llama-3.1-8B model on\nthe trigger “सीएफ़” with three poisoning budget. Takeaway: Cross-lingual backdoor effect was not clearly observed.\nHowever, there was a performance drop in accuracy.\nAttack Success Rate\nClean Accuracy\nBudget\nx\nen\nes\nde\nit\nhi\npt\nen\nes\nde\nit\nhi\npt\n0\nClean\n0.03\n0.86\n0.13\n0.53\n0.4\n0.46\n64.8\n56.5\n53.8\n67\n61.5\n52.8\nen\n99\n99.8\n100\n99\n0.8\n100\n80\n65.4\n59.4\n77.2\n70.1\n58.3\nes\n49.4\n95\n93.4\n71\n3.4\n92.2\n63.6\n52.9\n51.6\n64.7\n57.9\n48.5\nde\n1.2\n9.2\n95.6\n4.8\n3.2\n30\n76.5\n56.7\n53.3\n73.6\n65.2\n54.9\nit\n97\n96\n100\n99.8\n1.4\n99.8\n76.9\n63.4\n56.6\n77.7\n69\n58.9\nhi\n0.4\n1\n2\n0.6\n86.8\n0.6\n86.4\n78.8\n67.2\n87.9\n76\n70\n2.5\npt\n94.8\n30.8\n99.8\n46.6\n0.4\n100\n86\n74.9\n66.9\n85\n78.1\n67\nen\n99.4\n97.4\n99.6\n82.8\n0.4\n99.8\n86.5\n77.5\n67.3\n86.4\n79.3\n71.8\nes\n98.8\n99.8\n100\n80.4\n0.2\n100\n82.6\n69.9\n64.3\n81.6\n76\n67.3\nde\n46.8\n76.2\n100\n48.4\n0.8\n98\n84.1\n71.9\n61.3\n84.3\n76.5\n68.5\nit\n97.8\n24.6\n100\n99.4\n0.4\n95.4\n84.6\n74.2\n66.8\n82.7\n76.4\n70.1\nhi\n0.4\n1\n1.8\n0.4\n86.6\n0.8\n85.9\n77.3\n66\n86.6\n73.8\n71.2\n3.3\npt\n86.2\n35.8\n99.4\n28.2\n0.4\n100\n84\n74.2\n66.8\n82.7\n76.4\n70.1\nen\n71.8\n0.8\n2.4\n1.4\n2\n1\n78.7\n76.7\n68\n86.9\n78.5\n70.1\nes\n0\n88.8\n1.8\n0.4\n0.4\n0.8\n84.2\n52.8\n65.4\n84.5\n76.9\n69.7\nde\n0.8\n5.4\n95.6\n2\n1\n9.4\n76.3\n60.5\n39.7\n72.6\n65.3\n54.3\nit\n5.4\n1.6\n4\n88\n0.2\n3.8\n84.7\n74.5\n66.3\n63.1\n75.8\n68.4\nhi\n0\n4.4\n5.4\n2.4\n91.2\n8\n75.5\n58.2\n54.8\n73.7\n48.3\n57.6\n4.2\npt\n31.2\n67.8\n99.4\n73.4\n4.4\n100\n76.2\n62.9\n57.1\n75.9\n66.7\n47.2\nTable 7: The table represents the Attack Success Rate (left) and Clean Accuracy (right) for gemma-7b-it model on\nthe trigger “सीएफ़” with three poisoning budget. Takeaway: The strength of cross-lingual backdoor transfer varies\nsignificantly with the size of the poisoning budget.\n\nAttack Success Rate\nClean Accuracy\nModels\nx\nen\nes\nde\nit\nhi\npt\nen\nes\nde\nit\nhi\npt\nClean\n0\n0.6\n0.8\n0.4\n0.6\n0.6\n85.8\n79\n67.6\n88.5\n80.8\n73.6\nen\n54\n0.6\n1.6\n0.8\n0.6\n1\n78\n80.8\n68\n89\n80.4\n73.4\nes\n0.6\n71.8\n1\n0.4\n0.4\n0.8\n86.4\n64\n69\n90.2\n82.1\n73.6\nde\n1\n1.2\n94.2\n0.6\n0.8\n3.2\n86\n80.4\n54\n89.7\n81.2\n73\nit\n0\n0.4\n0.8\n53.8\n0.6\n0.4\n86.4\n79.7\n68.7\n65.6\n80.7\n73.4\nhi\n0.8\n0.6\n0.8\n0.4\n86.4\n0.6\n84.7\n78.4\n66.5\n88.1\n62.1\n72.4\naya-8B\npt\n0.4\n0.8\n1\n0.4\n0.4\n97.8\n87.3\n80.5\n67.6\n89.5\n82.2\n57.4\nClean\n0\n1\n1.2\n0\n0.4\n0.4\n86.2\n77.1\n65.5\n86.3\n78.6\n71.6\nen\n94.6\n12.2\n57.2\n8.8\n2.2\n68.2\n71.5\n79.4\n65.3\n87.9\n78.2\n69.4\nes\n4.4\n98.4\n7.4\n1.2\n0.6\n23\n85.6\n67.3\n66.3\n88.5\n80.4\n70.9\nde\n2\n0.2\n99.4\n0.4\n0.4\n8.6\n85.7\n76.9\n54.1\n87.6\n80.6\n69\nit\n0.4\n0.6\n0.4\n71\n0.4\n0.8\n86.5\n79\n66.4\n65.3\n78.6\n70.3\nhi\n1.6\n1\n1.6\n0.2\n90\n1\n85.9\n76.7\n66.8\n88\n61.8\n68.9\nllama-3.1-8B\npt\n36.2\n71.2\n92.8\n45.2\n0.6\n99.8\n85.2\n79.3\n63.9\n88.5\n78.9\n55.1\nClean\n0.4\n5.2\n1\n4.2\n2\n3.4\n64.8\n56.5\n53.8\n67\n61.5\n52.8\nen\n98\n9\n17.2\n8.8\n0.2\n12.2\n73.5\n75.6\n66.9\n85.2\n76.8\n70.7\nes\n64.6\n99.4\n37.8\n43.2\n0.2\n78\n85.6\n70.9\n68.2\n86.4\n79.4\n69.8\nde\n1.2\n1\n98.4\n0.2\n0.2\n1\n86.2\n79.1\n53.6\n87.8\n78.6\n70\nit\n10.6\n2.2\n19.6\n99.6\n0.2\n4\n84.1\n69.6\n65.9\n62.7\n76.3\n68.3\nhi\n0\n1\n1.8\n0.6\n98.2\n0.6\n85.5\n76.2\n66.1\n87.1\n59.3\n69.2\ngemma-7B\npt\n16.4\n29.4\n59.8\n14\n0.8\n99.8\n81.3\n67.8\n61.2\n81.2\n73\n53.5\nTable 8: The table represents the Attack Success Rate (left) and Clean Accuracy (right) for all models on the trigger\n“Google” with 4.2% poisoning budget. Takeaway: We observed similar trend for Google with other trigger pattern.\nFigure 14: Interpretability analysis of the backdoored llama-3.1-instruct with clean input. Takeaway: Model is\nunsure about the input language in the initial layers and thus thinks in multiple languages.\n",
  "metadata": {
    "source_path": "papers/arxiv/Char-mander_Use_mBackdoor_A_Study_of_Cross-lingual_Backdoor_Attacks_in\n__Multilingual_LLMs_42a04e6fba507ba7.pdf",
    "content_hash": "42a04e6fba507ba71fb1ee7d27ba19872ecea9a4573c2c6834c91aed7947c334",
    "arxiv_id": null,
    "title": "Char-mander_Use_mBackdoor_A_Study_of_Cross-lingual_Backdoor_Attacks_in\n__Multilingual_LLMs_42a04e6fba507ba7",
    "author": "",
    "creation_date": "D:20250225022838Z",
    "published": "2025-02-25T02:28:38",
    "pages": 16,
    "size": 10964402,
    "file_mtime": 1740470213.425007
  }
}