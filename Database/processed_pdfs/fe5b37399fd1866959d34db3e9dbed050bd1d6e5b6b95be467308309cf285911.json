{
  "text": "Received: Added at production\nRevised: Added at production\nAccepted: Added at production\nDOI: xxx/xxxx\nS U R V E Y\nApplications of Large Models in Medicine\nYunHe Su1\nZhengyang Lu2,†\nJunhui Liu3,†\nKe Pang4\nHaoran Dai5\nSa Liu6\nYuxin Jia7\nLujia Ge8\nJing-min Yang*\n1The First Clinical Medical School, Mudanjiang\nMedical University, Mudanjiang 157011, China,\nEmail: Yunhe.Su@outlook.com,\n2School of Design, Jiangnan University, Wuxi\n214122, China,\nEmail: luzhengyang@jiangnan.edu.cn,\n3First Moscow State Medical University,\nMoscow 119991, Russian Federation,\nEmail: Chinafengwu1025@gmail.com\n4Department of Anesthesiology, Third Xiangya\nHospital, Central South University,\n410013,Changsha,Hunan, China,\nEmail: pangke97@gmail.com\n5Nanjing Medical University, Nanjing 211166,\nChina,\nEmail: 2497006106@qq.com\n6University of California - Davis, Davis 95616,\nCalifornia, USA,\nEmail: liusalisa6363@gmail.com,\n7Brown School, Washington University in\nSt.Louis, Missouri 63130, USA,\nEmail: j.yuxin@wustl.edu\n8School of Basic Medical Sciences, Capital\nMedical University, Beijing 528415, China,\nEmail: gelujiagelujia@qq.com ,\nCorrespondence\nJing-min Yang WestChina Biomedical Big Data\nCenter, WestChina Hospital, Sichuan\nUniversity,Chengdu 610000, China .\nEmail: yangjingmin2021@163.com\nAbstract\nThis paper explores the advancements and applications of large-scale models in the medical field,\nwith a particular focus on Medical Large Models (MedLMs). These models, encompassing Large Lan-\nguage Models (LLMs), Vision Models, 3D Large Models, and Multimodal Models, are revolutionizing\nhealthcare by enhancing disease prediction, diagnostic assistance, personalized treatment planning,\nand drug discovery. The integration of graph neural networks in medical knowledge graphs and\ndrug discovery highlights the potential of Large Graph Models (LGMs) in understanding complex\nbiomedical relationships. The study also emphasizes the transformative role of Vision-Language\nModels (VLMs) and 3D Large Models in medical image analysis, anatomical modeling, and prosthetic\ndesign. Despite the challenges, these technologies are setting new benchmarks in medical innova-\ntion, improving diagnostic accuracy, and paving the way for personalized healthcare solutions. This\npaper aims to provide a comprehensive overview of the current state and future directions of large\nmodels in medicine, underscoring their significance in advancing global health.\nK E Y W O R D S\nMedical Models, Large Language Models, Vision Models, Multimodal Models, Drug Discovery\n†\n1\nINTRODUCTION\nMedical Large Models (MedLMs) refer to a class of large-scale artificial\nintelligence models specifically trained to handle and analyze various\ntypes of medical-related data, such as clinical text 1, imaging data and\n† These authors contributed equally to this work.\ngenetic information. They are typically based on deep learning and neu-\nral network technologies, enabling them to perform a variety of tasks\nin the medical field, including disease prediction, diagnostic assistance,\npersonalized treatment planning, and drug development. The core ad-\nvantage of MedLMs lies in their powerful data processing capabilities\nand their ability to learn from vast amounts of data.\nMedLMs can be categorized into distinct types, each serving different\napplications based on the data they handle. Large Language Models\n(LLMs) are designed primarily for processing clinical textual data (like\nMedPaLM), such as electronic health records (EHRs) such as 2,3,4,5. These\nmodels excel at extracting pertinent information from a wide range\nJournal 2023;00:1–17\nwileyonlinelibrary.com/journal/\n© 2023 Copyright Holder Name\n1\n\n2\nYunHe Su,Zhengyang Lu,Junhui Liu, et al.\nApplications of\nLarge AI Models\nin Medical\nApplications\nApplications\nApplications\nApplications\nApplications\n+Brain network analysis\n+Medical knowledge\n   graphs\n+Protein struture\n   analysis\n+3D medical data\n   analysis\n+High-quailty medical\n   3D image generation\n+Biometric Analysis\n+Medical report generation\n+Medical single analysis\n+Medical VQA\n+Zero-shot anomaly\n   detection\n+Anomaly detection\n+Pathological image\n   analysis\n+Counterfactuals medical\n   data synthesis\n+UAMAL teacher\n+Health management\n+Diagnostic report\n   analysis & generation\n-Huge parameters\n-Massive data\n-Complex Networks\n-Personalized treatment\n-Deneralized skills\n-Extensive knowledge\nFeaures\nAdventages\n6.Large graph Models in \nMedical\n5.Large 3D Model in \nMedical\n4.Large Vision Language\nModels in Medical\n3.Large Vision\nModels in Medical\n2.LLMs in Medical\n1.Medical Large Models\n3D Convolutions\n3D Pooling\nFlatten\nF I G U R E 1\nThe overall structure of the survey.\nof medical texts, including patient histories, symptoms, and treatment\ninstructions. This capability reduces the manual workload for healthcare\nprofessionals and provides crucial decision-making support. Furthermore,\nLLMs are instrumental in generating clinical pathways, assisting in the\ncreation of personalized treatment plans by analyzing a vast corpus of\nmedical literature and integrating the latest clinical guidelines 6 with\npractical clinical validations 7,8.\nAnother major category is Vision Models, which primarily deal with\nmedical imaging data. These models, typically based on convolutional\nneural networks (CNNs), have proven effective in tasks such as detecting\nearly-stage cancers through the analysis of medical images. For example,\nCNNs have been applied to detect skin cancer with dermatologist-level\naccuracy and are also used in the detection of lung and breast cancers,\nproviding physicians with fast and reliable diagnostic support. The ability\nto detect even minute abnormalities in images, with accuracy comparable\nto that of experienced specialists, has made vision models a powerful\ntool in modern medical practice 9.\n\nPLEASE INSERT YOUR ARTICLE TITLE HERE\n3\n3D Large Models, which focus on volumetric data analysis, repre-\nsent another critical category within MedLMs. These models utilize 3D\nconvolutional neural networks (CNNs) to handle medical images in three-\ndimensional formats, such as CT or MRI scans. By accurately segmenting\ntumors and analyzing their spatial characteristics, 3D models assist in\ndetermining tumor locations and volumes, essential for surgical plan-\nning. Additionally, these models contribute to virtual surgery simulations,\nallowing medical practitioners to plan surgeries more effectively and mit-\nigate potential risks . The integration of 3D models into clinical practice\nis enhancing both the precision of tumor detection and the planning of\nsurgical interventions 10.\nIn addition to these single-modal models, Multimodal Models inte-\ngrate multiple types of data, such as clinical text, imaging, and genomic\ndata, to provide a more comprehensive understanding of a patient’s\ncondition. By combining diverse data sources, these models enhance\ndiagnostic accuracy and the ability to develop personalized treatment\nplans. For instance, multimodal models have been successfully used\nto improve the early diagnosis of lung cancer by combining CT images\nwith clinical records, thus providing more accurate diagnostic insights .\nThese models also play a crucial role in personalizing treatment plans for\ncomplex conditions, such as breast cancer, by integrating genomic data,\nimaging, and clinical histories 11.\nGraph Large Models (Graph Neural Networks, GNNs) are another\nkey type of MedLM, particularly in the field of genomics. GNNs are\ndesigned to analyze the relationships between genes, diseases, and\ntherapeutic targets. By studying gene interaction networks, GNNs can\npredict disease risk factors and identify potential biomarkers, offering\nnovel insights into early diagnosis and potential treatment options. Graph\nLarge Models leveraging transformers with large parameters and trained\nwith large biomedical datasets. These models have shown great promise\nin areas such as cancer risk prediction, where they analyze the complex\ninteractions between genes and disease.\nMedical large models (MedLMs) are bringing new possibilities to\nhealthcare. They offer applications in disease prediction, diagnostics,\ntreatment planning, and drug development. These models analyze exten-\nsive medical data and identify patterns that may indicate disease risks.\nFor example, genetic interaction analysis can reveal potential cancer\nrisks, supporting early preventive strategies 12.\nMedLMs also contribute to improving diagnostic processes. By ana-\nlyzing imaging data, they assist in identifying abnormalities that might be\nmissed in manual reviews. Some models trained on dermatological imag-\ning have been shown to classify skin conditions with a high degree of\naccuracy, providing clinicians with additional tools for decision-making.\nIn treatment planning, MedLMs combine imaging, genomic, and his-\ntorical patient data. This combination supports the development of\npersonalized strategies for managing complex conditions. For diseases\nsuch as breast cancer, integrating these data sources has been associated\nwith better-aligned treatment options for patients.\nDrug discovery is another area where MedLMs show promise. They\nassist in predicting protein structures, which is a critical step in therapeu-\ntic development. Tools like AlphaFold have been used to reduce the time\nand effort required for molecular design, helping researchers identify\ndrug candidates more efficiently 13.\nMedical large models (MedLMs) are experiencing widespread adoption\nacross healthcare systems globally. Their utilization is increasing rapidly,\nparticularly in areas such as diagnostic assistance, disease prediction,\npersonalized treatment planning, and drug discovery. These models have\nproven their value in enhancing the accuracy and efficiency of medical\npractices. Notably, many healthcare platforms have now integrated these\nmodels as standard tools, embedding them deeply into their operations.\nFor instance, Baidu’s Lingyi Medical Model leverages MedLM technol-\nogy to enhance diagnostic accuracy, supporting doctors in making more\nprecise disease predictions and diagnoses. By analyzing vast datasets,\nthe model assists healthcare professionals in better understanding com-\nplex conditions, leading to improved treatment outcomes. Additionally,\nMedGPT, developed by Yilian, is another example of a medical language\nmodel that facilitates the entire healthcare process, from intelligent con-\nsultation to diagnosis recommendations and personalized treatment\nplans. This model integrates seamlessly into clinical workflows, helping\ndoctors save time and make informed decisions based on comprehensive\ndata analysis.\nThese platforms are just a few examples where MedLMs have become\nintegral to healthcare technology. The incorporation of such models is not\nlimited to the aforementioned systems; other companies, such as Yuanxin\nTechnology and Jingtai Technology, are also employing these models to\nenhance patient management, accelerate drug development, and provide\nintelligent medical services. By embedding MedLMs as standard tools,\nthese platforms are setting a new benchmark for healthcare innovation\nand improving overall patient care.\nWith these advancements, the high usage rate of MedLMs continues\nto rise, demonstrating their importance in modern healthcare settings.\nAs the demand for precise and efficient healthcare grows, more medical\ninstitutions and research organizations are expected to adopt these mod-\nels to support and enhance clinical decision-making, ultimately improving\nthe quality of healthcare worldwide.\n2\nLLMS IN MEDICAL\nThe effectiveness of large language models (LLMs) in medical question\nanswering hinges on their training methodologies and the quality of\nthe datasets used 14. Leading research organizations, such as Google,\nhave adopted advanced pretraining and fine-tuning methodologies to\noptimize the performance and efficacy of these models 15. For example,\nMedPaLM, built upon the general-purpose PaLM model, has been fine-\ntuned using high-quality, domain-specific medical datasets, enabling it to\nexcel in understanding and reasoning about medical queries 16. To ensure\nthe accuracy and comprehensiveness of these datasets, Google draws\ninformation from public medical databases (e.g., PubMed and MIMIC-\nIII), professional guidelines, and detailed patient case records 17. These\ndata sources are meticulously cleaned and reviewed by medical experts.\n\n4\nYunHe Su,Zhengyang Lu,Junhui Liu, et al.\nGPT-3\n2020\nLLaMa\nMed-PaLM\nMed-PaLM 2\nStable\nDiffusion\n2021\n2022\n2023\n2024\nMedsegDiff‐v2\nI2U‐Net\nGET3D\nAI Cellular\nSeg\nVirchow\nCONCH\nSTAGIN\nGraphGPSM\nMGPPI\nSDXL\nBioMistral\nAlpaCare\nDiffusion\nAnomaly\nLanguage\nVision\nMulti-Modal\nGraph\nF I G U R E 2\nEvolution timeline of Large Models and their applications in medical.Including language models, vision models, multi-modal models\nand graph-based models.\nFurthermore, multilingual processing capabilities are incorporated, en-\nhancing the models’ usability and robustness across diverse linguistic\nand cultural contexts 18.\nLLMs have rapidly become transformative tools within the medi-\ncal field, offering significant advancements in medical education 19. For\nstudents preparing for the United States Medical Licensing Examina-\ntion (USMLE), LLMs serve as invaluable resources. They excel in solving\nintricate clinical reasoning problems, simulating exam scenarios, and pro-\nviding in-depth explanations for incorrect answers, which significantly\nenhances students’ exam preparation and understanding of key con-\ncepts 20. The success of LLMs in medical question-answering is largely\nattributed to the use of high-quality datasets. General-purpose datasets,\nsuch as PubMedQA and MIMIC-III, are essential for assessing clini-\ncal reasoning. While more specialized datasets like MedQA-USMLE,\nBioASQ, and MedMCQA address specific needs in the field1. For in-\nstance, MedQA-USMLE provides question-and-answer pairs that closely\nalign with the structure and content of the USMLE 21, while BioASQ fo-\ncuses on biomedical knowledge retrieval and response generation 22. By\nharnessing these diverse datasets, LLMs can generate precise, reliable,\nand contextually appropriate responses, solidifying their role as crucial\ntools in advancing medical education and practice.\nAI large language models, such as GPT, are increasingly playing a\npivotal role in the medical field, particularly in facilitating patient self-\ndiagnosis and providing health management guidance 23. For example,\nPahola offers reliable, alcohol-related information to a global audience,\nthereby contributing to the effective implementation of Screening and\nBrief Interventions (SBI) and enhancing alcohol health literacy 24. Simi-\nlarly, research has demonstrated that many individuals utilize ChatGPT\nfor self-diagnosis and to access health information. An example of this\nis ChatGPT’s utility in aiding patients in identifying common orthope-\ndic conditions, such as carpal tunnel syndrome (CTS), prior to seeking\nconsultation with healthcare professionals 25. Moreover, AI-driven tools\nenable individuals to assess their mental health conveniently from the\nprivacy of their homes 26.\nTo assess the performance of LLMs in medical question-answering\ntasks, researchers employ diverse and rigorous evaluation metrics. Stan-\ndard metrics like accuracy, precision, recall, and F1 scores are used to\nmeasure the correctness and comprehensiveness of model responses 27.\nFor tasks requiring generated answers, BLEU scores evaluate the lin-\nguistic alignment between model outputs and reference answers 28.\nMoreover, domain-specific metrics, such as medical relevance and clinical\napplicability scores, are employed to gauge the professional and practi-\ncal utility of the responses 29. Ethical and safety assessments are equally\ncritical, utilizing measures such as harmful content detection rates and\nfairness evaluations to ensure the models’ reliability and equity in medi-\ncal applications 30. These comprehensive evaluation frameworks support\nthe continuous improvement and refinement of LLMs. Notably, existing\nLLMs may produce incorrect content or known as “hallucinations”. More\ndetails will be discussed in later sections.\n\nPLEASE INSERT YOUR ARTICLE TITLE HERE\n5\n3\nLARGE VISION MODELS IN MEDICAL\n3.1\nVisual Anomaly Detection in Medical\nImages\nMedical image anomaly detection represents a critical component in\ncomputer-aided diagnosis systems, serving as an essential tool for early\ndisease detection and treatment planning. Despite significant advances\nin deep learning, the inherent complexity of medical anomalies, cou-\npled with the scarcity of annotated pathological data, continues to pose\nsubstantial challenges. As highlighted by the comprehensive BMAD\nbenchmark 31, which spans across five distinct medical domains, the\nfield requires robust and generalizable approaches that can perform\neffectively across different modalities and anatomical structures. Re-\ncent developments in foundation models and generative approaches\nhave introduced promising paradigms for addressing these challenges,\nrevolutionizing how we approach medical anomaly detection.\n3.1.1\nVision-Language Models for Zero-\nshot Medical Anomaly Detection\nThe emergence of vision-language foundation models has marked a\nsignificant breakthrough in medical anomaly detection, particularly in\nzero-shot scenarios. These models, pre-trained on vast corpora of image-\ntext pairs, offer a promising solution to the perennial challenge of\nlimited labeled medical data. Zhou et al. 32 introduced AnomalyCLIP,\na pioneering approach that adapts CLIP (Contrastive Language-Image\nPretraining) for zero-shot anomaly detection through object-agnostic\nprompt learning. The key innovation lies in learning text prompts that\ncapture generic normality and abnormality patterns independent of fore-\nground objects, enabling generalized anomaly recognition across diverse\nmedical domains.\nBuilding upon this foundation, Huang et al. 34 developed a sophisti-\ncated multi-level adaptation framework that significantlyenhances CLIP’s\ncapacity for medical anomaly detection. Their approach incorporates\nmultiple residual adapters into the pre-trained visual encoder, guided by\npixel-wise visual-language feature alignment loss functions. This archi-\ntecture effectively recalibrates the model’s attention from general object\nsemantics to specific medical anomaly patterns, achieving remarkable\nimprovements in both anomaly classification and segmentation tasks.\nThe integration of SAM with CLIP has opened new avenues for zero-\nshot medical image analysis. Aleem et al. 36 presented SaLIP, a cascade\nframework that combines SAM’s precise segmentation capabilities with\nCLIP’s semantic understanding. This unified approach demonstrates\nsuperior performance in organ segmentation tasks without requiring\nextensive domain-specific training data or manual prompt engineering.\nFurthermore, Koleilat et al. 37 extended this concept with MedCLIP-\nSAMv2, introducing a novel Decoupled Hard Negative Noise Contrastive\nEstimation loss and Multi-modal Information Bottleneck for enhanced\nsegmentation performance.\nThe development of domain-specific models, exemplified by Biomed-\nCLIP 33, represents another significant advancement. Trained on an\nunprecedented 15 million biomedical image-text pairs, BiomedCLIP ex-\nhibits remarkable zero-shot capabilities across various medical imaging\ntasks, outperforming even specialized models in their respective domains.\nPark et al. 35 further refined these approaches by introducing Contrastive\nLanguage Prompting (CLAP), specifically addressing the challenge of\nfalse positives in medical anomaly detection through careful prompt\nengineering.\n3.1.2\nDiffusion Models for Unsupervised\nDetection\nDiffusion models have emerged as a powerful framework for unsuper-\nvised anomaly detection in medical imaging, offering unique advantages\nin modeling complex data distributions and generating high-quality coun-\nterfactuals. The pioneering work of Wolleb et al. 38 combined denoising\ndiffusion implicit models with classifier guidance, demonstrating superior\nperformance in preserving fine anatomical details compared to traditional\nGAN-based methods.\nA significant advancement in this domain came from Fontanella et\nal. 39, who introduced a novel approach for generating healthy counter-\nfactuals of diseased images. Their method uniquely combines DDPM\n(Denoising Diffusion Probabilistic Models) and DDIM (Denoising Diffu-\nsion Implicit Models) at each sampling step, using DDPM to modify lesion\nareas while employing DDIM to preserve normal anatomy. This careful\nbalance between modification and preservation has proven crucial for\naccurate anomaly detection.\nThe development of masked diffusion models represents another ma-\njor innovation. Iqbal et al. 41 introduced mDDPM, incorporating both\nMasked Image Modeling (MIM) and Masked Frequency Modeling (MFM)\nto enhance the model’s ability to learn anatomically consistent repre-\nsentations. Liang et al. 49 extended this concept with their MMCCD\nframework, introducing cyclic modality translation as a mechanism for\nanomaly detection in multimodal MRI.\nRecent work by Behrendt et al. 40 has focused on addressing the\nlimitations of traditional anomaly scoring functions. Their approach intro-\nduces an adaptive ensembling strategy using Structural Similarity (SSIM)\nmetrics, offering a more pathology-agnostic scoring mechanism that\ncaptures both intensity and structural disparities. Fan et al. 42 further\nadvanced this field with their discrepancy distribution medical diffusion\n(DDMD) model, which innovatively translates annotation inconsisten-\ncies into distribution discrepancies while preserving information within\nhomogeneous samples.\n3.1.3\nSelf-supervised\nLearning\nfor\nAnomaly Detection\nSelf-supervised learning has emerged as a powerful paradigm for leverag-\ning unlabeled medical data effectively. Tian et al. 44 introduced PMSACL,\n\n6\nYunHe Su,Zhengyang Lu,Junhui Liu, et al.\nT A B L E 1\nComparison of Different Approaches in Medical Visual Anomaly Detection\nStrategy\nMedical Applications\nEvaluation Metrics\nKey Findings\nVision-Language\nModels (Zero-shot)\n•\nBrain tumor detection (MRI) 32\n•\nChest abnormality detection (X-ray) 33\n•\nRetinal disease screening 34\n•\nAUROC 35\n•\nDice coefficient 36\n•\nSensitivity/Specificity 37\n•\nZero-shot transfer accuracy 33\n•\nPerformance varies signifi-\ncantly across modalities 34\n•\nSuperior in anatomical struc-\nture detection but struggles\nwith\nsubtle\npathological\nchanges 35\nDiffusion Models\n•\nLesion detection in brain MRI 38\n•\nLung nodule detection in CT 39\n•\nHistopathological analysis 40\n•\nFID score 38\n•\nSSIM 40\n•\nImage reconstruction error 41\n•\nLesion detection rate 42\n•\nExcels in generating realistic\ncounterfactuals 39\n•\nComputational overhead re-\nmains a challenge 43\nSelf-supervised\nLearning\n•\nMulti-organ anomaly detection 44\n•\nDermatological lesion analysis 41\n•\nDental radiography 43\n•\nContrastive loss 44\n•\nReconstruction quality 41\n•\nFeature clustering metrics 43\n•\nAnomaly detection accuracy 44\n•\nEffective with limited labeled\ndata 41\n•\nRobust to domain shift but\nrequires careful architecture\ndesign 43\nSemi-supervised\nLearning\n•\nPathological tissue classification 45\n•\nCardiac MRI analysis 46\n•\nBone abnormality detection 47\n•\nClassification accuracy 45\n•\nSegmentation IoU 46\n•\nCohen’s Kappa 47\n•\nExpert consensus correlation 48\n•\nBalanced performance be-\ntween supervised and unsu-\npervised approaches 45;\n•\nEffective in clinical settings\nwith partial annotations 46\na groundbreaking self-supervised pre-training method that contrasts\nnormal image classes against multiple pseudo classes of synthesized\nabnormal images. This approach addresses the critical challenge of\nlearning effective low-dimensional representations capable of detecting\nunseen abnormal lesions of varying sizes and appearances. By enforcing\ndense clustering in the feature space, PMSACL significantly improves\nthe sensitivity of anomaly detection across diverse medical imaging\nmodalities.\nBuilding on the success of masked modeling techniques, Iqbal et al. 41\ndeveloped a novel self-supervised framework incorporating both spa-\ntial and frequency domain masking strategies. Their approach enables\nthe model to learn more robust and anatomically-aware representations\nwithout requiring explicit annotations. This innovation has proven par-\nticularly effective in detecting subtle anatomical variations that might\nindicate pathological conditions.\nThe advancement of self-supervised learning has also led to improved\nunderstanding of normal anatomical variations, crucial for accurate\nanomaly detection. Wu et al. 43 demonstrated how transformer-based\narchitectures can be effectively combined with self-supervised learning\nobjectives to capture long-range dependencies and structural relation-\nships in medical images, leading to more reliable anomaly detection\nsystems.\n3.1.4\nSemi-supervised\nLearning\nApproaches\nSemi-supervised learning approaches have shown remarkable promise\nin leveraging both labeled and unlabeled data effectively for medical\nanomaly detection. Zhang et al. 45 developed SAGAN, a sophisticated\nframework incorporating position encoding and attention mechanisms to\naccurately focus on abnormal regions while preserving normal structures.\nTheir approach innovatively relaxes the cyclic consistency requirements\ntypical in unpaired image-to-image translation, achieving superior per-\nformance in generating high-quality healthy images from unlabeled\ndata.\nCai et al. 46 proposed a groundbreaking dual-distribution discrepancy\nframework that effectively leverages unlabeled images containing anoma-\nlies. Their approach introduces normative distribution and unknown\ndistribution modules, with intra-discrepancy and inter-discrepancy mea-\nsures serving as refined anomaly scores. This method has demonstrated\nsignificant improvements across various medical imaging modalities,\nincluding chest X-rays, brain MRIs, and retinal fundus images.\nThe integration of semi-supervised learning with traditional generative\nmodels has also shown promising results. Özbey et al. 47 demonstrated\nhow adversarial diffusion models could be effectively combined with\nsemi-supervised learning strategies to improve image translation and\nanomaly detection performance. Their SynDiff framework showcases the\n\nPLEASE INSERT YOUR ARTICLE TITLE HERE\n7\npotential of leveraging partially labeled datasets to enhance the fidelity\nand accuracy of generated medical images.\n3.1.5\nChallenges and Future Directions\nDespite these advances, several critical challenges remain in medical\nanomaly detection. First, the balance between model complexity and\nclinical practicality continues to be a significant concern. While diffusion\nmodels offer superior performance, their computational requirements\ncan be prohibitive in clinical settings, as noted by Wu et al. 43. Second, the\nintegration of multiple expert annotations remains challenging, though\nrecent work by Amit et al. 48 on consensus prediction offers promising\ndirections.\nLooking forward, the field appears to be moving toward more effi-\ncient and interpretable approaches. The success of frequency-guided\nmethods 50 and structure-aware adaptations suggests that future devel-\nopments may focus on incorporating domain-specific medical knowledge\ninto foundation models. Additionally, the growing interest in self-\nsupervised and semi-supervised approaches indicates a shift toward\nmethods that can better utilize the vast amounts of unlabeled medical\ndata available while maintaining the high standards of accuracy required\nin clinical applications.\n3.2\nApplications in Pathological Images.\nIn pathological image analysis, the main approaches include pathological\nimage segmentation, anomaly detection, and image generation 51,52. The\ngoal of pathological image segmentation is to divide different tissue or\nlesion regions within an image. Generally, it is the foundation of tasks\nincluding cancer tissue detection, lesion structure analysis, cell counting\nand so on. Models for pathological image segmentation often use con-\nvolutional neural network (CNN)-based architectures, particularly the\nU-Net model. U-Net Model is proposed by Olaf Ronneberger et al. in\n2015 53, and it is a CNN architecture for biomedical image segmenta-\ntion. It is widely used in tasks including cell boundary segmentation and\nthe delineation of lesion areas. The U-Net consists of two main compo-\nnents: the encoder path and the decoder path 54. The encoder extracts\nfeatures by transforming raw pathological images into low-resolution,\nhigh-semantic feature representations using convolutional and pooling\noperations, and it also introduces non-linearity through functions such\nas ReLU 55. The decoder restores image resolution by upsampling the en-\ncoded features to match the size of the input images, finally producing the\nsegmentation output 56,57. The decoder path involves two key processes:\ndeconvolution, which upsamples feature maps to restore resolution, and\nskip connections, which combine features from corresponding layers in\nthe encoder and decoder keep spatial information lost in encoder. Mean-\nwhile, convolutional operations integrate low-level details with high-level\nsemantics to refine the segmentation results 58. In addition, there are\nseveral improved models based on U-Net, such as the Attention U-Net,\nwhich was proposed by Oktay et al. in 2018 59. This model introduces\nattention modules to enhance the model’s focus on critical regions. At-\ntention modules are integrated into each skip connection to dynamically\nadjust the features transferred from the encoder to the decoder. These\nmodules calculate attention weights for the input features and amplify\nfeatures of important regions while suppressing features of irrelevant\nareas. Based on the classic encoder-decoder structure of U-Net, the at-\ntention modules are embedded at key nodes along the decoding path\nand improve the model’s focus on significant regions while maintain-\ning overall segmentation accuracy. Image segmentation can be used in\nvarious pathological imaging tasks, like cell and nuclear segmentation.\nFor example, when diagnosing corneal endothelial health states, U-Net-\nbased CNN have achieved high accuracy in segmenting endothelial cells\nacross images of varying cell sizes, and achieve precise measurement of\ncellular morphological parameters (AUROC 0.92, DICE 0.86) 60. More-\nover, because cell segmentation is the first step in quantitative tissue\nimaging data analysis and the basis for single-cell analysis, it is important\nin identifying malignant tumors. The abnormal enlargement of nuclei in\ncancer cells leads to a significantly increased nucleus-to-cytoplasm ratio,\nwhich is one of the criteria for diagnosing malignant tumors 61. Therefore,\nimage segmentation can also be applied to the dividing of malignant re-\ngions in pathological images. For example, in squamous cell carcinoma\n(SCC), researchers utilized a patch dataset extracted from 200 digitized\ntissue images of 84 patients to train a U-Net-based segmentation model.\nThe model achieved a segmentation AUC of 0.89 on the test set, and\nthe average segmentation time is 72 seconds per image, which shows\nhigher efficiency compared to traditional manual segmentation meth-\nods 62. Pathological image generation uses artificial intelligence (AI) to\nproduce high-quality synthetic pathology images and solves challenges\nsuch as limited datasets and annotation difficulties by enlarging and\nbalancing datasets. Therefore, it is generally used for upstream tasks 63.\nGenerative adversarial networks (GANs), introduced by Ian Goodfellow\net al. in 2014 64, are the primary methods for image generation. GANs\nconsists of two neural network, including Generator and Discriminator,\nand train adversarially to produce realistic data , and it shows outstand-\ning performance in the image generation 65. The method optimizes the\ngenerator and discriminator alternately. The generator samples from the\nreal dataset, and inputs the sampled noise into the generator, and pro-\nduces synthetic samples that resemble real data; the discriminator takes\nboth real and synthetic data as input and judges whether each sample\ncomes from the real dataset or is generated by the generator. The dis-\ncriminator is updated to improve its ability to distinguish between real\nand synthetic data, while the generator is updated to enhance its ability\nto generate synthetic data 66. Through multiple iterations, the generator\nproduces samples that are realistic enough to make it difficult for the\ndiscriminator to distinguish between real and synthetic data and then\ncomplete the task of generating pathological images. Moreover, Style-\nGAN, a GAN-based model, can also be used for generating pathological\nimages 67. In traditional GANs, random noise with a Gaussian or uniform\ndistribution is directly inputted into the generator to produce images. In\ncontrast, StyleGAN introduces a style-mapping network, which maps\nnoise Z to a new latent space W. The W is more expressive and easier\n\n8\nYunHe Su,Zhengyang Lu,Junhui Liu, et al.\nto use for controlling specific features in the generated images, which\nhelps adjust things like color, texture, or shape, making the images more\nrealistic and diverse. Additionally, independent random noise is injected\nat each layer to generate random details in the images 68. StyleGAN\nalso uses progressive growth 69, where the generator and discriminator\ninitially handle low-resolution images during early training. The model\ngradually increases the resolution as training progresses until it achieves\nthe target high resolution. To further improve the resolution of generated\nimages, a multi-scale conditional GAN method has been proposed. This\nmodel used a pyramid-like structure to progressively increase the reso-\nlution while generating high-resolution images and maintaining global\nconsistency and detailed features of glandular structures. Through adver-\nsarial training, the generator captures the global layout of glands and the\nmicro-textures of cell nuclei. Multi-layer discriminators ensure the au-\nthenticity and consistency of the generated image 70. In renal pathology\nimage analysis, the morphological characteristics of glomeruli provide\ncritical diagnostic and prognostic information. To improve diagnostic\nefficiency, an automated method based on CNNs was proposed. This\nmethod used GAN-based generative data augmentation to generate\nglomeruli pathological images with various morphologies and improve\ndata diversity and model performance. The results showed that after ap-\nplying generative data augmentation, the sensitivity of the classification\nmodel increased from 0.7077 to 0.7623, and specificity improved from\n0.9316 to 0.9443 71. In addition to the pathological image segmentation\nand image generation above, there is also a downstream task, which is\nanomaly detection of pathological images. This involves using extracted\nimage features to identify and localize abnormal images. In anomaly de-\ntection, there are generally two types of methods, which are supervised\nanomaly detection and unsupervised anomaly detection 72. Supervised\nanomaly detection requires a dataset of annotated images including both\nnormal and abnormal images. This kind of classification tasks could be\nperformed using CNN-based model 73. CNN models extract features\nthrough convolutional layers, reduce the resolution of feature maps us-\ning pooling layers, and map the extracted high-dimensional features to\nclassification outputs using fully connected layers. The final classification\nprobabilities for normal or abnormal states are generated using Soft-\nmax or Sigmoid functions in the output layer 74. For example, studies\nhave shown that research based on deep CNNs shows significant ad-\nvantages in classifying skin lesions. Using a dataset containing about\n130000 images and covering almost 2000 diseases, CNNs achieved great\nperformance comparable in two binary classification tasks. Besides, the\nCNN achieved an accuracy of 72.1% in a three-class disease partitioning\ntask and 55.4% in a nine-class disease partitioning task 75. In unsuper-\nvised or self-supervised anomaly detection, the primary methods include\ncontrastive learning 76 and generative models 73,77. Contrastive learn-\ning involves extracting high-level feature representations from normal\nsamples to construct a feature space. Abnormal samples, due to their\ndifferent feature distributions, deviate from the feature space of nor-\nmal samples. If the deviation is sufficiently large, they are identified as\nanomalies. Generative models are similar to GANs. They use an encoder\nto extract high-dimensional feature representations from input data, cap-\nturing complex patterns and learning features. Subsequently, a decoder\nreconstructs the input data from these high-dimensional features to\nrecreate the original image. For normal samples, as they conform to the\nfeature distribution learned by the model, the reconstruction error is low.\nHowever, for abnormal samples, the reconstruction error is high, lead-\ning to their identification as anomalies. Hui Liu et al. proposed a deep\nlearning framework based on weakly supervised contrastive learning.\nThis framework uses self-supervised pretraining on large-scale unlabeled\npatches from whole slide images (WSI) to extract highly informative\npathological features. Combined with multitask learning, the framework\nsuccessfully inferred breast cancer-related gene expression, molecular\nsubtypes, and clinical outcomes. Experiments showed that this method\nachieved outstanding performance across multiple datasets, and the\ngenerated spatial heatmaps were highly consistent with pathologists’\nannotations and spatial transcriptomics data. This highlights its potential\nin linking genotypes with phenotypes and in the clinical applications of\ndigital pathology 78. Besides, in related research, a generative model com-\nbining GANs and autoencoders was employed for anomaly detection. By\ntraining the model on normal tissue data, regularization and multi-scale\ncontextual data were used to improve generalizability. This approach\nachieved efficient anomaly detection on the toxicologic histopathology\n(TOXPATH) dataset, with an AUC of 0.953 79. In general, pathological\nslide images can be used in different upstream and downstream tasks,\nsuch as classification and segmentation tasks. These include using image\nsegmentation for cell counting 80,applying image generation in upstream\ntasks to perform pathological data augmentation 81,using image segmen-\ntation and anomaly detection for cancer diagnosis or the identification\nof other lesion regions 82,83,and conducting tasks such as tumor grading\nand progression prediction 84. In addition to the neural network models\nmentioned above, the increasing computational capacity has led to the\nemergence of large models. These large models show powerful feature\nlearning abilities through pretraining on large-scale datasets. In recent\nyears, the concept of foundation models 85 has emerged. These models\nnot only have larger parameter sizes, with commonly used architectures\nin the field of pathology image analysis including Vision Transformers,\nCLIP, and Mask2Former ranging from hundreds of millions to billions of\nparameters, but also use high-resolution pathology images for pretraining\non large-scale datasets. Generally, the architectures of these foundation\nmodels are based on Transformer 86. Initially it was developed for nat-\nural language processing, now it has been extended to the visual field.\nThrough the self-attention mechanism, it effectively models both global\nand local features in pathology images and improves performance of\ntasks of segmentation and classification. While traditional CNNs rely on\nconvolutional kernels to extract local features, Transformers rely entirely\non self-attention to model relationships between all regions of an image\nand therefore capture the interdependencies of different tissue struc-\ntures in pathology slides more effectively. Like neural network models,\nmany Transformer-based models are developed to address tasks such as\nsegmentation, classification diagnosis, and multimodality. For image seg-\nmentation tasks, in addition to U-Net, Transformer-based segmentation\n\nPLEASE INSERT YOUR ARTICLE TITLE HERE\n9\nlarge models, such as Mask2Former, have introduced multi-task segmen-\ntation frameworks and self-attention mechanisms to achieve pathology\nimage segmentation. Jia-Chun Sheng et al. studied the Transformer-\nbased Mask2Former segmentation model and evaluated its performance\non pathology datasets. Mask2Former uses Transformer-based architec-\nture. It used Swin Transformer as the encoder to extract multi-scale\nfeatures. A pixel decoder integrated and processed features of different\nresolutions. The Transformer decoder applied masked attention to fo-\ncus on foreground regions. This design improves its ability to segment\nsmall objects. By pretraining on natural images, Mask2Former adapts\nwell to small pathology datasets. It achieved great performance com-\nparable to or better than task-specific methods on the CRAG and GlaS\ndatasets. It also significantly reduces the computational cost of handling\nhigh-resolution features 87. In addition, Visual Transformer-based classi-\nfication models have been increasingly introduced in pathology. Unlike\ntraditional methods, these large models use the global modeling capabil-\nities of Transformers to handle complex pathology images, like gigapixel\nwhole slide images (WSIs). In pathology image classification, Hanwen\nXu et al. proposed Prov-GigaPath, which extracts local features through\na tile-level encoder and integrates global context features from WSIs\nusing LongNet 88. By being pretrained on real-world datasets containing\n1.3 billion tiles, the model effectively captures both local and global pat-\nterns using DINOv2 and a masked autoencoder (MAE). Prov-GigaPath\nachieved better performance in 25 out of 26 pathology tasks, including a\n23.5% improvement in AUROC and a 66.4% improvement in AUPRC for\nEGFR mutation prediction 89. Eugene Vorontsov et al. proposed the Vir-\nchow model. It was based on Visual Transformer architecture and trained\non 1.5 million WSIs. Through the DINOv2 algorithm to learn both global\nand local embedding of WSIs, the model achieved cancer detection with\nROC of 0.95 for nine common and seven rare cancers 90. Richard J. Che’s\nteam proposed the general-purpose self-supervised pathology model\nUNI. It was pretrained on over 100,000 diagnostic H&E-stained WSIs\nand showed exceptional performance in 34 computational pathology\ntasks, including cancer subtype generalization 91. Recently, multimodal\nfoundation models have gained attention in pathology image analysis. By\nintegrating visual and textual information, these models simultaneously\ncapture the structural features of pathological images and the semantic\ninformation of clinical text. CONCH is a foundational visual-language\nmodel that uses an image encoder, text encoder, and multimodal de-\ncoder. Through contrastive learning, it embeds images and text into a\nshared representation space while optimizing multimodal understanding\nthrough a captioning objective. During pretraining, CONCH leverages di-\nverse pathology image-text pairs for unsupervised learning, significantly\nenhancing feature extraction capabilities. In cancer subtype classifica-\ntion, CONCH achieved 91.3% zero-shot accuracy 92. The commonly used\nevaluation metrics in pathological image tasks are as follows:\n4\nAPPLICATIONS OF LARGE MULTI-\nMODAL MODEL IN MEDICAL\nVision language models (VLMs) are multimodal generative AI models\ncapable of reasoning over text, image, and video prompts. VLM has\ndemonstrated excellent processing capabilities in image & video genera-\ntion, text-centric visual question answering, zero-shot detection, video\nsummarization, and other fields, and some of the results have been\nsuccessfully applied to autonomous driving, AI-generated image & text\ngeneration, AR/VR techniques and other fields 97. In the past, machine\nlearning-based graphics processing technology has been widely used\nfor the diagnostics of radiology and medical education 98,99. As a new\ngeneration of image processing model, VLM has attracted the attention\nof medical workers with its potential, and its application as the superior\nsubstitution of traditional image processing models in the medical field\nis becoming another hot research direction.\nIn computer graphics, 3D modeling refers to the process of developing\nmathematical coordinate-based representations of the surface of objects\n(inanimate or biological) in three dimensions by manipulating edges, ver-\ntices and polygons in simulated 3D space through specialized software.\n3D modeling has been widely used in architecture and industrial design,\nvideo games, film industry, art and other fields, but traditional 3D model-\ning technique has a strong dependence on human resources and time\ncost. As a result, the cost and quality of the final product could be diffi-\ncult to control. The recent emergence of 3D large models is bringing new\nchanges to this field 100. Three-dimensional modeling was applied to mul-\ntiple types of tasks in medicine (such as prosthetic and implant design,\nimage reconstruction, anatomical modeling, medical education, etc.), and\n3D large models may bring a technical revolution to these applications.\n4.1\nVLM for Medical Image Analysis\nThe models and algorithms based on traditional machine learning mod-\nels (such as CNN) that have been widely used in the research of medical\nimaging 101. VLM as a technique that has shown great capabilities in pro-\ncessing complex relationships and multimodal information 97, is naturally\nadapted to the complexity of information contained in medical cases\n(text, charts, pictures, videos, audio, etc.); therefore, VLM is gradually\nattracting the attention of medical workers.\nThe application of VLM large models in medicine at present is mainly\nfocused on the processing of medical images, especially for pathologi-\ncal and radiological imaging, for which have a much greater demand for\nsoftwires with efficient graphic processing capabilities than other sub-\nspecialties. There are already some studies involving VLM large models\nfor radiology and pathology, and this topic is discussed in details at other\narticles of our journal.\n\n10\nYunHe Su,Zhengyang Lu,Junhui Liu, et al.\nT A B L E 2\nEvaluation Methods for Different Tasks\nTask Type\nEvaluation Method\nDescription\nImage Segmentation 93\nDice Coefficient\nMeasures the overlap between the predicted segmentation and the\nground truth, ranging from [0, 1], with higher values indicating better per-\nformance.\nIoU (Intersection over Union)\nMeasures the ratio of the intersection area to the union area between the\npredicted and ground truth segmentation regions, ranging from [0, 1].\nImage\nGeneration 94,95,96\nFID (Fréchet Inception Dis-\ntance)\nAssesses the difference in feature distributions between generated and\nreal images; lower values indicate better performance.\nIS (Inception Score)\nEvaluates the diversity and quality of generated images based on a classifi-\ncation model; higher values indicate better performance.\nEvaluation Methods in\nDownstream Tasks\nThe evaluation of image generation considers not only image quality (e.g.,\nFID, IS) but also its impact on downstream tasks like classification and\nsegmentation, using metrics such as accuracy or sensitivity to validate\neffectiveness.\nAnomaly detection\nAccuracy\nProportion of correctly classified samples, reflecting overall classification\nperformance.\nSensitivity/Recall\nAbility to detect anomalous samples (recall); higher values indicate better\nperformance.\nSpecificity\nAbility to correctly detect normal samples; higher values indicate better\nperformance.\nF1 Score\nHarmonic mean of precision and recall, providing a balanced performance\nmetric, ranging from [0, 1].\nROC\nPlots the curve of sensitivity against the false positive rate; performance is\nmeasured by AUC, with values closer to 1 indicating better performance.\n4.2\nVLM for Medical Single Analysis\nVLM models are also applied to the analysis of physiological signals. Just\nlike traditional machine learning models, researches about the availability\nof VLMs for ECG, EEG analysis are also being conducted 102,103,104.\nDue to the high correlation between VLM and computer vision, VLM\nis currently also used in the field of AI-based robotics. Currently, some\nEEG and sEMG combined technologies have been experimentally applied\nto the collection of limb movement data and the optimization of pros-\nthetic movement patterns 105,106,107. Meta has already released two large\ndatasets and benchmarks for sEMG-based typing and pose estimation\nin Dec. 2024 108.\n4.3\n3D Large Models for Biometric Analy-\nsis\n3D modeling technology has been widely used in the medical field, es-\npecially in human anatomical modeling, prosthetic design and 2D to 3D\nimage conversion & construction 109,110,111. With the advent of 3D large\nmodels, this technology has gradually been applied to the research in\ncorresponding medical fields.\nAnother area of 3D modeling–3D human structure construction, has\nalso been influenced by 3D large model with the production of some\nnew research progresses. 3D model construction has been applied for\neducational use to better demonstrate body structures. In addition, 3D\nprinting is widely used in orthopedics, surgery, instrument design, drug\nresearch and many others in recent years.\n4.4\nDiscussion and Outlook\nDiagnostic techniques in neurology, psychiatry based on machine learn-\ning algorithms have been one of the hot areas in AI related medical\nresearch in the past few years. Motion capture in combination with\ndeep learning is also applied to the research of clinical measurements\nfor physical medicine and rehabilitation 112,113. There are some studies\nproved to be useful on the screening & diagnosis of stroke, Parkinson’s\ndisease as well as some mental disorders based on facial expression anal-\nysis 114,115,116. VLM, as a new generation of graphic image model, also\nhave strong prospects in these directions. At present, some results of\nemotion analysis research related to facial expression capture based on\nVLM have been published 117,118,119. As algorithms develop and mature\nin the future, we can foresee that VLM will become more and more\nimportant in screening and remote diagnosis. The audio and video gener-\nation function based on VLM and 3D large models can also more vividly\nshow the required content to medical workers and patients, so as to be\napplied for demonstration, diagnosis explanation, education and popu-\nlarization. Medical knowledge based on 3D models has also been proven\nto have more advantages than traditional forms 125,126. 3D large mod-\nels can optimize traditional technology-based 3D anatomical structure\nconstruction and apply it to medical education and popularization. Tra-\nditional 3D model teaching software, such as visual body, has a certain\n\nPLEASE INSERT YOUR ARTICLE TITLE HERE\n11\nT A B L E 3\nEvaluation Methods for Different Models\nModel Type\nApplication\nEvaluation Metrics\nVLMs\nMedical Image Segmentation\nDice score 120\nmIoU metric 120\nMedical report generation\nBilingual Evaluation Understudy/BLEU 121\nRecall-Oriented Understudy for Gisting\nEvaluation/ROUGE 121\nMetric for Evaluation of Translation with\nExplicit ORrdering/ METEOR 121\nPerplexity 121\nBERTScore 121\nRadGraph F1 121\nHuman evaluation 121\nClinical efficacy metrics\nAccuracy 121\nPrecision 121\nRecall 121\nF1 121\nQuestion answering\nAccuracy 121\nExact match 121\nHuman evaluation 121\n3D large models\n3D Medical Image Analysis\nDice score 122\nHausdorff Distance 95%/HD95 122\nMedical Image Segmentation & genera-\ntion\nDice score 123\nHigh-quality Medical Image Generation\nFrechet Inception Distance/FID 124\nMaximum Mean Discrepancy 124\nPeak Signal-to-Noise Ratio/PSNR 124\nStructural Similarity Index/SSIM 124\ndistance from the requirements of actual teaching tasks in terms of the\nrichness and accuracy of details. Many teaching scenes or structures that\nneed to be understood in clinical applications are often not recorded or\nlack proper accuracy in these softwires. 3D large models can not only\nprovide better contents, but also customize them based on user needs\n(such as text image conversion, model scaling, highlighting, separation,\netc.) 100. In addition, 3D large models can also be applied to 3D printing\nto produce realistic human anatomy models 127, which may better serve\nthe purpose of demonstration.\nIn the past few decades, the development of prosthetics related to ma-\nchine vision has always attracted many experts. The relevant mechanical\ndesign has long been mature. However, due to algorithm, materials along\nwith other technical obstacles, traditional prosthetic designs always have\nlimitations in final functions and user experience. The recent introduction\nof multimodal LLM and neuro-physical interface-related technologies\nhas brought changes to this field. Auxiliary algorithms based on VLM\nand 3D large models, machine vision and image analysis technologies\nmay become the key puzzle to solve the problem. Now with the help of\nAI techniques, there’s finally a chance of modern prosthetics to achieve\na difference from hooks and sticks. The researchers at Massachusetts\nInstitute of Technology recently published a study on intelligent pros-\nthetic design based on mechanism of antagonistic muscle groups 128. It\ncan be seen that 3D large model technology can provide considerable\nassistance to the design of implants and the surgical implantation based\non the construction and analysis of anatomical structures.\nMachine vision and image analysis based on VLM and 3D large models\ncan also be applied to the research of personalized intelligent vehicles,\nthereby providing a better experience for paralyzed patients or those who\nrely on wheelchairs. Take Simultaneous Localization and Mapping/SLAM\nas an example, it is a technique with remarkable capabilities on construct\nsurrounding environments. SLAM system can provide reference informa-\ntion for the action strategies, which makes it a key component in medical\nrobotic design. As SLAM has its own limitations on map semantically\nsimilar objects in compact environments 129, VLM’s capabilities on image\nanalysis make it a choice with great potential to guide the actions of\nrobotics in SLAM constructed models, which can further be applied for\naforementioned fields.\nIn the future, 3D large models would be trained with a larger amount\nof data, which may facilitate to establish a unified database in the corre-\nsponding field, and construct subject models with a certain degree of\nuniversality, thereby saving the resources and time cost for personalized\nadjustment of modeling. 3D modeling has already been used to aid pro-\ncedures in endoscopy and some surgeries, it is also applied to aid the\nproduction of implants 130,131. The introduction of 3D large models may\nlower the overall cost of prosthetics & implants customization for gen-\neral surgery, orthopedics, plastic surgery, dentistry and other disciplines,\nand further guide the progress in these fields.\n\n12\nYunHe Su,Zhengyang Lu,Junhui Liu, et al.\nMicro-robots and soft robots have always been one of the hot areas\nof translational medicine. Research results in the field of soft robotics\nare gradually making such technologies practical. In these researches,\n3D large models can provide basic support for anatomical structure\nreconstruction and dynamical simulation of such designs.\n4.5\nLimitations\nMedical imaging will not be affected by image editing or false informa-\ntion like other contents, but the accuracy of trained models based on\nthe complexity of the disease is an important issue. Take neurology as\nan example. In brain CT and MRI of difficult cases, the tiny lesions inside\nthe brain often can only be visualized under the scale of millimeter level\nwith strong interferences, and are also greatly affected by the equip-\nment, shooting angle, patient compliance along with many other issues.\nFor example, in general neurological imaging, a trained radiologist some-\ntimes cannot correctly find the lesion, so the participation of superior\nspecialists from neuroimaging is often required. This usually involves a\ncomplete review and detailed analysis of the patient’s medical history,\nand the accumulation of this part of knowledge and experience cannot\nbe completely replaced by the current model in a short time. Although\ndata training of large models will gradually make progress, before achiev-\ning more ideal results, the application of such research in clinical practice\nwill inevitably be limited by the concerns of patient safety issues. In con-\ntrast, for popular science and patient education, the requirements for\naccuracy of image analysis and model construction are relatively loose\ncompared to clinical applications and professional teaching, and may\nusher in mature transformation one step earlier.\nNeuro-physical interface is indeed a hot field at present, its combina-\ntion with VLM and 3D large models is also very attractive. However, key\ntechnical issues related to BCI/NPI still exist 132. Limited by the devel-\nopment of materials science and computer science, even if the relevant\ntechnical conditions are complete, the patient’s rejection reaction, the\naccuracy of signal measurement, the propagation delay transduction,\nthe accuracy of motion conversion, the patient’s tolerance of invasive\n& non-invasive devices (appearance, size, weight, etc.) are all important\nissues related to the transformation of results.\n5\nLARGE GRAPH MODELS IN MEDICAL\n5.1\nIntroduction of Large Graph Models\nLarge Graph Models (LGMs) are characterized by having a vast number\nof parameters, which endows it with significantly greater capabilities\ncompared to smaller models 133. This enhanced capacity facilitates the\nunderstanding, analysis, and processing of tasks related to graph data.\nLGMs, particularly Graph neural networks (GNNs) and Graph transformer\narchitectures, have emerged as powerful tools for processing and ana-\nlyzing structured data, especially for complex and large-scale datasets.\nTheir ability to capture complex relationships between entities makes\nthem especially suitable for various applications in the medical field,\nwhere understanding such interactions is often crucial. LGMs utilizing\ntransformer architecture and can be scaling up like LLMs. So they may\nrequiring substantial computational overheads.\nHealthcare primarily focus on management for patient care and ad-\nministrative purposes rather than for data-driven research. Electronic\nhealth records (EHRs) typically consist of a mix of structured, semi-\nstructured, and unstructured data, encompassing structured tables,\nimages, waveforms, and clinical notes. These datasets capture a wide\narray of complex and interrelated concepts, leading to data that is in-\nherently high-dimensional and heterogeneous 134 with sub-optimal data\nquality due to the rapid pace of the clinical environment and the absence\nof manual curation.\nGraphs offer a structured approach to explicitly model relational\nstructures within data representations. Besides, graphs strike a balance\nbetween flexibility and structure when representing data, which allows\nfor the seamless integration of multiple data modalities and the ability to\nexploit interdependencies across these modalities. This capability facili-\ntates the mathematical incorporation of domain-specific prior knowledge\nto inform and enhance patient representations 135. Graph AI streamlines\nthe transfer of LGMs across various clinical tasks, allowing them to effec-\ntively apply to different patient populations with minimal or no additional\nparameters or retraining 136.\nGNNs are a type of neural network model specifically designed for\ngraph-structured data. They can learn representations for nodes, edges,\nand entire graphs. The field of graph representation learning has devel-\noped a variety of network architectures, each tailored to capture distinct\ntypes of complex relationships within graph-structured data. For tasks\ninvolving predictions at the node, edge, or graph level, message-passing\nand transformer-based architectures are among the most prevalent 133.\nThere are numerous applications for Large Graph Models in biomedical\nfield, including Brain Network Analysis, Medical Knowledge Graphs, Drug\nDiscovery Analysis, and Protein-Protein Interaction Networks.\n5.2\nBrain Network Analysis\nWithin the realm of neuroscience, GNNs are instrumental in analyzing\ncomplex datasets like brain connectomes. By modeling the brain as a net-\nwork of interconnected regions, GNNs can identify patterns associated\nwith neurological disorders.\nGraph-based approaches have offered valuable insights into brain\nfunctions by analyzing the connectome as a network, calculating func-\ntional connectivity (FC) between brain regions using functional neu-\nroimaging. There are challenges remain in capturing the dynamic nature\nof FC networks, which fluctuate over time. Addressing these limitations,\nresearchers built STAGIN 137, a method that incorporates spatio-temporal\nattention to learn the dynamic graph representation of brain connec-\ntomes, integrating temporal sequences of brain graphs with READOUT\nfunctions and a Transformer encoder for spatial and temporal explain-\nability. Experiments on HCP-Rest and HCP-Task datasets show superior\nperformance of STAGIN, with its spatio-temporal attention mechanisms\n\nPLEASE INSERT YOUR ARTICLE TITLE HERE\n13\nT A B L E 4\nSummary of Large Graph Models with Datasets and Evaluation Metrics in the Medical Field. Abbr. HCP (Human Connectome Project);\nSTAGIN (Spatio-Temporal Attention Graph Isomorphism Network); ADNI (Alzheimer’s Disease Neuroimaging Initiative); UMLS (Unified Medical\nLanguage System); GDSC (Genomics of Drug Sensitivity in Cancer); GNBR (Global Network of Biomedical Relationships); PDB (Protein Data Bank);\nHPRD (Human Protein Reference Database); OPHID (Online Predicted Human Interaction Database); BioGRID (H. sapiens dataset from the Biological\nGeneral Repository for Interaction Datasets); AUC-ROC (Area Under the Curve - Receiver Operating Characteristic)\nApplication\nDatasets\nMethod/ Model\nEvaluation Metrics\nBrain Network Analysis\nHCP-Rest, HCP-Task\nSTAGIN 137\nPrediction accuracy, AUC-ROC\nADNI\nLG-GNN 138\nPrediction accuracy, Sensitivity,\nSpecificity, F1 score, AUC-ROC\nMedical Knowledge Graphs\nSampling diseases from the\nKnowledge Graphs\nTxGNN (GNN) 139\nPrediction accuracy\nUMLS\nDR.KNOWS 140\nPrecision, Recall, F- score\nDrug Discovery Analysis\nDavis, Kiba\nGraphDTA (GNN) 141\nPrediction accuracy\nGNBR\nGraph embedding 142\nAUC-ROC\nProtein-Protein Interaction\nNetworks\nAlphaFold Protein Structure\nDatabase, PDB\nGraphGPSM (GNN) 143\nTM-scores, Prediction accuracy\nHPRD, OPHID, Bi-\noGRID, STRING\nMGPPI (GNN) 144\nPrediction accuracy, Precision,\nRecall, F1 score, AUC-ROC\nproviding interpretations aligned with existing neuroscientific knowledge,\nvalidating the approach.\nFunctional brain networks have been increasingly utilized for clas-\nsifying brain disorders such as Autism Spectrum Disorder (ASD) and\nAlzheimer’s Disease (AD). Traditional approaches often overlook non-\nimaging information and inter-subject relationships or fail to identify\ndisease-specific brain regions and biomarkers, resulting in less accurate\nclassifications. To overcome these challenges, researchers introduce the\nlocal-to-global graph neural network (LG-GNN) 138, which includes a lo-\ncal ROI-GNN for extracting feature embeddings of brain regions and\nidentifying biomarkers, and a global Subject-GNN that leverages these\nembeddings and non-imaging data to learn inter-subject relationships.\nThe LG-GNN was validated on public datasets for ASD and AD classifi-\ncation, achieving state-of-the-art performance across various evaluation\nmetrics.\n5.3\nMedical Knowledge Graphs\nGNNs also enhance medical knowledge graphs, which aggregate vast\nmedical knowledge to inform diagnostics and treatment plans. For\nexample, drug repurposing is typically an opportunistic effort to find new\nuses for approved drugs. However, it faces limitations due to AI mod-\nels’ focus on diseases with existing treatments. To solve this problem,\nTxGNN is introduced as a graph foundation model for zero-shot drug\nrepurposing, aiming to identify therapeutic candidates for diseases with\nfew or no treatments. By leveraging a medical knowledge graph, TxGNN\nuses a graph neural network and metric learning to rank drugs for poten-\ntial indications and contraindications across 17,080 diseases. As a result,\nit outperforms eight other methods with improved prediction accuracy\nfor both indications and contraindications 139.\nIn addition, EHRs are crucial for comprehensive patient care, although\ntheir complexity and verbosity can overwhelm healthcare providers and\nlead to diagnostic errors. While Large Language Models (LLMs) show\npromise in various language-related tasks, their application in healthcare\nmust prioritize accuracy and safety to prevent patient harm. An approach\nhas been proposed that enhances LLMs in automated diagnosis gen-\neration by integrating a medical knowledge graph (KG) derived from\nthe Unified Medical Language System (UMLS) and a novel graph model\ninspired by clinical diagnostic reasoning. Experiments with real-world\nhospital data reveal that this combined approach improves diagnostic\naccuracy and provides an explainable diagnostic pathway, advancing\nAI-augmented decision support systems in healthcare 140.\n5.4\nDrug Discovery Analysis\nThe process of discovering new therapeutic applications for existing\ndrugs involves key challenges, such as modeling the intricate interac-\ntions among genes, pathways, targets, and drugs, which leads to an\nexponentially vast search space.\nGNNs are revolutionizing the process of drug discovery by predicting\nmolecular properties essential for pharmaceutical efficacy. They provide\ninsights into molecular interactions by modeling compounds as graphs.\nGraphDTA 141 is a new model which represents drugs as graphs and em-\nploys GNNs to predict drug-target affinity. It is found that GNNs not only\nachieve superior predictions of drug-target affinity compared to tradi-\ntional non-deep learning models but also outperform other deep learning\napproaches. These results validate the suitability of deep learning models\nfor predicting drug-target binding affinity and highlight the advantages of\nusing graph representations for drugs in enhancing prediction accuracy.\n\n14\nYunHe Su,Zhengyang Lu,Junhui Liu, et al.\nDrug repurposing emerges as a promising alternative method for rare\ndiseases, utilizing existing FDA-approved drugs for new therapeutic indi-\ncations. To systematically generate drug repurposing hypotheses, it is\ncrucial to integrate data from pharmacology, genetics, and pathology,\nwhich is facilitated by the Global Network of Biomedical Relationships\n(GNBR), a comprehensive knowledge graph. By applying a knowledge\ngraph embedding method that models uncertainty and uses link pre-\ndiction, this approach effectively generates and validates new drug\nrepurposing hypotheses, achieving high performance (AUC-ROC = 0.89)\nand providing explanations for its predictions 142.\n5.5\nProtein-Protein Interaction Networks\nProtein structure scoring models are typically categorized into unified\nfield and protein-specific functions, yet current prediction methods\nstill fall short in accurately modeling complex structures, such as multi-\ndomain and orphan proteins. In understanding disease mechanisms,\nGNNs help predict protein-protein interactions, which are vital for\nbiological processes and pathways.\nDespite the preference for computational PPI prediction methods due\nto their cost-effectiveness and accuracy, many current approaches fall\nshort in extracting detailed structural information and lack interpretabil-\nity. MGPPI 143, a novel multiscale graph convolutional neural network,\naddresses these issues by effectively capturing both local and global pro-\ntein structures and enhancing interpretability through Gradient Weighted\nInteraction Activation Mapping (Grad-WAM). Demonstrating superior\nperformance across various datasets, MGPPI not only identifies key bind-\ning sites, such as those between the SARS-CoV-2 spike protein and\nhuman ACE2, but also highlights residues that could serve as biomark-\ners for predicting cancer patient survival, showcasing its potential for\nguiding personalized treatment and drug target identification.\nIn addition, GraphGPSM 144 is a new global scoring model based on\nan equivariant graph neural network (EGNN), has been developed to im-\nprove protein structure prediction and ranking. By integrating advanced\nfeatures like residue-level ultrafast shape recognition and Gaussian radial\nbasis function encoding with Rosetta energy terms, GraphGPSM shows\na strong correlation with TM-scores on CASP13, CASP14, and CAMEO\ntest sets, outperforming existing models like REF2015, ModFOLD8, and\nAlphaFold2, particularly in modeling challenging proteins.\n5.6\nConclusion\nLarge Graph Models, with their profound ability to model complex data\nstructures, have unprecedented potential in various medical applica-\ntions, offering innovative solutions to complex challenges encountered in\nhealthcare. Nonetheless, the significance of human-centered design and\nmodel interpretability in clinical decision-making remains paramount. As\ngraph AI models derive insights through localized neural transformations\non relational datasets, they present both opportunities and challenges in\nexplaining model logic. Knowledge graphs can improve interpretability\nby aligning insights generated by the models with established medical\nknowledge. New graph AI models are emerging that integrate diverse\ndata modalities through pre-training, facilitate interactive feedback loops,\nand encourage human-AI collaboration, ultimately leading to clinically\nrelevant predictions 136.\nThe future of GNNs in medicine hinges on continuing research and\nextensive datasets to refine these models further, interdisciplinary col-\nlaboration, driving forward the development of precision medicine and\nadvancing the global health landscape.\nREFERENCES\n1. Rajkomar A, Dean J, Kohane I. Machine Learning in Medicine.\nNew England Journal of Medicine. 2019;380:1347-1358. doi:\n10.1056/NEJMra1814259\n2. Zhu J, Wang Z, Wang W, et al. Xanthomatous hypophysitis: a\ncase report and comprehensive literature review. Frontiers in\nEndocrinology. 2021;12:735655.\n3. Zhu J, Wang Z, Zhang Y, et al. Suprasellar pituitary adenomas:\na 10-year experience in a single tertiary medical center and a\nliterature review. Pituitary. 2020;23:367–380.\n4. Zhu J, Lu L, Yao Y, et al. Long-term follow-up for ectopic ACTH-\nsecreting pituitary adenoma in a single tertiary medical center\nand a literature review. Pituitary. 2020;23:149–159.\n5. Li X, Deng K, Zhang Y, et al. Pediatric pituitary neuroendocrine\ntumors–a 13-year experience in a tertiary center. Frontiers in\nOncology. 2023;13:1270958.\n6. Xiao H, Zhou F, Liu X, et al. A Comprehensive Survey of Large\nLanguage Models and Multimodal Large Language Models in\nMedicine. arXiv preprint. 2024.\n7. Park T, Gu P, Kim CH, et al. Artificial intelligence in urologic on-\ncology: the actual clinical practice results of IBM Watson for On-\ncology in South Korea. Prostate International. 2023;11(4):218-\n221. doi: https://doi.org/10.1016/j.prnil.2023.09.001\n8. Boussina A, Krishnamoorthy R, Quintero K, others . Large Lan-\nguage Models for More Efficient Reporting of Hospital Quality\nMeasures. NEJM AI. 2024;1(11):10.1056/aics2400420. doi:\n10.1056/aics2400420\n9. Esteva A, Kuprel B, Novoa RA, Ko J. Dermatologist-Level Clas-\nsification of Skin Cancer with Deep Neural Networks. Nature.\n2017;542(7639):115-118. doi: 10.1038/nature21056\n10. Zhou H, Liu F, Gu B, et al. A Survey of Large Language Models in\nMedicine: Progress, Application, and Challenge. arXiv preprint.\n2023.\n11. Yang Z, Li L, Wang S. 3D Medical Image Segmentation us-\ning Convolutional Neural Networks. Journal of Medical Imaging.\n2020. doi: 10.1117/1.JMI.7.3.031502\n12. Wu Z, Zhang X, Wei X. Graph Neural Networks for Medical\nData Analysis. IEEE Transactions on Neural Networks and Learning\nSystems. 2022. doi: 10.1109/TNNLS.2022.3185390\n13. Jumper J, Evans R, Pritzel A, al. e. Highly Accurate Protein Struc-\nture Prediction with AlphaFold. Nature. 2021;596(7873):583-\n589. doi: 10.1038/s41586-021-03819-2\n14. Singhal K, Azizi S, Tu T, others . Large language models encode\nclinical knowledge. Nature. 2023;620(7972):172–180.\ndoi:\n10.1038/s41586-023-06291-2\n15. Distilling step-by-step: Outperforming larger language models\nwith less training. .\n16. Singhal K, Tu T, Gottweis J, others . Towards Expert-Level Med-\nical Question Answering with Large Language Models. arXiv\npreprint. 2023. doi: 10.48550/arXiv.2305.09617\n17. Peng Y, Yan S, Lu Z. Transfer Learning in Biomedical Natu-\nral Language Processing: An Evaluation of BERT and ELMo\non Ten Benchmarking Datasets. arXiv preprint. 2019.\ndoi:\n10.48550/arXiv.1906.05474\n\nPLEASE INSERT YOUR ARTICLE TITLE HERE\n15\n18. Huang K, Mo F, Li H, others . A Survey on Large Language Mod-\nels with Multilingualism: Recent Advances and New Frontiers.\narXiv preprint. 2024. doi: 10.48550/arXiv.2405.10936\n19. The future landscape of large language models in medicine. .\n20. Nori H, King N, McKinney SM, Carignan D, Horvitz E. Capabil-\nities of GPT-4 on Medical Challenge Problems. arXiv preprint.\n2023. doi: 10.48550/arXiv.2303.13375\n21. MedExQA: Medical Question Answering Benchmark with\nMultiple Explanations. .\n22. Krithara A, Nentidis A, Bougiatiotis K, Paliouras G. BioASQ-QA:\nA manually curated corpus for Biomedical Question Answering.\nScientific Data. 2023;10(1):170. doi: 10.1038/s41597-023-\n02068-4\n23. Shahsavar Y, Choudhury A. User Intentions to Use ChatGPT for\nSelf-Diagnosis and Health-Related Purposes: Cross-sectional\nSurvey Study. JMIR Human Factors. 2023;10:e47564.\ndoi:\n10.2196/47564\n24. Monteiro MG, Pantani D, Pinsky I, Rocha TAH. The develop-\nment of the Pan American Health Organization digital health\nspecialist on alcohol use. Frontiers in Digital Health. 2022;4. doi:\n10.3389/fdgth.2022.948187\n25. Kuroiwa T, Sarcon A, Ibara T, others . The Potential of Chat-\nGPT as a Self-Diagnostic Tool in Common Orthopedic Dis-\neases: Exploratory Study. Journal of Medical Internet Research.\n2023;25(1):e47621. doi: 10.2196/47621\n26. Wimbarti S, Kairupan BHR, Tallei TE. Critical review of\nself-diagnosis of mental health conditions using artificial\nintelligence. International Journal of Mental Health Nursing.\n2024;33(2):344–358. doi: 10.1111/inm.13303\n27. Unveiling LLM Evaluation Focused on Metrics: Challenges and\nSolutions. Funded by National Key R&D Program of China (No.\n2021YFF0901400).\n28. Chen A, Stanovsky G, Singh S, Gardner M. Evaluating Ques-\ntion Answering Evaluation. In: Association for Computational\nLinguistics 2019:119–124\n29. Abbasian M, Khatibi E, Azimi I, others . Foundation metrics for\nevaluating effectiveness of healthcare conversations powered\nby generative AI. npj Digital Medicine. 2024;7(1):1–14. doi:\n10.1038/s41746-024-01074-z\n30. Farhud DD, Zokaei S. Ethical Issues of Artificial Intelligence\nin Medicine and Healthcare. Iranian Journal of Public Health.\n2021;50(11):i–v. doi: 10.18502/ijph.v50i11.7600\n31. Bao J, Sun H, Deng H, He Y, Zhang Z, Li X. Bmad: Benchmarks\nfor medical anomaly detection. In: 2024:4042–4053.\n32. Zhou Q, Pang G, Tian Y, He S, Chen J. Anomalyclip: Object-\nagnostic prompt learning for zero-shot anomaly detection. arXiv\npreprint arXiv:2310.18961. 2023.\n33. Zhang S, Xu Y, Usuyama N, et al. BiomedCLIP: a multimodal\nbiomedical foundation model pretrained from fifteen million\nscientific image-text pairs. arXiv preprint arXiv:2303.00915.\n2023.\n34. Huang C, Jiang A, Feng J, Zhang Y, Wang X, Wang Y. Adapting\nvisual-language models for generalizable anomaly detection in\nmedical images. In: 2024:11375–11385.\n35. Park Y, Kim MJ, Kim HS. Contrastive Language Prompting\nto Ease False Positives in Medical Anomaly Detection. arXiv\npreprint arXiv:2411.07546. 2024.\n36. Aleem S, Wang F, Maniparambil M, et al. Test-Time Adaptation\nwith SaLIP: A Cascade of SAM and CLIP for Zero-shot Medical\nImage Segmentation. In: 2024:5184–5193.\n37. Koleilat T, Asgariandehkordi H, Rivaz H, Xiao Y. Medclip-samv2:\nTowards universal text-driven medical image segmentation.\narXiv preprint arXiv:2409.19483. 2024.\n38. Wolleb J, Bieder F, Sandkühler R, Cattin PC. Diffusion models\nfor medical anomaly detection. In: Springer. 2022:35–45.\n39. Fontanella A, Mair G, Wardlaw J, Trucco E, Storkey A. Diffusion\nmodels for counterfactual generation and anomaly detection\nin brain images. IEEE Transactions on Medical Imaging. 2024.\n40. Behrendt F, Bhattacharya D, Maack L, et al. Diffusion Models\nwith Ensembled Structure-Based Anomaly Scoring for Unsu-\npervised Anomaly Detection. arXiv preprint arXiv:2403.14262.\n2024.\n41. Iqbal H, Khalid U, Chen C, Hua J. Unsupervised anomaly de-\ntection in medical images using masked diffusion model. In:\nSpringer. 2023:372–381.\n42. Fan K, Cai X, Niranjan M. Discrepancy-based Diffusion\nModels for Lesion Detection in Brain MRI. arXiv preprint\narXiv:2405.04974. 2024.\n43. Wu J, Ji W, Fu H, Xu M, Jin Y, Xu Y. Medsegdiff-v2: Diffusion-\nbased medical image segmentation with transformer. In: . 38.\n2024:6030–6038.\n44. Tian Y, Liu F, Pang G, et al. Self-supervised pseudo multi-\nclass pre-training for unsupervised anomaly detection and\nsegmentation in medical images. Medical image analysis.\n2023;90:102930.\n45. Zhang Z, Sun Z, Liu Z, et al. Spatial-Aware Attention Generative\nAdversarial Network for Semi-supervised Anomaly Detection\nin Medical Image. In: Springer. 2024:638–648.\n46. Cai Y, Chen H, Yang X, Zhou Y, Cheng KT. Dual-distribution\ndiscrepancy with self-supervised refinement for anomaly\ndetection\nin\nmedical\nimages.\nMedical\nImage\nAnalysis.\n2023;86:102794.\n47. Özbey M, Dalmaz O, Dar SU, et al. Unsupervised medical image\ntranslation with adversarial diffusion models. IEEE Transactions\non Medical Imaging. 2023.\n48. Amit T, Shichrur S, Shaharabany T, Wolf L. Annotator consen-\nsus prediction for medical image segmentation with diffusion\nmodels. In: Springer. 2023:544–554.\n49. Liang Z, Anthony H, Wagner F, Kamnitsas K. Modality cycles\nwith masked conditional diffusion for unsupervised anomaly\nsegmentation in mri. In: Springer. 2023:168–181.\n50. LiY, Shao HC, Liang X, et al. Zero-shot medical image translation\nvia frequency-guided diffusion models. IEEE transactions on\nmedical imaging. 2023.\n51. Amerikanos P, Maglogiannis I. Image Analysis in Digital Pathol-\nogy Utilizing Machine Learning and Deep Neural Networks.\nJournal of Personalized Medicine. 2022;12(9).\n52. Galić I, others . Machine learning empowering personalized\nmedicine: A comprehensive review of medical image analysis\nmethods. Electronics. 2023;12(21):4411.\n53. Ronneberger O, Fischer P, Brox T. U-net: Convolutional net-\nworks for biomedical image segmentation. In: Springer 2015.\n54. Dai D, others . I2U-Net: A dual-path U-Net with rich informa-\ntion interaction for medical image segmentation. Medical Image\nAnalysis. 2024:103241.\n55. Liang B, others . N-Net: an UNet architecture with dual en-\ncoder for medical image segmentation. Signal, Image and Video\nProcessing. 2023;17(6):3073-3081.\n56. Oğuz A, Ertuğrul OF. Introduction to deep learning and diagno-\nsis in medicine. In: , , Elsevier, 2023:1-40.\n57. Chen S, others . Encoder–Decoder Structure Fusing Depth\nInformation for Outdoor Semantic Segmentation. Applied Sci-\nences. 2023;13(17):9924.\n58. Xu G, others . Development of Skip Connection in Deep Neural\nNetworks for Computer Vision and Medical Image Analysis: A\nSurvey. arXiv preprint arXiv:2405.01725. 2024.\n59. Oktay O, others . Attention u-net: Learning where to look for\nthe pancreas. arXiv preprint arXiv:1804.03999. 2018.\n60. Fabijańska A. Segmentation of corneal endothelium images\nusing a U-Net-based convolutional neural network. Artificial\nintelligence in medicine. 2018;88:1-13.\n61. Singh I, Lele T. Nuclear Morphological abnormalities in Cancer: a\nsearch for unifying mechanisms. In: , , Springer, 2022:443-467.\n\n16\nYunHe Su,Zhengyang Lu,Junhui Liu, et al.\n62. Mavuduru A, others . Using a 22-layer U-Net to perform seg-\nmentation of squamous cell carcinoma on digitized head and\nneck histological images. In: NIH Public Access 2020.\n63. Faryna K, Laak v. dJ, Litjens G. Automatic data augmen-\ntation to improve generalization of deep learning in H&E\nstained histopathology. Computers in Biology and Medicine.\n2024;170:108018.\n64. Goodfellow I, others . Generative adversarial nets. In: . 27. 2014.\n65. Zhao W, Mahmoud Q, Alwidian S. Evaluation of GAN-based\nmodel for adversarial training. Sensors. 2023;23(5):2697.\n66. Figueira A, Vaz B. Survey on Synthetic Data Generation, Evalu-\nation Methods and GANs. Mathematics. 2022;10:2733.\n67. Karras T, Laine S, Aila T. A style-based generator architecture\nfor generative adversarial networks. arXiv. 2018. arXiv preprint\narXiv:1812.04948.\n68. Melnik A, others . Face generation and editing with stylegan:\nA survey. IEEE Transactions on Pattern Analysis and Machine\nIntelligence. 2024.\n69. Sauer A, Schwarz K, Geiger A. Stylegan-xl: Scaling stylegan to\nlarge diverse datasets. In: 2022.\n70. Li W, others . High resolution histopathology image generation\nand segmentation through adversarial training. Medical Image\nAnalysis. 2022;75:102251.\n71. Juang CF, others . Deep learning-based glomerulus detection\nand classification with generative morphology augmentation\nin renal pathology images. Computerized Medical Imaging and\nGraphics. 2024;115:102375.\n72. Tschuchnig M, Gadermayr M. Anomaly detection in medical\nimaging-a mini review. In: Springer 2021.\n73. Zingman I, others . Learning image representations for anomaly\ndetection: application to discovery of histological alterations in\ndrug development. Medical Image Analysis. 2024;92:103067.\n74. Qin J, others . A biological image classification method based\non improved CNN. Ecological Informatics. 2020;58:101093.\n75. Esteva A, others . Dermatologist-level classification of skin can-\ncer with deep neural networks. nature. 2017;542(7639):115-\n118.\n76. Tavolara T, Gurcan M, Niazi M. Contrastive multiple instance\nlearning: An unsupervised framework for learning slide-level\nrepresentations of whole slide histopathology images without\nlabels. Cancers. 2022;14(23):5778.\n77. Hassanaly R, others . Evaluation of pseudo-healthy image recon-\nstruction for anomaly detection with deep generative models:\nApplication to brain FDG PET. arXiv preprint arXiv:2401.16363.\n2024.\n78. Liu H, Zhang Y, Luo J. Contrastive learning-based histopatholog-\nical features infer molecular subtypes and clinical outcomes of\nbreast cancer from unannotated whole slide images. Computers\nin Biology and Medicine. 2024;170:107997.\n79. Zehnder P, others . Multiscale generative model using reg-\nularized skip-connections and perceptual loss for anomaly\ndetection in toxicologic histopathology. Journal of Pathology\nInformatics. 2022;13:100102.\n80. Singh H, Kaur H. A Systematic Survey on Biological Cell Image\nSegmentation and Cell Counting Techniques in Microscopic Im-\nages Using Machine Learning. Wireless Personal Communications.\n2024;137(2):813-851.\n81. Ruiz-Casado J, Molina-Cabello M, Luque-Baena R. Enhancing\nHistopathological Image Classification Performance through\nSynthetic Data Generation with Generative Adversarial Net-\nworks. Sensors. 2024;24(12):3777.\n82. Gu Q, others . Using an anomaly detection approach for the\nsegmentation of colorectal cancer tumors in whole slide images.\nJournal of Pathology Informatics. 2023;14:100336.\n83. Durkee M, others . Artificial intelligence and cellular segmen-\ntation in tissue microscopy images. The American journal of\npathology. 2021;191(10):1693-1701.\n84. Jaroensri R, others . Deep learning models for histologic grading\nof breast cancer and association with disease prognosis. NPJ\nBreast cancer. 2022;8(1):113.\n85. Moor M, others . Foundation models for generalist medical\nartificial intelligence. Nature. 2023;616(7956):259-265.\n86. Han K, others .Transformer in transformer. In: . 34. 2021:15908-\n15919.\n87. Sheng JC, Liao YS, Huang CR. Apply Masked-attention Mask\nTransformer to Instance Segmentation in Pathology Images. In:\nIEEE 2023.\n88. Ding J, others . Longnet: Scaling transformers to 1,000,000,000\ntokens. arXiv preprint arXiv:2307.02486. 2023.\n89. Xu H, others . A whole-slide foundation model for digital\npathology from real-world data. Nature. 2024:1-8.\n90. Vorontsov E, others . A foundation model for clinical-grade\ncomputational pathology and rare cancers detection. Nature\nmedicine. 2024:1-12.\n91. Chen R, others . Towards a general-purpose foundation model\nfor computational pathology. Nature Medicine. 2024;30(3):850-\n862.\n92. Lu M, others . A visual-language foundation model for compu-\ntational pathology. Nature Medicine. 2024;30(3):863-874.\n93. Jin S, others . Automatic three-dimensional nasal and pharyn-\ngeal airway subregions identification via Vision Transformer. J\nDent. 2023;136:104595.\n94. Park S, Shin Y. Generative convolution layer for image genera-\ntion. Neural Netw. 2022;152:370-379.\n95. Cao P, others . Generative artificial intelligence to produce\nhigh-fidelity blastocyst-stage embryo images. Hum Reprod.\n2024;39(6):1197-1207.\n96. Modak S, Stein A. Generative AI-based Pipeline Architecture\nfor Increasing Training Efficiency in Intelligent Weed Control\nSystems. arXiv preprint arXiv:2411.00548. 2024.\n97. Bordes F, others . An Introduction to Vision-Language Modeling.\narXiv preprint. 2024.\n98. Sarvamangala DR, Kulkarni RV. Convolutional Neural Networks\nin Medical Image Understanding: A Survey. Evolutionary Intelli-\ngence. 2022;15(1):1–22. doi: 10.1007/s12065-020-00540-3\n99. Kourounis G, Elmahmudi AA, Thomson B, others . Computer Im-\nage Analysis with Artificial Intelligence: A Practical Introduction\nto Convolutional Neural Networks for Medical Professionals.\nPostgraduate Medical Journal. 2023;99(1178):1287–1294. doi:\n10.1093/postmj/qgad095\n100. Gao J, others . GET3D: A Generative Model of High Quality 3D\nTextured Shapes Learned from Images. arXiv preprint. 2022.\n101. Jia H, Zhang J, Ma K, others . Application of Convolutional Neu-\nral Networks in Medical Images: A Bibliometric Analysis. Quan-\ntitative Imaging in Medicine and Surgery. 2024;14(5):3501–3518.\ndoi: 10.21037/qims-23-1600\n102. Vaid A, Jiang J, Sawant A, others . A Foundational Vision Trans-\nformer Improves Diagnostic Performance for Electrocardio-\ngrams. npj Digital Medicine. 2023;6:108. doi: 10.1038/s41746-\n023-00840-9\n103. Mulkey MA, Huang H, Albanese T, others . Supervised Deep\nLearning with Vision Transformer Predicts Delirium Using\nLimited Lead EEG. Scientific Reports. 2023;13:7890.\ndoi:\n10.1038/s41598-023-35004-y\n104. Qi N, Piao Y, Zhang H, others . Seizure Prediction Based on\nImproved Vision Transformer Model for EEG Channel Opti-\nmization. Computer Methods in Biomechanics and Biomedical\nEngineering. 2024. doi: 10.1080/10255842.2024.2326097\n105. Liu X, Hu L, Tie L, others . Integration of Convolutional\nNeural Network and Vision Transformer for Gesture Recog-\nnition Using sEMG. Biomedical Signal Processing and Control.\n2024;98:106686. doi: 10.1016/j.bspc.2024.106686\n\nPLEASE INSERT YOUR ARTICLE TITLE HERE\n17\n106. Córdova JC, others . EMGTFNet: Fuzzy Vision Transformer to\nDecode Upperlimb sEMG Signals for Hand Gestures Recogni-\ntion. In: 2023:1–6.\n107. Wang S, Zheng J, Huang Z, others . Integrating Computer\nVision to Prosthetic Hand Control with sEMG: Preliminary\nResults in Grasp Classification. Frontiers in Robotics and AI.\n2022;9:948238. doi: 10.3389/frobt.2022.948238\n108. Open Sourcing Surface Electromyography Datasets at NeurIPS\n2024. 2024.\n109. Singh SP, Wang L, Gupta S, others . 3D Deep Learning on\nMedical Images: A Review. Sensors. 2020;20(18):5097. doi:\n10.3390/s20185097\n110. Sun Z. 3D Printing in Medicine: Current Applications and Fu-\nture Directions. Quantitative Imaging in Medicine and Surgery.\n2018;8(11):1069–1077. doi: 10.21037/qims.2018.12.06\n111. Manero A, Smith P, Sparkman J, others . Implementation of 3D\nPrinting Technology in the Field of Prosthetics: Past, Present,\nand Future. International Journal of Environmental Research and\nPublic Health. 2019;16(9):1641. doi: 10.3390/ijerph16091641\n112. Tien RN, Tekriwal A, Calame DJ, others . Deep Learning\nBased Markerless Motion Tracking as a Clinical Tool for Move-\nment Disorders. Frontiers in Signal Processing. 2022;2.\ndoi:\n10.3389/frsip.2022.884384\n113. Lorenz EA, Su X, Skjæret-Maroni N. A Review of Combined\nFunctional Neuroimaging and Motion Capture for Motor\nRehabilitation. Journal of NeuroEngineering and Rehabilitation.\n2024;21:3. doi: 10.1186/s12984-023-01294-6\n114. Lee YS, Park WH. Diagnosis of Depressive Disorder Model\non Facial Expression Based on Fast R-CNN. Diagnostics.\n2022;12(2):317. doi: 10.3390/diagnostics12020317\n115. Liu D, Liu B, Lin T, others . Measuring Depression Severity\nBased on Facial Expression and Body Movement Using Deep\nConvolutional Neural Network. Frontiers in Psychiatry. 2022;13.\ndoi: 10.3389/fpsyt.2022.1017064\n116. Lim WS, Chiu SI, Wu MC, others . An Integrated Biometric Voice\nand Facial Features for Early Detection of Parkinson’s Disease.\nnpj Parkinson’s Disease. 2022;8:145. doi: 10.1038/s41531-022-\n00414-8\n117. [Title Not Provided]. arXiv preprint; 2023.\n118. Yao Y, Mei X, Xu J, others . VLM-EMO: Context-Aware Emotion\nClassification with CLIP. In: 2024:1615–1620\n119. Zhao Z, Patras I. Prompting Visual-Language Models for Dy-\nnamic Facial Expression Recognition. In: 2023.\n120. Li Z, others . LViT: Language Meets Vision Transformer in Medi-\ncal Image Segmentation. IEEE Transactions on Medical Imaging.\n2022;43:96–107. doi: 10.1109/TMI.2023.XXXXXXX\n121. Hartsock I, Rasool G. Vision-Language Models for Med-\nical Report Generation and Visual Question Answering:\nA Review. Frontiers in Artificial Intelligence. 2024;7.\ndoi:\n10.3389/frai.2024.1430984\n122. Tang Y, others . Self-Supervised Pre-Training of Swin Transform-\ners for 3D Medical Image Analysis. In: 2022:20698–20708\n123. Lee HH, others . 3D UX-Net: A Large Kernel Volumetric Con-\nvNet Modernizing Hierarchical Transformer for Medical Image\nSegmentation. arXiv preprint. 2022.\n124. Wang H, others . 3D MedDiffusion: A 3D Medical Diffu-\nsion Model for Controllable and High-Quality Medical Image\nGeneration. arXiv preprint. 2024.\n125. Bui I, Bhattacharya A, Wong SH, others . Role of Three-\nDimensional Visualization Modalities in Medical Education.\nFrontiers in Pediatrics. 2021;9:760363.\ndoi: 10.3389/f-\nped.2021.760363\n126. Ardila CM, González-Arroyave D, Zuluaga-Gómez M. Efficacy\nof Three-Dimensional Models for Medical Education: A Sys-\ntematic Scoping Review of Randomized Clinical Trials. Heliyon.\n2023;9(2):e13395. doi: 10.1016/j.heliyon.2023.e13395\n127. Bao G, Yang P, Yi J, others . Full-Sized Realistic 3D Printed\nModels of Liver and Tumour Anatomy: A Useful Tool for the\nClinical Medicine Education of Beginning Trainees. BMC Medical\nEducation. 2023;23:574. doi: 10.1186/s12909-023-04535-3\n128. Song H, Hsieh TH, Yeon SH, others . Continuous Neural Control\nof a Bionic Limb Restores Biomimetic Gait After Amputation.\nNature Medicine. 2024;30:2010–2019. doi: 10.1038/s41591-\n024-02994-9\n129. [Title Not Provided]. arXiv preprint; 2024.\n130. Raheem AA, Hameed P, Whenish R, others . A Review on Devel-\nopment of Bio-Inspired Implants Using 3D Printing. Biomimetics.\n2021;6(4):65. doi: 10.3390/biomimetics6040065\n131. Meng M, Wang J, Huang H, others . 3D Printing Metal Im-\nplants in Orthopedic Surgery: Methods,Applications and Future\nProspects. Journal of Orthopaedic Translation. 2023;42:94–112.\ndoi: 10.1016/j.jot.2023.08.004\n132. Patrick-Krueger KM, Burkhart I, Contreras-Vidal JL. The State of\nClinical Trials of Implantable Brain–Computer Interfaces. Nature\nReviews Bioengineering. 2025;3:50–67. doi: 10.1038/s44222-\n024-00239-5\n133. Zhang Z, Li H, Zhang Z, Qin Y, Wang X, Zhu W. Graph Meets\nLLMs: Towards Large Graph Models. 2023.\n134. Data MC, Nair S, Hsu D, Celi LA. Challenges and opportunities in\nsecondary analyses of electronic health record data. Secondary\nanalysis of electronic health records. 2016:17–26.\n135. Nelson CA, Butte AJ, Baranzini SE. Integrating Biomedical Re-\nsearch and Electronic Health Records to Create Knowledge\nBased Biologically Meaningful Machine Readable Embeddings.\nbioRxiv. 2019. doi: 10.1101/540963\n136. Johnson R, Li MM, Noori A, Queen O, Zitnik M. Graph AI in\nMedicine. 2023.\n137. Kim BH, Ye JC, Kim JJ. Learning Dynamic Graph Representation\nof Brain Connectome with Spatio-Temporal Attention. In: Ran-\nzato M, Beygelzimer A, Dauphin Y, Liang P, Vaughan JW., eds.\nAdvances in Neural Information Processing Systems. 34. Curran\nAssociates, Inc. 2021:4314–4327.\n138. Zhang H, Song R, Wang L, et al. Classification of Brain Disor-\nders in rs-fMRI via Local-to-Global Graph Neural Networks.\nIEEE Transactions on Medical Imaging. 2023;42(2):444-455. doi:\n10.1109/TMI.2022.3219260\n139. Huang K, Chandak P, Wang Q, et al. Zero-shot drug repurposing\nwith geometric deep learning and clinician centered design.\nmedRxiv. 2024. doi: 10.1101/2023.03.19.23287458\n140. Gao Y, Li R, Caskey J, et al. Leveraging A Medical Knowledge\nGraph into Large Language Models for Diagnosis Prediction.\n2023.\n141. Nguyen T, Le H, Quinn TP, Nguyen T, Le TD, Venkatesh S.\nGraphDTA: predicting drug–target binding affinity with graph\nneural networks. Bioinformatics. 2020;37(8):1140-1147. doi:\n10.1093/bioinformatics/btaa921\n142. Sosa DN, Derry A, Guo M, Wei E, Brinton C, Altman RB. A\nLiterature-Based Knowledge Graph Embedding Method for\nIdentifying Drug Repurposing Opportunities in Rare Diseases.\nbioRxiv. 2019. doi: 10.1101/727925\n143. Zhao S, Cui Z, Zhang G, Gong Y, Su L. MGPPI: multiscale graph\nneural networks for explainable protein–protein interaction\nprediction. Frontiers in Genetics. 2024;15.\ndoi: 10.3389/f-\ngene.2024.1440448\n144. He G, Liu J, Liu D, Zhang G. GraphGPSM: a global scoring model\nfor protein structure using graph neural networks. Briefings\nin Bioinformatics. 2023;24(4):bbad219. doi: 10.1093/bib/b-\nbad219\n",
  "metadata": {
    "source_path": "papers/arxiv/Applications_of_Large_Models_in_Medicine_fe5b37399fd18669.pdf",
    "content_hash": "fe5b37399fd1866959d34db3e9dbed050bd1d6e5b6b95be467308309cf285911",
    "arxiv_id": null,
    "title": "Applications_of_Large_Models_in_Medicine_fe5b37399fd18669",
    "author": "",
    "creation_date": "D:20250212153226+08'00'",
    "published": "20250212153226+08'00'",
    "pages": 17,
    "size": 792934,
    "file_mtime": 1740470179.626752
  }
}