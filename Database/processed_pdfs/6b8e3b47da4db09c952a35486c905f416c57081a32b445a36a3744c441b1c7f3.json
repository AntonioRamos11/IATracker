{
  "text": "MUON IS SCALABLE FOR LLM TRAINING\nTECHNICAL REPORT\nJingyuan Liu1\nJianlin Su1\nXingcheng Yao2\nZhejun Jiang1\nGuokun Lai1\nYulun Du1\nYidao Qin1\nWeixin Xu1\nEnzhe Lu1\nJunjie Yan1\nYanru Chen1\nHuabin Zheng1\nYibo Liu1\nShaowei Liu1\nBohong Yin1\nWeiran He1\nHan Zhu1\nYuzhi Wang1\nJianzhou Wang1 Mengnan Dong1\nZheng Zhang1\nYongsheng Kang1\nHao Zhang1\nXinran Xu1\nYutao Zhang1\nYuxin Wu1\nXinyu Zhou1 ∗\nZhilin Yang1\n1 Moonshot AI\n2 UCLA\nABSTRACT\nRecently, the Muon optimizer (K. Jordan et al. 2024) based on matrix orthogonalization has demon-\nstrated strong results in training small-scale language models, but the scalability to larger models has\nnot been proven. We identify two crucial techniques for scaling up Muon: (1) adding weight decay and\n(2) carefully adjusting the per-parameter update scale. These techniques allow Muon to work out-of-\nthe-box on large-scale training without the need of hyper-parameter tuning. Scaling law experiments\nindicate that Muon achieves ∼2× computational efficiency compared to AdamW with compute\noptimal training. Based on these improvements, we introduce Moonlight, a 3B/16B-parameter\nMixture-of-Expert (MoE) model trained with 5.7T tokens using Muon. Our model improves the\ncurrent Pareto frontier, achieving better performance with much fewer training FLOPs compared\nto prior models. We open-source our distributed Muon implementation that is memory optimal\nand communication efficient. We also release the pretrained, instruction-tuned, and intermediate\ncheckpoints to support future research.\nCreated in Master PDF Editor\n10\n1\n100\n101\nPFLOP/s-days\n2.2\n2.3\n2.4\n2.5\n2.6\n2.7\n2.8\n2.9\n3.0\nLM loss \n0.519x FLOPs\nMuon\nAdamW\n(a)\n2e22\n5e22\n1e23\n2e23\n5e23\n1e24\nTraining FLOPs\n50\n55\n60\n65\n70\n75\n80\nMMLU Score\nLlama-2-13B\nLlama-3.1-8B\nQwen-2.5-7B\nQwen-2.5-14B\nGemma-2-9B\nStableLM-2-12B\nMAP-Neo-7B\nOLMo-0424-7B\nDCLM-7B\nOLMo-2-7B\nOLMo-2-13B\nQwen-2.5-3B\nDeepSeek-V2-Lite-2.4B\nDeepSeek-V3-Small-2.4B\nMoonlight-2.4B-1.2T\nMoonlight-2.4B-5.7T\nMMLU Performance Frontier\n(b)\nFigure 1: Scaling up with Muon. (a) Scaling law experiments comparing Muon and Adam. Muon is ∼2× more\ncomputational efficient than Adam with compute optimal training. (b) The MMLU performance of our Moonlight\nmodel optimized with Muon and other comparable models. Moonlight advances the Pareto frontier of performance vs\ntraining FLOPs.\n∗Corresponding author: zhouxinyu@moonshot.cn\narXiv:2502.16982v1  [cs.LG]  24 Feb 2025\n\nMuon is Scalable for LLM Training\nTECHNICAL REPORT\n1\nIntroduction\nThe rapid advancement of large language models (LLMs) (OpenAI et al. 2024; DeepSeek-AI et al. 2024; Grattafiori\net al. 2024; Gemini Team et al. 2024) has significantly pushed forward the progress in artificial general intelligence.\nHowever, training capable LLMs remains a computationally intensive and resource-demanding process due to scaling\nlaws (Kaplan et al. 2020; Hoffmann et al. 2022). Optimizers play a crucial role in efficiently and effectively training of\nLLMs, with Adam (Kingma et al. 2015) and its variant AdamW (Loshchilov et al. 2019) being the standard choice for\nmost large-scale training.\nRecent developments in optimization algorithms have shown potential to improve training efficiency beyond\nAdamW (Liu et al. 2024; K. Jordan et al. 2024; Yuan et al. 2024; Vyas et al. 2025; X.-L. Li 2018a; X.-L. Li\n2018b; Pooladzandi et al. 2024; X. Li 2022; X.-L. Li 2024; Pethick et al. 2025). Among these, K. Jordan et al. 2024\nproposed Muon, which updates matrix parameters with orthogonalized gradient momentum using Newton-Schulz\niteration. Initial experiments with Muon have demonstrated promising results in small-scale language model training.\nHowever, as discussed in this blog (K. Jordan et al. 2024), several critical challenges remain unaddressed: (1) how to\neffectively scale optimizers based on matrix orthogonalization to larger models with billions of parameters trained with\ntrillions of tokens, (2) how to compute approximate orthogonalization in a distributed setting, and (3) whether such\noptimizers can generalize across different training stages including pre-training and supervised finetuning (SFT).\nIn this technical report, we present a comprehensive study addressing these challenges. Our work builds upon\nMuon while systematically identifying and resolving its limitations in large-scale training scenarios. Our technical\ncontributions include:\n• Analysis for Effective Scaling of Muon: Through extensive analysis, we identify that weight decay plays a crucial\nrole in Muon’s scalability. Besides, we propose scale adjustments to Muon’s parameter-wise update rule. Such\nadjustments allow Muon to work out-of-the-box without hyper-parameter tuning, and also significantly improve\ntraining stability.\n• Efficient Distributed Implementation: We develop a distributed version of Muon with ZeRO-1 (Rajbhandari\net al. 2020) style optimization, achieving optimal memory efficiency and reduced communication overhead while\npreserving the mathematical properties of the algorithm.\n• Scaling Law Validation: We performed scaling law research that compares Muon with strong AdamW baselines,\nand showed the superior performance of Muon (1a). Based on the scaling law results, Muon achieves comparable\nperformance to AdamW trained counterparts while requiring only approximately 52% of the training FLOPs.\nOur comprehensive experiments demonstrate that Muon can effectively replace AdamW as the de facto optimizer\nfor large-scale LLM training, offering significant improvements in both training efficiency and model performance.\nAs a result of this work, we release Moonlight, a 16B-parameter MoE model trained using Muon, along with our\nimplementation and intermediate training checkpoints to facilitate further research in scalable optimization techniques\nfor LLMs.\n2\nMethods\n2.1\nBackground\nThe Muon Optimizer\nMuon (K. Jordan et al. 2024) has recently been proposed to optimize neural network weights\nrepresentable as matrices. At iteration t, given current weight Wt−1, momentum µ, learning rate ηt and objective Lt,\nthe update rule of the Muon optimizer can be stated as follows:\nMt = µMt−1 + ∇Lt(Wt−1)\nOt = Newton-Schulz(Mt)1\n(1)\nWt = Wt−1 −ηtOt\nHere, Mt is the momentum of gradient at iteration t, set as a zero matrix when t = 0. In Equation 1, a Newton-Schulz\niteration process (Bernstein et al. 2024) is adopted to approximately solve (MtMT\nt )−1/2Mt. Let UΣVT = Mt be\nthe singular value decomposition (SVD) of Mt, we will have (MtMT\nt )−1/2Mt = UVT, which orthogonalizes Mt.\nIntuitively, orthogonalization can ensure that the update matrices are isomorphic, preventing the weight from learning\nalong a few dominant directions (K. Jordan et al. 2024).\n1In practice, we follow (K. Jordan et al. 2024) to use a Nesterov-style momentum by putting µMt + ∇Lt(Wt−1) to the\nNewton-Schulz iteration instead of Mt.\n2\n\nMuon is Scalable for LLM Training\nTECHNICAL REPORT\nNewton-Schulz Iterations for Matrix Orthogonalization\nEquation 1 is calculated in an iterative process. At the\nbeginning, we set X0 = Mt/∥Mt∥F. Then, at each iteration k, we update Xk from Xk−1 as follows:\nXk = aXk−1 + b(Xk−1XT\nk−1)Xk−1 + c(Xk−1XT\nk−1)2Xk−1\n(2)\nwhere XN is the result of such process after N iteration steps. Here a, b, c are coefficients. In order to ensure the\ncorrect convergence of Equation 2, we need to tune the coefficients so that the polynomial f(x) = ax + bx3 + cx5 has\na fixed point near 1. In the original design of K. Jordan et al. 2024, the coefficients are set to a = 3.4445, b = −4.7750,\nc = 2.0315 in order to make the iterative process converge faster for small initial singular values. In this work, we\nfollow the same setting of coefficients.\nSteepest Descent Under Norm Constraints\nBernstein et al. 2024 proposed to view the optimization process in deep\nlearning as steepest descent under norm constraints. From this perspective, we can view the difference between Muon\nand Adam (Kingma et al. 2015; Loshchilov et al. 2019) as the difference in norm constraints. Whereas Adam is a\nsteepest descent under the a norm constraint dynamically adjusted from a Max-of-Max norm, Muon offers a norm\nconstraint that lies in a static range of Schatten-p norm for some large p (Franz 2024). When equation 1 is accurately\ncomputed, the norm constraint offered by Muon will be the spectral norm. Weights of neural networks are used as\noperators on the input space or the hidden space, which are usually (locally) Euclidean (Cesista 2024), so the norm\nconstraint on weights should be an induced operator norm (or spectral norm for weight matrices). In this sense, the\nnorm constraint offered by Muon is more reasonable than that offered by Adam.\n2.2\nScaling Up Muon\nWeight Decay\nWhile Muon performs significantly better than AdamW on a small scale as shown by K. Jordan\net al. 2024, we found the performance gains diminish when we scale up to train a larger model with more tokens. We\nobserved that both the weight and the layer output’s RMS keep growing to a large scale, exceeding the high-precision\nrange of bf16, which might hurt the model’s performance. To resolve this issue, we introduced the standard AdamW\n(Loshchilov et al. 2019) weight decay mechanism into Muon2.\nWt = Wt−1 −ηt(Ot + λWt−1)\n(3)\nWe experimented on Muon both with and without weight decay to understand its impact on the training dynamics\nof LLMs. Based on our scaling law research in Sec 3.2, we trained an 800M parameters model with 100B tokens\n(∼5× optimal training tokens). Figure 2 shows validation loss curves of the model trained with AdamW, vanilla Muon\n(without weight decay), and Muon with weight decay. While vanilla Muon initially converges faster, we observed that\nsome model weights grew too large over time, potentially limiting the model’s long-term performances. Adding weight\ndecay addressed this issue - the results demonstrate that Muon with weight decay outperforms both vanilla Muon and\nAdamW, achieving lower validation loss in the over-train regime. Therefore, we adjusted our update rule to equation 3,\nwhere λ is the weight decay ratio.\nConsistent update RMS\nAn important property of Adam and AdamW (Kingma et al. 2015, Loshchilov et al. 2019) is\nthat they maintain a theoretical update RMS around 13. However, we show that Muon’s update RMS varies depending\non the shape of the parameters, according to the following lemma:\nLemma 1. For a full-rank matrix parameter of shape [A, B], its theoretical Muon update RMS is\np\n1/ max(A, B) .\nThe proof can be found in the Appendix A. We monitored Muon’s update RMS during training and found it typically\nclose to the theoretical value given above. We note that such inconsistency can be problematic when scaling up the\nmodel size:\n• When max(A, B) is too large, e.g. the dense MLP matrix, the updates become too small, thus limiting the model’s\nrepresentational capacity and leading to suboptimal performances;\n• When max(A, B) is too small, e.g. treating each KV head in GQA (Shazeer 2019) or MLA (DeepSeek-AI et al.\n2024) as a separate parameter, the updates become too large, thus causing training instabilities and leading to\nsuboptimal performances as well.\n2The original implementation of Muon omits weight decay. A recent concurrent work in Muon incorporates weight decay and\ndemonstrates improved performance. See this commit and this discussion.\n3Due to Adam’s β1 < β2 and ϵ > 0, the actual update RMS is usually less than 1.\n3\n\nMuon is Scalable for LLM Training\nTECHNICAL REPORT\n0\n10000\n20000\n30000\n40000\n50000\n60000\nTraining Iterations\n2.25\n2.30\n2.35\n2.40\n2.45\n2.50\n2.55\nValidation Loss\n0\n10000\n20000\n30000\n40000\n50000\n60000\nTraining Iterations\n0.020\n0.015\n0.010\n0.005\n0.000\n0.005\n0.010\n0.015\nDifference in Val Loss (Vanilla Muon - Muon w/ weight decay)\nVanilla Moun wins \n0.023 at iteration 24000\nMoun w/ weight decay \n wins 0.017 at iteration 66000\nAdamW\nMuon w/o weight decay\nMuon w/ weight decay\nFigure 2: Validation loss curves for AdamW (green), Muon without weight decay (red), and Muon with weight decay (blue).\nIn order to maintain consistent update RMS among matrices of different shapes, we propose to scale the Muon update\nfor each matrix by its\np\nmax(A, B) to cancel the effect of Lemma 1 4 . Experiments in Sec 3.1 show that this strategy\nis beneficial for optimization.\nMatching update RMS of AdamW\nMuon is designed to update matrix-based parameters. In practice, AdamW\nis used in couple with Muon to handle non-matrix based parameters, like RMSNorm, LM head, and embedding\nparameters. We would like the optimizer hyper-parameters (learning rate η, weight decay λ) to be shared among matrix\nand non-matrix parameters.\nWe propose to match Muon’s update RMS to be similar to that of AdamW. From empirical observations, AdamW’s\nupdate RMS is usually around 0.2 to 0.4. Therefore, we scale Muon’s update RMS to this range by the following\nadjustment:\nWt = Wt−1 −ηt(0.2 · Ot ·\np\nmax(A, B) + λWt−1)\n(4)\nWe validated this choice with empirical results (see Appendix A for details). Moreover, we highlighted that with this\nadjustment, Muon can directly reuse the learning rate and weight decay tuned for AdamW.\nOther Hyper-parameters\nMuon contains two other tunnable hyper-parameters: Newton-Schulz iteration steps and\nmomentum µ. We empirically observe that when setting N to 10, the iterative process will yield a more accurate\northogonalization result than N = 5, but it won’t lead to better performances. Hence we set N = 5 in this work for\nthe sake of efficiency. We do not see a consistent performance gain in tuning momentum, so we chose 0.95, same as\nK. Jordan et al. 2024.\n2.3\nDistributed Muon\nZeRO-1 and Megatron-LM\nRajbhandari et al. 2020 introduced the ZeRO-1 technique that partitions the expensive\noptimizer states (e.g. master weights, momentum) all over the cluster. Megatron-LM (Shoeybi et al. 2020) integrated\nZeRO-1 into its native parallel designs. Based on Megatron-LM’s sophisticated parallel strategies, e.g. Tensor-Parallel\n(TP), Pipeline Parallel (PP), Expert Parallel (EP) and Data Parallel (DP), the communication workload of ZeRO-1 can\nbe reduced from gathering all over the distributed world to only gathering over the data parallel group.\nMethod\nZeRO-1 is efficient for AdamW because it calculates updates in an element-wise fashion. However, Muon\nrequires the full gradient matrix to calculate the updates. Therefore, vanilla ZeRO-1 is not directly applicable to Muon.\n4K. Jordan et al. 2024’s original implementation scales the updates by\np\nmax(1, A/B), which is equivalent to our proposal (up\nto a global scale) if all matrices have the same second dimension; Pethick et al. 2025 and You 2025 discussed a similar issue on\nupdate scaling factors concurrently to our work.\n4\n\nMuon is Scalable for LLM Training\nTECHNICAL REPORT\nAlgorithm 1 Distributed Muon\nRequire: Full Gradients G, DP partitioned Momentum m, DP partitioned parameters p, momentum µ.\n1: // Reduce-scatter G on DP for correct gradients\n2: g = reduce_scatter(G, dp_group)\n3: // Apply momentum to g using local partitioned momentum m\n4: g′ = update_with_momentum(g, m, µ)\n5: // DP Gather: gathering g′ across DP into a full matrix G\n6: G = gather(g′, dp_group)\n7: // Calculate Muon update\n8: U = Newton-Schulz(G)\n9: // Discard the rest of U and only keep the local partition u, then apply the update rule\n10: p′ = apply_update(p, u)\n11: // All-gather updated p′ into P\n12: P = all_gather(p′, dp_group)\n13: // Return the update RMS for logging\n14: return\np\nu2.mean()\nWe propose a new distributed solution based on ZeRO-1 for Muon, referred to as Distributed Muon. Distributed Muon\nfollows ZeRO-1 to partition the optimizer states on DP, and introduces two additional operations compared to a vanilla\nZero-1 AdamW optimizer:\n1. DP Gather. For a local DP partitioned master weight (1/DP the size of the model weight), this operation is to\ngather the corresponding partitioned gradients into a full gradient matrix.\n2. Calculate Full Update. After the above gathering, perform Newton-Schulz iteration steps on the full gradient\nmatrix as described in Sec 2.1. Note that we will then discard part of the full update matrix, as we only need the\npartition corresponding to the local parameters to perform update.\nThe implementation of Distributed Muon is described in Algorithm 1. The additional operations introduced by\nDistributed Muon are colored in blue.\nAnalysis\nWe compared Distributed Muon to a classic ZeRO-1 based distributed AdamW (referred as Distributed\nAdamW for simplicity) in several aspects:\n• Memory Usage. Muon uses only one momentum buffer, while AdamW uses two momentum buffers. Therefore,\nthe additional memory used by the Muon optimizer is half of Distributed AdamW.\n• Communication Overhead. For each device, the additional DP gathering is only required by the local DP\npartitioned parameters p. Therefore, the communication cost is less than the reduce-scatter of G or the all-gather of\nP. Besides, Muon only requires the Newton-Schulz iteration steps in bf16, thus further reducing the communication\noverhead to 50% comparing to fp32. Overall, the communication workload of Distributed Muon is (1, 1.25] of that\nof Distributed AdamW. The upper-bound is calculated as that the communication of Distributed Muon is 4 (fp32\nG reduce-scatter) + 2 (bf16 Muon gather) + 4 (fp32 P all-gather), while Distributed AdamW is 4 + 4. In practice,\nas we usually train with multiple DP, the empirical additional cost usually is closer to the lower-bound 1.5.\n• Latency. Distributed Muon has larger end-to-end latencies than Distributed AdamW because it introduces\nadditional communication and requires running Newton-Schulz iteration steps. However, this is not a significant\nissue because (a) only about 5 Newton-Schultz iteration steps are needed for a good result (discussed in Sec 2.2),\nand (b) the end-to-end latency caused by the optimizer is negligible compared to the model’s forward-backward\npass time (e.g. usually 1% to 3%). Moreover, several engineering techniques, such as overlapping gather and\ncomputation, and overlapping optimizer reduce-scatter with parameter gather, can further reduce latency.\nWhen training large-scale models in our distributed cluster, Distributed Muon has no noticeable latency overhead\ncompared to its AdamW counterparts. We will soon release a pull request that implements Distributed Muon for the\nopen-source Megatron-LM (Shoeybi et al. 2020) project.\n5If TP is enabled, Distributed Muon needs an extra bf16 TP gather on TP group.\n5\n\nMuon is Scalable for LLM Training\nTECHNICAL REPORT\nTable 1: Controlling Muon’s Update RMS Across Different Model Params\nMethods\nTraining loss\nValidation loss\nquery weight RMS\nMLP weight RMS\nBaseline\n2.734\n2.812\n3.586e-2\n2.52e-2\nUpdate Norm\n2.72\n2.789\n4.918e-2\n5.01e-2\nAdjusted LR\n2.721\n2.789\n3.496e-2\n4.89e-2\n3\nExperiments\n3.1\nConsistent Update RMS\nAs discussed in Sec 2.2, we aim to match the update RMS across all matrix parameters and also match it with that of\nAdamW. We experimented with two methods to control the Muon update RMS among parameters and compared them\nto a baseline that only maintains a consistent RMS with AdamW:\n1. Baseline. We multiplied the update matrix by 0.2 ·\n√\nH (H is the model hidden size) to maintain a consistent\nupdate RMS with AdamW. Note that max(A, B) equals to H for most matrices.\nWt = Wt−1 −ηt(0.2 · Ot ·\n√\nH + λWt−1)\n(5)\n2. Update Norm. We can directly normalize the updates calculated via Newton-Schulz iterations so its RMS strictly\nbecomes 0.2;\nWt = Wt−1 −ηt(0.2 · Ot/ RMS(Ot) + λWt−1)\n(6)\n3. Adjusted LR. For each update matrix, we can scale its learning rate by a factor of 0.2 ·\np\nmax(A, B) based on\nits shape.\nWt = Wt−1 −ηt(0.2 · Ot ·\np\nmax(A, B) + λWt−1)\n(7)\nAnalysis\nWe designed experiments to illustrate the impact of Muon update RMS at an early training stage, because\nwe observed that unexpected behaviors happened very quickly when training models at larger scale. We experimented\nwith small scale 800M models as described in 3.2. The problem of inconsistent update RMS is more pronounced when\nthe disparity between matrix dimensions increases. To highlight the problem for further study, we slightly modify\nthe model architecture by replacing the Swiglu MLP with a standard 2-layer MLP, changing the shape of its matrix\nparameters from [H, 2.6H] to [H, 4H]. We evaluated the model’s loss and monitored a few of its parameters’ RMS,\nspecifically, attention query (shape [H, H]) and MLP (shape [H, 4H]). We evaluated the model after training for 4B\ntokens out of a 20B-token schedule. From Table 1, we observed several interesting findings:\n1. Both Update Norm and Adjusted LR achieved better performances than Baseline;\n2. For the MLP weight matrix of shape [H, 4H], both Update Norm and Adjusted LR obtain a weight RMS that is\nroughly doubled comparing to Baseline. This is reasonable as\np\nmax(H, 4H)/\n√\nH = 2, so the update RMS of\nUpdate Norm and Adjusted LR is roughly two times of Baseline;\n3. For the attention query weight matrix of shape [H, H], Update Norm still norms the update, while Adjusted\nLR does not because\np\nmax(H, H)/\n√\nH = 1. As a result, Adjusted LR results in a similar weight RMS as\nBaseline, but Update Norm has a larger weight rms similar to its MLP.\nBased on these findings, we choose the Adjusted LR method for future experiments because it has lower cost.\n3.2\nScaling Law of Muon\nFor a fair comparison with AdamW, we performed scaling law experiments on a series of dense models in Llama\n(Grattafiori et al. 2024) architecture. Building a strong baseline is of crucial importance in optimizer research. Hence,\nwe perform a grid search for hyper-parameters of AdamW, following the compute-optimal training setup (Kaplan et al.\n2020) (the grid search experiments can be found in Appendix B). Details of the model architecture and hyper-parameters\ncan be found in Table 2. For Muon, as discussed in Sec 2.2, since we matched Muon’s update RMS to AdamW, we\ndirectly reused the hyper-parameters that are optimal for the AdamW baseline.\nThe fitted scaling law curve can be found in figure 3, and the fitted equations are detailed in table 3. As shown in\nFigure 1a, Muon only requires about 52% training FLOPs to match the performance of AdamW under compute-optimal\nsetting.\n6\n\nMuon is Scalable for LLM Training\nTECHNICAL REPORT\nTable 2: Scaling Law Models and Hyper-Parameters\n# Params. w/o Embedding\nHead\nLayer\nHidden\nTokens\nLR\nBatch Size*\n399M\n12\n12\n1536\n8.92B\n9.503e-4\n96\n545M\n14\n14\n1792\n14.04B\n9.143e-4\n128\n822M\n16\n16\n2048\n20.76B\n8.825e-4\n160\n1.1B\n18\n18\n2304\n28.54B\n8.561e-4\n192\n1.5B\n20\n20\n2560\n38.91B\n8.305e-4\n256\n*In terms of number of examples in 8K context length.\n10\n2\n10\n1\n100\n101\nPFLOP/s-days\n2.2\n2.4\n2.6\n2.8\n3.0\n3.2\n3.4\n3.6\n3.8\n4.0\nLM loss (seqlen=8K)\nMuon\nAdamW\nFigure 3: Fitted scaling law curves for Muon and AdamW optimizers.\n3.3\nPretraining with Muon\nModel Architecture\nTo evaluate Muon against contemporary model architectures, we pretrained from scratch using\nthe deepseek-v3-small architecture (DeepSeek-AI et al. 2024) as it demonstrates strong performance and the original\nresults serve as a reference for comparison. Our pretrained model has 2.24B activated and 15.29B total parameters (3B\nactivated and 16B total when including embedding). Minor modifications to the architecture are detailed in Appendix C.\nPretraining Data\nOur pretraining data details can be found in K. Team 2025. The maximum context length during\npretraining is 8K.\nPretraining\nThe model is trained in several stages. We use a 1e-3 auxfree bias update rate in stage 1 and 2, and 0.0\nauxfree bias update rate in stage 3. The weight decay is set to 0.1 for all stages. More details and discussions of model\ntraining can be found in the Appendix D.\n1. 0 to 33B tokens: In this stage, the learning rate linearly increases to 4.2e-4 in 2k steps. The batch size is kept\nat 2048 examples;\nTable 3: Fitted parameters of the scaling law curves\nMuon\nAdamW\nLM loss (seqlen=8K)\n2.506 × C−0.052\n2.608 × C−0.054\n7\n\nMuon is Scalable for LLM Training\nTECHNICAL REPORT\n2. 33B to 5.2T tokens: In this stage, the learning rate decays from 4.2e-4 to 4.2e-5 in a cosine style. We keep\nthe batch size at 2048 until 200B tokens, and then doubled to 4096 for the remaining;\n3. 5.2T to 5.7T tokens: In this stage (also referred as the cooldown stage), the learning rate increases to 1e-4 in\nin 100 steps, and then linearly decays to 0 in 500B tokens, and we keep a constant 4096 batch size. In this stage,\nwe use the highest quality data, focusing on math, code, and reasoning.\nEvaluation Benchmarks\nOur evaluation encompasses four primary categories of benchmarks, each designed to\nassess distinct capabilities of the model:\n• English Language Understanding and Reasoning: MMLU(5-shot)(Hendrycks, Burns, Basart, et al. 2021),\nMMLU-pro(5-shot) (Wang et al. 2024), BBH(3-shot) (Suzgun et al. 2022), TriviaQA(5-shot) (Joshi et al. 2017)\n• Code Generation: HumanEval(pass@1) (M. Chen et al. 2021), MBPP(pass@1)(Austin et al. 2021)\n• Mathematical Reasoning: GSM8K(4-shot) (Cobbe et al. 2021) MATH (Hendrycks, Burns, Kadavath, et al. 2021),\nCMATH (Wei et al. 2023)\n• Chinese Language Understanding and Reasoning: C-Eval(5-shot) (Y. Huang et al. 2023), CMMLU(5-shot)(H.\nLi et al. 2024)\nPerformance\nWe named our model trained with Muon “Moonlight”. We compared Moonlight with different public\nmodels on a similar scale. We first evaluated Moonlight at 1.2T tokens and compared it with the following models that\nhave the same architecture and trained with comparable number of tokens:\n• Deepseek-v3-Small (DeepSeek-AI et al. 2024) is a 2.4B/16B-parameter MoE model trained with 1.33T tokens;\n• Moonlight-A follows the same training settings as Moonlight, except that it uses the AdamW optimizer.\nFor Moonlight and Moonlight-A, we used the intermediate 1.2T token checkpoint of the total 5.7T pretraining, where\nthe learning rate is not decayed to minimal and the model has not gone through the cooldown stage yet.\nTable 4: Comparison of different models at around 1.2T tokens.\nBenchmark (Metric)\nDSV3-Small\nMoonlight-A@1.2T\nMoonlight@1.2T\nActivated Params†\n2.24B\n2.24B\n2.24B\nTotal Params†\n15.29B\n15.29B\n15.29B\nTraining Tokens\n1.33T\n1.2T\n1.2T\nOptimizer\nAdamW\nAdamW\nMuon\nEnglish\nMMLU\n53.3\n60.2\n60.4\nMMLU-pro\n-\n26.8\n28.1\nBBH\n41.4\n45.3\n43.2\nTriviaQA\n-\n57.4\n58.1\nCode\nHumanEval\n26.8\n29.3\n37.2\nMBPP\n36.8\n49.2\n52.9\nMath\nGSM8K\n31.4\n43.8\n45.0\nMATH\n10.7\n16.1\n19.8\nCMath\n-\n57.8\n60.2\nChinese\nC-Eval\n-\n57.2\n59.9\nCMMLU\n-\n58.2\n58.8\n† The reported parameter counts exclude the embedding parameters.\nAs shown in Table 4, Moonlight-A, our AdamW-trained baseline model, demonstrates strong performance compared to\nsimilar public models. Moonlight performs significantly better than Moonlight-A, proving the scaling effectiveness\nof Muon. We observed that Muon especially excels on Math and Code related tasks, and we encourage the research\ncommunity to further investigate this phenomena. After Moonlight is fully trained to 5.7T tokens, we compared it with\npublic models at similar scale and showed the results in Table 5:\n• LLAMA3-3B from Grattafiori et al. 2024 is a 3B-parameter dense model trained with 9T tokens.\n• Qwen2.5-3B from Yang et al. 2024 is a 3B-parameter dense model trained with 18T tokens.\n8\n\nMuon is Scalable for LLM Training\nTECHNICAL REPORT\nTable 5: Comparison of different models on various benchmarks.\nBenchmark (Metric)\nLlama3.2-3B\nQwen2.5-3B\nDSV2-Lite\nMoonlight\nActivated Param†\n2.81B\n2.77B\n2.24B\n2.24B\nTotal Params†\n2.81B\n2.77B\n15.29B\n15.29B\nTraining Tokens\n9T\n18T\n5.7T\n5.7T\nOptimizer\nAdamW\nUnknown\nAdamW\nMuon\nEnglish\nMMLU\n54.7\n65.6\n58.3\n70.0\nMMLU-pro\n25.0\n34.6\n25.5\n42.4\nBBH\n46.8\n56.3\n44.1\n65.2\nTriviaQA‡\n59.6\n51.1\n65.1\n66.3\nCode\nHumanEval\n28.0\n42.1\n29.9\n48.1\nMBPP\n48.7\n57.1\n43.2\n63.8\nMath\nGSM8K\n34.0\n79.1\n41.1\n77.4\nMATH\n8.5\n42.6\n17.1\n45.3\nCMath\n-\n80.0\n58.4\n81.1\nChinese\nC-Eval\n-\n75.0\n60.3\n77.2\nCMMLU\n-\n75.0\n64.3\n78.2\n† The reported parameter counts exclude the embedding parameters.‡ We tested all listed models with the full set of TriviaQA.\n• Deepseek-v2-Lite from DeepSeek-AI 2024 is a 2.4B/16B-parameter MOE model trained with 5.7T tokens.\nAs shown in Table 5, Moonlight outperforms models with similar architectures trained with an equivalent number of\ntokens. Even when compared to dense models trained on substantially larger datasets, Moonlight maintains competitive\nperformance. Detailed comparisons can be found in Appendix E. The performance of Moonlight is further compared\nwith other well-known language models on MMLU and GSM8k, as illustrated in Figure 1b and Appendix E Figure 8.6.\nNotably, Moonlight lies on the Pareto frontier of model performance versus training budget, outperforming many other\nmodels across various sizes.\n3.4\nDynamics of Singular Spectrum\nIn order to validate the intuition that Muon can optimize the weight matrices in more diverse directions, we conducted\na spectral analysis of the weight matrices trained with Muon and AdamW. For a weight matrix with singular values\nσ = (σ1, σ2, · · · , σn), we calculate the SVD entropy (Alter et al. 2000; Roy et al. 2007) of this matrix as follows:\nH(σ) = −\n1\nlog n\nn\nX\ni=1\nσ2\ni\nPn\nj=1 σ2\nj\nlog\nσ2\ni\nPn\nj=1 σ2\nj\nAs shown in Figure 4, we visualized the average SVD entropy of the weight matrices across different training checkpoints\nduring pretraining with 1.2T tokens. We can see that across all training checkpoints and all groups of weight matrices,\nthe SVD entropy of Muon is higher than that of AdamW, which verifies the intuition that Muon can provide a more\ndiverse spectrum of updates for the weight matrices. This discrepancy is more significant in the router weights for\nexpert selection, which indicates that mixture-of-expert models can benefit more from Muon.\nMoreover, we visualized the singular value distributions of each weight matrix at the checkpoint trained with 1.2T\ntokens as demonstrated in Appendix F. We find that, for over 90% of the weight matrices, the SVD entropy when\noptimized by Muon is higher than that of AdamW, providing strong empirical evidence for Muon’s superior capability\nin exploring diverse optimization directions.\n3.5\nSupervised Finetuning (SFT) with Muon\nIn this section, we present ablation studies on the Muon optimizer within the standard SFT stage of LLM training. Our\nfindings demonstrate that the benefits introduced by Muon persist during the SFT stage. Specifically, a model that is\nboth Muon-pretrained and Muon-finetuned outperforms others in the ablation studies. However, we also observe that\nwhen the SFT optimizer differs from the pretraining optimizer, SFT with Muon does not show a significant advantage\nover AdamW. This suggests that there is still considerable room for further exploration, which we leave for future work.\n6Performance metrics and computational requirements (FLOPs) for baseline models are sourced from (OLMo et al. 2024)\n9\n\nMuon is Scalable for LLM Training\nTECHNICAL REPORT\n0\n25\n0.80\n0.85\n0.90\nAttnQO\n0\n25\n0.85\n0.90\n0.95\nAttnKV\n0\n25\n0.93\n0.94\n0.95\nExperts\n0\n25\n0.90\n0.92\n0.94\nSharedExperts\n0\n25\n0.7\n0.8\n0.9\nRouter\n0\n25\n0.95\n0.96\n0.97\n0.98\nDense\nTraining Iterations (K)\nSVD Entropy\nAdamW\nMuon\nFigure 4: SVD entropy of weight matrices across different training iterations. We categorize the weight matrices into 6\ndifferent groups: 1) AttnQO denotes the weight matrices related to the query and output projection in the attention\nlayer; 2) AttnKV denotes the weight matrices related to the key and value projection in the attention layer; 3) Experts\ndenotes the weight matrices in expert models; 4) SharedExperts denotes the weight matrices in shared expert models; 5)\nRouter denotes the weight matrices in the router; 6) Dense denotes the weight matrices in the first dense layer. The\nSVD entropy is calculated as the macro-average of the weight matrices in each group across all layers. For weights in\nexpert models, we only calculate 3 out of 64 experts in different layers for efficiency.\n3.5.1\nAblation Studies on the Interchangeability of Pretrain and SFT Optimizers\nTo further investigate Muon’s potential, we finetuned Moonlight@1.2T and Moonlight-A@1.2T using both the Muon\nand AdamW optimizers. These models were finetuned for two epochs on the open-source tulu-3-sft-mixture dataset\n(Lambert et al. 2024), which contains 4k sequence length data. The learning rate followed a linear decay schedule,\nstarting at 5 × 10−5 and gradually reducing to 0. The results, shown in Table 6, highlight the superior performance of\nMoonlight@1.2T compared to Moonlight-A@1.2T.\nTable 6: Examining the impact of optimizer interchangeability between pretraining and SFT phases.\nBenchmark (Metric)\n# Shots\nMoonlight-1.2T\nPretraining Optimizer\n-\nMuon\nAdamW\nMuon\nAdamW\nSFT Optimzier\n-\nMuon\nMuon\nAdamW\nAdamW\nMMLU (EM)\n0-shot (CoT)\n55.7\n55.3\n50.2\n52.0\nHumanEval (Pass@1)\n0-shot\n57.3\n53.7\n52.4\n53.1\nMBPP (Pass@1)\n0-shot\n55.6\n55.5\n55.2\n55.2\nGSM8K (EM)\n5-shot\n68.0\n62.1\n64.9\n64.6\n3.5.2\nSFT with Muon on public pretrained models\nWe further applied Muon to the supervised fine-tuning (SFT) of a public pretrained model, specifically the Qwen2.5-7B\nbase model (Yang et al. 2024), using the open-source tulu-3-sft-mixture dataset (Lambert et al. 2024). The dataset\nwas packed with an 8k sequence length, and we employed a cosine decay learning rate schedule, starting at 2 × 10−5\nand gradually decreasing to 2 × 10−6. The results are presented in Table 7. For comparison, we show that the\nMuon-finetuned model achieves performance on par with the Adam-finetuned model. These results indicate that for\noptimal performance, it is more effective to apply Muon during the pretraining phase rather than during supervised\nfine-tuning.\nTable 7: Comparison of Adam and Muon optimizers applied to the SFT of the Qwen2.5-7B pretrained model.\nBenchmark (Metric)\n# Shots\nAdam-SFT\nMuon-SFT\nPretrained Model\n-\nQwen2.5-7B\nMMLU (EM)\n0-shot (CoT)\n71.4\n70.8\nHumanEval (Pass@1)\n0-shot\n79.3\n77.4\nMBPP (Pass@1)\n0-shot\n71.9\n71.6\nGSM8K (EM)\n5-shot\n89.8\n85.8\n10\n\nMuon is Scalable for LLM Training\nTECHNICAL REPORT\n4\nDiscussions\nThere are several possible directions for future research that could further explore and expand upon the current findings.\nIncorporating All Parameters into the Muon Framework\nCurrently, the Muon optimizer is utilized in conjunction\nwith the Adam optimizer, where certain parameters remain under the purview of Adam optimization. This hybrid\napproach, while functional, presents an opportunity for improvement. The integration of the optimization of all\nparameters exclusively within the Muon framework is a topic of significant research interest.\nExtending Muon to Schatten Norms\nThe Muon optimizer can be interpreted as the steepest descent method under\nthe spectral norm. Given the broad applicability and versatility of Schatten norms, extending Muon to encompass the\ngeneral Schatten norm is a promising direction. This extension may unlock additional optimization capabilities and\npotentially yield superior results compared to the current spectral norm-based implementation.\nUnderstanding and Solving the Pretraining-Finetuning Mismatch\nA notable phenomenon observed in practice\nis the suboptimal performance of models pretrained with AdamW when fine-tuned with Muon, and vice versa. This\noptimizer mismatch presents a significant barrier to effectively leveraging the extensive repository of AdamW-pretrained\ncheckpoints, thereby necessitating a rigorous theoretical investigation. A precise understanding of the underlying\nmechanisms is essential for devising robust and effective solutions.\n5\nConclusions\nIn this technical report, we presented a comprehensive study on the scalability of Muon in LLM training. Through\nsystematic analysis and improvements, we successfully applied Muon to a 3B/16B-parameter MoE model trained\non 5.7 trillion tokens. Our results demonstrate that Muon can effectively replace AdamW as the standard optimizer\nfor large-scale LLM training, offering significant advantages in both training efficiency and model performance. By\nopen-sourcing our implementation, the Moonlight model, and intermediate training checkpoints, we aim to facilitate\nfurther research in scalable optimization techniques and accelerate the development of training methods for LLMs.\n11\n\nMuon is Scalable for LLM Training\nTECHNICAL REPORT\nReferences\nAlter, Orly, Patrick O. Brown, and David Botstein. “Singular value decomposition for genome-wide expression data\nprocessing and modeling”. In: Proceedings of the National Academy of Sciences 97.18 (2000), pp. 10101–10106.\nDOI: 10.1073/pnas.97.18.10101. eprint: https://www.pnas.org/doi/pdf/10.1073/pnas.97.18.10101.\nURL: https://www.pnas.org/doi/abs/10.1073/pnas.97.18.10101.\nAustin, Jacob et al. Program Synthesis with Large Language Models. 2021. arXiv: 2108.07732 [cs.PL]. URL:\nhttps://arxiv.org/abs/2108.07732.\nBernstein, Jeremy and Laker Newhouse. Old Optimizer, New Norm: An Anthology. 2024. arXiv: 2409.20325 [cs.LG].\nURL: https://arxiv.org/abs/2409.20325.\nBi, Xiao et al. “Deepseek llm: Scaling open-source language models with longtermism”. In: arXiv preprint\narXiv:2401.02954 (2024).\nCesista, Franz Louis. Deep Learning Optimizers as Steepest Descent in Normed Spaces. 2024. URL: http://leloykun.\ngithub.io/ponder/steepest-descent-opt/.\nChen, Mark et al. “Evaluating Large Language Models Trained on Code”. In: (2021). arXiv: 2107.03374 [cs.LG].\nCobbe, Karl et al. Training Verifiers to Solve Math Word Problems. 2021. arXiv: 2110 . 14168 [cs.LG]. URL:\nhttps://arxiv.org/abs/2110.14168.\nDeepSeek-AI. DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model. 2024. arXiv:\n2405.04434 [cs.CL].\nDeepSeek-AI et al. DeepSeek-V3 Technical Report. 2024. arXiv: 2412.19437 [cs.CL]. URL: https://arxiv.org/\nabs/2412.19437.\nFranz, Louis Cesista. The Case for Muon. Oct. 2024. URL: https : / / x . com / leloykun / status /\n1846842887839125941 (visited on 02/18/2025).\nGrattafiori, Aaron et al. The Llama 3 Herd of Models. 2024. arXiv: 2407.21783 [cs.AI]. URL: https://arxiv.\norg/abs/2407.21783.\nHendrycks, Dan, Collin Burns, Steven Basart, et al. Measuring Massive Multitask Language Understanding. 2021.\narXiv: 2009.03300 [cs.CY]. URL: https://arxiv.org/abs/2009.03300.\nHendrycks, Dan, Collin Burns, Saurav Kadavath, et al. Measuring Mathematical Problem Solving With the MATH\nDataset. 2021. arXiv: 2103.03874 [cs.LG]. URL: https://arxiv.org/abs/2103.03874.\nHoffmann, Jordan et al. Training Compute-Optimal Large Language Models. 2022. arXiv: 2203.15556 [cs.CL].\nURL: https://arxiv.org/abs/2203.15556.\nHuang, Yuzhen et al. C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models. 2023.\narXiv: 2305.08322 [cs.CL]. URL: https://arxiv.org/abs/2305.08322.\nJordan, Keller et al. Muon: An optimizer for hidden layers in neural networks. 2024. URL: https://kellerjordan.\ngithub.io/posts/muon/.\nJoshi, Mandar et al. TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension.\n2017. arXiv: 1705.03551 [cs.CL]. URL: https://arxiv.org/abs/1705.03551.\nKaplan, Jared et al. Scaling Laws for Neural Language Models. 2020. arXiv: 2001.08361 [cs.LG]. URL: https:\n//arxiv.org/abs/2001.08361.\nKingma, Diederik P. and Jimmy Ba. “Adam: A Method for Stochastic Optimization”. In: 3rd International Conference\non Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.\nEd. by Yoshua Bengio and Yann LeCun. 2015. URL: http://arxiv.org/abs/1412.6980.\nLambert, Nathan et al. “Tülu 3: Pushing Frontiers in Open Language Model Post-Training”. In: (2024).\nLi, Haonan et al. CMMLU: Measuring massive multitask language understanding in Chinese. 2024. arXiv: 2306.09212\n[cs.CL]. URL: https://arxiv.org/abs/2306.09212.\nLi, Xi-Lin. “Preconditioned Stochastic Gradient Descent”. In: IEEE Transactions on Neural Networks and Learning\nSystems 29.5 (May 2018), pp. 1454–1466. ISSN: 2162-2388. DOI: 10.1109/tnnls.2017.2672978. URL: http:\n//dx.doi.org/10.1109/TNNLS.2017.2672978.\n– Preconditioner on Matrix Lie Group for SGD. 2018. arXiv: 1809.10232 [stat.ML]. URL: https://arxiv.org/\nabs/1809.10232.\n– Stochastic Hessian Fittings with Lie Groups. 2024. arXiv: 2402.11858 [stat.ML]. URL: https://arxiv.org/\nabs/2402.11858.\nLi, Xilin. Black Box Lie Group Preconditioners for SGD. 2022. arXiv: 2211.04422 [stat.ML]. URL: https:\n//arxiv.org/abs/2211.04422.\nLiu, Hong et al. “Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training”. In: The\nTwelfth International Conference on Learning Representations. 2024. URL: https://openreview.net/forum?\nid=3xHDeA8Noi.\nLoshchilov, Ilya and Frank Hutter. “Decoupled Weight Decay Regularization”. In: International Conference on Learning\nRepresentations. 2019. URL: https://openreview.net/forum?id=Bkg6RiCqY7.\n12\n\nMuon is Scalable for LLM Training\nTECHNICAL REPORT\nOLMo, Team et al. “2 OLMo 2 Furious”. In: arXiv preprint arXiv:2501.00656 (2024).\nOpenAI et al. GPT-4 Technical Report. 2024. arXiv: 2303.08774 [cs.CL]. URL: https://arxiv.org/abs/2303.\n08774.\nPethick, Thomas et al. Training Deep Learning Models with Norm-Constrained LMOs. 2025. arXiv: 2502.07529\n[cs.LG]. URL: https://arxiv.org/abs/2502.07529.\nPooladzandi, Omead and Xi-Lin Li. Curvature-Informed SGD via General Purpose Lie-Group Preconditioners. 2024.\narXiv: 2402.04553 [cs.LG]. URL: https://arxiv.org/abs/2402.04553.\nRajbhandari, Samyam et al. “ZeRO: Memory optimizations Toward Training Trillion Parameter Models”. In: (Nov.\n2020), pp. 1–16. DOI: 10.1109/sc41405.2020.00024. URL: http://dx.doi.org/10.1109/SC41405.2020.\n00024.\nRoy, Olivier and Martin Vetterli. “The effective rank: A measure of effective dimensionality”. In: 2007 15th European\nSignal Processing Conference. 2007, pp. 606–610.\nShazeer, Noam. Fast Transformer Decoding: One Write-Head is All You Need. 2019. arXiv: 1911.02150 [cs.NE].\nURL: https://arxiv.org/abs/1911.02150.\nShoeybi, Mohammad et al. Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism.\n2020. arXiv: 1909.08053 [cs.CL]. URL: https://arxiv.org/abs/1909.08053.\nSuzgun, Mirac et al. Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them. 2022. arXiv:\n2210.09261 [cs.CL]. URL: https://arxiv.org/abs/2210.09261.\nTeam, Gemini et al. Gemini: A Family of Highly Capable Multimodal Models. 2024. arXiv: 2312.11805 [cs.CL].\nURL: https://arxiv.org/abs/2312.11805.\nTeam, Gemma et al. “Gemma 2: Improving open language models at a practical size”. In: arXiv preprint\narXiv:2408.00118 (2024).\nTeam, Kimi. “Kimi k1.5: Scaling Reinforcement Learning with LLMs”. In: (2025).\nVyas, Nikhil et al. “SOAP: Improving and Stabilizing Shampoo using Adam”. In: The Thirteenth International\nConference on Learning Representations. 2025. URL: https://openreview.net/forum?id=IDxZhXrpNf.\nWang, Yubo et al. MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark. 2024.\narXiv: 2406.01574 [cs.CL]. URL: https://arxiv.org/abs/2406.01574.\nWei, Tianwen et al. CMATH: Can Your Language Model Pass Chinese Elementary School Math Test? 2023. arXiv:\n2306.16636 [cs.CL]. URL: https://arxiv.org/abs/2306.16636.\nYang, An et al. “Qwen2.5 Technical Report”. In: arXiv preprint arXiv:2412.15115 (2024).\nYou, Jiacheng. Jiacheng You’s discussion on Muon’s Update RMS. 2025. URL: https://x.com/YouJiacheng/\nstatus/1890094769386451309.\nYuan, Huizhuo et al. MARS: Unleashing the Power of Variance Reduction for Training Large Models. 2024. arXiv:\n2411.10438 [cs.LG].\n13\n\nMuon is Scalable for LLM Training\nTECHNICAL REPORT\nA\nUpdate RMS\nProof of Lemma 1\nProof. Without loss of generality, consider the orthogonal matrices U ∈Rn×n and V ∈Rm×m where n ≥m ≥r.\nWe will show that for X = U[:,:r]V[:r,:] (the update of the Muon has the same format), the RMS value is\np\nr/mn. From\nthe definition of matrix multiplication:\nXi,j =\nr\nX\nk=1\nUi,kVk,j\nThe RMS can be expressed as:\nRMS(X)2 =\n1\nmn\nn\nX\ni=1\nm\nX\nj=1\nr\nX\nk=1\nU 2\ni,kV 2\nk,j\n=\n1\nmn\nr\nX\nk=1\n n\nX\ni=1\nU 2\ni,k\n! \n\nm\nX\nj=1\nV 2\nk,j\n\n\n=\n1\nmn\nr\nX\nk=1\n1\n=\nr\nmn\nTherefore, RMS(X) =\np\nr/mn. For the common case where the matrices are full-rank, r = m, yielding RMS(X) =\np\n1/n.\nConsistent Update RMS Across Muon and AdamW\nAs discussed in 2.2, we’d like to match the update RMS\nbetween Muon and AdamW optimizers. This is validated by experiments on small-scale models. We set Muon’s\nUpdate RMS in the range of [0.05, 0.1, 0.2, 0.4, 0.8] and AdamW as baseline. We reported the loss and representative\nweight matrix RMS at 2k steps (about 2B tokens) in the Table 8. From the results, we find that 0.2 RMS and 0.4 RMS\nperformed similarly and much better than other settings. These findings are consistent with our empirical observation\nthat AdamW’s update RMS is in the range of 0.2 ∼0.4. We opted to control the update RMS of Muon to 0.2.\nTable 8: Muon Update RMS Experiments\nOptimizer\nAdamW\n0.05 RMS*\n0.1 RMS\n0.2 RMS\n0.4 RMS\n0.8 RMS\nLM training loss\n3.512\n3.355\n3.239\n3.198\n3.199\n3.386\nLM validation loss\n3.679\n3.503\n3.374\n3.325\n3.314\n3.543\nAttnQ weight RMS\n1.01e-2\n5.74e-3\n8.44e-3\n1.57e-2\n2.95e-2\n7.23e-2\nMlp weight RMS\n1.25e-2\n8.01e-3\n1.27e-2\n2.35e-2\n4.51e-2\n8.73e-2\n*Except the first column, all other candidates are using Muon with controlled RMS.\nB\nAdamW Baseline Scaling Law\nTo ensure the fairness and accuracy of our experiments, we conducted a series of experiments on our proprietary dataset\nto derive scaling law parameters that are optimal for AdamW. This includes determining the optimal model size(N),\nnumber of training tokens(D), learning rate(η), batch size(B) under a constrained computational budget (FLOPs,\nC). (Kaplan et al. 2020; Hoffmann et al. 2022; Bi et al. 2024) Table 9 presents the results of our systematic parameter\nsearch process.\nTable 9: Empirical Relationships Between Scaling Law Parameters and Computational Budget (FLOPs)\nN(C)\nD(C)\nη(C)\nB(C)\n0.0483359 · C0.5112684\n3.4480927 · C0.4887316\n0.0127339 · C−0.0574752\n0.0065202 · C0.4137915\n14\n\nMuon is Scalable for LLM Training\nTECHNICAL REPORT\n2 × 1010\n3 × 1010\n4 × 1010\n6 × 1010\nTrain Tokens\n2.20\n2.25\n2.30\n2.35\n2.40\n2.45\n2.50\n2.55\nLoss\nLoss vs. Train Tokens\nTraining FLOPs\n1.1e+20 FLOPs\n1.9e+20 FLOPs\n3.3e+20 FLOPs\n5.7e+20 FLOPs\n1.0e+21 FLOPs\n0.0006\n0.0008\n0.0010\n0.0012\n0.0014\nLearning Rate\n2.20\n2.25\n2.30\n2.35\n2.40\n2.45\nLoss\nLoss vs. Learning Rate\nTraining FLOPs\n1.5e+20 FLOPs\n2.5e+20 FLOPs\n4.2e+20 FLOPs\n6.9e+20 FLOPs\n1.1e+21 FLOPs\n200\n300\n400\n500\n600\n700\n800\n900\nBatch Size\n2.20\n2.25\n2.30\n2.35\n2.40\n2.45\nLoss\nLoss vs. Batch Size\nTraining FLOPs\n1.0e+20 FLOPs\n1.9e+20 FLOPs\n3.4e+20 FLOPs\n5.7e+20 FLOPs\n8.9e+20 FLOPs\nFigure 5: Optimization Landscapes for Scaling Law Hyper-parameters Across FLOPs Budgets\nHyper-Parameters Search\nTo systematically identify optimal scaling law hyper-parameters in the AdamW baseline,\nwe adopted a multistage search protocol. First, we selected multiple computational budgets (FLOPs levels) and\ninitialized model sizes, learning rates, and batch sizes based on empirical guidelines from prior studies. For each\nfixed FLOPs constraint, we varied the model size N while adjusting the training token count D inversely to maintain\nC = 6ND, thereby exploring the trade-off between model capacity and data efficiency. Each configuration was\ntrained to convergence, and the validation loss was recorded to determine the Pareto-optimal combinations of N and D.\nSubsequently, with the optimal N −D pairs fixed, we refined the learning rate and batch size through grid searches,\nensuring stability and convergence across configurations. To mitigate local minima and enhance robustness, this iterative\nprocedure was repeated 2–3 times, progressively narrowing the hyper-parameter space.\nThe optimization process is further illustrated in Figure 5, which depicts the loss landscapes as functions of training\ntokens, learning rate, and batch size across varying FLOPs budgets. Each bowl-shaped curve represents the loss surface\nfor a specific FLOPs level, with a distinct global minimum corresponding to the optimal hyper-parameter configuration.\nC\nModel Architecture\nMuon is agnostic to model architectures, and we used a model similar to Deepseek-V3-Small as described in DeepSeek-\nAI et al. 2024, because it is a strong model with open weights as a baseline. We made several small modifications in the\nMoonlight model and listed them here:\nMulti-token Prediction (MTP)\nMTP has not shown significant benefits to pretraining in our experiments. For\nsimplicity, we do not introduce MTP layers into the Moonlight model.\nAuxfree Bias Update\nIn DeepSeek-AI et al. 2024, auxfree bias is updated by: bi = bi + u × sign(ei), where u is\nthe update ratio, bi is the bias for the ith expert, and ei is the expert’s violating ratio. We slightly modified the update\nrule as: bi = bi + u × (sign(ei) −sign(e).mean()), where sign(e).mean() is the average of the signs of all expert’s\nviolating ratio, in order to control the magnitude of the bias, while does not change the topk selection logic.\nGate Scaling Factor\nDeepseek-V2-Lite did not use the gate scaling factor, and Deepseek-V3 used a scaling factor of\n2.5. We used a scaling factor of 2.446 to control a similar output rms like dense models. The code for calculating our\ngate scaling factor can be found in Figure 6.\nD\nTraining Stability\nNo Loss or Grad Norm Spike\nThe Moonlight training process was very smooth and we did not meet any loss spike\nor gradient norm spike. The loss and grad norm curve can be seen in Figure 7 (Moonlight is colored in blue and\nMoonlight-A trained by AdamW is colored in red)\nMax Attention Logit\nDuring training, we observed that while both the training loss and gradient norm remained\nstable throughout the process, the maximum attention logit (computed as the single largest logit value across the global\n15\n\nMuon is Scalable for LLM Training\nTECHNICAL REPORT\n1\nimport\nnumpy as np\n2\n3\ndef\nsigmoid(x):\n4\nreturn 1 / (1 + np.exp(-x))\n5\n6\ndef\ncalc_gate_scaling_factor (num_experts: int , topk: int , iter_times: int):\n7\n\"\"\" Calculate\nthe gate\nscaling\nfactor for MoE.\n8\n9\nArgs:\n10\nnum_experts (int): The number of experts.\n11\ntopk (int): The number of experts to select.\n12\niter_timers (int): The number of iterations.\n13\n14\nReturns:\n15\nfloat: The gate\nscaling\nfactor.\n16\n\"\"\"\n17\nfactors = []\n18\nfor _ in range(iter_times ):\n19\n20\n# mock\ngaussian\nlogits\n21\nlogits = np.random.randn(num_experts)\n22\n# select\ntopk\nlogits\n23\np = np.sort(sigmoid(logits ))[:: -1]\n24\np = p[: topk]\n25\n# renormalize\n26\np = p / p.sum()\n27\n# calculate\nthe\nscaling\nfactor\n28\nfactors.append( 1/ (p**2). sum ()**0.5)\n29\nreturn np.mean(factors)\nFigure 6: Python implementation for calculating the gate scaling factor.\nbatch) exhibited a distinct upward trajectory in specific layers during the initial training phase, exceeding a threshold of\n100. Notably, AdamW demonstrated healthier behavior in controlling this metric compared to alternative optimizers.\nTo further investigate the impacts of this phenomenon, we introduced the large attention logits ratio metric, defined as\nthe proportion of attention logits exceeding 100 within a batch. As shown in Fig.7, this ratio remained consistently low\n(about 10−4), indicating that extreme large logit values were sparse. Furthermore, the maximum logit values gradually\ndecrease as training progressed, suggesting that the optimization dynamics become healthier.\nRMSNorm Gamma Weight Decay\nIt is noteworthy that applying weight decay to the RMSNorm gamma parameter\nis crucial for ensuring training stability, as it effectively prevents excessively high output RMS values in each layer.\nE\nComparison with More Expensive Models\nTable 10 presents a comparative analysis between our Moonlight model (optimized with Muon) and publicly\navailable models trained with greater computational resources, including LLama3.1-8B (Grattafiori et al. 2024),\nGemma-9B (Gemma Team et al. 2024) and Qwen2.5-7B (Yang et al. 2024). Figure 8 illustrates the GSM8k performance\nbenchmarks of Moonlight against comparable models in the field.\nF\nSingular Value Distributions of Weight Matrices\nWe visualize the singular value distributions of weight matrices by plotting a line graph of its singular values in\ndescending order for each matrix, normalized by the largest one. As shown in Figures 9 and 10, we find that, for most\nof the weight matrices, the singular value distributions of them optimized by Muon are more flattened than that of\nAdamW, which further confirms the hypothesis that Muon can provide a more diverse spectrum of updates.\n16\n\nMuon is Scalable for LLM Training\nTECHNICAL REPORT\n5000\n10000\n15000\n20000\n25000\n30000\n35000\nIterations\n1.95\n2.00\n2.05\n2.10\n2.15\n2.20\nLoss\nMoonlight-A\nMoonlight\n(a) Training Loss\n5000\n10000\n15000\n20000\n25000\n30000\n35000\nIterations\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nGradient Norm\nMoonlight-A\nMoonlight\n(b) Gradient Norm\n5000\n10000\n15000\n20000\n25000\n30000\n35000\nIterations\n20\n40\n60\n80\n100\n120\nMax Attention Logit\nMoonlight-A\nMoonlight\n(c) Max Attention Logit (Layer 1)\n5000\n10000\n15000\n20000\n25000\n30000\n35000\nIterations\n0.00000\n0.00002\n0.00004\n0.00006\n0.00008\n0.00010\n0.00012\n0.00014\nLarge Attention logits Ratio\nMoonlight-A\nMoonlight\n(d) Large Attention Logits Ratio (Layer 1)\nFigure 7: Training dynamics comparison between Moonlight and Moonlight-A\nTable 10: Comparison of different models on various benchmarks.\nBenchmark (Metric)\nMoonlight\nLLAMA3.1-8B\nGemma2-9B\nQwen2.5-7B\nLarger Training Compute Model\nActivated Param†\n2.24B\n7.38B\n8.32B\n6.83B\nTotal Params†\n15.29B\n7.38B\n8.32B\n6.83B\nTraining Tokens\n5.7T\n15T\n8T\n18T\nOptimizer\nMuon\nAdamW\nUnknown\nUnknown\nEnglish\nMMLU\n70.0\n66.7\n71.3\n74.2\nMMLU-pro\n42.4\n37.1\n44.7\n45.0\nBBH\n65.2\n57.7\n68.2\n70.4\nTriviaQA‡\n66.3\n70.3\n-\n60.0\nCode\nHumanEval\n48.1\n37.2\n37.8\n57.9\nMBPP\n63.8\n47.6\n62.2\n74.9\nMath\nGSM8K\n77.4\n57.2\n70.7\n85.4\nMATH\n45.3\n20.3\n37.7\n49.8\n† The reported parameter counts exclude the embedding parameters.‡ We test all listed models with the full set of TriviaQA.\n17\n\nMuon is Scalable for LLM Training\nTECHNICAL REPORT\n2e22\n5e22\n1e23\n2e23\n5e23\n1e24\nTraining FLOPs\n10\n20\n30\n40\n50\n60\n70\n80\n90\nGSM8k Score\n      Llama-2-13B\nLlama-3.1-8B\nQwen-2.5-7B\nGemma-2-9B\nQwen-2.5-14B\nStableLM-2-12B\nAmber-7B\nMAP-Neo-7B\nOLMo-0424-7B\nDCLM-7B\nOLMo-2-7B\nOLMo-2-13B\nQwen-2.5-3B\nDeepSeek-V2-Lite-2.4B\nDeepSeek-V3-Small-2.4B\nMoonlight-2.4B-1.2T\nMoonlight-2.4B-5.7T\nGSM8k Performance Frontier\nFigure 8: The GSM8k performance of our Moonlight model optimized with Muon and other comparable models.\nWC\nL1\nL2\nL3\nL4\nL5\nL6\nL7\nL8\nL9\nL10\nL11\nL12\nL13\nL14\nL15\nL16\nL17\nL18\nL19\nL20\nL21\nL22\nL23\nL24\nL25\nL26\nL27\nWKC\nWKR\nWO\nWQC\nWQR\nWV\nAdamW\nMuon\nFigure 9: Distribution of singular values for each weight matrix in the attention layers. We use WC to denote the weight\nmatrices at each layer that compress the hidden states to the shared latent spaces for keys and values, WV to denote\nthe weight matrices up-projecting the values from the latent space, WO to denote the output projection matrices, and\nWKR, WKC, WQR and WQC to denote the projection matrices for the part of keys and queries with and without RoPE\nrespectively. We set the spines of each line graph red if the corresponding weight matrix optimized by Muon has a\nlower singular entropy than AdamW.\n18\n\nMuon is Scalable for LLM Training\nTECHNICAL REPORT\nE0WO\nL2\nL3\nL4\nL5\nL6\nL7\nL8\nL9\nL10\nL11\nL12\nL13\nL14\nL15\nL16\nL17\nL18\nL19\nL20\nL21\nL22\nL23\nL24\nL25\nL26\nL27\nE0WI\nE0WV\nE1WO\nE1WI\nE1WV\nE2WO\nE2WI\nE2WV\nE3WO\nE3WI\nE3WV\nSEWO\nSEWI\nSEWV\nRW\nAdamW\nMuon\nFigure 10: Distribution of singular values for each weight matrix in the feed-forward network (FFN) layers. We use WI,\nWV and WO to denote the weight matrices involved in the FFN layer with SwiGLU activation function, where WI\nrepresents the input projection to the Swish1 function, WV represents the extra input projection interacting with Swish1\nactivations, and WO represents the output projection. We use E0, E2, E3 to denote three arbitrarily selected expert\nmodels and SE to denote the weights in the shared expert model. We use RW to denote the weights in the router. We set\nthe spines of each line graph red if the corresponding weight matrix optimized by Muon has a lower singular entropy\nthan AdamW.\n19\n",
  "metadata": {
    "source_path": "papers/arxiv/Muon_is_Scalable_for_LLM_Training_6b8e3b47da4db09c.pdf",
    "content_hash": "6b8e3b47da4db09c952a35486c905f416c57081a32b445a36a3744c441b1c7f3",
    "arxiv_id": null,
    "title": "Muon_is_Scalable_for_LLM_Training_6b8e3b47da4db09c",
    "author": "",
    "creation_date": "D:20250225023458Z",
    "published": "2025-02-25T02:34:58",
    "pages": 19,
    "size": 2077505,
    "file_mtime": 1740470200.5161455
  }
}