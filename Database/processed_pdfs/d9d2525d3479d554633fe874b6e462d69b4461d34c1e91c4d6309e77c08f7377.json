{
  "text": "MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for\nRetrieval Augmented Generation\nMaría Andrea Cruz Blandón1*, Jayasimha Talur2†, Bruno Charron2,\nDong Liu2, Saab Mansour2, Marcello Federico2\n1Tampere University, 2Amazon\nAbstract\nAutomatic evaluation of retrieval augmented\ngeneration (RAG) systems relies on fine-\ngrained dimensions like faithfulness and rel-\nevance, as judged by expert human annotators.\nMeta-evaluation benchmarks support the devel-\nopment of automatic evaluators that correlate\nwell with human judgement. However, existing\nbenchmarks predominantly focus on English\nor use translated data, which fails to capture\ncultural nuances. A native approach provides a\nbetter representation of the end user experience.\nIn this work, we develop a Multilingual\nEnd-to-end Meta-Evaluation RAG benchmark\n(MEMERAG). Our benchmark builds on\nthe popular MIRACL dataset, using native-\nlanguage questions and generating responses\nwith diverse large language models (LLMs),\nwhich are then assessed by expert annotators\nfor faithfulness and relevance. We describe our\nannotation process and show that it achieves\nhigh inter-annotator agreement. We then anal-\nyse the performance of the answer-generating\nLLMs across languages as per the human eval-\nuators. Finally we apply the dataset to our main\nuse-case which is to benchmark multilingual\nautomatic evaluators (LLM-as-a-judge). We\nshow that our benchmark can reliably identify\nimprovements offered by advanced prompting\ntechniques and LLMs 1.\n1\nIntroduction\nRetrieval augmented generation (RAG) is emerging\nas a popular application of large language models\n(LLMs) and a powerful paradigm to improve LLMs\nfactuality (Gao et al., 2024).\nA RAG pipeline\nfirst retrieves relevant documents from an index\nbased on a query and then composes a response\nusing an LLM. Grounding the LLM response on\n*Work done as an intern at Amazon\n†Correspondence: talurj@amazon.com\n1We release https://anonymous our benchmark to sup-\nport the community developing accurate evaluation methods\nfor multilingual RAG systems.\nretrieved knowledge helps mitigate outdated knowl-\nedge, lack of domain expertise and reduce halluci-\nnations (Lewis et al., 2020; Gao et al., 2024). Col-\nlecting benchmarking data for RAG is challenging\ndue to the complexity of the pipeline that includes\ninformation retrieval and text generation. Text gen-\neration, our focus in this paper, has, in general, two\nmodes of automatic evaluation: reference-based\nand reference-free, which differ in the availability\nof human-generated gold references for each model\ninput. Both modes can either leverage single (e.g.\nBERTScore (Zhang et al., 2020)) or multidimen-\nsional (e.g. autoMQM (Fernandes et al., 2023))\nscores. Multidimensional evaluation (Burchardt,\n2013) provides more comprehensive understanding\nof text generation systems and is the de facto stan-\ndard in the machine translation (MT) community2.\nIn a reference-free evaluation setup, gold multi-\ndimensional judgements (factuality, relevance, etc)\nof model generations can be leveraged in a meta-\nevaluation framework. In this framework, auto-\nmated evaluators are evaluated against the human\njudgement to measure correlation. The automated\nevaluators can then be applied to measure the per-\nformance of new models’ outputs.\nPrevious work for RAG meta-evaluation mainly\nfocused on English (Fan et al., 2024) or lever-\naged human or machine translation of English\ndatasets (Sharma et al., 2024). Multilingual meta-\nevaluation is important to reliably measure per-\nformance across languages which can vary de-\npending on language characteristics (low vs. high\nresource, complex morphology, etc) and scripts\n(Latin vs. non-Latin). Translation-based bench-\nmarks, while permitting cross-language compar-\nisons, suffer from translationese phenomena such\nas introducing simpler syntax and lexical choices\n(Baker et al., 1993; Graham et al., 2020), thus lead-\n2Since 2021 in the WMT metrics shared task https://\nwww2.statmt.org/wmt24/metrics-task.html\n1\narXiv:2502.17163v1  [cs.CL]  24 Feb 2025\n\ning to data distributionally different from native\ndata and not necessarily reflecting native users pref-\nerences (Chen et al., 2024). Our position is that\ntranslation-based (parallel) benchmarks should be\ncomplemented by native multilingual benchmarks.\nTo bridge those gaps, we propose a native meta-\nevaluation multilingual benchmark for RAG sys-\ntems. Our benchmark is built on top of the pop-\nular MIRACL (Zhang et al., 2023) dataset3 that\nincludes native questions across 18 languages and\nrelevance judgements of retrieved passages for mul-\ntilingual retrieval evaluation. We extend MIRACL\nby generating answers in five languages with a di-\nverse set of LLMs, and collecting judgements on\nthe faithfulness and relevance of the answers us-\ning native expert human annotators. For the latter,\nwe devised a structured annotation process that\nachieved a high rate of inter-annotator agreement.\nTo evaluate the benchmark and set reference base-\nline results for others to compare against, we run\nLLM-as-a-judge experiments with various prompt-\ning techniques and state-of-the-art LLMs.\nTo summarize, our main contributions are:\n• We built and publicly release the first (to the\nbest of our knowledge) native multilingual\nmeta-evaluation RAG benchmark.\n• We developed a rigorous flow chart-based\nannotation process to achieve high inter-\nannotator agreement rate for both faithfulness\nand relevance judgements.\n• We evaluated the quality of the benchmark on\nthree multi-lingual meta-evaluation aspects:\nprompt selection, model selection, and fine-\ngrained analysis.\n• We establish reference baselines of multilin-\ngual automatic evaluators on our benchmark,\nshowcasing performance improvements when\nusing advanced prompting and LLMs.\n2\nRelated Work\nDue to the pipeline approach of RAG systems, the\nevaluation can be split into 3 main components:\n1) retriever metrics to identify relevant chunks of\ninformation to the input typically measured with\nrecall/precision@K (Manning et al., 2008); 2) gen-\nerator metrics to identify the “usefulness” of the\ngenerated answers in relation to the input, this is\ndone across fine grained dimensions such as faith-\nfulness and relevance, either leveraging references\nanswers (Es et al., 2024); 3) end-to-end (overall)\n3https://huggingface.co/datasets/miracl/miracl\nmetrics that take into account additional compo-\nnents such as preprocessing, chunking, query refor-\nmulation and cascading errors. Retrieval metrics\nhave been extensively studied in the information\nretrieval community, hence recent work focused on\nthe text generation performance of RAG systems.\nThis is also the focus of our work. One important\ndifference between generation with and without re-\ntrieved documents is the conflict between the para-\nmetric “world knowledge” and the non-parametric\nretrieved documents knowledge, hence the distinc-\ntion between faithfulness against the retrieved doc-\numents (RAG specific) and factuality according to\ngeneral knowledge (Maynez et al., 2020; Wu et al.,\n2024).\nRecent studies have investigated the importance\nof various components within multilingual RAG\nsystems. (Chirkova et al., 2024) utilized existing\nmultilingual QA datasets to evaluate different com-\nbinations of retrievers and generator models, find-\ning that task-specific prompt engineering is cru-\ncial for high-quality multilingual generation. In\nanother study, (Thakur et al., 2024) extend MIR-\nACL dataset to develop “MIRAGE-BENCH” a syn-\nthetic arena-based benchmark for ranking multilin-\ngual LLM generators. They employed preference\njudgments from a GPT-4o “judge” to train a rank-\ning model. However, an important limitation of\nsuch synthetic benchmarks is the potential for self-\npreference bias (Panickssery et al., 2024), where\nthe LLM judge may favor its own generations.\nIn this work, we focus on the faithfulness and\nrelevance aspects to ensure meaningful results.\nThese dimensions are typically assessed by hu-\nman evaluators based on model-generated outputs,\nhighlighting the need for developing automatic\nevaluation metrics that correlate well with human\njudgment—a process known as meta-evaluation.\nThis need has led to a recent trend in the English-\nlanguage research community of publishing meta-\nevaluation datasets and developing automated eval-\nuators (Es et al., 2024; Saad-Falcon et al., 2024).\nTo the best of our knowledge, no multilingual\nmeta-evaluation benchmark for RAG systems cur-\nrently exists. In this work, we address this gap by\ndeveloping such a benchmark. Our dataset facil-\nitates the creation of multilingual automatic eval-\nuators that correlate well with human judgments.\nThis, in turn, enables comprehensive end-to-end\nbenchmarking of RAG systems. We believe our\ndataset is the first to offer this capability in a multi-\nlingual context.\n2\n\nLang.\nQuestion\nLLM response\nHuman judgement\nFaithful\nRelevant\nEN\nWhen did Julia Child write \nher book Mastering the Art \nof French Cooking?\nShe co-authored it with Simone Beck and \nLouisette Bertholle.\n✓\nLogical \nconclusion\n✓\nAdds context to \nthe answer\nDE\nWelche bekannte Universität \ngibt es in Pennsylvania?\nHier sind einige Beispiele:\nX\nContradiction\nX\nUnrelated to the \nquestion\nES\n¿Cómo la ciudad de Chicago \ntiñe de verde el río para \ncelebrar San Patrick?\nAhora se utiliza un tinte vegetal seguro y no \nperjudicial para los organismos vivos en el río.\n✓\nDirect \nparaphrase\n✓\nAdds context to \nthe answer\nFR\nQuels sont les trains français \navec la vitesse la plus haute?\nLes trains S-103 et CRH 3, tous deux de type \nVelaro, sont les trains avec la vitesse \nd'exploitation la plus élevée.\nX\nAdds new \ninformation\n✓\nDirectly answers \nthe question\nHI\nक\" सर के उपचार म, -यु0त \nउ2कृ4ट गैस कौनसी है?\n=दए गए पाठांशC म, क\" सर के उपचार के Dलए \nFकसी गैस का उGलेख नहIं है।\n✓\nLogical \nconclusion\n✓\nDirectly answers \nthe question\nMIRACL dataset\nRetrieval benchmark\n \n \n \n \n \n \n \nMEMERAG (ours) Meta-evaluation benchmark\nFigure 1: Examples from our Multilingual End-to-end Meta-Evaluation for RAG (MEMERAG) dataset. We select\nMIRACL native multilingual questions (for a subset of 5 languages), generate responses using diverse LLMs, and\nannotate each generated sentence with human experts for faithfulness and relevance. The annotation includes both\ncoarse-grained (✓, ✕) and fine-grained labels. Our dataset forms a meta-evaluation benchmark where automated\nevaluators can be developed and assessed on their correlation to human judgement. Due to space constraints we\nomit the retrieved documents (context) for the question and show only one sentence per LLM response.\n3\nDataset Construction\nMeta-evaluation datasets enable the development\nof reliable automatic evaluators. In a RAG setup,\nthe input to the evaluator is composed of a question\nq (user input), a context c (set of passages auto-\nmatically retrieved to the question) and an answer\na generated by a language model to answer the\nquestion based on the context. The end-to-end eval-\nuator then needs to judge the quality of the answer\na given the context and the question (c, q). Fol-\nlowing previous work (Saad-Falcon et al., 2024; Es\net al., 2024) we focus on two quality dimensions:\nFaithfulness Is the answer grounded on the con-\ntext, regardless of your world knowledge?\nRelevance Is the answer relevant to the question,\nregardless of the context?\nWe build a multilingual end-to-end meta-\nevaluation RAG (MEMERAG) dataset by extend-\ning the MIRACL dataset (Zhang et al., 2023) to in-\nclude model-generated answers and human-based\nquality judgements. More precisely, we select rel-\nevant question-context pairs, generate answers us-\ning various language models and gather expert\nhuman annotations on the quality of those an-\nswers. Our dataset encompasses 5 languages: En-\nglish (EN), German (DE), Spanish (ES), French\n(FR), and Hindi (HI), which represent multiple lan-\nguage families and both high- and low-resource lan-\nguages. Figure 1 shows examples from the dataset,\nwith LLM-generated answers and coarse- to fine-\ngrained human-assigned labels for the faithfulness\nand relevance dimensions.\nThe MIRACL dataset is composed of questions\nwritten by humans in their native languages, one\nor more passages automatically retrieved from the\nWikipedia, and human annotations about the rele-\nvance of each passage. Building a dataset starting\nfrom native questions in each language allows to\nevaluate RAG pipelines without resorting to (ma-\nchine) translations, thus avoiding limitations and\nbiases associated with translation. Note, however,\nthat as questions were elicited from native speak-\ners independently across different languages, the\nresulting data set is not parallel.\n3.1\nQuestion Selection\nThe questions in the MIRACL dataset were gener-\nated by humans based on prompts. This leads to\nquestions that may be answerable by the prompt but\nmay have ambiguity outside of that context. In par-\nticular, we identified as problematic the questions\nfor which the right answer can change over time.\nFor example “Who is the president of Spain?” or\n3\n\n“How old is Drake Hogestyn?”. In a RAG setting,\ndifferent passages may have been written at differ-\nent times and provide conflicting context. Addi-\ntionally, the time of reference is usually not explicit\nin the question. To remove those complications we\nautomatically filtered out time-dependent questions\nacross all languages4. We combined the train and\ndev splits of the MIRACL dataset, which corre-\nsponds to a total of 3,662, 305, 2,810, 1,486 and\n1,519 questions respectively for EN, DE, ES, FR\nand HI. Of those, 244, 13, 120, 64 and 86 were\nidentified as time-dependent and filtered out.\n3.2\nContext Selection\nThe MIRACL dataset has an average of 10.3 pas-\nsages per question, which corresponds to an aver-\nage of 1,218 words of context in the English train\nand dev splits, with similar numbers in other lan-\nguages. To reduce the cognitive load on human\nannotators, we limit the context per query to 5 pas-\nsages.\nThe source dataset provides human-annotated\nbinary relevance labels for passages.\nTo more\naccurately simulate an automated retrieval pro-\ncess, we rank the passages for each question using\nBM25 (Schütze et al., 2008), as implemented in\n(Lù, 2024). We then select the top-5 ranked pas-\nsages for each question. If these top-5 passages\ndo not contain any human-annotated relevant pas-\nsages, we replace the lowest-ranked passage with\nthe highest-ranked relevant passage from the full\nset. This approach ensures that each question has\nat least one relevant passage in its context, avoid-\ning scenarios where annotators would evaluate re-\nsponses without any relevant information.\nIt is worth noting that simulating scenarios\nwhere no relevant passages exist is straightforward\n(e.g., by including only irrelevant passages). In\nsuch cases, for faithfulness evaluation, we would\nexpect responses like \"The provided documents do\nnot contain a relevant answer.\" Our method focuses\non faithfulness while efficiently utilizing human\nannotation efforts by ensuring that each evaluated\ncase has at least some relevant context.\n3.3\nAnswer Generation\nAfter question and passage selection, we gener-\nate an answer for each question-context pair and\n4See Appendix F for all prompts used during dataset con-\nstruction.\neach of five state-of-the-art LLMs.5 Those LLMs\nwere selected to cover a range of model sizes, open\nweight and proprietary models. We prompted all\nthe models in English6, asking to answer the ques-\ntion based only on the given context, and requesting\nthe answer to be provided in the same language as\nthe context and question. For all models, we set\nthe temperature to 0.1, and maximum number of\noutput tokens to 1000.\nWe thus produced answers for more than 1000\nquestions per language, except for German for\nwhich MIRACL only contains 305 questions. As\nour focus is on long-form answers, we further fil-\ntered out questions for which any of the 5 models\ngenerated an answer shorter than 10 words.\n3.4\nAnnotation Guidelines\nThe task of annotating answers with faithfulness\nis challenging due to several factors. First, it in-\nvolves some subjectivity which might impact Inter-\nAnnotator Agreement (IAA) (Kryscinski et al.,\n2020; Tang et al., 2024b). Then, its label space\nis not precisely defined in the literature (Tang et al.,\n2024b; Laban et al., 2023; Malaviya et al., 2024).\nFinally, although faithfulness should be ideally\nevaluated for atomic facts, it is generally evalu-\nated at the sentence or even document level, due to\nannotation costs.\nStarting with the factuality error taxonomy in-\ntroduced in Tang et al. (2024b), we ran a number\nof annotation pilots to refine the label space and\nguidelines.\nFinally, we converged to three coarse-grained la-\nbels (Supported, Not supported, Challenging to de-\ntermine), explained through 10 fine-grained labels.\nTo increase the consistency of the annotation (IAA),\nwe guide the annotation process through a flow\nchart (documented in Figure 3 in Appendix A). For\nrelevance, which is significantly less ambiguous\nto evaluate, we device a simple annotation process\nwith three labels: Directly answers the question,\nAdds context to the answer, and Unrelated to the\nquestion. Note that the first two labels can be used\nto describe \"relevant\" sentences, while the last label\nidentifies \"irrelevant\" sentences. (See Appendix A\nfor more details.)\n5We generated answers with Claude 3 Sonnet, Llama3\n70B, Llama3 8B, Mistral 7B, and GPT-4o mini.\n6There is evidence in the literature for better model accu-\nracy when the models are prompted to \"think\" in English (Lai\net al., 2023; Liu et al., 2024), though under particular scenar-\nios other languages could perform better (Behzad et al., 2024).\nWe leave additional prompting experiments for future work.\n4\n\nLang\n#Q\nAnswer\nContext\n#S\nAvg. #W\nAvg. #W\nEN\n250\n400\n30.3\n613.5\nDE\n250\n468\n27.3\n455.0\nES\n250\n563\n52.1\n522.3\nFR\n250\n540\n48.7\n478.3\nHI\n250\n351\n23.8\n571.5\nTotal\n1,250\n2,322\nTable 1: General statistics of the MEMERAG dataset.\n#Q: number of questions, #S: number of sentences, #W:\nnumber of words. Each answer is annotated at the sen-\ntence level, leading to 2,322 total sentences annotated\nby experts for faithfulness and relevance.\n3.5\nAnnotation Process\nFrom the question-context-answer triplets obtained\nin Section 3.3, we randomly sampled 250 questions\nper language (50 per answer-generating model,\nwithout overlapping questions for diversity). We\nemployed a professional vendor with native anno-\ntators7 to gather annotations for each sentence 8\nof the 250 answers per language. Among the 250\nanswers per language, a random subset of 10 were\nassigned to 3 annotators for computing the IAA\nand the rest to a single annotator. The statistics of\nthe annotated dataset are presented in Table 1.\nThe annotations were gathered via a web-based\ntool that implemented the flow chart of the anno-\ntation guidelines (see Appendix A for details). To\nfurther enhance IAA, we drew upon the findings\nof Krishna et al. (2023), which demonstrated that\nhighlighting relevant information aids annotators in\nperforming tasks and reaching consensus. Thus, we\nutilized the Llama 3 70B LLM to identify sentences\nwithin the retrieved passages that could potentially\nserve as supporting information to the answer sen-\ntences.\nThe English annotations required approximately\n25 hours of total annotation time, averaging 5.5\nminutes per question. This covered 250 questions,\nincluding 10 that were annotated by three different\nannotators for quality control. Similar time invest-\nments were observed for the other four languages.\nTable 2 summarizes the IAA per language for\nfaithfulness and relevance labels assigned by 3 an-\nnotators. We report IAA using Gwet’s AC1 (Gwet,\n7Annotators were compensated with a competitive hourly\nrate that is benchmarked against similar roles in their country\nof residence.\n8Sentences were segmented using the pySBD(Sadvilkar\nand Neumann, 2020) package.\n2008) and Fleiss Kappa (Fleiss, 1971). We observe\nhigh agreement for faithfulness (0.84-0.93 Gwet’s\nAC1 and 0.70-0.88 Fleiss Kappa) and even higher\nagreement for relevance (0.95-1.0 Gwet’s AC1 and\n0.63-1.0 Fleiss Kappa). This shows that the annota-\ntors are aligned and indicates a high quality of the\nannotations. Comparing to previous work, (Tang\net al., 2024b) report a Fleiss Kappa of 0.34-0.42 on\nfaithfulness labels which they deem fair to moder-\nate agreement. Note that a direct comparison with\nthis work is not possible as they deal with different\ntasks, nevertheless the high IAA we are reporting\nis a testament of the effectiveness of our flow chart-\nbased annotation design. Further details on IAA\nincluding fine-grained explanatory labels can be\nfound in Appendix A, Table 6.\nLang\nFaithfulness\nRelevance\nGwet’s\nFleiss\nGwet’s\nFleiss\nAC1\nKappa\nAC1\nKappa\nEN\n0.93\n0.77\n1.00\n1.00\nDE\n0.84\n0.81\n0.95\n0.73\nES\n0.91\n0.76\n1.00\n1.00\nFR\n0.89\n0.88\n0.93\n0.63\nHI\n0.89\n0.70\n1.00\n1.00\nTable 2: Inter-annotator agreement (IAA) on the faith-\nfulness and relevance dimensions with 3 annotators.\nAnnotations are at the answer sentence level.\n4\nAnnotation Results\nWe present in this section the results of the hu-\nman annotations for the 2,322 sentences of the\nMEMERAG dataset.\nLang\nFaithfulness\nRelevance\n✔\n✕\n?\n✔\n✓\n✕\nEN\n65.2\n31.5\n3.2\n65.2\n32.5\n2.2\nDE\n71.2\n26.7\n2.1\n61.3\n26.5\n12.2\nES\n65.7\n32.9\n1.4\n48.8\n43.9\n7.3\nFR\n62.0\n37.8\n0.2\n63.3\n29.3\n7.4\nHI\n73.8\n25.6\n0.6\n68.9\n21.4\n9.7\nTable 3: Label distribution in the benchmark. Percent-\nage of sentences labelled as supported (✔), not sup-\nported (✕), challenging to determine (?) for faithful-\nness, and as directly answers the question (✔), adds\ncontext to the answer (✓), unrelated to the question (✕)\nfor relevance, per language.\nTable 3 shows the distribution of faithfulness and\n5\n\nLabel\nEN\nDE\nES\nFR\nHI\nDirect paraphrase\n8.5\n41.7\n25.0\n33.7\n40.7\nLogical conclusion\n41.2\n28.2\n30.4\n28.0\n5.7\nOther\n15.5\n1.3\n10.3\n0.4\n27.4\nAdds new info\n7.0\n9.6\n16.0\n15.0\n14.8\nContradiction\n4.5\n11.3\n8.3\n5.9\n7.1\nMis-referencing\n1.5\n3.0\n2.3\n3.7\n0.3\nNuance shift\n6.8\n0.6\n4.3\n5.6\n1.7\nOpinion as fact\n0.5\n0.6\n0.2\n2.2\n0.3\nWrong reasoning\n10.0\n0.6\n1.4\n1.9\n0.3\nOther\n1.2\n0.9\n0.4\n3.5\n1.1\nChalleng. to determ.\n3.2\n2.1\n1.4\n0.2\n0.6\nTable 4: Fine-grained faithfulness label distribution\nin the benchmark.\nPercentage of sentences with\neach label per language.\nThe three sections corre-\nspond to the coarse-grained labels Supported/Not sup-\nported/Challenging to determine.\nrelevance labels across the five languages. The dis-\ntribution of labels is consistent across all languages,\nwith a few exceptions. On faithfulness, German\nand Hindi show higher percentages of Supported\nanswers. On relevance, Spanish presents a signif-\nicantly higher percentage of labels Adds context\nto the answer compared to other languages while\nEnglish had a very low share of labels Unrelated to\nthe question. As a partial explanation for this, we\nnote that Spanish questions generated the largest\nnumbers of output sentences, 563 vs. 400 (see\nTable 1) for English. Hence, we expect that the\nrelevance statistics reflect the tendency of Spanish\nanswers to be more verbose.\nFor a more granular insights into the annotations,\nwe show in Table 4 the distribution of the explana-\ntory fine-grained faithfulness labels for each lan-\nguage. (As label names are quite self-explanatory,\nwe refer for their precise meaning to Figure 3 in Ap-\npendix A.) Table 4 is split into three blocks, respec-\ntively, addressing fine-grained labels for Supported\n(top), Not supported (middle), and Challenging to\ndetermine (bottom) answers. We observe signif-\nicant differences across languages. For example,\nsupported answers for English are prominently un-\nder form of a logical conclusion from the context\n(41.2%), for German and Hindi from direct para-\nphrasing of information in the context (41.7% and\n40.7%), while for French and Spanish from a more\nbalanced combination of the two reasons. On the\nside of unsupported answers, the main mistake type\nin English is Wrong reasoning (10%), i.e. answers\nare non logical conclusions from the context, while\nthis type or error is significantly rarer (under 2%)\nfor all other languages. The rate of Adds new in-\nformation errors, a.k.a. hallucinations, ranges from\n7% for English to 16% for Spanish. The observed\ncross-linguistic variations in the distribution of la-\nbels can be attributed to the different nature of the\nquestions and accuracy of the models across the 5\nlanguages.\n5\nDataset Applications\nThe MEMERAG dataset is designed to support\nthe development of reliable automatic evaluation\nmethods. For that purpose, we describe in this\nsection how our dataset can be used as a bench-\nmark to enable various meta-evaluation use cases.\nWe focus on two applications: 1) Prompt selec-\ntion: The ability of our benchmark to effectively\nselect prompts for automatic evaluation, 2) Model\nselection: The effectiveness of our benchmark to\ndistinguish and select models. By concentrating on\nthese aspects, we can evaluate the benchmark’s util-\nity as a comprehensive tool for multilingual model\nassessment. While we provide reference baselines\nfor each application, our focus is on showcasing\nthe effectiveness of the benchmark rather than the\nunderlying capabilities of the LLMs.\n5.1\nExperimental Setup\nBenchmark Tasks\nOur benchmark is composed\nof multiple tasks defined by the annotation dimen-\nsion and subset considered. On the annotation di-\nmension, we focus our experiments on the coarse-\ngrained faithfulness dimension. This dimension\nis more challenging than relevance as highlighted\nby the lower IAA, while retaining a high-enough\nIAA to make for a trustworthy benchmark. We\ninvite benchmark users to also experiment on the\nother dimensions provided by the dataset depend-\ning on their use case. Note that we remove the\nsentences labelled as Challenging to determine by\nhuman annotators. On the annotation subset, we\nfirst distinguish the multilingual task which uses\nthe full dataset and the monolingual task, which\nonly considers a single language. Those task can\nthen be further broken down at the fine-grained\nlevel by considering the subset of sentences with a\ncertain fine-grained label. Performance is evaluated\nwith Balanced Accuracy (BAcc) (see Appendix E\nfor definition), with equal weights on each coarse-\ngrained label and language. We conduct signifi-\ncance testing using permutation tests (Good, 2013),\n6\n\nwith further details in Appendix D.\nReference Prompts\nTo demonstrate how the\nbenchmark can be used to select the appropri-\nate prompt, we experiment with multiple prompt-\ning strategies from simple to advanced techniques,\nstarting with zero-shot prompting, where LLMs\ndirectly classify statements as Supported or Not\nsupported. We then implement chain-of-thought\n(COT) prompting (Wei et al., 2022), which incorpo-\nrates an intermediate reasoning step. While these\nbasic prompting strategies provide a good start-\ning point for evaluation, our initial experiments\nrevealed limitations in their ability to capture the\nnuanced requirements of our specific task. With-\nout explicit guidelines in the prompt, automatic\nevaluators rely on their “world knowledge”, which\nmay not align with the specific requirements of\nthe evaluation task. To overcome this, we add in-\nstructions from the annotation guidelines (AG) in\nthe prompt. Adding annotation guidelines provides\nclear criteria for what constitutes Supported versus\nNot Supported sentences, reducing ambiguity in\nthe evaluation process. The various prompts are\npresented in Appendix G.\nReference Models\nWe experiment with four\nLLMs with varying model sizes and capabilities:\nGPT-4o mini, Qwen 2.5 32B, and two versions\nof Llama 3.2 (11B and 90B)9. In case an LLM\ndoes not produce one of the required labels, we\nrepeatedly prompt the LLM up to five times with\ntemperature and top_p equal to 0.1 to get a valid\nlabel. If the LLM fails to generate a label after five\nretries, we treat the datapoint as an error (wrong\nlabel).\n5.2\nExperimental Results\nOur benchmark enables systematic evaluation of\ndifferent approaches to automated faithfulness eval-\nuation. To illustrate this, we examine how the\nbenchmark can surface the effectiveness of vari-\nous automatic evaluation models and prompting\nstrategies.\nTable 5 demonstrates the benchmark’s ability to\ncompare different prompting approaches across lan-\nguages. The benchmark reveals consistent patterns,\n9In comparison to the LLMs selected for answer genera-\ntion, see Section 3.3, we upgraded Llama from 3 to 3.2 as the\ncontext length of 8K tokens was not sufficient for all prompts.\nWe picked GPT-4o mini as representative of proprietary mod-\nels. Additionally, Mistral was excluded as it performed poorly\nin initial experiments.\nPrompt\nGPT-4o\nmini\nQwen\n2.5 32B\nLlama\n3.2 90B\nLlama\n3.2 11B\nZS\n59.7\n66.7\n58.0\n55.4\nCOT\n61.4\n68.8\n59.9\n62.5\nAG\n71.6†\n72.6\n62.8†\n57.9\nAG+COT\n71.7\n71.8†\n64.4\n61.6†\nTable 5: Reference baselines for the multilingual task on\ncoarse-grained faithfulness. Balanced accuracy (BAcc)\nof the automatic evaluators averaged across the 5 lan-\nguages (EN, DE, ES, FR, HI) using zero-shot (ZS),\nchain-of-thought (COT), annotation guidelines (AG)\nand AG+COT prompting strategies. Bold indicates best\nperformance for the column, † indicates results not sta-\ntistically different from the best (p > 0.05). Additional\nresults on monolingual tasks and standard errors can be\nfound in Appendix D, Tables 8-10.\nshowing how different prompt designs impact eval-\nuation quality. As expected, adding a reasoning\nstep (COT) improves over zero-shot prompting. In\naddition, adding annotation guidelines (AG) helps\nalign automated evaluators with human judgments\nacross all languages. Comparing the two best mod-\nels GPT-4o mini and Qwen 2.5 32B, Qwen 2.5 32B\nexcels in the zero-shot and COT setups, showcas-\ning higher “out-of-the-box” alignment with human\njudgements. GPT-4o mini achieves similar perfor-\nmance once the annotation guidelines are added to\nthe prompt.\nFigure 2, shows the performance per language\nof various automatic evaluators with a fixed prompt\n(AG + COT), which allows us to select the best\nmodel for each language. We observe that GPT-4o\nmini performs best in English. For the rest of the\nlanguages Qwen 2.5 32B performs the best how-\never, the results are not statistically different from\nGPT-4o mini. Our benchmark also provides users\nwith the capability to conduct detailed, fine-grained\nanalyses of model performance across various di-\nmensions of faithfulness. The breakdown of au-\ntomatic evaluation performance by error type is\nshown in Appendix I.\n6\nConclusions\nWe introduced a high-quality and challenging mul-\ntilingual end-to-end meta-evaluation benchmark\nfor RAG (MEMERAG). Our carefully designed\nflow-chart-based annotation achieved a high inter-\nannotator agreement rate supporting the reliability\nof the benchmark. The introduced MEMERAG\ndataset opens the door for multiple application\n7\n\nLlama 3.2 11B\nQwen 2.5 32B\nLlama 3.2 90B\nGPT-4o mini\n40\n50\n60\n70\n80\n90\n100\n% BAcc\n60.0\n76.8\n63.2\n73.7\n% BAcc Scores for DE\nBest performance\nNot statistically different from best\nLlama 3.2 11B\nQwen 2.5 32B\nLlama 3.2 90B\nGPT-4o mini\n40\n50\n60\n70\n80\n90\n100\n% BAcc\n60.2\n62.5\n62.6\n68.4\n% BAcc Scores for EN\nLlama 3.2 11B\nQwen 2.5 32B\nLlama 3.2 90B\nGPT-4o mini\n40\n50\n60\n70\n80\n90\n100\n% BAcc\n59.1\n71.1\n63.4\n69.9\n% BAcc Scores for ES\nLlama 3.2 11B\nQwen 2.5 32B\nLlama 3.2 90B\nGPT-4o mini\n40\n50\n60\n70\n80\n90\n100\n% BAcc\n65.0\n74.4\n63.2\n73.7\n% BAcc Scores for FR\nLlama 3.2 11B\nQwen 2.5 32B\nLlama 3.2 90B\nGPT-4o mini\n40\n50\n60\n70\n80\n90\n100\n% BAcc\n65.2\n75.5\n75.1\n74.2\n% BAcc Scores for HI\nFigure 2: Reference baselines for the monolingual task on coarse-grained faithfulness. Balanced Accuracy (BAcc)\nof the automatic evaluators using various LLMs across five languages: EN, DE, ES, FR and HI. Each plot compares\nthe performance of four models: Llama 3.2 11B, Qwen 2.5 32B, Llama 3.2 90B, and GPT-4o mini using AG + COT\nprompt. The best-performing model for each language is highlighted with a darker blue bar. Bars with diagonal\nhatching indicate results not statistically different from the best (p > 0.05).\nscenarios, including but not limited to the demon-\nstrated cases, i.e. prompt selection and model se-\nlection.\nFor the meta-evaluation setup, we develop and\ncompare various LLMs-as-a-judge and observe that\nautomatic evaluators performance varies across the\nlanguages, influenced by language characteristics,\nnative-question complexity and LLM generation\nnuances. These variations underscore the impor-\ntance of our testbed, which demonstrated consis-\ntent results when comparing the prompting tech-\nniques (COT+guidelines > COT > zero-shot) and\nprovides a foundation for developing better multi-\nlingual evaluators.\n8\n\n7\nLimitations\nDue to time and cost constraints, our annotations\nand experiments are limited in terms of prompting\ntechniques, LLMs we experimented with and lan-\nguages we annotated. Nevertheless, we diversified\nour LLMs across size and “openness” while the\nlanguages represent two families and low and high\nresource ones. In addition, there exists in the liter-\nature fine-tuned factuality evaluators for English,\nthough we expect those to not work as well on non-\nEnglish languages. Another method is to approxi-\nmate factuality through entailment tasks (i.e. XNLI\ndataset) though such methods were shown (for En-\nglish) to be inferior to multi-task training and dis-\ntillation and data augmentation from LLMs (Tang\net al., 2024a). Fine-tuning multilingual evaluators\nand examining transfer learning across languages\nis interesting but is left for future work that can\nleverage our dataset for this purpose.\nAs we advocate for a native testing approach,\nthe questions across the languages are not paral-\nlel, which could introduce a dimension of differ-\nent questions and LLM generations complexities\nacross the different language test data. The data\nwe collected presents different challenges which\nare captured according to our fine grained error\nlabels (Table 4). Future work could balance the\nchallenges and complexities by collecting data for\nspecific challenging phenomena. Note that this bal-\nancing is not straightforward, it can be done on the\nquestion side though this is insufficient as it does\nnot control for the answer complexity. Controlling\nfor the answer complexity is a challenging prob-\nlem as the answer side is model generated (one\nmethod is to generate many answers and select for\ncertain phenomena with human in the loop which\nis costly).\nReferences\nMona Baker, Gill Francis, and Elena Tognini-Bonelli.\n1993. Corpus linguistics and translation studies: Im-\nplications and applications. In In Text and Technol-\nogy: In Honour of John Sin- clair, Netherlands. John\nBenjamins Publishing Company.\nShabnam Behzad, Amir Zeldes, and Nathan Schneider.\n2024. To ask LLMs about English grammaticality,\nprompt them in a different language. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2024, pages 15622–15634, Miami, Florida,\nUSA. Association for Computational Linguistics.\nAljoscha Burchardt. 2013. Multidimensional quality\nmetrics: a flexible system for assessing translation\nquality. In Proceedings of Translating and the Com-\nputer 35, London, UK. Aslib.\nPinzhen Chen, Simon Yu, Zhicheng Guo, and Barry\nHaddow. 2024. Is it good data for multilingual in-\nstruction tuning or just bad multilingual evaluation\nfor large language models? In Proceedings of the\n2024 Conference on Empirical Methods in Natu-\nral Language Processing, pages 9706–9726, Miami,\nFlorida, USA. Association for Computational Lin-\nguistics.\nNadezhda Chirkova, David Rau, Hervé Déjean, Thibault\nFormal, Stéphane Clinchant, and Vassilina Nikoulina.\n2024. Retrieval-augmented generation in multilin-\ngual settings. Preprint, arXiv:2407.01463.\nShahul Es, Jithin James, Luis Espinosa Anke, and\nSteven Schockaert. 2024. RAGAs: Automated evalu-\nation of retrieval augmented generation. In Proceed-\nings of the 18th Conference of the European Chap-\nter of the Association for Computational Linguistics:\nSystem Demonstrations, pages 150–158, St. Julians,\nMalta. Association for Computational Linguistics.\nWenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang,\nHengyun Li, Dawei Yin, Tat-Seng Chua, and Qing\nLi. 2024. A survey on rag meeting llms: Towards\nretrieval-augmented large language models. In Pro-\nceedings of the 30th ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining, KDD ’24,\npage 6491–6501, New York, NY, USA. Association\nfor Computing Machinery.\nPatrick Fernandes, Daniel Deutsch, Mara Finkelstein,\nParker Riley, André F. T. Martins, Graham Neubig,\nAnkush Garg, Jonathan H. Clark, Markus Freitag,\nand Orhan Firat. 2023. The Devil is in the Errors:\nLeveraging Large Language Models for Fine-grained\nMachine Translation Evaluation.\narXiv preprint.\nArXiv:2308.07286 [cs].\nJoseph Fleiss. 1971. Measuring nominal scale agree-\nment among many raters. Psychological Bulletin,\n76:378–.\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,\nJinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang,\nand Haofen Wang. 2024. Retrieval-Augmented Gen-\neration for Large Language Models: A Survey. arXiv\npreprint. ArXiv:2312.10997 [cs].\nPhillip Good. 2013.\nPermutation tests: a practical\nguide to resampling methods for testing hypotheses.\nSpringer Science & Business Media.\nYvette Graham, Barry Haddow, and Philipp Koehn.\n2020. Statistical power and translationese in machine\ntranslation evaluation. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 72–81, Online.\nAssociation for Computational Linguistics.\nKilem Li Gwet. 2008. Computing inter-rater reliability\nand its variance in the presence of high agreement.\n9\n\nBritish Journal of Mathematical and Statistical Psy-\nchology, 61(1):29–48.\nKalpesh Krishna, Erin Bransom, Bailey Kuehl, Mohit\nIyyer, Pradeep Dasigi, Arman Cohan, and Kyle Lo.\n2023. LongEval: Guidelines for human evaluation of\nfaithfulness in long-form summarization. In Proceed-\nings of the 17th Conference of the European Chap-\nter of the Association for Computational Linguistics,\npages 1650–1669, Dubrovnik, Croatia. Association\nfor Computational Linguistics.\nWojciech Kryscinski, Bryan McCann, Caiming Xiong,\nand Richard Socher. 2020. Evaluating the factual\nconsistency of abstractive text summarization. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9332–9346, Online. Association for Computa-\ntional Linguistics.\nPhilippe Laban, Wojciech Kryscinski, Divyansh Agar-\nwal, Alexander Fabbri, Caiming Xiong, Shafiq Joty,\nand Chien-Sheng Wu. 2023. SummEdits: Measuring\nLLM Ability at Factual Reasoning Through The Lens\nof Summarization. In Proceedings of the 2023 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 9662–9676, Singapore. Associa-\ntion for Computational Linguistics.\nViet Dac Lai, Nghia Ngo, Amir Pouran Ben Veyseh,\nHieu Man, Franck Dernoncourt, Trung Bui, and\nThien Huu Nguyen. 2023.\nChatGPT beyond En-\nglish: Towards a comprehensive evaluation of large\nlanguage models in multilingual learning. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2023, pages 13171–13189, Singapore.\nAssociation for Computational Linguistics.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459–9474.\nChaoqun Liu, Wenxuan Zhang, Yiran Zhao, Anh Tuan\nLuu, and Lidong Bing. 2024. Is translation all you\nneed? a study on solving multilingual tasks with\nlarge language models. Preprint, arXiv:2403.10258.\nXing Han Lù. 2024. Bm25s: Orders of magnitude faster\nlexical search via eager sparse scoring. Preprint,\narXiv:2407.03618.\nChaitanya Malaviya, Subin Lee, Sihao Chen, Elizabeth\nSieber, Mark Yatskar, and Dan Roth. 2024.\nEx-\npertQA: Expert-Curated Questions and Attributed\nAnswers. arXiv preprint. ArXiv:2309.07852 [cs].\nChristopher D. Manning, Prabhakar Raghavan, and Hin-\nrich Schütze. 2008. Introduction to Information Re-\ntrieval.\nCambridge University Press, Cambridge,\nUK.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\nRyan McDonald. 2020. On faithfulness and factu-\nality in abstractive summarization. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 1906–1919, On-\nline. Association for Computational Linguistics.\nArjun Panickssery, Samuel R. Bowman, and Shi Feng.\n2024. Llm evaluators recognize and favor their own\ngenerations. Preprint, arXiv:2404.13076.\nJon Saad-Falcon, Omar Khattab, Christopher Potts, and\nMatei Zaharia. 2024. ARES: An Automated Evalua-\ntion Framework for Retrieval-Augmented Generation\nSystems. arXiv preprint. ArXiv:2311.09476 [cs].\nNipun Sadvilkar and Mark Neumann. 2020. PySBD:\nPragmatic sentence boundary disambiguation. In\nProceedings of Second Workshop for NLP Open\nSource Software (NLP-OSS), pages 110–114, Online.\nAssociation for Computational Linguistics.\nHinrich Schütze, Christopher D Manning, and Prab-\nhakar Raghavan. 2008. Introduction to information\nretrieval, volume 39. Cambridge University Press\nCambridge.\nNikhil Sharma, Kenton Murray, and Ziang Xiao. 2024.\nFaux polyglot: A study on information disparity\nin multilingual large language models.\nPreprint,\narXiv:2407.05502.\nLiyan Tang,\nPhilippe Laban,\nand Greg Durrett.\n2024a.\nMiniCheck: Efficient Fact-Checking of\nLLMs on Grounding Documents. arXiv preprint.\nArXiv:2404.10774 [cs].\nLiyan Tang, Igor Shalyminov, Amy Wing-mei Wong,\nJon Burnsky, Jake W. Vincent, Yu’an Yang, Siffi\nSingh, Song Feng, Hwanjun Song, Hang Su, Lijia\nSun, Yi Zhang, Saab Mansour, and Kathleen McKe-\nown. 2024b. TofuEval: Evaluating Hallucinations of\nLLMs on Topic-Focused Dialogue Summarization.\narXiv preprint. ArXiv:2402.13249 [cs].\nNandan Thakur, Suleman Kazi, Ge Luo, Jimmy Lin, and\nAmin Ahmad. 2024. Mirage-bench: Automatic mul-\ntilingual benchmark arena for retrieval-augmented\ngeneration systems. Preprint, arXiv:2410.13716.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. CoRR, abs/2201.11903.\nKevin Wu, Eric Wu, and James Zou. 2024. Clashe-\nval: Quantifying the tug-of-war between an LLM’s\ninternal prior and external evidence. In The Thirty-\neight Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Eval-\nuating text generation with bert. In International\nConference on Learning Representations.\n10\n\nXinyu Zhang, Nandan Thakur, Odunayo Ogundepo,\nEhsan Kamalloo, David Alfonso-Hermelo, Xi-\naoguang Li, Qun Liu, Mehdi Rezagholizadeh, and\nJimmy Lin. 2023.\nMIRACL : A Multilingual\nRetrieval Dataset Covering 18 Diverse Languages.\nTransactions of the Association for Computational\nLinguistics, 11:1114–1131.\nA\nHuman Annotation Guidance\nBefore conducting large-scale annotations, we con-\nducted a pilot with 10 English RAG outputs and 3\nannotators. We asked annotators to evaluate faith-\nfulness at the sentence level either as Supported\nor with a subset of the factuality mistakes typol-\nogy in Tang et al. (2024b) (developed for sum-\nmarization): Contradiction, Hallucination, Mis-\nreferencing, Nuance meaning shift, Opinion stated\nas fact, Wrong reasoning, to which was an Other\nmistake label was added to account for unforeseen\nmistakes in the RAG setting. As this led to very\nlow IAA, we conducted another round reducing\nthe non-supported labels to Stating opinion as fact,\nDrawing wrong conclusions, Other mistake but this\nstill resulted in low IAA (Gwet’s AC1 0.45).\nUpon careful analysis of the annotator disagree-\nments in the pilot and further rounds of calibration,\nwe developed the annotation workflow shown in\nFigure 3. The key improvements were: (i) add a\nChallenging to determine label, (ii) have two levels\nof labels, a coarse-grained level (Supported, Not\nsupported, Challenging to determine) and a fine-\ngrained level for precision on the mistakes, (iii)\nenforce annotators to follow a specific reasoning\nwith a flow chart, (iv) use numbers for the fine-\ngrained level rather than labels which could be\nmisinterpreted. Those guidelines allowed to reach\nsignificantly higher IAA on faithfulness (Gwet’s\nAC1 0.81 coarse-grained). Likewise we iterated\non the relevance labels, starting from Must have,\nNice to have, or Irrelevant and converging to the\nmore explicit Directly answers the question, Adds\ncontext to the answer, or Unrelated to the ques-\ntion. This also increased the IAA significantly. The\nscreenshot of the user interface used by human\nannotators is shown in Figure 4\nLang\n#S\nIAA (Gwet’s AC1)\nFaithfulness\nRelevance\nCoarse\nFine\nCoarse\nFine\nEN\n13\n0.93\n0.63\n1\n0.92\nDE\n31\n0.84\n0.47\n0.95\n0.91\nES\n20\n0.91\n0.3\n1\n0.93\nFR\n23\n0.89\n0.58\n0.93\n0.92\nHI\n17\n0.89\n0.18\n1\n1\nTable 6: Inter-annotator agreement (IAA) on the faith-\nfulness and relevance dimensions with 3 annotators, for\ncoarse-grained and fine-grained levels. Annotations are\nat the answer sentence level (#S number of answer sen-\ntences is provided).\n11\n\nFigure 3: Annotation guideline for faithfulness labelling by human\nFigure 4: User interface used by human annotators for labelling faithfulness and relevance.\nB\nDetailed numbers on MEMEREG\ndataset\nIn this section, we look at the detailed statics\nof faithfulness and relevance in the MEMERAG\ndataset itself. Overall, the faithfulness and rele-\nvance in the dataset variate a lot from one language\nto another language and from one generator model\nto another. There is usually a range of 10-20 per-\ncentage points between the language with lowest\npercentage and the language with highest percent-\nage of supported sentences. This supports our as-\nsumption of variable behaviour/performance across\ndifferent languages.\nGiven the large variation, the percentage of sup-\nported answers is larger than that of unsuported\nanswers. Similarly, the ratio of relevant answers is\nin general larger than that of other relevance buck-\n12\n\nModel\nEN\nDE\nES\nFR\nHI\nLLM-A (615)\nFaithfulness\n52.7 / 43.8 / 3.5\n74.3 / 17.1 / 8.6\n58.6 / 41.4 / 0\n59.6 / 40.4 / 0\n73.2 / 26.8 / 0\nRelevance\n42.5 / 51.7 / 5.8\n60.9 / 37.4 / 1.7\n42.3 / 54.5 / 3.2\n48 / 50.3 / 1.7\n46.8 / 41.3 / 11.9\nLLM-B (315)\nFaithfulness\n76.1 / 17.9 / 6\n88.4 / 11.6 / 0\n88.2 / 11.8 / 0\n58.6 / 41.4 / 0\n84.9 / 15.1 / 0\nRelevance\n67.1 / 31.5 / 1.4\n72.7 / 22.1 / 5.2\n75 / 22.2 / 2.8\n87.1 / 12.9 / 0\n80.7 / 10.5 / 8.8\nLLM-C (315)\nFaithfulness\n61.4 / 35.1 / 3.5\n69.8 / 30.2 / 0\n68.1 / 31.9 / 0\n80.3 / 19.7 / 0\n80 / 20 / 0\nRelevance\n86.9 / 11.5 / 1.6\n78.3 / 20.3 / 1.4\n69.3 / 28 / 2.7\n87.1 / 10 / 2.9\n85.9 / 9.4 / 4.7\nLLM-D (743)\nFaithfulness\n67.3 / 30.8 / 1.9\n61.4 / 38 / 0.6\n61 / 35.7 / 3.3\n59.8 / 40.2 / 0\n63 / 34.6 / 2.5\nRelevance\n70.3 / 29.7 / 0\n41.5 / 33 / 25.5\n33 / 52.3 / 14.7\n54.8 / 26.1 / 19.1\n57.1 / 24.2 / 18.7\nLLM-E (334)\nFaithfulness\n77.2 / 21.1 / 1.7\n73.9 / 26.1 / 0\n69.7 / 29 / 1.3\n59.2 / 39.5 / 1.3\n73.3 / 26.7 / 0\nRelevance\n80.3 / 19.7 / 0\n75.4 / 17.4 / 7.3\n63.4 / 36.6 / 0\n73.8 / 25 / 1.2\n90.6 / 9.4 / 0\nTable 7: Percentage of sentences labelled as Supported/Not Supported/Challenging to determine for the faithfulness,\nand as Directly answers the question/Adds context to the answer/Unrelated to the question for the relevance, per\nlanguage and model. The total number of sentences generated by the model across all languages is given in\nparenthesis besides the model name.\nets (except for ES with LLM-D; EN, ES and FR\nwith LLM-A). Interestingly, when analysing the\nvairous results per model and language, we did no\nfind general pattern. This is counter intuitive to our\nassumption of having more supported sentences\nfor English language. Surprisingly, even though\nHindi is a low-resource language the models are\nproducing 70% supported sentences, except for\nLLM-D that produced only around 62% supported\nsentences.\nC\nFine grained automatic annotation\nanalysis\nFigure 5 shows a heatmap representing the failure\nmodes of automatic evaluators. We observe that\nimproving the prompting strategy from ZS to AG +\nCOT, reduces automatic evaluation errors across all\nerror categories, except for Llama 3.2 11B, where\nthe model makes more errors in the “Logical con-\nclusion category”. We also observe that “Wrong\nreasoning”, “Nuance shift” and “Logical conclu-\nsion” are the top error categories for all the models\ntested. Future work could explore prompting or\nfine-tuning techniques designed to handle specific\nerror types.\nD\nStatistical Significance Test Details\nIn Table 5 and Figure 2, we aim to determine\nwhether the scores (in terms of BAcc) for the best\nprompt and best model was significantly different\nfrom the other scores in the table. To test statistical\nsignificance, we use permutation test (Good, 2013),\na non-parametric method for comparing two related\nsamples. The null hypothesis for this test suggests\nthat there is no significant difference between the\nperformance of the best-performing prompt/models\nand the other prompts/models, while the alternative\nhypothesis suggests a significant difference exists.\nWe consider α = 0.05 as significance level. Con-\nsequently, when p > 0.05, we are not able to reject\nthe null hypothesis, indicating that prompts and\nmodels have similar performance.\nE\nBalanced Accuracy\nDue to the presence of class imbalance in our\ndataset, we employ balanced accuracy as the pri-\nmary metric to assess the performance of our au-\ntomatic evaluators. Balanced accuracy provides a\nmore robust measure of performance when dealing\nwith imbalanced classes, as it equally weights the\nrecall of each class.\nFor our binary classification task, where the au-\ntomatic evaluators predict either \"Supported\" or\n\"Not Supported\", the balanced accuracy (BAcc) is\ncalculated as follows:\nBAcc = 1\n2\n\u0012\nTP\nTP + FN +\nTN\nTN + FP\n\u0013\nWhere TP, TN, FN, and FP denote True Positives\n(correctly identified “Supported” instances), True\nNegatives (correctly identified “Not Supported” in-\nstances), False Negatives (“Supported” instances\nmisclassified as “Not Supported”), and False Posi-\ntives (“Not Supported” instances misclassified as\n“Supported”), respectively.\n13\n\nGPT4o mini\nLlama 3.2 11B\nLlama 3.2 90B\nQwen 2.5 32B\nAnnotator Model\nWrong reasoning\nNuance shift\nAdds new information\nLogical conclusion\nContradiction\nMis-referencing\nOther mistake\nOther\nDirect paraphrase\nOpinion as fact\nFine-grained Label\n28\n33\n30\n24\n25\n26\n26\n25\n19\n24\n20\n19\n11\n26\n8\n15\n10\n13\n13\n12\n6\n6\n6\n4\n4\n5\n4\n4\n4\n2\n1\n5\n2\n2\n1\n2\n1\n2\n2\n2\nZero Shot\nEnglish\nGPT4o mini\nLlama 3.2 11B\nLlama 3.2 90B\nQwen 2.5 32B\nAnnotator Model\nWrong reasoning\nNuance shift\nAdds new information\nLogical conclusion\nContradiction\nMis-referencing\nOther mistake\nOther\nOpinion as fact\nDirect paraphrase\nFine-grained Label\n27\n24\n30\n25\n25\n22\n25\n26\n16\n17\n18\n19\n10\n31\n15\n9\n12\n9\n9\n10\n5\n4\n6\n5\n4\n4\n4\n4\n1\n5\n2\n1\n2\n2\n2\n2\n0\n3\n2\n0\nChain of Thought\nEnglish\nGPT4o mini\nLlama 3.2 11B\nLlama 3.2 90B\nQwen 2.5 32B\nAnnotator Model\nLogical conclusion\nWrong reasoning\nNuance shift\nAdds new information\nContradiction\nOther\nMis-referencing\nDirect paraphrase\nOther mistake\nOpinion as fact\nFine-grained Label\n30\n24\n8\n28\n16\n33\n26\n14\n17\n24\n25\n21\n10\n22\n21\n11\n4\n14\n14\n4\n8\n2\n2\n8\n3\n6\n5\n4\n7\n2\n1\n6\n3\n4\n4\n4\n1\n2\n2\n1\nAnnotation Guidelines (AG)\nEnglish\nGPT4o mini\nLlama 3.2 11B\nLlama 3.2 90B\nQwen 2.5 32B\nAnnotator Model\nLogical conclusion\nNuance shift\nWrong reasoning\nAdds new information\nContradiction\nOther\nMis-referencing\nDirect paraphrase\nOther mistake\nOpinion as fact\nFine-grained Label\n18\n51\n11\n15\n22\n17\n26\n24\n19\n21\n23\n21\n11\n12\n16\n16\n6\n8\n10\n11\n5\n16\n3\n5\n5\n3\n5\n5\n3\n6\n3\n4\n2\n3\n4\n4\n2\n1\n2\n2\nAnnotation Guidelines (AG) + COT\nEnglish\n5\n10\n15\n20\n25\n30\n0\n5\n10\n15\n20\n25\n30\n5\n10\n15\n20\n25\n30\n10\n20\n30\n40\n50\nDisagreement between Ground Truth and Automatic Annotator for English\nFigure 5: Fine-grained faithfulness errors when automatic evaluator disagrees with ground truth.\n14\n\nF\nPrompts used for dataset construction\nIn section we describe the various prompts used in\nour pipeline. Out prompts are written as Jinja210\ntemplates.\nF.1\nTime-dependent answer filtering\nDuring internal pilots, we identified answers that\nrelied on current date / current affairs that could\nbe challenging to determine their faithfulness. For\nexample, to the question \"How old is Yann Le-\nCun?\" is the answer correct if the LLM uses the\ntime when it was trained? To avoid such cases we\nused Llama3 70B to filter out those cases using the\nprompt shown below:\nTime-dependent Answer filtering\nYou\nare\nan\nNLP\nassistant\nthat\nhelps\nto\nidentify\nif\na\nquestion\nrequires\nto\nknow\nwhen\nis\ntoday\n(day,\nor\nmonth,\nor\nyear),\ncurrent\naffairs,\nor\nup-to-date\ninformation.\nGive the step by step of\nhow\nto\nanswer\nthe\nquestion\nin\nbetween\nthe labels <rationale></rationale>.\nThen\nverify if the steps included to know any\ninformation\nabout\nthe\ncurrent\ntime\nand\ngive\nyour\nanswer\nin\nbetween\nthe\ntags:\n<label></label>. Please only use ’yes’ or\n’no’ in your final answer. You will be given\nthe question in {{language}}.\nProvide\nyour\nrationale\nand\nlabel\nin\nEnglish.\nF.2\nHighlighting relevant segments\nWe highlight relevant statements in the passage\nto help annotators focus on important parts of the\ncontext. We prompt Llama 3 70B, to predict all\nthe statements that support the passage. We use\ntemperature equal to 0.1 in this step.\nF.3\nAnswer generation prompt\nWe generate the answers using the same prompt\nacross all models and languages. All the instruc-\ntions were given in English, while the context and\nquestion were given in the testing language. For\nall models we set the temperature to 0.1 and the\nmaximum number of token to 1000.\n10https://jinja.palletsprojects.com/en/stable/\nHighlighting Relevant Segments\nYou are an agent that verifies if sentences\nare supported by a given context.\nYou will be given a context made of several\npassages referred as \"Passage\" split into\nsentences, each with a numeral identifier,\nand\neach\nreferred\nas\n\"Sentence\".\nYour\ntask is to determine if the sentence is\nsupported by some of the passages (using\nlabel 1) or is not supported (using label\n0). Please follow this process:\n(1)\nExplain\nwhy\nthe\nsentence\nis\nsupported\nor\nnot\nsupported,\nplease\nwrite your reasoning in between the tags\n<rationale></rationale>. If a sentence is\nsupported, list the number of the sentences\nin the passage or several passages that\nsupported\nit.\nIf\na\nsentence\nis\nnot\nsupported, explain why is not supported.\n(2) Write the final label for the sentence\nin between the tags <label></label>.\n(3)\nWrite\nthe\nid\nof\nthe\nsupporting\nsentences\nfrom\npassages\nin\nbetween\nthe\ntags\n<references></references>\nseparated\nby comma. Remember to use only 0 or 1 for\nlabel.\n### Context\n{% for passage in passages -%}\nPassage {{loop.index}}:\n{% set outer_loop = loop %}\n{% for sent in passage.text %}\n{{outer_loop.index}}.{{loop.index}}:\n{{sent|safe}}\n{% endfor %}\n{% endfor %}\n### Sentence:\n{{sentence|safe}}\nAnswer Generation Prompt\nYou\nare\nan\nNLP\nassistant\nwhose\npurpose\nis to answer a question based on given\npassages.\nThe passages may or may not\nhelp answer the question.\nYou will need\nto provide the answer based only on the\npassages.\nThe answer must be in language\nwithout fail. Be concise and direct without\nreferring to passages in the answer. Avoid\nexpressions\nsuch\nas\n\"According\nto\nthe\npassages\" or \"Based on the passages\".\n{% for passage in passages -%}\n- {{passage.text|safe}}\n{% endfor %}\nQuestion: {{question}}\nPlease answer directly the question\nabove in {{language}}.\n15\n\nG\nPrompts for automated evaluation\nThe four prompts evaluate answer faithfulness to\nsource passages with increasing complexity: Zero-\nshot (ZS) provides basic supported/not-supported\nclassification, Chain of Thought (COT) adds ex-\nplicit reasoning steps, Annotation Guidelines (AG)\nincludes detailed evaluation criteria, and AG+COT\ncombines detailed guidelines with reasoning steps.\nAll prompts output their final classification in <an-\nswer> tags.\nZero-shot Prompt (ZS)\nYou are an automatic annotator tasked with\ndetermining\nwhether\na\ngiven\nanswer\nis\ngrounded in the list of provided evidence\npassages. Your role is to carefully analyze\nthe relationship between the answer and\nthe evidence, and then classify the answer\nas either \"Supported\" or \"Not Supported\".\nProvide your answer directly in <answer>\n</answer> tag.\nEvidence Passages:\n{% for passage in context %}\n.\n{{loop.index}}: {{passage.text}}\n{% endfor %}\nAnswer: {{answer segment}}\nNow\nprovided\nyour\nlabel\ndirectly\nas\n”Supported” or ”Not Supported”.\nChain of thought (COT)\nYou are an automatic annotator tasked with\ndetermining\nwhether\na\ngiven\nanswer\nis\ngrounded in the list of provided evidence\npassages. Your role is to carefully analyze\nthe relationship between the answer and the\nevidence, write your reasoning in between\nthe\ntags\n<rationale></rationale>\nthen\nclassify the answer as either “Supported”\nor\n“Not\nSupported”\nand\nwrite\nanswer\nin\n<answer> </answer> tag\nEvidence Passages:\n{% for passage in context %}\n{{loop.index}}: {{passage.text}}\n{% endfor %}\nAnswer: {{answer segment}}\nNow\nprovided\nyour\nlabel\ndirectly\nas\n\"Supported\" or \"Not Supported\".\nAnnotation Guidelines (AG)\nGiven a set of evidence passages, and an\nanswer, determine if the answer is fully\nsupported\nby\nthe\nevidence\npassages\nor\nnot.\nAnalyze each sentence of the answer\ncarefully and verify that all information\nit\ncontains\nis\nexplicitly\nstated\nin\nor\ncan be directly inferred from the evidence\npassages.\nOutput\n\"Not\nSupported\"\nif\nANY\nof\nthe\nfollowing are true:\nThe answer contains any information not\nexplicitly stated in or directly inferable\nfrom the passages.\nThe answer contradicts any information in\nthe passages.\nThe answer introduces any new information\nnot found in the passages.\nThe answer misrepresents or inaccurately\nparaphrases information from the passages.\nThe answer draws conclusions not logically\nsupported by the given information.\nThe answer changes the level of certainty,\nspecificity,\nor\nnuance\nfrom\nwhat\nis\nexpressed in the passages.\nThe\nanswer\ndoes\nnot\ndirectly\naddress\nthe\nspecific\naspect\nasked\nabout\nin\nthe\nquestion.\nThe\nanswer\nconflates\nor\nmisrepresents\nseparate\npieces\nof\ninformation\nwhen\nsummarizing multiple passages.\nOutput Supported otherwise.\nProvide\nthis\ndetermination\nwithout\nany\nadditional\nexplanation\nin\n<answer></answer> tags. Analyze thoroughly\nbut output only the single-word label.\nEvidence Passages:\n{% for passage in context %}\n{{loop.index}}: {{passage.text}}\n{% endfor %}\nAnswer: {{answer segment}}\nNow\nprovided\nyour\nlabel\ndirectly\nas\n\"Supported\" or \"Not Supported\".\n16\n\nAnnotation guidelines with Chain of thought (AG + COT)\nGiven a set of evidence passages, and an answer, determine if the answer is\nfully supported by the evidence passages or not. Analyze each sentence of the\nanswer carefully and verify that all information it contains is explicitly\nstated in or can be directly inferred from the evidence passages.\nOutput \"Not Supported\" if ANY of the following are true:\nThe answer contains any information not explicitly stated in or directly\ninferable from the passages.\nThe answer contradicts any information in the passages.\nThe answer introduces any new information not found in the passages.\nThe answer misrepresents or inaccurately paraphrases information from the\npassages.\nThe answer draws conclusions not logically supported by the given information.\nThe answer changes the level of certainty, specificity, or nuance from what is\nexpressed in the passages.\nThe answer does not directly address the specific aspect asked about in the\nquestion.\nThe answer conflates or misrepresents separate pieces of information when\nsummarizing multiple passages.\nOutput Supported otherwise.\nwrite your reasoning in between the tags <rationale></rationale> and Provide\nyour final answer in <answer></answer> tags.\nEvidence Passages:\n{% for passage in context %}\n{{loop.index}}: {{passage.text}}\n{% endfor %}\nAnswer: {{answer segment}}\nNow provided your label directly as \"Supported\" or \"Not Supported\".\nH\nMore Results on Meta-Evaluators\n17\n\nEN\nDE\nES\nFR\nHI\nGPT4o mini\n68.37 ± 2.84\n73.58 ± 2.86\n69.84 ± 2.26\n73.74 ± 2.29\n74.10 ± 2.61\nQwen 2.5 32B\n62.45 ± 3.00\n76.79 ± 2.76\n71.09 ± 1.98\n74.38 ± 2.28\n75.41 ± 2.72\nLlama 3.2 90B\n62.60 ± 2.73\n63.11 ± 2.70\n63.36 ± 2.09\n63.17 ± 1.97\n75.05 ± 2.87\nLlama 3.2 11B\n60.29 ± 2.84\n59.91 ± 3.13\n59.02 ± 2.51\n65.02 ± 2.46\n65.22 ± 2.75\nTable 8: Balanced Accuracy for five languages and four LLMs using AG+COT prompting strategy. Standard errors\nare calculated using 1000 Bootstrap iterations.\nPrompt\nEN\nDE\nES\nFR\nHI\nZS\n57.22 ± 2.14\n61.54 ± 1.98\n56.94 ± 1.26\n60.00 ± 1.39\n68.96 ± 2.22\nCOT\n60.71 ± 2.34\n63.83 ± 2.15\n60.61 ± 1.44\n63.85 ± 1.53\n69.68 ± 2.42\nAG\n62.72 ± 2.24\n68.87 ± 2.27\n63.61 ± 1.36\n68.32 ± 1.53\n70.20 ± 2.02\nAG + COT\n63.42 ± 2.44\n68.35 ± 2.37\n65.83 ± 1.64\n69.08 ± 1.71\n72.44 ± 2.07\nTable 9: Balanced Accuracy for five languages using four different prompting strategies, averaged across four LLMs\n(GPT-4o mini, Llama 3.2 90B, Llama 3.2 11B, and Qwen 2.5 32B). Standard errors are calculated using 1000\nBootstrap iterations.\nLanguage\nPrompt\nGPT4o mini\nLlama 3.2 90B\nLlama 3.2 11B\nQwen 2.5 32B\nEN\nZS\n59.8\n58.0\n51.0\n60.1\nCOT\n61.8\n59.1\n60.0\n62.0\nAG\n70.0\n59.4\n53.0\n68.5\nAG + COT\n68.4\n62.6\n60.2\n62.5\nDE\nZS\n61.1\n58.6\n54.8\n71.9\nCOT\n63.3\n60.0\n59.6\n72.7\nAG\n72.0\n68.6\n60.6\n74.6\nAG + COT\n73.7\n63.3\n60.0\n76.8\nES\nZS\n53.8\n56.8\n53.7\n63.7\nCOT\n56.2\n58.2\n61.9\n66.2\nAG\n69.7\n59.6\n54.3\n70.8\nAG + COT\n69.9\n63.4\n59.1\n71.1\nFR\nZS\n59.5\n56.3\n56.0\n68.5\nCOT\n60.4\n58.5\n65.1\n71.6\nAG\n74.5\n62.4\n60.3\n76.2\nAG + COT\n73.7\n63.2\n65.0\n74.4\nHI\nZS\n72.2\n64.3\n66.1\n73.3\nCOT\n72.1\n67.6\n65.4\n73.8\nAG\n73.8\n67.1\n65.5\n74.5\nAG + COT\n74.2\n75.1\n65.2\n75.5\nTable 10: Meta evaluation results for all combination of LLMs and prompts we have tested.\n18\n\nFine-Grained Label\n#S\nGPT-4o mini\nLlama 3.2\n90B\nLogical conclusion\n151\n80.1\n87.4\nDirect paraphrase\n182\n93.4\n97.3\nOther\n2\n50.0\n50.0\nWe. Acc Sup.\n87.2\n92.5\nAdds new information\n81\n67.9\n33.3\nNuance shift\n30\n26.7\n16.7\nMis-referencing\n20\n35.0\n20.0\nContradiction\n32\n65.6\n50.0\nOpinion as fact\n12\n66.7\n25.0\nOther mistake\n19\n84.2\n52.6\nWrong reasoning\n10\n80.0\n40.0\nWe. Acc NS.\n60.3\n33.8\nBAcc\n73.7\n63.2\nTable 11: Accuracy of GPT-4o mini and Llama 3.2 90B\non various fine-grained labels, when evaluating French\nlanguage using AG + COT prompt\nI\nFine grained Analysis\nTo demonstrate the benchmark’s capability for fine-\ngrained analysis, Table 11 breaks down automatic\nevaluation performance by error type for two mod-\nels: GPT-4o mini and Llama 3.2 90B, both evaluat-\ning French answers using AG + COT prompt. The\nupper section of the table shows the distribution of\nerrors when the ground truth class is Supported, cat-\negorized into logical conclusion, direct paraphrase,\nor other correct categories. The lower section de-\ntails the types of mistakes made by each model for\nthe Not supported category. We observe that both\nmodels show a similar pattern for the Supported\ncategory, with logical conclusions being the most\ncommon (69.8% for GPT-4o mini and 76.0% for\nLlama 3.2 90B), followed by direct paraphrases.\nFor the Not supported category, both models strug-\ngle most with detecting “adding new information”\n(32.1% and 40.0% of mistakes, respectively) and\n“nuance shifts” (27.2% and 18.5%). The ranking of\nerror types is consistent across both models, despite\ntheir different overall BAcc (73.7% for GPT-4o\nmini vs 63.2% for Llama 3.2 90B see Table 10 in\nAppendix), suggesting similar challenges in evalua-\ntion. The benchmark’s fine-grained labeling system\nprovides a detailed view of evaluation challenges\nacross languages. By categorizing different types\nof faithfulness violations, it reveals which specific\nerrors are harder to detect for each model, offering\ninsights into both the strengths and limitations of\nautomated evaluation methods.\n19\n",
  "metadata": {
    "source_path": "papers/arxiv/MEMERAG_A_Multilingual_End-to-End_Meta-Evaluation_Benchmark_for\n__Retrieval_Augmented_Generation_d9d2525d3479d554.pdf",
    "content_hash": "d9d2525d3479d554633fe874b6e462d69b4461d34c1e91c4d6309e77c08f7377",
    "arxiv_id": null,
    "title": "MEMERAG_A_Multilingual_End-to-End_Meta-Evaluation_Benchmark_for\n__Retrieval_Augmented_Generation_d9d2525d3479d554",
    "author": "",
    "creation_date": "D:20250225024948Z",
    "published": "2025-02-25T02:49:48",
    "pages": 19,
    "size": 1340388,
    "file_mtime": 1740470175.5464797
  }
}