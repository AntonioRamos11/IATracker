{
  "text": "MOVING PAST SINGLE METRICS: EXPLORING SHORT-TEXT\nCLUSTERING ACROSS MULTIPLE RESOLUTIONS\nJustin K. Miller\nSchool of Physics\nUniversity of Sydney\nCamperdown, NSW 2006\njustin.k.miller@sydney.edu.au\nTristram J. Alexander\nSchool of Physics\nUniversity of Sydney\nCamperdown, NSW 2006\ntristram.alexander@sydney.edu.au\nFebruary 25, 2025\nABSTRACT\nCluster number is typically a parameter selected at the outset in clustering problems, and while\nimpactful, the choice can often be difficult to justify. Inspired by bioinformatics, this study exam-\nines how the nature of clusters varies with cluster number, presenting a method for determining\ncluster robustness, and providing a systematic method for deciding on the cluster number. The\nstudy focuses specifically on short-text clustering, involving 30,000 political Twitter bios, where\nthe sparse co-occurrence of words between texts makes finding meaningful clusters challenging. A\nmetric of proportional stability is introduced to uncover the stability of specific clusters between\ncluster resolutions, and the results are visualised using Sankey diagrams to provide an interroga-\ntive tool for understanding the nature of the dataset. The visualisation provides an intuitive way to\ntrack cluster subdivision and reorganisation as cluster number increases, offering insights that static,\nsingle-resolution metrics cannot capture. The results show that instead of seeking a single ‘optimal’\nsolution, choosing a cluster number involves balancing informativeness and complexity.\n1\nIntroduction\nWhen faced with a large collection of documents, it can be challenging to determine the nature of the topics the\ndocuments are covering. One way to approach this problem is to group documents together by theme, thus making\nit easier to understand the nature of the overall collection. This grouping of documents can be framed as a clustering\nproblem, in which the goal is to deduce the categories within the data [1]. Typically this categorisation would proceed\nby seeking to maximise the insight provided by the categorisations, while minimising the cognitive load required to\ndetermine\nwhich document goes into which category [2]. The effectiveness of clustering is thus contingent on the balance\nbetween the simplicity of the categories and the usefulness of the information the categories convey [3].\nThe challenge in text clustering therefore lies in finding a balance between creating clusters that are informative enough\nto capture meaningful distinctions in the data while remaining interpretable and easy to apply. This study examines\nhow varying the number of clusters affects this trade-off between informativeness and interpretability, highlighting\nthat there is no universally optimal clustering solution, but rather context-dependent cluster structures that best balance\nthese competing factors.\nCluster validation indices provide a multidimensional assessment that is essential for selecting the number of clus-\nters in line with specific analysis goals [3]. However, the choice of validation metric can significantly impact the\ndetermination of an ‘ideal’ cluster number, as different metrics emphasise different clustering characteristics [4]. For\nexample, metrics that prioritise within-cluster homogeneity, such as measures of low within-cluster distance, may lead\nto a preference for a higher number of clusters, as they aim to minimize internal variance. Conversely, metrics that\nfavor between-cluster separation, such as separation indices, can favor a smaller number of clusters by emphasizing\ndistinct boundaries between groups. Selecting an appropriate clustering solution therefore requires balancing these\narXiv:2502.17020v1  [cs.LG]  24 Feb 2025\n\nA PREPRINT - FEBRUARY 25, 2025\nmetrics based on the dataset’s characteristics, the user’s objectives, and broader considerations such as interpretability\n[5]. While achieving high performance metrics that align with pre-established labels or metadata is often empha-\nsised, clustering’s ultimate objective should be to organise data in ways that are practically useful, rather than merely\nidentifying “correct” categories [6, 7, 8, 9, 4].\nWhen evaluating clustering solutions, it is possible to adopt either a single-level or a multi-level perspective. Single-\nlevel evaluation focuses on assessing the quality of clusters at a fixed number of clusters (K), treating the solution\nas static and independent of other potential resolutions. Methods such as coherence [10] and silhouette scores [11]\nrepresent this single-level approach: they yield valuable insights into cluster compactness and separability but only\nfor one chosen K. In contrast, a multi-level approach considers how cluster structures evolve as K changes, making\nit possible to determine whether clusters persist across multiple resolutions or simply arise at a particular K. By\ncapturing these dynamic relationships and structural transitions, a multi-level perspective can reveal whether clusters\nrepresent enduring, meaningful patterns in the data or whether they emerge and vanish as the clustering granularity\nshifts.\nA multi-level approach therefore not only provides greater insight into the underlying data structure but also leads\nto clustering solutions that more closely reflect the dataset’s characteristics and the user’s objectives. Developing\nmethodologies that incorporate multi-level analysis thus offers a more comprehensive and nuanced validation process.\nIn this work, the evolution and stability of clusters in short-text data are examined across a range of resolutions.\nThe methodology incorporates multi-level stability assessments using Adjusted Mutual Information (AMI) and intro-\nduces a novel metric, Proportional Stability, alongside Sankey diagram visualisations to illustrate cluster transitions.\nThese tools capture both the robustness of clustering outcomes and the deeper structural changes that occur as the\nnumber of clusters increases. By highlighting the importance of examining cluster dynamics across resolutions, this\nwork advances clustering validation methodologies and supports more interpretable, contextually meaningful results\nin short-text clustering applications.\n2\nRelated Work\nStability of clusters\nMost clustering methods involve finding clusters after a random initialisation process [12, 13]. This stochastic element\nin the initialisation means that the resulting clusters may be different with different initialisations. Cluster stability\nmeasures how consistently an algorithm finds the same clusters within a dataset across the different random starting\npoints. Cluster stability is therefore a measure which may be used to assess the robustness and reliability of clustering\nmethods, with the assumption being that if there is some underlying structure in the data, the algorithm should be able\nto recover this consistently.\nCluster stability is central in the field of bioinformatics, where the high-dimensional nature of the data, coupled with\nbiological variation, makes stability an indicator that a clustering algorithm has captured biologically meaningful\nclusters (e.g., distinct cell types) [14]. Importantly, in bioinformatics, the notion of stability is tied to the biological\nreproducibility of the findings: when clusters correspond to real biological phenomena, they should persist across\ndifferent algorithms, initialisations, and even perturbations to the dataset [15, 16]. To test how stable the clusters\nfound by an algorithm are, one must consider both the variance in the data, and also the variance in the dimensions\nused i.e. ensuring that the clusters are not just forming as a result of variance within a few data points, or from a few\nvariables [17]. This work will draw on the approaches of bioinformatics, but applied to text analysis.\nA key difference between bioinformatics and text clustering lies in the interpretability of the clusters themselves. While\nbiological clusters often have clear interpretations based on known cell types or functions [18], the interpretability of\ntext clusters relies heavily on human judgment [19]. This adds another dimension to the utility of stability. In text\nclustering, stability might suggest that the algorithm has found coherent groupings, but it doesn’t guarantee that these\ngroupings are semantically meaningful or useful for the task at hand [12]. Hence, an additional human interpretability\nstep is often needed to evaluate whether the stable clusters align with meaningful categories or concepts [20].\nUltimately, stability serves as a valuable criterion for identifying meaningful clusters across fields, but the interpre-\ntation of stability can vary. In bioinformatics, stable clusters are likely to correspond to real, reproducible biological\nphenomena [17]. In text data, stable clusters may reflect coherent patterns, but assessing their utility requires further\ninterpretability checks, often involving human evaluation [21]. Stability, in this sense, acts as a precursor to, but not a\nguarantee of, meaningfulness [20].\n2\n\nA PREPRINT - FEBRUARY 25, 2025\nGaussian Mixture Model Clustering\nThis work uses Gaussian Mixture Models (GMMs) to find clusters in a high dimensional text embedding space. GMMs\nare a probabilistic approach to clustering that assume the data are generated from a mixture of several Gaussian distri-\nbutions [22]. Each Gaussian component represents a cluster, and the algorithm assigns probabilities of membership for\neach data point across these clusters, making GMM a soft clustering technique. This section outlines the theoretical\nfoundation and practical implementation of GMM clustering [23, 24, 25].\nThe GMM assumes that the dataset X = {x1, x2, . . . , xn} is generated from k Gaussian distributions, where each\ndata point xi ∈Rd belongs to a particular cluster with a certain probability. These parameters are generally esti-\nmated through the use of the Expectation-Maximization (EM) algorithm (E-step) [13], This paper uses the scikit learn\nimplementation of GMM, so uses this particular algorithm [22]. In the E-step, the algorithm calculates the posterior\nprobabilities (responsibilities) that each data point xi belongs to each Gaussian component j, denoted by γ(zij), where\nzij is the latent variable indicating the cluster membership of xi:\nγ(zij) =\nπj N(xi | µj, Σj)\nPk\nl=1 πl N(xi | µl, Σl)\n.\n(1)\nDefinitions of Terms:\n• γ(zij): The posterior probability (responsibility) that data point xi belongs to the j-th Gaussian component.\n• πj: The mixing coefficient (prior probability) for the j-th Gaussian component, satisfying Pk\nj=1 πj = 1 and\n0 ≤πj ≤1.\n• N(xi | µj, Σj): The multivariate Gaussian probability density function evaluated at xi with mean µj and\ncovariance matrix Σj. It is defined as:\nN(xi | µj, Σj) =\n1\n(2π)d/2|Σj|1/2 exp\n\u0012\n−1\n2(xi −µj)⊤Σ−1\nj (xi −µj)\n\u0013\n,\n(2)\nwhere:\n– d is the dimensionality of the data.\n– |Σj| denotes the determinant of the covariance matrix.\n– (·)⊤represents the transpose of a vector.\n• xi: The i-th data point in your dataset.\n• µj: The mean vector of the j-th Gaussian component.\n• Σj: The covariance matrix of the j-th Gaussian component.\n• k: The total number of Gaussian components in the mixture model.\n• zij: The latent variable indicating the cluster membership of xi; zij = 1 if xi belongs to cluster j, and\nzij = 0 otherwise.\n• Pk\nl=1: The summation over all k Gaussian components.\nThis step assigns soft cluster memberships, meaning that each data point is assigned a probability of belonging to each\ncluster.\nCluster stability using Adjusted Mutual Information\nAdjusted Mutual Information (AMI) is a measure used to compare the similarity between two different clusterings of\nthe same dataset. It builds upon the concept of Mutual Information (MI), which quantifies the amount of information\nshared between two clusterings. MI is able to distinguish between true agreement and agreement that happens purely\nby chance, by looking at how the knowledge of one set of clusters reduces the surprise at the other set. However,\nMI does not account for the similarity that might occur by chance. AMI adjusts the MI score by accounting for\nthe expected similarity between random clusterings, providing a more accurate and reliable metric [26]. AMI is\nsymmetric, meaning that the AMI between the two sets is the same regardless of the order of the sets. The AMI used\nin this paper uses SKlearn’s implementation of AMI [27].\nMutual Information between two clusterings U and V is defined as:\n3\n\nA PREPRINT - FEBRUARY 25, 2025\nMI(U, V ) =\nX\nu∈U\nX\nv∈V\nP(u, v) log\n\u0012 P(u, v)\nP(u) P(v)\n\u0013\nWhere:\n• U = {u1, u2, . . . , uKU } is the set of clusters in clustering U.\n• V = {v1, v2, . . . , vKV } is the set of clusters in clustering V .\n• P(u) is the probability that a randomly selected data point belongs to cluster u in U.\n• P(v) is the probability that a randomly selected data point belongs to cluster v in V .\n• P(u, v) is the joint probability that a data point belongs to cluster u in U and cluster v in V .\nThe AMI adjusts the MI to account for chance, defined as:\nAMI(U, V ) =\nMI(U, V ) −E[MI(U, V )]\nAVG[H(U), H(V )] −E[MI(U, V )]\n(3)\nWhere:\n• MI(U, V ) is the Mutual Information between U and V .\n• E[MI(U, V )] is the expected Mutual Information between U and V if the cluster assignments were random.\n• H(U) and H(V ) are the entropies of U and V , respectively.\n• AVG[H(U), H(V )] is the average of the entropies of U and V .\nEntropy of a set of clusters measures the uncertainty associated with the cluster assignments:\nH(U) = −\nX\nu∈U\nP(u) log P(u)\nH(V ) = −\nX\nv∈V\nP(v) log P(v)\nThe AMI score ranges from 0 to 1, where close to 1 represents agreement between the two sets and close to 0 represents\nno relation between the two sets.\nMulti-level approaches to clustering\nPrevious work has explored multi-level approaches to clustering. One of which uses visualisations called ”Clustering\nTrees” [28]. These trees illustrate the proportion of each cluster at a given K that originates from clusters at lower\nresolutions. By applying this method to synthetic datasets with varying numbers of clusters, the visualisation aids in\nidentifying stable clusters and can help inform the appropriate choice of K. Using this method helps determine when\nadditional complexity (in the form of increasing K) leads to stable clusters and when further increasing K does not\nprovide meaningful insights.\nAnother approach to this is the use of MRTree [29]. Instead of focusing on a visual approach, MRtree uses a more\nquantifiable method. It determines the optimal number of clusters by analysing stability across different values of K\nusing the Adjusted Rand Index (ARI). Stability is calculated by comparing the clustering results from the reconciled\nhierarchical tree to the initial non-hierarchical clustering at each resolution. The optimal number of clusters corre-\nsponds to the resolution where stability is highest, as indicated by consistent and well-defined partitions. A change\npoint, where stability drops sharply with further increases in resolution, signifies the threshold for meaningful cluster-\ning. In the biological context, this approach generally indicates that the selected number of clusters reflects robust and\nbiologically relevant structures in the data [15, 16].\n4\n\nA PREPRINT - FEBRUARY 25, 2025\n3\nMethod\nThis paper builds on the methodology presented in Miller and Alexander (2025) [30], which introduced an approach for\nhuman-interpretable clustering of short text using large language models. Specifically, this work adopts the framework\nfound in the earlier work for text embedding and clustering, applying it to analyze the movement of Twitter user\nbiographies (bios) between clusters as the number of clusters increased from 1 to 20.\nOur analysis involves clustering bios using Gaussian Mixture Models (GMM), generating cluster names with the\nGoogle Gemini language model, extracting significant keywords for each cluster, and visualizing the transitions using\na Sankey diagram. By leveraging the foundation established in [30], we extend the methodology to explore how\nclusters evolve as the number of components increases, providing new insights into the interpretability of clustering\nresults.\nClustering\nTwitter bios were collected and preprocessed by converting emojis into text descriptions using the emoji library [31].\nEmbeddings for each bio were created using MiniLM [32], representing the bios in a high-dimensional vector space\nsuitable for clustering.\nClustering was performed using Gaussian Mixture Models (GMM) from the scikit-learn library. The number of\nclusters k was varied from 1 to 20 to observe how cluster assignments changed with increasing cluster numbers. For\neach value of k, a GMM was fitted with diagonal covariance, a maximum of 2000 iterations, and a random state of 0\nto ensure reproducibility. The GMM assigned each bio to one of the k clusters based on the embeddings, resulting in\na mapping of bios to clusters across different values of k.\nCluster Naming\nTo generate descriptive names for each cluster, the Google Gemini language model was utilized. For each cluster at\neach value of k, a prompt was created that included the top 10 most frequent words in the cluster and a random sample\nof 20 bios from that cluster. The prompt was structured as follows:\n“Create a name for the following cluster of Twitter bios. It has the following top 10 most frequent\nwords:\nword1, word2, word3, ..., word10\nAnd this is a random sample of Twitter bios from the cluster:\n1. Bio1\n2. Bio2\n3. Bio3\n. . .\n20. Bio20”\nThe prompt was sent to the Google Gemini model via its API, requesting a concise and descriptive name for the cluster.\nTo handle potential non-unique names (as the model might generate the same name for different clusters), uniqueness\nwas ensured by appending numbers to duplicate names. This method allowed differentiation of clusters with identical\nnames without altering their appearance in the visualisation.\nCluster Visualisation\nA Sankey diagram was constructed using the Plotly Python library to visualize the movement of bios between clusters\nas the number of clusters increased from 1 to 20. For each bio, cluster assignments across values of k from 1 to 20\nwere tracked. Edges between clusters at consecutive values of k (k and k + 1) were created based on how many bios\ntransitioned between a cluster at k and k + 1. Edges representing fewer than 150 bios were filtered out to focus on\nsignificant transitions. Each node in the diagram represented a cluster at a specific value of k, labeled with the unique\ncluster names generated by the Google Gemini model. Nodes were colored based on cluster stability (see Eq. (4) for\ndetails as to how stability is calculated).\nStability Assessment\nThe first stability assessment aimed to determine the stability of the clustering algorithm to the selection of dimensions\nwithin the embedding space. The goal of this assessment was to identify if any dimensions in MiniLM are essential,\n5\n\nA PREPRINT - FEBRUARY 25, 2025\nor if the loss of dimensions impacted the clustering. The embeddings were based on the MiniLM model, which\nprovides 384-dimensional representations of the bios. For this analysis, an 80% random subsample of the dimensions\n(approximately 307 dimensions) was selected without replacement from the full set of 384 dimensions.\nClustering was performed on these reduced-dimensionality embeddings using the same Gaussian Mixture Model\n(GMM) configuration as described previously, varying the number of clusters k from 1 to 20. The Adjusted Mu-\ntual Information (AMI) score was calculated between the clusters obtained from the subsampled dimensions and the\noriginal clusters derived from the full set of dimensions. The AMI is a measure of the agreement between two cluster-\nings, adjusted for chance, with a value of 1 indicating perfect agreement and a value close to 0 indicating agreement\nno better than random.\nThis resampling and clustering process was repeated 100 times, each time selecting a different random 80% subset of\ndimensions. The distribution of AMI scores across the iterations provided an indication of the stability of the clustering\nresults with respect to variations in the feature space.\nThe second assessment focused on the robustness of the clustering results to variations in the data sample. An 80%\nrandom subsample of the bios was selected without replacement from the full dataset. Clustering was performed on\nthis subset using the full 384-dimensional embeddings and the same GMM configuration.\nThe AMI score was calculated between the clusters obtained from the 80% bios sample and the clusters obtained from\nthe full dataset. This process was repeated 100 times with different random samples of bios. The resulting distribution\nof AMI scores indicated the extent to which the clustering results were consistent across different samples of the data.\nThe third assessment investigated the effect of random initialisation on the clustering results. The GMM clustering\nalgorithm involves stochastic elements that can lead to different results depending on the random seed used for ini-\ntialization. Clustering was performed using different random seeds, specifically seeds ranging from 1 to 100, while\nkeeping all other parameters constant.\nFor each seed, the AMI score was calculated between the clusters obtained with that seed and the clusters obtained\nwith the initial seed (seed = 0). Analyzing the AMI scores across different seeds provided insight into the sensitivity\nof the clustering outcomes to random initialization and whether certain cluster structures were consistently identified\nregardless of the seed.\nTo measure the stability accross clusters we introduce the measure called ”Proportional Stability” which is given by:\nProportional Stability = 1\nK\nK\nX\nk=1\nmax\nk′ |Ck ∩C′\nk′|\n|Ck|\n(4)\nWhere:\n• K is the number of clusters in the current model.\n• Ck is the set of data points in cluster k of the current model.\n• C′\nk′ is the set of data points in cluster k′ of the previous model.\n• |Ck| denotes the number of data points in cluster Ck.\n• |Ck ∩C′\nk′| is the number of data points common to both Ck and C′\nk′.\n• max\nk′ |Ck ∩C′\nk′| finds the previous cluster C′\nk′ that contributes the maximum number of data points to Ck.\n4\nResults\nIn the following results, we first examine the single-level stability of the Gaussian Mixture Model (GMM)-based\nclustering of Twitter user bios across varying numbers of clusters (K). Single-level stability refers to the robustness of\nthe clustering solution at a fixed number of clusters. We then consider the multi-level stability, which focuses on how\ncluster structures evolve as K increases. To investigate this, we visualise how user bios transition between clusters\nat successive resolutions, and characterise these clusters using descriptive names generated by the Google Gemini\nlanguage model.\nSingle-Level Stability\nFigure 1 presents results on clustering stability, through the average Adjusted Mutual Information (AMI) score Eq. (3),\nunder three conditions: subsetting dimensions, subsetting data, and varying random seeds. The AMI remains relatively\n6\n\nA PREPRINT - FEBRUARY 25, 2025\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\nCluster Resolution\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAdjusted Mutual Information (AMI)\n80% Data\n80% Dimensions\nDifferent Seeds\nFigure 1: The average AMI (3) between the original clustering (seed = 0), and 100 iterations of clusters created using\nonly 80% of dimensions of the embedding space created by the MiniLM Language Model, 80% of the Data, and\ndifferent seeds. The error bars represent the standard deviation of the AMI.\nstable across all three measures. Although there is a minor fluctuation at three clusters, where stability dips slightly\nfor column and data resampling, this difference is within one standard deviation of other K values and is therefore not\nstatistically notable. Beyond 10 clusters, the AMI shows minimal fluctuation across all three metrics, indicating that\nthe clustering configuration is relatively robust at higher K values. From a practical standpoint, these results suggest\nno strongly unsuitable values of K based solely on stability. High stability means that, under the tested conditions,\nthe clusters remain structurally consistent even when the feature space or initialisation changes. The stability analysis\nthus provides a method to exclude certain K values, identifying clusters that are less sensitive to perturbations and\ntherefore more likely to represent genuine, consistent patterns in the data, rather than clusters arising from random\nnoise.\nMulti-Level Stability\nAlthough Figure 1 offers insight into the stability of each individual K, it does not reveal how clusters evolve as\nK changes. Multi-level stability examines whether increasing K produces new clusters that are entirely distinct, or\nwhether they emerge from the subdivision of existing clusters. Understanding this evolution is essential, as it allows\nanalysts to discern whether cluster structures remain coherent as granularity increases, or whether cluster identities\nshift substantially.\nFigure 2 shows the AMI between consecutive models as K increases, along with the average Proportional Stability—a\nmeasure of how closely each cluster at the new level corresponds to one or more clusters at the previous level. The\nAMI generally hovers around 0.7–0.8, indicating that new clusters typically form by splitting from existing ones,\nrather than representing entirely novel structures. Similarly, Proportional Stability remains above 0.8 for most K\nvalues, suggesting that clusters retain a substantial portion of their membership as the clustering resolution becomes\nfiner. Across all cluster transitions from K = 2 to K = 20 K=20, only 8 clusters exhibit a proportional stability below\n0.5. This indicates that the vast majority of clusters are primarily derived from a single dominant cluster at the previous\nlevel. It should be emphasised that when changing K the clustering is carried out without any information about the\nclustering results at the different K value. This implies that the overarching cluster solutions appear to remain similar\nacross consecutive values of K. However, it is important to examine individual clusters to fully understand their\n7\n\nA PREPRINT - FEBRUARY 25, 2025\n2-3\n3-4\n4-5\n5-6\n6-7\n7-8\n8-9\n9-10 10-11 11-12 12-13 13-14 14-15 15-16 16-17 17-18 18-19 19-20\nCluster Range (start-end)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAMI\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nProportional Stability\nFigure 2:\nThe AMI (3) (blue line, circles) and proportional stability (4) (green line, squares) are shown between\nsuccessive clustering levels, as K increases. The scattered green circles show the individual proportional stability\nresults for each cluster. A proportional stability close to 1 indicates that a cluster has largely a single parent. The red\ndashed line corresponds to a proportional stability of 0.5. Clusters sitting below this line are ‘new’ in that they are\ncombinations of clusters at the lower resolution.\nbehaviour, as some clusters may remain stable throughout, reflecting cohesive user communities, while others may\nfragment or recombine as K grows.\nFigure 3 provides a more granular view of multi-level stability. Initially, the clustering separates bios into broad\ncategories, such as political versus non-political. As K increases, new clusters generally emerge as subdivisions of\nexisting clusters rather than aggregations of disparate segments. This pattern suggests that some thematic groupings\nremain coherent even as the clustering resolution increases. These stable memberships are often reflected in the cluster\nnames generated by Gemini. For example, from K = 6 to K = 11, a stable set of clusters consistently include terms\nsuch as ”MAGA” and ”Patriot.” This consistency in both membership and linguistic characterisation underscores\nthe presence of cohesive user communities, possibly reflecting strongly aligned political identities. In contrast, certain\nclusters such as ”Soulful Truths Matter” show lower Proportional Stability (as low as 0.261) and fragment into multiple\nsmaller clusters at higher resolutions, showing that not all clusters are equally persistent as K changes. Nonetheless,\nsuch visualisations, when combined with expert or domain-specific insights, can guide the choice of appropriate K\nvalues for more detailed analysis. Increasing K inevitably raises cognitive load, as more clusters require greater\ninterpretative effort. Thus, selecting a lower K, when clusters of interest first appear, may be preferable when the goal\nis to derive meaningful insights without overcomplicating the analysis.\n5\nDiscussion\nThe stability metrics, including Adjusted Mutual Information (AMI) and Proportional Stability, consistently showed\nhigh values (generally above 0.7 and 0.8, respectively), reinforcing the observation that the clusters are robust to\nchanges in K. Notably, the AMI between successive clusterings remained relatively constant, suggesting that the\nfundamental structure of the data is being preserved as the number of clusters increases. There are, of course, a few\nexceptions within each resolution, where new clusters form from many other clusters, and likewise some clusters\nwill splinter into multiple other clusters with no clear successor at a higher resolution. However, on the whole, most\nclusters are able to trace their lineage through many levels of clustering.\nThis stability implies that the clustering algorithm is capturing intrinsic clusters in the data that are resilient to the\nchoice of K. This is somewhat surprising because, as shown in equation (1), the responsibilities γ(zij) that assign\n8\n\nA PREPRINT - FEBRUARY 25, 2025\nFigure 3: Shows the proportion of bios that can be found in a single cluster at the previous hierarchical clustering level\nas the number of clusters increases from 1 - 11. Names are created using Google Gemini and a sample of bios from\nthe cluster and the top words. The colour of each cluster is given by the Proportional Stability of the cluster. Where\nthe more yellow (lighter) a cluster is the greater proportion of it came from a cluster at a previous resolution. Whereas\nthe more blue (darker) a cluster is, the more the cluster is made up of a mix of different clusters at a lower resolution.\ndata points to clusters depend on K through the summation over all k components in the denominator. Changing K\nalters the number of Gaussian components and their parameters πj, µj, and Σj, which are re-estimated during the EM\nalgorithm. Since these parameters can vary significantly with different values of K, there is no guarantee that similar\nclusters will appear when K changes.\nAn interesting observation was that the AMI for all three stability metrics remained fairly constant at higher values of\nK (Figure 2). This may be a result of how, as K increases, the relative percentage increase in the number of clusters\nbecomes smaller, leading to less drastic changes between clusterings.\nThe stability assessments with respect to random initialisations, resampling data, and resampling dimensions demon-\nstrated that the clustering results are robust to these perturbations (Figure 1). The AMI scores remained high across\ndifferent seeds, subsets of data, and subsets of dimensions, indicating that the clustering algorithm consistently identi-\nfies similar structures in the data. This robustness is essential for practical applications, as it suggests that the clustering\nresults are reliable and not unduly influenced by random factors.\nNevertheless, several limitations should be acknowledged. First, the study relied on a single clustering algorithm\n(GMM), and while it is well-suited for data that may be modelled as a mixture of Gaussians, other algorithms may cap-\nture different aspects of the data structure. Second, the embeddings were generated using a specific model (MiniLM),\nand different embedding techniques may yield different results. Third, the use of the Google Gemini language model\nfor cluster naming introduces potential biases and may not always produce accurate or unique names, as the model’s\noutputs are influenced by its training data and prompt design. Additionally, this method was only tested on one dataset.\nThe stability observed may be a feature of this particular dataset rather than the method itself. Future testing on other\ndatasets is necessary to ensure reliability.\n6\nConclusion\nThis study assessed the stability of clustering short-text data using a combination of Gaussian Mixture Models (GMM)\nand embeddings generated by a large language model (LLM). Specifically, we examined how clustering results evolve\ninternally and across different values of K. Our findings demonstrate that clustering is stable across resolutions, with\nclusters primarily subdividing rather than reorganizing as K increases. This pattern was visualized in the Sankey\n9\n\nA PREPRINT - FEBRUARY 25, 2025\ndiagram (Figure 3), where most data points remained within the same cluster lineage, underscoring that the algorithm\ncaptures intrinsic structures that are robust to changes in K.\nThese results challenge the traditional focus on identifying a single “optimal” number of clusters. Instead, they advo-\ncate for examining the interpretability and utility of clusters across multiple resolutions. Visualisations such as Sankey\ndiagrams allow users to explore the relationships and transitions between clusters, enabling decisions informed by\npractical utility rather than abstract metrics.\nFuture work should expand on this approach by applying it to different datasets and clustering methods to assess\ngeneralisability. Incorporating human evaluations to assess the semantic coherence and usability of clusters would also\nprovide valuable insights. Ultimately, this study underscores the importance of prioritizing user-centered clustering\nsolutions that balance stability, interpretability, and visual accessibility, moving the focus from what is “optimal” to\nwhat is most useful.\nReferences\n[1] I. V. Mechelen, A.-L. Boulesteix, R. Dangl, N. Dean, C. Hennig, F. Leisch, D. Steinley, and M. J. Warrens, “A\nwhite paper on good research practices in benchmarking: The case of cluster analysis,” Wiley Interdisciplinary\nReviews:\nData Mining and Knowledge Discovery, vol. 13, no. 6, p. e1511, 2023. [Online]. Available:\nhttps://doi.org/10.1002/widm.1511\n[2] E. Rosch,\n“Principles of categorization,”\nin Cognition and Categorization,\nE. Rosch and B. B.\nLloyd,\nEds.\nHillsdale,\nNJ: Lawrence Erlbaum Associates,\n1978,\npp. 27–48. [Online]. Available:\nhttps://escholarship.org/uc/item/0sz9c8qh\n[3] C. Hennig, “Cluster validation by measurement of clustering characteristics relevant to the user,” in Data Analysis\nand Applications 1: Clustering and Regression, Modeling–Estimating, Forecasting and Data Mining, C. Gatu,\nE. Diday, and G. Saporta, Eds.\nHoboken, NJ: John Wiley & Sons, 2019, vol. 2, pp. 1–24.\n[4] S. E. Akhanli and C. Hennig, “Comparing clusterings and numbers of clusters by aggregation of calibrated\nclustering validity indexes,” Statistics and Computing, vol. 30, no. 5, pp. 1523–1544, 2020. [Online]. Available:\nhttps://doi.org/10.1007/s11222-020-09958-2\n[5] J. Grimmer, M. E. Roberts, and B. M. Stewart, “Machine learning for social science: An agnostic approach,”\nAnnual Review of Political Science, vol. 24, pp. 395–419, 2021.\n[6] W. Pedrycz, “Fuzzy clustering with a knowledge-based guidance,” Pattern Recognition Letters, vol. 25, no. 4,\npp. 469–480, 2004. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0167865503002770\n[7] P. Lakhawat, M. Mishra, and A. Somani, “A novel clustering algorithm to capture utility information\nin transactional data,” in Proceedings of the 8th International Joint Conference on Knowledge Discovery,\nKnowledge Engineering and Knowledge Management.\nSCITEPRESS - Science and Technology Publications,\n2016, pp. 456–462. [Online]. Available:\nhttp://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/\n0006092104560462\n[8] J. M. Kraus,\nC. M¨ussel,\nG. Palm,\nand H. A. Kestler,\n“Multi-objective selection for collecting\ncluster alternatives,” Computational Statistics, vol. 26, no. 2, pp. 341–353, 2011. [Online]. Available:\nhttps://doi.org/10.1007/s00180-011-0244-6\n[9] R. Krishnapuram and J. Keller, “A possibilistic approach to clustering,” IEEE Transactions on Fuzzy Systems,\nvol. 1, no. 2, pp. 98–110, 1993. [Online]. Available: https://ieeexplore.ieee.org/document/227387\n[10] K. Stevens, P. Kegelmeyer, D. Andrzejewski, and D. Buttler, “Exploring topic coherence over many models\nand many topics,” in Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language\nProcessing and Computational Natural Language Learning.\nJeju Island, Korea: Association for Computational\nLinguistics, 2012, pp. 952–961. [Online]. Available: https://aclanthology.org/D12-1087\n[11] P. J. Rousseeuw, “Silhouettes:\nA graphical aid to the interpretation and validation of cluster analysis,”\nJournal of Computational and Applied Mathematics, vol. 20, pp. 53–65, 1987. [Online]. Available:\nhttps://www.sciencedirect.com/science/article/pii/0377042787901257\n[12] M. Ahmed,\nR. Seraj,\nand S. M. S. Islam,\n“The k-means algorithm:\nA comprehensive survey\nand performance evaluation,” Electronics, vol. 9, no. 8, p. 1295, 2020. [Online]. Available:\nhttps:\n//www.mdpi.com/2079-9292/9/8/1295\n[13] R. Hosseini and S. Sra, “An alternative to em for gaussian mixture models:\nBatch and stochastic\nriemannian optimization,” Mathematical Programming, vol. 181, no. 1, pp. 187–223, 2020. [Online]. Available:\nhttps://doi.org/10.1007/s10107-019-01381-4\n10\n\nA PREPRINT - FEBRUARY 25, 2025\n[14] V. Y. Kiselev, T. S. Andrews, and M. Hemberg, “Challenges in unsupervised clustering of single-cell\nrna-seq data,”\nNature Reviews Genetics,\nvol. 20,\nno. 5,\npp. 273–282,\n2019. [Online]. Available:\nhttps://www.nature.com/articles/s41576-018-0088-9\n[15] J. Handl, J. Knowles, and D. B. Kell, “Computational cluster validation in post-genomic data analysis,”\nBioinformatics,\nvol. 21,\nno. 15,\npp. 3201–3212,\n2005. [Online]. Available:\nhttps://doi.org/10.1093/\nbioinformatics/bti517\n[16] T. Ronan, Z. Qi, and K. M. Naegle, “Avoiding common pitfalls when clustering biological data,” Science\nSignaling, vol. 9, no. 432, pp. re6–re6, 2016. [Online]. Available:\nhttps://www.science.org/doi/10.1126/\nscisignal.aad1932\n[17] L. Yu, Y. Cao, J. Y. H. Yang, and P. Yang, “Benchmarking clustering algorithms on estimating the number of cell\ntypes from single-cell rna-sequencing data,” Genome Biology, vol. 23, no. 1, p. 49, 2022.\n[18] R. Qi,\nA. Ma,\nQ. Ma,\nand Q. Zou,\n“Clustering and classification methods for single-cell rna-\nsequencing data,” Briefings in Bioinformatics, vol. 21, no. 4, pp. 1196–1208, 2020. [Online]. Available:\nhttps://doi.org/10.1093/bib/bbz062\n[19] F. Morstatter and H. Liu, “In search of coherence and consensus: Measuring the interpretability of statistical\ntopics,” Journal of Machine Learning Research, vol. 18, no. 169, pp. 1–32, 2018. [Online]. Available:\nhttp://jmlr.org/papers/v18/17-069.html\n[20] L. I. Kuncheva and D. P. Vetrov, “Evaluation of stability of k-means cluster ensembles with respect to random\ninitialization,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 28, no. 11, pp. 1798–1808,\n2006.\n[21] E. Lord, M. Willems, F.-J. Lapointe, and V. Makarenkov, “Using the stability of objects to determine the number\nof clusters in datasets,” Information Sciences, vol. 393, pp. 29–46, 2017.\n[22] (2024) 2.1. gaussian mixture models. scikit-learn. [Online]. Available:\nhttps://scikit-learn/stable/modules/\nmixture.html\n[23] Massachusetts Institute of Technology, “Algorithmic aspects of machine learning: Chapter 6 notes,” 2015.\n[Online]. Available: https://ocw.mit.edu/courses/18-409-algorithmic-aspects-of-machine-learning-spring-2015/\ne339520c4069ca5e785b29a3c604470e MIT18 409S15 chapp6.pdf\n[24] X. Li, J. Zhou, and H. Wang, “Gaussian mixture models with rare events,” Journal of Machine Learning\nResearch, vol. 25, no. 252, pp. 1–40, 2024. [Online]. Available: http://jmlr.org/papers/v25/23-1245.html\n[25] S.\nMukherjee,\n“Lecture\nnotes\non\nexpectation-maximization\nalgorithm,”\n2018.\n[Online].\nAvailable:\nhttp://www2.stat.duke.edu/∼sayan/Sta613/2018/lec/emnotes.pdf\n[26] N. X. Vinh, J. Epps, and J. Bailey, “Information theoretic measures for clusterings comparison: Variants,\nproperties, normalization and correction for chance,” Journal of Machine Learning Research, vol. 11, no. 95,\npp. 2837–2854, 2010. [Online]. Available: http://jmlr.org/papers/v11/vinh10a.html\n[27] (2024) Adjusted mutual info score. scikit-learn. [Online]. Available:\nhttps://scikit-learn/stable/modules/\ngenerated/sklearn.metrics.adjusted mutual info score.html\n[28] L. Zappia and A. Oshlack, “Clustering trees: A visualization for evaluating clusterings at multiple resolutions,”\nGigaScience, vol. 7, no. 7, p. giy083, 2018.\n[29] M. Peng, B. Wamsley, A. G. Elkins, D. H. Geschwind, Y. Wei, and K. Roeder, “Cell type hierarchy\nreconstruction via reconciliation of multi-resolution cluster tree,” Nucleic Acids Research, vol. 49, no. 16, p.\ne91, 2021. [Online]. Available: https://doi.org/10.1093/nar/gkab481\n[30] J. K. Miller and T. J. Alexander, “Human-interpretable clustering of short text using large language models,”\nRoyal Society Open Science, vol. 12, p. 241692, 2025.\n[31] T. Kim, K. Wurster, and T. Jalilov, “emoji: Emoji for python,” 2022, version 2.0.0. [Online]. Available:\nhttps://pypi.org/project/emoji/\n[32] W. Wang,\nF. Wei,\nL. Dong,\nH. Bao,\nN. Yang,\nand M. Zhou,\n“Minilm:\nDeep self-attention\ndistillation for task-agnostic compression of pre-trained transformers,”\nin Proceedings of the 34th\nInternational Conference on Neural Information Processing Systems (NeurIPS).\nCurran Associates\nInc., 2020, pp. 5776–5788. [Online]. Available:\nhttps://proceedings.neurips.cc/paper files/paper/2020/file/\n3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n11\n",
  "metadata": {
    "source_path": "papers/arxiv/Moving_Past_Single_Metrics_Exploring_Short-Text_Clustering_Across\n__Multiple_Resolutions_61dcc76bae2c00f9.pdf",
    "content_hash": "61dcc76bae2c00f929a99f7e248355e2513e6ec73acd555fd4d415280b2f27fc",
    "arxiv_id": null,
    "title": "Moving_Past_Single_Metrics_Exploring_Short-Text_Clustering_Across\n__Multiple_Resolutions_61dcc76bae2c00f9",
    "author": "",
    "creation_date": "D:20250225023840Z",
    "published": "2025-02-25T02:38:40",
    "pages": 11,
    "size": 464907,
    "file_mtime": 1740470194.1077178
  }
}