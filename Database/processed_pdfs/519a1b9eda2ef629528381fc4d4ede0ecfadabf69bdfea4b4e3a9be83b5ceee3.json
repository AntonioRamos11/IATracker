{
  "text": "SFLD: Reducing the content bias for AI-generated Image Detection\nSeoyeon Gye*\nJunwon Ko*\nHyounguk Shon*\nMinchan Kwon\nJunmo Kim\nSchool of Electrical Engineering, KAIST, South Korea\n{sawyun, kojunewon, hyounguk.shon, kmc0207, junmo.kim}@kaist.ac.kr\nAbstract\nIdentifying AI-generated content is critical for the safe\nand ethical use of generative AI. Recent research has fo-\ncused on developing detectors that generalize to unknown\ngenerators, with popular methods relying either on high-\nlevel features or low-level fingerprints. However, these\nmethods have clear limitations: biased towards unseen con-\ntent, or vulnerable to common image degradations, such as\nJPEG compression. To address these issues, we propose\na novel approach, SFLD, which incorporates PatchShuf-\nfle to integrate high-level semantic and low-level textural\ninformation. SFLD applies PatchShuffle at multiple lev-\nels, improving robustness and generalization across various\ngenerative models. Additionally, current benchmarks face\nchallenges such as low image quality, insufficient content\npreservation, and limited class diversity. In response, we in-\ntroduce TwinSynths, a new benchmark generation method-\nology that constructs visually near-identical pairs of real\nand synthetic images to ensure high quality and content\npreservation. Our extensive experiments and analysis show\nthat SFLD outperforms existing methods on detecting a\nwide variety of fake images sourced from GANs, diffusion\nmodels, and TwinSynths, demonstrating the state-of-the-art\nperformance and generalization capabilities to novel gener-\native models. The TwinSynths dataset is publicly available\nat https://huggingface.co/datasets/koooooooook/\nTwinSynths.\n1. Introduction\nThe rapid advancement of AI image generation tech-\nnologies has brought significant achievements but also\ngrowing social concern, as these technologies are increas-\ningly misused for the creation of fake news, malicious\ndefamation, and other forms of digital deception. In re-\nsponse, AI-generated image detection is receiving more at-\ntention. There is a wide variety of generative models, along\nwith commercial models with unknown internal architec-\n*Equal contribution.\nBedroom\n(Unseen)\nCar\n(Seen)\nCat\n(Seen)\n0\n20\n40\n60\n80\n100\nAccuracy\nreal_acc\nfake_acc\n(a) UnivFD [32]\nBedroom\n(Unseen)\nCar\n(Seen)\nCat\n(Seen)\n0\n20\n40\n60\n80\n100\nAccuracy\nreal_acc\nfake_acc\n(b) SFLD (ours)\nFigure 1. Class-wise detection accuracy for StyleGAN-{bedroom,\ncar, cat} class categories. The bedroom class does not appear at\ntraining, while car and cat does. UnivFD [32] catastrophically\nfails to identify synthetic bedroom images, which hints at model\nbias towards high-level image content.\ntures. This highlights the need for a generalized detector\ncapable of distinguishing between real and fake images, re-\ngardless of the generative model structure.\nIn this context, early research focused on identifying\nthe characteristic fingerprints of generated images. Recent\nwork, NPR [46] shows that pixel-level features, induced by\nthe upsampling layers commonly found in current genera-\ntive models, can serve as cues for detection. However, there\nare clear practical limitations to relying on low-level fin-\ngerprints. First, the approach is vulnerable to simple image\ndegradations, such as JPEG compression or blurring, which\nare common in real-world online environments [49]. Addi-\ntionally, the model may become biased toward the specific\nfakeness seen at training in cases where generalization to\nnovel generators is not sufficiently considered [32,56]. For\ninstance, a detector trained on GAN-generated images may\nlearn the characteristics of GANs as the fake features, while\nmistakenly perceiving images generated by diffusion mod-\nels as real. This bias limits the detector’s generalizability\nacross different types of generative models.\nTo tackle these limitations, UnivFD [32] utilizes a robust,\npre-trained image encoder. This image embedding is task-\nagnostic, enabling it to capture high-level semantic informa-\ntion from images. However, we found that UnivFD exhibits\na bias towards the observed content in the training images,\narXiv:2502.17105v1  [cs.CV]  24 Feb 2025\n\nLSUN-cat (Real)\nDiffusion (Fake)\nProGAN (Fake)\n(a) Examples of the conventional benchmark.\nImageNet (Real)\nDDIM (Fake)\nDCGAN (Fake)\n(b) Examples of proposed TwinSynths benchmark.\nFigure 2. Comparison of benchmarks. (a) Real images and fake GAN images are sampled from the test ProGAN set in the ForenSynths [49].\nFake diffusion images are sampled from benchmark of Ojha et al. [32], each from LDM, GLIDE and DALL-E dataset. (b) Real images are\nsampled from ImageNet dataset, and corresponding fake images are generated by each model.\nlearning another specific fakeness. Fig. 1 shows that Uni-\nvFD misclassifies most GAN-generated images of a novel\nclass (StyleGAN-bedroom) as real. The bedroom class is\nabsent from the training set, which may lead the detector to\nmistakenly classify most images as real, demonstrating the\ndetector’s reliance on seen content during training.\nWe propose a novel technique called PatchShuffle,\nwhich is the core of our fake image detection model, SFLD\n(pronounced “shuffled”). PatchShuffle divides the image\ninto non-overlapping patches and randomly shuffles them.\nThis procedure disrupts the high-level semantic structure of\nthe image while preserving low-level textural information.\nThis allows the detection model to better focus on both con-\ntext and texture. SFLD utilizes an ensemble of classifiers at\nmultiple levels of PatchShuffle, leveraging hierarchical in-\nformation across various patch sizes. This approach ensures\nthat the model leverages both the semantic and textural as-\npects of the image to improve fake image detection. The re-\nsults demonstrate that SFLD achieves superior performance\nwith enhanced robustness and better generalization.\nFurthermore, we observe that previous benchmarks have\nthree limitations: (1) low image quality. The previous\nbenchmarks contain a significant portion of low-quality im-\nages that lag behind the capabilities of current generative\nmodels. As a result, the practical usefulness of these bench-\nmarks is significantly reduced. (2) lack of content preser-\nvation. Some subsets—particularly foundation generative\nmodels—lack access to the training data used for the check-\npoints. Consequently, the content of the generated and real\nimages often differs significantly, making it difficult to de-\ntermine whether a detector focuses on real/fake discrimina-\ntive features or other irrelevant features. (3) limited class\ndiversity. Existing benchmarks primarily focus on expand-\ning the variety of generative models without considering the\ngenerated class diversity and scalability among generative\nmodels. As shown in Fig. 1, this makes it difficult to iden-\ntify detection bias towards certain classes, as well as hard to\nrepresent the in-the-wild performance of the detector due to\nlimited class diversity.\nTo address these challenges, we propose a new bench-\nmark generation methodology and corresponding bench-\nmark, TwinSynths. It consists of synthetic images that are\nvisually near-identical to paired real images for practical\nand fair evaluations. TwinSynths constructs image pairs that\npreserve both quality and content while retaining the ar-\nchitectural characteristics of each generative model. Also,\nTwinSynths enables flexible class expansion by generat-\ning synthetic images tailored to the real image. Using this\nbenchmark, we evaluate the performance of our proposed\nSFLD method as well as existing detection models.\nOur main contributions are summarized as follows:\n• We propose SFLD, a novel AI-generated image detection\nmethod that integrates semantic and texture artifacts on\ngenerated images, achieving state-of-the-art performance.\n• We propose a new approach on benchmarks and the sub-\nset of generated images that can ensure the quality and\ncontent of generated images.\n• We validate our method through extensive experiments\nand analysis that support our hypothesis.\n2. Method\n2.1. Patch Shuffling Fake Detection\nBackbone. We utilize the visual encoder of CLIP ViT-\nL/14 [13, 36] to leverage the pre-trained feature space.\n\nPatchify\nCLIP\nVisual\nEncoder\nEmbedding\nFC\nLayer\nFC\nLayer\n...\n...\nShuffle\nFC\nLayer\nClassifiers\npatch size =\npatch size =\npatch size =\nFigure 3. Architecture of the proposed fake image detector\n(SFLD). zsi refers to the logit score generated from an input im-\nage processed via si×si patch size. Σ indicates weighted sum.\nThis choice is based on Ojha et al. [32], which showed\nthat it outperforms other models such as CLIP:ResNet-\n50, ImageNet:ResNet-50, and ImageNet:ViT-B/16 in distin-\nguishing real from fake images. The results indicated that\nboth the architecture and the pre-training data are crucial.\nBased on this insight, we chose the ViT model for our back-\nbone. As shown in Fig. 3, we extract CLIP features and train\na fully connected layer to classify real and fake images.\nPatchShuffle. To effectively integrate both semantic and\ntextural features, PatchShuffle disrupts the global struc-\nture of an image while preserving local features. In the\nPatchShuffle process, the input images are divided into non-\noverlapping patches of size s × s and then randomly shuf-\nfled. This operation produces a new shuffled image xs.\nFor a given s, the logit score of the shuffled image is,\n  z _{s} = \\ psi (f(x_s)) \\,, \n(1)\nwhere f(·) represents a pre-trained CLIP encoder and ψ(·)\nis a single fully connected layer appended to f.\nThere are classifiers for each patch size of shuffled im-\nages to leverage local structure information hierarchically\nwithin the image. We selected patch sizes of 28, 56, and\n224 for the proposed SFLD. As shown in Fig. 3, s0 is 224,\ns1 is 56 and s2 is 28. These configurations are studied in\ndetail in Sec. 4.5. For each patch size sj, the classifier ψsj\nis trained independently. Notably, UnivFD takes a center-\ncropped 224×224 image as input to the CLIP encoder.\nTherefore, when using a patch size of 224 in PatchShuffle, it\neffectively corresponds to the same setting as UnivFD [32].\nWe employ binary cross-entropy loss for each classifier:\n  \\ m\na\nt\nh\ncal\n \n{L } =  -\\fra c  { 1 }{N } \\\ns\nu m  _{i=1\n}^{N} \\left [ y_i \\log \\sigma (z_{s_j}) + (1 - y_i) \\log \\left (1 - \\sigma (z_{s_j}) \\right ) \\right ] \n(2)\nwhere N is the number of data and yi ∈{0, 1} is the label\nwhether an input xi is real (yi = 0) or fake (yi = 1).\nSFLD. SFLD combines multiple classifiers trained on\nshuffled images with different patch sizes. By varying the\npatch size, SFLD incorporates models that focus on various\nlevels of structural features, ranging from fine-grained local\ndetails to more global patterns.\nDuring testing, Nviews = 10 shuffled views are gener-\nated for each patch size. The logits from these views are av-\neraged and processed by the corresponding classifier. The\nfinal probability PSF LD(y|x) is computed by averaging the\nlogits across patch sizes and applying the sigmoid function:\n  P_{\\text  {\nS\nF\nL\nD\n}\n}(y\n|x)\n \n=\n \\sigm\na \\lef\nt\n (\\\nfrac\n {1\n}{\nk} \\sum _{j=1}^{k} \\psi _{s_j}\\left (\\frac {1}{N_{\\text {views}}} \\sum _{i=1}^{N_{\\text {views}}} f(x_{s_j}^i)\\right )\\right ) \\,, \n(3)\nwhere k is the number of patch sizes used in the ensemble\n(e.g., k = 3 in our configuration).\nBinary classification is done using a threshold of 0.5\non PSF LD. Although the fusion method is simple and not\ntuned for each test class, its simplicity enables strong gener-\nalization across diverse fake image sources. By combining\nclassifiers trained on different patch sizes, SFLD achieves\na robust and general detection performance. Algorithm 1\nshows the full workflow of SFLD, especially the fusion of\nmultiple classifiers during inference.\n2.2. TwinSynths\nIn Sec. 1, we pointed out three shortcomings in the previ-\nous benchmarks: low image quality, lack of content preser-\nvation, and limited class diversity. This issue must be ad-\ndressed to allow a comprehensive comparison of detectors.\nTherefore, we propose a novel dataset creation method-\nology and TwinSynths benchmark, consisting of GAN-\nand diffusion-based generated images that are paired with\nvisually-identical real counterparts. To create a practical\nbenchmark for evaluating generated image detectors, it is\nessential to ensure the generation of high-quality images\nthat preserve the original content. To achieve this, the image\ngeneration process should ideally sample a distribution that\nclosely resembles a real distribution. From this perspective,\nthe image generation or sampling process can be interpreted\nas effectively fitting the generator to a single real image.\nThrough this approach, we construct image pairs that pre-\nserve the content of the images while reflecting the archi-\ntectural traits of the generative models. Additionally, this\nmethodology allows for the expansion of target classes in\nthe benchmark by generating paired images for any real im-\nage. Fig. 2b are some examples of TwinSynths. We can see\nthat the content of the paired real image is faithfully repro-\nduced and the quality of the generated image is guaranteed.\nTwinSynths-GAN benchmark. The GAN-based sub-\nsets in the previous benchmark have disparate training con-\nfigurations, especially the class of training images, resulting\nin a discrepancy between the generated and the real images.\n\nIn order to generate a high quality image that preserves the\ncontent of the paired real image while leveraging the train-\ning methodology of GANs, we trained the generator from\nscratch using a single real image. The MSE loss was pro-\nvided to the generator to generate an image that is identi-\ncal to the original image. For reproduction, the latent vec-\ntor for the generator input is maintained at a fixed value.\nWe created 8,000 generated images from 80 selected Im-\nageNet [41] classes, which is much larger than previous\nbenchmarks. We selected 40 classes following the ProGAN\nsubset in ForenSynths [49], while the other 40 classes were\nchosen arbitrarily. We utilized DCGAN [35] architecture.\nTwinSynths-DM benchmark. In comparison to GAN-\nbased subsets, diffusion-based subsets in conventional\nbenchmarks were generated with off-the-shelf pretrained\nmodels, having much severer content discrepancy between\nreal and generated images. In order to generate a high qual-\nity image that preserves the content of paired real image\nwhile leveraging the inference process of diffusion mod-\nels, we used DDIM inversion [44] to generate image that\nis similar to the real image. We apply a DDIM forward\nprocess to the real image to make it noisy and perform\ntext-conditioned DDIM denoising process using the prompt\ntemplate ‘a photo of {class name}’. For the prompts, we\nused the class names from ImageNet. This process makes\nTwinSynths-DM preserve the similarity with the paired real\nimages. We used the same image classes used to create\nTwinSynths-GAN. We utilized the pretrained decoder and\nscheduler of [44].\n3. Experiments\n3.1. Settings\nDatasets. Following the conventions of AI-generated\nimage detection, all detectors were trained using the Foren-\nSynths train set [49]. This train set consists of real images\nused to train ProGAN [18] and ProGAN-generated images.\nWe evaluate the performance of SFLD on several bench-\nmarks, including conventional benchmarks, TwinSynths,\nand low-level vision/perceptual loss benchmarks. For more\ndetailed descriptions of the datasets and configurations\nused, please refer to Appendix B.\nBaseline methods. We compare the performance of the\nproposed SFLD with existing AI-generated image detec-\ntion methods. It includes CNNSpot [49], FreDect [14],\nGramNet [25], Fusing [17], LNP [23], LGrad [45], Uni-\nvFD [32], and NPR [46]. We conducted evaluations on the\ndetection methods with our test dataset. The evaluation is\ndone by the official models [32, 49], re-implemented mod-\nels [14,17,23,25,45] by Zhong et al. [54], or trained model\nwith the official codes using 20-classes train set [46].\nEvaluation metrics. We assess the performances of\nthe detection models by average precision score (AP) and\nclassification accuracy (Acc.), following previous works\n[32,46,49]. The AP metric is not dependent on the threshold\nvalue, whereas the Acc. is calculated with a fixed threshold\nof 0.5 across all generation models.\n3.2. Results on Conventional Benchmark\nTabs. 1 and 2 shows the detection performance on\nconventional benchmarks in AP and Acc. All baselines\nare trained on only the ProGAN train dataset consist-\ning of 20 classes. Higher performance is colored darker.\nSFLD demonstrates robust and generalized performance\nacross various generators in the benchmark. Note that\nSFLD achieves above 90.0 AP on every unseen genera-\ntor. SFLD has an average of 98.43 AP, outperforming the\nbest-performing baseline, UnivFD, by up to 2.14 in aver-\nage. While for some tasks NPR has shown outperforming\nAP values in some generators, it has shown relatively low\nperformance on some generators. In this regard, we found\nthat NPR is sensitive to some image degradation or differ-\nent post-processing methods in different generative models,\nwhich limits its practicality. Refer to Sec. 4.3 for further\ncomparison of robustness on image degradation.\nSFLD also exhibits state-of-the-art performance in clas-\nsification accuracy. It performs particularly well on chal-\nlenging datasets like DeepFake and ADM. On DeepFake,\nit improves accuracy from 74.6% to 84.2% (+9.6), and on\nADM, from 79.5% to 86.0% (+6.5). These gains highlight\nits superior generalization in difficult scenarios.\n3.3. Analysis on TwinSynths\nTab. 3 illustrates the detection performance on Twin-\nSynths in AP. The results demonstrate that SFLD is effec-\ntive in TwinSynths while some detectors have shown a sig-\nnificant drop in performance. Note that the TwinSynths fo-\ncused on three key aspects: image quality, content preserva-\ntion, and class diversity. This suggests that the high perfor-\nmance on conventional benchmarks may not guarantee the\ndetector’s performance in real-world scenarios.\nThe results of TwinSynths allow an indirect analysis of\nthe factors that the detectors focus on. For convenience, we\nnow define high-level features and low-level features. high-\nlevel features are semantic information and their artifacts\noriginate from distribution disparity between real images\nand generated images. low-level features are texture infor-\nmation and their artifacts stem from the generator traces and\nimage quality of generated images. The TwinSynths-GAN\npreserves the content of the real image with minimal alter-\nation, as the images are generated from a single real im-\nage. This results in UnivFD, which captures high-level fea-\nture artifacts on the entire image, resulting in poor perfor-\nmance on the TwinSynths-GAN subset. In contrast, NPR,\nwhich captures high-frequency artifacts in neighboring pix-\nels, demonstrates better performance than UnivFD on the\n\nMethod\nPro\nGAN\nStyle\nGAN\nStyle\nGAN2\nBig\nGAN\nCycle\nGAN\nStar\nGAN\nGau\nGAN\nDeep\nfake\nDALL\nE\nGlide\n100_10\nGlide\n100_27\nGlide\n50_27 ADM LDM\n100\nLDM\n200\nLDM\n200_cfg\nAvg.\nCNNSpot [49]\n100\n99.8\n99.5\n86.0\n94.9\n99.0\n90.8\n84.5\n72.9\n82.5\n80.1\n84.7\n78.3\n71.5\n70.3\n73.6\n85.53\nFreDect [14]\n100\n96.3\n72.7\n93.9\n88.8\n99.4\n84.5\n71.9\n95.0\n52.2\n53.9\n55.0\n57.3\n93.1\n92.7\n90.4\n81.07\nGramNet [25]\n100\n88.2\n100\n62.7\n74.2\n100\n55.0\n93.5\n98.8\n99.7\n99.3\n99.1\n79.8\n99.8\n99.8\n99.8\n90.61\nFusing [17]\n100\n97.5\n100\n89.1\n95.5\n99.8\n87.7\n69.3\n77.1\n83.6\n81.3\n86.2\n82.6\n75.5\n76.2\n77.9\n86.20\nLNP [23]\n100\n92.5\n100\n90.2\n93.9\n100\n77.9\n73.7\n94.9\n92.1\n88.5\n89.5\n85.5\n93.9\n93.6\n93.7\n91.24\nLGrad [45]\n100\n84.2\n99.9\n87.9\n94.4\n100\n91.7\n64.3\n95.6\n97.1\n94.8\n96.3\n74.9\n96.3\n96.2\n96.5\n91.88\nUnivFD [32]\n100\n97.2\n98.0\n99.3\n99.8\n99.4\n100\n81.8\n97.7\n95.5\n95.8\n96.0\n88.3\n99.4\n99.4\n93.2\n96.29\nNPR [46]\n100\n99.4\n99.9\n87.4\n90.0\n100\n76.7\n82.7\n99.2\n100\n99.8\n99.9\n84.2\n100\n99.9\n99.9\n94.94\nSFLD (224+28)\n100\n99.8\n99.9\n99.9\n100\n100\n100\n91.5\n99.1\n96.7\n97.0\n97.5\n94.5\n99.3\n99.3\n94.2\n98.03\nSFLD (224+56)\n100\n99.8\n99.9\n99.8\n100\n100\n100\n90.9\n99.2\n98.2\n98.4\n98.7\n94.4\n99.6\n99.6\n95.8\n98.39\nSFLD\n100\n99.9\n99.9\n99.9\n100\n100\n100\n93.3\n99.3\n97.6\n97.9\n98.4\n95.4\n99.3\n99.3\n95.0\n98.43\nTable 1. Generalization performance on the conventional benchmark reported in AP. SFLD (224+28) indicates the ensemble of the classifier\nwith patch sizes 224 and 28. And SFLD indicates the ensemble of the three classifiers with patch sizes 224, 56, and 28.\nMethod\nPro\nGAN\nStyle\nGAN\nStyle\nGAN2\nBig\nGAN\nCycle\nGAN\nStar\nGAN\nGau\nGAN\nDeep\nfake\nDALL\nE\nGlide\n100_10\nGlide\n100_27\nGlide\n50_27 ADM LDM\n100\nLDM\n200\nLDM\n200_cfg\nAvg.\nCNNSpot [49]\n100\n90.2\n86.9\n71.2\n87.6\n94.6\n81.4\n50.7\n57.7\n62.4\n61.3\n64.4\n62.5\n54.9\n54.8\n56.0\n71.02\nFreDect [14]\n99.4\n80.3\n56.1\n82.7\n81.6\n94.5\n81.0\n62.5\n81.6\n49.7\n52.2\n53.4\n57.8\n79.3\n79.0\n76.7\n72.97\nGramNet [25]\n100\n50.8\n100\n67.9\n72.8\n100\n57.4\n62.0\n87.8\n95.6\n93.4\n91.8\n79.5\n98.7\n98.5\n98.5\n84.65\nFusing [17]\n100\n71.0\n97.1\n76.7\n85.7\n97.2\n76.1\n53.0\n56.1\n60.9\n59.7\n61.6\n62.4\n53.8\n54.5\n56.0\n70.10\nLNP [23]\n99.8\n78.1\n99.6\n81.1\n82.1\n99.9\n71.7\n56.1\n83.5\n80.3\n76.7\n78.0\n67.2\n80.6\n79.6\n81.7\n80.98\nLGrad [45]\n99.7\n71.4\n96.0\n80.3\n86.6\n98.4\n80.3\n51.9\n86.0\n90.4\n87.1\n90.0\n68.1\n87.9\n87.4\n87.8\n84.30\nUnivFD [32]\n100\n84.4\n75.7\n95.2\n98.7\n95.9\n99.7\n67.7\n87.5\n78.1\n78.7\n79.2\n70.0\n95.2\n94.6\n74.2\n85.89\nNPR [46]\n100\n95.4\n96.9\n82.9\n90.0\n99.9\n79.8\n74.6\n83.0\n97.9\n96.6\n97.1\n74.3\n98.0\n97.9\n97.7\n91.38\nSFLD (224+28)\n100\n95.8\n89.0\n97.2\n99.1\n99.3\n97.8\n80.1\n94.6\n87.0\n87.1\n88.9\n83.9\n95.6\n95.5\n80.8\n91.94\nSFLD (224+56)\n100\n90.6\n86.5\n97.8\n99.5\n99.0\n98.9\n82.7\n94.0\n89.2\n89.2\n90.9\n81.0\n97.0\n96.6\n80.1\n92.05\nSFLD\n100\n96.7\n91.9\n96.5\n99.2\n99.4\n96.0\n84.2\n95.2\n90.6\n90.7\n92.5\n86.0\n95.6\n95.7\n82.9\n93.30\nTable 2. Generalization performance on the conventional benchmark reported in accuracy.\nMethod\nTwin-GAN\nTwin-DM\nAvg.\nCNNSpot\n62.92\n46.93\n54.93\nFreDect\n54.57\n55.64\n55.11\nGramNet\n71.98\n36.10\n54.04\nFusing\n61.80\n48.62\n55.21\nLGrad\n59.51\n34.25\n46.88\nUnivFD [32]\n58.09\n74.38\n66.24\nNPR [46]\n78.19\n35.76\n56.98\nPatchShuffle (28)\n73.56\n65.52\n69.54\nPatchShuffle (56)\n75.90\n60.73\n68.32\nSFLD (224+28)\n70.43\n75.80\n73.12\nSFLD (224+56)\n70.16\n72.44\n71.30\nSFLD\n73.82\n72.05\n72.94\nTable 3. Performance comparisons on TwinSynths. Values indicate\nAP score. DM refers to diffusion model.\nTwinSynths-GAN subset. On the other hand, the generated\nimages in TwinSynths-DM contain low-level discriminative\nfeatures introduced by the DDIM decoder, which incorpo-\nrates additional fully connected layers and post-processing\nsteps following the upsampling blocks. We can see that\nNPR exhibits lower performance, whereas UnivFD demon-\nstrates higher performance. Nevertheless, SFLD demon-\nstrates superior and robust performance on both bench-\nTasks\nSITD\nSAN\nCRN\nIMLE\nUnivFD [32]\n65.9\n81.2\n96.4\n98.4\nNPR [46]\n55.2\n60.0\n50.0\n50.0\nSFLD\n71.9\n90.5\n95.8\n98.7\nTable 4. Low-level vision and perceptual benchmarks. Values in-\ndicate AP scores.\nmarks, indicating its ability to capture both low-level feature\nartifacts and high-level feature artifacts. Notably, no exist-\ning detector has ever exhibited such a high level of perfor-\nmance on both benchmarks.\n3.4. Low-level Vision and Perceptual Benchmark\nTab. 4 shows the detection performance on different\nbenchmarks from ForenSynths [49]. Low-level vision mod-\nels, including SITD and SAN, preserve high-level features\nof real images. Perceptual models (CRN and IMLE) color\nsemantically segmented images to match real images, pre-\nserving semantic information. Notably, while NPR was able\nto detect some super-resolution images from SAN, it failed\nto perform well in other image-to-image translation tasks.\nThis indicates that detectors specialized in identifying low-\n\nTasks\nUnivFD [32]\nSFLD (ours)\nReal\nFake\nReal\nFake\nProGAN\n99.9\n100\n100\n100\nStyleGAN\n99.4\n69.4\n99.4\n93.9\nStyleGAN2\n99.8\n51.5\n100\n83.8\nBigGAN\n98.1\n92.2\n93.2\n99.8\nCycleGAN\n98.9\n98.4\n98.3\n100\nStarGAN\n93.6\n98.1\n98.9\n99.9\nGauGAN\n99.3\n100\n92.0\n100\nDeepfake\n94.8\n40.6\n85.2\n83.2\nDALLE\n99.1\n75.8\n96.2\n94.1\nADM\n97.2\n42.8\n95.3\n76.6\nGlide_100_10\n99.1\n57.0\n96.2\n85.0\nGlide_100_27\n99.1\n58.2\n96.2\n85.2\nGlide_50_27\n99.1\n59.3\n96.2\n88.7\nLDM_100\n99.1\n91.2\n96.2\n94.9\nLDM_200\n99.1\n90.0\n96.2\n95.2\nLDM_200_cfg\n99.1\n49.2\n96.2\n69.6\nAvg.\n98.4\n73.4\n96.0\n90.6\nTable 5. Classification accuracy on real and fake sets on Foren-\nSynths [49] and diffusion sets in Ojha et al. [32].\nlevel feature artifacts from ProGAN struggle to general-\nize to images generated from different vision tasks. Con-\nversely, a detector that focuses on high-level feature ar-\ntifacts demonstrates strong performance on these bench-\nmarks. SFLD integrates semantic and structural information\nfrom different patch sizes to show superior performance on\nlow-level vision and perceptual benchmarks.\n4. Discussion\n4.1. Detailed Comparison with UnivFD\nThis section presents a comprehensive comparison of\nSFLD against UnivFD. Tab. 5 shows the classification ac-\ncuracy of the prediction results of real and fake images on\neach generator in the conventional benchmark. It is evident\nthat SFLD exhibits superior performance in predicting gen-\nerated images. Notably, UnivFD is unable to predict fake\nimages in some generated subsets, whereas SFLD demon-\nstrates its strength in both real and generated images. This\nresult supports that SFLD can capture both low-level feature\nartifacts and high-level feature artifacts, making the detec-\ntor better generalize on novel generators.\n4.2. Score Ensembling\nScatter plots. Ensembling of the detection scores of the\noriginal image and patch-shuffled images is supported by\nFig. 4. In all cases, ensembling the two detectors with patch\nsizes 224 and 28 as an average of the two logit scores con-\nsistently improved binary separation and thus resulted in su-\nperior performance with the default threshold (as evidenced\nby Tabs. 1 and 2). This proves that the two detection meth-\nods work as complementary functions.\n20\n10\n0\n10\n20\nUnivFD logits\n20\n15\n10\n5\n0\n5\n10\n15\n20\nShuffle 28x28 logits\nGround truth\n0_real\n1_fake\n(a) StyleGAN\n20\n10\n0\n10\n20\nUnivFD logits\n20\n15\n10\n5\n0\n5\n10\n15\n20\nShuffle 28x28 logits\nGround truth\n0_real\n1_fake\n(b) BigGAN\n20\n10\n0\n10\n20\nUnivFD logits\n20\n15\n10\n5\n0\n5\n10\n15\n20\nShuffle 28x28 logits\nGround truth\n0_real\n1_fake\n(c) DALL-E\n20\n10\n0\n10\n20\nUnivFD logits\n20\n15\n10\n5\n0\n5\n10\n15\n20\nShuffle 28x28 logits\nGround truth\n0_real\n1_fake\n(d) GLIDE_50_27\nFigure 4. Scatter plots of per-sample scores. X-axis is the UnivFD\nlogits, and Y-axis is the logit from PatchShuffle with patch size\n28. The decision boundary of UnivFD (red) and SFLD (green) are\nshown. See Appendix A for extended results.\nA closer look into failure cases. Fig. 5 visualizes\nsome exact failure cases with StyleGAN-generated images\n(Fig. 4a). Fig. 5a shows a case where UnivFD fails and\nPatchShuffle succeeds. These images seem to cause Uni-\nvFD to fail because the high-level feature is well gener-\nated (high global structure fidelity). In contrast, PatchShuf-\nfle, which focuses on local structure, succeeds in detec-\ntion. Our method with score ensembling was able to capture\nthese examples illustrated as the green line in Fig. 4. On the\nother hand, Fig. 5b shows a case where PatchShuffle fails\nand UnivFD succeeds. These generated images have well-\ngenerated local structures like textures but have defects in\nglobal structures such as ears, eyes, and faces. However,\nthere are very few examples corresponding to this. This\nanalysis indicates that using both local and global informa-\ntion is necessary for detecting generated images.\n4.3. Robustness Against Image Degradation\nApplying a Gaussian blur and JPEG compression to an\nimage is a common degradation that can naturally occur.\nFig. 6 illustrates the impact of each attack on two subsets\nof generated images. The diffusion-subset and GAN-subset\nare subsets of diffusion and GAN generators, respectively,\ndrawn from the conventional benchmark. Gaussian indi-\ncates the addition of a Gaussian blur with a standard devia-\ntion of σ. JPEG indicates the application of JPEG compres-\nsion with a specified compression quality. Note that JPEG\n\n(a) Fake image examples on the second quadrant of Fig. 4a.\n(b) Fake image examples on the fourth quadrant of Fig. 4a.\nFigure 5. A closer look into the failure cases from the StyleGAN-\ngenerated test images.\n   None(=0)\n1\n2\n3\n4\ngaussian_\n20\n30\n40\n50\n60\n70\n80\n90\n100\nAP (%)\nRandom Guess\nUnivFD\nPatch Shuffle (28)\nPatch Shuffle (56)\nNPR\nSFLD (Ours)\n(a)\nGaussian blur,\nGANs from [49].\nNone(=0)\n1\n2\n3\n4\ngaussian_\n20\n30\n40\n50\n60\n70\n80\n90\n100\nAP (%)\nRandom Guess\nUnivFD\nPatch Shuffle (28)\nPatch Shuffle (56)\nNPR\nSFLD (Ours)\n(b)\nGaussian blur,\nDMs from [32].\nNone\n100\n90\n70\n50\n30\nJPEG Compression Quality (%)\n20\n30\n40\n50\n60\n70\n80\n90\n100\nAP (%)\nRandom Guess\nUnivFD\nPatch Shuffle (28)\nPatch Shuffle (56)\nNPR\nSFLD (Ours)\n(c)\nJPEG quality,\nGANs from [49].\nNone\n100\n90\n70\n50\n30\nJPEG Compression Quality (%)\n20\n30\n40\n50\n60\n70\n80\n90\n100\nAP (%)\nRandom Guess\nUnivFD\nPatch Shuffle (28)\nPatch Shuffle (56)\nNPR\nSFLD (Ours)\n(d)\nJPEG quality,\nDMs from [32].\nFigure 6. Robustness against simulated image degradation. Meth-\nods include Gaussian blur and JPEG compression.\ncompression with quality 100 does not result in the same\nimage, as JPEG compression reduces color information and\nrounds coefficients, thereby losing some information.\nIf the model is vulnerable to image degradation, we\ncan infer that it is influenced by the features targeted by\nthe degradation. Specifically, Gaussian blur affects both\nhigh- and low-level features in the image, while JPEG com-\npression primarily targets low-level features (see Fig. 13).\nFigs. 6a and 6b demonstrates that SFLD always shows the\nbest performance against Gaussian blur, since it integrates\nboth high- and low-level features through ensemble/fusion,\nenabling each to compensate for the information lost in\nthe other. Figs. 6c and 6d illustrates that SFLD restores\nrobustness against JPEG compression, supporting the fun-\ndamental principle behind our model. Additionally, Uni-\n0_real, \"motorbike\"\n0_real, \"bird\"\n0_real, \"cow\"\n0_real, \"cat\"\n0_real, \"person\"\n0_real, \"horse\"\n1_fake, \"motorbike\"\n1_fake, \"bird\"\n1_fake, \"cow\"\n1_fake, \"cat\"\n1_fake, \"person\"\n1_fake, \"horse\"\n(a) UnivFD [32] examples\n0_real, \"motorbike\"\n0_real, \"bird\"\n0_real, \"cow\"\n0_real, \"cat\"\n0_real, \"person\"\n0_real, \"horse\"\n1_fake, \"motorbike\"\n1_fake, \"bird\"\n1_fake, \"cow\"\n1_fake, \"cat\"\n1_fake, \"person\"\n1_fake, \"horse\"\n(b) Patch-shuffle 28×28 examples\nFigure 7. Class activation maps (CAM) for UnivFD [32] and the\npatch-shuffled detector (ours). GradCAM [15,43] was used to ob-\ntain the heatmaps. The ground truth real/fake labels and class la-\nbels are displayed on top of each image. Note that for Fig. 7b, the\nheat map is split into patches then reverse-shuffled back to the cor-\nresponding spatial location of the input image.\nvFD, which focuses on capturing high-level feature artifacts\nis also robust against JPEG compression. However, NPR,\nwhich focuses on capturing low-level feature artifacts, is\nvulnerable to both Gaussian blur and JPEG compression\neven at JPEG compression quality 100.\n4.4. Qualitative Analysis\nGradCAM visualization. See Fig. 7 for image attribu-\ntion heat maps generated using GradCAM [15,43]. The ex-\namples are from the ProGAN test set. In addition, the heat\nmaps are averaged across ten predictions to reduce the ran-\ndomness from the patch permutation. The CAM of UnivFD\nfocuses on the class-dependent salient region, whereas the\npatch-shuffled detector focuses on the entire image region.\nFeature visualization. Because taking an average of the\nlogits generated via a linear layer is equivalent to taking an\naverage of the feature embeddings, we can understand the\nSFLD embeddings by taking the average of the embeddings\nover multiple shuffles. Fig. 8 visualizes the feature embed-\ndings by projecting onto a 2D plane using UMAP [42]. We\nused the ProGAN test set to extract the embeddings.\nBecause UnivFD learns the features directly from the\nCLIP visual encoder, the embeddings form class-dependent\nclusters. This creates class-dependent decision boundary,\nwhich may introduce unintended content bias to the real-\nfake detector. In contrast, because PatchShuffle destroys\n\n0_real\n1_fake\ndog\nmotorbike\nchair\ntrain\nsheep\ncar\ncow\nairplane\nbicycle\nhorse\ntvmonitor\npottedplant\nboat\nbottle\nbird\nperson\nbus\ndiningtable\ncat\nsofa\n(a) Embeddings from UnivFD [32,36]\n0_real\n1_fake\ndog\nmotorbike\nchair\ntrain\nsheep\ncar\ncow\nairplane\nbicycle\nhorse\ntvmonitor\npottedplant\nboat\nbottle\nbird\nperson\nbus\ndiningtable\ncat\nsofa\n(b) PatchShuffle(28) embeddings averaged across Nviews = 10 shuffles.\nFigure 8. UMAP visualization [42] of feature embeddings. Left\nand right plots show the same projected embeddings colored by re-\nal/fake labels (left) and object category labels (right). Our method\ndestroys the class information from the embeddings, thereby im-\nproving the generalization by reducing the content bias.\nclass-related information from the image, the correspond-\ning embeddings show more dispersion within each class.\n4.5. Effect of PatchShuffle Hyperparameters\nImproving feature extraction with PatchShuffle. We\nsuggest additional details to get better CLIP features from\nthe shuffled images. To improve stability against the ran-\ndomness introduced by PatchShuffle, we use the averaged\nlogits of Nviews = 10 randomly shuffled patch combina-\ntions for each input image during testing.\nMoreover, in our problem setup, training images are\nfixed at 256×256 size, while test images can vary in size.\nResizing test images is avoided, as image degradation due\nto resizing (e.g., JPEG compression or blur) has been shown\nto impact the detection of AI-generated images negatively\n[49]. Instead, recent detectors [32,46] prefer cropping over\nresizing. Our backbone model without PatchShuffle also ex-\ntracts CLIP features from 224×224 center-cropped images\nwithout resizing. However, we can extract information not\nonly from the center of the image but from the entire image\nby taking advantage of the proposed PatchShuffle, which\nallows non-consecutive patchwise combinations. We divide\nthe entire test image into non-overlapping patches of the\ngiven patch size and combine these patches into 224×224\nimages. This approach enables the detector to analyze infor-\nmation from the entire image, rather than being constrained\nto a single central region. See Appendix D for more details.\nPatch size. The optimal patch size should be sufficiently\nsmall to disrupt the underlying image structure while pre-\nserving some high-level feature artifacts. The results for the\nperformance difference according to patch sizes on a con-\n2\n4\n7\n14\n28\n56\n112 224\nPatch Size (px)\n70\n75\n80\n85\n90\n95\n100\nAccuracy (%)\n70\n75\n80\n85\n90\n95\n100\nmAP (%)\nAcc, Nviews = 10\nAcc, Nviews = 1\nmAP, Nviews = 10\nmAP, Nviews = 1\n(a) Sweep over patch size\n1\n5\n10\n20\n30\nNumber of shuffles for averaging\n90\n91\n92\n93\n94\n95\n96\n97\nmAP (%)\n0\n10\n20\n30\n40\nInference Time (ms)\nmAP\nInference Time\n(b) Sweep over Nviews\nFigure 9. Best patch sizes were found at 28×28 and 56×56.\nNviews = 10 showed the best balance between performance and\ninference cost.\nventional benchmark are presented in Fig. 9a. Each patch\nsize model in x-axis refers to the ensemble between the\ncorresponding PatchShuffle model and UnivFD(patch size\n224). It can be observed that an too small patch size and\nan excessively large patch size do not assist the model in\ncapturing useful high-level and low-level feature artifacts.\nTherefore, the majority of experiments in this paper utilized\npatch sizes 28x28 and 56x56 according to this result.\nNumber of shuffled views. To ensure the stability of the\nrandom patch shuffle, SFLD generates multiple versions of\nshuffled image from a single test image and employs the\naverage of them as the score. As illustrated in Fig. 9b, mAP\nenhances with higher Nviews. However, due to the tradeoff\nwith inference time, we chose Nviews = 10, and all results\npresented in this paper were obtained with this setting. The\nresults in Fig. 9b are from the PatchShuffle model with a\npatch size of 28, without an ensemble with UnivFD. The\ninference time was measured using RTX 4090 GPU.\n5. Conclusions\nIn this paper, we introduced SFLD, a novel method for\ndetecting AI-generated images that effectively combines\nglobal semantic structures and textural structures to im-\nprove detection performance. By leveraging random patch\nshuffling and an ensemble of classifiers trained on patches\nof varying sizes, our approach effectively addresses the\nshortcomings of existing methods, such as their content bias\nand susceptibility to image perturbations. Also, We pro-\nposed a new quality-ensuring benchmark, TwinSynths. It is\nthe first to consider a scenario of infinitely real-like fake\nimages, providing a valuable resource for future research in\nthis area. We demonstrated that SFLD outperforms SOTA\nmethods in generalization to various generators, even in\nchallenging scenarios simulated with TwinSynths.\nAcknowledgements This work was supported by Institute\nof Information & communications Technology Planning\n& Evaluation (IITP) grant funded by the Korea govern-\nment(MSIT) (No.RS-2021-II212068, Artificial Intelligence\nInnovation Hub).\n\nReferences\n[1] Ali Borji. Generated faces in the wild: Quantitative compar-\nison of stable diffusion, midjourney and dall-e 2. In Arxiv,\n2022. 14, 15\n[2] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large\nscale gan training for high fidelity natural image synthesis.\narXiv, 2018. 12\n[3] George Cazenavette, Avneesh Sud, Thomas Leung, and Ben\nUsman. Fakeinversion: Learning to detect images from un-\nseen text-to-image models by inverting stable diffusion. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 10759–10769, 2024. 10\n[4] Lucy Chai et al. What makes fake images detectable? un-\nderstanding properties that generalize. In ECCV, pages 103–\n120. Springer, 2020. 15\n[5] Chen Chen, Qifeng Chen, Jia Xu, and Vladlen Koltun.\nLearning to see in the dark. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pages\n3291–3300, 2018. 12\n[6] Qifeng Chen and Vladlen Koltun. Photographic image syn-\nthesis with cascaded refinement networks. In Proceedings of\nthe IEEE international conference on computer vision, pages\n1511–1520, 2017. 12\n[7] Yunjey Choi et al. Stargan: Unified generative adversarial\nnetworks for multi-domain image-to-image translation. In\nCVPR, pages 8789–8797, 2018. 12\n[8] Riccardo Corvi, Davide Cozzolino, Giada Zingarini, Gio-\nvanni Poggi, Koki Nagano, and Luisa Verdoliva.\nOn the\ndetection of synthetic images generated by diffusion mod-\nels. In ICASSP 2023-2023 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP), pages\n1–5. IEEE, 2023. 14\n[9] Tao Dai, Jianrui Cai, Yongbing Zhang, Shu-Tao Xia, and\nLei Zhang. Second-order attention network for single im-\nage super-resolution. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n11065–11074, 2019. 12\n[10] Boris\nDayma\net\nal.\nDall·\ne\nmini.\nHugging-\nFace. com. https://huggingface. co/spaces/dallemini/dalle-\nmini (accessed Sep. 29, 2022), 2021. 12\n[11] Deepfakes. Deepfakes/faceswap: Deepfakes software for all.\n14\n[12] Prafulla Dhariwal et al. Diffusion models beat gans on im-\nage synthesis. Advances in neural information processing\nsystems, 34:8780–8794, 2021. 12\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In International Conference on Learning Representa-\ntions, 2021. 2\n[14] Joel Frank et al.\nLeveraging frequency analysis for deep\nfake image recognition. In ICML, pages 3247–3258. PMLR,\n2020. 4, 5, 14\n[15] Jacob Gildenblat and contributors. Pytorch library for cam\nmethods. https://github.com/jacobgil/pytorch-grad-\ncam, 2021. 7, 12\n[16] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade\nGordon, Nicholas Carlini, Rohan Taori, Achal Dave,\nVaishaal Shankar, Hongseok Namkoong, John Miller, Han-\nnaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Open-\nclip, July 2021. 15\n[17] Yan Ju et al. Fusing global and local features for generalized\nai-synthesized image detection. In 2022 IEEE International\nConference on Image Processing (ICIP), pages 3465–3469.\nIEEE, 2022. 4, 5\n[18] Tero Karras et al. Progressive growing of gans for improved\nquality, stability, and variation. In ICLR, 2018. 4, 12\n[19] Tero Karras et al. A style-based generator architecture for\ngenerative adversarial networks.\nIn CVPR, pages 4401–\n4410, 2019. 12\n[20] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila. Analyzing and improving\nthe image quality of StyleGAN. In Proc. CVPR, 2020. 12\n[21] Ke Li, Tianhao Zhang, and Jitendra Malik. Diverse image\nsynthesis from semantic layouts via conditional imle.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 4220–4229, 2019. 12\n[22] Tsung-Yi Lin et al. Microsoft coco: Common objects in con-\ntext. In ECCV, pages 740–755. Springer, 2014. 12\n[23] Bo Liu, Fan Yang, Xiuli Bi, Bin Xiao, Weisheng Li, and\nXinbo Gao. Detecting generated images by real images. In\nEuropean Conference on Computer Vision, pages 95–110.\nSpringer, 2022. 4, 5\n[24] Ziwei Liu et al. Deep learning face attributes in the wild. In\nICCV, pages 3730–3738, 2015. 12\n[25] Zhengzhe Liu et al. Global texture enhancement for fake face\ndetection in the wild. In CVPR, pages 8060–8069, 2020. 4,\n5\n[26] Yunpeng Luo, Junlong Du, Ke Yan, and Shouhong Ding.\nLareˆ 2: Latent reconstruction error based method for\ndiffusion-generated image detection.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 17006–17015, 2024. 14\n[27] Francesco Marra et al. Do gans leave artificial fingerprints?\nIn 2019 IEEE conference on multimedia information pro-\ncessing and retrieval (MIPR), pages 506–511. IEEE, 2019.\n14\n[28] Scott McCloskey and Michael Albright.\nDetecting gan-\ngenerated imagery using color cues.\narXiv preprint\narXiv:1812.08247, 2018. 14\n[29] Huaxiao Mo, Bolin Chen, and Weiqi Luo. Fake faces identi-\nfication via convolutional neural network. In Proceedings of\nthe 6th ACM workshop on information hiding and multime-\ndia security, pages 43–47, 2018. 14\n[30] Lakshmanan Nataraj, Tajuddin Manhar Mohammed, BS\nManjunath, Shivkumar Chandrasekaran, Arjuna Flenner,\nJawadul H Bappy, and Amit K Roy-Chowdhury.\nDetect-\ning gan generated fake images using co-occurrence matrices.\nElectronic Imaging, 31:1–7, 2019. 14\n\n[31] Alex Nichol et al. Glide: Towards photorealistic image gen-\neration and editing with text-guided diffusion models. arXiv\npreprint arXiv:2112.10741, 2021. 12\n[32] Utkarsh Ojha et al. Towards universal fake image detectors\nthat generalize across generative models. In CVPR, pages\n24480–24489, 2023. 1, 2, 3, 4, 5, 6, 7, 8, 12, 13, 14, 15\n[33] Maxime Oquab, Timothée Darcet, Theo Moutakanni, Huy V.\nVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Rus-\nsell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-\nWen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nico-\nlas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou,\nJulien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bo-\njanowski. Dinov2: Learning robust visual features without\nsupervision, 2023. 15\n[34] Taesung Park et al. Semantic image synthesis with spatially-\nadaptive normalization. In CVPR, pages 2337–2346, 2019.\n12\n[35] Alec Radford et al.\nUnsupervised representation learn-\ning with deep convolutional generative adversarial networks.\narXiv preprint arXiv:1511.06434, 2015. 4\n[36] Alec Radford et al. Learning transferable visual models from\nnatural language supervision. In ICML, pages 8748–8763.\nPMLR, 2021. 2, 8\n[37] Jonas Ricker, Simon Damm, Thorsten Holz, and Asja Fis-\ncher. Towards the detection of diffusion model deepfakes,\n2024. 14\n[38] Robin Rombach et al. High-resolution image synthesis with\nlatent diffusion models. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npages 10684–10695, 2022. 12\n[39] Andreas Rössler, Davide Cozzolino, Luisa Verdoliva, Chris-\ntian Riess, Justus Thies, and Matthias Nießner. FaceForen-\nsics++: Learning to detect manipulated facial images. In In-\nternational Conference on Computer Vision (ICCV), 2019.\n14\n[40] Andreas Rossler et al.\nFaceforensics++: Learning to de-\ntect manipulated facial images. In Proceedings of the ICCV,\npages 1–11, 2019. 12\n[41] Olga Russakovsky et al. Imagenet large scale visual recog-\nnition challenge. International journal of computer vision,\n115(3):211–252, 2015. 4, 12\n[42] Tim Sainburg, Leland McInnes, and Timothy Q. Gentner.\nParametric umap: learning embeddings with deep neural\nnetworks for representation and semi-supervised learning.\nArXiv e-prints, 2020. 7, 8\n[43] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek\nDas, Ramakrishna Vedantam, Devi Parikh, and Dhruv Ba-\ntra. Grad-cam: visual explanations from deep networks via\ngradient-based localization. International journal of com-\nputer vision, 128:336–359, 2020. 7, 12\n[44] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. In International Conference\non Learning Representations. 4\n[45] Chuangchuang Tan et al. Learning on gradients: Generalized\nartifacts representation for gan-generated images detection.\nIn CVPR (CVPR), pages 12105–12114, June 2023. 4, 5\n[46] Chuangchuang Tan, Yao Zhao, Shikui Wei, Guanghua Gu,\nPing Liu, and Yunchao Wei. Rethinking the up-sampling op-\nerations in cnn-based generative network for generalizable\ndeepfake detection. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n28130–28139, 2024. 1, 4, 5, 8, 15\n[47] Shahroz Tariq, Sangyup Lee, Hoyoung Kim, Youjin Shin,\nand Simon S Woo.\nGan is a friend or foe? a frame-\nwork to detect various fake face images. In Proceedings of\nthe 34th ACM/SIGAPP Symposium on Applied Computing,\npages 1296–1303, 2019. 14\n[48] Rafael Valle, Wilson Cai, and Anish Doshi.\nTequila-\ngan: How to easily identify gan samples.\narXiv preprint\narXiv:1807.04919, 2018. 14\n[49] Sheng-Yu Wang et al. Cnn-generated images are surprisingly\neasy to spot... for now. In CVPR, pages 8695–8704, 2020. 1,\n2, 4, 5, 6, 7, 8, 12\n[50] Zhendong Wang et al. Dire for diffusion-generated image de-\ntection. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision (ICCV), pages 22445–22455,\nOctober 2023. 14\n[51] Fisher Yu et al. Lsun: Construction of a large-scale image\ndataset using deep learning with humans in the loop. arXiv\npreprint arXiv:1506.03365, 2015. 12\n[52] Ning Yu et al. Attributing fake images to gans: Learning\nand analyzing gan fingerprints. In Proceedings of the ICCV,\npages 7556–7566, 2019. 14\n[53] Yichi Zhang and Xiaogang Xu. Diffusion noise feature: Ac-\ncurate and fast generated image detection. arXiv preprint\narXiv:2312.02625, 2023. 14\n[54] Nan Zhong, Yiran Xu, Zhenxing Qian, and Xinpeng Zhang.\nRich and poor texture contrast: A simple yet effective ap-\nproach for ai-generated image detection.\narXiv preprint\narXiv:2311.12397, 2023. 4\n[55] Jun-Yan Zhu et al. Unpaired image-to-image translation us-\ning cycle-consistent adversarial networks. In ICCV, pages\n2223–2232, 2017. 12\n[56] Mingjian Zhu, Hanting Chen, Mouxiao Huang, Wei Li,\nHailin Hu, Jie Hu, and Yunhe Wang.\nGendet: Towards\ngood generalizations for ai-generated image detection. arXiv\npreprint arXiv:2312.08880, 2023. 1, 15\n\nA. Additional results on scatter plots\nAdditional results to Sec. 4.2 are presented in Fig. 10.\n20\n10\n0\n10\n20\nUnivFD logits\n20\n15\n10\n5\n0\n5\n10\n15\n20\nShuffle 28x28 logits\nGround truth\n0_real\n1_fake\n(a) ProGAN\n20\n10\n0\n10\n20\nUnivFD logits\n20\n15\n10\n5\n0\n5\n10\n15\n20\nShuffle 28x28 logits\nGround truth\n0_real\n1_fake\n(b) StyleGAN\n20\n10\n0\n10\n20\nUnivFD logits\n20\n15\n10\n5\n0\n5\n10\n15\n20\nShuffle 28x28 logits\nGround truth\n0_real\n1_fake\n(c) StyleGAN2\n20\n10\n0\n10\n20\nUnivFD logits\n20\n15\n10\n5\n0\n5\n10\n15\n20\nShuffle 28x28 logits\nGround truth\n0_real\n1_fake\n(d) BigGAN\n20\n10\n0\n10\n20\nUnivFD logits\n20\n15\n10\n5\n0\n5\n10\n15\n20\nShuffle 28x28 logits\nGround truth\n0_real\n1_fake\n(e) CycleGAN\n20\n10\n0\n10\n20\nUnivFD logits\n20\n15\n10\n5\n0\n5\n10\n15\n20\nShuffle 28x28 logits\nGround truth\n0_real\n1_fake\n(f) StarGAN\n20\n10\n0\n10\n20\nUnivFD logits\n20\n15\n10\n5\n0\n5\n10\n15\n20\nShuffle 28x28 logits\nGround truth\n0_real\n1_fake\n(g) GauGAN\n20\n10\n0\n10\n20\nUnivFD logits\n20\n15\n10\n5\n0\n5\n10\n15\n20\nShuffle 28x28 logits\nGround truth\n0_real\n1_fake\n(h) DeepFake\n20\n10\n0\n10\n20\nUnivFD logits\n20\n15\n10\n5\n0\n5\n10\n15\n20\nShuffle 28x28 logits\nGround truth\n0_real\n1_fake\n(i) DALL-E\n20\n10\n0\n10\n20\nUnivFD logits\n20\n15\n10\n5\n0\n5\n10\n15\n20\nShuffle 28x28 logits\nGround truth\n0_real\n1_fake\n(j) GLIDE_100_10\n20\n10\n0\n10\n20\nUnivFD logits\n20\n15\n10\n5\n0\n5\n10\n15\n20\nShuffle 28x28 logits\nGround truth\n0_real\n1_fake\n(k) GLIDE_100_27\n20\n10\n0\n10\n20\nUnivFD logits\n20\n15\n10\n5\n0\n5\n10\n15\n20\nShuffle 28x28 logits\nGround truth\n0_real\n1_fake\n(l) GLIDE_50_27\n20\n10\n0\n10\n20\nUnivFD logits\n20\n15\n10\n5\n0\n5\n10\n15\n20\nShuffle 28x28 logits\nGround truth\n0_real\n1_fake\n(m) ADM\n20\n10\n0\n10\n20\nUnivFD logits\n20\n15\n10\n5\n0\n5\n10\n15\n20\nShuffle 28x28 logits\nGround truth\n0_real\n1_fake\n(n) LDM_100\n20\n10\n0\n10\n20\nUnivFD logits\n20\n15\n10\n5\n0\n5\n10\n15\n20\nShuffle 28x28 logits\nGround truth\n0_real\n1_fake\n(o) LDM_200\n20\n10\n0\n10\n20\nUnivFD logits\n20\n15\n10\n5\n0\n5\n10\n15\n20\nShuffle 28x28 logits\nGround truth\n0_real\n1_fake\n(p) LDM_200_cfg\n20\n10\n0\n10\n20\nUnivFD logits\n20\n15\n10\n5\n0\n5\n10\n15\n20\nShuffle 28x28 logits\nGround truth\n0_real\n1_fake\n(q) SITD\n20\n10\n0\n10\n20\nUnivFD logits\n20\n15\n10\n5\n0\n5\n10\n15\n20\nShuffle 28x28 logits\nGround truth\n0_real\n1_fake\n(r) SAN\n20\n10\n0\n10\n20\nUnivFD logits\n20\n15\n10\n5\n0\n5\n10\n15\n20\nShuffle 28x28 logits\nGround truth\n0_real\n1_fake\n(s) CRN\n20\n10\n0\n10\n20\nUnivFD logits\n20\n15\n10\n5\n0\n5\n10\n15\n20\nShuffle 28x28 logits\nGround truth\n0_real\n1_fake\n(t) IMLE\nFigure 10. Scatter plots of per-sample scores. X-axis is UnivFD logits, and Y-axis is the logit from PatchShuffle with patch size 28. The\ndecision boundary of UnivFD (red) and SFLD (green) are shown.\n\nB. Datasets\nB.1. Train dataset\nTo establish a baseline for comparison, we adopt the\nmost common setting for training the detection model,\nnamely the train set from ForenSynths [49]. The train set\nconsists of real images and ProGAN [18]-generated images.\nIt involves 20 different object class categories, each contain-\ning 18K real images from the different LSUN [51] datasets\nand 18K synthetic images generated by ProGAN.\nB.2. Test dataset\nWe evaluate the performance of SFLD on (1) conven-\ntional benchmarks, (2) TwinSynths which we proposed, (3)\nlow-level vision and perceptual loss benchmarks. In this\nsection, we provide a detailed description of the configura-\ntions for the conventional benchmarks and low-level vision\nand perceptual loss benchmarks.\nConventional benchmark This is from ForenSynths\n[49] and Ojha et al. [32], including 16 different subsets of\ngenerated images, synthesized by seven GAN-based gener-\native models, eight diffusion-based generative models and\none deepfake model. The subset of GAN-based fake im-\nages are from ForenSynths [49], including ProGAN [18],\nStyleGAN [19], StyleGAN2 [20], BigGAN [2], Cycle-\nGAN [55], StarGAN [7], and GauGAN [34]. The subset\nof diffusion-based fake images are from Ojha et al. [32], in-\ncluding DALL-E [10], three different variants of Glide [31],\nADM(guided-diffusion) [12], and three different variants\nof LDM [38]. Deepfake set is from FaceForensices++ [40]\nwhich is included in ForenSynths [49]. The real images cor-\nresponding to the fake images described above were di-\nrectly taken from the same datasets. Those are sampled\nfrom LSUN [51], ImageNet [41], CycleGAN [55], CelebA\n[24], COCO [22], and FaceForensics++ [40].\nLow-level vision and perceptual loss benchmarks\nLow-level vision benchmark consists of SITD [5] and SAN\n[9]. These are image processing models that approximate\nlong exposures in low light conditions from short expo-\nsures in raw camera input or process super-resolution on\nlow-resolution images. Perceptual benchmark consists of\nCRN [6] and IMLE [21]. These models color the seman-\ntic segmentation map into a realistic image while directly\noptimizing a perceptual loss. These benchmarks are from\nForenSynths [49].\nC. Qualitative analysis on TwinSynths dataset\nWe show the GradCAM visualization of UnivFD [32]\nand Patch-shuffle 28×28 using the TwinSynths dataset in\nFig. 11. Similar to Sec. 4.4, UnivFD is shown to focus on the\nclass-dependent salient region, whereas our method focuses\non the entire image region. Moreover, we observed that for\nBenchmark\nSFLD (224+24)\nSFLD (224+56)\nSFLD\ncenter\nfull image\ncenter\nfull image\ncenter\nfull image\nmain benchmark\n98.04\n98.03\n98.37\n98.39\n98.40\n98.43\nCRN\n94.41\n96.62\n94.17\n97.24\n91.97\n95.79\nIMLE\n97.55\n98.65\n98.12\n99.23\n96.92\n98.64\nSITD\n59.36\n64.82\n67.71\n76.66\n60.38\n71.90\nTable 6. mAP results of the various sizes of test images, comparing\ntwo different patch selecting methods. Center denotes that the im-\nages have been center-cropped to 224×224, while full image means\nthat random patches from the full image have been combined to re-\nconstruct a 224×224 image.\nTwinSynths dataset, UnivFD does respond identically to re-\nal/fake images which indicate its inability to capture subtle\nfake image fingerprints, whereas our method shows the re-\nsponse to such a difference.\nReal\nGAN\nDM\n(a) UnivFD [32] examples\nReal\nGAN\nDM\n(b) PatchShuffle(patch size 28) examples\nFigure 11. Class activation maps (CAM) for UnivFD [32] and\nthe patch-shuffled detector (ours) in TwinSynths dataset. Each\nrow shows examples from TwinSynths-real, TwinSynths-GAN,\nTwinSynths-DM sets. GradCAM [15, 43] was used to obtain the\nheatmaps.\nD. Effect of selecting patches from the whole\nimage\nFig. 12 illustrates the concept of patch extraction of\nSFLD mentioned in Sec. 2.1. Unlike many alternative de-\ntection methodologies, SFLD extracts patches from any po-\nsition within the input image at the test time. This approach\nenhances the detector’s receptive field and improves perfor-\n\nCenter crop\nCombine patches\nfrom the whole image\nFigure 12. Illustration of the test input processing strategy. In typ-\nical methods, a test image is center-cropped before being passed\nto the detector. Our patch shuffling strategy allows us to select\npatches from the entire image region, effectively increasing its re-\nceptive field.\nFigure 13. Examples of two image degradation\nmance for images that have higher resolution than 224×224.\nIn Tab. 6, we compare results on benchmarks that have high-\nresolution images. We consider different SFLD ensemble\noptions and the location of the selected patch. The main\nbenchmark consists mostly of 256×256 images, which have\nlittle margin with a 224×224 center crop. Meanwhile, the\nCRN and IMLE benchmarks have 512×256 images, and\nthe SITD benchmark includes images much larger up to\n2,848×4,256 or 4,032×6,030.\nWe observed that the discrepancy between the two\nmethodologies was minimal when the test image was small.\nHowever, as the image size increased, the performance of\nthe method that solely focused on the center of an image\nbecame increasingly constrained.\nE. Image degradation examples\nFig. 13 shows examples of image gradations. According\nto our definition of high- and low-level features, we can con-\nsider that the gaussian blur attacks both high- and low-level\nfeatures in the image, and the JPEG compression attacks on\nlow-level features in the image.\nF. Robustness against image degradation\nSince image degradation was not considered during\ntraining, it may be useful to examine the changes in output\ndistribution (as shown in Fig. 16 in supplementary mate-\n7\n14\n28\n56\n112\n224\nPatch Size\n80\n85\n90\n95\n100\nAP (%)\nprogan\ncyclegan\nbiggan\nstylegan\nstylegan2\ngaugan\nstargan\n(a) GAN-based generators.\n7\n14\n28\n56\n112\n224\nPatch Size\n80\n85\n90\n95\n100\nAP (%)\nguided\nldm_200\nldm_200_cfg\nldm_100\nglide_100_27\nglide_50_27\nglide_100_10\ndalle\n(b) Diffusion-based generators.\nFigure 14. Results of the ensemble models of UnivFD and the\npatch-shuffled model with each patch size. For 224, it is the same\nas UnivFD.\nrial) to analyze the model’s operational tendencies in detail.\nFig. 16 reveals distinctions between the high-level feature\nmodel (UnivFD Fig. 16b), low-level feature model (NPR\nFig. 16c), and integrated model. The distributions of SFLD\nand UnivFD remain distinguishable, despite a slight de-\ncline in discrimination performance. However, NPR aligns\nreal and generated images into the same distribution. This\nbehavior arises from the operational mechanism of each\nmodel. NPR primarily focuses on low-level features, result-\ning in a catastrophic failure to maintain robustness against\nJPEG compression. UnivFD demonstrates relative robust-\nness due to its emphasis on high-level features through\nCLIP visual encoders; however, there is a slight perfor-\nmance penalty because the visual encoder does not com-\npletely disregard low-level features. In contrast, SFLD ex-\nhibits robustness against JPEG compression by integrating\nboth high- and low-level features through ensemble/fusion,\nallowing each to compensate for the information lost in the\nother.\nG. Effect of patch sizes\nTo supplement Fig. 9a in the main text, we checked the\nAP for each generator, rather than the average AP on the\nconventional benchmark. Fig. 14 illustrates that SFLD con-\nsistently maintains high performance as long as the patch\nsize is not smaller than the patch size of the image encoder\nbackbone. This is because when the shuffling patch size sN\nis smaller than the ViT’s patch size, the input tokens are af-\nfected by patch-shuffling to get an unnatural image patch,\nresulting in the encoder not properly embedding the visual\nfeature.\nH. Ablation on the pre-trained image encoder\nThe pre-trained image encoder is employed to learn the\nfeatures of the “real” class. According to [32], directly fine-\ntuning the encoder makes the detector overfit to a specific\ngenerator used in training. This results in low generaliza-\ntion to unseen generators. Therefore, we utilized the frozen\nCLIP:ViT-L/14 model following UnivFD.\nTab. 7 show that our patch shuffling and ensembling\n\nUnivFD\nSFLD(Ours)\nMethod\n75\n80\n85\n90\n95\n100\nAP(%)\nBedroom(Unseen)\nCar(Seen)\nCat(Seen)\n(a) CLIP-ViT\nUnivFD\nSFLD(Ours)\nMethod\n75\n80\n85\n90\n95\n100\nAP(%)\nBedroom(Unseen)\nCar(Seen)\nCat(Seen)\n(b) DINOv2-ViT\nUnivFD\nSFLD(Ours)\nMethod\n75\n80\n85\n90\n95\n100\nAP(%)\nBedroom(Unseen)\nCar(Seen)\nCat(Seen)\n(c) OpenCLIP-ViT\nFigure 15. Class-wise detection results for StyleGAN-{bedroom,\ncar, cat} class categories reported in AP. bedroom class is a novel\nclass that is not in the training set.\nstrategy improves the performance regardless of the pre-\ntrained backbone. All models are trained only with real and\ngenerated images from ProGAN and tested on the various\nunseen generated images in conventional benchmark. For\nImageNet-ViT, we used ViT-B/16 model, following Uni-\nvFD paper [32]. Since its encoders have patch size of 16,\nwe utilized 16 and 32 for patch sizes instead of 28 and 56.\nMoreover, note that simply employing different pre-training\ndatasets or strategies – ImageNet, DINOv2, OpenCLIP –\ndoes not address the content bias problem. (see Fig. 15)\nI. In-the-wild applications of SFLD\nWe applied our SFLD to in-the-wild AI-generated im-\nage detection, especially to a deepfake detection bench-\nmark. We have already demonstrated performance on a\nFaceForensics++ [39] subset, which is a deepfake detection\nbenchmark created using face manipulation software [11].\nHere, we have added Tab. 8 with experiments using Gener-\nated Faces in the Wild [1] datasets. SFLD shows state-of-\nthe-art performance in detecting real-world deepfakes.\nJ. Pseudocode of SFLD\nSee Algorithm 1.\nK. Related works\nAI-generated image detection on specific image gen-\neration models Research on distinguishing between syn-\nthetic and real images using deep learning models has in-\ncreased with the development of image generation models.\nEarly works were focused on finding the fingerprints\nin images generated with GANs, which were targeted at\nhigh-performing image generation models. Two major ap-\nproaches were the use of statistics from the image domain\n[28, 30] and the training of CNN-based classifiers. In par-\nticular, in the case of using CNNs, there are two main ap-\nproaches: focusing on the image domain [29, 47,52] or the\nfrequency domain [14,27,48]. Specifically, GAN-generated\nAlgorithm 1 PyTorch-style pseudocode of SFLD\n\"\"\"\nArgs:\nimage: A test image instance\nn_views: Number of views for random patch shuffle\naveraging. Defaults to 10.\nvisual_encoder: A CLIP-pretrained ViT-L/14 visual\nencoder.\nReturns:\noutput: a real/fake score normalized to [0,1] range.\n\"\"\"\n# prediction from 224x224 unshuffled view\nfeature = visual_encoder(image)\noutput_224 = classifier_univfd(feature)\n# prediction from 56x56 random shuffled views\noutput_56 = []\nfor _ in range(n_views):\nimage_shuffled = patch_shuffle(image, size=56)\nfeature = visual_encoder(image_shuffled)\noutput = classifier_56(feature)\noutput_56.append(output)\noutput_56 = mean(output_56)\n# prediction from 28x28 random shuffled views\noutput_28 = []\nfor _ in range(n_views):\nimage_shuffled = patch_shuffle(image, size=28)\nfeature = visual_encoder(image_shuffled)\noutput = classifier_28(feature)\noutput_28.append(output)\noutput_28 = mean(output_28)\n# ensemble the logit scores\noutput = mean([output_224, output_56, output_28])\noutput = output.sigmoid()\nimages have been found to exhibit sharp periodic artifacts\nin this frequency domain, leading to a variety of applica-\ntions [8,14,37].\nRecently, generative models took a big leap forward with\nthe advent of diffusion models, which called for fake im-\nage detection methods that are able to respond to diffusion\nmodels. However, some studies show that existing models\ntrained to detect conventional GANs often fail in images\nfrom diffusion models. For example, periodic artifacts that\nwere clearly visible in GAN were rarely found in diffusion\nmodels [8, 37]. In response, new detection methods opti-\nmized for diffusion models have emerged, for example, ap-\nproaches that use diffusion models to reconstruct test im-\nages and evaluate them based on how well they are recon-\nstructed [26,50,53].\nGeneralization of AI-generated image detection Re-\ncently, the community has shifted its focus towards gen-\neral AI-generated image detectors that are not specific to\nGAN or diffusion. In particular, the development of com-\nmercially deployed generated models that do not reveal the\nmodel structure has increased the demand for such a univer-\nsal detector.\nApart from existing attempts to learn a specialized fea-\nture extractor that simply classifies real/fake in a binary\nmanner, Ojha et al. [32] used the features extracted from\n\n20\n15\n10\n5\n0\n5\n10\n15\n20\n0\n50\n100\n150\n200\n250\n300\n350\nSFLD without JPEG compression\nReal\nFake\n20\n15\n10\n5\n0\n5\n10\n15\n20\n0\n50\n100\n150\n200\n250\nSFLD at JPEG compression quality 100\nReal\nFake\n20\n15\n10\n5\n0\n5\n10\n15\n20\n0\n50\n100\n150\n200\n250\n300\nSFLD at JPEG compression quality 30\nReal\nFake\n(a) The changes of SFLD output distribution\n20\n15\n10\n5\n0\n5\n10\n15\n20\n0\n50\n100\n150\n200\n250\nUnivFD without JPEG compression\nReal\nFake\n20\n15\n10\n5\n0\n5\n10\n15\n20\n0\n50\n100\n150\n200\nUnivFD at JPEG compression quality 100\nReal\nFake\n20\n15\n10\n5\n0\n5\n10\n15\n20\n0\n50\n100\n150\n200\n250\n300\nUnivFD at JPEG compression quality 30\nReal\nFake\n(b) The changes of UnivFD output distribution\n150\n100\n50\n0\n50\n100\n150\n0\n100\n200\n300\n400\n500\nNPR without JPEG compression\nReal\nFake\n150\n100\n50\n0\n50\n100\n150\n0\n50\n100\n150\n200\n250\n300\nNPR at JPEG compression quality 100\nReal\nFake\n150\n100\n50\n0\n50\n100\n150\n0\n25\n50\n75\n100\n125\n150\n175\n200\nNRP at JPEG compression quality 30\nReal\nFake\n(c) The changes of NPR output distribution\nFigure 16. The changes of model output distribution against JPEG compression\nPatch sizes\nPre-training\nImageNet-ViT\nPatch sizes\nPre-training\nDINOv2-ViT [33]\nOpenCLIP-ViT [16]\nCLIP-ViT\nAcc.\nAP\nAcc.\nAP\nAcc.\nAP\nAcc.\nAP\n224 (UnivFD [32])\n62.45\n69.30\n224 (UnivFD [32])\n81.89\n91.75\n86.49\n96.90\n85.89\n96.29\n224+16\n63.88\n72.23\n224+28\n82.88\n93.42\n86.50\n97.59\n91.94\n98.03\n224+32\n63.34\n71.36\n224+56\n82.44\n93.04\n86.87\n97.70\n92.05\n98.39\n224+32+16 (ours)\n63.70\n72.18\n224+56+28 (ours)\n82.26\n93.26\n86.19\n97.49\n93.30\n98.43\nTable 7. Detection accuracy and AP on a conventional benchmark of the proposed patch shuffling and ensembling (SFLD) strategy across\nvarious pre-trained encoders. For the ImageNet encoder, ViT-B/16 is used. For the other encoders, ViT-L/14 is used.\nMethod\nGFW [1]\nAcc.\nmAP\nNPR [46]\n53.30\n47.63\nUnivFD [32]\n70.07\n85.55\nSFLD(224+56)\n77.80\n86.70\nSFLD\n77.28\n86.70\nTable 8. Performance on the in-the-wild deepfake detection bench-\nmark.\na strong vision-language pre-trained encoder that is not\ntrained on a particular AI-generated image. Zhu et al. [56]\ncombined anomaly detection methods to increase the dis-\ncrepancy between real and fake image features.\nFurthermore, several studies have concentrated on an-\nalyzing pixel-level traces on images inevitably left by the\nimage generators. Tan et al. [46] exploited the artifacts that\narise from up-sampling operations, based on the fact that\nmost popular generator architectures include up-sampling\noperations. Chai et al. [4] tried to restrict the receptive field\nto emphasize local texture artifacts.\nWe design a simple yet powerful general AI-generated\nimage detector that utilizes the feature space of the large\npre-trained Vision Language Model. We apply image ref-\normation to capture not only global semantic artifacts but\nlocal texture artifacts from the input images, ensuring de-\ntection performance and generalizability on unseen genera-\ntors.\n",
  "metadata": {
    "source_path": "papers/arxiv/SFLD_Reducing_the_content_bias_for_AI-generated_Image_Detection_519a1b9eda2ef629.pdf",
    "content_hash": "519a1b9eda2ef629528381fc4d4ede0ecfadabf69bdfea4b4e3a9be83b5ceee3",
    "arxiv_id": null,
    "title": "SFLD_Reducing_the_content_bias_for_AI-generated_Image_Detection_519a1b9eda2ef629",
    "author": "",
    "creation_date": "D:20250225024513Z",
    "published": "2025-02-25T02:45:13",
    "pages": 15,
    "size": 12639746,
    "file_mtime": 1740470185.4391394
  }
}