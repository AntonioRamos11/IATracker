{
  "text": "ENACT-Heart – ENsemble-based Assessment Using\nCNN and Transformer on Heart Sounds\nJiho Han1\nIndustrial AI Lab\nSimPlatform Co. Ltd. Affiliate Research Institute\nGeumcheon-gu, Seoul, Republic of Korea\njihohan@simplatform.com\nAdnan Shaout\nDepartment of Electrical and Computer Engineering\nThe University of Michigan – Dearborn\nDearborn, MI, USA\nshaout@umich.edu\nFig. 1: The pipeline of our ENACT-Heart consists of three core steps: data augmentation, expert analysis of each modality, and\nanalysis fusion. 1) Data augmentation through Gaussian Noise allows increased variability and generalization of the overall\nmodel. 2) Spectrogram analysis is performed through ViT, and audiovisual diagram analysis is done through CNN, allowing\neach model to leverage its strengths in feature extraction for different modalities.\nAbstract— This study explores the application of Vision\nTransformer (ViT) principles in audio analysis, specifically fo-\ncusing on heart sounds. This paper introduces ENACT-Heart –\na novel ensemble approach that leverages the complementary\nstrengths of Convolutional Neural Networks (CNN) and ViT\nthrough a Mixture of Experts (MoE) framework, achieving a\nremarkable classification accuracy of 97.52%. This outperforms\nthe individual contributions of ViT (93.88%) and CNN (95.45%),\ndemonstrating the potential for enhanced diagnostic accuracy\nin cardiovascular health monitoring. These results demonstrate\nthe potential of ensemble methods in enhancing classification\nperformance for cardiovascular health monitoring and diagnosis.\nIndex\nTerms—audio\nvisualization,\ncardiology,\nensemble\nmethod, mixture of experts, spectrogram, transformer\n1Jiho Han was with the Department of Computer Science and Engineering,\nThe University of Michigan - Dearborn, Dearborn, MI, USA during the study.\nI. INTRODUCTION\nCardiac diagnostics have undergone remarkable advance-\nments over the centuries, yet the analysis of heart sounds\nremains a fundamental aspect of assessing cardiovascular\nhealth. These sounds, primarily associated with the closure\nof heart valves, offer crucial insights into cardiac function.\nThe first heart sound (S1), produced by the closure of the\natrioventricular valves, and the second heart sound (S2), asso-\nciated with the closure of the semilunar valves, are commonly\nrecognized as the characteristic ’lub’ and ’dub.’ In a healthy\nheart, these sounds provide clear indications of proper valve\nfunction. Over time, the practice of analyzing these sounds\nhas become an indispensable non-invasive tool in medical\ndiagnostics, offering a reliable means to detect abnormalities\nand evaluate cardiac performance.\narXiv:2502.16914v1  [cs.SD]  24 Feb 2025\n\nA. Anomalies in the Heart Sounds\nHeart murmurs, extra heart sounds, and extrasystoles are\ncommon anomalies detected during cardiac auscultation. Each\ntype of anomaly provides valuable information about potential\nunderlying cardiac conditions.\nHeart murmurs are among the most common anomalies\ndetected through auscultation. Produced by turbulent blood\nflow strong enough to generate audible noise, these ”whoosh-\ning” sounds can be heard in various scenarios, including some\nhealthy individuals. While innocent murmurs, also known as\nfunctional or benign murmurs, are typically harmless and\nnot associated with structural heart abnormalities, pathologic\nmurmurs indicate underlying conditions such as valve defects,\ncongenital heart defects, or abnormal blood flow patterns.\nExtra Heart Sounds refer to additional heart sounds be-\nyond the normal ”lub-dub” pattern. These may manifest as\n”lub-lub dub” or ”lub dub-dub” sequences. Extra heart sounds\ncan sometimes indicate underlying conditions, although they\nmay also occur in healthy individuals. Detection of these\nsounds is crucial as they may not be easily identified through\nother diagnostic tools like ultrasound.\nExtrasystole involves irregular heart rhythms, typically\npresenting as extra or skipped heartbeats. These can be heard\nas sequences such as ”lub-lub dub” or ”lub dub-dub.” While\nextrasystoles may be benign, they can also signify underlying\nheart disease, making early detection important for effective\ntreatment.\nA detailed explanation of these anomalies can be found in\nTable I.\nB. Organization of the Paper\nThis paper is organized as follows: Section II covers the\nbackground of the study, providing the necessary theoretical\nfoundation. III provides an in-depth review of related work,\ndiscussing the advancements and methodologies in cardiac\nsound analysis and the integration of machine learning models\nin cardiovascular diagnostics. Section IV details the proposed\napproach, including the data preprocessing techniques, the\ngeneration of audiovisual data, and the model architectures.\nSection V describes the experimental setup, including the\ndataset used, training procedures, and evaluation metrics. The\nresults of the experiments are presented in VI, highlighting the\nperformance of individual models and the ensemble method.\nFinally, Section VII concludes the paper with a summary\nof findings, potential implications for clinical practice, and\ndirections for future research.\nC. Contributions\nThe primary impacts of the proposed experiment are the\nfollowing:\n• We developed ENACT-Heart (See Fig. 1) – a novel\ntransformer-based ensemble method specifically designed\nfor diagnosing medical audio data through advanced visu-\nalization techniques. ENACT-Heart demonstrates state-of-\nthe-art performance, surpassing other existing ensemble\nmethods in the field.\nTABLE I: Types of anomalies in heart sound [1].\nCategory\nDescription\nNormal\n• Healthy heart sounds with a clear ”lub dub” pattern.\n• May contain background noises and occasional\nrandom noise.\nMurmur\n• Abnormal heart sounds with a ”whooshing, roaring,\nrumbling, or turbulent fluid” noise between ”lub”\nand ”dub”, or between ”dub” and ”lub”.\n• ”Lub” and ”dub” are still present.\n• Murmurs do not occur directly on ”lub” or ”dub”.\nExtra Heart\nSound\n• Additional heart sounds such as ”lub-lub dub” or\n”lub dub-dub”.\n• May or may not be a sign of disease.\n• Important to detect as it may not be detected well\nby ultrasound.\nExtrasystole\n• Heart sounds out of rhythm involving extra or\nskipped heartbeats, such as ”lub-lub dub” or ”lub\ndub-dub”.\n• May or may not be a sign of disease.\n• Treatment is likely to be more effective if diseases\nare detected earlier.\n• We explored the feasibility of employing the Mixture of\nExperts (MoE) approach across different AI architectures,\ncombining CNN and ViT. This integration effectively\nleverages multiple image modalities derived from the\nsame input data, enhancing diagnostic accuracy.\n• We applied a ViT model to medical time-series sound\ndata by converting it into audiovisual representations.\nThis innovative approach, still an active area of research,\nopens new avenues for analyzing and interpreting com-\nplex medical signals.\nII. HISTORICAL BACKGROUND\nThe diagnosis of heart conditions dates back to the early\ndays of medicine, where physicians relied on palpation and\npulse assessment to detect abnormalities. A significant break-\nthrough occurred in the 1700s when Jean Baptiste de Senac,\nphysician to King Louis XV of France, established the connec-\ntion between atrial fibrillation and mitral valve disease. Senac’s\nwork laid the foundation for cardiology as a distinct field of\nstudy [2].\nThe invention of the stethoscope by Ren´e Laennec in 1816\nmarked a pivotal moment in cardiac diagnostics. Laennec\nintroduced the technique of ”mediate auscultation” using his\nnewly created paper acoustic device, allowing for more ac-\ncurate detection of heart sounds and abnormalities [3]. This\ninnovation remains a cornerstone in the history of cardiology.\nThese early diagnostic methods, based on manual interpre-\ntation of heart sounds, evolved significantly over the centuries.\nWith technological advancements, traditional auscultation has\nbeen augmented by electrocardiograms (ECGs) and other\nimaging modalities. In recent years, the integration of artificial\nintelligence (AI) and machine learning has opened new av-\nenues for the analysis of heart sounds, leading to more precise\nand efficient diagnostic tools [4], [5].\n\nIII. RELATED WORKS\nA. Thematically Related Works\nThematically, the proposed method lies in the field of\ncomputer-assisted diagnosis (CAD) systems for heart disease.\nCAD systems for heart diseases utilize computation techniques\nsuch as machine learning, pattern recognition, and AI to\nanalyze cardiac data and provide decision-support tools for\nhealthcare providers.\nThere have been numerous attempts to apply CAD systems\nfor heart diseases in diverse modalities, including but not\nlimited to ECG, cardiac CT/MRI, etc. These systems analyze\ncardiac data to identify abnormalities and patterns - especially\nindicators of certain heart diseases. However, the direction of\nthe majority of these researches are pointed mainly toward\ncomputer vision over audio AI, mainly due to the advanced\ndeep learning models available.\nFor some researches that emphasized sound classification,\nits methodologies have varied slightly from the approach\nproposed in this study. For instance, Jumphoo et al. utilized\na CNN for feature extraction and Data-efficient Image Trans-\nformer (DieT), a variant of the ViT model, for classification\ntasks through stacking [6]. Another heart sound classification\nmodel, proposed by Liu et al., also uses ViT for classification\nbut employs a different image modality called bispectral\npatterns and relies solely on ViT without integrating other\nmodels [7]. While these studies highlight the effectiveness of\nViT individually, they do not explore the potential benefits of\nusing an ensemble approach.\nOverall, the integration of multiple distinct AI models and\nmodalities from the same sound inputs, as proposed in the\nENACT-Heart using an MoE approach, has not been attempted\nyet. This novel methodology leverages the strengths of both\nViT and CNN models, potentially offering a more robust and\naccurate solution for heart sound classification.\nB. Methodologically Related Works\nThe use of computer vision as a tool for machine hearing\nis an emerging approach. There have been attempts to use\ncomputer vision techniques as a method of machine hearing -\nanalyzing audio signals by treating them as visual data.\nHsu et al. [8] presented a deep learning-based music clas-\nsification through mel-spectrogram and Fourier tempogram\nfeatures. Although the concept of using multiple different\naudiovisual modalities and models from singular sound data\nis there, the paper employed the short-chunk CNN + ResNet\nas the backbone architecture of their models.\nIV. PROPOSED APPROACH\nThe choice of heart sound analysis in this study is driven by\nits unique diagnostic value, which complements other modal-\nities such as ECG. Despite the advent of modern diagnostic\ntechniques and sophisticated imaging modalities, cardiac aus-\ncultation and heart sounds remain invaluable diagnostic tools.\nWhile ECG is widely regarded as the gold standard for diag-\nnosing cardiac rhythm disorders and ischemic heart disease, it\nmay not capture certain aspects of cardiac function that heart\nsound analysis can, such as detecting murmurs, rubs, and other\nabnormal heart sounds indicative of structural abnormalities\nlike valvular heart diseases or ventricular hypertrophy. There-\nfore, heart sound analysis provides additional, complementary\ninformation that can enhance diagnostic accuracy.\nResearches has demonstrated that it is possible to process\nspectrograms from audio data as images and apply computer\nvision algorithms such as CNN [9]–[11]. The core problem of\nthe current approaches in using regular CNN-based computer\nvision methods on audio spectrogram representation lies in\nthe distinctiveness of the spectrogram in comparison to other\nimage data.\nVisual transformers leverage attention mechanisms to cap-\nture dependencies between different parts of the input data.\nThis allows them to model long-range dependencies more\neffectively than traditional CNNs, whose feature extraction is\nlimited to local receptive fields. By aggregating information\nfrom across the entire spectrogram, transformers can show a\nglobal contextual understanding of the audio signal, enabling\nthem to capture non-local dependencies and extract meaningful\nfeatures from spectrograms.\nA. Spectral Data Visualization & Analysis\nIn spectral visualization and analysis, researchers employ\nvarious techniques to gain insights into the frequency content\nof signals. These methodologies enable the examination of\nhow frequencies evolve over time, providing valuable infor-\nmation for tasks such as audio processing, speech recognition,\nand biomedical signal analysis.\nSpectrogram. Spectrograms stand as one of the primary\ntools in spectral visualization. They offer detailed represen-\ntations of frequency spectra over time, revealing how the\nfrequency composition of a signal changes temporally. By\nplotting frequency on the vertical axis, time on the horizontal\naxis, and intensity or magnitude using color or brightness,\nspectrograms provide a comprehensive view of signal dy-\nnamics. This detailed visualization allows analysts to identify\nspecific features, patterns, and transient events within the\nsignal, making spectrograms invaluable for tasks requiring\nfine-grained temporal frequency analysis.\nSpectral Centroid. In contrast to the detailed temporal-\nfrequency mapping provided by spectrograms, spectral cen-\ntroids offer a simplified summary of a signal’s frequency\ncontent. The spectral centroid indicates the ”center of mass”\nor average frequency of a signal within each time frame.\nThis single-value representation reduces the complexity of the\ndata while still providing a concise summary of the signal’s\nfrequency characteristics. Spectral centroids are particularly\nuseful for enhancing computational efficiency and maintaining\nrobustness against noise and variations in the signal. However,\nthey lack the detailed temporal information that spectrograms\nprovide.\nThe synergy in using spectrograms and spectral centroids\nwith different models lies in their ability to capture distinct and\ncomplementary features of audio signals. Spectrograms pro-\nvide a comprehensive visualization of the frequency content\n\nover time, highlighting complex, high-dimensional patterns. In\ncontrast, spectral centroids and waveforms represent simpler,\nmore repetitive features, which are well-suited to the strengths\nof CNNs in learning local patterns through convolution and\npooling operations.\nBy employing a MoE approach, the proposed model ef-\nfectively combines these diverse representations. The spec-\ntrograms allow the model to capture detailed, global time-\nfrequency information, while the spectral centroids and wave-\nforms facilitate the extraction of robust, localized features.\nThis integration leverages the strengths of both ViT and CNNs,\nresulting in a more accurate and holistic analysis of heart\nsounds.\nB. MoE\nMoE is a powerful ensemble learning methodology used in\nmachine learning and statistical modeling. Within ensemble\nmethods, multiple models are combined to improve predictive\nperformance compared to any individual model. MoE takes\nthis concept a step further by combining various models and\nadjusting the weight of their contributions adaptively per the\ninput data.\nIn MoE, the ”experts” are individual models or learners,\neach specializing in a particular region of the input space\nor addressing specific patterns in the data. These experts\nmake predictions independently based on their specialized\nknowledge. The key innovation of MoE lies in the gating\nnetwork, which dynamically selects the most relevant expert\nor combination of experts for each input instance.\nThe gating network, often implemented as a neural network,\nlearns to assign weights to the experts based on the input data.\nThese weights determine the contribution of each expert to the\nfinal prediction. By adaptively combining the predictions of\nmultiple experts, MoE can capture complex relationships in the\ndata and achieve superior predictive performance compared to\ntraditional ensemble methods. The flowchart of the proposed\nexperiment, depicted in Figure 2, illustrates the entire process,\nfrom input data processing to the final output generated by the\nMoE.\nV. EXPERIMENTS\nIn this section, we detail the experimental setup used to\nevaluate the performance of the ENACT-Heart. The overall\nworkflow of this process is summarized in Fig. 2, which\noutlines the key steps from data preparation to the final\nensemble prediction.\nA. Dataset Used\nThe heart sound dataset provided in the PASCAL Classify-\ning Heart Sounds Challenge was used for training and testing\nthe models [12]. The audio files in the dataset vary in length,\nranging from 1 second to 30 seconds.\nB. Data Preprocessing\nCreating Dataframe. The PASCAL dataset consists of two\nmain folders, each containing labeled audio files. These folders\nFig. 2: Overview of the ENACT-Heart\nWorkflow. The\nflowchart outlines the data preparation, model training, and\nensemble process used to combine ViT and CNN predictions.\nwere combined into a single dataframe, which includes the file\npaths and corresponding labels. An exploratory data analysis\n(EDA) was performed to understand the distribution of classes\nand identify any potential issues such as missing or mislabeled\ndata.\nNormalizing. To address the inconsistent length of audio\nrecordings, a preprocessing step was implemented. Each audio\nfile was segmented into 5-second clips. If an audio file was\nshorter than 5 seconds, it was zero-padded to meet the required\ninput length.\nData Augmentation. To increase the dataset size and\nintroduce variability, Gaussian noise was added to the audio\nfiles, generating 9 additional noisy versions for each original\nfile. Gaussian noise with a mean of 0 and a standard deviation\nof 0.1 was added to the audio files to generate augmented\ndata. Each audio file was segmented into 5-second clips to\nstandardize the input length for model training. This resulted in\na total of 10 versions per audio file (1 original + 9 augmented).\nThe augmented audio data was included in the dataframe, and\ncorresponding labels were updated to reflect the augmentation\nprocess.\nC. Audiovisual Image Data Generation\nTwo types of visual representations were generated for each\naudio file: spectrograms and centroid graphs.\nSpectrogram. Spectrograms were generated to visualize the\nfrequency content of the audio files over time. Given the\npresence of various background noises in real-world condi-\ntions, a low-pass filter set at 195 Hz was applied. This filter\nhelps to emphasize the cardiac sounds, which predominantly\noccur in the lower frequency range, while reducing noise from\n\nhigher frequencies. The spectrogram images were saved with\ndimensions that matched the input requirements of the models.\nCentroid of Amplitude Visualization. The spectral cen-\ntroid, representing the ”center of mass” of the spectrum, was\ncalculated for each audio file. This metric provides a concise\nrepresentation of where the majority of the spectral energy\nis concentrated. A centroid graph was generated, overlaying\nthe normalized waveform and centroid values. This combined\nvisualization provided a robust input for the CNN model. The\npurpose of this approach was twofold:\n1) The waveform contains all the detailed information of\nthe audio signal, capturing every nuance and variation.\n2) The spectral centroid highlights the important features\nof the signal by indicating where the audio information\nis concentrated (i.e., beats). This simplification helps the\nCNN model to more easily pick out relevant patterns, en-\nhancing its ability to classify the recordings accurately.\nD. Model Training\nViT. The ViT model was trained using the generated\nspectrogram images. The training process involved splitting\nthe data into training and validation sets, followed by model\ntraining with appropriate hyperparameters such as batch size\nof 32 and 50 epochs, using an Adam optimizer with a learning\nrate of 0.001. Data augmentation techniques, such as random\nnoise addition, were applied to improve model robustness.\nCNN. The CNN model was trained using the centroid\ngraphs. The CNN model architecture included three convolu-\ntional layers followed by max-pooling and dropout layers. The\ndata was split into training and validation sets, and the model\nwas trained with optimized hyperparameters, including a batch\nsize of 32 and 50 epochs, using an Adam optimizer with a\nlearning rate of 0.001. The CNN model also benefited from the\ndata augmentation techniques applied during preprocessing.\nE. Ensemble Method\nTo leverage the strengths of both the ViT and CNN models,\nthe MoE ensemble method was employed. The ensemble\nmodel combined the predictions from both models by assign-\ning different weights, wViT and wCNN, to each model’s predic-\ntions. Specifically, weight combinations were systematically\ntested, with wViT ranging from 0 to 1 in increments of 0.05,\nand wCNN = 1 −wViT.\nThe ensemble prediction Pe was calculated using the fol-\nlowing equation:\nPensemble = wViT × PViT + wCNN × PCNN,\nwhere wViT = 0.05k\nwCNN = 1 −wViT\nk ∈[0, 20] ∩Z\n(1)\nVI. RESULTS\nA. Individual Model Performance\nIn this study, we evaluated the performance of two state-\nof-the-art models, ViT and CNN, on a dataset of heart sound\nrecordings. The goal was to classify the recordings into five\ncategories: artifact, extrahls, extrastole, murmur, and normal.\nViT. Although the ViT model demonstrated strong per-\nformance across most classes, it was outperformed by the\nCNN in several key areas. This discrepancy is primarily\ndue to the repetitive nature of heart sounds, which consist\nof recurring local patterns that CNNs are particularly adept\nat capturing and analyzing. Consequently, CNN’s ability to\neffectively recognize these local patterns contributed to its\nsuperior performance in this context. This, however, doesn’t\nmean the usage of ViT model is futile, as ViT might have\nidentified characteristics that is not evident through the CNN\nmodel.\nCNN. The CNN model, on the other hand, achieved higher\noverall precision and demonstrated more balanced perfor-\nmance across all classes. This proves the point mentioned\nearlier: CNN on centroids can provide a more robust model\nin comparison to ViT.\nB. Ensemble Model Performance\nTo leverage the strengths of both models, we implemented\nan ensemble method by combining the predictions of the ViT\nand CNN models using a Mixture of Experts approach. The\nensemble was created using an additive weighted approach,\nwhere different weights were tested to find the optimal com-\nbination.\nENACT-Heart achieved the highest accuracy of 97.52%,\nsignificantly outperforming both individual models. The im-\nprovement in accuracy demonstrates the effectiveness of the\nensemble approach, particularly in enhancing the model’s ro-\nbustness and generalization capabilities. The ensemble method\neffectively combined the strengths of both ViT and CNN. The\neffectiveness of the ENACT-Heart compared to the individual\nViT and CNN models is illustrated in Table II. Additionally,\nthe performance of the ENACT-Heart , as illustrated by its\nconfusion matrix in Fig. 3, further demonstrates its statistics\nover different types of diseases.\nC. Comparison with State-of-the-Art\nThe proposed ENACT-Heart demonstrates competitive per-\nformance when compared to other state-of-the-art models in\nheart sound classification. Utilizing a MoE ensemble approach\nthat integrates ViT and CNN, the ENACT-Heart achieves an\nimpressive accuracy of 97.52%. This surpasses the accuracy\nof individual ViT (93.88%) and CNN (95.45%) models. Ad-\nditionally, the ENACT-Heart maintains high precision (0.98),\nrecall (0.97), and F1-score (0.98), indicating a balanced per-\nformance across various metrics.\nIn comparison, other notable studies in the field exhibit\nslightly different accuracies. For instance, the work by Liu\net al. [7] utilizing bispectrum features and ViT reported an\naccuracy of 91%, while Yang et al. [13] achieved an accuracy\n\nTABLE II: Comparison of ENACT-Heart with ViT and CNN on the pre-processed PASCAL dataset [12].\nClass\nViT\nCNN\nENACT-Heart\nPrecision\nRecall\nF1-Score\nPrecision\nRecall\nF1-Score\nPrecision\nRecall\nF1-Score\nartifact\n0.80\n0.96\n0.87\n0.92\n0.93\n0.92\n0.93\n0.97\n0.95\nextrahls\n0.73\n0.65\n0.69\n0.86\n0.67\n0.76\n0.89\n0.74\n0.81\nextrastole\n0.98\n0.84\n0.91\n0.98\n0.87\n0.92\n1.00\n0.91\n0.96\nmurmur\n0.94\n0.97\n0.96\n0.95\n0.97\n0.96\n0.98\n0.99\n0.99\nnormal\n0.99\n0.95\n0.97\n0.97\n0.98\n0.98\n0.98\n0.99\n0.99\naccuracy\n0.94\n0.95\n0.98\nmacro avg\n0.89\n0.88\n0.88\n0.94\n0.89\n0.91\n0.96\n0.92\n0.94\nweighted avg\n0.94\n0.94\n0.94\n0.95\n0.95\n0.95\n0.97\n0.98\n0.97\nFig. 3: Confusion matrix of the performance of\nENACT-Heart\nof 98.74% using a combination of Transformer and CNN\nmodels. Similarly, Wang et al. [14] presented the PCTMF-Net\nmodel, which recorded an accuracy of 99.36% on the Yansen\ndataset but only 93% on the PhysioNet Challenge dataset,\nhighlighting variability across different datasets. Furthermore,\nJumphoo et al. [6] reported an accuracy of 99.44% with Conv-\nDeiT, and their precision, recall, and F1-score metrics are\ncomparable to those of the ENACT-Heart.\nAs summarized in Table III, while some studies report\nhigher accuracies, the balanced performance of ENACT-\nHeart across multiple metrics underscores its reliability and\nrobustness in heart sound classification tasks. The high ac-\ncuracy and comprehensive performance metrics indicate the\npotential of the MoE ensemble method in enhancing diagnostic\naccuracy and reliability in cardiovascular health monitoring.\nVII. CONCLUSION AND DISCUSSION\nIn conclusion, the combination of ViT and CNN models\nusing an ensemble method improved the classification perfor-\nmance in every aspect in general. This study highlights the\nimportance of evaluating individual models to identify their\nTABLE III: Comparison of other State-of-the-Art MoE heart\nsound classification studies.\nStudy\nAuthors\nModel(s) Used\nMetrics\nENACT-Heart\n(Proposed Model)\nJ. Han,\nA. Shaout\nMoE\n(Ensemble of\nViT & CNN)\nAccuracy: 0.9752\n(PASCAL DB)\nPrecision: 0.98\nRecall: 0.97\nF1-Score : 0.98\nHeart sound\nclassification based on\nbispectrum features and\nVision Transformer\nmodel (Nov. 2023) [7]\nZ. Liu, H. Jiang,\nF. Zhang,\nW. Ouyang,\nX. Li\nViT, CNN\nAccuracy: 0.91\nAUC: 0.98\nAssisting Heart Valve\nDiseases Diagnosis via\nTransformer-Based\nClassification\n(May. 2023) [13]\nD. Yang, Y. Lin,\nJ. Wei, X. Lin,\nX. Zhao, Y. Yao\nTransformer,\nCNN\nAccuracy: 0.9874\nAUC: 0.99\nPCTMF-Net: heart\nsound classification with\nparallel\nCNNs-transformer and\nspectral analysis\n(Jul. 2023) [14]\nR. Wang,\nY. Duan,\nY. Li, D. Zheng,\nX. Liu, C.T. Lam\nCNN,\nTransformer\nAccuracy: 0.9936\n(Yansen Dataset)\n0.93 (PhysioNet\nChallenge)\nExploring Data-Efficient\nImage\nTransformer-based\nTransfer Learning\n(Jan. 2024) [6]\nT. Jumphoo,\nK.\nPhapatanaburi,\nW. Pathonsuwan\nConv-DeiT\nAccuracy: 0.9944\nPrecision: 0.9852\nRecall: 0.9854\nF1-Score: 0.9851\nHeart Sound\nClassification Network\nBased on Convolution\nand Transformer (Aug.\n2023) [15]\nJ. Cheng,\nK. Sun\nCNN,\nTransformer\nAccuracy: 0.964\n0.997\n0.957 (3 distinct\ndataset)\nMulti-classification\nneural network model\nfor detection of\nabnormal heartbeat\naudio signals\n(Jul. 2022) [1]\nH. Malik,\nU. Bashir,\nA. Ahmad\nRNN\nLSTM\nAccuracy: 0.99771\n(PASCAL DB)\n0.9870 (PhysioNet\nChallenge)\nstrengths and the potential benefits of using ensemble methods\nto achieve superior results.\nThe data augmentation techniques employed also played\na key role in enhancing model robustness and performance.\nThese findings can inform future research and development of\nadvanced classification systems in the medical field.\nThe proposed ENACT-Heart model demonstrates significant\npromise in the field of heart sound classification, particularly\nwhen considered alongside the advancements in smart wear-\nable devices. With the proliferation of wearable technology,\nthe collection and analysis of audio data have become more\naccessible and widespread. This is especially pertinent in the\nmedical field, where heart sound data can be continuously\nmonitored and analyzed in real-time, offering invaluable in-\n\nsights into a patient’s cardiovascular health.\nMoreover, from a practical point of view, the advancement\nof smart wearable devices presents a significant opportunity for\nimproving healthcare accessibility, especially in low-resource\nsettings. In many developing countries, access to advanced\nmedical diagnostics is limited due to the lack of infrastruc-\nture and trained healthcare professionals. Wearable devices\nequipped with advanced models like ENACT-Heart can bridge\nthis gap by enabling non-invasive, continuous monitoring of\nheart health, thus providing timely and accurate diagnostics\nwithout the need for expensive and bulky equipment.\nThis technology can revolutionize the practice of medicine\nin poorer regions, making high-quality healthcare more ap-\nproachable and affordable. The ability to monitor and ana-\nlyze heart sounds continuously can lead to early detection\nof cardiovascular issues, prompt intervention, and ultimately,\nbetter health outcomes. As wearable devices become more\naffordable and their usage more prevalent, the integration of\nsophisticated models like ENACT-Heart can play a crucial\nrole in democratizing access to advanced medical diagnostics\nglobally.\nIn summary, the synergy between the ENACT-Heart model\nand smart wearable technology holds great potential for en-\nhancing healthcare delivery, particularly in underserved re-\ngions. By providing a reliable and efficient means of heart\nsound classification, this approach not only advances the field\nof medical diagnostics but also contributes to the broader goal\nof equitable healthcare access.\nREFERENCES\n[1] H. Malik, U. Bashir, and A. Ahmad, “Multi-classification neural network\nmodel for detection of abnormal heartbeat audio signals,” Biomedical\nEngineering Advances, vol. 4, p. 100048, Dec. 2022. [Online]. Available:\nhttps://www.sciencedirect.com/science/article/pii/S266709922200024X\n[2] J. McMichael, “History of atrial fibrillation 1628-1819 Harvey - de\nSenac - La¨ennec,” Br Heart J, vol. 48, no. 3, pp. 193–197, Sep. 1982.\n[3] R. T. H. Laennec, A Treatise on the Diseases of the Chest and on Mediate\nAuscultation.\n[4] M. N. Esbin, O. N. Whitney, S. Chong, A. Maurer, X. Darzacq, and\nR. Tjian, “Overcoming the bottleneck to widespread testing: a rapid\nreview of nucleic acid testing approaches for COVID-19 detection,”\nRNA, vol. 26, no. 7, pp. 771–783, Jul. 2020, company: Cold Spring\nHarbor Laboratory Press Distributor: Cold Spring Harbor Laboratory\nPress Institution: Cold Spring Harbor Laboratory Press Label: Cold\nSpring Harbor Laboratory Press Publisher: Cold Spring Harbor Lab.\n[Online]. Available: http://rnajournal.cshlp.org/content/26/7/771\n[5] S.-C.\nHuang,\nA.\nPareek,\nS.\nSeyyedi,\nI.\nBanerjee,\nand\nM.\nP.\nLungren, “Fusion of medical imaging and electronic health records\nusing\ndeep\nlearning:\na\nsystematic\nreview\nand\nimplementation\nguidelines,” npj Digit. Med., vol. 3, no. 1, pp. 1–9, Oct. 2020,\npublisher:\nNature\nPublishing\nGroup.\n[Online].\nAvailable:\nhttps:\n//www.nature.com/articles/s41746-020-00341-z\n[6] T.\nJumphoo,\nK.\nPhapatanaburi,\nW.\nPathonsuwan,\nP.\nAnchuen,\nM. Uthansakul, and P. Uthansakul, “Exploiting Data-Efficient Image\nTransformer-Based Transfer Learning for Valvular Heart Diseases\nDetection,” IEEE Access, vol. 12, pp. 15 845–15 855, 2024, conference\nName: IEEE Access. [Online]. Available: https://ieeexplore.ieee.org/\nabstract/document/10413456\n[7] Z. Liu, H. Jiang, F. Zhang, W. Ouyang, X. Li, and X. Pan,\n“Heart sound classification based on bispectrum features and Vision\nTransformer mode,” Alexandria Engineering Journal, vol. 85, pp.\n49–59, Dec. 2023. [Online]. Available: https://www.sciencedirect.com/\nscience/article/pii/S1110016823010128\n[8] W.-H. Hsu, B.-Y. Chen, and Y.-H. Yang, “Deep Learning Based\nEDM Subgenre Classification using Mel-Spectrogram and Tempogram\nFeatures,” Oct. 2021, arXiv:2110.08862 [cs, eess]. [Online]. Available:\nhttp://arxiv.org/abs/2110.08862\n[9] P. Verma and J. O. Smith, “Neural Style Transfer for Audio\nSpectograms,”\nJan.\n2018,\narXiv:1801.01589\n[cs,\neess].\n[Online].\nAvailable: http://arxiv.org/abs/1801.01589\n[10] A. A. Cabrera-Ponce, J. Martinez-Carranza, and C. Rascon, “Detection\nof\nnearby\nUAVs\nusing\na\nmulti-microphone\narray\non\nboard\na\nUAV,” International Journal of Micro Air Vehicles, vol. 12, p.\n175682932092574,\nJan.\n2020.\n[Online].\nAvailable:\nhttp://journals.\nsagepub.com/doi/10.1177/1756829320925748\n[11] R. Hyder, S. Ghaffarzadegan, Z. Feng, J. Hansen, and T. Hasan, “Acous-\ntic Scene Classification Using a CNN-SuperVector System Trained with\nAuditory and Spectrogram Image Features,” Aug. 2017, pp. 3073–3077.\n[12] P. Bentley, G. Nordehn, M. Coimbra, and S. Mannor, “The PASCAL\nClassifying Heart Sounds Challenge 2011 (CHSC2011) Results.”\n[Online]. Available: http://www.peterjbentley.com/heartchallenge/index.\nhtml\n[13] D. Yang, Y. Lin, J. Wei, X. Lin, X. Zhao, Y. Yao, T. Tao,\nB. Liang, and S.-G. Lu, “Assisting Heart Valve Diseases Diagnosis\nvia\nTransformer-Based\nClassification\nof\nHeart\nSound\nSignals,”\nElectronics, vol. 12, no. 10, p. 2221, May 2023. [Online]. Available:\nhttps://www.mdpi.com/2079-9292/12/10/2221\n[14] R.\nWang,\nY.\nDuan,\nY.\nLi,\nD.\nZheng,\nX.\nLiu,\nC.\nT.\nLam,\nand T. Tan, “PCTMF-Net: heart sound classification with parallel\nCNNs-transformer and second-order spectral analysis,” Vis Comput,\nvol. 39, no. 8, pp. 3811–3822, Aug. 2023. [Online]. Available:\nhttps://doi.org/10.1007/s00371-023-03031-5\n[15] J. Cheng and K. Sun, “Heart Sound Classification Network Based\non Convolution and Transformer,” Sensors, vol. 23, no. 19, p. 8168,\nSep. 2023. [Online]. Available: https://www.mdpi.com/1424-8220/23/\n19/8168\n",
  "metadata": {
    "source_path": "papers/arxiv/ENACT-Heart_--_ENsemble-based_Assessment_Using_CNN_and_Transformer_on\n__Heart_Sounds_ebe9b7ef236a8abb.pdf",
    "content_hash": "ebe9b7ef236a8abb93af5d581f41a79d92f7a743fad1fc925d8309e0eeb1e16c",
    "arxiv_id": null,
    "title": "ENACT-Heart_--_ENsemble-based_Assessment_Using_CNN_and_Transformer_on\n__Heart_Sounds_ebe9b7ef236a8abb",
    "author": "",
    "creation_date": "D:20250225022938Z",
    "published": "2025-02-25T02:29:38",
    "pages": 7,
    "size": 632626,
    "file_mtime": 1740470205.3964713
  }
}