{
  "text": "Convergence of Shallow ReLU Networks\non Weakly Interacting Data\nLéo Dana1, Loucas Pillaud-Vivien2, and Francis Bach1\n1Sierra, Inria.\n2CERMICS, ENPC.\nAbstract\nWe analyse the convergence of one-hidden-layer ReLU networks trained by gradi-\nent flow on n data points. Our main contribution leverages the high dimensionality\nof the ambient space, which implies low correlation of the input samples, to demon-\nstrate that a network with width of order log(n) neurons suffices for global conver-\ngence with high probability. Our analysis uses a Polyak–Łojasiewicz viewpoint along\nthe gradient-flow trajectory, which provides an exponential rate of convergence of\n1\nn. When the data are exactly orthogonal, we give further refined characterizations\nof the convergence speed, proving its asymptotic behavior lies between the orders\n1\nn and\n1\n√n, and exhibiting a phase-transition phenomenon in the convergence rate,\nduring which it evolves from the lower bound to the upper, and in a relative time\nof order\n1\nlog(n).\n1\nIntroduction\nUnderstanding the properties of models used in machine learning is crucial for provid-\ning guarantees to downstream users. Of particular importance, the convergence of the\ntraining process under gradient methods stands as one of the first issues to address in\norder to comprehend them. If, on the one hand, such a question for linear models and\nconvex optimization problems (Bottou et al., 2018; Bach, 2024) are well understood, this\nis not the case for neural networks, which are the most used models in large-scale ma-\nchine learning. This paper focuses on providing quantitative convergence guarantees for\na one-hidden-layer neural network.\nTheoretically, such global convergence analysis of neural networks has seen two main\nachievements in the past years: (i) the identification of the lazy regime, due to a particular\ninitialization, where convergence is always guaranteed at the cost of being essentially a\nlinear model (Jacot et al., 2018; Arora et al., 2019; Chizat et al., 2019), and (ii) the proof\nthat with an infinite amount of hidden units a two-layer neural network converges towards\nthe global minimizer of the loss (Mei et al., 2018; Chizat and Bach, 2018; Rotskoff and\nVanden-Eijnden, 2018). However, neural networks are trained in practice outside of these\nregimes, as neural networks are known to perform feature learning, and experimentally\nreach global minimum with a large but finite number of neurons. Quantifying in which\nregimes neural networks converge to a global minimum of their loss is still an important\nopen question.\n1\narXiv:2502.16977v1  [stat.ML]  24 Feb 2025\n\nThis article aims to identify such a regime, characterized by low-correlated inputs,\nunder which we can rigorously prove the convergence of shallow neural networks trained\nvia gradient flow. The previous assumptions needed for convergence relied on special\ninitialization scales (Chizat et al., 2019; Boursier et al., 2022), on an infinite number of\nneurons (Jacot et al., 2018; Chizat and Bach, 2018), or on the exact orthogonality of the\ninput data (Boursier et al., 2022; Frei et al., 2023). In this article, we aim to simply\nfix the dimension d, and ask how many randomly sampled examples can be interpolated\nby the network with high probability. This high dimensionality regime, corresponding\nto d being larger than n2, is our main assumption to prove convergence of shallow ReLU\nneural networks. In addition, we are interested in understanding the general dynamics\nof the system, and in particular the system’s convergence speed to a global minimizer of\nthe loss.\nWe summarize our contributions in the analysis of the learning dynamics of a one-\nhidden-layer ReLU network on a finite number of data n via gradient flow.\n• Our main contribution is that shallow neural networks in high dimension d ≥Cn2\ninterpolates exactly random whitened data with high probability as soon as the\nneural network has more that log(n) neurons, for any initialization scale. We also\nshow that the loss converges to zero exponentially fast with a rate at least of order\n1\nn.\n• Then, when the inputs are orthogonal, we refine our analysis in order to characterize\nthe range of possible asymptotic speeds, which we find to be at most of order\n1\n√n.\nMoreover, we conjecture that this speed is always of the highest order\n1\n√n with high\nprobability and verify empirically this claim.\n• Finally, for orthonormal inputs and a special initialization of the network, we high-\nlight a phase transition in the convergence rate during the system’s evolution, and\ncompute the associated cut-off time and transition period.\n2\nProblem Setup\nNotations.\nWe use ||v|| to denote the euclidean norm of a vector v, ⟨·|·⟩its scalar\nproduct, and ||M|| for the operator norm associated with || · || of a matrix M. Moreover,\nlet ¯v =\nv\n||v||.\nLoss function.\nLet (xi, yi)i=1:n ∈(Rd × R)n be a sample of input vectors and real\noutputs.\nLet d ∈N∗be the dimension of the vector space and n ∈N∗the number\nof data points. In order to learn the regression problem of mapping xi to yi, we use\none-hidden-layer ReLU neural networks1, which we write:\nhθ(x) = 1\np\np\nX\nj=1\najσ(⟨wj|x⟩) ,\n(1)\nwhere p ∈N∗is the number of units, σ(x) = max{0, x} for x ∈R is the rectified\nlinear unit (ReLU), and the parameters are gathered in θ = (aj, wj)1≤j≤p ∈(R×Rd)p. To\n1This model can accommodate to having a bias by adding an extra dimension to the data, identical\nfor all xi and small enough to ensure our statements stay true.\n2\n\nsimplify the ReLU notation, we define σ(⟨wj|xi⟩) = ⟨wj|xi⟩+ and 1⟨wj|xi⟩>0 = 1j,i. When\nmentioning neurons of the network, we refer to ⟨wj|xi⟩+, while second layer neurons refer\nto aj. Neurons can be activated if ⟨wj|xi⟩+ > 0, and are correctly activated if moreover\najyi > 0. Upon this prediction class and data, we analyse the regression loss with square\nerror,\nL(θ) := 1\n2n\nn\nX\ni=1\n(yi −hθ(xi))2 .\n(2)\nAs soon as d ≥n, (xi)i=1:n can form a free family, in which case the set of minima of\nL, which consists of all interpolators, is non-empty.\nGradient flow.\nIn order to understand a simplified version of the optimization dynam-\nics of this neural network, we study the continuous-time limit of gradient descent. We\ninitialize θt=0 = θ0 and follow for all t ≥0 the ordinary differential equation\nd\ndtθt = −p∇θtL(θt) ,\n(3)\nwhere we choose a particular element of the sub-differential of the ReLU σ′(x) = 1x>0,\nfor any x ∈R. This choice is motivated by both prior empirical work from Bertoin et al.\n(2021) and theoretical work from Boursier et al. (2022, Proposition 2) and Jentzen and\nRiekert (2023). We also decided to accelerate the dynamics by a factor p as only this\nscaling gives a consistent mean field limit for the gradient flow when the number of\nneurons tends to infinity (see Definition 2.2 by Chizat and Bach (2018)).\nWeight invariance.\nThe 1-homogeneity of the ReLU provides a continuous symmetry\nin the function θ 7→hθ and hence the loss2. This feature is known to lead automati-\ncally to invariants in the gradient flow as explained generally by Marcotte et al. (2024).\nThe following lemma is not new (Wojtowytsch, 2020, p.11), and shows that, from this\ninvariance, we deduce that the two layers have balanced contributions throughout the\ndynamics.\nLemma 1. For all j ∈J1, pK, for all t ≥0, |aj(t)|2 −||wj(t)||2 = |aj(0)|2 −||wj(0)||2 ,\nand thus, if |aj(0)| ≥||wj(0)||, then aj(t) maintains its sign and |aj(t)| ≥||wj(t)||.\nInitialization.\nThroughout the paper, we initialize the network’s weights wj and aj\nfrom a joint distribution where both marginals are non-zero, centered, rotational-invariant,\nhave finite covariance, and we take norms of aj and wj independent of d, n, p. Moreover,\nwe need an assumption of asymmetry of the norm at initialization.\nAssumption 1 (Asymmetric norm initialization). We assume that the weights of the\nnetwork at initialization satisfy ∀j ∈J1, pK, |aj(0)| ≥||wj(0)||.\nArticles by Boursier and Flammarion (2024a,b) already used this assumption to study\ntwo-layer neural networks in order to use the property described in Lemma 13.\n2Indeed, the subspace built from all parameters θγ = ( aj\nγj , γjwj)1≤j≤p, when γ varies in (R∗\n+)p, maps\nto the same network, i.e., hθγ = hθ1.\n3Note that Pytorch’s default initialization does not follow the assumptions of our initialization, in\nparticular since |aj(0)| ≥||wj(0)|| for all j is very unlikely. One way to initialize the network, as we do\nin Section 5, is to first sample wj and then sample aj conditionally on the norm asymmetry.\n3\n\nData.\nWe define the data matrix X = (x1, . . . , xn) ∈Rd×n. Denote C−\nx = mini ||xi||\nand C−\ny = mini |yi|; in what follows, we suppose that C−\nx > 0 and C−\ny > 0, i.e., the input\nand output data are bounded away from the origin. Similarly, we also let C+\nx = maxi ||xi||\nand C+\ny = maxi |yi|. We note C+,−\nx,y\nto refer to the set of these constants. Finally, we\nintroduce the following hypothesis on the low correlation between the inputs.\nAssumption 2 (Low correlated inputs). We assume that the data satisfy\n||XTX −DX|| < (C−\nx )2\n√n\nC−\ny\nC+\ny\n,\n(4)\nwhere DX denotes the diagonal matrix with coefficients ||xi||2.\nThe term ||XTX −DX|| is a control on the magnitude of the correlations (⟨xi, xj⟩)i̸=j.\nAs an extreme case, when it equals zero, the inputs are orthogonal. This assumption\nis purely deterministic at this stage. Later, we show that this weak interaction between\nthe inputs is highly likely to occur for random whitened vectors in high dimensions (see\nCorollary 1).\nDimensions.\nThroughout the paper, even if the results provided are all non-asymptotic\nin nature, the reader can picture that the numbers n, p, d (respectively data, neurons and\ndimension) are all large. Moreover, they verify the following constraint: n is less than d,\nand p can be thought of the order log(n), meaning only a “low” number of neurons is\nrequired.\n2.1\nRelated works\nConvergence of neural networks.\nNeural networks are known to converge under\nspecific data, parameter, or initialization hypotheses, among which: the neural tangent\nkernel regime studied by Jacot et al. (2018); Arora et al. (2019); Du et al. (2019); Allen-\nZhu et al. (2019), that has been shown to correspond in fact to a lazy regime where\nthere is no feature learning because of the initialization scale. Another field of study is\nthe mean-field regime, where feature learning can happen but where the optimization\nhas been shown to converge only in the infinite width case (Mei et al., 2018; Chizat and\nBach, 2018; Rotskoff and Vanden-Eijnden, 2018). Note that it is also possible to produce\ngeneric counter examples, where convergence does not occur (Boursier and Flammarion,\n2024b). Beyond these, there have been attempts to generalize convergence results under\nlocal PL (or local curvature) conditions as shown by Chatterjee (2022); Liu et al. (2022);\nZhou et al. (2021), but they remain unsatisfactory to explain the good general behavior\nof neural networks due to the constraint it imposes on the initialization. Convergence\ntheorems similar in spirit to Theorem 1 can be found in an article by Chen et al. (2022).\nThe main difference relies on two features: only the inner weights are trained and their\nresult necessitates a large value of outer weights when n is large, which is the regime\nof interest of the present article. Finally, it is worth mentioning other works on neural\nnetworks dynamics, e.g., the study of the implicit bias either for regression (Boursier et al.,\n2022) or classification (Lyu and Li, 2020; Ji and Telgarsky, 2020), or sample complexity\nto learn functions in a specific context (Glasgow, 2023).\n4\n\nPolyak-Łojasiewicz properties.\nDating back from the early sixties, Polyak derived a\nsufficient criterion for a smooth gradient descent to converge to a global minimizer (Polyak,\n1964). This corresponds to the later-called Polyak-Łojasiewicz (PL) constant µ of a func-\ntion f : Rd →R+, that can be defined as the best exponential rate of convergence\nof gradient flow over all initializations, or equivalently to the following minimum ratio\nµ = minx∈Rd ||∇f(x)||2\nf(x)\n. This has found many applications in non-convex optimization, as\nit is the case for neural network optimization, and is very popular for optimization in\nthe space of measures (Gentil, 2020). Other notions of PL conditions have emerged in\nthe literature to characterize local convergence, by bounding the PL constant over a ball\nµ∗(z, r) = minx∈B(z,r)\n||∇f(x)||2\nf(x)\n(Chatterjee, 2022; Liu et al., 2022) and comparing it to\nf(z). We use a notion of PL which is local and trajectory-wise to prove lower bounds\nvalid on each trajectory.\n3\nConvergence in high dimension\nIn this first section, our goal is to understand when the gradient flow converges toward\na global minimizer of the loss. Note that the parametrization of the prediction function\nhθ by a neural network often implies the non-convexity of the objective L and prevents\nany direct application of convex tools in order to ensure global convergence. Generally\nspeaking, even if gradient flows are expected to converge to critical points of the param-\neter space (Lee et al., 2016), such that ∇θL(θ) = 0, they might become stuck in local\nminimizers that do not interpolate the data.\n3.1\nLocal PL-curvature\nConvexity is not the only tool that provides global convergence: as known in the op-\ntimization community, showing that\n||∇L(θ)||2\nL(θ)\nis uniformly lower bounded suffices. As\nmentioned in Section 2.1, this is known as the Polyak-Lojasiewicz condition (Polyak,\n1964). Taking a dynamical perspective on this, we define a trajectory-wise notion of this\n“curvature” condition which we name the local-PL curvature of the system, and define\nfor all t ≥0,\nµ(t) := p∥∇L(θt)∥2\nL(θt)\n= −\nd\ndtL(θt)\nL(θt)\n(5)\nwith the second equality being a property of the gradient flow. Intuitively, this coeffi-\ncient describes the curvature in parameter space that θt “sees” at time t ≥0. The following\nlemma is classical and shows how it can be used to prove the global convergence of the\nsystem, as well as a quantification on the rate.\nLemma 2. Let ⟨µ(t)⟩:= 1\nt\nR t\n0 µ(u)du the time average of the local-PL curvature, which\nwe name the average-PL curvature. We have L(θt) = L(θ(0))e−⟨µ(t)⟩t.\nHence, if the total average-PL curvature ⟨µ∞⟩:= limt→∞⟨µ(t)⟩is strictly positive,\nwe can deduce an upper bound on the loss and convergence to 0 at the exponential speed\n⟨µ∞⟩. This shows that the average-PL curvature is actually the instantaneous exponential\ndecay rate of the loss, and thus controls the speed at which the system converges.\n5\n\n3.2\nGlobal convergence of neural networks for weakly correlated\ninputs\nWe are ready to state the main theorem of the paper on the minimization of the loss.\nTheorem 1. Let ε > 0, p ≥4 log(n\nε ), and suppose Assumption 1.\nWe fix the data\n(xi, yi)1≤i≤n and suppose it satisfies Assumption 2. Then with probability at least 1 −ε\nover the initialization of the network, the loss converges to 0 with ⟨µ∞⟩≥C\nn , where we\ndefine C = (C−\nx )2\nC+\nx C−\ny . Moreover, for any t ≥0, we have the lower bound\nµ(t) ≥C\nn min\ni\n\f\f\f\f1 −ri(t)\nyi\n\f\f\f\f .\n(6)\nNote that, at best, the number of neurons required in Theorem 1 is logarithmic. This\nfiniteness stands in contrast with the infinite number required in the mean-field regime,\nand the polynomial dependency typical of the neural tangent kernel (NTK) regime (Jacot\net al., 2018; Allen-Zhu et al., 2019). In the orthogonal case, the ReLU makes the log(n)\ndependency necessary and sufficient, as shown in Lemma 5, as the residual ri goes to zero\nif and only if a neuron gets initialized as ajyi > 0 and ⟨wj|xi⟩> 0 for each i.\nAssumption 2 is crucial for this proof: it means that the examples are insufficiently\ncorrelated with each other for the weights to collapse onto a single direction. As proved\nby Boursier and Flammarion (2024a, Theorem 1), the direction ¯w∗= arg minθ={ ¯w,a} L(θ)\nwill attract all neurons if it is accessible from anywhere on the initialization landscape4.\nThis phenomenon known as early alignment and first described by Maennel et al. (2018),\nwill prevent interpolation if examples are highly correlated (Boursier and Flammarion,\n2024a, Theorem 2).\nThe fact that our result holds for any initialization scale shows\nthat near-orthogonal inputs prevent accessibility to ¯w∗and make the early alignment\nphenomenon benign, as found by Boursier et al. (2022); Frei et al. (2023).\nNote finally that our norm-asymmetric initialization (Assumption 1) is sufficient for\nglobal convergence with high probability, but may not be necessary. That said, we present\nin Appendix C.1 a detailed example of interpolation failure when the assumption is not\nsatisfied.\nConvergence in high dimension.\nIn this paragraph we assume that the data (xi, yi)i=1:n\nare generated i.i.d. from some distribution PX,Y . We first show that, with high probabil-\nity, Assumption 2 is almost always valid if the dimension is larger than the square root\nof the number of data points. Additionally, we assume that the law anti-concentrates at\nthe origin. These two features are gathered in the following lemma.\nLemma 3. Let (xi, yi)1≤i≤n be generated i.i.d. from a probability distribution PX,Y such\nthat the marginal PX is sub-Gaussian, has zero-mean, and satisfy Ex∼PX[xxT] = λ2\nd Id for\nsome λ independent of d, n, while, on R∗, the marginal law PY has compact support.5\nThere exists C > 0 depending only on the constants C+,−\nx,y\nand the initialization weights,\nsuch that, if d ≥C\n\u0000n2 + n log\n\u0000 1\nε\n\u0001\u0001\n, then, with probability 1−ε, Assumption 2 is satisfied.\n4This accessibility condition is in fact the absence of saddle point for some function of normed neurons,\nwhich imply that neurons can rotate from anywhere on the sphere to ¯w∗.\n5Note that this means that PY can have an atom at 0.\n6\n\nThe hypothesis in the previous lemma is satisfied by standard distributions like Gaus-\nsians N(0, 1\ndId) for the inputs. The following corollary restates Theorem 1 for data that\nare generically distributed as in Lemma 3, and when the dimension is large enough.\nCorollary 1. Let ε > 0. Suppose Assumption 1 and that (xi, yi)1≤i≤n are i.i.d. generated\nfrom a probability distribution satisfying the same properties as in Lemma 3. There exists\na constant C > 0 depending only on the constants C+,−\nx,y\nsuch that, if the network has\np ≥8 log\n\u0000 n\nε\n\u0001\nneurons in dimension d ≥C\n\u0000n2 + n log\n\u0000 1\nε\n\u0001\u0001\n, then, with probability at least\n1 −ε over the initialization of the network and the data generation, the loss converges to\n0 at exponential speed of rate at least 1\nn.\nBeyond the high-dimensionality of the inputs, Corollary 1 does not require any initial-\nization specificity (small or large), and the number of neurons required to converge can be\nas low as log(n). Hence, let us put emphasis on the fact that the global nice structure of\nthe loss landscape comes from the high-dimensionality: this does not come from a specific\nregion in which the network is initialized as in the NTK (or lazy) regime (Chatterjee,\n2022), nor rely on the infinite number of neurons (Wojtowytsch, 2020).\nRemark that, under the near-orthogonality assumption, in the large d limit, the largest\namount of data that “fits” in the vector space is only d, and corresponds to a perturbation\nof the canonical basis6. On average, Corollary 1 finally states that the average number of\ndata points for which we can show convergence is of the order\n√\nd. Trying to push back\nthis limit up to order d is an important question for future research and seems to ask\nfor other techniques. Experiments underlying this question are presented in Section 5\n(Figure 2).\n3.3\nSketch of Proof\nThe proof of convergence relies on three key points: (i) the loss strictly decreases as\nlong as each example is activated by at least a neuron, (ii) for a data point, if there\nexists a neuron which is activated at initialization, then at least one neuron remains\nactivated throughout the dynamics, (iii) At initialization, condition (ii) is satisfied with\nlarge probability. Let us detail shortly how each item articulates with one another.\n(i).\nFirst, Lemma 6, stated and proved in Appendix, shows that, by computing the\nderivatives of the loss, we get a lower bound on the curvature\nµ(t) ≥2\nn((C−\nx )2 −||XTX −DX||) min\ni\n(\n1\np\np\nX\nj=1\n|aj|21j,i\n)\n.\n(7)\nTo prove the strict positivity, one needs to show that ||XTX −DX|| is small enough,\nand that for each data i, there exists j such that |aj|21j,i is strictly positive. Thanks to\nthe initialization of the weights, |aj|2 ≥|aj(0)|2 −||wj(0)||2 > 0, and to Assumption 2,\n1\n√\n2(C−\nx )2 > ||XTX −DX||. Thus, we have convergence if at any time, for any data input,\none neuron remains active, i.e., formally, for all t ≥0, and all i ∈J1, nK, there exists\nj ∈J1, pK such that ⟨wj(t)|xi⟩+ > 0. Hence, the loss decreases as long as one neuron\nremains active per data input. We see next how to show this crucial property.\n6In Appendix C.2, we show that our method can in fact accommodate 2d examples: an orthogonal\nbasis and their antipodal vectors.\n7\n\n(ii).\nLet us fix the data index i ∈J1, nK, and yi > 0 without loss of generality. Let us\ndefine j∗\ni\n= arg maxajyi>0⟨wj(t)|xi⟩the index of the largest correctly initialized neuron.\nSince aj cannot change sign thanks to Assumption 1, ⟨wj∗\ni (t)|xi⟩is continuous, and has\na derivative over each constant segment of j∗\ni . The strict positivity of this neuron is an\ninvariant of the dynamics: if ri ≥yi, the derivative of the neuron shows it increases,\nand if ri < yi, the residual has decreased, which implies that the ⟨wj∗\ni (t)|xi⟩is strictly\npositive. Thus, if a neuron is correctly initialized for the data point i, a neuron stays\nactive throughout the dynamics.\n(iii).\nFinally, Lemma 5 shows P(∀i, ∃j, ⟨wj(0)|xi⟩> 0 ∩ajyi > 0) ≥1 −n\n\u0000 3\n4\n\u0001p, which\nimplies that for p ≥4 log(n\nε ), the network is well initialized with probability at least 1−ε.\n4\nOrthogonal Data\nIn this section, we go deeper on the study of the gradient flow, assuming that the input\ndata are perfectly orthogonal, or equivalently that ||XTX −DX|| = 0. Since most of the\nintuition for the convergence is drawn from the orthogonal case, it offers stronger results\nwhich we detail. In particular, we are able to closely understand the local-PL curvature\n(µ(t))t≥0 evolution and asymptotic behaviour.\n4.1\nAsymptotic PL curvature\nTheorem 1 has shown that the local-PL curvature is lower bounded by a term of order\n1\nn, allowing us to show an exponential convergence rate of this order.\nThe following\nproposition shows that in the orthogonal case the curvature can also be upper bounded.\nProposition 1. Let ε > 0. Given orthogonal inputs, Assumption 1, d large enough, and\np ≥4 log(n\nε ), with probability 1 −ε, we have an upper-bound on the local-PL curvature:\nthere exists C > 0 depending only on the initialization and the constants C+,−\nx,y\nsuch that\nfor all t ≥Cn,\nµ(t) ≤C\nrp\nn max\ni\n\f\f\f\f1 −ri(t)\nyi\n\f\f\f\f + C\nn .\n(8)\nThis upper bound uses two properties that are characteristic of the orthogonal case.\nFirst, once a neuron is inactive on some data input, then, it can never re-activate again.\nThe second property is that for an initialization scale independent on n, there is a phase\nduring which correctly initialized neurons increase while the others decrease to 0. This\nextinction phase, proved in Lemma 7, is short in comparison to the time needed to fit the\nresiduals, and leaves the system decoupled between positive and negative outputs yi.\nIn the limit where n goes to infinity, Proposition 1 shows that the network does\nnot learn since the local-PL is 0. This is an artifact of the orthogonality of the inputs:\nthe interaction between inputs should accelerate the dynamics. However, although all\nquantities have well defined limits as n →+∞, the limits cannot be understood as a\ngradient descent in an infinite dimensional space7.\nProposition 1 is in fact valid for p fixed, and an initialization of the weights for which\nevery data is correctly initialized by a neuron. In that case, Proposition 1 shows that the\n7One would like to write the loss as an expectation over the data point, yet it is impossible as there\nis no uniform distribution on N.\n8\n\nasymptotic curvature cannot be larger than the order\n1\n√n. While the local-PL curvature\nis between the order 1\nn and\n1\n√n, the next proposition shows that any intermediate order\n1\nnα, for α ∈[1\n2, 1], can be reached asymptotically, with strictly positive probability, using\na particular initialization of the network.\nGroup initialization.\nIn the following, we use pn to denote the number of neurons,\nand partition the n data points in pn groups of cardinality kn (note that pnkn = n). We\nre-index the examples per group as by (xj\ni, yj\ni ) = (xi+(j−1)kn, yi+(j−1)kn), for all i ∈J1, knK\nand j ∈J1, pnK. Moreover, we use a special initialization of the network such that for all\nj, q ∈J1, pnK, i ∈J1, knK,\n\u001a⟨wj|xq\ni⟩> 0 if j = q\n⟨wj|xq\ni⟩≤0 if j ̸= q\nand\naj = sj||wj|| ,\n(9)\ni.e., wj is correctly activated on the group j only.\nProposition 2. Suppose the group initialization described above, with orthonormal in-\nputs, i.e., XTX = I, and that the signs of all outputs of the group j are equal to sj. We\nfix kn = n2(1−α) with α ∈[1\n2, 1]. Then, for t ≥Cn3α−1 log (Cn), the local-PL curvature\nsatisfies\nK1\nnα ≤µ(t) ≤K2\nnα ,\n(10)\nwhere C = max(αC−\ny , (\n1\n2C−\ny )\n1\nα), K1 = 2C−\ny minj\n||wj(0)||2\n2+||wj(0)||2 and K2 = 4C+\ny .\nProposition 2 states that any asymptotic value ⟨µ∞⟩∈[K1\nn , K2\n√n] can be achieved with\nstrictly positive probability using group initialization. But of what order is the most\nlikely limit of the curvature for standard initialization? Experiment 5.2 in Section 5.2\nsuggest that, with high probability, the asymptotic curvature is always of the order\n1\n√n.\nConjecture 1. Let ε > 0. There exist C1, C2 > 0 depending only on the data and the\ninitialization, such that for p ≥C1 log(n\nε ) and for orthogonal examples, with probability\nat least 1 −ε over the initialization of the network, we have convergence of the loss to 0\nand\n⟨µ∞⟩= C2\n√n.\n(11)\n4.2\nPhase transition in the PL curvature\nIn the previous section, we emphasized the asymptotic order of the local-PL curvature\nwith respect to n and hypothesized that it is of the order\n1\n√n in most cases. In this\nsection, we are interested in the evolution of the local-PL curvature during the dynamics.\nLemma 4 below computes the local-PL curvature at initialization in the large p regime,\nand shows that initially it is of order 1\nn.\nLemma 4. At initialization, the local-PL curvature µ(0) is a random variable which\nsatisfy √p( 2\nnµ(0) −β0) −→\np→+∞N (0, γ2\n0), and with β0, γ0 depending only on the data and\nthe distributions of the network’s neurons.\n9\n\nFigure 1: Simulation of the loss trajectory of a network with 2 neurons and group ini-\ntialization, each activated separately on half the data points. Ln is the rescaled loss for\nn examples, and L∞is its limit as n goes to infinity. We can see two phase transitions in\nthe very high-dimensional regime.\nThe constant β0 is strictly positive as soon as the limit network does not directly\nequal the labels, which is natural to assume since they are unknown a priori. Thus the\nexponential rate of decrease of the loss in the early times of the dynamics is of order\n1\nn. Importantly, Proposition 2 with a single group has an asymptotic speed of order\n1\n√n,\nmeaning that the local-PL curvature transitions between 1\nn and\n1\n√n. If Conjecture 1 is\ntrue, then this phenomenon happens with high probability during the dynamics.\nLet us study this phenomenon through the example of Proposition 2, with a fixed\nnumber of neurons p. In this case, the following theorem shows that there are exactly p\nphase transitions of the loss, which each corresponds to a data group being fitted. To be\nprecise, let us define L∞(t) = limn→+∞Ln(t), with Ln(t) = L(θ(t×tn)), tn =\n√np\n4 log(np),\nand p fixed (kn = n\np). We prove that L∞is constant by parts with at most p parts.\nTheorem 2. Suppose the same data hypothesis and initialization as Proposition 2. We\ndefine ||Dn\nj ||2 =\n1\nkn\nPkn\ni=1(yj\ni )2 for each cluster, and suppose its limit ||D∞\nj ||2 finite. Then,\nthe function L∞is constant by parts with at most p parts, and the transitions happen at\neach time tj =\n1\n||D∞\nj ||. Moreover, for all ε ∈]0, 1[, there exist times tj\nn(ε) satisfying\nLj(tj\nn(ε)) = ε\n2||Dn\nj ||2,\ntj\nn(ε)\ntj\nn(1 −ε)\n∼n 1 and tj\nn(1 −ε) −tj\nn(ε)\ntn\n∼n\n1\n2||D∞\nj ||\nlog (Cj(ε))\nlog(n)\n,\n(12)\nwhere Lj is the part of the loss corresponding to the group j, and Cj(ε) > 1 depends\non ε and the initializations and data of the group j.\nThe theorem shows that each transition of Ln occurs in the time frame which decreases\nas\n1\nlog(n). Note that these transitions are subtle: one needs extremely large dimensions in\norder to differentiate two close transitions as shown on Figure 1 in Appendix. The phase\ntransitions of the loss are in fact associated with transitions ||wj||2 from a constant order\n10\n\nFigure 2: Left: Probability that a network trained on n data converges to 0 loss. We\nobserve a transition at n = 3000, from likely to unlikely convergence.\nRight: Loss at convergence normalized by the loss at initialization. For n ≥3000, the\nloss increases to 0.6%, which is equivalent to fitting all but one example.\nto an order √n, and by Lemma 6 with transitions on the local-PL from order 1\nn to order\n1\n√n.\n5\nExperiments\nIn this Section, we aim to perform deeper experimental investigations on the system,\nwhich we could not do formally. Precisely, we want to answer two questions:\n1. What is the probability that the loss reaches 0 for n data points in dimension\nd, under the distributional hypotheses of Lemma 3 (sub-Gaussian, zero-mean and\nwhitened data)? What is the maximum n for a fixed d such that global convergence\nholds with high probability ?\n2. In the orthogonal case, is the asymptotic exponential convergence rate of order\n1\n√n\n(on average over the initialization) as stated in Conjecture 1?\nThe data and weights distribution which have been used for the experiments below\ncan be found in Appendix B, and the code is available on GitHub.\n5.1\nProbability of Convergence\nThis section aims to test the limit in which Corollary 1 holds when the number of data\npoints increase.\nIntuitively, as the number of examples n grows, the neural network\nbecomes less and less overparametrized, and hence is expected to fail to globally converge.\nKnowing if and when this occurs with high probability is important for us to understand\nhow much our current threshold C\n√\nd can be improved. We thus plot the probability\nof convergence, as well as the loss at convergence to obtain additional information when\nthe probability is zero. We train 500 one-layer neural networks with the normalization\npresented in Section 2, dimension d = 100, n ranging from 2500 to 3500, and pn = C log(n)\nneurons. Additional details on the training procedure can be found in Appendix B.\n11\n\nFigure 2 shows that for n ≤2900, the probability of convergence is very likely, for\nn ≥3100 the probability is almost zero, and in between, there is a sharp transition.\nThis sharp transition is visible for any value d at some point N(d, p), which we name the\nconvergence threshold. By measuring the point for different values of d and p, we see\nthat the threshold scales like N(d, p) ≃C(p)d, with C(p) which is sub-linear (see Figure\n4 in Appendix B). In particular, for n ≤Cdp, there exists a network that interpolates\nthe data, meaning that the convergence threshold is not a threshold for the existence\nof a global minimum. The threshold’s scaling is linear in d which implies that proving\nconvergence for Cd data in dimension d seems feasible.\n5.2\nEmpirical asymptotic local-PL curvature\nIn this section we test Conjecture 1, and to do so we measure µ(t) during the dynamics,\nand mostly at the end of the dynamics, since we know by Lemma 4 that near 0 the\nlocal-PL curvature is of order 1\nn. To provide the strongest evidence for the conjecture,\nwe measured the order of the local-PL curvature in three ways: by directly measuring\nthe local-PL µ(t∞) = log\n\u0010\nL(t∞−1)\nL(t∞)\n\u0011\nat the last epoch t∞, by measuring the average-PL\ncurvature ⟨µ∞⟩=\n1\nt∞log\n\u0010\nL(0)\nL(t∞)\n\u0011\n, and finally by mesuring the lower and upper bounds\non the local-PL given in Lemma 6.\nFollowing Conjecture 1, all approximations should likely be decreasing in\n1\n√n as n\nincreases. To show this, we plot the log-log graph of each measure above. We train 500\nnetworks in dimension d = 2000, with n ranging from 1000 to 2000, and pn = C log(n).\nAll resulting plots appear linear in the log-log scale, with a slope close to −1\n2 (see Figure\n5 in Appendix B), meaning that the scalings are indeed in\nC\n√n. This empirically confirms\nour conjecture that the local-PL curvature has order\n1\n√n asymptotically.\n6\nConclusion\nWe have studied the convergence of the gradient flow on a one-hidden-layer ReLU net-\nworks on finite datasets. Our analysis leverages a local Polyak-Łojasiewicz viewpoint on\nthe gradient-flow dynamics, revealing that for a large dimension d in the order of n2 data\npoints, we can guarantee global convergence with high probability using only\nlog(n) neurons. The specificity of the system relies on the low-correlation between the\ninput data due to the high dimension. Moreover, in the orthogonal setting the loss’s\nexponential rate of convergence is at least of order 1\nn and at most of order\n1\n√n, which\nis also the average asymptotic order as experimentally verified. For a special initialization\nof the network, a phase transition in this rate occurs during the dynamics.\nFuture Directions.\nWe are most enthusiastic about proving the convergence of the\nnetworks for linear threshold d ≥Cn, which should require new proof techniques, as well\nas quantifying the impact of large amounts of neurons on the system, which has been\noverlooked in our study. Future work should also consider a using a teacher-network to\ngenerate the outputs, in order to link the probability or interpolation with the complexity,\nin terms of neurons, of the teacher.\n12\n\n7\nAcknowledgements\nThis work has received support from the French government, managed by the National\nResearch Agency, under the France 2030 program with the reference \"PR[AI]RIE-PSAI\"\n(ANR-23-IACL-0008).\n13\n\nReferences\nZeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning\nvia over-parameterization. In International conference on machine learning, pages 242–\n252, 2019.\nSanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis\nof optimization and generalization for overparameterized two-layer neural networks. In\nInternational Conference on Machine Learning, pages 322–332, 2019.\nFrancis Bach. Learning Theory from First Principles. MIT Press, 2024.\nDavid Bertoin, Jérôme Bolte, Sébastien Gerchinovitz, and Edouard Pauwels. Numerical\ninfluence of Relu’(0) on backpropagation. Advances in Neural Information Processing\nSystems, 34:468–479, 2021.\nLéon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods for large-scale\nmachine learning. SIAM Review, 60(2):223–311, 2018.\nEtienne Boursier and Nicolas Flammarion. Early alignment in two-layer networks training\nis a two-edged sword. arXiv preprint arXiv:2401.10791, 2024a.\nEtienne Boursier and Nicolas Flammarion. Simplicity bias and optimization threshold in\ntwo-layer relu networks. arXiv preprint arXiv:2410.02348, 2024b.\nEtienne Boursier, Loucas Pillaud-Vivien, and Nicolas Flammarion. Gradient flow dy-\nnamics of shallow Relu networks for square loss and orthogonal inputs. Advances in\nNeural Information Processing Systems, 35:20105–20118, 2022.\nSourav Chatterjee. Convergence of gradient descent for deep neural networks. arXiv\npreprint arXiv:2203.16462, 2022.\nZhengdao Chen, Eric Vanden-Eijnden, and Joan Bruna. On feature learning in neural\nnetworks with global convergence guarantees. International Conference on Learning\nRepresentations, 2022.\nLenaic Chizat and Francis Bach.\nOn the global convergence of gradient descent for\nover-parameterized models using optimal transport. Advances in Neural Information\nProcessing Systems, 31, 2018.\nLenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable\nprogramming. Advances in Neural Information Processing Systems, 32, 2019.\nSimon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably\noptimizes over-parameterized neural networks. International Conference on Learning\nRepresentations, 2019.\nSpencer Frei, Gal Vardi, Peter Bartlett, Nathan Srebro, and Wei Hu. Implicit bias in\nleaky reLU networks trained on high-dimensional data. The Eleventh International\nConference on Learning Representations, 2023.\nIvan Gentil.\nL’entropie, de Clausius aux inégalités fonctionnelles.\nHAL preprint\nhal-02464182, 2020.\n14\n\nMargalit Glasgow.\nSgd finds then tunes features in two-layer neural networks with\nnear-optimal sample complexity: A case study in the xor problem. arXiv preprint\narXiv:2309.15111, 2023.\nArthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence\nand generalization in neural networks.\nAdvances in Neural Information Processing\nSystems, 31, 2018.\nArnulf Jentzen and Adrian Riekert. Convergence analysis for gradient flows in the training\nof artificial neural networks with Relu activation. Journal of Mathematical Analysis\nand Applications, 517(2):126601, 2023.\nZiwei Ji and Matus Telgarsky. Directional convergence and alignment in deep learning.\nAdvances in Neural Information Processing Systems, 33:17176–17186, 2020.\nJason D. Lee, Max Simchowitz, Michael I. Jordan, and Benjamin Recht. Gradient descent\nonly converges to minimizers. In Conference on learning theory, pages 1246–1257, 2016.\nChaoyue Liu, Libin Zhu, and Mikhail Belkin. Loss landscapes and optimization in over-\nparameterized non-linear systems and neural networks. Applied and Computational\nHarmonic Analysis, 59:85–116, 2022.\nKaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural\nnetworks. International Conference on Learning Representations, 2020.\nHartmut Maennel, Olivier Bousquet, and Sylvain Gelly. Gradient descent quantizes Relu\nnetwork features. arXiv preprint arXiv:1803.08367, 2018.\nSibylle Marcotte, Rémi Gribonval, and Gabriel Peyré. Abide by the law and follow the\nflow: Conservation laws for gradient flows. Advances in Neural Information Processing\nSystems, 36, 2024.\nSong Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape\nof two-layer neural networks. Proceedings of the National Academy of Sciences, 115\n(33), 2018.\nB. T. Polyak.\nGradient methods for solving equations and inequalities.\nUSSR\nComputational Mathematics and Mathematical Physics, 4(6):17–32, 1964.\nGrant Rotskoff and Eric Vanden-Eijnden. Parameters as interacting particles: long time\nconvergence and asymptotic error scaling of neural networks.\nAdvances in Neural\nInformation Processing Systems, 31, 2018.\nRoman Vershynin.\nIntroduction to the non-asymptotic analysis of random matrices.\narXiv preprint arXiv:1011.3027, 2010.\nStephan Wojtowytsch. On the convergence of gradient descent training for two-layer\nRelu-networks in the mean field regime. arXiv preprint arXiv:2005.13530, 2020.\nMo Zhou, Rong Ge, and Chi Jin. A local convergence theory for mildly over-parameterized\ntwo-layer neural network. In Conference on Learning Theory, pages 4577–4632, 2021.\n15\n\nOrganization of the Appendix\nThe appendices of this article are structured as follows. Appendix A contains the\nproofs of each of the 10 statements of the paper in an entitled subsection, with additional\nlemmas included in the relevant subsections. Only Corollary 1 doesn’t have a complete\nproof as it is a simple combination of Lemma 3 and Theorem 1. Appendix B contains ad-\nditional details on the experiments that were performed in Section 5, as well as graphs for\nthe scaling laws. Finally, Appendix C contains general discussions about the possibility\nto provably learn 2d inputs in dimension d, and the possible collapse of the second-layer\nweights.\nA\nProofs\nA.1\nTheorem 1\nLemma 5 shows that a number of neurons of order log(n) is both necessary and sufficient\nto obtain the event I, which corresponds to a nice initialization of the network.\nLemma 5. Suppose yi ̸= 0, and let I the event: for all i, there exists j such that,\n⟨wj(0)|xi⟩> 0 and aj(0)yi > 0. For all ε > 0,\n• if p ≥4 log(n\nε ), then P(I) ≥1 −ε,\n• if p ≤3 log(n\nε ) −2, then P(I) ≤1 −ε,\nand thus, P(I) = 1 −ε implies p ∈[3 log(n\nε ) −2, 4 log(n\nε )].\nProof. of Lemma 5\nLet us note ⟨wj(0)|xi⟩= Wi,j and Aj = aj(0)yi random variables which are symmetric.\nAj are independent with all variables, while Wj,i are independent with all variables Aj\nand Wq,k with q ̸= j.\nP(I) = P(∀i, ∃j, ⟨wj(0)|xi⟩> 0 ∩ajyi > 0)\n= P\n \\\ni\n[\nj\nWi,j > 0 ∩Aj > 0\n!\n= 1 −P\n [\ni\n\\\nj\nWi,j ≤0 ∪Aj ≤0\n!\n≥1 −nP\n \\\nj\nWi,j ≤0 ∪Aj ≤0\n!\n= 1 −nP (Wi,j ≤0 ∪Aj ≤0)p\n= 1 −n(1 −P (Wi,j > 0 ∩Aj > 0))p\n= 1 −n (1 −P (Wi,j > 0) P (Aj > 0))p\n= 1 −n\n\u00123\n4\n\u0013p\n(13)\n16\n\nReplacing the expression with p = 4 log(n\nε ) ≥\n1\n1\n3 −1\n2\n1\n32 log(n\nε ) ≥\nlog( n\nε )\nlog( 4\n3 ), we find that the\nprobability is larger than 1 −ε. Now for the other bound,\nP(I) = P(∀i, ∃j, ⟨wj(0)|xi⟩> 0 ∩ajyi > 0)\n= P\n \\\ni\n[\nj\nWi,j > 0 ∩Aj > 0\n!\n≤P\n \\\ni\n[\nj\nWi,j > 0\n!\n= P\n [\nj\nW1,j > 0\n!n\n= (1 −P (W1,1 > 0)p)n\n= (1 −2−p)n\n≤\n\u0012\n1 −4\n\u0010 ε\nn\n\u00113 log(2)\u0013n\n≤\n\u0010\n1 −4 ε\nn\n\u0011n\n≤1 −ε\n(14)\nwhere we use (1 −x\nn)n ≤1 −x\ne ≤1 −x\n4 valid on x ∈[0, 1].\nLemma 6. For any set of parameters θ = (aj, wj)j=1:p, the following bounds on the\nlocal-PL curvature hold.\nµ(t) ≤2\nn(C+\nx + ||XTX −DX||) max\ni\n1\np\np\nX\nj=1\n(|aj|2 + ||wj||2)1j,i\nµ(t) ≥2\nn(C−\nx −||XTX −DX||) min\ni\n1\np\np\nX\nj=1\n|aj|21j,i\n(15)\nProof. of Lemma 6\nLet us start by writing the derivatives of the two variables of the system. We define\nPj ∈Rn×n the diagonal matrix with elements 1j,i.\nd\ndtwj = aj\nn\nn\nX\ni=1\nrixi1j,i = aj\nn XPjR\nd\ndtaj = 1\nn\nn\nX\ni=1\nri⟨wj|xi⟩+ = 1\nnwT\nj XPjR\n(16)\nWe now compute the derivatives for the residuals ri.\nd\ndtri = −1\np\np\nX\nj=1\n\u0012 d\ndtaj\n\u0013\n⟨wj|xi⟩+ + aj\n\u0012 d\ndt⟨wj|xi⟩+\n\u0013\n= −1\nnp\np\nX\nj=1\n1j,ixT\ni wjwT\nj XPjR + |aj|21j,ixT\ni XPjR\nd\ndtR = −1\nnp\n\"\np\nX\nj=1\nPjXTwjwT\nj XPj + |aj|2PjXTXPj\n#\nR\n(17)\n17\n\nThe matrix written between brackets is symmetric, and the local-PL curvature lies be-\ntween its smallest and largest eigenvalues.\nd\ndt||R||2 = −2\nnp\n\"\np\nX\nj=1\n(wT\nj XPjR)2 + |aj|2||XPjR||2\n#\n(18)\nSince 0 ⪯wjwT\nj ⪯||wj||2Id, we get the following bound on the derivative\n−2\nnp\np\nX\nj=1\n(||wj||2 + |aj|2)||XPjR||2 ≤d\ndt||R||2 ≤−2\nnp\np\nX\nj=1\n|aj|2||XPjR||2\n(19)\nWe can manipulate the term ||XPjR||2 to make ||XTX −DX|| appear.\n||XPjR||2 = RTP T\nj XTXPjR\n= RTP T\nj (DX −(XTX −DX))PjR\n= ||\np\nDXPjR||2 −||\np\nXTX −DXPjR||2\n≥(C−\nx )2||PjR||2 −||XTX −DX||||PjR||2\n≤(C+\nx )2||PjR||2 + ||XTX −DX||||PjR||2\n(20)\nFinally, by observing that µ(t) =\nd\ndt||R||2\n||R|2 , one has the lower bound\nµ(t) ≥2\nnp\np\nX\nj=1\n|aj|2||PjR||2\n||R||2\n\u0000(C−\nx )2 −||XTX −DX||\n\u0001\n≥2\nn\n1\n||R||2\n\u0000(C−\nx )2 −||XTX −DX||\n\u0001\nn\nX\ni=1\nr2\ni\np\np\nX\nj=1\n|aj|21j,i\n≥2\nn\n\u0000(C−\nx )2 −||XTX −DX||\n\u0001\nmin\ni\n1\np\np\nX\nj=1\n|aj|21j,i,\n(21)\nand and the upper bound\nµ(t) ≤2\nnp\np\nX\nj=1\n|aj|2||PjR||2\n||R||2\n\u0000(C+\nx )2 + ||XTX −DX||\n\u0001\n≤2\nn\n\u0000(C+\nx )2 + ||XTX −DX||\n\u0001\nmax\ni\n1\np\np\nX\nj=1\n(|aj|2 + ||wj||2)1j,i.\n(22)\nProof. of Theorem 1\nThis is a precise proof based on the sketch visible in Section 3. The proof of convergence\nrelies on three key points:\n(i) The loss strictly decreases as long as each example is activated by at least a neuron.\n(ii) For a data point, if there exists a neuron which is activated at initialization, then\nat least one neuron remains activated throughout the dynamics.\n(iii) At initialization, the previous condition is satisfied with large probability.\nWe finish the proof with the lower bounds on µ(t) and ⟨µ∞⟩.\n18\n\n(i) First, Lemma 6 shows that, by computing the derivatives of the loss, we get a\nlower bound on the curvature.\nµ(t) ≥2\nn\n\u0000(C−\nx )2 −||XTX −DX||\n\u0001\nmin\ni\n(\n1\np\np\nX\nj=1\n|aj|21j,i\n)\n(23)\nWe want to show the strict positivity of this lower bound. First, using Assumption 2, we\nhave for all n ≥2 that\n(C−\nx )2 −||XTX −DX|| ≥(C−\nx )2\n\u0012\n1 −1\n√n\nC−\ny\nC+\ny\n\u0013\n≥\n\u0012\n1 −1\n√\n2\n\u0013\n(C−\nx )2\n(24)\nwhich also holds for n = 1 since then ||XTX −DX|| = 0. Moreover, thanks to the\nasymmetric initialization, we have |aj|2 ≥|aj(0)|2 −||wj(0)||2 > 0, which means that µ(t)\nis bounded away from 0 as long as for all i there exists j satisfying ⟨wj(t)|xi⟩+ > 0.\n(ii) Let us fix the data index i ∈J1, nK, and yi > 0 without loss of generality. Let us\ndefine j∗\ni\n= arg maxajyi>0⟨wj(t)|xi⟩the index of the largest correctly initialized neuron.\nSince aj cannot change sign thanks to Assumption 1, ⟨wj∗\ni (t)|xi⟩is continuous, and has a\nderivative over each constant segment of j∗\ni . We can write the derivatives of this neuron\nas\nd\ndt⟨wj∗\ni |xi⟩= aj∗\ni\nn\nn\nX\nk\nrk⟨xi|xk⟩1j∗\ni ,k\n= aj∗\ni\nn eT\ni XTXPjR\n≥|aj∗\ni |\nn\n\u0002\nri||xi||21j∗\ni ,isj∗\ni −||XTX −DX||||R||\n\u0003\n= |aj∗\ni |\nn\nh\nri||xi||21j∗\ni ,isj∗\ni −||XTX −DX||\np\n2nL(θt)\ni\n≥|aj∗\ni |\nn\nh\nri||xi||21j∗\ni ,isj∗\ni −||XTX −DX||\np\n2nL(θ(0))\ni\n(25)\nWith the condition ||XTX −DX|| <\n1\n√n(C−\nx )2 C−\ny\nC+\ny and the fact that sj∗\ni yi > 0, we get the\ninequality\nd\ndt⟨wsj∗\ni |xi⟩>\n|asj∗\ni |C−\ny (C−\nx )2\nn\n\u0014ri\nyi\n1sj∗\ni ,i −1\n\u0015\n(26)\nNow, the strict positivity of ⟨wj∗\ni |xi⟩is an invariant of the system: if ri\nyi ≥1, then ⟨wj∗\ni |xi⟩\nstrictly increases, and otherwise we have\n0 < yi −ri\nyi\n= 1\np\np\nX\nj=1\naj\nyi\n⟨wj|xi⟩+ ≤⟨wj∗\ni (t)|xi⟩1\np\np\nX\nj=1\n|aj|\n|yi|\n(27)\nWhich implies that ⟨wj∗\ni |xi⟩stays strictly positive throughout the dynamics.\n(iii) As shown in Lemma 5, for p ≥4 log(n\nε ), we have the strict positivity with\nprobability 1 −ε.\nP(∀i, ∃j, ⟨wj(0)|xi⟩> 0 ∩ajyi > 0) ≥1 −ε.\n(28)\n19\n\nFinally, we prove the lower bounds on the PL. Let us recall that |aj| ≥||wj|| and that\n1j,i ≥⟨¯wj(t)|¯xi⟩+, which gives us\n1\np\np\nX\nj=1\n|aj|21j,i ≥\n\f\f\f\f\f\n1\np\np\nX\nj=1\naj⟨wj|¯xi⟩+\n\f\f\f\f\f =\n\f\f\f\f\nyi −ri(t)\nC+\nx\n\f\f\f\f .\n(29)\nWe can plug these into equation 23 to obtain\nµ(t) ≥2 −\n√\n2\nn\n(C−\nx )2\nC+\nx\nC−\ny min\ni\n\f\f\f\f1 −ri(t)\nyi\n\f\f\f\f\n(30)\nFrom this last equation, by integration, we obtain\n1\nt\nZ t\n0\nµ(u)du ≥2 −\n√\n2\nn\n(C−\nx )2\nC+\nx\nC−\ny\n\u0012\n1 −1\nt\nZ t\n0\nmax\ni\n\f\f\f\f\nri(u)\nyi\n\f\f\f\f du\n\u0013\n(31)\nLet tδ satisfying maxi |ri(t)| ≤δ for all t ≥tδ. tδ exists and is finite since the loss reaches\n0. Thus, we have for any δ > 0 that\n1\nt\nZ t\n0\nµ(u)du ≥2 −\n√\n2\nn\n(C−\nx )2\nC+\nx\nC−\ny\n \n1 −tδ\nt max\ni\np\n2nL(θ0)\n|yi| √p\n−t −tδ\nt\nδ\n!\n(32)\nwhic in the limit t →+∞gives\n⟨µ∞⟩≥2 −\n√\n2\nn\n(C−\nx )2\nC+\nx\nC−\ny (1 −δ)\n(33)\nTaking δ →0 gives the desired bound.\nA.2\nLemma 3\nProof. of Lemma 3\nThis proof will heavily rely on the result of Vershynin (2010, Remark 5.59) on the con-\ncentration of sub-Gaussian random variables. It states that if A ∈Rn×d is a matrix, the\ncolumns of which are n independent centered, whitened8, sub-Gaussian random variables\nin dimension d, then with probability 1 −2e−t2,\n\f\f\f\f\n\f\f\f\f\n1\ndATA −Id\n\f\f\f\f\n\f\f\f\f ≤C\nrn\nd +\nt\n√\nd\n(34)\nwith C > 0 depending only on maxi ||Ai||ψ2 the sub-Gaussian norm of the columns. We\nuse this property with Ai =\n√\nd\nλ xi which satisfies every hypothesis, in particular it is\nwhitened since by assumption E[xixT\ni ] = λ2\nd Id, and t =\nq\nlog\n\u0000 2\nε\n\u0001\n, which gives\n\f\f\f\f\n\f\f\f\f\n1\nλ2XTX −Id\n\f\f\f\f\n\f\f\f\f =\n\f\f\f\f\n\f\f\f\f\n1\ndATA −Id\n\f\f\f\f\n\f\f\f\f ≤C\nrn\nd +\ns\nlog\n\u0000 2\nε\n\u0001\nd\n(35)\n8In this article, Vershynin (2010, Remark 5.59) uses the isotrop of the columns, but defines it as\nE[xxT ] = Id, which we rather refer to as a whitened distribution.\n20\n\nMoreover, note that\n(C−\nx )2 ≥1 + min\ni (||xi||2 −1) ≥1 −||XTX −Id||\n(36)\nThus, the condition in Assumption 2 is satisfied if\n\f\f\f\fXTX −λ2Id\n\f\f\f\f ≤λ\nC−\ny\nC−\ny + √nC+\ny\n(37)\nNow, since C−\ny , C+\ny are independent of n since PY has compact support away from 0,\ntaking\nd > 16n max\n\u0012\nC2n, log\n\u00122\nε\n\u0013\u0013 \u0012C−\ny\nC+\ny\n\u00132\n(38)\nis sufficient for Assumption 2 with probability 1 −ε.\nThe proof of Corollary 1 is then the combination of the proof of Theorem 1, using\n\f\f\f\fXTX −λ2Id\n\f\f\f\f instead of ||XXT −DX||, with ε\n2.\nA.3\nProposition 1\nLemma 7. Suppose orthogonal inputs and convergence of the system to 0 loss, then with\nprobability 1 −ε for all t ≥tn with tn = 2n maxj,i\n⟨wj(0)|xi⟩+\n|yi|||xi||2(|a(0)|2−||wj(0)||2),\naj(0)yi > 0 =⇒⟨wj(t)|xi⟩+ ≥⟨wj(0)|xi⟩+\naj(0)yi < 0 =⇒⟨wj(t)|xi⟩+ ≤0\n(39)\nThis Lemma states that, for orthogonal data, incorrectly initialized neuron vanish,\nand cannot become active again. Thus, after time tn, the system is decoupled between\nthe positive and negative labels, and only correctly initialized neuron, which are useful\nto the prediction, persist.\nProof. of Lemma 7\nWe start by computing the derivative of a neuron in the orthogonal setting.\nd\ndt⟨wj|xi⟩+ = ajri1j,i||xi||2\nn\nd\ndtaj = 1\nn\nn\nX\ni=1\nri⟨wj|xi⟩+\n(40)\nLet us only discuss the case of neurons that are positive at initialization since other-\nwise nothing happens. We observe that, as long as all ri ̸= 0, then the neurons vary\nmonotonously. This implies that if ajyi < 0, then the corresponding neuron will reach 0\nin finite time. Let t∗\nn the first time any |ri −yi| = yi\n2 , which is finite with high probability.\nFor j, i such that ajyi < 0, we have\nd\ndt⟨wj|xi⟩+ ≤−\np\n|aj(0)|2 −||wj(0)||2|yi|\n2n\n||xi||21j,i\n⟨wj|xi⟩+ ≤⟨wj(0)|xi⟩+ −1j,i\n|yi|\n2n ||xi||2Cjt\n(41)\n21\n\nwhere C2\nj = |aj(0)|2 −||wj(0)||2. Let ˜tn = maxj,i\n2n⟨wj(0)|xi⟩+\n|yi||xi||2Cj , if ˜tn ≤t∗\nn, then we have\nextinction in finite time, i.e., the incorrectly initialized neurons have reached 0. Let us\nshow that at ˜tn, residuals have almost not moved.\nFirst, we bound the second-layer\nneurons aj,\nd\ndtaj ≤1\nn\nv\nu\nu\nt\nn\nX\ni=1\nr2\ni\nv\nu\nu\nt\nn\nX\ni=1\n⟨wj|xi⟩2\n+\n≤\n√\n2L(θt) max\ni ⟨wj|xi⟩+\n≤\n√\n2L(θ0) max\ni ⟨wj|xi⟩+\naj −aj(0) ≤\n√\n2L(θ0)\nZ t\n0\nmax\ni ⟨wj(u)|xi⟩+du\n(42)\nLet C1 =\n3\n2n maxi |yi|||xi||2, C2 = maxi⟨wj(0)|xi⟩2\n+ + aj(0)2 C1\n2 , and C3 = C1\n\u0010\nL(θ0)\n√\n2 + 1\n2\n\u0011\n.\nThe upper bound on aj turns into an upper bound on wj,\nd\ndt max\ni ⟨wj(u)|xi⟩+ ≤C1\n\u0012\naj(0) +\n√\n2L(θ0)\nZ t\n0\nmax\ni ⟨wj(u)|xi⟩+du\n\u0013\nmax\ni ⟨wj(t)|xi⟩2\n+ −max\ni ⟨wj(0)|xi⟩2\n+ ≤C1aj(0)\nZ t\n0\nmax\ni ⟨wj(u)|xi⟩+du+\n+ C1\n√\n2L(θ0)\nZ t\n0\nmax\ni ⟨wj(u)|xi⟩2\n+du\nmax\ni ⟨wj(t)|xi⟩2\n+ ≤C2 + C3\nZ t\n0\nmax\ni ⟨wj(u)|xi⟩2\n+du\n(43)\nLet us pose Aj(t) =\nR t\n0 maxi⟨wj(u)|xi⟩2\n+du, We can solve the differential inequality\nA′\nj(t) ≤C2 + C3Aj(t)\nAj(t) + C2\nC3\n≤(Aj(0) + C2\nC3\n)eC3t\n(44)\nIn the end, we obtain\nmax\ni ⟨wj|xi⟩+ ≤C2eC3t\n(45)\nImportantly, since C3˜tn is a constant of n, we have that at there exists K1, K2 which\ndoesn’t depend on n such that at ˜tn for all j, i,\n⟨wj(˜tn)|xi⟩+ ≤K1⟨wj(0)|xi⟩+\n|aj| ≤K2|aj(0)|\n(46)\nThus, we can bound the deviation of the residuals at the beginning of the dynamics.\n|ri(˜tn) −yi| =\n\f\f\f\f\f\n1\np\np\nX\nj=1\naj⟨wj|xi⟩+\n\f\f\f\f\f\n≤1\np\np\nX\nj=1\n|aj| ⟨wj|xi⟩+\n≤K1K2\n1\np\np\nX\nj=1\n|aj(0)| ⟨wj(0)|xi⟩+\n(47)\n22\n\nFinally, since aj, wj are initialized with norms independent of d, it means that by rota-\ntional invariance ⟨wj(0)|xi⟩has variance\n1\n√\nd and mean 0. Thus, in large dimension, we\nget that |ri(˜tn)−yi|\n→\nd→+∞0. This shows that for d sufficiently large, we have ˜tn ≤t∗\nn.\nProof. of Proposition 1\nThanks to Lemma 7, there exists tn such that for t ≥tn, each example has only correctly\nactivated neurons. Without loss of generality suppose that all labels are positive. Then\nthe network only has positive contributions aj(t)⟨wj|xi⟩≥0 for all i. Let N(j, i) the\nnumber of indices q such that aj(t)⟨wj|xi⟩≤aq(t)⟨wq|xi⟩. We have\nN(j, i)\np\naj⟨wj|xi⟩≤1\np\np\nX\nq=1\naq⟨wq|xi⟩= yi −ri\n(48)\nThus, we can bound the norm of wj,\n||wj||4 ≤a2\nj||wj||2\n=\nn\nX\ni=1\na2\nj⟨wj|¯xi⟩2\n≤\nn\nX\ni=1\np2\nN(j, i)2\n\u0012yi −ri\n||xi||\n\u00132\n≤max\ni\n\u0012yi −ri\nC−\nx\n\u00132\nn\nX\ni=1\np2\nN(j, i)2\n(49)\nThis helps us majorate the sum of |aj|2 + ||wj||2\n1\np\np\nX\nj=1\n\u0000|aj|2 + ||wj||2\u0001\n1j,i ≤1\np\np\nX\nj=1\n\u0000|aj(0)|2 −||wj(0)||2\u0001\n1j,i + 2\np\np\nX\nj=1\n||wj||2\n≤¯C + 2\np\np\nX\nj=1\nmaxi(yi −ri)\nC−\nx\nv\nu\nu\nt\nn\nX\ni=1\np2\nN(j, i)2\n≤¯C + 2maxi(yi −ri)\nC−\nx\np\nX\nj=1\nv\nu\nu\nt\nn\nX\ni=1\n1\nN(j, i)2\n≤¯C + π\nr\n2\n3\nmaxi(yi −ri)\nC+\nx\n√np\n≤¯C + 6maxi(yi −ri)\nC−\nx\n√np\n(50)\nwhich we can use in the bound from Lemma 6 to end the proof. Thus, the constant C\nfrom the Proposition statement is\nC = max\n\u0012\n2 max\nj,i\n⟨wj(0)|xi⟩+\n|yi|||xi||2(|a(0)|2 −||wj(0)||2),\n2(C+\nx )21\np\np\nX\nj=1\n\u0000|aj(0)|2 −||wj(0)||2\u0001\n1j,i,\n12(C+\nx )2\nC−\nx\nC+\ny\n\u0013\n(51)\n23\n\nA.4\nProposition 2\nProof. of Proposition 2\nRecall that the number of neurons is the number of groups pnkn = n, that we have for\nq ̸= j that ⟨wj|xq\ni⟩≤0 at all time, meaning that hθ(xj\ni) = aj\npn⟨wj|xj\ni⟩+ = sj\npn||wj||+⟨wj|xj\ni⟩+,\nand sj does not change by Lemma 1. This implies that the dynamics is decoupled: wj\nand wq can be studied separately.\nLet us compute the dynamics for the neuron j. We let Dn\nj =\n1\nkn\nPkn\ni=1 yj\ni xj\ni, ||wj||2\n+ =\nPn\ni=1⟨wj|xi⟩2\n+, and ¯x+ =\nx\n||x||+. We first consider the alignment between wj and Dn\nj :\nd\ndt⟨¯Dn\nj |sj ¯w+\nj ⟩=\n\u001c\n¯Dn\nj\n\f\fId −¯w+\nj ( ¯w+\nj )T\f\f\n1\n||wj||+\nd\ndtwj\n\u001d\n= 1\nn⟨¯Dn\nj |Id −¯w+\nj ( ¯w+\nj )T|R⟩\n= 1\nn⟨¯Dn\nj |Id −¯w+\nj ( ¯w+\nj )T|knDn\nj −sj\npn\n||wj||+wj⟩\n=\n√kn||Dn\nj ||\nn\n(1 −⟨¯Dn\nj |sj ¯w+\nj ⟩2)\n(52)\nThis equation can be solved closed-form, and we obtain\n⟨¯Dn\nj |sj ¯w+\nj ⟩= sinh (cj\nnt) + ⟨¯Dn\nj |sj ¯w+\nj (0)⟩cosh (cj\nnt)\ncosh\n\u0000cj\nnt\n\u0001\n+ ⟨¯Dn\nj |sj ¯w+\nj (0)⟩sinh\n\u0000cj\nnt\n\u0001\n= 1\ncj\nn\nd\ndt\n\u0002\nlog\n\u0000cosh\n\u0000cj\nnt\n\u0001\n+ ⟨¯Dn\nj |sj ¯w+\nj (0)⟩sinh\n\u0000cj\nnt\n\u0001\u0001\u0003\n(53)\nwith cj\nn =\n2√kn||Dn\nj ||\nn\n. Now we can compute the norm of the neuron.\nd\ndt||wj||2\n+ = 2||wj||+\nsj\nn\nk\nX\ni=1\nri⟨wj|xi⟩+\n= 2\nn||wj||2\n+(⟨Dn\nj |sj ¯w+\nj ⟩−1\npn\n||wj||2\n+)\n||wj||2\n+e\n2\nnpn\nR t\n0 ||wj(u)||2\n+du = ||wj(0)||2\n+e\n2\nn\nR t\n0 ⟨Dn\nj |sj ¯w+\nj (u)⟩du\ne\n2\nnpn\nR t\n0 ||wj(u)||2\n+du −1 =\n2\nnpn\n||wj(0)||2\n+\nZ t\n0\ne\n2\nn\nR u\n0 ⟨Dn\nj |sj ¯w+\nj (v)⟩dvdu\n||wj(t)||2\n+ =\n||wj(0)||2\n+e\n2\nn\nR t\n0 ⟨Dn\nj |sj ¯w+\nj (u)⟩du\n1 +\n2\nnpn||wj(0)||2\n+\nR t\n0 e\n2\nn\nR u\n0 ⟨Dn\nj |sj ¯w+\nj (v)⟩dvdu\n(54)\nFinally, we can replace the expression of the correlation.\n||wj(t)||2\n+ =\npn\n√kn||Dn\nj ||||wj(0)||2\n+\n\u0000cosh (cj\nnt) + ⟨¯Dn\nj |sj ¯w+\nj (0)⟩sinh (cj\nnt)\n\u0001\npn\n√kn||Dn\nj || + ||wj(0)||2\n+\n\u0000sinh\n\u0000cj\nnt\n\u0001\n+ ⟨¯Dn\nj |sj ¯w+\nj (0)⟩(cosh\n\u0000cj\nnt\n\u0001\n−1)\n\u0001 (55)\nWe use this equation in Lemma 6, and easily obtain the upper bound thanks to the\nmonotonicity of ||wj(t)||2.\nµ(t) ≤\n4\nnpn\nmax\ni\np\nX\nj\n||wj(t)||2\n+1j,i = 4 maxj ||wj(t)||2\n+\nnpn\n≤4C+\ny\n√kn\nn\n= 4C+\ny\nnα\n(56)\n24\n\nFor the lower bound, we have the bound for t ≥\nα\n2C−\ny n3α−1 log\n\u0010\nn(C+\ny )\n1\nα\n\u0011\n≥\n1\ncj\nn log(pn\n√kn||Dn\nj ||)\nby monotonicity. Indeed,\ncosh\n\u0000cj\nnt\n\u0001\n+ ⟨¯Dn\nj |sj ¯w+\nj (0)⟩sinh\n\u0000cj\nnt\n\u0001\n≥1\n2ecj\nnt \u00001 + ⟨¯Dn\nj |sj ¯w+\nj (0)⟩\n\u0001\n(57)\nand\nsinh\n\u0000cj\nnt\n\u0001\n+ ⟨¯Dn\nj |sj ¯w+\nj (0)⟩(cosh\n\u0000cj\nnt\n\u0001\n−1) ≤1\n2ecj\nnt \u00001 + ⟨¯Dn\nj |sj ¯w+\nj (0)⟩\n\u0001\n(58)\nwhich implies that\n||wj(t)||2\n+ ≥\npn\n√kn||Dn\nj ||||wj(0)||2\n+ecj\nnt \u00001 + ⟨¯Dn\nj |sj ¯w+\nj (0)⟩\n\u0001\n2pn\n√kn||Dn\nj || + ||wj(0)||2\n+ecj\nnt \u00001 + ⟨¯Dn\nj |sj ¯w+\nj (0)⟩\n\u0001\n≥pn\np\nkn||Dn\nj ||\n||wj(0)||2\n+\n\u00001 + ⟨¯Dn\nj |sj ¯w+\nj (0)⟩\n\u0001\n2 + ||wj(0)||2\n+\n\u00001 + ⟨¯Dn\nj |sj ¯w+\nj (0)⟩\n\u0001\n≥pn\np\nkn||Dn\nj ||\n||wj(0)||2\n+\n2 + ||wj(0)||2\n+\n(59)\nsince ⟨¯Dn\nj |sj ¯w+\nj (0)⟩≥0. Finally, we obtain the desired lower bound.\nµ(t) ≥\n2\nnpn\nmin\ni\np\nX\nj\n||wj(t)||2\n+1j,i = 2 minj ||wj(t)||2\n+\nnpn\n≥2C−\ny\nnα min\nj\n||wj(0)||2\n+\n2 + ||wj(0)||2\n+\n(60)\nA.5\nLemma 4\nProof. of Lemma 4\nLet us recall the equation of the local-PL curvature on the system.\nµ(0) = 2\nnp\np\nX\nj=1\n(wj(0)XPjR(0))2 + |aj(0)|2||XPjR(0)||2\n(61)\nHere there are 3 types of variables: wj, aj and Pj which is the diagonal matrix with\ndiagonal 1j,i. We apply the Central Limit Theorem on the average on p to find the limit\nlaw of µ(0). Let us first look at the residual limit at t = 0 for large p.\nR(0) = Y −1\np\np\nX\nj=1\najwT\nj XPj = Y −E[awTXP] −ξp\n√p = ˜Y −ξp\n√p\n(62)\nWith ξp −→\np→+∞ξ ∼N(0, V(awTXP)). We split the residuals in the expression of µ(0).\n(wT\nj XPjR)2 = (wT\nj XPj ˜Y )2 −2\n√p\n˜Y T(wT\nj XPj)T(wT\nj XPj)ξp + 1\np2(wT\nj XPjξp)2\n||XPjR||2 = ||XPj ˜Y ||2 −2\n√p\n˜Y T(XPj)T(XPj)ξp + 1\np2||XPjξp||2\n(63)\nThis means that we can compute the average\nµ(0) −→\np→+∞\n2\nnE[(wTXP ˜Y )2 + |a|2||XP ˜Y ||2] = 2\nnβ0\n(64)\n25\n\nand the deviation, as the deviation of the central terms and the first order deviations of\nthe residuals,\n√p(µ(0) −2\nnβ0) −→\np→+∞N(0, γ2\n0)\n(65)\nwith\nγ2\n0 = V((wTXP ˜Y )2 + |a|2||XP ˜Y ||2)\n+\n\f\f\f\n\f\f\f\np\nV(ξ)E[(wTXP)T(wTXP)]˜Y\n\f\f\f\n\f\f\f\n2\n+\n\f\f\f\n\f\f\f\np\nV(ξ)E[|a|2(XP)T(XP)]˜Y\n\f\f\f\n\f\f\f\n2\n.\n(66)\nwhere the first term is the deviation of the mean, and the two other come from the\ndeviation of the residuals.\nA.6\nTheorem 2\nProof. of Theorem 2\nWe consider the setting of Proposition 2 but with a fixed number of neuron p, and as in\nits proof, we focus on one specific neuron j for which we suppose sj = 1. We can rewrite\nequation 55.\n||wj(t)||2\n+ =\np√kn||Dn\nj ||||wj(0)||2\n+\n\u0000cosh (cj\nnt) + ⟨¯Dn\nj | ¯w+\nj (0)⟩sinh (cj\nnt)\n\u0001\np√kn||Dn\nj || + ||wj(0)||2\n+\n\u0000sinh\n\u0000cj\nnt\n\u0001\n+ ⟨¯Dn\nj | ¯w+\nj (0)⟩(cosh\n\u0000cj\nnt\n\u0001\n−1)\n\u0001\n(67)\nLet us rewrite the loss of the group j.\nLj(t) =\n1\n2kn\nkn\nX\ni=1\n(rj\ni )2\n=\n1\n2kn\nkn\nX\ni=1\n\u0012\nyj\ni −||wj||+\np\n⟨¯w+\nj |xj\ni⟩\n\u00132\n= 1\n2\n\u0014\n||Dn\nj ||2 −\n2\n√knp||Dn\nj ||⟨¯Dn\nj | ¯w+\nj ⟩||wj||2\n+ +\n1\nknp2||wj||4\n+\n\u0015\n(68)\nLet tj\nn(κ) =\n1\ncj\nn log(κp√kn||Dn\nj ||), where cj\nn =\n2||Dn\nj ||\n√np\nwhich depends on the variable κ > 0,\nwe have\n||wj(tj\nn(κ))||2\n+ = p\np\nkn||Dn\nj ||\nκ||wj(0)||2\n+\n\u0010\n1 + K(j,n)2\nκ2\n+ ⟨¯Dn\nj | ¯w+\nj (0)⟩\n\u0010\n1 −K(j,n)2\nκ2\n\u0011\u0011\n2 + κ||wj(0)||2\n+\n\u0012\n1 −K(j,n)2\nκ2\n+ ⟨¯Dn\nj | ¯w+\nj (0)⟩\n\u0010\n1 −K(j,n)\nκ\n\u00112\u0013\n(69)\nwith K(j, n) =\n1\n√knp||Dn\nj ||. Moreover, we have\n⟨¯Dn\nj |sj ¯w+\nj (tj\nn(κ))⟩=\n1 −K(j,n)2\nκ2\n+ ⟨¯Dn\nj | ¯w+\nj (0)⟩\n\u0010\n1 + K(j,n)2\nκ2\n\u0011\n1 + K(j,n)2\nκ2\n+ ⟨¯Dn\nj | ¯w+\nj (0)⟩\n\u0010\n1 −K(j,n)2\nκ2\n\u0011.\n(70)\n26\n\nThus, by taking n large enough, there exists κ(j, n, ε) such that Lj(tj\nn(κ(j, n, ε))) =\nLj(tj\nn(ε)) = ε||Dn\nj ||2. Moreover, κ(j, n, ε) →κj(ε) when n goes to infinity. Indeed,\nLj(tj\nn(ε)) →1\n2\n\u0014\n||D∞\nj || −\nκj(ε)||wj(0)||2\n+\n2 + κj(ε)||wj(0)||2\n+\n\u00152\n= ε\n2||D∞\nj ||2\n(71)\nwhich means κj(ε) =\n2\n||wj(0)||2\n+\n||D∞\nj ||(1−√ε)\n1+||D∞\nj ||(1−√ε). Thus, we have a phase transition since\ntj\nn(ε)\ntn\n= 4log(κ(j, n, ε)p√kn||Dn\nj ||)\ncj\nn√np log(n)\n∼n\n1\n||D∞\nj ||\n(72)\nand the cutoff window is\ntj\nn(ε) −tj\nn(1 −ε)\ntn\n= log\n\u0012\nκ(j, n, ε)\nκ(j, n, 1 −ε)\n\u0013\n1\ncj\nntn\n∼n\n1\n2||D∞\nj || log\n\u0012\nκj(ε)\nκj(1 −ε)\n\u0013\n1\nlog(n)\n(73)\nwhere we recall that tn =\n√np\n4 log(n). We conclude that the normalized loss thus has at\nmost p phase transition at times\n1\n||D∞\nj ||. Moreover, the constant in the Theorem is\nCj(ε) =\n(1 −√ε)\n1 + ||D∞\nj || (1 −√ε)\n1 + ||D∞\nj ||\n\u00001 −√1 −ε\n\u0001\n\u00001 −√1 −ε\n\u0001\n(74)\nA.7\nOther results\nProof. of Lemma 1\nLet us write the two derivatives of the parameters.\nd\ndtwj = aj\nn\nn\nX\ni=1\nrixi1j,i\nd\ndtaj = 1\nn\nn\nX\ni=1\nri⟨wj|xi⟩+\n(75)\nWe verify that ⟨d\ndtwj|wj⟩= aj\nd\ndtaj, which gives the desired property once integrated.\nProof. of Lemma 2\nBy definition, we have µ(t) = |∇L(θt)|2\nL(θt) , and by property of the gradient flow, |∇L(θt)|2 =\n−d\ndtL(θt). Thus,\nd\ndtL(θt) = −µ(t)L(θt)\nlog(L(θt)) −log(L(θ(0))) = −\nZ t\n0\nµ(x)dx\nL(θt) = L(θ(0))e−⟨µ(t)⟩t\n(76)\n27\n\nFigure 3: This graph shows the scaling law of the convergence threshold for a fixed\nnumber of neurons. It suggests that the scaling is linear in d: N(d, p) = C(p)d.\nB\nExperiments\nThis Appendix contains additional details on the experiments done in Section 5. Data\ngeneration and weight initialization were performed as follows: we initialize all neurons\nindependently as wj ∼N(0, 1\ndId) as well as\naj\n|aj| ∼U({−1, 1}) and |aj| −||wj|| ∼Exp(1)\nwhich implies |aj(0)| ≥||wj(0)||. For the data, we consider yi ∼U([−2, −1] ∪[1, 2]) and\n||xi|| ∼U([1, 2]) in order to control the constants C−\nx , C−\ny ≥1 and C+\nx , C+\ny ≤2. Finally,\nin order to fall within the assumptions of Lemma 3, we let\nxi\n||xi|| ∼U(Sd−1) in Section 5.1,\nand\nxi\n||xi|| be an orthogonal family in Section 5.2.\nExperiment 1.\nFor the experiment in Figure 2, we trained 500 networks in dimension\n100, with n between 2500 and 3500, with 25 runs for each value of n. We used p =\n⌊\nlog( n\nε )\nlog( 4\n3 )⌋+1 neurons for each experiment with ε = 0.05, since this is the optimal threshold\nobtained in Lemma 5. We trained the networks with gradient descent using a learning\nrate of 1 for a total time t∞= 1.5 ×\n√np\n4 log(np) and thus e = t∞\nlr epochs.\nWe considered that a network converged as long as its loss went below C−\ny\n2n , which\nthen guarantees convergence to 0. We thus early stopped the training and declared the\nloss was exactly 0. Otherwise, the convergence went for all epochs and the network was\nassumed to not be able to reach 0 loss. In doted line, we interpolate the probability plot\nusing a sigmoid function, and learned automatically the convergence threshold N(d, p).\nFor the scaling law on d, we fixed p at 30, and trained networks with dimension\nvarying from 10 to 100, and n ranging from N(d, p) −15d to N(d, p) + 15d, with step d.\nFor each dimension, we interpolate the probability graph using a sigmoid, and plotted\nthe linear trend on Figure 3. For the scaling in p, we fixed d = 30, and varied p from 50\nto 400, and plotted the trend on Figure 4 which shows that the scaling in p is sub-linear.\n28\n\nFigure 4: This graph show the scaling law of the convergence threshold for a fixed number\nof neurons. It suggests that the scaling is not linear in d, but it is hard to differentiate\nbetween a sub-linear polynomial growth or a logarithmic growth.\nExperiment 2.\nFor this experiment, we trained 500 networks in dimension 2000, with\nn between 1000 and 2000, with 25 runs for each value of n. We used the same number of\nneurons, learning rates, and epochs as in experiment 1. Let us recall the 4 measures we\nplotted on the Figure 5:\n1. The instantaneous local-PL curvature at the end of the training, µ(t∞) = log\n\u0010\nL(t∞−1)\nL(t∞)\n\u0011\n,\n2. The average-PL curvature throughout the training, ⟨µ∞⟩= log\n\u0010\nL(0)\nL(t∞)\n\u0011\n,\n3. The lower bound on the local-PL at the end of the training, µlow = 2\nn mini\n1\np\nPp\nj=1 |aj|21j,i,\n4. The upper bound on the local-PL at the end of the training, µupp = 16\nn maxi\n1\np\nPp\nj=1 |aj|21j,i.\nEach of the slope being close to −1\n2, we conclude from this log-log graph that ⟨µ∞⟩=\nK\n√n\nas foreseen in Conjecture 1.\nEach plot’s related experiments were performed on a MacBook Air under 2 hours\nwithout acceleration materials.\nC\nAdditional results\nC.1\nCollapse of the second layer\nSimilar to the early alignment phenomenon described by Boursier and Flammarion (2024a,b,\nTheorem 2), where the neurons can rotate and collapse to align on a single vector pre-\nventing minimization of the loss, the weights aj of the second layer can also collapse on a\nsingle direction. Under the hypothesis that |aj(0)| ≥||wj(0)||, the scalar aj cannot change\n29\n\nFigure 5: Scaling laws in log-log for different measures of the local-PL curvature in\ndimension 2000. Each curve is in fact linear with slope close to −1\n2, which is expected by\nConjecture 1.\nsign, which prevents this scenario in the article’s results. But if |aj(0)| < ||wj(0)||, they\ncan change sign, and prevent global minimization even when the neurons are correctly\ninitialized. Proposition 3 gives an example of such collapse in low dimension.\nProposition 3. Suppose that d = n = p = 2. Let (x1, x2) be the canonical basis of\nR2, with the outputs satisfying y1y2 < 0, λ = |y2\ny1|.\nLet |a1(0)|, |a2(0)| ≤δ, and let\nminj,i⟨wj(0)|xi⟩> 0. Then, for δ small enough, y1 large enough, and\nr\nmax\nj,i ⟨wj(0)|xi⟩8\ny1\n≤λ ≤minj,i⟨wj(0)|xi⟩\nmaxj,i⟨wj(0)|xi⟩\n(77)\nwe have limt→+∞L(θt) > 0.\nThe proof relies on the ratio between outputs being large λ, in order to steer the aj\nto change signs, but not too large to then make the neuron go extinct before the signs\nof aj may change again. This traps the network in a state of sub-optimal loss, and if aj\nwere initialized as large as the vectors, this collapse could not have happened.\nProof. of Proposition 3 Without loss of generality, let us suppose y1 > 0 and y2 < 0.\nWe will show that there are values of λ, ε for which the system will not converge. The\nderivatives of aj at the beginning of the dynamics writes\nd\ndtaj = 1\n4(r1⟨wj|x1⟩+ r2⟨wj|x2⟩)\n\f\f\f\f\nd\ndta1 −y1\n4 (⟨w1|x1⟩−λ⟨w1|x2⟩)\n\f\f\f\f ≤max(|a1|, |a2|) max(||w1||, ||w2||)\n\f\f\f\f\nd\ndta2 −y1\n4 (⟨w2|x1⟩−λ⟨w2|x2⟩)\n\f\f\f\f ≤max(|a1|, |a2|) max(||w1||, ||w2||)\n(78)\n30\n\nand the derivatives of ⟨wj|xi⟩are\nd\ndt⟨wj|xi⟩= ajri1j,i\n2\n\f\f\f\f\nd\ndt⟨wj|x1⟩−ajy11j,1\n2\n\f\f\f\f ≤max(|a1|, |a2|) max(||w1||, ||w2||)\n\f\f\f\f\nd\ndt⟨wj|x2⟩+ λajy21j,2\n2\n\f\f\f\f ≤max(|a1|, |a2|) max(||w1||, ||w2||)\n(79)\nNow suppose that for t ≤T, |aj|, ||wj|| ≤M and ⟨wj|xi⟩≥m > 0, we have\naj(t) ≥−δ +\n\u0010y1\n4 (m −λM) −M 2\u0011\nt\naj(t) ≤δ +\n\u0010y1\n4 (M −λm) + M 2\u0011\nt\n(80)\nThus, for T >\nδ\n(\ny1\n4 (m−λM)−M2) with λ ≥m\nM and y1 ≥\n4M2\nm−λM , we have aj(T) > 0. We now\nwish to find the constants M, m such that the previous equation will hold. To find the\nconstraint on m and M, let us write\n⟨wj|x1⟩≥⟨wj(0)|x1⟩−\n\u00121\n2y1M + M 2\n\u0013\nt\n⟨wj|x2⟩≥⟨wj(0)|x2⟩−\n\u0012λ\n2y1M + M 2\n\u0013\nt\n⟨wj|x1⟩≤⟨wj(0)|x1⟩+\n\u00121\n2y1M + M 2\n\u0013\nt\n⟨wj|x2⟩≤⟨wj(0)|x2⟩+\n\u0012λ\n2y1M + M 2\n\u0013\nt\n(81)\nThus, the constraints are\nm ≥min\n\u0012\nmin\nj,i ⟨wj(0)|x2⟩, δ\n\u0013\n−δ\n2y1M + 4M 2\ny1(m −λM) −4M 2\nM ≤max\n\u0012\nmax\nj,i ⟨wj(0)|x2⟩, δ\n\u0013\n+ δ\n2y1M + 4M 2\ny1(m −λM) −4M 2\n(82)\nWe see that the constraint are satisfied with m ≥minj,i⟨wj(0)|x2⟩−2δ > 0 and M ≤\nmaxj,i⟨wj(0)|x2⟩+ 2δ if: δ is small enough, y1 is large enough, and λ <\nminj,i⟨wj(0)|x2⟩\nmaxj,i⟨wj(0)|x2⟩.\nThus, there exists T > 0 such that at time T, we have a1(T), a2(T) > 0, and no neurons\nwent extinct.\nNow, let us show that neurons ⟨wj|x2⟩will go to 0 for some time T > 0, while\nthe neurons aj stay positive. We can use the same equations as before, with this time\n|aj|, ||wj|| ≤N for t ≤T , and get\n⟨wj|x2⟩≤⟨wj(0)|x2⟩−\n\u0012λ\n2y1N −N 2\n\u0013\nt\n(83)\nThus, for T = 2 maxj⟨wj(0)|x2⟩\nλy1N−2N2\nand λy1 ≥2N, we have extinction of the neurons. To find\nthe constraint on N, use the bounds on the growth of aj and ⟨wj(0)|x1⟩. The constraint\nis\nN ≤max\n\u0012\nmax\nj ⟨wj(0)|x1⟩, δ\n\u0013\n+ 2 max\nj ⟨wj(0)|x2⟩y1 + 2N\nλy1 −2N\n(84)\n31\n\nThus, the constraints are satisfied with N ≤maxj,i⟨wj(0)|xi⟩(1 + 3\nλ) as long as y1 is large\nenough, and\nλ ≥\nr\nmax\nj,i ⟨wj(0)|xi⟩8\ny1\n.\n(85)\nAfter time T , the neurons ⟨wj(0)|x2⟩went extinct, and thus we have L(θt) ≥y2\n2\n4 > 0.\nC.2\nAntipodality\nIn this section, we give a trick to show how to make the network learn 2d vectors in\ndimension d. Indeed, one can see, if ⟨¯xi|¯xk⟩= −1, then there is no problem of large\ninteraction between inputs as 1j,i1j,k = 0, since a vector wj is either activated on xi or\nactivated on xk. Thus, if we note ¯x−i = −¯xi, we can replace the condition in Assumption\n2, by\nN2 =\n\f\f\f\f\f\n\f\f\f\f\fXTX −\n \nD+\nX\n−\np\nD+\nXD−\nX\n−\np\nD+\nXD−\nX\nD−\nX\n!\f\f\f\f\f\n\f\f\f\f\f ≤(C−\nx )2\n√n\nC−\ny\nC+\ny\n(86)\nwith D+\nX the diagonal matrix with diagonal terms ||xi||2, and D−\nX the diagonal matrix with\ndiagonal terms ||x−i||2. Thus, adding antipodal vectors will result in the same equations:\nd\ndt⟨wj∗\ni |xi⟩≥|aj∗\ni |\nn\nh\nri||xi||21j∗\ni ,isj∗\ni −r−i||xi||||x−i||1j∗\ni ,−i −N2\np\n2nL(θ(0))\ni\n= |aj∗\ni |\nn\nh\nri||xi||21j∗\ni ,isj∗\ni −N2\np\n2nL(θ(0))\ni\n(87)\nSince, by definition of j∗\ni , ⟨wj∗\ni |xi⟩≥0, thus 1j∗\ni ,−i = 0. And\n||XPjR||2 ≥(C−\nx )2 \u0002\n||PjR||2 −2(R+P +\nj )TR−P −\nj\n\u0003\n−N2||PjR||2\n≥(C−\nx )2||PjR||2 −N2||PjR||2\n(88)\nwhere R+\ni = ri, R−\ni = r−i, (P +\nj )i = 1j,i and (P −\nj )i = 1j,−i. As said P +\nj P −\nj = 0. These two\nequations being the most important for the proof of Theorem 1, it holds with antipodal\ninput data.\nThis shows a weakness of the proof: we expect this to generalize when the data are\nperturbed by ε small, yet here going from exactly antipodal to nearly-antipodal make\nthe proof incorrect. To accommodate for this, one has to control the small zone were\ninteraction can exists and prevent vectors from entering that zone at all time.\n32\n",
  "metadata": {
    "source_path": "papers/arxiv/Convergence_of_Shallow_ReLU_Networks_on_Weakly_Interacting_Data_d292a9f76649a7ec.pdf",
    "content_hash": "d292a9f76649a7ec81a4fa0dfbb55e747dd4c784bd78c5e8386fdb4910bd9992",
    "arxiv_id": null,
    "title": "Convergence_of_Shallow_ReLU_Networks_on_Weakly_Interacting_Data_d292a9f76649a7ec",
    "author": "",
    "creation_date": "D:20250225023445Z",
    "published": "2025-02-25T02:34:45",
    "pages": 32,
    "size": 2617985,
    "file_mtime": 1740470201.096184
  }
}