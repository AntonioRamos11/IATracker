{
  "text": "Improving LLM General Preference Alignment\nvia Optimistic Online Mirror Descent\nYuheng Zhang 1 Dian Yu 2 Tao Ge 2 Linfeng Song 2 Zhichen Zeng 1 Haitao Mi 2 Nan Jiang 1 Dong Yu 2\nAbstract\nReinforcement learning from human feedback\n(RLHF) has demonstrated remarkable effective-\nness in aligning large language models (LLMs)\nwith human preferences. Many existing alignment\napproaches rely on the Bradley-Terry (BT) model\nassumption, which assumes the existence of a\nground-truth reward for each prompt-response\npair. However, this assumption can be overly\nrestrictive when modeling complex human pref-\nerences. In this paper, we drop the BT model\nassumption and study LLM alignment under gen-\neral preferences, formulated as a two-player game.\nDrawing on theoretical insights from learning in\ngames, we integrate optimistic online mirror de-\nscent into our alignment framework to approxi-\nmate the Nash policy. Theoretically, we demon-\nstrate that our approach achieves an O(T −1)\nbound on the duality gap, improving upon the\nprevious O(T −1/2) result. More importantly, we\nimplement our method and show through experi-\nments that it outperforms state-of-the-art RLHF\nalgorithms across multiple representative bench-\nmarks.\n1. Introduction\nReinforcement learning from human feedback (RLHF) has\nplayed a pivotal role in aligning large language models\n(LLMs) with human preferences. The goal of RLHF is to\nfine-tune LLMs to generate responses that are preferred by\nhumans. It has been successfully deployed in state-of-the-\nart models, including Instruct-GPT (Ouyang et al., 2022)\nand Claude (Bai et al., 2022b). The first RLHF framework\nfor LLMs was developed by Ouyang et al. (2022), where\nafter the pre-training stage, the LLM is fine-tuned to max-\nimize the reward signal from a reward model using the\nproximal policy optimization (PPO) algorithm (Schulman\n1University\nof\nIllinois\nUrbana-Champaign\n2\nTencent\nAI Lab,\nBellevue.\nCorrespondence to:\nYuheng Zhang\n<yuhengz2@illinois.edu>.\net al., 2017). This pipeline requires training both the reward\nmodel and the policy model. In addition, policy gradient\napproaches such as PPO often exhibit high variance and\ninstability during training (Peng et al., 2023), leading to\nincreased computational costs.\nTo develop a more stable and computationally lightweight\nalignment approach, Rafailov et al. (2024b) propose the\nDirect Preference Optimization (DPO) algorithm, which di-\nrectly trains the LLM on a preference dataset and bypasses\nthe need for a reward model. DPO uses an offline preference\ndataset, and since its development, a line of research has ex-\nplored different exploration strategies and proposed online\ndirect preference alignment algorithms (Xiong et al., 2024;\nXie et al., 2024; Dong et al., 2024; Yuan et al., 2024). All\nthese methods assume that human preferences can be mod-\neled using the Bradley-Terry (BT) model, where a reward\nfunction R∗exists such that, for any prompt x and response\npair (y1, y2), the preference between y1 and y2 satisfies:\nP(y1 ≻y2 | x) = σ(R∗(x, y1) −R∗(x, y2)),\nwhere σ(z) =\n1\n1+exp(−z) is the sigmoid function.\nHowever, the existence of a reward function and the BT\nmodel are strong assumptions that can be overly restrictive\nwhen modeling complex human preferences. For example,\nthe preference signals in the BT model are always transi-\ntive: if A is preferred to B and B is preferred to C, then\nA must always be preferred to C. This transitive property\ncontradicts evidence from human decision-making (May,\n1954; Tversky, 1969), especially when preferences are at\nthe population level and aggregated from different human\ngroups (May, 1954; Ye et al., 2024). Furthermore, the limi-\ntations of the BT model have also been observed in RLHF\npractice. Jiang et al. (2023) show that a preference model\nwith 0.4B parameters achieves performance comparable to\nLlama-2-13B-based reward models. Ye et al. (2024) train a\nBT reward model and a preference model separately using\nthe same base model and preference dataset, and their results\ndemonstrate that the preference model consistently outper-\nforms the reward model on Reward-Bench (Lambert et al.,\n2024) under both base models. These findings motivate\nus to drop the BT model assumption and instead consider\ngeneral preferences.\n1\narXiv:2502.16852v1  [cs.LG]  24 Feb 2025\n\nImproving LLM General Preference Alignment via Optimistic Online Mirror Descent\nIn this work, we study the problem of aligning LLMs with\ngeneral preferences and formulate it as a two-player zero-\nsum game. Our objective is to approximate the Nash policy\nof the game, which ensures a win rate of at least 50% against\nany other policy. As established in the game theory litera-\nture (Bai et al., 2020; Liu et al., 2021), self-play algorithms\nhave proven to be highly effective in approximating Nash\npolicies. Building on this, we aim to propose a novel online\nRLHF algorithm that further leverages the self-play strcture\nto enhance general preference alignment for LLMs. Our\ncontributions are summarized as follows.\nContributions.\nWe propose a novel online general pref-\nerence alignment algorithm, Optimistic Nash Policy Opti-\nmization (ONPO). Inspired by recent advancements in game\ntheory, our algorithm integrates optimistic online mirror de-\nscent (Rakhlin & Sridharan, 2013; Syrgkanis et al., 2015)\ninto the self-play framework. By utilizing a reward predic-\ntor in a two-step update strategy, ONPO more effectively\nleverages the self-play mechanism and achieves a faster\nconvergence rate of O(T −1), improving upon the previous\nO(T −1/2) result.\nONPO can be efficiently implemented by directly minimiz-\ning a loss objective on a preference dataset, making it com-\nputationally lightweight in practice. We evaluate ONPO\non several representative benchmarks, comparing it with\nstate-of-the-art general preference alignment algorithms.\nExperimental results demonstrate that ONPO consistently\noutperforms or achieves performance comparable to the\nbaselines across different base models and benchmarks. No-\ntably, on the AlpacaEval 2.0 benchmark (Li et al., 2023a),\nONPO achieves a 21.2% and 9.9% relative improvement\nover the strongest baseline when using Mistral-Instruct and\nLlama-3-8B as the base models, respectively.\nOrganization.\nSection 2 presents related work on RLHF\nand learning in games. The problem formulation and pre-\nliminaries are provided in Section 3. Our algorithm and its\ntheoretical guarantees are detailed in Section 4. In Section 5,\nwe compare our approach with other general preference\nalignment algorithms and explore its extension to the multi-\nturn setting. Experimental results are presented in Section 6.\nFinally, we conclude the paper and discuss future directions\nin Section 7.\n2. Related Work\nReward-Based RLHF.\nSince the first RLHF framework\nproposed by Christiano et al. (2017), RLHF has achieved\ntremendous success in aligning large language models\n(LLMs), powering models such as Instruct-GPT (Ouyang\net al., 2022), Llama 2 (Touvron et al., 2023), and Claude (Bai\net al., 2022b). The RLHF pipeline typically involves train-\ning a reward model followed by applying policy gradient\nmethods such as PPO (Schulman et al., 2017) to optimize\na KL-regularized objective (Korbak et al., 2022; Li et al.,\n2023b). Nevertheless, the use of PPO in RLHF introduces\nchallenges, including instability during training (Choshen\net al., 2019) and high computational costs (Yuan et al., 2023).\nTo address these limitations, Rafailov et al. (2024b) pro-\nposed the DPO algorithm, which directly optimizes pref-\nerences by minimizing a loss objective on offline datasets.\nAdditionally, other direct preference learning algorithms\nhave been developed, including offline methods (Ethayarajh\net al., 2024) and online (iterative) methods (Xie et al., 2024;\nXiong et al., 2024; Yuan et al., 2024). However, all these\nalgorithms are reward-based and rely on the Bradley-Terry\n(BT) model assumption. In this paper, we remove the BT\nmodel assumption and consider general preference align-\nment.\nRLHF with General Preferences.\nAzar et al. (2024) is\nthe first to consider the general preference without BT model\nassumption. They propose the offline IPO algorithm to\nlearn the optimal policy when the comparator policy is\nfixed. Munos et al. (2023) formulate the alignment problem\nas a two-player zero-sum game and propose the iterative\nNash-MD algorithm to find the Nash policy of the game.\nSubsequently, there has been a line of work (Ye et al., 2024;\nCalandriello et al., 2024; Rosset et al., 2024; Wu et al., 2024)\ndeveloping online algorithms for learning the Nash policy.\nThe closest work related to ours is Zhang et al. (2024), which\nalso employs a no-regret learning algorithm for self-play.\nHowever, our algorithm incorporates an optimistic predic-\ntor into the policy update, achieving improved theoretical\nguarantees and better empirical performance. A detailed\ncomparison between our algorithm and other general prefer-\nence alignment algorithms is provided in Section 5.\nLearning in Games.\nOnline learning and self-play algo-\nrithms are widely used in approximating the equilibrium of\ngames, including normal-form games (Freund & Schapire,\n1999; Daskalakis et al., 2011; Mai et al., 2018; Roy et al.,\n2019; Chen & Peng, 2020; Wei et al., 2020; Daskalakis\net al., 2021), extensive-form games (Zinkevich et al., 2007;\nKroer et al., 2020; Kozuno et al., 2021; Lee et al., 2021; Bai\net al., 2022a) and Markov games (Wei et al., 2017; Jin et al.,\n2021; Liu et al., 2021; Mao & Bas¸ar, 2023). Our work is\ninspired by the faster convergence properties of optimistic\nonline mirror descent in equilibrium learning (Rakhlin &\nSridharan, 2013; Syrgkanis et al., 2015).\n3. Preliminary\nProblem Setup.\nWe study the contextual formulation\nwhich is extensively used in previous RLHF litera-\nture (Rafailov et al., 2024b; Xiong et al., 2024). The prompt\n2\n\nImproving LLM General Preference Alignment via Optimistic Online Mirror Descent\nx ∈X is sampled from an unknown prompt distribution\nd1. Y is the response space and an LLM is characterized\nby a policy π : X →∆(Y) which outputs the response\nprobability given the context. For any policy π, we use Eπ\nto denote the expectations under π.\nGeneral Preferences.\nIn this work, we drop the BT model\nassumption (Bradley & Terry, 1952) and focus on directly\naligning LLMs with general preferences. To this end, we\ndefine a general preference oracle as follows:\nDefinition 3.1 (General Preference Oracle). There exists a\npreference oracle P : X × Y →Y →[0, 1], which can be\nqueried to obtain the binary preference signal:\nz ∼Ber\n\u0000P(y1 ≻y2 | x)),\nwhere z = 1 indicates y1 is preferred to y2, and z = 0\nindicates the opposite.\nUnlike the BT model assumption, which assumes the exis-\ntence of a reward function R∗for each x and y, the general\npreference oracle always compares y1 to another y2. This\nsetup aligns with practical scenarios, where it is often easier\nfor users to compare two responses than to assign an abso-\nlute score to a single response. Since the preference signal\nalways involves two responses, potentially come from two\ndifferent policies, we formulate the LLM alignment prob-\nlem as a two-player zero-sum game. The objective of this\ngame is the expected win rate between the two players:\nJ(π1, π2) := Ex∼d1Ey1∼π1,y2∼π2\n\u0002\nP(y1 ≻y2 | x)\n\u0003\n.\nHere π1 is the policy of the max-player, aiming to maxi-\nmize the objective, while π2 is the policy of the min-player,\naiming to minimize it.\nNash Policies and Duality Gap.\nOur learning goal is to\nfind the Nash equilibrium of the game, which is defined as:\nπ∗\n1, π∗\n2 := argmax\nπ1\nargmin\nπ2\nJ(π1, π2).\nDue to the symmetric nature of the game, the Nash policies\nfor both players are identical, i.e., π∗\n1 = π∗\n2 = π∗, and the\ngame value is J(π∗, π∗) = 0.5. Since Nash policies are\nthe best responses to each other, for any policy π, we have\nJ(π∗, π) ≥0.5, indicating that the Nash policy will not\nlose to any other policy. To quantify how well a policy π\napproximates π∗, we define the duality gap as:\nDualGap(π) := max\nπ1 J(π1, π) −min\nπ2 J(π, π2).\nThe duality gap is non-negative and DualGap(π) = 0 if and\nonly if π = π∗. Hence, our goal is to find a policy that min-\nimizes the duality gap. Once we achieve DualGap(π) ≤ϵ,\nwe say that π is an ϵ-approximate Nash policy.\n4. Algorithm\nIn this section, we begin by briefly reviewing the self-\nplay algorithm with online mirror descent (OMD) updates,\nwhich is used in previous general preference alignment al-\ngorithm (Zhang et al., 2024). Next, we present our proposed\nalgorithm, which leverages the faster convergence proper-\nties of optimistic OMD, inspired by advancements in game\ntheory (Rakhlin & Sridharan, 2013; Syrgkanis et al., 2015).\nThrough theoretical analysis, we show that our approach\nachieves an improved bound on the duality gap. Finally, we\ndescribe the implementation of our algorithm. Following\nAzar et al. (2024); Zhang et al. (2024), we omit the con-\ntext x throughout the rest of the paper since each context is\nindependent.\n4.1. Self-play Algorithm with OMD Update\nSelf-play algorithms are widely used in approximating the\nNash policy (Bai et al., 2020; Liu et al., 2021). The key\nidea is to let the policy play against itself, enabling iterative\nself-improvement. The algorithm is performed in an online\nmanner, with each iteration using online mirror descent\n(OMD) to update the policy. Specifically, at iteration t, we\nfind the policy that maximizes the following objective:\nπt+1 = argmax\nπ\n⟨π, rt⟩−1\nη KL(π∥πt),\n(1)\nwhere rt(y) = P(y ≻πt) = Ey′∼πt[P(y ≻y′)] is the ex-\npected win rate of response y against the current policy πt,\nand η > 0 is the learning rate. This objective ensures that\nπt+1 not only aims to maximize the win rate over πt but also\nremains close to πt, as measured by the KL divergence term.\nThe stability introduced by the KL regularization is critical\nfor achieving a sublinear regret bound. Without this regu-\nlarization, one can construct examples where the algorithm\nsuffers from linear regret, which is undesirable (Lattimore\n& Szepesv´ari, 2020).\nSimilar to the analysis in Zhang et al. (2024), we can show\nthat the uniform mixture of π1:T achieves an O(T −1/2)\nduality gap, as stated in the following theorem. The proof is\ndeferred to Appendix A.1.\nTheorem 4.1. Let D\n=\nmaxπ KL(π∥π1) and ¯π\n=\n1\nT\nPT\nt=1 πt. Self-play algorithm in Eq. (1) with η =\nq\nD\nT\nsatisfies:\nDualGap(¯π) ≤4\n√\nD\n√\nT\n.\nZhang et al. (2024) also demonstrate that self-play with\nOMD achieves last-iterate convergence. This result is at-\ntributed to the strong convexity induced by the KL regular-\nization terms in their game objectives. However, since our\n3\n\nImproving LLM General Preference Alignment via Optimistic Online Mirror Descent\nobjective does not include these KL terms, the last-iterate\nconvergence may not hold in our game formulation.\n4.2. Optimistic Nash Policy Optimization\nWhile self-play with OMD update already achieves an\nO(\n√\nT) regret bound, which is near-optimal in many online\nlearning scenarios, there is still room for improvement by\nbetter leveraging the self-play structure. Recent advance-\nments in learning in games (Rakhlin & Sridharan, 2013;\nSyrgkanis et al., 2015) demonstrate that a faster conver-\ngence rate of O(T −1) can be achieved when both players\nadopt optimistic OMD update. In this subsection, we in-\ntroduce how to integrate optimistic OMD into the self-play\nalgorithm, resulting in an algorithm called Optimistic Nash\nPolicy Optimization (ONPO).\nThe key idea of optimistic OMD is to incorporate a reward or\nloss predictor at each iteration. Recall that in OMD update,\nwe use the expected win rate over the current policy πt as\nthe reward vector rt to compute πt+1. While in optimistic\nOMD, the learner utilizes a reward predictor mt and adopts\na two-step update strategy:\nπt = argmax\nπ\n⟨π, mt⟩−1\nη KL(π∥π′\nt)\nπ′\nt+1 = argmax\nπ\n⟨π, rt⟩−1\nη KL(π∥π′\nt).\nHere πt aims to maximize the reward predictor mt and\nthe auxiliary policy π′\nt+1 is updated after observing the\nactual reward rt. The word “optimistic” comes from that\nthe learner believes that the predictor mt provides a good\napproximation of the true reward rt.\nNext, we describe how to apply optimistic OMD in our\nself-play algorithm. In both OMD and optimistic OMD,\nthe KL regularization term is consistently used to ensure\nthat the next policy remains close to the previous policies.\nThis regularization provides stability, making it reasonable\nto assume that the change from πt to πt+1 is small. Based\non this observation, we directly use the reward information\nfrom the previous iteration as the predictor, i.e., let mt =\nrt−1 = Ey′∼πt−1 [P(y ≻y′)].\nIn the following theorem, we demonstrate that ONPO\nachieves an O(1/T) duality gap, improving over the previ-\nous O(1/\n√\nT) result.\nTheorem 4.2. Let D\n=\nmaxπ KL(π∥π′\n1) and ¯π\n=\n1\nT\nPT\nt=1 πt, ONPO algorithm with η = min{ 1\n2,\n√\nD} satis-\nfies:\nDualGap(¯π) ≤4\n√\nD\nT\n.\nHere, π′\n1 = π1 is the initialization policy. Theoretically, π′\n1\ncan be set as a uniform policy, in which case D is bounded\nby log |Y|. In RLHF practice, π′\n1 is typically a supervised\nfine-tuned policy.\nThe proof is provided in Appendix A.2. The key to achiev-\ning the O(1/T) rate lies in the regret bounded by variation\nin utilities (RVU) property of optimistic OMD. Specifically,\nthe stability terms ∥rt−rt−1∥2\n∞are canceled out by the neg-\native term −∥πt −πt−1∥2\n1, which arises from the self-play\nmechanism where rt represents the win rate over πt.\nNotably, the duality gap bound in Zhang et al. (2024) also\ndepends on the maximum log density ratio between πt and\na reference policy πref, due to the KL-regularized game\nformulation. When optimistic OMD is applied in such a\nregularized game, the stability terms transform into\nmax\ny\n\f\f\f\fP(y ≻πt) −P(y ≻πt−1) + log\nπt(y)\nπt−1(y)\n\f\f\f\f ,\nwhich cannot be canceled by the negative terms. However,\nthe motivation behind regularizing the game is to keep the\nlearner’s policy close to the reference policy πref, which\naligns with the stability introduced in our update rule. There-\nfore, explicit regularization in our game objective is not\nnecessary.\n4.3. Implementation of ONPO\nIn this subsection, we describe the implementation of ONPO\nwith query access to the preference oracle P. The primary\nchallenge in implementing ONPO lies in computing rt(y),\nwhich involves taking an expectation over the entire policy\nπt. Fortunately, this challenge can be addressed by avoiding\nthe direct estimation of rt(y) and instead relying on binary\npreference feedback between responses.\nTo achieves this, our goal is to design a loss function that\ndoes not involve P(y ≻πt) for policy optimization. We\nfocus on obtaining the loss objective for πt here and the\nderivation for π′\nt is similar. The key observation is that, πt\nhas a closed-form solution which satisfies ∀y, y′ ∈Y,\nlog πt(y)\nπt(y′) −log π′\nt(y)\nπ′\nt(y′) = η (P(y ≻πt−1)−P(y′ ≻πt−1).\nTherefore, similar to the techniques used in Azar et al.\n(2024); Zhang et al. (2024), solving πt is equivalent to\nfinding the minimizer of the following loss function:\nEy,y′∼πt−1\nh\u0000gt(π, y, y′) −η\n\u0000P(y ≻πt−1) −P(y′ ≻πt−1)\n\u0001\u00012i\n.\nwhere gt(π, y, y′) = log π(y)\nπ(y′) −log π′\nt(y)\nπ′\nt(y′). Since the inside\nwin rate term is with respect to πt−1 and we also have\nan expectation over πt−1 outside, the loss function can be\nfurther written as\nEy,y′∼πt−1,yw,yl∼λp(y,y′)\n\u0014\u0010\ngt(π, yw, yl) −η\n2\n\u00112\u0015\n,\n4\n\nImproving LLM General Preference Alignment via Optimistic Online Mirror Descent\nAlgorithm 1 Implementation of ONPO\n1: Input: Number of iterations T, learning rate η, prefer-\nence oracle P, supervised fine-tuned policy πSFT.\n2: Initialize π′\n1 ←πSFT, π1 ←πSFT.\n3: for iteration t = 1, 2, . . . , T −1 do\n4:\nSample response pairs from the current policy πt:\n{y(i)\n1 , y(i)\n2 }n\ni=1 ∼πt.\n5:\nConstruct preference dataset Dt = {y(i)\nw , y(i)\nl }n\ni=1\nwith feedback from the oracle P.\n6:\nCalculate π′\nt+1 as:\nπ′\nt+1 = argmin\nπ\nEyw,yl∼Dt\n\u0014\u0010\ngt(π, yw, yl) −η\n2\n\u00112\u0015\n.\n7:\nCalculate πt+1 as:\nπt+1 = argmin\nπ\nEyw,yl∼Dt\n\u0014\u0010\ngt+1(π, yw, yl) −η\n2\n\u00112\u0015\n.\n8: end for\n9: Output πT .\nwhere λp is the preference distribution (Calandriello et al.,\n2024):\nλp(y, y′) =\n(\n(y, y′)\nwith probability P(y ≻y′)\n(y′, y)\nwith probability 1 −P(y ≻y′).\nTo calculate the loss function, we only need the access to\nsample from the current policy, which is standard and easy\nto implement in practice. Putting everything together, the\nimplementation of ONPO is summarized in Algorithm 1.\nIn the beginning, we initialize π′\n1 and π1 with the supervised\nfine-tuned policy πSFT. At each iteration t, we sample re-\nsponses from the current policy πt and use the preference\nfeedback from the oracle P to construct the dataset Dt. Then\nwe can directly minimize the corresponding loss functions\non Dt to find π′\nt+1 and πt+1 respectively. We use the last\niteration policy πT as the output policy, which is consistent\nwith online RLHF practice (Dong et al., 2024; Wu et al.,\n2024; Zhang et al., 2024).\n5. Discussion\nIn this section, we first discuss the differences between\nONPO and other general preference alignment methods.\nThen we introduce how to extend ONPO to the multi-turn\nsetting.\n5.1. Comparison between ONPO and Other General\nPreference Alignment Methods\nIPO.\nAzar et al. (2024) is the first to address general pref-\nerence alignment in LLMs. The optimization objective of\nIPO is:\nmax\nπ\nEy∼π,y′∼µ [P(y ≻y′)] −τKL(π∥πref),\nwhere µ is a fixed policy. From a game-theoretic perspective,\nthe goal of IPO is to find the best response to µ. However,\nthis approach only ensures that the learned policy outper-\nforms µ, which leaves the possibility that another policy\ncould outperform the learned policy. In contrast, our ap-\nproach focuses on learning the Nash policy in a two-player\ngame. This provides stronger theoretical guarantees, as the\nNash policy will not lose to any other policy.\nNash-MD.\nMunos et al. (2023) is the first to formulate the\nalignment problem as a two-player zero-sum game. Their\ngame objective includes KL regularization terms, which\nensure that the player’s policy remains close to the reference\npolicy πref. The KL terms are weighted by a parameter τ.\nThey proposed an iterative algorithm, Nash-MD, to learn\nthe Nash policy of the game. At each iteration t, the policy\nis updated as:\nπt+1 = argmax\nπ\nP(π ≻π′\nt) −1\nηt\nKL(π, π′\nt),\nwhere π′\nt is a geometric mixture policy of the current policy\nπt and the reference policy πref:\nπ′\nt(y) =\nπt(y)1−ηtτπref(y)ηtτ\nP\ny′ πt(y′)1−ηtτπref(y′)ηtτ .\nNash-MD requires sampling from the mixture policy π′\nt.\nHowever, the response space Y is often exponentially large,\nmaking the exact computation of π′\nt intractable. To address\nthis, Munos et al. (2023) propose sampling from an approx-\nimate policy. The theoretical guarantees of this approxima-\ntion remain unclear. In contrast, our approach only requires\nsampling from the current policy πt, which is straightfor-\nward to implement in practice.\nOnline IPO.\nCalandriello et al. (2024) propose the online\nIPO population loss:\nE\ny,y′∼SG[π]\nyw,yl∼λp(y,y′)\n\"\u0012\nlog π(yw)πref(yl)\nπ(yl)πref(yw) −1\n2τ\n\u00132#\n,\nwhere SG is the stop-gradient operator, which prevents gra-\ndients from propagating through the data-generation process.\nUnlike the offline IPO approach, which always samples\nfrom a fixed policy µ, online IPO leverages responses gen-\nerated by the current policy π.\nSince the policy π is updated throughout training, policy\ngradient methods are used to minimize the objective. How-\never, as discussed earlier, policy gradient methods in RLHF\n5\n\nImproving LLM General Preference Alignment via Optimistic Online Mirror Descent\nhave limitations, including being resource-intensive and un-\nstable to train. In contrast, ONPO avoids these challenges\nby directly minimizing a loss function over a preference\ndataset, offering a more stable and efficient implementation.\nDNO.\nThe theoretical version of DNO (Algorithm 1\nin Rosset et al. (2024)) relies on computing rt(y) =\nEy′∼πt [P(y ≻y′)], which requires taking an expectation\nover the current policy πt. This computation is challenging\nto implement in practice, so Rosset et al. (2024) propose a\npractical version, DNO-Prct (Algorithm 2), where πt+1 is\nupdated as follows:\nargmax\nπ\nEyw,yl∼Dt log\n\u0014\nσ\n\u0012\nη log π(yw)πt(yl)\nπt(yw)π(yl)\n\u0013\u0015\n.\nWhen constructing the dataset Dt, only response pairs with\nlarge margins are selected. This selection is motivated by\nthe fact that, to approximate DNO, the ideal condition is\nσ(rt(yw) −rt(yl)) ≈1. However, this cannot be fully\nachieved since rt(y) ∈[0, 1]. Notably, the objective of\nDNO-Prct is identical to the DPO objective (Rafailov et al.,\n2024b). Therefore, DNO-Prct can be viewed as an iterative\nversion of DPO.\nSPPO.\nWu et al. (2024) propose a self-play algorithm\nSPPO. The policy update in SPPO is:\nπt+1 = argmin\nπ\nEy∼πt\n\u0012\nlog π(y)\nπt(y) −η\n\u0012\nbP(y ≻πt) −1\n2\n\u0013\u00132\n,\nwhere bP is a heuristic approximation of P(y ≻πt). How-\never, obtaining an accurate estimation of P(y ≻πt) is chal-\nlenging in practice. For example, Hoeffding’s inequality\nsuggests that more than 100 queries are needed to ensure\n\f\f\fP(y ≻πt) −bP(y ≻πt)\n\f\f\f ≤0.1. This requirement results\nin high annotation and computation costs, as 100 oracle\nqueries are needed for a single response y. In contrast,\nONPO bypasses the need to estimate P(y ≻πt) and instead\nrelies on binary preference signals between two responses.\nINPO.\nZhang et al. (2024) propose a self-play algorithm,\nINPO, which employs OMD to iteratively update the policy,\nas described in Section 4.1. Leveraging the faster conver-\ngence properties of optimistic OMD, ONPO achieves an\nimproved duality gap bound of O(T −1), compared to the\nO(T −1/2) bound of INPO.\n5.2. Extension to the Multi-Turn Setting\nIn this subsection, we describe how ONPO can be extended\nto the multi-turn setting, which is formulated as a contextual\nMarkov decision process (CMDP) (Shani et al., 2024). The\ninteraction between the LLM and the environment unfolds\nas follows: the LLM starts at a fixed initial state s1 ∈S\nand takes an action y1 ∼π(· | s1). The environment then\ntransitions to the next state s2 ∼P(· | s1, y1) according to\nthe transition dynamics P, and the LLM subsequently takes\naction y2 ∼π(· | s2). This process repeats for H steps, ulti-\nmately reaching the final state sH+1. At the end of the inter-\naction, the preference oracle compares two final states and\nprovides a preference signal: z ∼Ber\n\u0000P(s1\nH+1 ≻s2\nH+1)\n\u0001\n.\nThis CMDP formulation effectively captures various LLM\napplications, including chatbot interactions and token-level\nMDPs (Rafailov et al., 2024a).\nIn the multi-turn setting, the challenge is that preferences\nare only provided for the final states, and there is no direct\nfeedback for intermediate states. To address this, we use\nQ-value functions, which capture the long-term expected\noutcomes, in the optimization objective. For each state sh,\nthe update rule for πt+1(· | sh) is:\nargmax\nπ\n⟨π, Qπt,πt(sh, ·)⟩−1\nη KL(π(· | sh)∥πt(· | sh)),\nwhere Qπt,πt(sh, yh)\n=\nEπt [P(sH+1 ≻πt) | sh, yh]\nand P(s ≻πt) represents Eπt [P(s ≻sH+1)].\nHere\n⟨π, Qπt,πt(sh, ·)⟩measures the probability of π outperform-\ning πt at state sh. The update rule for π′\nt+1 is similar, except\nthat the KL divergence is computed between π and π′\nt.\nThe primary challenge in implementing ONPO in the multi-\nturn setting lies in the efficient estimation of Qπt,πt. Shani\net al. (2024) propose to use an actor-critic framework that\nemploys policy-gradient methods such as PPO (Schulman\net al., 2017) for policy optimization.\nHowever, policy-\ngradient methods are known to exhibit high variance and\nsensitivity to implementation details, leading to increased\ncomputational costs. In this paper, we focus on implement-\ning ONPO in the single-turn setting and leave the implemen-\ntation under the multi-turn setting for future work.\n6. Experiments\n6.1. Main Results\nExperiment Setup.\nWe implement ONPO following the\nonline RLHF workflow described in Dong et al. (2024).\nTwo base models are used as the initial policy π1: Llama-\n3-SFT1, based on Llama-3-8B (Dubey et al., 2024), and\nMistral-Instruct-v0.32, an instruct fine-tuned version of the\nMistral-7B-v0.3. For the general preference oracle, we use\na pairwise preference model3, which demonstrates better\nperformance compared to the BT reward model (Zhang\n1https://huggingface.co/RLHFlow/\nLLaMA3-SFT\n2https://huggingface.co/mistralai/\nMistral-7B-Instruct-v0.3\n3https://huggingface.co/RLHFlow/\npair-preference-model-LLaMA3-8B\n6\n\nImproving LLM General Preference Alignment via Optimistic Online Mirror Descent\nTable 1. Results on three benchmarks. “ONPO+Mistral-It” refers to tuning the Mistral-Instruct model with ONPO, while “ONPO+Llama-\n3-SFT” refers to tuning the Llama-3-SFT model with ONPO. Results where the baseline outperforms ONPO are underlined.\nModel\nSize\nAlpacaEval 2.0\nArena-Hard\nMT-Bench\nIterative DPO + Mistral-It\n7B\n32.0\n22.2\n7.35\nSPPO + Mistral-It\n7B\n33.1\n24.5\n7.51\nINPO + Mistral-It\n7B\n35.3\n25.3\n7.46\nONPO + Mistral-It\n7B\n42.8\n29.7\n7.68\nIterative DPO + Llama-3-SFT\n8B\n28.3\n31.9\n8.34\nSPPO + Llama-3-SFT\n8B\n38.5\n32.9\n8.23\nINPO + Llama-3-SFT\n8B\n44.2\n37.0\n8.28\nONPO + Llama-3-SFT\n8B\n48.6\n36.4\n8.40\nLlama-3-8B-it\n8B\n24.8\n21.2\n7.97\nTulu-2-DPO-70B\n70B\n21.2\n15.0\n7.89\nLlama-3-70B-it\n70B\n34.4\n41.1\n8.95\nMixtral-8x22B-it\n141B\n30.9\n36.4\n8.66\nGPT-3.5-turbo-0613\n-\n22.7\n24.8\n8.39\nGPT-4-0613\n-\n30.2\n37.9\n9.18\nClaude-3-Opus\n-\n40.5\n60.4\n9.00\nGPT-4 Turbo (04/09)\n-\n55.0\n82.6\n-\net al., 2024). Training details for the preference model are\navailable in Dong et al. (2024).\nAt each iteration, the current policy generates K = 8 re-\nsponses using a set of prompts4. To select yw (winner) and\nyl (loser), we follow the tournament approach in Zhang et al.\n(2024), where the eight responses are compared pairwise to\nidentify the winning and losing responses.\nSince online or iterative alignment methods have been\nshown to outperform offline counterparts, we focus on com-\nparing ONPO with other online methods for a fair evaluation.\nThese include iterative DPO (Dong et al., 2024), SPPO (Wu\net al., 2024) and INPO (Zhang et al., 2024), where the latter\ntwo are general preference alignment approaches.\nWe evaluate the models on three representative benchmarks:\nAlpacaEval 2.0 (Li et al., 2023a), Arena-Hard (Li et al.,\n2024) and MT-Bench (Zheng et al., 2024). AlpacaEval\n2.0 has 805 instructions from five datasets, including self-\ninstruct test set (Wang et al., 2022), Open Assistant test set,\nAnthropic’s helpful test set (Bai et al., 2022b), Vicuna test\nset (Zheng et al., 2024) and Koala test set (Geng et al., 2023).\nArena-Hard includes 500 challenging user queries from\nChatbot Arena. Both AlpacaEval 2.0 and Arena-Hard com-\npare model-generated answers against reference answers\nfrom a baseline model, using GPT-4 Preview-1106 as the\njudge model. We report the win rate for Arena-Hard and\n4https://huggingface.co/datasets/RLHFlow/\nprompt-collection-v0.1\nthe length-controlled (LC) win rate (Dubois et al., 2024)\nfor AlpacaEval 2.0. MT-Bench consists of 80 multi-turn\nquestions, where responses are rated by GPT-4 on a 1-10\nscale, with the average rating reported.\nResults.\nThe model performance is summarized in Ta-\nble 1. Our results show that ONPO consistently outper-\nforms or achieves comparable performance to the baselines\nacross both base models. Among the three benchmarks,\nthe length-controlled (LC) win rate in AlpacaEval 2.0 ex-\nhibits the highest 0.98 Spearman correlation with Chatbot\nArena rankings (Dubois et al., 2024). In this benchmark,\nONPO outperforms the strongest baseline by a clear mar-\ngin—achieving a 9.9% improvement on Llama-3-SFT and a\n21.2% improvement on Mistral-It. These results align with\nour theoretical findings, demonstrating that ONPO benefits\nfrom an improved bound on the duality gap. We also com-\npare ONPO with other LLMs that have significantly larger\nparameters, such as Llama-3-70B-it, Mixtral-8x22B-it and\nGPT-4-Turbo. Remarkably, our ONPO even outperforms\nmodels with at least nine times more parameters.\n6.2. More Results on Academic Tasks\nIn this subsection, we evaluate the model’s reasoning\nand calibration abilities across six academic benchmarks:\nGPQA (Rein et al., 2023) for graduate-level science ques-\ntion answering, MMLU-Pro (Wang et al., 2024) for mul-\ntitask language understanding, Hellaswag (Zellers et al.,\n2019) for commonsense inference, Winogrande (Sakaguchi\n7\n\nImproving LLM General Preference Alignment via Optimistic Online Mirror Descent\nTable 2. Model performance on more academic benchmarks (AVG: average).\nModel\nGPQA\nHellaswag\nMMLU-Pro\nWinogrande\nTruthfulQA\nGSM8K\nAVG\nMistral-It\n30.1\n83.5\n30.4\n74.2\n59.7\n49.5\n54.6\nIterative DPO\n29.6\n83.3\n28.0\n75.1\n64.0\n45.7\n54.3\nSPPO\n28.7\n83.5\n28.1\n73.9\n66.4\n49.9\n55.1\nINPO\n28.8\n82.9\n28.9\n74.9\n64.7\n46.3\n54.4\nONPO\n30.4\n83.7\n29.9\n75.1\n65.5\n47.8\n55.4\nFigure 1. Performance of ONPO with different values of η on\nArena-Hard and AlpacaEval 2.0. ONPO consistently outperforms\nthe best baseline, which achieves a win rate of 25.3 on Arena-Hard\nand 35.3 on AlpacaEval, respectively.\net al., 2021) for difficult commonsense reasoning, Truth-\nfulQA (Lin et al., 2021) to assess the model’s tendency to\nreproduce falsehoods, and GSM8K (Cobbe et al., 2021) for\nmathematical reasoning.\nIt is important to note that these benchmarks primarily evalu-\nate a model’s intrinsic knowledge and capabilities, which are\ndeveloped during the pre-training stage rather than the align-\nment stage. However, as observed in prior work (Ouyang\net al., 2022; OpenAI, 2023), alignment can sometimes have\na negative impact on these abilities—a phenomenon known\nas the “alignment tax”. Therefore, our purpose in presenting\nthese results is to verify that our alignment method preserves\nthe model’s abilities rather than demonstrating performance\nimprovements.\nWe show the results using Mistral-Instruct-v0.3 as the base\nmodel and compare ONPO with three baselines as well as\nthe base model itself. The results in Table 2 show that ONPO\nachieves a slightly higher average performance than both\nthe base model and the baselines, demonstrating that ONPO\ndoes not over-align the model and effectively preserves its\nintrinsic knowledge and abilities.\n6.3. Hyperparameter Sensitivity Analysis\nIn this subsection, we analyze the sensitivity of ONPO to\nthe hyperparameter η, which serves as the learning rate in\nthe update rule. We conduct experiments using Mistral-\nInstruct-v0.3 as the base model and vary η from 200/3 to\n200. The results, presented in Figure 1, indicate that ONPO\nconsistently achieves strong performance across different\nvalues of η and outperforms the baselines, demonstrating its\nrobustness to hyperparamter variations.\n7. Conclusion and Future Work\nWe propose Optimistic Nash Policy Optimization (ONPO),\na novel approach for aligning LLMs with general prefer-\nences via self-play. By integrating optimistic online mirror\ndescent, ONPO achieves an improved duality gap bound for\napproximating the Nash policy of the game. Our experimen-\ntal results demonstrate that ONPO consistently outperforms\nor matches state-of-the-art general preference alignment\nmethods across multiple benchmarks. For future work, we\naim to explore the implementation of ONPO under the multi-\nturn setting. In addition, we plan to design different strate-\ngies for actively selecting preference data to further enhance\nalignment performance.\nImpact Statement\nThis paper presents work whose goal is to advance the field\nof Machine Learning. There are many potential societal\nconsequences of our work, none of which we feel must be\nspecifically highlighted here.\nReferences\nAzar, M. G., Guo, Z. D., Piot, B., Munos, R., Rowland, M.,\nValko, M., and Calandriello, D. A general theoretical\nparadigm to understand learning from human preferences.\nIn International Conference on Artificial Intelligence and\nStatistics, pp. 4447–4455. PMLR, 2024.\nBai, Y., Jin, C., and Yu, T. Near-optimal reinforcement\nlearning with self-play. Advances in neural information\nprocessing systems, 33:2159–2170, 2020.\n8\n\nImproving LLM General Preference Alignment via Optimistic Online Mirror Descent\nBai, Y., Jin, C., Mei, S., and Yu, T. Near-optimal learning\nof extensive-form games with imperfect information. In\nInternational Conference on Machine Learning, pp. 1337–\n1382. PMLR, 2022a.\nBai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., Das-\nSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T.,\net al. Training a helpful and harmless assistant with rein-\nforcement learning from human feedback. arXiv preprint\narXiv:2204.05862, 2022b.\nBradley, R. A. and Terry, M. E. Rank analysis of incom-\nplete block designs: I. the method of paired comparisons.\nBiometrika, 39(3/4):324–345, 1952.\nCalandriello, D., Guo, D., Munos, R., Rowland, M., Tang,\nY., Pires, B. A., Richemond, P. H., Lan, C. L., Valko, M.,\nLiu, T., et al. Human alignment of large language models\nthrough online preference optimisation. arXiv preprint\narXiv:2403.08635, 2024.\nChen, X. and Peng, B. Hedging in games: Faster conver-\ngence of external and swap regrets. Advances in Neural\nInformation Processing Systems, 33:18990–18999, 2020.\nChoshen, L., Fox, L., Aizenbud, Z., and Abend, O. On the\nweaknesses of reinforcement learning for neural machine\ntranslation. arXiv preprint arXiv:1907.01752, 2019.\nChristiano, P. F., Leike, J., Brown, T., Martic, M., Legg,\nS., and Amodei, D. Deep reinforcement learning from\nhuman preferences. Advances in neural information pro-\ncessing systems, 30, 2017.\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H.,\nKaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,\nR., et al. Training verifiers to solve math word problems.\narXiv preprint arXiv:2110.14168, 2021.\nDaskalakis, C., Deckelbaum, A., and Kim, A. Near-optimal\nno-regret algorithms for zero-sum games. In Proceedings\nof the twenty-second annual ACM-SIAM symposium on\nDiscrete Algorithms, pp. 235–254. SIAM, 2011.\nDaskalakis, C., Fishelson, M., and Golowich, N. Near-\noptimal no-regret learning in general games. Advances\nin Neural Information Processing Systems, 34:27604–\n27616, 2021.\nDong, H., Xiong, W., Pang, B., Wang, H., Zhao, H., Zhou,\nY., Jiang, N., Sahoo, D., Xiong, C., and Zhang, T. Rlhf\nworkflow: From reward modeling to online rlhf. arXiv\npreprint arXiv:2405.07863, 2024.\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle,\nA., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan,\nA., et al. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783, 2024.\nDubois, Y., Galambosi, B., Liang, P., and Hashimoto, T. B.\nLength-controlled alpacaeval: A simple way to debias\nautomatic evaluators. arXiv preprint arXiv:2404.04475,\n2024.\nEthayarajh, K., Xu, W., Muennighoff, N., Jurafsky, D., and\nKiela, D. Kto: Model alignment as prospect theoretic\noptimization. arXiv preprint arXiv:2402.01306, 2024.\nFreund, Y. and Schapire, R. E. Adaptive game playing using\nmultiplicative weights. Games and Economic Behavior,\n29(1-2):79–103, 1999.\nGeng, X., Gudibande, A., Liu, H., Wallace, E., Abbeel,\nP.,\nLevine,\nS.,\nand Song,\nD.\nKoala:\nA dia-\nlogue model for academic research.\nBlog post,\nApril 2023. URL https://bair.berkeley.edu/\nblog/2023/04/03/koala/.\nJiang, D., Ren, X., and Lin, B. Y. Llm-blender: Ensembling\nlarge language models with pairwise ranking and genera-\ntive fusion. arXiv preprint arXiv:2306.02561, 2023.\nJin, C., Liu, Q., Wang, Y., and Yu, T. V-learning–a simple,\nefficient, decentralized algorithm for multiagent rl. arXiv\npreprint arXiv:2110.14555, 2021.\nKorbak, T., Perez, E., and Buckley, C. L. Rl with kl penalties\nis better viewed as bayesian inference. arXiv preprint\narXiv:2205.11275, 2022.\nKozuno, T., M´enard, P., Munos, R., and Valko, M. Model-\nfree learning for two-player zero-sum partially observ-\nable markov games with perfect recall. arXiv preprint\narXiv:2106.06279, 2021.\nKroer, C., Waugh, K., Kılınc¸-Karzan, F., and Sandholm, T.\nFaster algorithms for extensive-form game solving via\nimproved smoothing functions. Mathematical Program-\nming, 179(1):385–417, 2020.\nLambert, N., Pyatkin, V., Morrison, J., Miranda, L., Lin,\nB. Y., Chandu, K., Dziri, N., Kumar, S., Zick, T., Choi,\nY., et al. Rewardbench: Evaluating reward models for\nlanguage modeling. arXiv preprint arXiv:2403.13787,\n2024.\nLattimore, T. and Szepesv´ari, C. Bandit algorithms. Cam-\nbridge University Press, 2020.\nLee, C.-W., Kroer, C., and Luo, H. Last-iterate convergence\nin extensive-form games. Advances in Neural Information\nProcessing Systems, 34:14293–14305, 2021.\nLi, T., Chiang, W.-L., Frick, E., Dunlap, L., Zhu, B., Gon-\nzalez, J. E., and Stoica, I. From live data to high-quality\nbenchmarks: The arena-hard pipeline, 2024.\n9\n\nImproving LLM General Preference Alignment via Optimistic Online Mirror Descent\nLi, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani, I.,\nGuestrin, C., Liang, P., and Hashimoto, T. B. Alpacaeval:\nAn automatic evaluator of instruction-following models,\n2023a.\nLi, Z., Xu, T., Zhang, Y., Lin, Z., Yu, Y., Sun, R., and Luo,\nZ.-Q. Remax: A simple, effective, and efficient rein-\nforcement learning method for aligning large language\nmodels. In Forty-first International Conference on Ma-\nchine Learning, 2023b.\nLin, S., Hilton, J., and Evans, O. Truthfulqa: Measuring\nhow models mimic human falsehoods. arXiv preprint\narXiv:2109.07958, 2021.\nLiu, Q., Yu, T., Bai, Y., and Jin, C. A sharp analysis of\nmodel-based reinforcement learning with self-play. In\nInternational Conference on Machine Learning, pp. 7001–\n7010. PMLR, 2021.\nMai, T., Mihail, M., Panageas, I., Ratcliff, W., Vazirani, V.,\nand Yunker, P. Cycles in zero-sum differential games and\nbiological diversity. In Proceedings of the 2018 ACM\nConference on Economics and Computation, pp. 339–\n350, 2018.\nMao, W. and Bas¸ar, T. Provably efficient reinforcement\nlearning in decentralized general-sum markov games. Dy-\nnamic Games and Applications, 13(1):165–186, 2023.\nMay, K. O. Intransitivity, utility, and the aggregation of\npreference patterns. Econometrica: Journal of the Econo-\nmetric Society, pp. 1–13, 1954.\nMunos, R., Valko, M., Calandriello, D., Azar, M. G., Row-\nland, M., Guo, Z. D., Tang, Y., Geist, M., Mesnard, T.,\nMichi, A., et al. Nash learning from human feedback.\narXiv preprint arXiv:2312.00886, 2023.\nOpenAI, R. Gpt-4 technical report. arxiv 2303.08774. View\nin Article, 2(5), 2023.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\net al. Training language models to follow instructions\nwith human feedback. Advances in neural information\nprocessing systems, 35:27730–27744, 2022.\nPeng, B., Song, L., Tian, Y., Jin, L., Mi, H., and Yu, D.\nStabilizing rlhf through advantage model and selective\nrehearsal. arXiv preprint arXiv:2309.10202, 2023.\nRafailov, R., Hejna, J., Park, R., and Finn, C. From r to\nq∗: Your language model is secretly a q-function. arXiv\npreprint arXiv:2404.12358, 2024a.\nRafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Er-\nmon, S., and Finn, C. Direct preference optimization:\nYour language model is secretly a reward model. Ad-\nvances in Neural Information Processing Systems, 36,\n2024b.\nRakhlin, S. and Sridharan, K. Optimization, learning, and\ngames with predictable sequences. Advances in Neural\nInformation Processing Systems, 26, 2013.\nRein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang,\nR. Y., Dirani, J., Michael, J., and Bowman, S. R. Gpqa:\nA graduate-level google-proof q&a benchmark. arXiv\npreprint arXiv:2311.12022, 2023.\nRosset, C., Cheng, C.-A., Mitra, A., Santacroce, M., Awadal-\nlah, A., and Xie, T. Direct nash optimization: Teaching\nlanguage models to self-improve with general preferences.\narXiv preprint arXiv:2404.03715, 2024.\nRoy, A., Chen, Y., Balasubramanian, K., and Mohapa-\ntra, P.\nOnline and bandit algorithms for nonstation-\nary stochastic saddle-point optimization. arXiv preprint\narXiv:1912.01698, 2019.\nSakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y.\nWinogrande: An adversarial winograd schema challenge\nat scale. Communications of the ACM, 64(9):99–106,\n2021.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and\nKlimov, O. Proximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347, 2017.\nShani, L., Rosenberg, A., Cassel, A., Lang, O., Calandriello,\nD., Zipori, A., Noga, H., Keller, O., Piot, B., Szpektor, I.,\net al. Multi-turn reinforcement learning from preference\nhuman feedback. arXiv preprint arXiv:2405.14655, 2024.\nSyrgkanis, V., Agarwal, A., Luo, H., and Schapire, R. E.\nFast convergence of regularized learning in games. Ad-\nvances in Neural Information Processing Systems, 28,\n2015.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., et al. Llama 2: Open foundation and fine-\ntuned chat models. arXiv preprint arXiv:2307.09288,\n2023.\nTversky, A. Intransitivity of preferences. Psychological\nreview, 76(1):31, 1969.\nWang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A.,\nKhashabi, D., and Hajishirzi, H. Self-instruct: Aligning\nlanguage models with self-generated instructions. arXiv\npreprint arXiv:2212.10560, 2022.\nWang, Y., Ma, X., Zhang, G., Ni, Y., Chandra, A., Guo, S.,\nRen, W., Arulraj, A., He, X., Jiang, Z., et al. Mmlu-pro:\n10\n\nImproving LLM General Preference Alignment via Optimistic Online Mirror Descent\nA more robust and challenging multi-task language under-\nstanding benchmark. arXiv preprint arXiv:2406.01574,\n2024.\nWei, C.-Y., Hong, Y.-T., and Lu, C.-J. Online reinforce-\nment learning in stochastic games. Advances in Neural\nInformation Processing Systems, 30, 2017.\nWei, C.-Y., Lee, C.-W., Zhang, M., and Luo, H. Linear\nlast-iterate convergence in constrained saddle-point opti-\nmization. arXiv preprint arXiv:2006.09517, 2020.\nWu, Y., Sun, Z., Yuan, H., Ji, K., Yang, Y., and Gu, Q.\nSelf-play preference optimization for language model\nalignment. arXiv preprint arXiv:2405.00675, 2024.\nXie, T., Foster, D. J., Krishnamurthy, A., Rosset, C., Awadal-\nlah, A., and Rakhlin, A. Exploratory preference optimiza-\ntion: Harnessing implicit q*-approximation for sample-\nefficient rlhf. arXiv preprint arXiv:2405.21046, 2024.\nXiong, W., Dong, H., Ye, C., Wang, Z., Zhong, H., Ji, H.,\nJiang, N., and Zhang, T. Iterative preference learning\nfrom human feedback: Bridging theory and practice for\nrlhf under kl-constraint. In Forty-first International Con-\nference on Machine Learning, 2024.\nYe, C., Xiong, W., Zhang, Y., Jiang, N., and Zhang, T. A the-\noretical analysis of nash learning from human feedback\nunder general kl-regularized preference. arXiv preprint\narXiv:2402.07314, 2024.\nYuan, W., Pang, R. Y., Cho, K., Sukhbaatar, S., Xu, J.,\nand Weston, J. Self-rewarding language models. arXiv\npreprint arXiv:2401.10020, 2024.\nYuan, Z., Yuan, H., Tan, C., Wang, W., Huang, S., and\nHuang, F. Rrhf: Rank responses to align language mod-\nels with human feedback without tears. arXiv preprint\narXiv:2304.05302, 2023.\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi,\nY. Hellaswag: Can a machine really finish your sentence?\narXiv preprint arXiv:1905.07830, 2019.\nZhang, Y., Yu, D., Peng, B., Song, L., Tian, Y., Huo, M.,\nJiang, N., Mi, H., and Yu, D. Iterative nash policy op-\ntimization: Aligning llms with general preferences via\nno-regret learning.\narXiv preprint arXiv:2407.00617,\n2024.\nZheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z.,\nZhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging\nllm-as-a-judge with mt-bench and chatbot arena. Ad-\nvances in Neural Information Processing Systems, 36,\n2024.\nZinkevich, M., Johanson, M., Bowling, M., and Piccione, C.\nRegret minimization in games with incomplete informa-\ntion. Advances in neural information processing systems,\n20, 2007.\n11\n\nImproving LLM General Preference Alignment via Optimistic Online Mirror Descent\nA. Proofs for Section 4\nA.1. Proof for Theorem 4.1\nProof. According to the regret analysis of OMD (Lattimore & Szepesv´ari, 2020), for any policy π, we have\nT\nX\nt=1\n⟨π, rt⟩−\nT\nX\nt=1\n⟨πt, rt⟩≤KL(π∥π1)\nη\n+ η\nT\nX\nt=1\n∥rt∥2\n∞\n≤2\n√\nTD.\nThe rest proof follows from Theorem 3 in Zhang et al. (2024).\nA.2. Proof for Theorem 4.2\nProof. Let ψ(π) = P\ny π(y) log π(y), the KL divergence between π1 and π2 can also be written as the Bregman divergence\nterm:\nKL(π1∥π2) = Dψ(π1, π2) = ψ(π1) −ψ(π2) −⟨∇ψ(π2), π1 −π2⟩.\nSince ψ is strongly convex with respect to L1 norm, we can apply regret analysis from Rakhlin & Sridharan (2013);\nSyrgkanis et al. (2015) and obtain that for any π′\nT\nX\nt=1\n⟨π′ −πt, rt⟩≤KL(π′∥π′\n1)\nη\n+ η\nT\nX\nt=1\n∥rt −rt−1∥2\n∞−1\n4η\nT\nX\nt=2\n∥πt −πt−1∥2\n1.\nWe observe that for any t ≥2 and any y,\n|rt(y) −rt−1(y)| = |\nX\ny′\nP(y ≻y′)(πt(y) −πt−1(y))| ≤∥πt −πt−1∥1.\nOnce we have\n1\n4η ≥η, the terms η∥rt −rt−1∥2\n∞and −1\n4η∥πt −πt−1∥2\n1 cancel out and we get\nT\nX\nt=1\n⟨π′ −πt, rt⟩≤2\n√\nD.\nNext, we decompose the duality gap as:\nDualGap(¯π) = max\nπ1 J(π1, ¯π) −1\n2\n|\n{z\n}\nTerm A\n+ 1\n2 −min\nπ2 J(¯π, π2)\n|\n{z\n}\nTerm B\n.\nWe show how to bound Term A and Term B is bounded similarly due to the symmetric nature of the game. Let π′ =\nargmaxπ1 J(π1, ¯π), we have\nJ(π′, ¯π) −1\n2 = 1\nT\nT\nX\nt=1\nJ(π′, πt) −J(πt, πt)\n= 1\nT\nT\nX\nt=1\n⟨π′ −πt, rt⟩\n≤2\n√\nD\nT\n.\nThe proof is finished by also having 1\n2 −minπ2 J(¯π, π2) ≤2\n√\nD\nT\n.\nB. Additional Experiment Details\nFor the implementation of ONPO, we follow the hyperparameters in Dong et al. (2024), including the cosine learning\nrate scheduler with a peak learning rate of 5 × 10−7, a 0.03 warm-up ratio, and a global batch size of 128. We use a\ngrid search for 1/η over [0.1, 0.05, 0.02, 0.01, 0.005] and set 1/η = 0.01. Llama-3-SFT is trained for 5 iterations, while\nMistral-Instruct, having already undergone instruction fine-tuning, is thereby trained for 3 iterations.\n12\n",
  "metadata": {
    "source_path": "papers/arxiv/Improving_LLM_General_Preference_Alignment_via_Optimistic_Online_Mirror\n__Descent_198ae71ac7adf894.pdf",
    "content_hash": "198ae71ac7adf89414a9dd478f9231bdafa085909f2d99b13a519c5fef06f558",
    "arxiv_id": null,
    "title": "Improving LLM General Preference Alignment  via Optimistic Online Mirror Descent",
    "author": "Yuheng Zhang, Dian Yu, Tao Ge, Linfeng Song, Zhichen Zeng, Haitao Mi, Nan Jiang, Dong Yu",
    "creation_date": "D:20250225022415Z",
    "published": "2025-02-25T02:24:15",
    "pages": 12,
    "size": 556664,
    "file_mtime": 1740470218.393339
  }
}