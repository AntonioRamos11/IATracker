{
  "text": "TDMPBC: SELF-IMITATIVE REINFORCEMENT LEARNING FOR\nHUMANOID ROBOT CONTROL\nZifeng Zhuang1∗\nDiyuan Shi1∗\nRunze Suo1\nXiao He1\nHongyin Zhang1\nTing Wang1†\nShangke Lyu1†\nDonglin Wang1†\n1Westlake University\n∗Equal Contribution\n†Corresponding Authors\nABSTRACT\nComplex high-dimensional spaces with high Degree-of-Freedom and complicated action spaces, such as\nhumanoid robots equipped with dexterous hands, pose significant challenges for reinforcement learning (RL)\nalgorithms, which need to wisely balance exploration and exploitation under limited sample budgets. In\ngeneral, feasible regions for accomplishing tasks within complex high-dimensional spaces are exceedingly\nnarrow. For instance, in the context of humanoid robot motion control, the vast majority of space corresponds to\nfalling, while only a minuscule fraction corresponds to standing upright, which is conducive to the completion\nof downstream tasks. Once the robot explores into a potentially task-relevant region, it should place greater\nemphasis on the data within that region. Building on this insight, we propose the Self-Imitative Reinforcement\nLearning (SIRL) framework, where the RL algorithm also imitates potentially task-relevant trajectories.\nSpecifically, trajectory return is utilized to determine its relevance to the task and an additional behavior\ncloning is adopted whose weight is dynamically adjusted based on the trajectory return. As a result, our\nproposed algorithm achieves 120% performance improvement on the challenging HumanoidBench with 5%\nextra computation overhead. With further visualization, we find the significant performance gain does lead to\nmeaningful behavior improvement that several tasks are solved successfully.\n1\nIntroduction\nHumanoid robots with dexterous hands have vast and promis-\ning application scenarios due to their behavior flexibility and\nhuman-like morphology [Bonci et al., 2021, Stasse and Flayols,\n2019, Choudhury et al., 2018]. Unfortunately, such complex\nhigh-dimensional space is extremely challenging for policy\nlearning with online reinforcement learning (RL) [Sferrazza\net al., 2024, Peters et al., 2003], which interactively explores the\nenvironment and learns optimal decision-making from scratch\nunder the guidance of reward function. This paradigm has\nachieved significant success in fields like gaming AI [Silver\net al., 2016, 2017, Schrittwieser et al., 2020, Hessel et al., 2018]\nand quadrupedal robots control [Miki et al., 2022, Lee et al.,\n2020, Hwangbo et al., 2019]. But when facing humanoid robots\nequipped with dexterous hands, existing RL methods struggle\nto learn effectively and efficiently. Even sample-efficient model-\nbased state-of-the-art (SOTA) algorithms, such as TD-MPC2\n[Hansen et al., 2023] and DreamerV3 [Hafner et al., 2023],\nperform poorly in humanoid control.\nIn high-dimensional complex spaces, the regions capable of\naccomplishing tasks are typically exceedingly narrow and dif-\nficult to explore compared to the entire space. For humanoid\nrobot motion control, upright posture is a prerequisite to com-\nFigure 1: Tasks accomplished by TDMPBC: 1) navigating\nthrough pole-filled areas by staying close to the wall, 2) main-\ntain balance on unstable board with the spherical pivot beneath\nthe board in motion, 3) window cleaning with arm-controled\ncleaning tools and 4) achieving a successful basketball shot.\nplete any downstream tasks. Maintaining an upright posture is\nsimilar to balancing an inverted pendulum, where only an ex-\ntremely small vertical region within the entire space can sustain\nthis posture, while other regions lead to rapid falls. Therefore,\nwhen the algorithm explores an upright posture, the humanoid\nrobot should place particular emphasis on it.\nFurthermore, upright posture can be intuitively reflected in\nreturn. Only if the current timestep maintains an upright pos-\nture is it possible to continue obtaining rewards in the follow-\ning timesteps. If the current step results in a fall, given that\nhumanoid robots are virtually incapable of standing up after\narXiv:2502.17322v1  [cs.RO]  24 Feb 2025\n\nTDMPBC: Self-Imitative Reinforcement Learning for Humanoid Robot Control\nPREPRINT\n0\n50\n100\nNormalized Return\n+124%\n+62%\n+559%\n14 Locomotion Tasks\n0\n50\n100\n+123%\n+55%\n+196%\n15 Manipulation Tasks\nTDMPBC@2M\nTD-MPC2@2M\nDreamerV3@10M\nSAC@10M\nFigure 2: Performance of TDMPBC with 2M interaction steps\ncompared to the baselines TD-MPC2 with 2M, DreamerV3\nwith 10M and SAC with 10M on HumanoidBench.\nfalling, subsequent rewards become unattainable. Under the\ncumulative effect, the ability to maintain an upright posture\nwill ultimately be reflected very prominently in the return. To\nsummarize, the return can be approximated as an indicator\nof whether the humanoid robot has entered a task-completing\nregion in its control.\nBased on the above observation and analysis, we propose\na framework called Self-Imitative Reinforcement Learning\n(SIRL) to assist online learning in complex high-dimensional\nhumanoid robot control. Building upon the foundation of RL\nalgorithms, SIRL additionally imitates trajectories with high\nreturns. This enables the humanoid robot to quickly learn the\nupright posture, thereby accelerating the completion of down-\nstream tasks. Specifically, we augment the policy training\nobjective in TD-MPC2 [Hansen et al., 2023, 2022a] with an\nadditional behavior cloning term whose weight is dynamically\nadjusted based on the trajectory return. Since the trajectories\nbeing imitated are generated by the algorithm itself during ex-\nploration, rather than expert demonstrations as in traditional\nimitation learning (IL), our framework is termed self-imitative.\nAdditionally, we refer to TD-MPC2 augmented with a behavior\ncloning loss as TDMPBC.\nWe have validated our proposed TDMPBC on HumanoidBench\n[Sferrazza et al., 2024] which contains 31 challenging tasks\nfor the Unitree H1 robot with dexterous hands. Compared\nwith the baseline TD-MPC2, our proposed method achieves\nan approximate increase more than 120% for the normalized\nreturn1. What’s more, our method enjoys a significantly faster\nconvergence rate and excels in terms of sample-efficiency. In\nhumanoid locomotion tasks, our algorithm is capable of com-\npleting 8 tasks out of 14 with only 2M training steps, whereas\nthe baseline could only accomplish 1 task.\n2\nPreliminaries\nReinforcement Learning\nReinforcement Learning (RL) is\na framework of sequential decision.\nTypically, this prob-\nlem is formulated by a Markov Decision Process (MDP)\n1We normalize the return to the range [0, Rtarget]. For the push and\npackage manipulation tasks, where the return may be negative, we\ndo not display them in Figure 2. The performance of TDMPBC on\nthese two tasks is on par with the baseline, which does not affect the\nabove conclusions we have drawn.\nM = {S, A, r, p, d0, γ}, with state space S, action space\nA, scalar reward function r, transition dynamics p, initial\nstate distribution d0(s0) and discount factor γ [Sutton et al.,\n1998]. The objective of RL is to learn a policy π (at|st) at\ntimestep t, where at ∈A and st ∈S. Given this definition,\nthe distribution of trajectory τ = (s0, a0, · · · , sH, aH) gener-\nated by the interaction with the environment M is Pπ (τ) =\nd0(s0) QT\nt=0 π (at|st) p (st+1|st, at), where T is the length of\nthe trajectory and can be infinite. Then, the goal of RL can\nbe written as an expectation under the trajectory distribution\nJ (π) = Eτ∼Pπ(τ)\nhPT\nt=0 γtr(st, at)\ni\n. This objective can\nalso be measured by a value function Qπ (s, a), the expected\ndiscounted return given the action a in state s: Qπ (s, a) =\nEτ∼Pπ(τ|s,a)\nhPT\nt=0 γtr(st, at)|s0 = s, a0 = a\ni\n.\nTD-MPC2\nModel-based RL algorithm TD-MPC2 learns a\nlatent decoder-free world model and selects actions during\ninference via planning with learned model [Hansen et al., 2023].\nSpecifically, TD-MPC2 consists of five components:\nState Encoder:\nzt = hϕ (st) ,\nLatent Dynamics:\nzt+1 = dϕ (zt, at) ,\nReward Function:\nˆrt = Rϕ (zt, at) ,\nValue Function:\nˆqt = Qϕ (zt, at) ,\nPolicy Prior:\nˆat ∼πθ (·|zt) ,\nwhere st is the states, at is the actions and zt is the latent\nrepresentation. The encoder hϕ, dynamics dϕ, reward Rϕ,\nvalue Qϕ compose the world model in TD-MPC2 that is trained\nby minimizing the following objective:\nL (ϕ) = E(st,at,rt,st+1)H\nt=0∼B\n\" H\nX\nt=0\nλt\u0010\nl(t)\nd\n+ l(t)\nr\n+ l(t)\nq\n\u0011#\n,\nl(t)\nd\n= ∥dϕ (zt, at) −¯zt+1∥2\n2 ,\nl(t)\nr\n= CE\nh\nRϕ (zt, at) −rt\ni\n,\nl(t)\nq\n= CE\n\u0014\nQϕ (zt, at) −\n\u0010\nrt + γQ¯ϕ\n\u0000zt+1, πθ (·|zt+1)\n\u0001\u0011\u0015\n,\nwhere (st, at, rt, st+1)H\nt=0 is a trajectory with length H sam-\npled from the replay buffer B, ¯zt+1 = h ¯ϕ (st+1) is the target\nlatent representation and CE is the cross-entropy loss. The pol-\nicy prior π is a stochastic maximum entropy policy that learns\nto maximize the objective:\nL (θ) = EsH\nt=0∼B\n\" H\nX\nt=0\nλth\nQϕ (zt, πθ (·|zt)) −αH (π (·|zt))\ni#\n,\nwhere H is the entropy of policy π and the parameter α can be\nautomatically adjusted based on an entropy target [Haarnoja\net al., 2018] or moving statistics [Hafner et al., 2023].\nDuring inference, TD-MPC2 plan actions using a sampling-\nbased planner Model Predictive Path Integral (MPPI) [Williams\net al., 2015] to iteratively fits a time-dependent multivariate\nGaussian with diagonal covariance over the trajectory space\n2\n\nTDMPBC: Self-Imitative Reinforcement Learning for Humanoid Robot Control\nPREPRINT\nsuch that the estimated return ˆR is maximized:\nˆR =\nH−1\nX\nt=0\nγtRϕ (zt, at) + γHQϕ (zt, at) .\n(1)\nTo accelerate this planning, a fraction of trajectories are gener-\nated by the learned policy prior πθ.\nImitation Learning\nIn Imitation Learning (IL) [Zare et al.,\n2024], the ground truth reward is not observed and only a set of\ndemonstrations D = {(st, at)} collected by the expert policy\nis provided. The goal of IL is to recover a policy that matches\nthe expert. Behavior cloning [Pomerleau, 1988] is the most\nsimple and straightforward approach\nπBC = argmax\nπ\nE(st,at)∼D [log π (at|st)] .\n3\nSelf-Imitative Reinforcement Learning\nIn this section, we first highlight the relationship between the\nmotion control of humanoid robots and the upright posture.\nWe further observe that maintaining an upright posture corre-\nsponds to higher returns. Building on this insight, we introduce\nSelf-Imitative Reinforcement Learning (SIRL) and present the\nimplementation based on the model-based algorithm TD-MPC2\n[Hansen et al., 2023]. During the online reinforcement learning\nprocess, SIRL provides additional guidance to the humanoid\nrobot to imitate trajectories with high returns. Finally, we an-\nalyze the characteristics and applicability of this framework\nfrom multiple perspectives.\n3.1\nMotivation and Analysis\nIn humanoid robot motion control, stable upright posture is the\nessential foundation for both locomotion tasks and whole-body\nmanipulation tasks. This point is not only intuitive but also can\nbe reflected in the design of the reward function. For example,\nin Isaac Lab, the reward term related to maintaining upright\nposture is assigned one extremely high weight:\nr1 = 200 × rupright + 1.0 × rvelocity + · · · ,\n(2)\nwhere rupright represents an abstraction of all the reward terms\nassociated with maintaining an upright posture, rather than a\nspecific reward term. While the term rvelocity rewards velocity\ntracking. When tracking different desired velocities, such as 0\nm/s, 1 m/s and 5m/s, each corresponds to distinct tasks, namely\nstand, walk, and run.\nOther reward terms are omitted for simplicity. In other envi-\nronments, such as HumanoidBench [Sferrazza et al., 2024], the\nupright term rupright serves as a weight to affect the value of the\nentire reward function:\nr2 = rupright × (rvelocity + · · · ) .\n(3)\nHere rupright ∈[0, 1]. Intuitively, the upright state with rupright =\n1 should only occupy an extremely narrow region in the whole\nstate-action space. In contrast, the vast majority of the space\nbelongs to rupright = 0. Therefore, once the algorithm explores\ninto the region where an upright posture can be maintained, it\nshould place particular emphasis on it. After all, maintaining\nan upright posture facilitates further exploration and learning\nfor downstream tasks. The next question is how to determine\nwhether the upright posture has been explored in the control\ntasks of humanoid robots.\nIn reality, for humanoid robots, only standing upright rupright =\n1 and falling down rupright = 0 are stable states. Other postures,\nsuch as rupright = 0.6, will eventually evolve into the stable\nstate of falling. Moreover, once a humanoid robot falls, it\ncannot recover to a standing state on its own, which means that\nsubsequent rewards become unattainable. Under the cumulative\neffect, the ability to maintain a stable standing posture will\nultimately be very prominently reflected in the return. We\nfurther illustrate this phenomenon with the following example.\nReturn\n22.3\n271.4\nFigure 3: The first row presents the return mean of the trajecto-\nries obtained by evaluating the policy trained for 100000 steps\non the task run, along with the violin plot distribution of rupright\nacross all timesteps. The second row shows the results obtained\nafter training for 300000 steps.\nFrom the Figure 3, we observe that the distribution of rupright\nobtained from policies trained for 100000 and 300000 steps\nexhibit relatively small differences. But the final returns show\na significant disparity (22.3 v.s. 271.4). This discrepancy arises\nbecause minor differences in whether the humanoid remains up-\nright at each timestep are cumulatively amplified over multiple\ntimesteps.\nOverall, the motion control of humanoid robots is closely re-\nlated to maintaining an upright posture, and whether or not\nthe robot remains upright can lead to a substantial difference\nin final return. Based on this finding, an idea for accelerating\nor assisting humanoid robots emerges: during the online rein-\nforcement learning (RL) exploration process, we can provide\nadditional guidance to the robot to imitate trajectories with high\nreturns. This approach enables the robot to first learn how to\nmaintain an upright posture, which then serves as a foundation\nfor completing the entire task.\n3.2\nFramework and Methods\nNow we introduce the concept of Self-Imitative Reinforcement\nLearning (SIRL) which aims to accelerate the learning process\nof humanoid robots. During the online exploration process, in\naddition to the basic RL loss, the policy πθ is also required\n3\n\nTDMPBC: Self-Imitative Reinforcement Learning for Humanoid Robot Control\nPREPRINT\nto imitate trajectories with high returns stored in the replay\nbuffer. Unlike classical imitation learning, where trajectories\nare typically provided by an expert, the trajectories imitated\nhere are generated by the policy itself. That is why we call the\nframework as “self-imitative”.\nWe have implemented the SIRL framework based on the TD-\nMPC2 algorithm [Hansen et al., 2023]. Specifically, only the\npolicy training loss function is modified and the difference is\nhighlighted by red:\nLπ (θ) =E(st,at,Rt)H\nt=0∼B\n\" H\nX\nt=0\nλth\nω (Rt) · log πθ (at|zt)\n|\n{z\n}\nSelf-Imitative\n+ Qϕ (zt, πθ (·|zt)) −αH (π (·|zt))\n|\n{z\n}\nReinforcement Learning\ni#\n.\n(4)\nHere Rt = PT\nt′=0 γtr(st, at) is the return of the whole tra-\njectory and all the st, at within this trajectory have the same\nreturn Rt. Compared to the original RL loss function, Another\nbehavior cloning loss is introduced with the weight\nω (Rt) = β · exp\n\u0012Rt −G\nG\n\u0013\n,\n(5)\nwhere G is a reference return value used to determine the level\nof the current return Rt. Ideally, G = Rtarget should be the\ntarget return, the standard for determining success or failure.\nHowever, this approach requires the introduction of additional\nprior information, which may affect the generality of the algo-\nrithm. Alternatively, we propose using the maximum return\nRmax of the trajectories in the current replay buffer as the refer-\nence standard.\n3.3\nDiscussion and Analysis\nFrom the implementation perspective, our proposed algorithm\ncan be viewed as TD-MPC2 + BC, which might seem similar to\nthe offline algorithm TD3+BC [Fujimoto and Gu, 2021]. How-\never, the scenarios and problems they address are completely\ndifferent. As an offline algorithm [Zhuang et al., 2023, Fuji-\nmoto et al., 2019, Kumar et al., 2020], TD3 + BC incorporates\nBC [Pomerleau, 1988] to prevent out-of-distribution (OOD)\nstate-action pairs that lie beyond the offline dataset. In contrast,\nTDMPBC is a fully online algorithm that integrates imitation\nlearning to accelerate exploration and learning within complex\nhigh-dimensional spaces.\nGenerally speaking, imitation learning [Zare et al., 2024] em-\nphasizes exploitation and is a relatively conservative algorithm,\nwhereas reinforcement learning places greater emphasis on ex-\nploration. TDMPBC can be regarded as a RL algorithm that\nincorporates conservatism. With the dynamic adjustment of BC\nweights, TDMPBC can be seen as a process where RL explores\nthe space first, followed by rapid learning through imitation\nlearning. The introduction of imitation learning does indeed\ncarry the risk of causing the algorithm to converge to local\noptima. However, in the context of high-dimensional complex\nspaces such as humanoid robot motion control, converging to a\nlocal optimum like the upright posture is highly probable and\noften beneficial for downstream tasks.\n4\nRelated Work\nBehavior control for Humanoid robots is a long-standing prob-\nlem, initially explored with simplified humanoid agent [Tunya-\nsuvunakool et al., 2020] and recently with full-size humanoid\nrobot [Zhuang et al., 2024a, Fu et al., 2024] such as Unitree H1.\nHumanoid robots are of particular interest to the reinforcement\nlearning community because of the high-dimensional action\nspace [Merel et al., 2017, Hansen et al., 2022a, 2023, 2024].\nTo overcome the challenges of exploration in high-dimensional\naction spaces, some algorithms learn policies by imitating hu-\nman behavior [Fu et al., 2024] or enhance exploration through\nmassive parallelization [Zhuang et al., 2024a]. In contrast, our\nproposed algorithm attempts to learn from scratch without the\naid of massive parallelization [Makoviychuk et al., 2021]. We\nhave extensively evaluated our algorithm on the Humanoid-\nBench [Sferrazza et al., 2024], a benchmark built on humanoid\nrobot with dexterous hands [Zakka et al., 2022] that contains\nnot only 14 locomotion tasks but also 17 whole-body manipu-\nlation tasks. In the LocoMujoco [Al-Hafez et al., 2023], the H1\nrobot is not equipped with dexterous hands and only focus on\nlocomotion tasks.\nConfronted with tasks involving high-dimensional action\nspaces, model-based RL algorithms [Ha and Schmidhuber,\n2018, Hansen et al., 2022a, Hafner et al., 2023, 2019] often\nprove to be more sample-efficient compared to model-free alter-\nnatives [Haarnoja et al., 2018, Fujimoto et al., 2018]. However,\nwhen it comes to humanoid robots with dexterous hands, even\nthe SOTA model-based algorithms struggle to solve it [Sfer-\nrazza et al., 2024]. Our algorithm integrates the concept of\nimitation learning [Liu et al., 2023, Zhang et al., 2024] with\nthe reinforcement learning framework, introducing a loss term\nof behavioral cloning [Pomerleau, 1988]. It may bear a resem-\nblance to the offline RL [Zhuang et al., 2024b, Fujimoto et al.,\n2019] algorithm TD3+BC [Fujimoto and Gu, 2021] but our\nproblem setting is completely different to theirs. Additionally,\nit should be noted that the SIRL framework is fundamentally an\nonline RL paradigm that does not rely on expert data, different\nfrom IBRL [Hu et al., 2023] or MoDem [Hansen et al., 2022b].\n5\nExperiments\nWe evaluate our proposed TDMPBC in HumanoidBench [Sfer-\nrazza et al., 2024], which contains 14 locomotion tasks and\n17 whole-body manipulation tasks. This benchmark is built\non the Unitree H1 robot with dexterous hands, which has 151-\ndimension observation space and 61-dimension action space.\nRemarkably, this benchmark aims to evaluate online RL algo-\nrithms and does not include any expert demonstration. At the\nsame time, SIRL is an improved online RL paradigm rather\nthan imitation learning paradigm.\nSpecifically, the experiments cover the following five aspects\nof TDMPBC: 1) Performance comparison with representative\nRL algorithms across 31 HumanoidBench tasks; 2) The impact\nof the selection of hyperparameter β and reference return value\nG; 3) Increased runtime caompared to TD-MPC2; 4) The phe-\nnomenon of policy performance chasing the highest return in\n4\n\nTDMPBC: Self-Imitative Reinforcement Learning for Humanoid Robot Control\nPREPRINT\n0\n5\n10\n0\n500\n1000\nReturn\nwalk (L)\n0\n5\n10\n0\n500\nstand (L)\n0\n5\n10\n0\n500\nrun (L)\nTDMPBC\nTD-MPC2\nSAC\nDreamerV3\nTarget\n0\n5\n10\n0\n5000\n10000\nReturn\nreach (L)\n0\n5\n10\n0\n500\n1000\nhurdle (L)\n0\n5\n10\n500\n1000\ncrawl (L)\n0\n5\n10\n500\n1000\nmaze (L)\n0\n5\n10\n0\n500\n1000\nReturn\nsit_simple (L)\n0\n5\n10\n0\n500\n1000\nsit_hard (L)\n0\n5\n10\n0\n500\n1000\nbalance_simple (L)\n0\n5\n10\n0\n500\nbalance_hard (L)\n0\n5\n10\n0\n500\nReturn\nstair (L)\n0\n5\n10\n0\n500\n1000\nslide (L)\n0\n5\n10\n0\n500\n1000\npole (L)\n0\n5\n10\n500\n0\n500\npush (M)\n0\n5\n10\n0\n1000\n2000\nReturn\ncabinet (M)\n0\n5\n10\n0\n500\nhighbar (M)\n0\n5\n10\n0\n250\n500\ndoor (M)\n0\n5\n10\n1000\n2000\n3000\ntruck (M)\n0\n5\n10\n0\n200\nReturn\ncube (M)\n0\n5\n10\n0\n1000\n2000\nbookshelf_simple (M)\n0\n5\n10\n0\n1000\n2000\nbookshelf_hard (M)\n0\n5\n10\n0\n500\n1000\nbasketball (M)\n0\n5\n10\n0\n500\nReturn\nwindow (M)\n0\n5\n10\n0\n250\n500\nspoon (M)\n0\n5\n10\n0\n2\n4\nkitchen (M)\n0\n5\n10\n10000\n0\npackage (M)\n0\n5\n10\nStep (×10e6)\n0\n500\nReturn\npowerlift (M)\n0\n5\n10\nStep (×10e6)\n0\n200\n400\nroom (M)\n0\n5\n10\nStep (×10e6)\n0\n200\ninsert_small (M)\n0\n5\n10\nStep (×10e6)\n0\n200\ninsert_normal (M)\nFigure 4: This figure presents the evaluation results on the HumanoidBench, where we conduct experiments with a total of three\nseeds and the shaded area representing one standard deviation. The baseline results are directly from the HumanoidBench.\n5\n\nTDMPBC: Self-Imitative Reinforcement Learning for Humanoid Robot Control\nPREPRINT\nthe replay buffer during algorithm training; 5) Demonstration\nand analysis of some representative learned behaviors.\n5.1\nPerformance Comparison\n5.1.1\nBaselines\nWe choose these three representative online reinforcement learn-\ning algorithms as our baselines:\n• SAC (Soft Actor-Critic) [Haarnoja et al., 2018]: the state-of-\nthe-art model-free off-policy RL algorithm with maximum\nentropy learning [Eysenbach and Levine, 2021];\n• DreamerV3 [Hafner et al., 2023]: the state-of-the-art model-\nbased RL algorithm that learns from the imaginary model\nrollouts;\n• TD-MPC2 [Hansen et al., 2023]: the state-of-the-art model-\nbased RL algorithm with online planning achieved via model\npredictive control (MPC).\nAs for on-policy algorithm PPO (Proximal Policy Optimization)\n[Schulman et al., 2017], its performance is inferior without the\nmassive GPU parallelization so PPO is not our baseline.\nIn Humanoidbench, TD-MPC2 interacts with the environment\nfor 2M steps, which takes approximately the same amount of\ntime as SAC and DreamerV3 interacting for 10M steps. There-\nfore, the default training steps for TD-MPC2 are set to 2M,\nwhile others are set to 10M. Similarly, the default training steps\nfor TDMPBC are also 2M. For tasks where performance sig-\nnificantly surpasses TD-MPC2 but still shows a clear upward\ntrend without reaching the target, we choose to continue train-\ning up to 10M steps to demonstrate asymptotic performance.\nThese environments include the hurdle, balance simple,\nbalance hard, and stair tasks in Locomotion, as well as the\ncabinet and window tasks in Whole-body Manipulation.\n5.1.2\nResults on Locomotion\nHumanoidBench contains a total of 14 locomotion tasks, cor-\nresponding to the first 14 training curves ending with (L) in\nFigure 4. It should be noted that, while the locomotion tasks\ncan be accomplished without dexterous hands, the H1 robot\nhere is indeed equipped with dexterous hands. The entire robot\nhas 151-dimensional observations (51 dimension for body and\n50 dimension for each hand), plus a 61-dimensional action\nspace, which is quite challenging for RL control. TDMPBC\nachieves significantly faster convergence and higher final per-\nformance compared to the baseline in all environments except\nfor reach and crawl. More importantly, TDMPBC surpasses\nthe target (represented by the grey dashed line) in 8 tasks, indi-\ncating successful task completion. In contrast, the baselines are\nonly capable of completing the crawl task.\n5.1.3\nResults on Whole-Body Manipulation\nHumanoidBench contains a total of 17 whole-body manipula-\ntion tasks, corresponding to the last 17 training curves ending\nwith (M) in Figure 4. Whole-body manipulation requires not\nonly the control of body posture but also the operation of dex-\nterous hands to accomplish grasping. Although our algorithm\nhas achieved obvious improvements, the final results are still\nfar behind the target return. Our algorithm also struggles to\nsimultaneously control the body and achieve dexterous hand\ngrasping. A prematurely converging curve implies that the\nhumanoid rapidly masters one thing while the other one fails.\n5.2\nAblation Study\n5.2.1\nAblation on hyperparameter β\nThe β balances the original reinforcement learning and our\nproposed self-imitative behavior cloning in policy loss function\nEquation 4. Due to the presence of the exponential function\nand Rt ≤G, the range of behavior cloning item is between\n(0, 1]. Meanwhile, Q is obtained by discrete regression in a\nlog-transformed space, which means the Q has been normalized\n[Hafner et al., 2023]. Due to the same scale between RL loss\nand behavior cloning loss, the default value of hyperparameter\nis β = 1. In Section 5.1, the experimental results are presented\nfor the case where β = 1. To further investigate the robustness\nof TDMPBC, we conducted additional control experiments\nwith β = 0.5 and β = 2.0. We found that the performance of\nTDMPBC is highly robust to values of β around 1.\n0\n200\n400\n600\n800\n1000\nReturn\nrun\n0\n2000\n4000\n6000\n8000\nreach\nTD-MPC2\nRmax\nRtarget\n0\n100\n200\n300\n400\n500\nReturn\nhurdle\nTD-MPC2\n= 1\n= 0.5\n= 2\n0\n100\n200\n300\n400\n500\nhurdle\n0.0\n0.5\n1.0\n1.5\n2.0\nStep (×10e6)\n100\n200\n300\n400\n500\n600\nReturn\nmaze\n0.0\n0.5\n1.0\n1.5\n2.0\nStep (×10e6)\n100\n200\n300\n400\n500\n600\nmaze\n(a) Ablation on β\n(b) Ablation on G\nFigure 5: The left figures illustrate the impact of different hy-\nperparameter values (β = 0.5, 1.0, 2.0) on the performance of\nTDMPBC across three tasks: run, hurdle, and maze. The\nright figures demonstrate the effects of two different goal set-\ntings (G = Rmax and G = Rtarget) on the performance of\nTDMPBC across three tasks: reach, hurdle, and maze.\n6\n\nTDMPBC: Self-Imitative Reinforcement Learning for Humanoid Robot Control\nPREPRINT\nFigure 6: The visualization of baseline TD-MPC2 on the pole task. The robot collides with the pole and falls to the ground.\nFigure 7: The visualization of our TDMPBC on the pole task. To avoid collisions, the robot chooses to stay close to the wall,\nthereby passing through quickly and stably. The ground is marked in blue to more clearly illustrate the moving toward the wall.\n5.2.2\nAblation on reference return value G\nThe G = Rtarget of the current task serves as a globally op-\ntimal benchmark but necessitates the introduction of addi-\ntional information. In contrast, the maximum return G =\nRmax in the current replay buffer represents the upper limit\nachievable by the current policy, which grows in tandem\nwith the policy’s performance as training progresses. Mean-\nwhile, G = Rtarget functions more as a pre-established, rela-\ntively higher objective. In Figure 5, we found no significant\n0.0\n0.5\n1.0\n1.5\n2.0\nStep\n1e6\n0\n200\n400\n600\n800\nReturn\nTarget\nslide\nsit_hard\nhurdle\nFigure 8: Gradually, Rmax may\npotentially exceed Rtarget.\ndifferences in overall per-\nformance between the two.\nTherefore, we opted for G =\nRmax to avoid incorporating\nprior information. It is also\nworth noting that in tasks\nwhere the return can exceed\nthe target, G = Rmax may ul-\ntimately surpass G = Rtarget.\n5.3\nRuntime\nCompared to TD-MPC2, TDMPBC introduces only a marginal\nincrease in computational burden for policy loss calculations\nand requires the replay buffer to additionally store the cur-\nrent maximum return value. We measure the time required\nto run three seeds simultaneously on a Tesla V100-SXM2-\n32G GPU across three different tasks in Table 1. When the\nGPU is upgraded to an NVIDIA A100-SXM4-40GB, the time\ncan be further reduced to approximately 20 hours. On average,\nTDMPBC only increases the time by less than 5%, yet achieved\na remarkable performance improvement of over 120%.\nTable 1: A comparison of the runtime of our proposed algorithm\nTDMPBC and the baseline TD-MPC2 on the same hardware.\nTasks\nTDMPBC\nTD-MPC2\nImprovement\nwalk\n37.71 ± 0.13\n35.23 ± 0.38\n+ 7.06%\nreach\n37.73 ± 0.64\n36.46 ± 0.71\n+ 3.48%\nhurdle\n36.61 ± 0.06\n35.34 ± 0.13\n+ 3.61%\nAverage\n37.35 (h)\n35.68 (h)\n+ 4.72%\n5.4\nTraining phenomenon\nIn the experiments, we observed that the return obtained by\nevaluating the current policy is often lower than the maximum\nreturn in the replay buffer, regardless of whether it is our pro-\nposed TDMPBC or the baseline algorithm TD-MPC2 in Figure\n9. Intuitively, it appears as though the policy is constantly chas-\ning the current maximum return, and the behavior cloning (BC)\nin SIRL accelerates this.\nFigure 9: The curves of the policy’s return (solid line) and the\nmaximum return in the current replay buffer (dashed line) dur-\ning training. The left figure shows these curves for TDMPBC\non the balance-hard and hurdle tasks, while the right figure\ncompares the curves for TDMPBC and TD-MPC2 on the walk.\n5.5\nBehavior Visualization\nIn this subsection, we present representative behaviors obtained\nfrom TDMPBC, including the pole and balance-hard tasks\nin locomotion, as well as the window task in whole-body ma-\nnipulation.\npole: In the pole task, the humanoid robot is required to\nnavigate forward through a dense forest of tall, slender poles\nwithout collision. The robot trained with TD-MPC2 continu-\nously collides with the poles and is unable to move forward\nproperly, eventually falling over. Once step inside the pole\nforest, the robot swiftly moves towards one of the side walls. It\nthen proceeds to hug the wall, keep escaping the dense poles,\nthereby avoiding potential collisions. This clever avoidance\nstrategy, closely resembling human behavior, is exactly the\nreason our algorithm converges rapidly.\n7\n\nTDMPBC: Self-Imitative Reinforcement Learning for Humanoid Robot Control\nPREPRINT\nFigure 10:\nThe visualization of our TDMPBC on the\nbalance-hard task, where the humanoid robot aims to main-\ntain balance on an unstable board, with the spherical pivot\nbeneath the board in motion.\nbalance hard: The humanoid robot is required to maintain\nbalance on an unstable plank, beneath which lies a movable\nsphere. Upon initialization, the robot nearly falls backward\nbut manages to stabilize itself by swinging its arms, ultimately\nachieving a balanced posture. During this process, the sphere\nis displaced from the center of the plank to a position slightly\nto the left.\nFigure 11: The visualization of our TDMPBC on the window\ntask, where the humanoid aims to grab a window wiping tool\nand keep its tip parallel to a window by following a prescribed\nvertical speed.\nwindow\n: For the window task, the robot should ideally use\nits dexterous hands to control the cleaning tools. However, the\nrobot does not master the control of its dexterous hands and\ninstead chooses to press against the cleaning surface with its\narms to complete the task. Despite the algorithm achieving a\nhigh return on this task, it does not perform the task as expected.\n6\nConclusion, Limitation and Future Work\nIn this paper, we propose the Self-Imitative Reinforcement\nLearning (SIRL) framework to accelerate the online learning of\nhumanoid robots. We have made a simple yet effective modifi-\ncation to TD-MPC2 by incorporating a behavior cloning (BC)\nloss term into the policy training loss function. Our proposed\nalgorithm has demonstrated significant performance improve-\nments in the HumanoidBench with a little additional compu-\ntation overhead. Current research has achieved remarkable\nprogress in locomotion tasks, yet there remains substantial\nroom for improvement in whole-body manipulation. Looking\nahead, we plan to transition TDMPBC from simulated environ-\nments to real-world deployment, further exploring its strengths\nand weaknesses.\nReferences\nAndrea Bonci, Pangcheng David Cen Cheng, Marina Indri, Gia-\ncomo Nabissi, and Fiorella Sibona. Human-robot perception\nin industrial environments: A survey. Sensors, 21(5):1571,\n2021.\nOlivier Stasse and Thomas Flayols. An overview of humanoid\nrobots technologies. Biomechanics of Anthropomorphic Sys-\ntems, pages 281–310, 2019.\nAvishek Choudhury, Huiyang Li, Christopher Greene, and\nSunanda Perumalla. Humanoid robot-application and in-\nfluence. arXiv preprint arXiv:1812.06090, 2018.\nCarmelo Sferrazza, Dun-Ming Huang, Xingyu Lin, Young-\nwoon Lee, and Pieter Abbeel. Humanoidbench: Simulated\nhumanoid benchmark for whole-body locomotion and ma-\nnipulation. arXiv preprint arXiv:2403.10506, 2024.\nJan Peters, Sethu Vijayakumar, and Stefan Schaal. Reinforce-\nment learning for humanoid robotics. In Proceedings of\nthe third IEEE-RAS international conference on humanoid\nrobots, pages 1–20, 2003.\nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez, Lau-\nrent Sifre, George Van Den Driessche, Julian Schrittwieser,\nIoannis Antonoglou, Veda Panneershelvam, Marc Lanctot,\net al. Mastering the game of go with deep neural networks\nand tree search. nature, 529(7587):484–489, 2016.\nDavid Silver, Julian Schrittwieser, Karen Simonyan, Ioannis\nAntonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lu-\ncas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the\ngame of go without human knowledge. nature, 550(7676):\n354–359, 2017.\nJulian Schrittwieser, Ioannis Antonoglou, Thomas Hubert,\nKaren Simonyan, Laurent Sifre, Simon Schmitt, Arthur\nGuez, Edward Lockhart, Demis Hassabis, Thore Graepel,\net al. Mastering atari, go, chess and shogi by planning with\na learned model. Nature, 588(7839):604–609, 2020.\nMatteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul,\nGeorg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mo-\nhammad Azar, and David Silver. Rainbow: Combining im-\nprovements in deep reinforcement learning. In Proceedings\nof the AAAI conference on artificial intelligence, volume 32,\n2018.\nTakahiro Miki, Joonho Lee, Jemin Hwangbo, Lorenz Well-\nhausen, Vladlen Koltun, and Marco Hutter. Learning robust\nperceptive locomotion for quadrupedal robots in the wild.\nScience robotics, 7(62):eabk2822, 2022.\nJoonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen\nKoltun, and Marco Hutter. Learning quadrupedal locomotion\nover challenging terrain. Science robotics, 5(47):eabc5986,\n2020.\nJemin Hwangbo, Joonho Lee, Alexey Dosovitskiy, Dario Belli-\ncoso, Vassilios Tsounis, Vladlen Koltun, and Marco Hutter.\nLearning agile and dynamic motor skills for legged robots.\nScience Robotics, 4(26):eaau5872, 2019.\nNicklas Hansen, Hao Su, and Xiaolong Wang. Td-mpc2: Scal-\nable, robust world models for continuous control. arXiv\npreprint arXiv:2310.16828, 2023.\n8\n\nTDMPBC: Self-Imitative Reinforcement Learning for Humanoid Robot Control\nPREPRINT\nDanijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy\nLillicrap. Mastering diverse domains through world models.\narXiv preprint arXiv:2301.04104, 2023.\nNicklas Hansen, Xiaolong Wang, and Hao Su. Temporal differ-\nence learning for model predictive control. arXiv preprint\narXiv:2203.04955, 2022a.\nRichard S Sutton, Andrew G Barto, et al. Introduction to\nreinforcement learning. 1998.\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey\nLevine. Soft actor-critic: Off-policy maximum entropy deep\nreinforcement learning with a stochastic actor. In Interna-\ntional conference on machine learning, pages 1861–1870.\nPMLR, 2018.\nGrady Williams, Andrew Aldrich, and Evangelos Theodorou.\nModel predictive path integral control using covariance vari-\nable importance sampling. arXiv preprint arXiv:1509.01149,\n2015.\nMaryam Zare, Parham M Kebria, Abbas Khosravi, and Saeid\nNahavandi. A survey of imitation learning: Algorithms,\nrecent developments, and challenges. IEEE Transactions on\nCybernetics, 2024.\nDean A Pomerleau. Alvinn: An autonomous land vehicle in a\nneural network. Advances in neural information processing\nsystems, 1, 1988.\nScott Fujimoto and Shixiang Shane Gu. A minimalist approach\nto offline reinforcement learning. Advances in neural infor-\nmation processing systems, 34:20132–20145, 2021.\nZifeng Zhuang, Kun Lei, Jinxin Liu, Donglin Wang, and Yilang\nGuo. Behavior proximal policy optimization. arXiv preprint\narXiv:2302.11312, 2023.\nScott Fujimoto, David Meger, and Doina Precup. Off-policy\ndeep reinforcement learning without exploration. In Interna-\ntional Conference on Machine Learning, pages 2052–2062.\nPMLR, 2019.\nAviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.\nConservative q-learning for offline reinforcement learning.\nAdvances in Neural Information Processing Systems, 33:\n1179–1191, 2020.\nSaran Tunyasuvunakool, Alistair Muldal, Yotam Doron, Siqi\nLiu, Steven Bohez, Josh Merel, Tom Erez, Timothy Lillicrap,\nNicolas Heess, and Yuval Tassa. dm control: Software and\ntasks for continuous control. Software Impacts, 6:100022,\n2020.\nZiwen Zhuang, Shenzhe Yao, and Hang Zhao. Humanoid\nparkour learning. arXiv preprint arXiv:2406.10759, 2024a.\nZipeng Fu, Qingqing Zhao, Qi Wu, Gordon Wetzstein, and\nChelsea Finn. Humanplus: Humanoid shadowing and imita-\ntion from humans. arXiv preprint arXiv:2406.10454, 2024.\nJosh Merel, Yuval Tassa, Dhruva TB, Sriram Srinivasan, Jay\nLemmon, Ziyu Wang, Greg Wayne, and Nicolas Heess.\nLearning human behaviors from motion capture by adversar-\nial imitation. arXiv preprint arXiv:1707.02201, 2017.\nNicklas Hansen, Jyothir SV, Vlad Sobal, Yann LeCun, Xi-\naolong Wang, and Hao Su. Hierarchical world models as\nvisual whole-body humanoid controllers. arXiv preprint\narXiv:2405.18418, 2024.\nViktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo,\nMichelle Lu, Kier Storey, Miles Macklin, David Hoeller,\nNikita Rudin, Arthur Allshire, Ankur Handa, et al. Isaac\ngym: High performance gpu-based physics simulation for\nrobot learning. arXiv preprint arXiv:2108.10470, 2021.\nKevin Zakka, Yuval Tassa, and MuJoCo Menagerie Contribu-\ntors. MuJoCo Menagerie: A collection of high-quality sim-\nulation models for MuJoCo, 2022. URL http://github.\ncom/google-deepmind/mujoco_menagerie.\nFiras Al-Hafez, Guoping Zhao, Jan Peters, and Davide Tateo.\nLocomujoco: A comprehensive imitation learning bench-\nmark for locomotion.\narXiv preprint arXiv:2311.02496,\n2023.\nDavid Ha and J¨urgen Schmidhuber. Recurrent world models\nfacilitate policy evolution. Advances in neural information\nprocessing systems, 31, 2018.\nDanijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad\nNorouzi. Dream to control: Learning behaviors by latent\nimagination. arXiv preprint arXiv:1912.01603, 2019.\nScott Fujimoto, Herke Hoof, and David Meger. Addressing\nfunction approximation error in actor-critic methods. In\nInternational conference on machine learning, pages 1587–\n1596. PMLR, 2018.\nJinxin Liu, Li He, Yachen Kang, Zifeng Zhuang, Donglin Wang,\nand Huazhe Xu. Ceil: Generalized contextual imitation learn-\ning. Advances in Neural Information Processing Systems,\n36:75491–75516, 2023.\nZiqi Zhang, Jingzehua Xu, Zifeng Zhuang, Jinxin Liu, et al.\nContext-former: Stitching via latent conditioned sequence\nmodeling. arXiv preprint arXiv:2401.16452, 2024.\nZifeng Zhuang, Dengyun Peng, Ziqi Zhang, Donglin Wang,\net al. Reinformer: Max-return sequence modeling for offline\nrl. arXiv preprint arXiv:2405.08740, 2024b.\nHengyuan Hu, Suvir Mirchandani, and Dorsa Sadigh. Imita-\ntion bootstrapped reinforcement learning. arXiv preprint\narXiv:2311.02198, 2023.\nNicklas Hansen, Yixin Lin, Hao Su, Xiaolong Wang, Vikash\nKumar, and Aravind Rajeswaran. Modem: Accelerating\nvisual model-based reinforcement learning with demonstra-\ntions. arXiv preprint arXiv:2212.05698, 2022b.\nBenjamin Eysenbach and Sergey Levine. Maximum entropy\nrl (provably) solves some robust rl problems. arXiv preprint\narXiv:2103.06257, 2021.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford,\nand Oleg Klimov. Proximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347, 2017.\n9\n\nTDMPBC: Self-Imitative Reinforcement Learning for Humanoid Robot Control\nPREPRINT\nA\nContributions\nThe first version of this paper is completed by the following five authors: Zifeng Zhuang∗, Diyuan Shi∗, Ting Wang✉,\nShangke Lyu✉and Donglin Wang✉. The first version was submitted to ICRA (about 2024.9), but it was desk-rejected due to\nformatting issues. Zifeng Zhuang∗proposes and leads the entire project. Diyuan Shi∗develops the code and conducts all the\nexperiments, and thus is credited as a co-first author. Dr. Ting Wang✉implementes the policy evaluation and visualization,\nparticipats in the writing of the paper and provides valuable feedback. Dr. Shangke Lyu✉provides extensive discussions\non article structure, which ensures the smooth progress of the entire project. Professor Donglin Wang✉fully supports the\ndevelopment of this project, provides all the necessary resources.\nIn the second version of the paper (current version), three additional authors contributed including Runze Suo, Xiao He\nand Hongyin Zhang. Runze Suo implements the modification of weights and update the code. Xiao He participats in data\npreprocessing and visualization of the paper. Hongyin Zhang offers numerous valuable suggestions for the writing of the paper.\nB\nHumanoidBench\nHumanoidBench [Sferrazza et al., 2024] is a comprehensive simulated humanoid robot benchmark designed to evaluate and\nadvance research in whole-body locomotion and manipulation tasks. It aims to provide a standardized platform for testing and\ndeveloping algorithms for humanoid robots, addressing the challenges of complex dynamics, sophisticated coordination, and\nlong-horizon tasks.\nA.1 Simulated Humanoid Robot:\nHumanoidBench features a humanoid robot equipped with two dexterous hands, specifically\nthe Unitree H1 robot with Shadow Hands. The robot is simulated using the MuJoCo physics engine, which provides fast and\naccurate physics simulation. The environment supports both position control and torque control, with the action space normalized\nto [−1, 1] for 61 actuators (19 for the body and 21 for each hand).\nA.2 Tasks\nHumanoidBench includes a diverse set of 31 tasks, divided into 14 locomotion tasks and 17 whole-body manipulation\ntasks. These tasks range from simple locomotion (e.g., walking, running) to complex manipulation (e.g., package unloading, tool\nusage, furniture assembly). Below is the specific list of tasks:\nLocomotion tasks\n• Walk: The robot maintains a velocity of approximately 1 m/s, ensuring it does not fall to the ground.\n• Stand: The robot maintains a standing pose.\n• Run: The robot maintains a velocity of approximately 5 m/s, ensuring it does not fall to the ground.\n• Reach: The robot’s left hand reaches a randomly initialized 3D point.\n• Hurdle: The robot maintains a velocity of approximately 5 m/s while clearing hurdles, ensuring it does not fall to the\nground.\n• Crawl: The robot traverses through a tunnel at a velocity of approximately 1 m/s.\n• Maze: In a maze, the robot reaches its target location by making multiple turns at intersections.\n• Sit: In sit simple, the robot sits on a chair located nearby behind it. The sit hard task involves a movable chair,\nwhere the robot sits on a chair positioned at random directions and locations.\n• Balance: The robot maintains its balance on an unstable board. In balance simple, the spherical pivot beneath the\nboard remains stationary, whereas in balance hard, the pivot is mobile.\n• Stair: The robot ascends and descends stairs at a velocity of 1 m/s.\n• Slide: The robot slides upwards and downwards at a velocity of 1 m/s.\n• Pole: The robot advances through a dense forest composed of high thin poles, without colliding with them.\nWhole-Body Manipulation tasks\n• Push: The robot moves a box to a randomly initialized 3D point on a table.\n• Cabinets: The robot opens four different types of cabinet doors (such as hinged doors, sliding doors, drawers, and\npull-up cabinets) and performs various pick-and-place manipulations inside the cabinets.\n• Highbar: The robot maintains a grip with both hands on the high bar, swinging while maintaining its hold until it\nreaches a vertical, upside-down position.\n10\n\nTDMPBC: Self-Imitative Reinforcement Learning for Humanoid Robot Control\nPREPRINT\n• Door: The robot turns the doorknob to open the door, and walks through it while keeping the door open.\n• Truck: The robot unloads packages from a truck onto a platform.\n• Cube: The robot manipulates a cube with each hand, aligning both cubes with specific, randomly initialized target\norientations.\n• Bookshelf: The robot rearranges five objects on a bookshelf, which is equivalent to five designated sub-tasks, each\ninvolving the positioning of a different object to a target location. The sub-tasks must be completed in order. In\nbookshelf simple, the order of sub-tasks is always the same, whereas in bookshelf hard, the order is randomized.\n• Basketball: The robot catches a ball coming from a random direction and throws it into the basket.\n• Window: The robot, holding a window cleaning tool, maintains its tip parallel to the window while adhering to a\nspecified vertical velocity.\n• Spoon: The robot picks up a spoon located next to a pot and uses it to draw circles in the pot.\n• Kitchen: The robot performs a series of actions in a kitchen environment, including opening the microwave door,\nmoving a kettle, turning a burner, and switching on and off the lights.\n• Package: The robot moves a box to a randomly initialized target location.\n• Powerlift: The robot lifts a barbell of a designed mass.\n• Room: The robot arranges a 5m by 5m room, minimizing the positional variance in the x and y axes by filling it with\nrandomly dispersed objects.\n• Insert: The robot inserts the ends of a rectangular block into two small pegs. Insert small and insert normal\nindicate different object sizes.\nA.3 Observation and Action Space\nThe observation space for the robot state includes joint positions and velocities, totaling\n151 dimensions (49 for the body and 51 for each hand). Additionally, the environment provides task-specific observations, such\nas object positions and velocities, to facilitate interaction with the environment. The action space is normalized to [−1, 1] for 61\nactuators, which includes 19 actuators for the humanoid body and 21 actuators for each hand. The action space is designed to be\nconsistent across all tasks to minimize domain-specific tuning.\nC\nExperimental Details\nWe implemented our TDMPBC algorithm on the source code of TD-MPC2 in HumanoidBench https://github.com/\ncarlosferrazza/humanoid-bench. The parameters used were the default parameters of HumanoidBench when running\nTD-MPC2. For the additional hyperparameter β that we introduced, it was set to β = 1 in all experiments.\nD\nMore Experiment Results\nMore experiment results are presented here. Figure 12 compares the evaluation curves of our proposed TDMPBC against the\nbaseline TD-MPC2, while Table 2 provides a summary of performance across various methods on HumanoidBench tasks,\neffectively demonstrating the efficacy of our approach.\n11\n\nTDMPBC: Self-Imitative Reinforcement Learning for Humanoid Robot Control\nPREPRINT\n0\n1\n2\n0\n500\n1000\nReturn\nwalk (L)\n0\n1\n2\n0\n500\nstand (L)\n0\n1\n2\n0\n500\nrun (L)\nTDMPBC\nTD-MPC2\n0\n1\n2\n0\n5000\nReturn\nreach (L)\n0\n5\n10\n0\n500\n1000\nhurdle (L)\n0\n1\n2\n500\n1000\ncrawl (L)\n0\n1\n2\n200\n400\n600\nmaze (L)\n0\n1\n2\n0\n500\n1000\nReturn\nsit_simple (L)\n0\n1\n2\n0\n500\n1000\nsit_hard (L)\n0\n5\n10\n0\n500\n1000\nbalance_simple (L)\n0\n5\n10\n0\n500\nbalance_hard (L)\n0\n5\n10\n0\n500\nReturn\nstair (L)\n0\n1\n2\n0\n500\n1000\nslide (L)\n0\n1\n2\n0\n500\n1000\npole (L)\n0\n1\n2\n500\n0\npush (M)\n0\n5\n10\n0\n500\nReturn\ncabinet (M)\n0\n1\n2\n0.00\n0.25\n0.50\nhighbar (M)\n0\n1\n2\n0\n200\ndoor (M)\n0\n1\n2\n500\n1000\n1500\ntruck (M)\n0\n1\n2\n0\n50\n100\nReturn\ncube (M)\n0\n1\n2\n0\n500\nbookshelf_simple (M)\n0\n1\n2\n0\n500\nbookshelf_hard (M)\n0\n1\n2\n0\n100\n200\nbasketball (M)\n0\n5\n10\n0\n500\nReturn\nwindow (M)\n0\n1\n2\n0\n200\n400\nspoon (M)\n0\n1\n2\n0.05\n0.00\n0.05\nkitchen (M)\n0\n1\n2\n10000\n5000\npackage (M)\n0\n1\n2\nStep (×10e6)\n0\n200\nReturn\npowerlift (M)\n0\n1\n2\nStep (×10e6)\n0\n100\n200\nroom (M)\n0\n1\n2\nStep (×10e6)\n0\n100\ninsert_small (M)\n0\n1\n2\nStep (×10e6)\n0\n100\n200\ninsert_normal (M)\nFigure 12: The evaluation curves of our proposed TDMPBC and the baseline TD-MPC2. We conduct experiments with a total of\nthree seeds and the shaded area representing one standard deviation.\n12\n\nTDMPBC: Self-Imitative Reinforcement Learning for Humanoid Robot Control\nPREPRINT\nTable 2: Summary of Results for HumanoidBench Tasks. In the results, non-target-exceeding scores are displayed in gray, and\nthe best results for each task are highlighted in bold.\nTasks\nTarget\nTDMPBC@2M\nTD-MPC2@2M\nDreamerV3@10M\nSAC@10M\nwalk\n700\n932.08 ± 0.82\n644.19 ± 344.25\n751.02 ± 28.25\n36.40 ± 30.28\nstand\n800\n929.67 ± 0.69\n749.79 ± 133.86\n845.36 ± 33.43\n141.98 ± 56.35\nrun\n700\n874.58 ± 21.69\n66.14 ± 9.97\n629.33 ± 81.75\n18.36 ± 3.30\nreach\n12000\n7013.83 ± 685.29\n7120.75 ± 253.64\n7926.20 ± 546.66\n3800.29 ± 344.43\nhurdle\n700\n843.54 ± 63.58\n64.68 ± 9.70\n137.46 ± 9.07\n13.85 ± 8.90\ncrawl\n700\n920.54 ± 45.88\n931.69 ± 33.19\n950.98 ± 10.38\n471.95 ± 12.13\nmaze\n1200\n497.66 ± 124.50\n224.62 ± 25.65\n301.77 ± 36.47\n149.40 ± 13.84\nsit simple\n750\n928.82 ± 1.12\n733.90 ± 255.79\n710.96 ± 208.93\n275.94 ± 33.41\nsit hard\n750\n908.75 ± 2.27\n508.98 ± 365.77\n662.55 ± 22.79\n61.06 ± 13.78\nbalance simple\n800\n688.86 ± 239.99\n34.07 ± 4.42\n29.89 ± 0.27\n62.61 ± 2.73\nbalance hard\n800\n317.27 ± 379.72\n48.18 ± 8.49\n45.04 ± 6.13\n50.82 ± 2.56\nstair\n700\n640.37 ± 46.99\n66.50 ± 6.77\n132.14 ± 1.80\n18.02 ± 4.91\nslide\n700\n926.26 ± 2.17\n141.30 ± 19.09\n367.61 ± 37.71\n19.65 ± 7.25\npole\n700\n958.58 ± 1.11\n207.46 ± 43.65\n589.01 ± 74.35\n123.30 ± 49.65\npush\n700\n83.54 ± 164.92\n-168.50 ± 45.46\n-144.62 ± 73.83\n-263.98 ± 54.44\ncabinet\n2500\n664.13 ± 14.69\n147.62 ± 34.00\n105.45 ± 52.28\n183.28 ± 63.76\nhighbar\n750\n0.26 ± 0.05\n0.47 ± 0.19\n7.58 ± 2.11\n18.43 ± 20.10\ndoor\n600\n270.92 ± 30.80\n179.83 ± 91.33\n165.83 ± 104.86\n131.79 ± 12.89\ntruck\n3000\n1402.91 ± 49.02\n1164.00 ± 38.29\n1341.04 ± 33.58\n1128.40 ± 16.26\ncube\n370\n33.79 ± 4.65\n104.14 ± 25.21\n63.57 ± 2.83\n104.34 ± 32.21\nbookshelf simple\n2000\n805.64 ± 25.83\n194.37 ± 33.46\n773.76 ± 33.82\n363.68 ± 71.15\nbookshelf hard\n2000\n707.57 ± 45.21\n64.19 ± 3.07\n577.13 ± 60.55\n300.05 ± 97.77\nbasketball\n1200\n111.83 ± 126.20\n120.66 ± 21.77\n46.22 ± 26.74\n34.10 ± 10.00\nwindow\n650\n714.22 ± 37.04\n63.27 ± 19.87\n158.37 ± 68.57\n65.68 ± 107.28\nspoon\n650\n386.39 ± 0.64\n70.69 ± 23.39\n331.83 ± 11.30\n118.28 ± 47.47\nkitchen\n4\n0.00 ± 0.00\n0.00 ± 0.00\n0.00 ± 0.00\n0.00 ± 0.00\npackage\n1500\n-6957.24 ± 1154.91\n-7228.07 ± 467.74\n-8124.26 ± 337.79\n-6946.94 ± 30.56\npowerlift\n800\n339.15 ± 0.22\n92.48 ± 11.16\n260.93 ± 24.58\n128.70 ± 9.52\nroom\n400\n203.53 ± 13.99\n84.20 ± 17.95\n123.92 ± 10.52\n14.50 ± 0.18\ninsert small\n350\n174.47 ± 4.34\n117.24 ± 21.27\n115.93 ± 18.22\n11.87 ± 13.05\ninsert normal\n350\n214.55 ± 4.12\n176.66 ± 10.58\n144.09 ± 18.02\n47.00 ± 67.78\n13\n",
  "metadata": {
    "source_path": "papers/arxiv/TDMPBC_Self-Imitative_Reinforcement_Learning_for_Humanoid_Robot_Control_eb133e2a36e7a06a.pdf",
    "content_hash": "eb133e2a36e7a06aeecf767abcb3d4b6e7ee19268f72e50d8e9f3f1589a7f86c",
    "arxiv_id": null,
    "title": "TDMPBC_Self-Imitative_Reinforcement_Learning_for_Humanoid_Robot_Control_eb133e2a36e7a06a",
    "author": "",
    "creation_date": "D:20250225030114Z",
    "published": "2025-02-25T03:01:14",
    "pages": 13,
    "size": 4533800,
    "file_mtime": 1740470161.9615743
  }
}