{
  "text": "Forgetting Any Data at Any Time: A Theoretically Certified Unlearning\nFramework for Vertical Federated Learning\nLinian Wang\nKey Lab of High Confidence Software Technologies (Peking University),\nMinistry of Education & School of Computer Science, Peking University\nLeye Wang\nKey Lab of High Confidence Software Technologies (Peking University),\nMinistry of Education & School of Computer Science, Peking University\nAbstract\nPrivacy concerns in machine learning are heightened by\nregulations such as the GDPR, which enforces the “right to\nbe forgotten” (RTBF), driving the emergence of machine un-\nlearning as a critical research field. Vertical Federated Learn-\ning (VFL) enables collaborative model training by aggregat-\ning a sample’s features across distributed parties while pre-\nserving data privacy at each source. This paradigm has seen\nwidespread adoption in healthcare, finance, and other privacy-\nsensitive domains. However, existing VFL systems lack robust\nmechanisms to comply with RTBF requirements, as unlearn-\ning methodologies for VFL remain underexplored. In this\nwork, we introduce the first VFL framework with theoretically\nguaranteed unlearning capabilities, enabling the removal of\nany data at any time. Unlike prior approaches—which impose\nrestrictive assumptions on model architectures or data types\nfor removal—our solution is model- and data-agnostic, of-\nfering universal compatibility. Moreover, our framework sup-\nports asynchronous unlearning, eliminating the need for all\nparties to be simultaneously online during the forgetting pro-\ncess. These advancements address critical gaps in current VFL\nsystems, ensuring compliance with RTBF while maintain-\ning operational flexibility. We make all our implementations\npublicly available at https://github.com/wangln19/vertical-\nfederated-unlearning.\n1\nIntroduction\nWhen personal data is used to train machine learning models,\nprivacy protection can be considered from two key perspec-\ntives: first, ensuring that data does not leave the local domain\nduring the training process to minimize the risk of data leak-\nage [20]; second, providing users with the ability to revoke\ntheir consent and delete their personal data after the model has\nbeen trained [22]. To address the issue of data isolation and\nprivacy risks during the training process, FL has emerged as\na promising solution. In addition, regulations like the GDPR\nhave proposed the “right to be forgotten” (RTBF), which ne-\ncessitates the removal of personal data from trained models.\nThis need has given rise to the concept of machine unlearn-\ning [3].\nFederated Unlearing (FU) combines these two orthogonal\nchallenges, offering comprehensive privacy protection. FL\ncan be categorized into two main types: Horizontal Federated\nLearning (HFL), where participants share the same features\nbut have different samples, and Vertical Federated Learning\n(VFL), where participants share the same samples but have\ndifferent features. VFL has found widespread application in\ncross-silo scenarios, such as those in banking and healthcare.\nHowever, Vertical Federated Unlearning (VFU) is still under-\ninvestigated. In real-world scenarios, unlearning requests are\nfrequently generated, with diverse objectives, such as client,\nfeature, or sensitive information removal [22, 27]. Current\nVFU methods only address specific data removal types (e.g.,\nclient [8,26] or feature [17] unlearning) and are often tailored\nto particular VFL models (e.g., logistic regression [8] or gra-\ndient boosting trees [17]). This underscores the critical need\nfor a unified VFU framework capable of handling multiple\ndata removal scenarios and models.\nAnother challenge in VFU lies in its computational and\ncommunication overheads. In VFL systems, each client main-\ntains a sub-model trained on its local feature data. However,\nunlearning data from one party triggers cascading impacts\nacross all parties’ sub-models. Existing VFU methods ad-\ndress this by enforcing synchronous unlearning [11]: upon\nan unlearning request, all clients must coordinate in real-time\nto update their sub-models. Such reliance on synchroniza-\ntion is operationally impractical—network instability, client\ndowntime, or resource constraints frequently disrupt client\nparticipation, thus breaking the unlearning process.\nIn this paper, we propose a novel VFL framework with\nrobust VFU capabilities of forgetting any data at any time.\nThis framework is capable of handling diverse unlearning\nobjectives in a unified manner and supports asynchronous un-\nlearning method to alleviate the burden on clients. Built on the\nwidely adopted aggregate VFL (AggVFL) systems [9,19,31],\nour framework unifies different unlearning targets from the\nperspective of confidence scores: the local models of clients\n1\narXiv:2502.17081v1  [cs.LG]  24 Feb 2025\n\nC1:Information Removal \nC2:Feature Removal \nC3:Information Removal \nC4:Client Removal \nC2:Feature Removal \nClient 1\nUnlearning Request\nClient 2\nClient 3\nClient 4\n......\nClient K\n(Active Party)\nFigure 1: What Asynchronous Unlearning Is. When Client 2\npropose a feature removal request, only Client 1, 2 and K are\nonline to unlearn.\nin VFL (regardless of simple linear regression or complex\ndeep neural networks) are treated as feature extractors, each\ngenerating class-specific confidence scores for the same sam-\nple based on different local features. The global model of\nthe active party (the client who owns training labels [19])\naggregates these outputs to maintain a matrix representing\nthe training set, which then produces final predictions.\nFirst, we convert VFU tasks for different objectives (client,\nfeature, etc.) into the closed-form updates for the confidence\nmatrix. The confidence matrix provides fine-grained informa-\ntion for unlearning without introducing extra privacy leakage\nrisk compared to traditional VFL training processes [15,29].\nSecond, we propose to leverage the confidence matrix for\nbackpropagation, thus enabling both model training and un-\nlearning in a unified manner within our framework (Sec. 3.2).\nFor learning models with strongly convex [4] loss functions\n(e.g., mean squared error and logistic loss with L2 regular-\nization), we prove that our method offers certified unlearning\nwith theoretical guarantees (Sec. 3.4).\nTo address the issue of excessive burdens placed on clients\nduring unlearning, we also introduce an asynchronous un-\nlearning method (Sec. 3.3). In VFU, the client initiating the\nunlearning request demands prompt removal of data influ-\nences from all clients’ sub-models, while the remaining clients\nrequire minimizing their unlearning burden and cannot be\nexpected to keep staying online. Therefore, the forgetting\ntimes for different clients may vary, something traditional\nsynchronous unlearning methods do not account for [8,26].\nOur asynchronous unlearning approach allows only a subset\nof clients to participate in each unlearning epoch, significantly\nimproving the unlearning practicality. As shown in Figure 1,\nwhen a client initiates an unlearning request, only the request-\ning client and the active party need to be online to process the\nrequest. Other clients may choose to participate in immediate\nunlearning or defer the process until later (e.g., until they sub-\nmit their own unlearning requests). Our experimental results\ndemonstrate that asynchronous unlearning reduces computa-\ntional and communication overhead while preserving model\nperformance close to that of synchronous unlearning.\nThis paper makes the following key contributions:\n1. A Unified Framework for Multiple VFU Targets with\nTheoretical Guarantees:We present the first framework that\nsupports diverse unlearning requests, including client, feature,\nsensitive information removal, etc., offering a flexible and\nscalable solution to address the RTBF requests in data regula-\ntion laws. For models with strongly convex losses, our method\noffers certified unlearning with theoretical guarantees.\n2. An Asynchronous VFU Method: We propose the first\nasynchronous VFU system, designed to efficiently balance\nfrequent unlearning requests from diverse clients while signif-\nicantly lowering computational and communication overhead.\n3. Extensive Experimental Validation: Our extensive ex-\nperimental results validate the effectiveness and applicability\nof the proposed VFU framework, demonstrating its practical\nviability in real-world scenarios.\n2\nRelated Work\nVertical Federated Learning (VFL) is a distributed machine\nlearning paradigm that enables collaboration between mul-\ntiple parties without sharing sensitive data. In general, there\nare two types of VFL, i.e., Aggregate VFL (AggVFL) and\nSplit VFL (SplitVFL) [19]. AggVFL employs a non-trainable\nglobal module (e.g., Sigmoid activation) to aggregate inter-\nmediate results through secure computation. Representative\nwork on AggVFL includes SecureBoost [7] for federated gra-\ndient boosting trees and SFTL [18] for cryptographic defense.\nOn the other hand, SplitVFL utilizes trainable global modules\naligned with vertical split neural networks [14]. This type of\nVFL has been studied in various works, including privacy-\npreserving split learning [25] and communication-efficient\nCELU-VFL [10]. In this work, we focus on AggVFL due to\nits superior compatibility with applications requiring strict\ndata isolation.\nTheoretically-Guaranteed Machine Unlearning\nMachine unlearning has attracted considerable attention in\nrecent years. However, most existing research on machine un-\nlearning lacks theoretical guarantees about its efficacy. There\nare currently two main approaches that provide formal guaran-\ntees of complete forgetting. The first approach, exemplified by\nSISA [3], focuses on exact unlearning. These methods [6,28]\npartition both the model and the training set, performing re-\ntraining only on the sub-model relevant to the unlearning\nrequest, thereby ensuring complete unlearning. The second\napproach draws from differential privacy concepts [5] and\nproves that the model post-unlearning is statistically indis-\ntinguishable from a retrained model, demonstrating the ef-\nfect of approximate unlearning, also called certified unlearn-\ning [13,21,27].\nWhile the first approach offers exact unlearning, it is in-\nefficient in scenarios where unlearning requests are scatted\n2\n\nTable 1: Vertical Federated Unlearning Research Summary\nPaper\nUnlearning Target\nAsynchronous\nCertified\nModel-Agnostic\n[8]\nClient\n[17]\nFeature&Sample\n[26]\nClient\n✓\n[12]\nLabel\n✓\nOur Work\nMultiple\n✓\n✓\n✓\nin many sub-models [27]. Hence, our proposed framework\naligns with the second approach, achieving certified approxi-\nmate unlearning while ensuring efficiency.\nFederated Unlearning\nFederated unlearning (FU) can be categorized based on the\ntype of unlearning target [22], including client removal, fea-\nture removal, and sample removal, among others. Most exist-\ning unlearning methods address specific unlearning requests\nand are not easily interoperable [32]. However, in real-world\napplications, the targets for forgetting vary widely. This calls\nfor a unified approach that can handle multiple forgetting\nrequests simultaneously.\nExisting research on FU primarily focuses on Horizontal\nFU (HFU), with few papers addressing Vertical FU (VFU).\nDeng et al [8] first introduce a VFU method for Logistic Re-\ngression (LR) models and client removal. Zhang et al [17]\npropose VFU approach for Gradient Boosted Decision Trees,\nwhich can unlearn samples by recalculating split points, or\nunlearn features by changing the splitting feature. Wang et\nal [26] introduce a model-agnostic VFU method that accel-\nerates retraining, but it only addresses client removal. Gu\net al [12] propose a VFU method to forget labels through\ngradient ascent techniques.\nRecent research on FU has acknowledged the significance\nof asynchronous unlearning, but all on HFU tasks. Su et\nal [24] divide the HFL system into multiple small modules\nbased on slicing, enabling asynchronous forgetting across\ndifferent modules. This method assumes unlearning requests\narrive during the system training phase. Gu et al [11] assume\nthat all clients possess identical models, so that after a client\nperforms a unlearning operation, the updated model can be\nbroadcast to all clients, thus achieving global unlearning.\nVFU presents unique challenges in asynchronous unlearn-\ning. In VFU, each client holds only a partial model, and for-\ngetting operations require global coordination among clients.\nThis introduces distinct difficulties for asynchronous forget-\nting, as the model segments are distributed across clients.\nTable 1 provides a comparative summary of our approach\nand existing VFU methods, emphasizing the distinct contri-\nbutions of our research.\n3\nMethod\n3.1\nBasic Idea\nOur approach is based on the following idea: in linear models\ny = ax1 + bx2 + c, forgetting x1 corresponds to subtracting\nthe corresponding term ax1 from y and then fine-tuning the\nmodel to remove x1’s influence on the parameters b and c.\nFor more complex models in VFL, we can apply this idea\nat the output layer. In a classification problem, we maintain\na confidence matrix in the active party of the VFL, where\neach row represents a sample and each column represents the\nconfidence for a particular class. Upon receiving a forgetting\nrequest, we subtract the corresponding confidence value from\nthe matrix, eliminating the forgotten target’s influence on\nthe output. We then fine-tune the model using the updated\noutput to remove the forgotten target’s effect on the model\nparameters.\nFor instance, in binary classification, the confidence vec-\ntor associated with a sample is two-dimensional, where each\nelement corresponds to one of the two classes. Consider two\nclients, i and j, where Client i has a classification confidence\nvector of [−1,1] and Client j has a confidence vector of [0,3]\nfor a specific sample. During aggregation, the global model\ncombines these vectors, producing a summed confidence vec-\ntor [−1,4], which is stored in a row of the confidence matrix.\nTo remove Client i’s contribution on this sample, its confi-\ndence vector [−1,1] is subtracted from the corresponding row\nof the matrix. The updated confidence matrix is then used to\ncompute the gradient with respect to the label. By updating\nClient j’s parameters using this revised matrix, the influence\nof Client i on Client j’s parameters is effectively removed.\n3.2\nCompatibility with Diverse Requests\nWe want to design a VFL system that supports diverse un-\nlearning requests (e.g., client removal, feature removal) while\nensuring data remains within the local domain. This requires\na mechanism to communicate forgotten information to each\nclient in a granular yet privacy-preserving manner.\nWe leverage the confidence matrix as a unified represen-\ntation for unlearning. When an unlearning request is made,\nthe requesting client calculates the difference in confidence\nvectors before and after unlearning and sends this difference\nto the active party. The active party updates the confidence\nmatrix to reflect the removal of the forgotten data. For Granu-\nlarity, any data change (e.g., client removal, feature removal)\ncan be represented as an update to the confidence matrix. For\nPrivacy, this unlearning process transmits the same informa-\ntion as the training process in VFL, ensuring no additional\nprivacy leakage.\nAdditionally, our framework makes a slight adjustment to\nthe backpropagation steps, so the system starts the update\nprocess from the stored confidence matrix. This ensures that\n3\n\nafter the confidence matrix is updated due to unlearning, neu-\nral network training can proceed normally, avoiding reliance\non raw data in the VFL system. The unlearning process is as\nfollows, as illustrated in Figure 2:\n1. The requesting client computes the confidence difference\nvector and sends it to the active party.\n2. The active party updates the confidence matrix and com-\nputes gradients based on the updated matrix and labels.\n3. Gradients are sent to the clients, who update their model\nparameters and compute new confidence differences.\n4. The active party aggregates all confidence differences\nand updates the confidence matrix for the next round.\nWe use the widely-adopted AggVFL structure, the advan-\ntages of the AggVFL structure include:\n1. Privacy protection: The local model outputs (e.g., class\nconfidence) are high-level features distinct from raw\ndata, ensuring data remains within the domain. Aggre-\ngation process can be further enhanced using existing\nprivacy-preserving techniques like obfuscating individ-\nual contributions.\n2. Mathematical rigor: The summation-based aggrega-\ntion of confidence outputs ensures that the derivatives\nof the loss with respect to the global confidence matrix\nand each client’s output are mathematically equivalent.\nThis property is crucial for proving certified unlearning\nguarantees.\n3.3\nAchieving Asynchronous Unlearning\n3.3.1\nAsynchronous Unlearning Setting\nIn VFU, two key time constraints must be considered:\n1. VFL Inference Phase: During inference, the VFL sys-\ntem relies on data and computations from all clients.\nThus, all clients must be online, and any unlearning re-\nquests must be completed prior to inference.\n2. Unlearning Deadline: Unlearning requests are associ-\nated with a deadline, requiring all related tasks to be\ncompleted within the specified timeframe.1\nOur asynchronous unlearning framework operates within\na defined time window between two consecutive inference\nphases. Unlike synchronous approaches, not all clients need\nto remain online. When an unlearning request is initiated,\nonly the requesting client and the active party must be online.\nThe requesting client stores the data to be forgotten, making\n1While GDPR and CCPA grant users the right to delete personal data, they\ndo not mandate a specific timeframe for completing unlearning or mitigating\nits impact on models.\nit particularly sensitive to unlearning speed. This client can\nperform unlearning immediately, ensuring compliance with\nprivacy regulations without requiring other clients to stay\nonline. Other clients can defer their unlearning processes,\nreducing their operational burden.\nThe asynchronous setup introduces two main challenges:\n1. Quantifying Client Impact: Accurately measuring how\neach client’s model update contributes to the global\nmodel update.\n2. Compensating for Offline Clients: Addressing the ab-\nsence of offline clients during unlearning by leveraging\nupdates from online clients and stored information, en-\nsuring no additional privacy leakage.\nThe first challenge is effectively addressed by our\nconfidence-based framework. During each update, the im-\npact of each client’s update is directly reflected in the change\nin its output confidence, which then influences the global\nmodel’s output, and subsequently the gradient for the next\nupdate. The second challenge is how to calculate the change\nin confidence output resulting from model parameter updates\nfor offline clients. We solve this by introducing the stability\nof the update contribution factor in the operation of a VFL\nsystem.\n3.3.2\nThe Stability of The Update Contribution Factor\nEach passive party computes the confidence score hi of its\nlocal model. The active party aggregates these scores, comput-\ning H = ∑hi, and feeds H into the global model G to produce\nthe final output p = G(H).\nDuring backpropagation, as client parameters θi are up-\ndated, their confidence scores hi = fi(θi,Xi) also change.\nLet δ(hi) denote the change in hi. The global model’s in-\nput change δ(H) is a linear combination of these individual\nchanges: δ(H) = ∑δ(hi), which in turn affects the global\noutput p.\nThe linear relationship δ(H) = ∑δ(hi) provides insight into\neach client’s contribution to the global model update. The ra-\ntio Ri = δ(hi)\nδ(H), noted as termed the update contribution factor,\nquantifies client i’s influence on the model’s change during\nupdates. Notably, in VFL systems using logistic regression,\nthese factors remain stable across training epochs.\nTo illustrate, consider two clients, i and j, with features i1\nto i4 and j1 to j5, respectively. Their confidence scores are\ncomputed as:\nhi = fi(θi,xi1,xi2,xi3,xi4)\nhj = f j(θj,xj1,x j2,xj3,xj4,xj5).\nSince the training set remains fixed, the representations\noutput by each client depend solely on their parameters. We\n4\n\nsample\nclass\nXK\nhK\nX2\nh2\nX1\nh1\nH\nOutput\nLabel\nLoss\nsample\nclass\nXK\nδhK\nX2\nX1\nδh1\nH\nOutput\nLabel\nLoss\n1\n2\n3\n3\nFigure 2: Structure and the Unlearning Process of Our Method.\napproximate changes in these representations using a first-\norder Taylor expansion. This approximation is justified in\nunlearning scenarios, where training set changes are minimal,\nand parameter updates before and after unlearning are not\nsignificantly different.\nδ(hi) = δ(θi1) ∂hi\n∂θi1\n+δ(θi2) ∂hi\n∂θi2\n+δ(θi3) ∂hi\n∂θi3\n+δ(θi4) ∂hi\n∂θi4\nδ(hj) = δ(θj1) ∂h j\n∂θ j1\n+···+δ(θ j5) ∂hj\n∂θ j5\nSubsequently, the change in the parameters can be directly\nderived from the backpropagation update rule, denoted that\nη is the learning rate. By substituting these changes into the\nupdate rule, we observe that the ratio of the changes in the\nrepresentations of client i and client j depends solely on the\ntraining data in the regression models. Since the training data\nremains unchanged between epochs, this ratio remains stable\nover time.\nδ(θi1) = −η ∂L\n∂θi1\n= −η ∂L\n∂H\n∂H\n∂hi\n∂hi\n∂θi1\nδ(hi) = −[( ∂hi\n∂θi1\n)2 +···+( ∂hi\n∂θi4\n)2]η ∂L\n∂H\n∂H\n∂hi\n= −[x2\ni1 +···+x2\ni4]η ∂L\n∂H\nδ(h j) = −[( ∂hj\n∂θ j1\n)2 +···+( ∂hj\n∂θj5\n)2]η ∂L\n∂H\n∂H\n∂h j\n= −[x2\nj1 +···+x2\nj5]η ∂L\n∂H\nδ(hi)\nδ(hj) = [x2\ni1 +···+x2\ni4]\n[x2\nj1 +···+x2\nj5]\nRi = δ(hi)\nδ(H) =\n[x2\ni1 +···+x2\ni4]\n[x2\ni1 +···+x2\ni4]+[x2\nj1 +···+x2\nj5]\nFrom the above derivation, it is clear that for a Multilayer\nPerceptron (MLP), the contribution coefficients of different\nclients to the update of z may vary across epochs. However,\nunder the unlearning setting, if the MLP has a limited number\nof layers, this variation is negligible. We provide a proof of\nthis observation in Appendix A.\nSince the update contribution factor remains stable, we\ncan leverage the confidence updates from online clients to\nestimate those of the offline client K. For the global model\nupdate, we have:\nδ(hK) =\nRK\n∑i∈{online} Ri\n∑\ni∈{online}\nδ(hi)\nδ(H) =\n1\n∑i∈{online} Ri\n∑\ni∈{online}\nδ(hi)\nThe update contribution factor must be computed and\nstored by the active party prior to receiving unlearning re-\nquests. This can be readily accomplished at the conclusion of\nthe VFL system’s training phase.\n3.4\nAchieving Certified Unlearning\n3.4.1\nDefinition\nConsider a learning algorithm A that, when trained on a\ndataset D, generates a model θ ∈Θ. An unlearning method\nU that transforms a model θ into a corrected version θU =\nU(θ,D,D′). D′ contains the perturbations ˜Z needed for un-\nlearning, while the corresponding original data is Z. The con-\ncept of ε-certified unlearning means that it is hard to dis-\ntinguish models after unlearned U from the set of possible\nretrained models A(D′) [13,27].\nDefinition 1. Given some ε > 0 and a learning algorithm A,\nan unlearning method U is ε-certified if\ne−ε ≤\nP\n\u0010\nU\n\u0000A(D),D,D′\u0001\n∈T\n\u0011\nP\n\u0000A(D′) ∈T\n\u0001\n≤eε\nholds for all T ⊂Θ,D,and D′.\nThe (ε,δ)-certified unlearning is similarly defined.\n5\n\nDefinition 2. Under the assumptions of Definition 1, an un-\nlearning method U is (ε,δ)-certified if\nP\n\u0010\nU\n\u0000A(D),D,D′\u0001\n∈T\n\u0011\n≤eεP\n\u0000A(D′) ∈T\n\u0001\n+δ\nand\nP\n\u0000A(D′) ∈T\n\u0001\n≤eεP\n\u0010\nU\n\u0000A(D),D,D′\u0001\n∈T\n\u0011\n+δ\nhold for all T ⊂Θ,D,and D′.\n3.4.2\nDesign for Certified Unlearning\nTo achieve rigorously defined certified unlearning, our method\nincorporates two critical components:\n1. Noise Injection during Training: We add Gaussian\nnoise b ∼N (0,σ2I) to the gradients during model train-\ning and unlearning, ensuring bounded parameter sensi-\ntivity.\n2. First-Round Gradient Ascent: During the initial un-\nlearning update, we simultaneously perform:\nθunlearn = θ∗−τ\n\u0010\n∇L(θ∗;D′)\n|\n{z\n}\ndescent on new data\n−∇L(θ∗;D)\n|\n{z\n}\nascent on old data\n\u0011\nTheorem 1 (Certified Unlearning Guarantee). Assume the\nloss ℓ(θ;z) is convex, γ-smooth with L2 regularization λ\n2∥θ∥2\n2.\nFor any data modification (Z, ˜Z), our method achieves (ε,δ)-\ncertified unlearning with δ = 1.5e−c2/2 when:\n• Training noise p ∼N (0,c(1 + τγzn)γzM|Z|/ε)d for\nsome c > 0\n• Assume that ∥xi∥2 ≤1 for all data points and the gra-\ndient ∇ℓ(z,θ) is γz-Lipschitz. Further let ˜Z change the\nfeatures j,..., j+F by magnitudes at most m j,...,m j+F,\nand M = ∑F\nj=1 m j\nProof Sketch. The certification follows three key arguments:\n1. Gradient residual bound using Lipschitz continuity and\nour update rule\n2. Relating the gradient residual to the sensitivity of per-\nturbation vector b b using the L2-regularized strong con-\nvexity.\n3. Applying Gaussian noise over models to yield (ε,δ)-\nguarantees.\nFull proof appears in Appendix 5.\nFeature\nSample\nFigure 3: Suppose the whole matrix is the data from one client.\nIn client removal, the whole matrix is removed. In feature\nremoval, the feature to be removed is marked with blue. In\nsensitive information removal, the sensitive information to be\nremoved is marked with red.\n4\nEvaluation\nWhile our method addresses asynchronous unlearning—a\ncapability absent in existing approaches—our evaluation is\nstructured in two stages. Specifically, we aim to verify:\nSynchronous Unlearning. Our method performs compa-\nrably, or even better, than existing unlearning methods [8,26]\nwhen all clients are online.\nAsynchronous Unlearning. During asynchronous unlearn-\ning, our method maintains comparable performance to syn-\nchronous unlearning.\n4.1\nExperimental Setup\nUnlearning Scenarios\nThere are three common unlearning scenarios in VFL:\n1. Client Removal: A client exits the VFL system, and its\ndata’s influence must be completely removed. This is equiva-\nlent to retraining the model by setting all data from the client\nto zero [27].\n2. Feature Removal: A certain feature (or feature set)\nfrom a client, which involves sensitive user information, is no\nlonger available due to policy changes or other reasons. The\ninfluence of this feature must be removed from the system.\nThis is equivalent to retraining the model with the feature set\nto zero [27].\n3. Sensitive Information Removal: A feature from a client\nmay be sensitive to a subset of users, prompting requests for\nremoval (e.g., some users modify the visibility of the age field\nfrom ‘public’ to ‘private’). This can be achieved by retraining\nthe model where the sensitive information is replaced by the\nmean value of that feature across all samples [27].\nThe relationships between these three scenarios are illus-\ntrated in Figure 3. As sensitive information may pertain to a\nsingle feature of a single sample (the smallest data unit), our\nmethod can easily extend to tasks such as sample removal\n(forgetting all data from a single sample) and class removal\n(forgetting all samples of a particular class).\n6\n\nPerformance Measures\nFollowing [26], we conduct evaluation from two aspects, fedil-\nity and efficiency:\nFidelity assesses whether unlearning affects the utility of\nthe model. The performance of the unlearned model should\nremain close to that of the original model, and the unlearning\nprocess should not significantly impact the model’s practi-\ncal utility. The metrics include Accuracy and AUC of the\nunlearned model on the test set.\nEfficiency concerns the time taken for unlearning com-\npared to retraining. The unlearning process should be as fast\nas possible. We compute efficiency using the total number of\nepochs and communication cost.\nBaselines\nWe compare our method with three existing unlearning ap-\nproaches.\nVFULR [8]: VFULR addresses the client removal problem\nin the VFL logistic regression system. The method directly\nsubtracts the contributions of the client to be forgotten from\nthe sum of linear terms, followed by a single update.\nVFUFR [26]: VFUFR addresses the client removal issue\nin general VFL systems, not limited to logistic regression.\nThe method stores the original bottom model in each client\nto accelerate the retraining process, and it uses an enhanced\noptimizer for further speedup.\nRetrain: This method retrains the model from scratch on\nthe new training dataset after data removal. It generally yields\nthe best performance and serves as an upper bound for unlearn-\ning methods, though it is computationally expensive [22].\nNotably, VFULR and VFUFR are designed for VFL client-\nremoval tasks on synchronous unlearning; we thus limit our\ncomparison with these two baselines on this scenario.\n4.2\nSynchronous Client Removal\nIn existing VFL literature, client removal is a commonly stud-\nied unlearning scenario. In this context, we address the first\nkey question: whether our method performs comparably to\nor even better than existing unlearning approaches when all\nclients are online (i.e., synchronous unlearning).\nTo validate this, we select five datasets: Adult Income [2],\nCredit [30], Diabetes [16], Nursery [23], and Malware [1].\nAmong these, Adult Income, Credit, Diabetes and Nursery are\ntabular datasets, while Malware is a text-based dataset. For\nthe text dataset, we extract bag-of-words features [33]. We\nsplit 80% of the data for training and reserve 20% for testing.\nTable 2 overviews the datasets and their vertical federated set-\nting. Without loss of generality, we set one client for removal;\nthe number of features in the removed client is in Table 2.\nFor each dataset, we conduct experiments using both the LR\nand MLP models, resulting in a total of 6×2=12 experimental\nTable 2: Datasets for unlearning.\nAdult Income\nCredit\nDiabetes\nNursery\nMalware\n#Samples\n48842\n30000\n768\n10950\n49226\n#Features\n108\n23\n8\n19\n2081\n#Clients\n16\n4\n4\n4\n16\n#Feat. in Removed Client\n27\n6\n2\n7\n520\nsetups. Since VFULR is designed only for the LR model,\nthere is no VFULR results in the MLP model experiments.\nFidelity: Table 3 and 4 show the fidelity results for both\nLR and MLP model unlearning. Higher accuracy and AUC\nindicate better performance. As expected, Retrain performs\nthe best in most cases, while it generally needs much more\nepochs to converge (we will show in the efficiency results).\nOur method performs similarly to VFUFR and outperforms\nVFULR. In datasets with a higher number of features, the\nperformance difference between retraining and various un-\nlearning methods is not significant. In contrast, in datasets\nlike Diabetes, which have fewer features, client removal re-\nsults in a noticeable drop in model performance. This may\nbe due to the fact that, in datasets with many features, the\nclients removed during unlearning likely do not contain high-\ninformation features.\nEfficiency: Table 5 and 6 show the efficiency results. We\nmeasure training time in terms of epochs. VFULR performs\nonly one update, thus being the most efficient; but its fidelity\nis not satisfactory as aforementioned. Other methods can\nperform multiple updates. We employed an early stopping\nmechanism, limiting retraining to a maximum of 400 epochs\nand other methods to a maximum of 50 epochs, since the\nretraining method typically requires more epochs to converge\n(the hyper-parameter choice will be discussed in Section 4.5).\nThe results show that the retraining method typically requires\nhundreds of epochs, significantly higher than other methods.\nOur method has a similar training efficiency as VFUFR. We\nalso evaluate communication cost per epoch. All the methods\ntransfer similar data amount in each epoch.\nIn summary, for synchronous VFL unlearning in client-\nremoval scenarios, our method achieves performance compa-\nrable to the state-of-the-art VFUFR framework. Meanwhile,\nour approach surpasses VFUFR by offering unique advan-\ntages: greater flexibility in handling diverse unlearning scenar-\nios (e.g., feature or sensitive data removal), the ability to per-\nform asynchronous unlearning when not all clients are online,\nand theoretically grounded certified unlearning guarantees.\nThese additional capabilities will be evaluated subsequently.\n4.3\nSync. & Async. Feature Removal\nSince VFULR and VFUFR do not support the feature removal\nscenario, our analysis is exclusively benchmarked against Re-\ntrain. We further evaluate our method on both synchronous\nand asynchronous settings, demonstrating comparable per-\n7\n\nTable 3: Fidelity of VFL Client Removal for LR models on Synchronous Unlearning.\nAdult Income\nCredit\nDiabetes\nNursery\nMalware\nAverage\nMetric\nAccuracy\nAUC\nAccuracy\nAUC\nAccuracy\nAUC\nAccuracy\nAUC\nAccuracy\nAUC\nAccuracy\nAUC\nVFULR\n0.832\n0.886\n0.779\n0.689\n0.630\n0.652\n0.737\n0.908\n0.975\n0.933\n0.791\n0.814\nVFUFR\n0.847\n0.896\n0.781\n0.688\n0.695\n0.747\n0.728\n0.906\n0.979\n0.947\n0.806\n0.837\nOurs\n0.846\n0.896\n0.781\n0.688\n0.695\n0.740\n0.730\n0.906\n0.979\n0.948\n0.806\n0.836\nRetrain\n0.853\n0.905\n0.797\n0.686\n0.695\n0.782\n0.731\n0.906\n0.979\n0.954\n0.811\n0.847\nTable 4: Fidelity of VFL Client Removal for MLP models on Synchronous Unlearning.\nAdult Income\nCredit\nDiabetes\nNursery\nMalware\nAverage\nMetric\nAccuracy\nAUC\nAccuracy\nAUC\nAccuracy\nAUC\nAccuracy\nAUC\nAccuracy\nAUC\nAccuracy\nAUC\nVFUFR\n0.859\n0.912\n0.776\n0.743\n0.623\n0.709\n0.735\n0.910\n0.983\n0.957\n0.795\n0.846\nOurs\n0.859\n0.911\n0.778\n0.742\n0.636\n0.704\n0.737\n0.910\n0.982\n0.956\n0.798\n0.845\nRetrain\n0.859\n0.912\n0.805\n0.751\n0.740\n0.818\n0.749\n0.911\n0.984\n0.958\n0.827\n0.870\nformance between the two settings. For the asynchronous\nunlearning setting, we suppose that a random 25% of clients\nwill drop out in each epoch. Due to the page limitation, we\nshow experiment results on two datasets: Adult Income and\nMalware. The experimental setup is similar to that of the\nclient removal scenario, except that in this case, we unlearn a\nportion of the features from the clients rather than unlearning\nentire clients. The results are presented in Figure 4.\nFidelity: The first row of Figure 4 shows the test accuracy\nmetric. The performance of our method is very similar to\nretraining, with the test accuracy decreasing only slightly as\nthe proportion of features removed increases, which we have\nalready analyzed in the client removal scenario. Specifically,\nthere is no significant difference between the accuracy of our\nmethod on synchronous or asynchronous unlearning, verify-\ning the effectiveness on the asynchronous setting. The AUC\nresults are similar and not shown due to the page limitation.\nEfficiency: The second and third rows of Figure 4 dis-\nplay the epoch number and communication cost per epoch.\nFrom the figure, it is evident that the change in the propor-\ntion of feature removal has minimal effect on the efficiency\nmetrics. In terms of the number of training epochs, the re-\ntraining method requires several times more epochs than our\nmethods. It is noteworthy that our method on asynchronous\nunlearning incurs approximately 25% less communication\ncost than synchronous unlearning per epoch, as fewer clients\nare online, reducing the communication burden. The propor-\ntion of reduction in communication cost is directly related to\nthe proportion of clients that are offline.\n4.4\nSync. & Async. Sensitive Info. Removal\nSensitive information removal represents a more fine-grained\nrefinement of the feature removal scenario, where only spe-\ncific features from certain samples within a client are removed.\nWe select two datasets, Credit and Diabetes, for experimen-\ntation due to their inclusion of sensitive attributes that users\nmight prefer to keep private. Specifically, Credit contains the\n‘age’ feature, while Diabetes includes the ‘number of pregnan-\ncies’ feature. These attributes may be regarded as sensitive\nby some users, prompting our focus on unlearning such data\npoints. The results are presented in Figure 5.\nFidelity: The first row in the figure presents the test ac-\ncuracy metric. As the proportion of sensitive information\nremoval increases, the test accuracy generally decreases. This\ndecrease is more pronounced in Diabetes which includes only\n8 features in total, and the retrained model curve is slightly\nhigher than that of our method, indicating some performance\ngap. This observation is also consistent with previous results\nin Sec. that datasets with s\nEfficiency: The third and fourth rows in the figure show the\nnumber of epochs and the communication cost per epoch. For\nthe training epochs, although there are some fluctuations in\nthe results, the retrained method requires several times more\nepochs than the unlearning methods. In the communication\ncost per epoch for the Diabetes dataset, when the sensitive\ninformation removal rate is close to 0.2, the communication\ncost per epoch of the asynchronous method is actually higher\nthan that of the synchronous method. This occurs because\nthe asynchronous method converges after approximately 20\nepochs, and the additional communication overhead for eval-\nuation is divided by a smaller denominator, resulting in a\nrelatively large impact.\n4.5\nUnlearning Epoch Number Choice\nOur method performs unlearning through multiple training\nrounds. This raises the question: how many rounds of un-\n8\n\nTable 5: Efficiency of VFL Client Removal for LR models on Synchronous Unlearning.\nAdult Income\nCredit\nDiabetes\nNursery\nMalware\nMetric\n#Epoch\nComm./Epoch\n#Epoch\nComm./Epoch\n#Epoch\nComm./Epoch\n#Epoch\nComm./Epoch\n#Epoch\nComm./Epoch\nVFULR\n1\n4.17\n1\n0.732\n1\n0.0187\n1\n0.791\n1\n4.81\nVFUFR\n50\n3.91\n50\n0.553\n50\n0.0141\n50\n0.597\n50\n4.51\nOurs\n50\n3.99\n50\n0.576\n50\n0.0144\n50\n0.609\n50\n4.60\nRetrain\n180\n3.91\n400\n0.553\n129\n0.0141\n400\n0.597\n209\n4.51\nTable 6: Efficiency of VFL Client Removal for MLP models on Synchronous Unlearning.\nAdult Income\nCredit\nDiabetes\nNursery\nMalware\nMetric\n#Epoch\nComm./Epoch\n#Epoch\nComm./Epoch\n#Epoch\nComm./Epoch\n#Epoch\nComm./Epoch\n#Epoch\nComm./Epoch\nVFUFR\n50\n3.91\n50\n0.553\n50\n0.0141\n50\n0.597\n50\n4.51\nOurs\n50\n3.99\n50\n0.564\n50\n0.0144\n50\n0.609\n50\n4.60\nRetrain\n400\n3.91\n236\n0.553\n400\n0.0141\n168\n0.597\n225\n4.51\nlearning are required? Typically, in model training, an early\nstopping mechanism is used to prevent overfitting and deter-\nmine the number of training epochs. In a VFL system, the\ntest set data is generally not accessible during unlearning.\nTherefore, we use the training loss as the criterion for early\nstopping. While the training loss typically decreases during\nregular training, our asynchronous method approximates up-\ndates for clients that are offline. Thus, using the training loss\nfor early stopping is still reasonable.\nOn the other hand, if too many rounds of unlearning are\nperformed, the efficiency advantage over retraining is lost.\nTherefore, we cannot allow an excessive number of rounds.\nWe need to determine a maximum number of rounds that\nis fewer than the retraining epochs, but still ensures suffi-\ncient training. We demonstrate this using the MLP model on\nthe Malware dataset in a client removal experiment. When\nall clients are online, unlearning could potentially require\nhundreds of epochs. By recording the training loss, training\naccuracy, and gradient residual during the training process, we\nobserve that the loss, accuracy, and gradient residual decrease\nquickly in the early stages of training and stabilize in the later\nstages, as shown in Figure 6. Therefore, we select 50 epochs\nas the maximum number of rounds for unlearning. In the fig-\nure, we also plot the retraining curve for comparison, and it\nis evident that unlearning is more efficient than retraining.\n4.6\nAsynchronous Unlearning Online Rate\nOur asynchronous unlearning method enables updates even\nwhen a subset of clients is offline. A critical question is how\nthe proportion of online clients affects performance. To ad-\ndress this, we evaluate client removal using the LR model on\nthe Adult Income dataset. The dataset contains 108 features\ndistributed among 16 clients: the target client (Client 0) holds\nTable 7: The number of online clients has little impact on the\neffectiveness of unlearning.\nOnline Client Number\nTest Accuracy\nTest AUC\n3\n0.813\n0.892\n4\n0.817\n0.893\n6\n0.828\n0.894\n9\n0.841\n0.895\n12\n0.847\n0.896\n16\n0.846\n0.896\n27 features, while the remaining 15 clients each possess 5 or\n6 features. In our setup, the active party (Client 15) and the\nrequesting client (Client 0) are always online, with the system\nsupporting up to 16 concurrent clients.\nWe measure the fidelity of the unlearned model using test\naccuracy and AUC. In Table 7, we tested the impact of dif-\nferent numbers of online clients after performing 50 rounds\nof unlearning. The results indicate that the number of online\nclients has minimal effect on fidelity. While model fidelity\nslightly improves with higher online rates, the asynchronous\nmethod achieves nearly identical performance to the fully\nonline system when approximately 3/4 of clients are online.\nThis reduces the total online time and communication costs\nby 25%, offering a substantial efficiency gain.\n5\nConclusion\nIn VFU, multiple unlearning requests targeting different ob-\njectives may arise, which the current methods cannot handle\nin a compatible manner. Additionally, the requirement for\n9\n\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\nFeature Removal Rate\n0.80\n0.82\n0.84\n0.86\n0.88\n0.90\n0.92\n0.94\n0.96\n0.98\n1.00\nTest Accuracy\nAdult Income\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\nFeature Removal Rate\n0.80\n0.82\n0.84\n0.86\n0.88\n0.90\n0.92\n0.94\n0.96\n0.98\n1.00\nTest Accuracy\nAdult Income\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\nFeature Removal Rate\n0.80\n0.82\n0.84\n0.86\n0.88\n0.90\n0.92\n0.94\n0.96\n0.98\n1.00\nTest Accuracy\nMalware\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\nFeature Removal Rate\n0.80\n0.82\n0.84\n0.86\n0.88\n0.90\n0.92\n0.94\n0.96\n0.98\n1.00\nTest Accuracy\nMalware\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\nFeature Removal Rate\n0\n30\n60\n90\n120\n150\n180\n210\n240\nFinal Epoch\nAdult Income\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\nFeature Removal Rate\n0\n50\n100\n150\n200\n250\n300\n350\n400\nFinal Epoch\nAdult Income\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\nFeature Removal Rate\n0\n25\n50\n75\n100\n125\n150\n175\n200\nFinal Epoch\nMalware\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\nFeature Removal Rate\n0\n15\n30\n45\n60\n75\n90\n105\nFinal Epoch\nMalware\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\nFeature Removal Rate\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nCommunication Cost per Epoch\nAdult Income\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\nFeature Removal Rate\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nCommunication Cost per Epoch\nAdult Income\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\nFeature Removal Rate\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\nCommunication Cost per Epoch\nMalware\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\nFeature Removal Rate\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\nCommunication Cost per Epoch\nMalware\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\nFigure 4: Feature Removal Results.\nall clients to participate in unlearning imposes a significant\nburden on the clients. To address the practical challenges in\nVFU tasks, we propose a new unlearning framework that can\ncompatibly handle different unlearning objectives. For learn-\ning models with strongly convex loss functions, our method\nprovides certified unlearning with theoretical guarantees. Fur-\nthermore, considering the characteristics of VFU, we intro-\nduce the first asynchronous VFU system capable of unlearn-\ning. This system balances the needs of different clients while\nreducing computation and communication costs. Extensive\nexperiments validate the applicability of our method.\nHowever, our method still has limitations: the theoretical\nanalysis of certified unlearning currently applies only to mod-\nels with strongly convex loss functions. Additionally, our\napproach is currently focused on VFL systems using the Ag-\ngVFL structure, and further research is needed to extend it\nto more complex split-NN-based VFL systems. These chal-\nlenges require further exploration, and we hope to continue\nexpanding the practical applications of VFU, achieving com-\nprehensive privacy protection both during and after training.\nReferences\n[1] Daniel Arp, Michael Spreitzenbarth, Malte Hübner,\nHugo Gascon, and Konrad Rieck. Drebin: Effective\nand explainable detection of android malware in your\npocket. 02 2014.\n[2] Barry Becker and Ronny Kohavi.\nAdult.\nUCI\nMachine\nLearning\nRepository, 1996.\nDOI:\nhttps://doi.org/10.24432/C5XW20.\n[3] Lucas Bourtoule, Varun Chandrasekaran, Christopher A.\nChoquette-Choo, Hengrui Jia, Adelin Travers, Baiwu\nZhang, David Lie, and Nicolas Papernot. Machine un-\nlearning, 2020.\n[4] Stephen Boyd. Convex optimization. Cambridge UP,\n2004.\n[5] Kamalika Chaudhuri, Claire Monteleoni, and Anand D.\nSarwate. Differentially private empirical risk minimiza-\ntion. J. Mach. Learn. Res., 12(null):1069–1109, July\n2011.\n[6] Chong Chen, Fei Sun, Min Zhang, and Bolin Ding. Rec-\nommendation unlearning. In Proceedings of the ACM\nWeb Conference 2022, WWW ’22, page 2768–2777,\nNew York, NY, USA, 2022. Association for Computing\nMachinery.\n[7] Kewei Cheng, Tao Fan, Yilun Jin, Yang Liu, Tianjian\nChen, Dimitrios Papadopoulos, and Qiang Yang. Secure-\nboost: A lossless federated learning framework, 2021.\n10\n\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSensitive Information Removal Rate\n0.66\n0.68\n0.70\n0.72\n0.74\n0.76\n0.78\n0.80\n0.82\n0.84\nTest Accuracy\nCredit\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSensitive Information Removal Rate\n0.66\n0.68\n0.70\n0.72\n0.74\n0.76\n0.78\n0.80\n0.82\n0.84\nTest Accuracy\nCredit\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSensitive Information Removal Rate\n0.66\n0.68\n0.70\n0.72\n0.74\n0.76\n0.78\n0.80\n0.82\n0.84\nTest Accuracy\nDiabetes\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSensitive Information Removal Rate\n0.700\n0.725\n0.750\n0.775\n0.800\n0.825\n0.850\n0.875\n0.900\nTest Accuracy\nDiabetes\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSensitive Information Removal Rate\n0\n40\n80\n120\n160\n200\n240\n280\nFinal Epoch\nCredit\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSensitive Information Removal Rate\n0\n25\n50\n75\n100\n125\n150\n175\n200\nFinal Epoch\nCredit\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSensitive Information Removal Rate\n0\n50\n100\n150\n200\n250\n300\n350\n400\nFinal Epoch\nDiabetes\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSensitive Information Removal Rate\n0\n50\n100\n150\n200\n250\n300\n350\n400\nFinal Epoch\nDiabetes\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSensitive Information Removal Rate\n0.00\n0.06\n0.12\n0.18\n0.24\n0.30\n0.36\n0.42\n0.48\n0.54\nCommunication Cost per Epoch\nCredit\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSensitive Information Removal Rate\n0.00\n0.06\n0.12\n0.18\n0.24\n0.30\n0.36\n0.42\n0.48\n0.54\nCommunication Cost per Epoch\nCredit\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSensitive Information Removal Rate\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\n0.014\n0.016\nCommunication Cost per Epoch\nDiabetes\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSensitive Information Removal Rate\n0.0000\n0.0015\n0.0030\n0.0045\n0.0060\n0.0075\n0.0090\n0.0105\n0.0120\n0.0135\nCommunication Cost per Epoch\nDiabetes\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\nFigure 5: Sensitive Information Removal Results.\n[8] Zihao Deng, Zhaoyang Han, Chuan Ma, Ming Ding,\nLong Yuan, Chunpeng Ge, and Zhe Liu. Vertical feder-\nated unlearning on the logistic regression model. Elec-\ntronics, 12(14), 2023.\n[9] Chong Fu, Xuhong Zhang, Shouling Ji, Jinyin Chen,\nJingzheng Wu, Shanqing Guo, Jun Zhou, Alex X Liu,\nand Ting Wang. Label inference attacks against vertical\nfederated learning. In 31st USENIX security symposium\n(USENIX Security 22), pages 1397–1414, 2022.\n[10] Fangcheng Fu, Xupeng Miao, Jiawei Jiang, Huanran\nXue, and Bin Cui. Towards communication-efficient\nvertical federated learning training via cache-enabled\nlocal updates. Proceedings of the VLDB Endowment,\n15(10):2111–2120, June 2022.\n[11] Hanlin Gu, Win Kent Ong, Chee Seng Chan, and Lixin\nFan. Ferrari: Federated feature unlearning via optimiz-\ning feature sensitivity, 2025.\n[12] Hanlin Gu, Hong Xi Tae, Chee Seng Chan, and Lixin\nFan. A few-shot label unlearning in vertical federated\nlearning, 2024.\n[13] Chuan Guo, Tom Goldstein, Awni Hannun, and Lau-\nrens Van Der Maaten. Certified data removal from ma-\nchine learning models. In Proceedings of the 37th Inter-\nnational Conference on Machine Learning, ICML’20.\nJMLR.org, 2020.\n[14] Otkrist Gupta and Ramesh Raskar. Distributed learning\nof deep neural network over multiple agents, 2018.\n[15] Stephen Hardy, Wilko Henecka, Hamish Ivey-Law,\nRichard Nock, Giorgio Patrini, Guillaume Smith, and\nBrian Thorne. Private federated learning on vertically\npartitioned data via entity resolution and additively ho-\nmomorphic encryption, 2017.\n[16] Michael Kahn. Diabetes. UCI Machine Learning Repos-\nitory. DOI: https://doi.org/10.24432/C5T59G.\n[17] Bowen Li, Jian Zhang, Jie Li, and Chentao Wu. Se-\ncurecut: Federated gradient boosting decision trees\nwith nbsp;efficient machine unlearning.\nIn Pattern\nRecognition: 27th International Conference, ICPR 2024,\nKolkata, India, December 1–5, 2024, Proceedings, Part\nV, page 350–365, Berlin, Heidelberg, 2024. Springer-\nVerlag.\n[18] Yang Liu, Yan Kang, Chaoping Xing, Tianjian Chen,\nand Qiang Yang.\nA secure federated transfer learn-\ning framework. IEEE Intelligent Systems, 35(4):70–82,\n2020.\n11\n\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEpoch\n0\n5\n10\n15\n20\n25\n30\nTrain Loss\nOurs\nRetrain\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEpoch\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nTrain Accuracy\nOurs\nRetarin\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEpoch\n0\n2\n4\n6\n8\nGradient Residual\nOurs\nRetarin\nFigure 6: The variation curves of training loss, training accuracy, and gradient residuals as updates progress.\n[19] Yang Liu, Yan Kang, Tianyuan Zou, Yanhong Pu, Yuan-\nqin He, Xiaozhou Ye, Ye Ouyang, Ya-Qin Zhang, and\nQiang Yang. Vertical federated learning: Concepts, ad-\nvances, and challenges. IEEE Transactions on Knowl-\nedge and Data Engineering, 2024.\n[20] H. Brendan McMahan, Eider Moore, Daniel Ram-\nage, Seth Hampson, and Blaise Agüera y Arcas.\nCommunication-efficient learning of deep networks\nfrom decentralized data, 2023.\n[21] Seth Neel, Aaron Roth, and Saeed Sharifi-Malvajerdi.\nDescent-to-delete: Gradient-based methods for machine\nunlearning, 2020.\n[22] Thanh Tam Nguyen, Thanh Trung Huynh, Zhao Ren,\nPhi Le Nguyen, Alan Wee-Chung Liew, Hongzhi Yin,\nand Quoc Viet Hung Nguyen. A survey of machine\nunlearning, 2024.\n[23] Vladislav\nRajkovic.\nNursery.\nUCI\nMa-\nchine\nLearning\nRepository, 1989.\nDOI:\nhttps://doi.org/10.24432/C5P88W.\n[24] Ningxin Su and Baochun Li. Asynchronous federated\nunlearning. In IEEE INFOCOM 2023 - IEEE Confer-\nence on Computer Communications, pages 1–10, 2023.\n[25] Praneeth Vepakomma, Otkrist Gupta, Tristan Swedish,\nand Ramesh Raskar.\nSplit learning for health: Dis-\ntributed deep learning without sharing raw patient data.\narXiv preprint arXiv:1812.00564, 2018.\n[26] Zichen Wang, Xiangshan Gao, Cong Wang, Peng Cheng,\nand Jiming Chen. Efficient vertical federated unlearning\nvia fast retraining. ACM Trans. Internet Technol., 24(2),\nMay 2024.\n[27] Alexander Warnecke, Lukas Pirch, Christian Wressneg-\nger, and Konrad Rieck. Machine unlearning of features\nand labels, 2023.\n[28] Haonan Yan, Xiaoguang Li, Ziyao Guo, Hui Li, Fenghua\nLi, and Xiaodong Lin. Arcane: An efficient architecture\nfor exact machine unlearning. In Lud De Raedt, edi-\ntor, Proceedings of the Thirty-First International Joint\nConference on Artificial Intelligence, IJCAI-22, pages\n4006–4013. International Joint Conferences on Artificial\nIntelligence Organization, 7 2022. Main Track.\n[29] Shengwen Yang, Bing Ren, Xuhui Zhou, and Liping Liu.\nParallel distributed logistic regression for vertical fed-\nerated learning without third-party coordinator. ArXiv,\nabs/1911.09824, 2019.\n[30] I-Cheng Yeh.\nDefault of Credit Card Clients.\nUCI Machine Learning Repository, 2009.\nDOI:\nhttps://doi.org/10.24432/C55S3H.\n[31] Lei Yu, Meng Han, Yiming Li, Changting Lin, Yao\nZhang, Mingyang Zhang, Yan Liu, Haiqin Weng, Yuseok\nJeon, Ka-Ho Chow, and Stacy Patterson. A survey of pri-\nvacy threats and defense in vertical federated learning:\nFrom model life cycle perspective, 2024.\n[32] Yang Zhao, Jiaxi Yang, Yiling Tao, Lixu Wang, Xiaoxiao\nLi, Dusit Niyato, and H. Vincent Poor. Exploring fed-\nerated unlearning: Analysis, comparison, and insights,\n2025.\n[33] Tianyuan Zou, Zixuan Gu, Yu He, Hideaki Takahashi,\nYang Liu, and Ya-Qin Zhang. Vflair: A research library\nand benchmark for vertical federated learning, 2024.\nAppedix A\nPrevious work has shown that in a VFL system using logistic\nregression, the update contribution coefficients of each client\nremain fixed during each update. We now extend this result\nto Multi-Layer Perceptron (MLP). While the contribution co-\nefficients do not remain constant, we demonstrate that, under\nan appropriate setting for the unlearning tasks, the change in\ncoefficients is minimal when the MLP’s depth and width are\nconstrained.\n12\n\nMathematical Notation\nConsider a classification problem with K labels and a cross-\nentropy loss function. The input vector is denoted as x ∈Rn,\nand the output y ∈RK is a one-hot encoded vector, where\nall components are 0 except for a single entry yk = 1, which\ncorresponds to the correct class label k for the input x. We use\na fully connected neural network fθ(·) with L layers, applying\nthe ReLU activation function after each linear transformation.\nThe ReLU function, defined as\nσ(x) = [x ≥0]x,\noutputs the element-wise maximum of each input component\nand zero. The network’s output is a vector of logits h ∈RK,\ncomputed recursively as follows:\nh(p) = W(p)x(p) +b(p),\nx(p+1) = σ(h(p)).\nLet the input and output of the p-th layer be denoted by x(p)\nand h(p), respectively. We set x(1) = x and h = fθ(x) = h(L).\nThe parameters of the network are represented as\nθ = col(w(1),b(1),...,w(L),b(L)) ∈RP,\nwhere w(p) is the flattened version of the weight matrix W(p)\nfor the p-th layer, and b(p) is the corresponding bias vector.\nThe output confidence vector p ∈RK is defined as the softmax\nof the logits h, i.e.,\npi = softmax(h)i =\nexp(hi)\n∑K\nj=1 exp(hj) ∈(0;1).\nThe loss function is cross-entropy loss:\nℓ(h,y) = CE(p,y) = −\nK\n∑\nk=1\nyk log pk ∈R+.\nHere the following is denoted:\n• Matrix representation of the ReLU activation function:\nD(p) = diag([h(p) ⩾0]),\n• The partial derivative of logits w.r.t. logits at p-th layer:\nG(p) =\n∂h\n∂h(p) = W(L)D(L−1)W(L−1)D(L−2) ·...·D(p),\n• Its stacked version:\nF⊤=\n\n\n\n\n\n\n\n(G(1))⊤⊗x(1)\n(G(1))⊤\n...\n(G(L))⊤⊗x(L)\n(G(L))⊤\n\n\n\n\n\n\n\n,\nConsider the next epoch, in which all the parameters in the\nMLP are updated by\n¯W(p) = W(p) +τ∆W(p),\n¯b(p) = b(p) +τ∆b(p),\np = 1,...,L.\nThen conduct back propagation this time, we would have\n¯h(p) = ¯W(p)¯x(p) + ¯b(p),\n¯x(p+1) = σ(¯h(p)),\n¯D(p) = diag([¯h(p) ⩾0]),\n¯G(p) = ¯W(L) ¯D(L−1) ¯W(L−1) ¯D(L−2) ·...· ¯D(p).\nEquipped with these notations,\n¯F⊤=\n\n\n\n\n\n\n\n( ¯G(1))⊤⊗¯x(1)\n( ¯G(1))⊤\n...\n( ¯G(L))⊤⊗¯x(L)\n( ¯G(L))⊤\n\n\n\n\n\n\n\n.\nOur goal is to bound\n∥¯F∥2\nF\n∥F∥2\nF\n.\nDerivation\nFirstly, we need to add an assumption:\nFor any epoch t, the distance between parament θt and\noptimum parament θ∗is bounded by a small value g, an as-\nsumption widely used in NTK,\n∥θt −θ∗∥2 < g.\nIn machine unlearning, the data to be unlearned consti-\ntutes only a small portion of the original training set. As a\nresult, the difference between model parament before and\nafter unlearning is minimal, validating the assumption.\nP.S. Our experiments show that the gradient of the original\nmodel on the training set after unlearning is negligible, further\nconfirming the validity of this assumption.\nSince we assume that the distance between Ft and F∗is\nsmall, meaning that Ft is gradually approaching F∗during\nthe iteration process, then the two quantities can be related as\nfollows:\n∥Ft∥2\nF = ∥F∗∥2\nF +2⟨Ft −F∗,F∗⟩+∥Ft −F∗∥2\nF.\nThus,\n∥Ft∥2\nF\n∥F∗∥2\nF\n= 1+ 2⟨Ft −F∗,F∗⟩\n∥F∗∥2\nF\n+ ∥Ft −F∗∥2\nF\n∥F∗∥2\nF\n.\nWhen Ft is close to F∗, ⟨Ft −F∗,F∗⟩is relatively small,\nThus ∥Ft∥2\nF\n∥F∗∥2\nF equals to 1+ ∥Ft−F∗∥2\nF\n∥F∗∥2\nF\n.\n13\n\nOur goal is to bound\n∥Ft −F∗∥2\nF,\nnote that since loss function is not considered\n∥F∗∥2\nF ̸= 0,\n(Ft −F∗)⊤=\n\n\n\n\n\n\n\n(G(1))⊤⊗x(1) −(G(1)∗)⊤⊗x(1)∗\n(G(1) −G(1)∗)⊤\n...\n(G(L))⊤⊗x(L) −(G(L)∗)⊤⊗x(L)∗\n(G(L) −G(L)∗)⊤\n\n\n\n\n\n\n\n.\nThen, using the property that squared spectral norm of\nvertically-stacked matrices is less or equal to the sum of their\nsquared spectral norms (it is easy to observe), we get:\n∥Ft −F∗∥2\n2\n⩽\nL\n∑\np=1\n\u0010\n∥(G(p))⊤⊗x(p) −(G(p)∗)⊤⊗x(p)∗∥2\n2\n+∥(G(p) −G(p)∗)⊤∥2\n2\n\u0011\n.\nThe Kronecker product satisfies the distributive property\nover matrix addition,\n∥Ft −F∗∥2\n2 ⩽\nL\n∑\np=1\n\u0010\n∥(G(p))⊤⊗x(p) −(G(p)∗)⊤⊗x(p)\n+(G(p)∗)⊤⊗x(p) −(G(p)∗)⊤⊗x(p)∗∥2\n2\n+∥(G(p) −G(p)∗)⊤∥2\n2\n\u0011\n⩽\nL\n∑\np=1\n\u0010\n∥(G(p) −G(p)∗)⊤⊗x(p)\n+(G(p)∗)⊤⊗(x(p) −x(p)∗)∥2\n2\n+∥(G(p) −G(p)∗)⊤∥2\n2\n\u0011\n.\nBy the triangle inequality and the sub-additivity of the\nspectral norm,\n∥Ft −F∗∥2\n2 ⩽\nL\n∑\np=1\n\u0010\n∥(G(p) −G(p)∗)⊤⊗x(p)∥2\n2\n+∥(G(p)∗)⊤⊗(x(p) −x(p)∗)∥2\n2\n+∥(G(p) −G(p)∗)⊤∥2\n2\n\u0011\n.\nSpectral norm of the Kronecker matrix product is equal to\ntheir ordinary product norm,\n∥Ft −F∗∥2\n2 ⩽\nL\n∑\np=1\n \n∥G(p) −G(p)∗∥2\n2 ·\n\u0010\n∥x(p)∥2\n2 +1\n\u0011\n+∥(G(p)∗)⊤∥2\n2 ·∥(x(p) −x(p)∗)∥2\n2\n!\n.\nIn this problem, we assume that the distance between\nthe network parameters θt and the optimal parameters θ∗\nis bounded by a small value g, and the goal now is to derive\nan upper bound for the difference in the intermediate val-\nues ∥x(p) −x(p)∗∥2\n2 and ∥G(p) −G(p)∗∥2\n2 for each layer of the\nMLP.\nBound on ∥x(p) −x(p)∗∥2\n2:\nThe correct expression for the difference in activations at\nlayer p is:\nx(p)−x(p)∗= σ\n\u0010\nW(p)x(p−1) −W(p)∗x(p−1)∗+b(p) −b(p)∗\u0011\n,\nWe define the activation function as being Lipschitz contin-\nuous with a constant γz. For ReLU function we used, γz = 1.\nThe difference in activations can be bounded as:\n∥x(p) −x(p)∗∥2\n2 ≤2γ2\nz\nh\n∥W(p)x(p−1) −W(p)∗x(p−1)∗∥2\n2\n+∥b(p) −b(p)∗∥2\n2\ni\n≤4γ2\nz\nh\n∥W(p) −W(p)∗∥2\n2∥x(p−1)∗∥2\n2\n+∥W(p)∥2\n2∥x(p−1) −x(p−1)∗∥2\n2\n+∥b(p) −b(p)∗∥2\n2\ni\n.\nAssuming that the ∥x(t)∥2\n2 is bounded by C2\nx, and ∥W(p)∥2\n2\nis bounded by C2\nW, we can use the fact that the perturbation\nin the parameters is bounded by g, and propagate this bound\nthrough the layers. Since∥x(1) −x(1)∗∥2\n2 = 0, the bound on the\ndifference in activations at layer p becomes:\n∥x(p) −x(p)∗∥2\n2 ≤4γ2\nz\nh\ng2C2\nx +g2 +C2\nW∥x(p−1) −x(p−1)∗∥2\n2\ni\n≤4γ2\nzg2 \u0002\nC2\nx +1\n\u0003\u0014(4γ2\nzC2\nW)p−1 −1\n4γ2zC2\nW −1\n\u0015\n+(4γ2\nzC2\nW)p−1∥x(1) −x(1)∗∥2\n2\n= 4γ2\nzg2 \u0002\nC2\nx +1\n\u0003\u0014(4γ2\nzC2\nW)p−1 −1\n4γ2zC2\nW −1\n\u0015\n.\nBound on ∥G(p) −G(p)∗∥2\n2:\nFor the gradients, we consider the difference in gradients\nat layer p:\nG(p) −G(p)∗=\n∂h\n∂h(p) −∂h∗\n∂h(p)∗,\n14\n\nwhere\n∂h\n∂h(p) = W(L)D(L−1)W(L−1)D(L−2) ···D(p) is the chain\nof derivatives through the layers.\nG(p) = G(p+1)W(p+1)D(p),\nG(p)∗= G(p+1)∗W(p+1)∗D(p)∗,\n∥G(p) −G(p)∗∥2\n= ∥G(p+1)W(p+1)D(p) −G(p+1)∗W(p+1)∗D(p)∗∥2\n≤2∥G(p+1)∥2 ·∥W(p+1)D(p) −W(p+1)∗D(p)∗∥2\n+2∥W(p+1)∗D(p)∗∥2 ·∥G(p+1) −G(p+1)∗∥2.\nAssuming that the ∥G(p+1)∥2\n2 is bounded by C2\nG, and\n∥W(p+1)∗D(p)∗∥2\n2 is bounded by C2\nW, similarly:\n∥G(p) −G(p)∗∥2\n≤2C2\nG∥W(p+1)D(p) −W(p+1)∗D(p)∗∥2\n+2C2\nW∥G(p+1) −G(p+1)∗∥2\n≤2C2\nGg2 +2C2\nW∥G(p+1) −G(p+1)∗∥2\n≤2C2\nGg2 (2C2\nW)L−p −1\n2C2\nW −1\n+(2C2\nW)L−p∥G(L) −G(L)∗∥2\n≤2C2\nGg2 (2C2\nW)L−p −1\n2C2\nW −1\n+(2C2\nW)L−pg2.\nThe original inequality is given by:\n∥Ft −F∗∥2\n2 ≤\nL\n∑\np=1\n\u0010\n∥G(p) −G(p)∗∥2\n2 ·(∥x(p)∥2\n2 +1)\n+∥(G(p)∗)⊤∥2\n2 ·∥(x(p) −x(p)∗)∥2\n2\n\u0011\n.\nSubstitute ∥G(p) −G(p)∗∥2\n2:\n∥G(p) −G(p)∗∥2 ≤2C2\nGg2 (2C2\nW)L−p −1\n2C2\nW −1\n+(2C2\nW)L−pg2.\n∥G(p) −G(p)∗∥2\n2 ·(∥x(p)∥2\n2 +1)\n≤[2C2\nGg2 (2C2\nW)L−p −1\n2C2\nW −1\n+(2C2\nW)L−pg2](∥x(p)∥2\n2 +1).\nSubstitute ∥x(p) −x(p)∗∥2\n2:\n∥x(p) −x(p)∗∥2\n2 ≤4γ2\nzg2 \u0002\nC2\nx +1\n\u0003 (4γ2\nzC2\nW)p−1 −1\n4γ2zC2\nW −1\n.\n∥(G(p)∗)⊤∥2\n2 ·∥(x(p) −x(p)∗)∥2\n2\n≤∥(G(p)∗)⊤∥2\n2 ·4γ2\nzg2 \u0002\nC2\nx +1\n\u0003\n· (4γ2\nzC2\nW)p−1 −1\n4γ2zC2\nW −1\n.\nWe assume the following bounds:\n∥x(p)∥2\n2 ≤C2\nx,\n∥(G(p)∗)⊤∥2\n2 ≤C2\nG.\nSubstitute these bounds into the inequality:\n∥Ft −F∗∥2\n2\n≤\nL\n∑\np=1\n(\u0014\n2C2\nGg2 (2C2\nW)L−p −1\n2C2\nW −1\n+(2C2\nW)L−pg2\n\u0015\n(C2\nx +1)\n+4C2\nGγ2\nzg2(C2\nx +1)(4γ2\nzC2\nW)p−1 −1\n4γ2zC2\nW −1\n)\n≤\nL\n∑\np=1\n(\u0014\n2C2\nG\n(2C2\nW)L−p −1\n2C2\nW −1\n+(2C2\nW)L−p\n+4C2\nGγ2\nz\n(4γ2\nzC2\nW)p−1 −1\n4γ2zC2\nW −1\n\u0015\ng2(C2\nx +1)\n)\n.\nsince\n∥A∥F ≤\np\nrank(A)∥A∥2,\nwe get\n∥Ft −F∗∥2\nF ⩽\np\nrank(F)∥Ft −F∗∥2\n2.\nAssuming that the layers of the MLP are constrained to\nhave a width of less than w,\nrank(F) = max(w2,2L).\nrank(F) is determined by the largest width or depth of the\nMLP. So,\n∥Ft −F∗∥2\nF\n≤\nq\nmax(w2,2L)\nL\n∑\np=1\n(\u0014\n2C2\nG\n(2C2\nW)L−p −1\n2C2\nW −1\n+(2C2\nW)L−p\n+4C2\nGγ2\nz\n(4γ2\nzC2\nW)p−1 −1\n4γ2zC2\nW −1\n\u0015\ng2(C2\nx +1)\n)\n.\nWhen g is close to 0, ∥Ft −F∗∥2\nF is close to 0.\nThe analysis of the bound expression reveals that the sta-\nbility of the update contribution coefficient is influenced by\nseveral factors, including the MLP model’s depth and width,\nas well as the absolute magnitudes of the parameters, activa-\ntions, and gradients at each layer. Of these, the model’s depth\nhas the most significant impact.\nAppedix B\nWe can also prove that in VFL, performing unlearning only\nfor the client requesting unlearning is insufficient to achieve\nthe overall unlearning objective.\n15\n\nObjective\nWe aim to prove that when only a subset of clients partic-\nipates in the unlearning process, the unlearning operation\ncannot achieve approximate unlearning. To achieve approxi-\nmate unlearning, we need to ensure that the parameters after\nunlearning are very close to the parameters of a retrained\nmodel. This can be done by controlling the gradient residual.\nComposition of Prediction P\nIn vertical federated learning, the prediction P is calculated\nbased on the intermediate values of all client output.\nP = Global(\nk\n∑\ni=1\nhi(θi,Zi)).\nwhere: - hi(θi,Zi) is the intermediate output of the i-th\nclient. - k is the number of clients participating in federated\nlearning. - Global() is the global model located in the activate\nparty.\nGradient Representation\nTo derive the requirements for approximate unlearning, we\nfirst examine the gradient of the logistic regression model’s\nloss function with respect to the model parameters. Using the\nchain rule, the gradient can be written as the product of the\nfollowing three terms:\n∇θL = ∂L\n∂P · ∂P\n∂h · ∂h\n∂θ.\nwhere:\n•\n∂L\n∂P : The derivative of the loss function (e.g., cross-\nentropy loss) with respect to the prediction P.\n•\n∂P\n∂h : The derivative of the prediction P with respect to\neach client’s intermediate value h.\n•\n∂h\n∂θ: The derivative of each client’s intermediate value h\nwith respect to the client’s parameters θ.\nOur goal is to make the gradient ∇θL close to zero, which\nwould achieve approximate unlearning. The main factor af-\nfecting the gradient is the first term, ∂L\n∂P , i.e., the derivative of\nthe loss function with respect to the prediction P. This term\ndetermines the size of the final gradient. If the prediction after\nunlearning is close to the retrained prediction, this term will\nbe close to zero, which would bring the gradient close to zero.\nSo, to achieve the approximate unlearning, the difference be-\ntween the unlearned predictions and the retrained predictions\nshould be small\n| ˜P−˜P∗| = |Global(\nk\n∑\ni=1\nhi(˜θi, ˜Zi))−Global(\nk\n∑\ni=1\nhi(˜θ∗\ni , ˜Zi))|,\nwhere ˜Z is the training set after the unlearning request, ˜θi\nis the unlearned parament of i-th client and ˜θ∗\ni is the retrained\nparament.\nIn AggVFL, the global model is not trainable, so the\nsummed predictions from the clients should be close to that\nof the retrained model’s output. To achieve the approximate\nunlearning, we want that there exists a small value ε > 0 that\n|\nk\n∑\ni=1\nhi(˜θi, ˜Zi)−\nk\n∑\ni=1\nhi(˜θ∗\ni , ˜Zi)| < ε.\nSuppose overall there are k features, among them the first j\nfeatures take part in the unlearning process\n˜θ = [˜θT\n1 ,..., ˜θT\nj ,θ∗T\nj+1,...,θ∗T\nk ]T.\nThe unchanged intermediate client output with features\ndenoted as θ∗T\ni , are different from the retrained value with a\ndistance |hU(θ∗\ni , ˜Zi)| caused by unlearned feature.\n|\nk\n∑\ni=1\nhi(θ∗\ni , ˜Zi)−\nk\n∑\ni=1\nhi(˜θ∗\ni , ˜Zi)|\n= |\nk\n∑\ni=1\nhi(θ∗\ni , ˜Zi)−\nk\n∑\ni=1\nhi(θ∗\ni ,Zi)+\nk\n∑\ni=1\nhi(θ∗\ni ,Zi)−\nk\n∑\ni=1\nhi(˜θ∗\ni , ˜Zi)|\n= |hU(θ∗, ˜Z)+ε|.\nThe approximate unlearning target below is hard to achieve\nwhen j is small because h has a much larger dimension, the\nunlearned features’ gap can hardly be covered by j features.\n|\nk\n∑\ni=1\nhi(˜θi, ˜Zi)−\nk\n∑\ni=1\nhi(˜θ∗\ni , ˜Zi)|\n= |\nj\n∑\ni=1\nhi(˜θi, ˜Zi)−\nj\n∑\ni=1\nhi(θ∗\ni , ˜Zi)+\nk\n∑\ni=1\nhi(θ∗\ni , ˜Zi)−\nk\n∑\ni=1\nhi(˜θ∗\ni , ˜Zi)|\n= |\nj\n∑\ni=1\nhi(˜θi, ˜Zi)−\nj\n∑\ni=1\nhi(θ∗\ni , ˜Zi)+hU(θ∗, ˜Z)+ε|.\nBesides, we can interpret the prediction difference in terms\nof vector spaces. Let A = ∑j\ni=1 hi(˜θi, ˜Zi) represent the mod-\nified outputs, and B = ∑k\ni=j+1 hi(θ∗\ni , ˜Zi) represent the un-\nchanged outputs. The difference between the modified and\nretrained outputs is:\n|A−B| = |hU(θ∗, ˜Z)+ε|.\nWhen j is small, the unlearning operation can only change\na small part of the output space. Since hU(θ∗, ˜Z) involves\ncontributions from the unchanged clients’ outputs, and their\nrank may be high, the modification of j clients’ outputs is\ninsufficient to reduce the overall error. Hence, the difference\ncannot be small enough to achieve approximate unlearning.\n16\n\nThe approximate unlearning target is difficult to achieve\nwhen j is small because the change induced by unlearning\nonly a few clients cannot cover the large-dimensional gap\ncaused by the unlearned features. The rank of the output\nmatrix will not be sufficiently reduced, and the residual error\nwill remain significant, preventing the global prediction from\nmatching the retrained model’s output. Therefore, unlearning\nby a subset of clients is insufficient for achieving approximate\nunlearning.\nAppedix C\nThe results are shown in Figure 7 and 8.\nAppedix D\nOur method involves multiple rounds of retraining, with the\nparameters of the pre-trained original model serving as the\ninitial state. To achieve the rigorously defined certified un-\nlearning, additional adjustments are required. It is worth not-\ning that we make two key assumptions: First, the loss function\nis convex. Second, we include an L2 regularization 1\n2∥θ∥2\n2.\nIn the first round of updates, we simultaneously perform\ngradient ascent on the original data and gradient descent on the\nmodified data. It can be shown that this update is equivalent\nto the expression in Equation (1) from the well-established\ninfluence function-based certified forgetting method [27]. Ac-\ncording to Lemma 1 in that paper, the gradient bound of the\nupdated loss function can be derived from Equation (1).\n∆(Z, ˜Z) = −τ\n\u0010\n∇L(θ∗;D′)−∇L(θ∗;D)\n\u0011\n= −τ\n\u0010\n∑\n˜z∈˜Z\n∇θℓ(˜z,θ∗)+∇L(θ∗;D′ \\ ˜Z)\n−∑\nz∈Z\n∇θℓ(z,θ∗)−∇L(θ∗;D\\Z)\n\u0011\n= −τ\n\u0010\n∑\n˜z∈˜Z\n∇θℓ(˜z,θ∗)−∑\nz∈Z\n∇θℓ(z,θ∗)\n\u0011\nLemma 1.\n[27]Assume that ∥xi∥2 ≤1 for all data points\nand the gradient ∇ℓ(z,θ) is γz-Lipschitz with respect to z at\nθ∗and γ-Lipschitz with respect to θ. Further let ˜Z change\nthe features j,..., j +F by magnitudes at most m j,...,m j+F.\nIf M = ∑F\nj=1 mj the following upper bounds hold: For the\nfollowing update form\n∆(Z, ˜Z) = −τ\n\u0010\n∑\n˜z∈˜Z\n∇θℓ(˜z,θ∗)−∑\nz∈Z\n∇θℓ(z,θ∗)\n\u0011\n(1)\nWe have\n\r\r∇L\n\u0000θ∗\nZ→˜Z,D′\u0001\r\r\n2 ≤(1+τγn)γzM|Z|\nThe gradient residual ∇L(θ;D′) of a model θ with respect\nto the corrected dataset D′ is zero only when θ = A(D′). For\nstrongly convex loss functions, the magnitude of this gradient\nresidual, ∥∇L(θ;D′)∥2, reflects the discrepancy between the\nmodel θ and the one obtained by retraining on D′.\nNext, in subsequent updates, we employ an early stopping\nmechanism to ensure that the training loss continues to de-\ncrease on the updated training set. Since the loss function is\nstrongly convex, the gradient of the loss function after the\nupdate will also be smaller than that of the first round.\n\r\r∇L(θt+1)\n\r\r2 ≤\n\r\r∇L(θt)\n\r\r2\nFinally, we prove that after unlearning, the gradient of the\nloss function has an upper bound. Based on Lemma 2 from\nthe [27], we are able to demonstrate that certified unlearning\nholds.\nWhen a vector b is added, the gradient residual r for the\nloss function Lb becomes:\nr = ∇Lb(θ;D′) = ∑\nz∈D′\n∇ℓ(z,θ)+λθ+b\nBy manipulating the distribution of b, certified unlearning\ncan be achieved, akin to sensitivity-based techniques [5].\nLemma 2. [27]Let A be the learning algorithm that returns\nthe unique minimum of Lb(θ;D′) and let U be an unlearning\nmethod that produces a model θU. If ∥∇L(θU;D′)∥2 ≤ε′ for\nsome ε′ > 0 we have the following guarantees.\n1. If b is drawn from a distribution with density p(b) =\ne−ε\nε′ ∥b∥2 then U performs ε-certified unlearning for A.\n2. If p ∼N (0,cε′/ε)d for some c > 0 then U performs\n(ε,δ)-certified unlearning for A with δ = 1.5e−c2/2.\n17\n\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\nFeature Removal Rate\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\n0.014\nKL Divergence\nAdult Income\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\nFeature Removal Rate\n0.0000\n0.0006\n0.0012\n0.0018\n0.0024\n0.0030\n0.0036\n0.0042\n0.0048\n0.0054\nKL Divergence\nAdult Income\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\nFeature Removal Rate\n0.0000\n0.0004\n0.0008\n0.0012\n0.0016\n0.0020\n0.0024\n0.0028\nKL Divergence\nMalware\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\nFeature Removal Rate\n0.0000\n0.0005\n0.0010\n0.0015\n0.0020\n0.0025\n0.0030\n0.0035\n0.0040\n0.0045\nKL Divergence\nMalware\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\nFeature Removal Rate\n0.80\n0.82\n0.84\n0.86\n0.88\n0.90\n0.92\n0.94\n0.96\n0.98\n1.00\nTest AUC\nAdult Income\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\nFeature Removal Rate\n0.80\n0.82\n0.84\n0.86\n0.88\n0.90\n0.92\n0.94\n0.96\n0.98\n1.00\nTest AUC\nAdult Income\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\nFeature Removal Rate\n0.80\n0.82\n0.84\n0.86\n0.88\n0.90\n0.92\n0.94\n0.96\n0.98\n1.00\nTest AUC\nMalware\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\nFeature Removal Rate\n0.80\n0.82\n0.84\n0.86\n0.88\n0.90\n0.92\n0.94\n0.96\n0.98\n1.00\nTest AUC\nMalware\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\nFeature Removal Rate\n0.0\n0.6\n1.2\n1.8\n2.4\n3.0\n3.6\n4.2\n4.8\nTest Loss\nAdult Income\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\nFeature Removal Rate\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\nTest Loss\nAdult Income\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\nFeature Removal Rate\n0.00\n0.08\n0.16\n0.24\n0.32\n0.40\n0.48\n0.56\n0.64\nTest Loss\nMalware\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\nFeature Removal Rate\n0.00\n0.08\n0.16\n0.24\n0.32\n0.40\n0.48\n0.56\nTest Loss\nMalware\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\nFigure 7: Feature Removal Results.\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSensitive Information Removal Rate\n0.00000\n0.00004\n0.00008\n0.00012\n0.00016\n0.00020\n0.00024\n0.00028\nKL Divergence\nCredit\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSensitive Information Removal Rate\n0.0000\n0.0004\n0.0008\n0.0012\n0.0016\n0.0020\n0.0024\n0.0028\n0.0032\n0.0036\nKL Divergence\nCredit\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSensitive Information Removal Rate\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\nKL Divergence\nDiabetes\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSensitive Information Removal Rate\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\nKL Divergence\nDiabetes\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSensitive Information Removal Rate\n0.66\n0.68\n0.70\n0.72\n0.74\n0.76\n0.78\n0.80\n0.82\n0.84\nTest AUC\nCredit\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSensitive Information Removal Rate\n0.66\n0.68\n0.70\n0.72\n0.74\n0.76\n0.78\n0.80\n0.82\n0.84\nTest AUC\nCredit\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSensitive Information Removal Rate\n0.66\n0.68\n0.70\n0.72\n0.74\n0.76\n0.78\n0.80\n0.82\n0.84\nTest AUC\nDiabetes\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSensitive Information Removal Rate\n0.700\n0.725\n0.750\n0.775\n0.800\n0.825\n0.850\n0.875\n0.900\nTest AUC\nDiabetes\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSensitive Information Removal Rate\n0.0\n0.3\n0.6\n0.9\n1.2\n1.5\n1.8\n2.1\n2.4\n2.7\n3.0\nTest Loss\nCredit\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSensitive Information Removal Rate\n0.0\n0.3\n0.6\n0.9\n1.2\n1.5\n1.8\n2.1\n2.4\n2.7\n3.0\nTest Loss\nCredit\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSensitive Information Removal Rate\n0.0\n0.3\n0.6\n0.9\n1.2\n1.5\n1.8\n2.1\n2.4\n2.7\n3.0\nTest Loss\nDiabetes\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSensitive Information Removal Rate\n0.0\n0.3\n0.6\n0.9\n1.2\n1.5\n1.8\n2.1\n2.4\n2.7\n3.0\nTest Loss\nDiabetes\nasyn\nasyn_DP\nasyn_GA\nasyn_DP_GA\nbatch\nbatch_DP\nbatch_GA\nbatch_DP_GA\nretrain\nFigure 8: Sensitive Information Removal Results.\n18\n",
  "metadata": {
    "source_path": "papers/arxiv/Forgetting_Any_Data_at_Any_Time_A_Theoretically_Certified_Unlearning\n__Framework_for_Vertical_Federated_Learning_0670571a1acacd13.pdf",
    "content_hash": "0670571a1acacd1362d7759c3f2a4bbd7135428e81cfb51c34cac8b2d31da2ce",
    "arxiv_id": null,
    "title": "Forgetting_Any_Data_at_Any_Time_A_Theoretically_Certified_Unlearning\n__Framework_for_Vertical_Federated_Learning_0670571a1acacd13",
    "author": "",
    "creation_date": "D:20250225024247Z",
    "published": "2025-02-25T02:42:47",
    "pages": 18,
    "size": 2518802,
    "file_mtime": 1740470188.8633678
  }
}