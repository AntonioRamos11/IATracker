{
  "text": "LSERVE: EFFICIENT LONG-SEQUENCE LLM SERVING WITH\nUNIFIED SPARSE ATTENTION\nShang Yang * 1 Junxian Guo * 1 2 Haotian Tang 1 Qinghao Hu 1\nGuangxuan Xiao 1 Jiaming Tang 1 Yujun Lin 1 Zhijian Liu 3 Yao Lu 3 Song Han 1 3\nhttps://hanlab.mit.edu/projects/lserve\nABSTRACT\nLarge language models (LLMs) have shown remarkable potential in processing long sequences, yet efficiently\nserving these long-context models remains challenging due to the quadratic computational complexity of attention\nin the prefilling stage and the large memory footprint of the KV cache in the decoding stage. To address these\nissues, we introduce LServe, an efficient system that accelerates long-sequence LLM serving via hybrid sparse\nattention. This method unifies different hardware-friendly, structured sparsity patterns for both prefilling and\ndecoding attention into a single framework, where computations on less important tokens are skipped block-wise.\nLServe demonstrates the compatibility of static and dynamic sparsity in long-context LLM attention. This design\nenables multiplicative speedups by combining these optimizations. Specifically, we convert half of the attention\nheads to nearly free streaming heads in both the prefilling and decoding stages. Additionally, we find that only a\nconstant number of KV pages is required to preserve long-context capabilities, irrespective of context length. We\nthen design a hierarchical KV page selection policy that dynamically prunes KV pages based on query-centric\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9× and decoding by 1.3-2.1× over vLLM,\nmaintaining long-context accuracy. Code is released at https://github.com/mit-han-lab/omniserve.\n1\nINTRODUCTION\nLarge Language Models (LLMs) have dramatically trans-\nformed the field of artificial intelligence. With expand-\ning context window lengths, LLMs now demonstrate re-\nmarkable performance across diverse long-sequence appli-\ncations (Gemini Team, Google, 2024), including multi-turn\nconversations, long document analysis (Zhang et al., 2024b;\nGoyal & Durrett, 2020; Huang et al., 2021), multi-modal\nunderstanding (Xue et al., 2024a; Liu et al., 2024b; Lin et al.,\n2024a), and code completion (Li et al., 2023; Lozhkov et al.,\n2024). Many of these applications require processing hun-\ndreds of thousands of context tokens in real-world settings,\npresenting unique challenges. In particular, the demand\nfor fast prefilling, or minimizing the time to the first token,\nand the burden on the decoding phase due to the large KV\n(key-value) caches necessary for such contexts, represent\nsignificant hurdles.\nLong-sequence LLMs are about more than just an extended\ncontext.\nThe recently announced OpenAI o1 (OpenAI,\n2024) demonstrates exceptional capabilities in complex rea-\nsoning tasks, such as deciphering, mathematics, and cod-\n* indicates equal contribution. Part of the work was done while\nShang Yang and Haotian Tang are interning at NVIDIA. 1MIT\n2SJTU 3NVIDIA.\nMemory Saving\nDecoding Speed\nPrefilling Speed\nPrefilling Dynamic \nSparsity (MInference)\nDecoding Dynamic \nSparsity (Quest)\nStatic Sparsity \n(DuoAttention)\nKV Cache Quantization\nLServe (Ours) \nLServe: Efficient Long-Sequence LLM Serving\nSparse Attention Unification\n•\nUnified Framework for Sparse Attention \n•\nHybrid Static & Dynamic Sparsity \nSystem-Algorithm Co-optimization\n•\nHierarchical Paging System \n•\nReusable Page Selector\nFigure 1: LServe is an efficient system for serving long-\nsequence LLMs that leverages hybrid sparse attention. With\nthe unification of different sparse patterns as well as KV\ncache quantization, LServe achieves significant speedups in\nboth prefilling stage and decoding stage while also reducing\nthe memory consumption.\ning. These advancements are achieved through inference-\ntime scaling and the generation of extensive chains of\nthought (Wei et al., 2022). According to Qin et al. (2024),\no1’s internal reasoning process can extend to 20k tokens\nfor mathematical problems, making it the first known long-\ngeneration LLM. Contrary to the conventional belief that\nthe prefilling stage dominates runtime in long-sequence\nLLMs, when Llama-3-8B (Dubey et al., 2024) is run using\nTensorRT-LLM (NVIDIA, 2023) with 256k input tokens\nand 20k output tokens (comparable to o1’s reasoning trace\nlength), the prefilling time is 116 seconds, while decoding\ntakes 540 seconds — almost 5× longer.\narXiv:2502.14866v1  [cs.CL]  20 Feb 2025\n\nLServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention\n2\nTo enhance the efficiency of long-sequence LLMs, it is es-\nsential to optimize both the prefilling and decoding stages\nrather than focusing on just one. Beyond model architec-\ntural modifications in the pre-training stage (Ainslie et al.,\n2023; Brandon et al., 2024), existing acceleration solutions\nfor long-sequence LLMs primarily address efficiency from\ntwo angles. The first approach centers on KV cache quanti-\nzation, where methods such as QServe (Lin et al., 2024b),\nKIVI (Liu et al., 2024c), and KVQuant (Hooper et al., 2024)\nemploy low-bit quantization to reduce memory usage and\nI/O traffic, potentially increasing generation throughput.\nHowever, these quantization techniques do not lower the\nnumber of computations performed in the attention loop, re-\nsulting in suboptimal generation speeds as sequence lengths\ngrow. The second approach utilizes approximate sparse at-\ntention to improve long-sequence LLM performance. For\nexample, StreamingLLM (Xiao et al., 2023), H2O (Zhang\net al., 2024c), and TOVA (Oren et al., 2024) apply static\nmasking mechanisms to reduce attention complexity, though\nat the expense of accuracy in long-context tasks and irregu-\nlar KV cache memory layouts. DuoAttention (Xiao et al.,\n2024) advances this strategy by pruning attention computa-\ntions at a coarser granularity using an optimization-based\napproach. Other methods, such as MInference (Jiang et al.,\n2024b) and Quest (Tang et al., 2024), implement dynamic\nsparse attention to accelerate either the prefilling or decod-\ning stage. However, these approaches do not reduce KV\ncache memory consumption and lack a unified framework to\naddress efficiency challenges in both stages simultaneously.\nTo this end, we introduce LServe, an efficient system for\nserving long-sequence LLMs that leverages hybrid sparse\nattention. Recognizing that not all tokens hold equal impor-\ntance, LServe integrates multiple hardware-friendly, struc-\ntured sparsity patterns into a unified block sparse attention\nframework (see Figure 4). Block-level sparsity acceler-\nates attention computation by processing the KV history\nin discrete blocks. By skipping blocks, we directly reduce\nthe number of sequential iterations, resulting in measured\nspeedups during both the prefilling and decoding stages.\nBuilding on the unified block sparse attention framework,\nLServe further illustrates acceleration opportunities from\nstatic and dynamic sparsity.\nFor static sparsity, inspired by DuoAttention (Xiao et al.,\n2024), we modify the attention masks in the original model\nby converting half of the attention heads into Λ-shaped\nmasks, transforming these attention heads into streaming\nheads. Additionally, we fuse the computation of streaming\nand standard attention heads into unified GPU kernels for\nboth the prefilling and decoding stages, translating theoreti-\ncal computation and memory savings that translate to up to\n1.7× measured speedup.\nFor dynamic sparsity, we observe that query-centric spar-\nsity (Tang et al., 2024) allows for nearly lossless KV com-\npression: the required number of KV tokens to maintain\nlong-context capabilities remains constant (e.g., 4096), re-\ngardless of context length. To optimize efficiency, we design\na hierarchical page selector to identify important KV pages\nfor each query token, reusing the selection results across\ntokens to reduce page selection overhead by 4×.\nOur key observation is that static and dynamic sparsity\npatterns are orthogonal in long-sequence LLMs. By unify-\ning static and dynamic sparsity with KV cache quantization\ninto a single GPU kernel, LServe achieves compounded\nefficiency benefits from each individual optimization for\ndecoding stage attention.\nWe\nbenchmark\nLServe\nacross\nthree\nlong-sequence\nLLMs—Llama-3-8B, Minitron-4B, and Llama-2-7B—at\ncontext lengths up to 512k tokens. Compared to state-\nof-the-art frameworks like vLLM (Kwon et al., 2023b),\nQServe (Lin et al., 2024b), MInference (Jiang et al., 2024b),\nand DuoAttention (Xiao et al., 2024), LServe accelerates\nprefilling stage by up to 2.9× and achieves an average of\n1.3×-2.1× speedup in the decoding stage. Furthermore,\nLServe accomplishes these speedups while retaining the\nlong-context capabilities of the original dense, floating-point\nmodels, demonstrating that hybrid attention sparsity is a free\nlunch for long-sequence LLM serving.\n2\nBACKGROUND AND MOTIVATION\n2.1\nBackground\nLLM Inference.\nLLMs are transformer-based architec-\ntures with stacked identical layers, each containing attention\nblocks, feed-forward networks (FFN), and normalization\ncomponents. LLM inference involves two stages: an initial\nprefilling stage that handles multiple tokens concurrently,\nfollowed by auto-regressive decoding stage where only one\ntoken will be processed for each request in a decoding step.\nAttention. The attention mechanism exchanges informa-\ntion across tokens. It first transforms input x through lin-\near projections to generate query vectors q ∈RN×HD,\nand key-value pairs k, v ∈RN× ˆ\nHD, where ˆH represents\nthe key/value head count. Traditional multi-head attention\n(MHA) maintains H = ˆH, and contemporary architectures\n(Touvron et al., 2023; Jiang et al., 2023; 2024a) employ\ngrouped-query attention (GQA) (Ainslie et al., 2023) where\nH = n ˆH(n ∈Z) to shrink the size of KV cache. The\ncurrent k and v is then concatenated with KV cache from\nS preceding tokens, yielding K, V ∈R(S+N)× ˆ\nHD. The\nattention computation can then be formulated as follows:\nSh = qhKT\nˆh\n√\nD\n, oh = softmax (Sh) Vˆh, ˆh =\n\u0016h\nn\n\u0017\n(1)\nTherefore, the complexity of attention can be expressed\n\nLServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention\n3\n(b) Latency breakdown of LLM decoding\n8K\n16K\n32K\n64K\n128K\n0\n0.25\n0.5\n0.75\n1\nAttention\nOthers\nGEMM\nInput Length\n(a) Latency breakdown of LLM prefilling\n8K\n16K\n32K\n64K\n128K\n0\n0.25\n0.5\n0.75\n1\nAttention\nOthers\nGEMM\nInput Length\nFigure 2: Latency breakdown of LLM inference for both\nprefilling and decoding stage. As sequence length increases,\nattention dominates both stages due to its quadratic complex-\nity in prefilling stage and linear complexity during decoding\nstage. In contrast, GEMM exhibits linear complexity during\nprefilling stage and constant complexity during decoding\nstage. Latency numbers measured with Llama-3-8B on\nNVIDIA A100 GPU.\nas O (N(S + N)HD), which increases quadratically in\nthe prefilling stage and linearly in the decoding stage with\nrespect to sequence length. When S is long, both decoding\nstage and prefilling stage are bounded by attention.\nPaged Attention. In LLM serving, the generation length of\neach sequence is highly variable and unpredictable. Padding\nall sequences to the maximum length results in consider-\nable memory waste and fragmentation. To address this,\nvLLM (Kwon et al., 2023b) introduces PagedAttention, a\nKV cache management algorithm inspired by operating sys-\ntems’ virtual memory. Instead of allocating a continuous\nmemory buffer for each sequence’s KV cache, PagedAtten-\ntion partitions the cache into fixed-size blocks (or pages),\neach holding KV data for a set number of consecutive to-\nkens (typically 16 to 64). A page block table records the\nphysical address of each page, allowing the PagedAttention\nkernel to use indirect addressing to retrieve KV features.\nTensorRT-LLM (NVIDIA, 2023) and QServe (Lin et al.,\n2024b) implement quantized page attention to reduce mem-\nory bandwidth usage during the decoding stage, resulting in\nfurther generation speedups.\n2.2\nMotivation\nServing long-sequence LLMs is challenging due to the high\ncost of attention. Figure 2 profiles the latency breakdown of\nLlama-3-8B with a batch size of 1 across various sequence\nlengths on the A100 GPU. In both the prefilling and de-\ncoding stages, attention kernels account for at least 50% of\nthe runtime at sequence lengths over 64k, rising to 75% at\n128k. According to QServe (Lin et al., 2024b), the ratio\nof attention kernels in end-to-end runtime will increase as\nthe batch size scale up. Therefore, in real-world serving\nscenarios, optimizing the attention becomes increasingly\ncritical.\nAccelerating attention in long-sequence LLMs requires a\ndeep understanding of attention kernel implementation on\nGPUs, as illustrated in Figure 3. During the prefilling stage,\nQ0\nQ1\nQ2\nQ3\nQ4\nK0\nK1\nK2\nK3\nK4\nK0\nK1\nK2\nK3\nK4\nV0\nV1\nV2\nV3\nV4\nO0 O1 O2 O3 O4\nDense calculation\n(time=6)\nParallel (prefil.)  \nSequential (dec.)\nSequential\nK0\nK1\nK2\nK3\nK4\nK0\nK2\nK4\nV0\nV2\nV4\nO0 O2 O4\nBlock sparse calculation\n(time=4)\nQ4\nQ4\nSequential\nParallel (prefil.)  \nSequential (dec.)\nQ0\nQ1\nQ2\nQ3\nQ4\nFigure 3: Attention calculation on GPUs: In both the\ndecoding and prefilling stages, each query token iterates\nover all key and value tokens sequentially in a block-by-\nblock manner. Skipping KV blocks reduces the number of\nsequential iterations, directly accelerating attention.\nthe attention kernel is parallelized across batch size, atten-\ntion heads, and query tokens, with query tokens set to 1 in\nthe decoding stage. In both stages, the computation along\nthe KV token dimension remains sequential. In each iter-\nation, a block (depicted as a grid with an orange contour\nin Figure 3) is computed collaboratively by all threads in\nthe current thread block. Although skipping certain com-\nputation within each block is possible, it yields minimal\nspeedup. This is due to the lockstep execution of threads\nwithin a GPU warp, where faster threads are forced to wait\nfor slower ones.\nThat said, rather than focusing on sparsity within each it-\neration, a more effective way to accelerate attention is to\nreduce the number of sequential iterations along the KV\ntoken dimension. This approach leads to our unified block\nsparse attention formulation, where attention computation\nis skipped in a blockwise manner. In this scheme, aside\nfrom the most recent KV block, each block is either fully\ncomputed or entirely skipped during the prefilling stage.\nDuring decoding, each sequence contains only one query to-\nken, reducing the dimensionality of each orange-contoured\ngrid to 1×P, where P represents the page size (i.e., the\nnumber of KV tokens per page). We will detail LServe’s\nsparsity pattern selection in Section 3.\nAdditionally, because the decoding stage is memory-bound,\nKV cache quantization also contributes to speed improve-\nments. Quantization is orthogonal to block sparsity, as it\nreduces the runtime of each iteration, while sparsity reduces\nthe number of iterations.\n3\nLSERVE: LONG-SEQUENCE SERVING\nWITH UNIFIED SPARSE ATTENTION\nWe introduce LServe, an efficient long-sequence LLM serv-\ning system featuring sparse attention. In LServe, diverse\n\nLServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention\n4\nLocal Blocks\nSink Blocks\nContextual \nHistory\nSelected \nPages\n(a) Dense Attention\n(b) Block-Sparse Attention\n(c) Block-Sparse: Streaming Heads\n(d) Block-Sparse: Page Pruning\nFigure 4: Unified block sparse attention pattern. LServe\nintegrates various sparsity patterns into a unified framework.\nsparse attention patterns are unified within a block-sparse\nformulation (Figure 4), and are flexibly supported through\nfused CUDA kernels. LServe also supports weight, acti-\nvation and KV quantization, which significantly improves\ngeneration throughput at shorter context lengths.\n3.1\nUnified Block Sparse Attention\nAs shown in Figure 3, skipping computations in the atten-\ntion kernel by blockwise processing accelerates execution\nby shortening the sequential loop. Building on this, we\nintroduce a unified block sparse attention pattern for both\nthe prefilling and decoding stages: each thread block com-\nputes a TQ × TK tile (and TK × TV ) in parallel. Here,\nTQ > 1 in the prefilling stage and TQ = 1 in the decoding\nstage, with TK (or TV ) corresponding to the page size in\nPagedAttention (Kwon et al., 2023a).\nWe define block sparsity in LServe as follows: for each\nTQ × TK tile in the attention calculation, it is either fully\nskipped (Figure 4(b), light gray blocks) or retained as in\nstandard causal attention (Figure 4(b), blue blocks). Given\nthat each GPU streaming multiprocessor can execute only a\nlimited number of thread blocks simultaneously, the atten-\ntion kernel execution time can be approximated by the total\ncount of TQ × TK (and TK × TV ) blocks. With a block\nsparsity of r, where rN of the N total blocks are empty, the\ntheoretical speedup from block sparse attention is 1/(1 −r).\nFor example in Figure 4(b), 10 out of N=21 blocks are\nnon-empty. Thus, the theoretical speedup ratio is 2.1×.\nFigure 4(c)(d) shows two sparsity patterns used in LServe.\nThe first is streaming attention (Figure 4(c)), a specialized\nform of block-sparse attention where each token only at-\ntends to its immediate neighbors and initial tokens, known\nas attention sinks (Xiao et al., 2023). Unlike dense attention,\nwhere computation for each row scales with the token index,\nstreaming attention keeps the computation for each token\nconstant—in this case, only two local blocks and one sink\nblock, as shown in Figure 4(c). This pattern is nearly cost-\nfree in applications with extremely long contexts. Because\nstreaming attention follows a fixed pattern, we designate\nwhich heads use it in offline, and make it static for different\ninput sequences in both prefilling and decoding.\nThe second type of sparsity, illustrated in Figure 4(d), is\npage sparsity, which is specifically designed for the decod-\ning stage where TQ = 1 applies to both skipped and selected\npages. Unlike streaming attention, page sparsity in LServe\nis dynamic, allowing different query tokens to attend to dif-\nferent KV pages. As noted in Deja Vu (Liu et al., 2023),\ndynamic sparsity results in higher compression ratios than\nstatic sparsity. Our observations indicate that static sparsity\noffers up to a 2× efficiency gain, whereas dynamic sparsity\nbounds the decoding complexity to a constant, with each\nquery attending only to a fixed number of KV tokens.\n3.2\nLServe System Overview\nWe present an overview of LServe in Figure 5. Built on\nQServe, which natively supports quantized LLMs, LServe\nenhances the baseline system by introducing sparsity into\nboth prefilling and decoding dataflows. The two-way paged\nKV cache serves as the bridge between these two stages.\nAs discussed in Section 3.1, we statically partition the at-\ntention heads of a pretrained LLM into two groups: dense\nheads and streaming heads. Unlike conventional LLM serv-\ning systems, which maintain a single KV cache, we utilize\nseparate KV caches for the dense and streaming heads. The\nKV cache for the streaming heads is organized similarly to\nthe pages in QServe, with scaling factors and zero points\nstored immediately after the token features. Additionally,\nthe KV cache for the dense heads includes key statistics that\nfacilitate critical page selection during the decoding stage.\nIn the prefilling stage, the key differences between LServe\nand conventional dense-attention LLM serving systems are\ntwofold: (1) we replace the dense attention kernel with our\nunified block sparse attention kernel, and (2) we write back\nquantized KV features using two distinct kernels.\nIn the decoding stage, our system incorporates dynamic\nattention sparsity. Rather than developing an entirely new\ndynamic sparse attention kernel, we decompose the problem\ninto two components: (1) dynamic page selection and (2)\na dense attention kernel with shorter page tables, where\nthe shorter page tables are provided by the page selector.\nNotably, our page selector employs hierarchical paging and\nreusable page selection, enhancing both long-context accu-\nracy and page selection efficiency.\n\nLServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention\n5\nDense  \nHead Pages\nK_stats\n￼H × D\n￼B\n￼S\nLServe System\nQ\nK\nV\nFused Sparse Attention Kernel (Prefilling)\nDense Heads\nStreaming Heads\nStreaming \nHead Pages\n￼H × D\n￼B\n￼1\nQ\nK\nV\n   Output Projection\nFFN Layers\nDense Head\nStreaming Head\n￼H: Number of Heads\n￼B: Batch Size\n￼S: Context Length\n￼D: Head Dimension\nTo the next layer…\nPrefilling Dataflow\nTo the next layer…\nKey Statistics\nScales & Zeros\n￼D\nDecoding Dataflow\nPaged KV Cache\nPage Selector\nHierarchical \nPaging (￼3.5.2)\n§\nReusable \nSelector (￼3.5.3)\n§\nK_stats\nSelected \nPages\n   Output Projection\nFFN Layers\nDense Head \nPage Table\nUpdate \nKV\nSkipped \nPages\nFused Sparse Attention Kernel (Decoding)\n#0\n0x7A40\n#1\n0x7BC0\n#2\n0x7B00\n…\n…\nLogical \nBlock ID\nK & V \nPage Addr\nStreaming Head \nPage Table\n#0\n0x9C80 \n#1\n0xAD00 \n#N-1\n0xDFC0 \n#N\n0xD040 \nLogical \nBlock ID\nK & V \nPage Addr\nOnly Sink & \nLocal Pages\nFigure 5: LServe system overview. In prefilling stage, LServe processes both dense heads and streaming heads within a\nfused sparse attention kernel. Past Keys and Values are stored in two separate paging systems: one for streaming heads\nand the other for dense heads. In decoding stage, LServe applies dynamic sparsity on dense heads with a page selection\nprocedure. Only selected KV Pages will be loaded for the decoding stage attention. We omit normalization layers and\nresidual connections in this figure for the sake of simplicity.\n3.3\nPrefilling Stage: Sparsity Determination\nWe adopt the approach from DuoAttention (Xiao et al.,\n2024) to classify each attention head as either a re-\ntrieval head or a streaming head. Using DuoAttention’s\noptimization-based identification method, we obtain a gat-\ning value α ∈[0, 1] for each head, where values closer to\n1 signify a retrieval head, and values closer to 0 indicate a\nstreaming head. To classify a head as a retrieval head, we\ncompare α to a threshold τ, determined by a sparsity quan-\ntile. For instance, with a target sparsity of 50% across atten-\ntion heads, τ equals the median of all gate values, thereby\ndesignating half of the heads as retrieval heads.\n3.4\nPrefilling Stage: Kernel Implementation\nTo effectively translate sparsity into performance gains, it\nis essential to avoid iterating over a complete sequential\nloop and relying on conditional statements to determine\ndata loading and computation requirements. This method\nis inefficient for GPU computation patterns, which thrive\non minimizing branching within loops. Instead, we should\nfocus on iterating only over the necessary blocks by accu-\nrately calculating offsets to load data and assess whether a\nblock should be processed.\nTo facilitate this, we introduce an iterator-based abstraction\nthat standardizes indexing operations. This allows us to loop\nexclusively over the blocks requiring computation, with data\noffsets easily computed using offset = iter(i + 1) −iter(i).\nThis abstraction efficiently skips unnecessary blocks with\nminimal overhead and necessitates few changes to the kernel\nfunction, thus enhancing maintainability. Take the streaming\nheads as an example, the iterators are determined outside the\nattention kernel since streaming heads are configured offline\nand the attention pattern is fixed. Once the attention on sink\ntokens is complete, the iterator automatically updates the\nmemory pointer to the first local token in the KV cache\nwith minimal overhead. Additionally, our iterator-based\nformulation unifies the more general block sparse pattern\n(see Figure 4).\n3.5\nDecoding Stage: Sparsity Determination\nTo further enhance the long-context LLM decoding through-\nput, we introduce dynamic sparsity upon the input-agnostic\nstatic sparsity in Sec. 3.1.\n3.5.1\nChallenge: the Page Size Dilemma\nIn the decoding stage, the attention operation is memory-\nbound, so state-of-the-art systems typically implement KV\ncache quantization to reduce device memory usage and\nenhance throughput. However, this quantization introduces\nchallenges for further optimization. Specifically, reducing\nthe bit-width of KV tokens necessitates larger page sizes\nto maintain GPU memory bandwidth utilization. Failure to\ndo so can lead to significant throughput loss (Table 1). Yet,\n\nLServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention\n6\nAccuracy\n0.0\n0.2\n0.8\n1.0\nAccuracy\n0.0\n0.2\n0.8\n1.0\nAccuracy\n0.0\n0.2\n0.8\n1.0\n43K\n85K\n128K\n171K\n211K\n256K\n0K\n0\n22\n44\n67\n89\n43K\n85K\n128K\n171K\n211K\n256K\n0K\n(b) Page Size: 16, Token Budget: 4096\n0\n22\n44\n67\n89\n0\n22\n44\n67\n89\n43K\n85K\n128K\n171K\n211K\n256K\n0K\n43K\n85K\n128K\n171K\n211K\n256K\n0K\n(e) Page Size: 32, Token Budget: 8192\n43K\n85K\n128K\n171K\n211K\n256K\n0K\n0\n22\n44\n67\n89\n43K\n85K\n128K\n171K\n211K\n256K\n0K\n0\n22\n44\n67\n89\n(f) Page Size: 64, Token Budget: 16384\nDocument Depth (%)\n0\n22\n44\n67\n89\n43K\n85K\n128K\n171K\n211K\n256K\n0K\n(a) Dense Attention\n(c) Page Size: 32, Token Budget: 4096\n(d) Page Size: 64, Token Budget: 4096\nDocument Depth (%)\nDocument Depth (%)\nDocument Length\nDocument Length\nFigure 6: We evaluate the Llama-3-8B model with the\nNeedle-in-a-Haystack (NIAH) (Kamradt, 2024) bench-\nmarks. The effectiveness of query-aware page selection\nalgorithms (e.g., Quest (Tang et al., 2024)) gets impaired\nwhen the KV page granularity grows (b,c,d). Naively scal-\ning up the page sizes will lead to significant performance\nloss even if we linearly increase the number of selected\npages (token budget) (e,f).\nlarger KV page sizes complicate the sparsification process;\nfor example, Quest (Tang et al., 2024), which estimates\ntoken criticality using page-wise statistics, fails when page\nsizes increase (Figure 6). This observation poses challenges\nto balance between accuracy and efficiency.\n3.5.2\nHierarchical Paging: Mitigating the\naccuracy-efficiency tradeoff\nWe observe that the failure of query-aware KV cache selec-\ntion paradigm (Figure 6) is not due to the coarser granularity\nof sparse attention (i.e., larger page size). Rather, the under-\nlying cause lies in that page-wise statistical indicators be-\ncome homogenized and less representative especially when\nthere are excessive tokens within a single page. To address\nthis issue, we design a simple-yet-effective hierarchical pag-\ning system that introduces an abstract layer of virtual logical\npage for estimating token criticality, while preserving the\noriginal memory layout of KV cache in (physical pages).\nAs illustrated in Figure 7, our hierarchical paging groups\nNL tokens into a logical page and NP tokens into a phys-\nical page (NP = g · NL, g ∈Z), that is, a physical page\ncontains g logical pages. Tokens within the same logical\nTable 1: Page size significantly impacts the LLM serving\nsystem’s efficiency: Larger page size is more hardware-\nfriendly as it improves contiguity of memory layout and the\nGPU bandwidth utilization during attention computation.\nFor example, simply shrinking the page size in QServe (Lin\net al., 2024b) leads to prominent slow-down of the end-to-\nend system. We evaluate the per-step decoding latency (ms\n/ step) of QServe on a single A100 GPU for demonstration.\nWe use Llama3-8B model architecture, with the batch size\nof 32.\nSeq len\nPage Size\n16\n32\n64\n128\n512\n11.0 ms\n10.7 ms\n10.5 ms\n10.5 ms\n1024\n13.8 ms\n13.0 ms\n12.7 ms\n12.7 ms\n2048\n22.1 ms\n20.1 ms\n18.3 ms\n18.2 ms\n4096\n35.7 ms\n31.6 ms\n28.1 ms\n28.1 ms\n8192\n77.1 ms\n63.0 ms\n51.0 ms\n50.6 ms\nMax Slowdown\n1.52×\n1.25×\n1.01×\n1.00×\npage will collectively contribute to the same criticality es-\ntimator. In LServe, we utilize the channel-wise minimum\nand maximum values of keys in the same logical page as\nits representative vectors, which has been proven to be an\neffective metric (Tang et al., 2024) for page importance esti-\nmation with a moderate page size (≤16). The current query\nwill attend to representative vectors of each logical page to\ncalculate the corresponding importance score as follows:\nSj =\nD\nX\ni\nmax\n\u0010\nq[i] ∗kj\nmax[i], q[i] ∗kj\nmin[i]\n\u0011\n(2)\nwhere S is the importance score of logical page, j ∈\n{a, b, ...} is the index of logical page, i is the channel index,\nand D refers to the head dimension.\nThe importance of each physical page is determined by the\nmax-reduction over the importance scores of its correspond-\ning logical pages. Finally, LServe selects the top-K physical\npages (based on the predefined token budget) with highest\nimportance scores as the input of sparse attention kernel.\n3.5.3\nReducing sparse attention overheads with locality\nOne remaining question is: as physical page size increases,\nwill the hierarchical paging require a higher token budget\nfor sparse attention to retain accuracy?\nGiven a generation step, assume the most important history\ntokens are distributed in a logical page set P = {P(i, j)},\nwhere i ∈{1, 2, ...}, j ∈{a, b, , ...} are the physical and\nlogical index of a page accordingly. If these important to-\nkens are randomly and sparsely distributed in the context,\nchances are that all logical pages in P are scattered in dif-\nferent physical pages, that is, for any P1, P2 ∈P, i1 ̸= i2.\nIn this case, all |P| physical pages (|P| · NP tokens) are se-\nlected to avoid losing important information. However, the\n\nLServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention\n7\n3\n6\n8\n8\n-1\n6\n7\n8\n5\n4\n3\n8\n8\n6\n2\n4\n0\n-7\n6\n8\n-4\n6\n7\n8\n3\n0\n8\n6\n-2\n4\n2\n-4\n-5\n6\n3\n-8 -3\n4\n5\n1\n-8\n3\n3\n-8 -1\n0\n-1 -6\n0\n-3 -5\n8\n8\n3\n1\n4\n2\n-3\n3\n-7 -5 -5 -6 -1\n-2\n4\n1\n-6 -3\n6\n2\n-2\n5\n3\n2\n-3\n2\n-6 -5\n3\n∑max(q[i] ⋅kmax[i], q[i] ⋅kmin[i])\n37\n6\n22\nKeys\nLogical Page (a)\nLogical Page (b)\nkmax\nkmin\nkmax\nkmin\nK_stats\nPhysical Page 4\n3\n6\n8\n8\n-1\n6\n7\n8\n5\n4\n3\n8\n8\n6\n2\n4\n0\n-7\n6\n8\n-4\n6\n7\n8\n3\n0\n8\n6\n-2\n4\n2\n-4\n-5\n6\n3\n-8 -3\n4\n5\n1\n-8\n3\n3\n-8 -1\n0\n-1 -6\n0\n-3 -5\n8\n8\n3\n1\n4\n2\n-3\n3\n-7 -5 -5 -6 -1\n-2\n4\n1\n-6 -3\n6\n2\n-2\n5\n3\n2\n-3\n2\n-6 -5\n3\n∑max(q[i] ⋅kmax[i], q[i] ⋅kmin[i])\n37\n6\n67\nKeys\nLogical Page (b)\nkmax\nkmin\nkmax\nkmin\nK_stats\nPhysical Page 3\n6\n6\n8\n7\n8\n8\n7\n-1\n-7 -4 -7 -5 -5 -5 -8 -5\n-7 -4\n1\n-5 -5 -5 -8 -5\n2\n-3\n5\n7\n3\n7\n-2 -2\n6\n-3 -7\n1\n8\n-4\n7\n-2\n-2\n6\n8\n3\n-3\n8\n-8 -1\n-2\n7\n0\n3\n0\n3\n0\n-7\n2\n0\n-5\n4\n-5\n6\n2\n1\n-1\n8\n-8 -3\n4\n5\n1\n2\n-4\n0\n-3 -5\n1\n0\n6\n4\n2\n8\n0\n4\n4\n6\n6\n4\n-4\n0\n-8 -5 -5\n0\n0\n-7\n∑max(q[i] ⋅kmax[i], q[i] ⋅kmin[i])\n49\n78\n78\nKeys\nLogical Page (a)\nkmax\nkmin\nkmax\nkmin\nK_stats\nPhysical Page 2\nLogical Page (b)\n✅\n❌\n✅ Selected\n❌ Discarded\n1\n-2\n2\n-2\n1\n1\n1\n-3\nCurrent query:\nPhysical Page 1\n2\n3\n2\n8\n4\n5\n2\n6\n-6 -1 -6\n1\n-5 -6 -6 -2\n-1\n2\n-6\n8\n0\n-6\n0\n6\n2\n2\n-1\n1\n4\n4\n-2 -2\n-6\n3\n1\n6\n2\n1\n2\n-1\n-2 -1\n2\n8\n-5\n5\n-6\n1\n0\n-2\n3\n-3 -3\n1\n5\n0\n3\n-1 -2\n0\n2\n4\n5\n7\n2\n6\n-8\n4\n-3 -7\n2\n-1\n4\n5\n-4\n2\n-8\n2\n-3 -3\n4\n6\n3\n4\n2\n4\n5\n7\n0\n-2 -8 -3 -8 -7 -3 -3\n∑max(q[i] ⋅kmax[i], q[i] ⋅kmin[i])\n40\n23\nKeys\nkmax\nkmin\nkmax\nkmin\n40\nK_stats\nLogical Page (a)\nLogical Page (b)\n1\n-2\n2\n-2\n1\n1\n1\n-3\nCurrent query:\n✅\n❌\nHead_dim\nPage Size (#Tokens)\nPage Size (#Tokens)\nHead_dim\nFigure 7: Hierarchical Paging in LServe system. We assume\nthe each physical page contains Np = 8 tokens and each\nlogical page has Nl = 4 tokens. The kmax and kmin vectors\nare concatenated to the end of each physical page, and\nare pre-computed during the context stage and previous\ndecoding steps. The importance of each physical page is\ndecided by the max of the importance scores of the logical\npages it contains. We omitted KV quantization in this figure\nfor the sake of simplicity.\nna¨ıve paging only needs to keep |P| · NL tokens since it di-\nrectly shrinks page sizes to a smaller granularity (e.g., NL).\nConsequently, our hierarchical paging may suffer from a\ndecrease in attention sparsity by NP /NL.\nFortunately, the semantic continuity of natural language\nendows the attention operation with intrinsic locality, allow-\ning LServe to maintain a consistent sparse attention token\nbudget for larger physical page sizes. During the decoding\nstage, the coherence of contextual tokens makes the current\nquery token incline to attend to consecutive pages in the\nKV cache. As a result, logical pages with highest impor-\ntance scores tend to cluster within similar physical pages.\nThis kind of spatial locality effectively alleviates the need\nfor a increased token budget, thereby reducing the overhead\ncaused by the contradiction between quantization and sparse\nattention. Experimental results in Figure 13 further affirm\nthat our hierarchical paging well preserves the model accu-\nracy even with the same token budget as the vanilla page\nselector with smaller page sizes.\nMoreover, the attention mechanism in decoding stage also\nexhibits the temporal locality: adjacent query tokens also\nheavily attend to similar historical pages. And there is no\nneed for queries at consecutive decoding steps to select\nsalient pages independently. Instead, the page selection\ndecision can be shared across queries, aligning with the\nblock-sparse attention formulation illustrated in Figure 4(d).\nTo this end, we present Reusable Page Selection in LServe.\nSparse \nAttention\nPrevious Page \nSelection\nSparse \nAttention\nDynamic Page \nSelector\nSparse \nAttention\nT0\nPrevious Page \nSelection\n(a) Vanilla pipeline of dynamic sparse attention.   \n(b) Sparse attention with reusable page selector.   \nDynamic Page \nSelector\nSparse \nAttention\nDynamic Page \nSelector\nSparse \nAttention\nDynamic Page \nSelector\nSparse \nAttention\nT0 + 1\nT0 + 2\nT0 + 3\nDynamic Page \nSelector\nSparse \nAttention\nT0\nReuse\nSparse \nAttention\nT0 + 1\nT0 + 2\nT0 + 3\nDynamic Page \nSelector\nReuse\nFigure 8: We introduce Reusable Page Selector in LServe,\nwhich utilize the similarity of queries of consecutive to-\nkens to cut down the selector overhead. The chunk size of\nreusable selector is set to 2 in this figure for the demonstra-\ntion purpose.\nAs in Figure 8, we activate the page selector only at the\nvery beginning of pre-defined chunks. For the consecutive\ntokens within the same chunk, we reuse the page selection\nresults from the first token of the chunk. Utilizing the tem-\nporal sparsity of attention, reusable page selection substan-\ntially improves the long-context generation speed by a great\nmargin without sacrificing accuracy. As demonstrated in\nFigure 14, even though dynamic sparse attention effectively\nrestrict the complexity of decoding attention, the latency of\npage selector increases linearly with regard to the sequence\nlength. When the number of history tokens surpasses 64K,\nthe na¨ıve page selector becomes the bottleneck to system\nefficiency, whereas our reusable page selector significantly\nalleviates this problem.\n3.6\nDecoding Stage: Kernel Implementation\nDuring the decoding stage, attention heads are processed in\nparallel on GPU, enabling different sparsity patterns to be\napplied independently on each head. This flexibility enables\nsome heads to operate with page-level sparsity while others\nfollow the streaming computation pattern.\nTo leverage this, we employ a two-level indexing hierarchy\nto unify the operations for streaming heads and dense heads\nwith dynamic sparsity. Specifically, the low-level (physi-\ncal) index corresponds to the iteration step of current GPU\nthread, which executes in a consecutive manner as in dense\nattention, while logical index denotes the actual position\nof the target token within the entire KV cache. For each\ndense head, the page selector provides an index table to map\nphysical index to logical index. Streaming heads are treated\nas dynamic sparse heads with index table only containing\nthe sink and local pages.\n\nLServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention\n8\nTable 2: Accuracy evaluation on LongBench (Bai et al.,\n2023). We compare our method with vanilla dense attention\non 2 models and 10 different benchmarks.\nModel\nLlama-3-8B\nLlama-2-7B\nBenchmark\nDense\nLServe\nDense\nLServe\n2WikiMQA\n30.3\n31.6\n35.4\n35.1\nDuReader\n30.3\n30.8\n25.4\n24.7\nHotpotQA\n41.7\n42.7\n47.4\n49.6\nMultiNews\n27.7\n27.7\n26.6\n26.6\nQasper\n31.7\n29.3\n32.6\n29.5\nQMSum\n23.8\n24.0\n21.0\n21.3\nSamSum\n41.2\n39.3\n41.8\n41.5\nTriviaQA\n84.9\n83.7\n86.2\n86.5\nAverage\n38.9\n38.6\n39.5\n39.4\n43K\n85K\n128K\n171K\n211K\n256K\n0K\n0\n22\n44\n67\n89\n0\n22\n44\n67\n89\n(a) Llama-3-8B - Dense\nDocument Length\n43K\n85K\n128K\n171K\n211K\n256K\n0K\nAccuracy\n0.0\n0.2\n0.8\n1.0\n(b) Llama-3-8B - LServe\nDocument Length\nDocument Depth (%)\nFigure 9: Accuracy evaluation on Needle-in-a-Haystack.\n4\nEVALUATION\n4.1\nEvaluation Setup\nImplementation. We implement LServe in CUDA and PTX\nassembly on the basis of QServe (Lin et al., 2024b) and\nTensorRT-LLM (NVIDIA, 2023) system. The specialized\nCUDA kernels are compiled into PyTorch extensions for\nbetter flexibility and compatibility with the purely PyTorch-\nbased serving interface.\nTestbed. Our primary experiments are conducted on a server\nequipped with 8 NVIDIA A100 80GB GPUs, 2 AMD EPYC\n7763 CPUs (128 cores), and 2TB of memory. Unless ex-\nplicitly stated, all experiments utilize the A100 GPUs. Ad-\nditionally, we perform some evaluations on a cloud instance\nwith a single NVIDIA L40S 48GB GPU to assess system\nperformance across different GPU architectures. All evalua-\ntions use PyTorch 2.5.0 with CUDA 12.4 and cuDNN 9.2.0.\nModels. To comprehensively assess system performance\nacross various LLM architectures, we utilize the widely\nadopted GQA-based model Llama-3-8B (Dubey et al.,\n2024), the MHA-based model Llama-2-7B (Touvron et al.,\n2023), and the smaller-scale model Minitron-4B (Muralid-\nharan et al., 2024). Additionally, to support long-context\ninference, we employ the context-extended Llama-3-8B\nversion Gradient (Pekelis et al., 2024).\nMetrics. Our primary focus is on serving throughput. For\nTable 3: Accuracy evaluation on RULER (Hsieh et al.,\n2024). We evaluate the accuracy of Llama-3-8B on RULER\nbenchmarks, including challenging tasks such as multi-hop\ntracing and aggregation to test behaviors beyond searching\nfrom context. LServe-N denotes that the token budget for\ndynamic sparsity is N. Note that for long-context inputs,\nlatency is not dominated by attention alone in LServe, with\npage selector and GEMM also contributing to it. Experi-\nments reveal that LServe-8192 is only up to 6% slower than\nLServe-4096 when the sequence length exceeds 128K.\nLlama-3-8B\n32K\n64K\n128K\n160K\n192K\n256K\nDense\n90.5\n86.8\n83.8\n79.3\n79.6\n79.4\nLServe-4096\n91.0\n85.6\n81.0\n79.0\n76.1\n75.7\nLServe-8192\n91.8\n86.1\n81.7\n81.2\n79.7\n79.1\nthe prefilling stage, we use time-to-first-token (TTFT) as\na key metric, while for the decoding stage, we emphasize\nminimizing the per-token generation latency.\nBaselines. We consider the following systems as baselines,\nusing their latest versions1 to ensure a fair comparison. We\nactivated W8A8 precision for baselines if available.\n• vLLM (Kwon et al., 2023b), one of the most popular LLM\nserving systems featuring PagedAttention.\n• QServe (Lin et al., 2024b), efficient LLM serving system\nfeaturing W4A8KV4 quantization.\n• MInference (Jiang et al., 2024b), the state-of-the-art long-\ncontext prefilling stage acceleration system.\n• DuoAttention (Xiao et al., 2024), a strong long-sequence\nLLM inference framework with static sparse attention.\nAdditionally, we compare our approach with the state-of-the-\nart long-context decoding stage acceleration system, Quest\n(Tang et al., 2024). Since Quest only supports MHA models,\nwe conduct and discuss this comparison in Table 4.\n4.2\nEnd-to-end Accuracy\nWe evaluate the accuracy of our hybrid block-sparse mecha-\nnism with LongBench (Bai et al., 2023) tasks, the Needle-\nin-a-Haystack (NIAH) (Kamradt, 2024) pressure tests, as\nwell as the challenging RULER (Hsieh et al., 2024) bench-\nmarks. Table 2 compares the LongBench accuracy between\nLServe and dense baseline. Results show that LServe well\npreserves the performance of two models across different\ntest sets. Figure 9 showcases the NIAH evaluation results of\nour system, where LServe also achieves the same level of\naccuracy compared to the dense baseline. In Table 3, we test\nLServe with RULER benchmarks. Unless otherwise speci-\nfied, we convert half of the attention heads into streaming\nheads and keep token budget for dynamic sparsity to 4096\nfor the benchmarks.\n1vLLM 0.6.3\n\nLServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention\n9\n64K\n96K\n128K\n160K\n192K\n224K\n256K\n320K\nGeomean\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n0.69\n0.55\n0.60\n0.63\n0.61\n0.71\n0.78\n0.83\n0.92\n0.63\n0.26\n0.40\n0.47\n0.56\n0.77\n0.98\n1.08\n1.10\n0.50\n0.49\n0.50\n0.50\n0.51\n0.51\n0.52\n0.51\n0.48\n0.12\n0.09\n0.10\n0.11\n0.12\n0.13\n0.15\n0.16\n0.15\nMInference\nDuoAttention\nQServe\nvLLM\nLServe (Ours)\nA100  \nLlama-3-8B\n16K\n32K\n64K\n96K\n128K\n160K\n192K\n224K\nGeomean\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n0.48\n0.37\n0.41\n0.53\n0.64\n0.39\n0.20\n0.23\n0.31\n0.37\n0.47\n0.65\n0.83\n0.46\n0.47\n0.47\n0.48\n0.47\n0.47\n0.45\n0.43\n0.43\n0.05\n0.03\n0.04\n0.07\n0.12\nA100  \nLlama-2-7B\n64K\n96K\n128K\n160K\n192K\n224K\n256K\n512K\nGeomean\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n0.67\n0.47\n0.61\n0.63\n0.66\n0.71\n0.69\n0.81\n0.89\n0.57\n0.13\n0.42\n0.49\n0.58\n0.78\n0.95\n1.06\n1.09\n0.52\n0.49\n0.53\n0.53\n0.53\n0.53\n0.53\n0.52\n0.51\n0.06\n0.03\n0.04\n0.04\n0.05\n0.06\n0.07\n0.09\n0.10\nA100  \nMinitron-4B\n32K\n64K\n96K\n128K\n160K\n192K\n224K\n256K\nGeomean\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n0.75\n0.59\n0.62\n0.65\n0.68\n0.72\n0.77\n0.79\n0.88\n0.66\n0.39\n0.39\n0.53\n0.60\n0.68\n0.83\n0.40\n0.33\n0.34\n0.36\n0.37\n0.39\n0.41\n0.42\n0.44\n0.13\n0.09\n0.10\n0.12\n0.14\nL40S \nLlama-3-8B​​​​​​​​​\nOOM\nOOM\nOOM\nOOM\nOOM\nOOM\nOOM\nOOM\nOOM​​​​​​​​​\nOOM\nOOM\nOOM\nOOM\nOOM\nOOM\nFigure 10: Decoding Speed Evaluation. The y-axis indicates the relative throughput of each system, normalized by the\nspeed of LServe. Note that MInference exhibits limited decoding performance due to its unoptimized decoding stage with\ndense attention, but when integrated into vLLM, it can achieve throughput comparable to that of vLLM.\n64K\n96K\n128K\n192K\n256K\n320K\nGeomean\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n0.90\n1.00\n1.03\n0.96\n0.96\n0.82\n0.68\n0.65\n0.39\n0.53\n0.61\n0.84\n0.83\n0.83\n0.34\n0.20\n0.24\n0.29\n0.45\n0.49\n0.56\n0.60\n0.34\n0.48\n0.58\n0.73\n0.78\n0.86\nQServe\nvLLM\nDuoAttention\nMInference\nLServe (Ours)\nA100  \nLlama-3-8B\n16K\n32K\n64K\n96K\n128K\n160K\nGeomean\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n0.49\n0.92\n0.70\n0.36\n0.24\n0.79\n0.81\n0.79\n0.79\n0.76\n0.75\n0.81\n0.56\n0.43\n0.49\n0.63\n0.75\n0.81\n0.69\n0.72\n0.78\n0.85\n0.97\n0.90\nA100  \nLlama-2-7B​​​​​​​\nOOM\nOOM\nOOM\nOOM\nFigure 11: Prefilling Speed Evaluation. Performance com-\nparison of long-sequence prefilling across different serving\nframeworks, normalized to LServe’s speed.\n4.3\nEnd-to-end Efficiency\nDecoding Efficiency. Figure 10 presents the efficiency\nbenchmarking results for the decoding stage. We use the\nsame sparsity configurations as in Section 4.2. Compared\nwith the state-of-the-art serving systems, LServe demon-\nstrates significant and consistent efficiency improvements\nacross different GPU platforms and model architectures.\nOn Llama-3-8B and Minitron-4B, LServe achieves 1.5×\naverage speedup over vLLM. For MHA-based model Llama-\n2-7B, LServe runs more than 2.0× faster than baselines on\naverage. Additionally, we demonstrate that LServe also\nfunctions well on other GPU devices such as L40S with\nAda Lovelace Architecture. LServe achieves up to 1.7×\nspeedup over vLLM.\nPrefilling Efficiency. In Figure 11, we compare the prefill-\ning speed of LServe against 4 baselines on Llama-3-8B and\nTable 4: LServe achieves lower latency over Quest (Tang\net al., 2024) system in both prefilling stage and decoding\nstage. We benchmark the two systems on Llama-2-7B\nmodel, since Quest does not support GQA (Ainslie et al.,\n2023) architecture.\nStage\nSystem\nSequence Length\n4K\n8K\n16K\n32K\n64K\nPrefilling\nLatency (s)\nQuest\n0.51\n0.82\n1.62\n3.61\nOOM\nLServe\n0.24\n0.49\n1.08\n2.32\n5.27\nSpeedup\n2.1 ×\n1.7×\n1.5×\n1.6×\n/\nDecoding\nLatency (ms)\nQuest\n13.13\n13.58\n14.08\n14.86\nOOM\nLServe\n10.02\n10.29\n10.22\n10.24\n11.54\nSpeedup\n1.3×\n1.3×\n1.4×\n1.5×\n/\nLlama-2-7B. LServe maintains superior prefilling through-\nput across different sequence lengths. For instance, on\nLlama-2-7B, LServe achieves an average of 1.8× higher\nprefilling throughput over vLLM. LServe is also compatible\nwith the prefilling dynamic sparsity in MInference, which\nwe activated after 128K sequence length.\n4.4\nEnd-to-End Comparison with Quest\nWe also compares our system against Quest (Tang et al.,\n2024) in Table 4. Across different sequence lengths, LServe\nconsistently outperforms Quest in both prefilling (1.6-2.1×\nspeedup) and decoding stages (1.3-1.5× speedup).\n\nLServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention\n10\n0\n10\n20\n30\n40\n50\n40\n50\n60\n70\n80\n90\n4.2\n8.5\n12.7\n16.9\n21.2\n25.4\n5.5\n10.8\n15.9\n20.3\n25.3\n30.0\n7.1\n14.3\n20.9\n27.3\n34.1\n40.4\nMInference\nLServe\nOracle\nLatency (ms)\nSparsity Level (%)\nDense Attention: 42.3 ms\n1.3 x\n1.3 x\nFigure 12: Prefilling Stage Attention Kernel Evaluation.\n43K\n85K\n128K\n171K\n211K\n256K\n0K\nDocument Depth (%)\n0\n22\n44\n67\n89\n43K\n85K\n128K\n171K\n211K\n256K\n0K\n43K\n85K\n128K\n171K\n211K\n256K\n0K\n0\n22\n44\n67\n89\n43K\n85K\n128K\n171K\n211K\n256K\n0K\n0\n22\n44\n67\n89\nAccuracy\n0.0\n0.2\n0.8\n1.0\n(a) \n,  Budget: 3072\nNP = 16, NL = 16\n(b) \n,  Budget: 3072\nNP = 32, NL = 16\n(c) \n,  Budget: 3072\nNP = 64, NL = 16\nDocument Length\nDocument Length\nDocument Length\n Physical Page Size,   \n Logical Page Size\nNP :\nNL :\nFigure 13: Hierarchical paging enables LServe to preserve\nthe long-context retrieval capabilities of the original model\nwithout increasing the key-value (KV) token budget. We\nuse Llama-3-8B for the ablation.\n5\nANALYSIS\nIn this section, we present in-depth analysis on our design\nchoices in the LServe system from both the accuracy and\nthe efficiency perspective. We also scrutinize the sources of\nperformance gains in Section 4.\n5.1\nPrefilling Stage Sparse Attention Kernel\nWe benchmark the performance of our block sparse attention\nkernel for the prefilling stage in Figure 12. Compared with\nthe implementation by MInference (Jiang et al., 2024b),\nour kernel consistently achieves 1.3× speedup at the same\nsparsity level. Oracle stands for the theoretical upper-bound\nspeedup ratio: Latencyoracle = Latencydense ∗(1 −sparsity).\n5.2\nEffectiveness of Hierarchical Paging\nWe use the Needle-in-a-Haystack (Kamradt, 2024) test to\ndemonstrate that the hierarchical paging design effectively\nmaintains the model’s long-context capability on larger page\nblocks without increasing the token budget. In contrast to\nthe performance drop observed with increased page gran-\nularity in Figure 6, LServe leverages a hierarchical page\nstructure to decouple the pruning algorithm’s page granular-\nity from the physical memory layout of the KV cache. This\napproach enables our sparse attention mechanism to remain\nboth accurate and hardware-efficient. Figure 13 highlights\nthis improvement: with a page size of 64 and the same to-\nken budget, LServe achieves accuracy comparable to the\nbaseline algorithm (Tang et al., 2024), which prunes history\ntokens at a granularity of 16.\n0.00\n0.15\n0.30\n0.45\n0.60\n8K\n16K\n32K\n64K\n128K\n256K\n(b) Reusable Page Selector \nSelector shared \nacross 4 queries\n0.00\n0.15\n0.30\n0.45\n0.60\n8K\n16K\n32K\n64K\n128K\n256K\nDynamic Page Selector\nSparse Attention\n(a) Vanilla Page Selector \nLatency (ms) \nFigure 14: Effect of Reusable Page Selection. The over-\nhead of the dynamic page selector is significant, as its com-\nplexity increases linearly with input sequence length. Our\nReusable Page Selection effectively mitigates this issue. The\nlatency breakdown is evaluated on Llama-3-8B.\nTable 5: The reusable page selector in LServe preserves the\nmodel’s long-context accuracy while significantly reducing\nselection overhead by 4× with a reuse interval of 4. We\nevaluate Llama-3-8B on RULER (Hsieh et al., 2024) at a\nsequence length of 64K. LServe-N denotes that the token\nbudget for dynamic sparsity is N.\nReuse Interval\nDense\n1\n2\n4\n8\n16\nLServe-4096\n86.8\n86.2\n85.6\n85.6\n84.8\n83.2\nLServe-8192\n86.8\n86.1\n85.8\n85.5\n85.6\n84.8\n5.3\nMitigating Page Selection Overhead\nReusable Page Selection.\nDuring decoding, although the\nattention kernel maintains constant complexity due to a\ncapped number of historical KV tokens, the complexity of\nthe page selector still scales linearly with sequence length.\nAs illustrated in Figure 14, for a sequence length of 128K\nand a 4K token budget for sparse attention, the page selector\n(0.24 ms) is already twice as slow as the sparse attention\nkernel (0.12 ms). With our reusable page selector, however,\nLServe significantly reduces page selection overhead by a\nfactor of C, where C is the reuse interval. We further show\nthat LServe is resilient to different reuse interval choices.\nTable 5 demonstrates no significant performance degrada-\ntion until the reuse interval exceeds 8, so we set it to 4 by\ndefault in LServe.\nContext Pooling Overhead.\nTo enable page selection\nduring decoding, we must calculate representative features\nusing min-max pooling in the prefilling stage. It is important\nto note that a single pooling kernel executes under 1 ms,\nwhile the entire prefilling stage completes in approximately\n17 seconds with 128K context length. Consequently, the\ncontext pooling overhead is negligible.\n\nLServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention\n11\n0\n250\n500\n750\n1000\n4K\n8K\n16K\n32K\n64K\n128K\n256K\n82\n82\n82\n81\n82\n82\n81\n118\n117\n116\n118\n117\n117\n118\n2,052\n748\n384\n203\n124\n71\n68\n3,492\n1,238\n545\n306\n183\n120\n87\nBaseline Attention\n+Static Sparsity Only (50%)\n+Dynamic Sparsity Only (4K budget)\nLServe Attention\n3,492 2,052\nLatency (us)\nSequence Length (#Tokens)\nFigure 15: Efficiency gains from static and dynamic spar-\nsity in LServe. These sparsity patterns contribute to a\ncompound speedup effect, with static sparsity being more\neffective at shorter contexts, and dynamic sparsity offering\ngreater benefits at longer contexts. We report the latency of\na single attention layer in Llama-2-7B.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n4K\n8K\n16K\n32K\n64K\n128K\n256K\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n0.94\n0.89\n0.22\n0.47\n0.61\n0.79\n0.97\n1.00\n1.00\n0.13\n0.35\n0.49\n0.68\n0.86\n0.94\n0.97\nDense Attention\n+50% Streaming Heads\n+Dynamic Sparsity (4K budget)\nLServe\nNVIDIA A100 GPU  \nNormalized Throughput\nInput Length (#Tokens)\nFigure 16: End-to-end speedup breakdown in LServe:\nConsistent with findings from attention layer analysis, static\nsparsity (50% streaming heads) yields greater benefits at\nshorter context lengths.\nIn contrast, dynamic sparsity\nachieves up to 4.5× end-to-end speedup for longer se-\nquences. Results are based on measurements using Llama-\n3-8B with unit batch size.\n5.4\nSparse Attention Kernel for Decoding Stage\nWe analyze the effectiveness of different sparsity patterns\nin decoding attention. In Figure 15, we apply static sparsity\nby converting 50% of attention heads to streaming heads,\nachieving a 1.3-1.7× speedup across various input sequence\nlengths. Additionally, we introduce dynamic sparsity with\na fixed KV budget of 4096 tokens, which bounds the com-\nputational complexity of decoding attention to a constant,\ndelivering a 30× speedup over the dense baseline for an in-\nput length of 256K. Although sparsity selection introduces\nminor overhead for shorter sequences, this is mitigated by\nreusable page selection. Additionally, we also perform the\nend-to-end ablation study in Section 5.5.\n5.5\nEnd-to-End Speedup Breakdown\nIn Figure 16, we highlight the sources of performance im-\nprovement in LServe. By leveraging static sparsity, LServe\nachieves end-to-end speedups of up to 1.7× over the dense\nbaseline. Additionally, dynamic sparsity, aided by a reusable\npage selector, significantly reduces generation latency, yield-\ning a 7.7× speedup for sequence lengths of 256K. Lastly,\nLServe configures sparse patterns through offline profiling,\neffectively avoiding slowdowns from dynamic sparsity at\nshorter context lengths.\n6\nRELATED WORK\nLLM Serving Systems. Various systems have been devel-\noped to enhance LLM deployment efficiency. Orca (Yu et al.,\n2022) uses iteration-level scheduling and selective batch-\ning for distributed systems. vLLM (Kwon et al., 2023b)\nintroduces PagedAttention, inspired by virtual memory, to\noptimize KV cache management. TensorRT-LLM (NVIDIA,\n2023) is the industry’s leading solution also featuring in-\nflight batching and PagedAttention inspired by vLLM.\nLightLLM (Contributors, 2023a) further reduces mem-\nory waste in PagedAttention by introducing TokenAtten-\ntion. SGLang (Zheng et al., 2023) advances LLM pro-\ngramming with a domain-specific language and RadixAt-\ntention. LMDeploy (Contributors, 2023b) improves de-\nployment with persistent batching and blocked KV cache.\nNanoflow (Zhu et al., 2024) features intra-device scheduling\nand asynchronous CPU scheduling, while QServe (Lin et al.,\n2024b) improves LLM serving throughput through W4A8KV4\nquantization and system codesign. MLC-LLM (team, 2023)\naccelerates deployment on edge devices via compiler-based\noptimizations. Inspired by contextual sparsity (Liu et al.,\n2023), PowerInfer (Song et al., 2023; Xue et al., 2024b) de-\nploys LLMs on memory-constrained devices via offloading.\nSparse Attention.\nBigBird (Zaheer et al., 2020) re-\nduces attention complexity by blending local, global,\nand random attention masks. Subsequent methods like\nStreamingLLM (Xiao et al., 2023), H2O (Zhang et al.,\n2024c), and TOVA (Oren et al., 2024) simplify attention pat-\nterns by discarding KV caches mid-way through the context.\nHowever, these approaches struggle to retain the original\nmodels’ long-context capabilities due to limited global con-\ntext modeling. Recent works like DuoAttention (Xiao et al.,\n2024), RetrievalAttention (Liu et al., 2024a), and SeerAt-\ntention (Gao et al., 2024) address this issue by introducing\nretrieval heads (Wu et al., 2024) or combining full atten-\ntion with local attention heads. Quest (Tang et al., 2024)\nintroduces dynamic, query-aware sparsity for accelerated\ndecoding, while MInference (Jiang et al., 2024b) extends\nsimilar ideas to the prefilling stage. FastGen (Ge et al., 2023)\noptimizes decoding by profiling attention heads to discard\ntokens. PQCache (Zhang et al., 2024a) and ShadowKV (Sun\net al., 2024) further advance the selective attention meth-\nods with product quantization and low-rank decomposition.\nAdditionally, LongLoRA (Chen et al., 2023) finetunes short-\ncontext LLMs to long-context ones after converting global\nattention to shifted sparse attention.\n7\nCONCLUSION\nWe introduce LServe, an efficient serving system for long-\nsequence LLMs that leverages hybrid sparse attention. By\nincorporating unified block sparse attention, we achieve sig-\nnificant acceleration of the attention mechanism for both\n\nLServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention\n12\nprefilling and decoding stages in long-sequence models. We\nfurther show that head-level static sparsity and query-aware\ndynamic sparsity are orthogonal and can be effectively com-\nbined with minimal impact on accuracy. LServe surpasses\nstate-of-the-art systems, delivering an average of 1.3×-2.1×\nspeedup in the decoding stage and up to 2.9× speedup in\nthe prefilling stage, preserving the models’ long-context\ncapabilities.\nACKNOWLEDGEMENTS\nWe thank MIT-IBM Watson AI Lab, MIT AI Hardware Pro-\ngram, MIT Amazon Science Hub, National Science Foun-\ndation, and Hyundai for supporting this research. We also\nthank June Yang, Bo Li, and Kaiyu Xie for their helpful\ndiscussions.\nREFERENCES\nAinslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y.,\nLebr´on, F., and Sanghai, S. Gqa: Training generalized\nmulti-query transformer models from multi-head check-\npoints. arXiv preprint arXiv:2305.13245, 2023.\nBai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du,\nZ., Liu, X., Zeng, A., Hou, L., Dong, Y., Tang, J., and Li,\nJ. Longbench: A bilingual, multitask benchmark for long\ncontext understanding. arXiv preprint arXiv:2308.14508,\n2023.\nBrandon, W., Mishra, M., Nrusimha, A., Panda, R.,\nand Kelly, J. R.\nReducing transformer key-value\ncache size with cross-layer attention.\narXiv preprint\narXiv:2405.12981, 2024.\nChen, Y., Qian, S., Tang, H., Lai, X., Liu, Z., Han, S., and\nJia, J. Longlora: Efficient fine-tuning of long-context\nlarge language models. arXiv preprint arXiv:2309.12307,\n2023.\nContributors, L. Lightllm: A light and fast inference service\nfor llm. https://github.com/ModelTC/lightllm,\n2023a.\nContributors, L. Lmdeploy: A toolkit for compressing,\ndeploying, and serving llm. https://github.com/Int\nernLM/lmdeploy, 2023b.\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle,\nA., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan,\nA., et al. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783, 2024.\nGao, Y., Zeng, Z., Du, D., Cao, S., So, H. K.-H., Cao,\nT., Yang, F., and Yang, M.\nSeerattention: Learning\nintrinsic sparse attention in your llms. arXiv preprint\narXiv:2410.13276, 2024.\nGe, S., Zhang, Y., Liu, L., Zhang, M., Han, J., and Gao,\nJ. Model tells you what to discard: Adaptive kv cache\ncompression for llms. arXiv preprint arXiv:2310.01801,\n2023.\nGemini Team, Google. Gemini 1.5: Unlocking multimodal\nunderstanding across millions of tokens of context. arXiv\npreprint arXiv:2403.05530, 2024.\nGoyal, T. and Durrett, G. Evaluating factuality in genera-\ntion with dependency-level entailment. arXiv preprint\narXiv:2010.05478, 2020.\nHooper, C., Kim, S., Mohammadzadeh, H., Mahoney,\nM. W., Shao, Y. S., Keutzer, K., and Gholami, A.\nKvquant: Towards 10 million context length llm in-\nference with kv cache quantization.\narXiv preprint\narXiv:2401.18079, 2024.\n\nLServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention\n13\nHsieh, C.-P., Sun, S., Kriman, S., Acharya, S., Rekesh, D.,\nJia, F., Zhang, Y., and Ginsburg, B. Ruler: What’s the\nreal context size of your long-context language models?\narXiv preprint arXiv:2404.06654, 2024.\nHuang, L., Cao, S., Parulian, N., Ji, H., and Wang, L. Effi-\ncient attentions for long document summarization. arXiv\npreprint arXiv:2104.02112, 2021.\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\nChaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G.,\nLample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint\narXiv:2310.06825, 2023.\nJiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary,\nB., Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna,\nE. B., Bressand, F., et al. Mixtral of experts. arXiv\npreprint arXiv:2401.04088, 2024a.\nJiang, H., Li, Y., Zhang, C., Wu, Q., Luo, X., Ahn, S., Han,\nZ., Abdi, A. H., Li, D., Lin, C.-Y., et al. Minference\n1.0: Accelerating pre-filling for long-context llms via dy-\nnamic sparse attention. arXiv preprint arXiv:2407.02490,\n2024b.\nKamradt, G. Llmtest needleinahaystack: Doing simple\nretrieval from llm models at various context lengths to\nmeasure accuracy. https://github.com/gkamradt/LL\nMTest NeedleInAHaystack, 2024. Accessed: 2024-05-\n23.\nKwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu,\nC. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient\nmemory management for large language model serving\nwith pagedattention. In Proceedings of the 29th Sym-\nposium on Operating Systems Principles, pp. 611–626,\n2023a.\nKwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu,\nC. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient\nmemory management for large language model serving\nwith pagedattention. In Proceedings of the ACM SIGOPS\n29th Symposium on Operating Systems Principles, 2023b.\nLi, R., Allal, L. B., Zi, Y., Muennighoff, N., Kocetkov, D.,\nMou, C., Marone, M., Akiki, C., Li, J., Chim, J., et al.\nStarcoder: may the source be with you! arXiv preprint\narXiv:2305.06161, 2023.\nLin, J., Yin, H., Ping, W., Molchanov, P., Shoeybi, M.,\nand Han, S. Vila: On pre-training for visual language\nmodels. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 26689–\n26699, 2024a.\nLin, Y., Tang, H., Yang, S., Zhang, Z., Xiao, G., Gan, C.,\nand Han, S. Qserve: W4a8kv4 quantization and sys-\ntem co-design for efficient llm serving. arXiv preprint\narXiv:2405.04532, 2024b.\nLiu, D., Chen, M., Lu, B., Jiang, H., Han, Z., Zhang, Q.,\nChen, Q., Zhang, C., Ding, B., Zhang, K., et al. Re-\ntrievalattention: Accelerating long-context llm inference\nvia vector retrieval. arXiv preprint arXiv:2409.10516,\n2024a.\nLiu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tun-\ning. Advances in neural information processing systems,\n36, 2024b.\nLiu, Z., Wang, J., Dao, T., Zhou, T., Yuan, B., Song, Z.,\nShrivastava, A., Zhang, C., Tian, Y., Re, C., et al. Deja\nvu: Contextual sparsity for efficient llms at inference time.\nIn International Conference on Machine Learning, pp.\n22137–22176. PMLR, 2023.\nLiu, Z., Yuan, J., Jin, H., Zhong, S., Xu, Z., Braverman,\nV., Chen, B., and Hu, X. Kivi: A tuning-free asym-\nmetric 2bit quantization for kv cache. arXiv preprint\narXiv:2402.02750, 2024c.\nLozhkov, A., Li, R., Allal, L. B., Cassano, F., Lamy-Poirier,\nJ., Tazi, N., Tang, A., Pykhtar, D., Liu, J., Wei, Y., et al.\nStarcoder 2 and the stack v2: The next generation. arXiv\npreprint arXiv:2402.19173, 2024.\nMuralidharan, S., Sreenivas, S. T., Joshi, R., Chochowski,\nM., Patwary, M., Shoeybi, M., Catanzaro, B., Kautz, J.,\nand Molchanov, P. Compact language models via pruning\nand knowledge distillation. CoRR, format/2407.14679,\n2024. URL https://arxiv.org/abs/2407.14679.\nNVIDIA. TensorRT-LLM: A TensorRT Toolbox for Op-\ntimized Large Language Model Inference, 2023. URL\nhttps://github.com/NVIDIA/TensorRT-LLM.\nOpenAI. Introducing openai o1, 2024. URL https://op\nenai.com/o1/.\nOren, M., Hassid, M., Adi, Y., and Schwartz, R. Transform-\ners are multi-state rnns. arXiv preprint arXiv:2401.06104,\n2024.\nPekelis, L., Feil, M., Moret, F., Huang, M., and Peng, T.\nLlama 3 gradient: A series of long context models, 2024.\nURL https://gradient.ai/blog/scaling-rotatio\nnal-embeddings-for-long-context-language-mod\nels.\nQin, Y., Li, X., Zou, H., Liu, Y., Xia, S., Huang, Z., Ye,\nY., Yuan, W., Liu, H., Li, Y., and Liu, P. O1 Replication\nJourney: A Strategic Progress Report – Part 1 . arXiv\npreprint arXiv:2410.18982, 2024.\nSong, Y., Mi, Z., Xie, H., and Chen, H. Powerinfer: Fast\nlarge language model serving with a consumer-grade gpu.\narXiv preprint arXiv:2312.12456, 2023.\n\nLServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention\n14\nSun, H., Chang, L.-W., Bao, W., Zheng, S., Zheng, N., Liu,\nX., Dong, H., Chi, Y., and Chen, B. Shadowkv: Kv\ncache in shadows for high-throughput long-context llm\ninference. arXiv preprint arXiv:2410.21465, 2024.\nTang, J., Zhao, Y., Zhu, K., Xiao, G., Kasikci, B., and Han,\nS. Quest: Query-aware sparsity for efficient long-context\nllm inference. arXiv preprint arXiv:2406.10774, 2024.\nteam, M. MLC-LLM, 2023. URL https://github.com\n/mlc-ai/mlc-llm.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., et al. Llama 2: Open foundation and fine-\ntuned chat models. arXiv preprint arXiv:2307.09288,\n2023.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi,\nE., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting\nelicits reasoning in large language models. Advances in\nneural information processing systems, 35:24824–24837,\n2022.\nWu, W., Wang, Y., Xiao, G., Peng, H., and Fu, Y. Re-\ntrieval head mechanistically explains long-context factu-\nality. arXiv preprint arXiv:2404.15574, 2024.\nXiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Ef-\nficient streaming language models with attention sinks.\narXiv preprint arXiv:2309.17453, 2023.\nXiao, G., Tang, J., Zuo, J., Guo, J., Yang, S., Tang, H.,\nFu, Y., and Han, S. Duoattention: Efficient long-context\nllm inference with retrieval and streaming heads. arXiv\npreprint arXiv:2410.10819, 2024.\nXue, F., Chen, Y., Li, D., Hu, Q., Zhu, L., Li, X., Fang, Y.,\nTang, H., Yang, S., Liu, Z., et al. Longvila: Scaling long-\ncontext visual language models for long videos. arXiv\npreprint arXiv:2408.10188, 2024a.\nXue, Z., Song, Y., Mi, Z., Chen, L., Xia, Y., and Chen, H.\nPowerinfer-2: Fast large language model inference on a\nsmartphone. arXiv preprint arXiv:2406.06282, 2024b.\nYu, G.-I., Jeong, J. S., Kim, G.-W., Kim, S., and Chun, B.-\nG. Orca: A distributed serving system for Transformer-\nBased generative models. In 16th USENIX Symposium\non Operating Systems Design and Implementation (OSDI\n22), pp. 521–538, Carlsbad, CA, July 2022. USENIX\nAssociation. ISBN 978-1-939133-28-1. URL https:\n//www.usenix.org/conference/osdi22/presentat\nion/yu.\nZaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Al-\nberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q.,\nYang, L., et al. Big bird: Transformers for longer se-\nquences.\nAdvances in neural information processing\nsystems, 33:17283–17297, 2020.\nZhang, H., Ji, X., Chen, Y., Fu, F., Miao, X., Nie, X., Chen,\nW., and Cui, B. Pqcache: Product quantization-based\nkvcache for long context llm inference. arXiv preprint\narXiv:2407.12820, 2024a.\nZhang, T., Ladhak, F., Durmus, E., Liang, P., McKeown,\nK., and Hashimoto, T. B. Benchmarking large language\nmodels for news summarization. Transactions of the Asso-\nciation for Computational Linguistics, 12:39–57, 2024b.\nZhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai,\nR., Song, Z., Tian, Y., R´e, C., Barrett, C., et al. H2o:\nHeavy-hitter oracle for efficient generative inference of\nlarge language models. Advances in Neural Information\nProcessing Systems, 36, 2024c.\nZheng, L., Yin, L., Xie, Z., Huang, J., Sun, C., Yu, C. H.,\nCao, S., Kozyrakis, C., Stoica, I., Gonzalez, J. E., Barrett,\nC., and Sheng, Y. Efficiently programming large language\nmodels using sglang, 2023.\nZhu, K., Zhao, Y., Zhao, L., Zuo, G., Gu, Y., Xie, D., Gao,\nY., Xu, Q., Tang, T., Ye, Z., et al. Nanoflow: Towards\noptimal large language model serving throughput. arXiv\npreprint arXiv:2408.12757, 2024.\n",
  "metadata": {
    "source_path": "papers/arxiv/LServe_Efficient_Long-sequence_LLM_Serving_with_Unified_Sparse\n__Attention_7fd182cfa0337738.pdf",
    "content_hash": "7fd182cfa033773802ce2209ad40ab60226f6a3ddc62da62104e45a9fe3896b9",
    "arxiv_id": null,
    "title": "LServe: Efficient Long-sequence LLM Serving with  Unified Sparse Attention",
    "author": "Shang Yang, Junxian Guo, Haotian Tang, Qinghao Hu, Guangxuan Xiao, Jiaming Tang, Yujun Lin, Zhijian Liu, Yao Lu, Song Han",
    "creation_date": "D:20250221020531Z",
    "published": "2025-02-21T02:05:31",
    "pages": 14,
    "size": 4434068,
    "file_mtime": 1740346980.5616522
  }
}