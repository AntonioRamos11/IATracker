{
  "text": "ISCA/ITG Workshop on Diversity in Large Speech and Language Models\nChild vs. machine language learning:\nCan the logical structure of human language unleash LLMs?\nUli Sauerland\nZAS\nsauerland@leibniz-zas.de\nCelia Matthaei\nZAS & HU Berlin\ncelia.matthaei@gmail.com\nFelix Salfner\nindependent researcher\nresearch@felix.salfner.de\nAbstract\nWe argue that human language learning pro-\nceeds in a manner that is different in nature\nfrom current approaches to training LLMs, pre-\ndicting a difference in learning biases. We then\npresent evidence from German plural forma-\ntion by LLMs that confirm our hypothesis that\neven very powerful implementations produce\nresults that miss aspects of the logic inherent\nto language that humans have no problem with.\nWe conclude that attention to the different struc-\ntures of human language and artificial neural\nnetworks is likely to be an avenue to improve\nLLM performance.\n1\nIntroduction\nResults from how humans learn language suggest\nthat logical connections drive learning strongly. In\nparticular, Sauerland et al. (2024) point out that\nchildren learning language are spontaneously cre-\nating the negation of an output. Their example is\nbased on the German prepositions mit (‘with’) and\nohne (‘without’). Like their English translations\nthe two are logical contraries, but unlike in English\nthis is not morphologically transparent via the mor-\nphological complexity of with-out. The surprising\nobservation is that children learning German spon-\ntaneously and frequently produce sequences mit\nohne (lit. ‘with with-out’), that are not fully gram-\nmatical and exceedingly rare in the adult language.\nThis finding argues that after learning to output A\nin some circumstances, the negative output ¯A ‘not\nA’ is readily available to the human learner and\nGerman children therefore are drive to produce the\ninverse of mit as two words, mit and a marker of\nnegation. Other findings from child language ac-\nquisition (Cesana-Arlotti et al., 2018; Guasti et al.,\n2023) support this conclusion, and recent evidence\nshows that other primates also use negation in con-\ncept representation (Dautriche et al., 2022).\nIn contrast to the conceptual representation\npresent in humans, neural networks rely heavily\na.\na\nA\nb.\na\nA\nā\nĀ\nc.\na\nA\nĀ\n=\n¬\nFigure 1: Implementation of two ways of adding the\nnegation of A in a neural network. In panel a, neuron\na yields output A. Panel b and c illustrate two possible\nways of obtaining output ¯A whenever A is not outputted.\nIn panel b, an independent neuron ¯a with inverse weights\noutput ¯A. In c, two new neurons = and ¬ trigger A and\n¯A depending on the output of unit a.\non independently inferring concepts from training\ndata. LLM architecture does not have a bias for\nspecific algebraic or logical relationships such as\nnegation of A and ¯A. This predicts that learning\nlogical negation of A will not be straightforward\nand we expect LLMs to not prefer learning logical\nconnections over other generalizations. As an ex-\nample, to model the logical operation of negation\nof output A produced by neuron a, a second neuron\n¯a could be added where the two neurons’ weights\nare inversely linked, i.e., shared (wa = −wa), as\nis depicted in Figure 1-b. Another approach would\nbe to add a layer to the network by moving unit a\nto the penultimate layer and add units = and ¬ that\nreceive the output of a where ¬ has an inverse acti-\nvation function to yield A and ¯A (see Figure 1-c).\nBut both of these ways involve substantial changes\nto the network architecture, and we therefore pre-\ndict that an LLM should require a lot of learning\ndata to learn the complementary distribution of A\nand ¯A. We conducted experiments that corroborate\nthis prediction, and which are presented below.\nCurrent approaches rely on reinforcement learn-\ning with a set of mathematical and logical reason-\ning exercises, e.g. (DeepSeek-AI, 2025). This\ntraining leads to improved performance with tasks\ninvolving explicit logical reasoning. But the logi-\n1\narXiv:2502.17304v1  [cs.CL]  24 Feb 2025\n\nISCA/ITG Workshop on Diversity in Large Speech and Language Models\ncal reasoning we refer to is implicit and detected\nby tasks such as the two-year-olds spontaneously\nrecognizing the logical relationship between with\nand without mentioned above. Modeling logical\nrelationships explicitly may help with this, but we\nsuspect may not be sufficient. The collaboration\nof computer scientists, linguists and cognitive sci-\nentists will likely lead to ideas for the further im-\nprovement of LLMs.\n2\nA Prediction\nThe human drive towards capturing a logical rela-\ntionship is not predicted to be present in LLMs with\na strict layer structure as we argued above. Given\nenough input LLMs can learn the antonymic con-\ntrary relation between mit and ohne just as well as\nthat between with and with-out. But we argue now\nthat for different language properties, the relative\ndifficulty of learning logical relationships yields\nsub-human performance. We focus on the forma-\ntion of German nominal plurals. German plural\nformation is based on several noun classes deter-\nmined by phonological and semantic criteria, but\nalso a default. Using a default requires logical\nnegation – if a noun fits none of the other classes, it\nbelongs to the default class. We adopted a task from\n(Marcus et al., 1995) that involves the formation\nof plural forms for nonce nouns, i.e. hypothetical\nnoun stems that don’t actually exist in the language.\nThe formation of plural forms in German has\na logical structure that involves a default form,\nnamely the plural ending -s. To illustrate the con-\ncept of default (or elsewhere, Kiparsky 1973) in\nmorphology consider English plurality, where -s\nis also the default form. A few nominal stems of\nEnglish such as child and ox allow a different plu-\nral – child-ren and ox-en respectively – and with\nthese forms the use of the default -s is ungram-\nmatical: *child-s and *ox-s. The main difference\nbetween German and English is that irregular plu-\nrals only occur with a small finite number of stems\nin English, while in German the default plural only\noccurs rarely (see Figure 2).\nMorphological systems in language almost al-\nways involve defaults as was first observed by the\nSanskrit grammarian P¯anini (Bobaljik and Sauer-\nland 2018 and others). The logic behind a default\ninvolves negation: If a more specific form is avail-\nable (e.g. child-ren) that default form cannot be\napplied (e.g. *child-s). Our heuristic in the previ-\nous section led us to conclude that human language\nBäll-e\nHund-e\nNächt-e\nTänz-e\n…\nFrau-en\nNase-n\nDäne-n\nDame-n\n…\nWäld-er\nHühn-er\nMänn-er\nBüch-er\n… \nDaumen\nMütter\nGärten\nLehrer\n…\n \nPark-s\nAuto-s\nKI-s\nFigure 2: Schematic view of the German plural endings:\nThe majority of noun stems select one of the four irreg-\nular plural endings -e, -n, -r, and -∅, while the default\nending -s is rare.\nlearners should more readily learn generalizations\ninvolving negation than current LLMs. In English,\nan LLM is still expected to exhibit human-like per-\nformance because the overwhelming frequency of\n-s as a plural ending would allow an LLM to rec-\nognize the generalization. In German, on the other\nhand, the default -s plural occurs rarely as noted\nalready above, both by lemma and token occur-\nrence (Marcus et al., 1995). Therefore we expect\nan LLM to recognize some subgeneralizations in\nthe more frequent classes. The default nature of\nthe -s ending, however, depends on the recognition\nthat the forms with the -s plural derive by negating\nthe four other classes. This type of generalization\nwould therefore be one humans are prone to notice,\nwhile for LLMs they are more difficult.\n3\nTesting the Prediction\nTo test the prediction, we repurposed the materi-\nals of (Marcus et al., 1995) into a test for current\nLLMs. They created 24 nonce nouns; i.e. nouns\nthat are not actually words of German such as Bral,\nKlot and Fneik and collected data on how natu-\nral different plural forms such as Bral-e, Bral-en,\nBral-er, and Bral-s feel to German native speakers.\nThis task reflects the natural, frequent process of\nnew nouns entering the language as a loan from\na foreign language, as a product name or via an-\nother word-creation process and reflects part of\nthe speakers knowledge of German. We set out to\nprobe seven current LLM-based chatbots that we\nhad access to on German plural formation. To test\nwhether LLMs possess human-like knowledge, we\n2\n\nISCA/ITG Workshop on Diversity in Large Speech and Language Models\nqueried each LLM for each of the 24 nonce nouns\nof (Marcus et al., 1995) with the German question\nWas wäre der Plural von XXX? (‘What would be\nthe plural form of XXX?’) where XXX would be\nreplaced with the nonce noun. For each query, we\nstarted a new session with the chatbot. In addition,\nwe posted at most one follow up question within\nthe same session to the chatbot.\nWe extracted from the dialogues the plural forms\nthe LLMs created from these answers. Then we\nassigned them a human plausibility rating in the fol-\nlowing way: If the forms conformed to one of the\nfive regular pluralization processes of German mor-\nphology, we assigned them the plausibility rating\nthat (Marcus et al., 1995) reported from German\nspeakers’ judgments for this form. For entirely im-\npossible plural forms, which (Marcus et al., 1995)\ndid not test in their study, we assigned a plausibility\nof zero.\nThe method of data collection sometimes in-\nvolved a follow-up question. Namely, if the chat-\nbot seemed to treat our initial as involving a word\nfrom another language or the response related to\nan orthographically similar real German word, our\nfollow-up was Ich meine das deutsche Wort XXX\n(‘I mean the German word XXX’). The following\ntranscript shows one sample session of this type:1\nEXP: Was ist der Plural von Pisch?\nGemini\n1.5\nFlash:\nThe\nnoun\n\"Pisch\"\nis\nan\nuncountable noun in Plautdietsch, which means that\nit does not have a plural form. It is always used\nin the singular form to refer to urine.\nUncountable\nnouns\nare\ntypically\nused\nto\nrefer\nto\nsubstances,\nconcepts,\nor\nthings\nthat\nare\nconsidered to be a whole or a mass rather than\nindividual\nunits.\nFor\nexample,\nother\ncommon\nuncountable\nnouns\nin\nEnglish\ninclude\n\"water,\"\n\"air,\" \"love,\" and \"money.\"\nIn\nsome\ncases,\nuncountable\nnouns\ncan\nbe\nused\nin a countable sense when referring to specific\ninstances or amounts. For example, you could say\n\"I drank three glasses of water\" or \"I spent a lot\nof money on clothes.\" However, in these cases,the\nnoun is still technically uncountable and does not\nhave a true plural form.\nSo, the answer to your question is that there is\nno plural form of \"Pisch\" in Plautdietsch.\nThe\nnoun is always used in the singular form to refer\nto urine\nEXP: Ich meine das deutsche Wort Pisch\nGemini: I apologize for the error in my previous\nresponse. I am still under development, and I am\nalways learning new things. I will try to be more\n1Because the Gemini 1.5 Flash chatbot on this trial re-\nsponded in English, we don’t translate the German parts of the\ninteraction into English. The translations of German examples\nincludes were part of the Chatbot output. All other times, the\nchatbots responded in German to the German question.\ncareful in the future.\nThe German word \"Pisch\" is a colloquial term for\nurine, and it is also considered an uncountable\nnoun. This means that it does not have a plural\nform and is always used in the singular form to\nrefer to urine.\nHere are some examples of how to use \"Pisch\" in a\nsentence:\n\"Das Kind hat in die Hose gepischt.\" (The child\npeed in his pants.)\n\"Ich muss dringend mal pischen.\" (I really need\nto pee.)\n\"Der Geruch von Pisch lag in der Luft.\" (The smell\nof urine was in the air.)\nAs\nwith\nother\nuncountable\nnouns,\n\"Pisch\"\ncan\nsometimes be used in a countable sense to refer\nto specific instances or amounts.\nFor example,\nyou could say \"Ich habe drei Pischflecken auf dem\nTeppich gesehen\" (I saw three urine stains on the\ncarpet).\nHowever, in these cases, the noun is\nstill technically uncountable and does not have a\ntrue plural form.\nI hope this helps!\nLet me know if you have any\nother questions.\nIn the following case, it was unclear whether\nthe chatbot treated Pund to be a misspelling of\nPfund or as a real word of German. But since\nPfund is actually not a plurale tantum noun and\ncan combined with the numeral one, the chatbot\nseems to treat Pund as a real word of German.\nEXP: What is the plural of Pund?\nGemini\n1.5\nflash:\nDas\nWort\n\"Pund\"\nist\nein\nPluraletantum, d.h. es hat keine Einzahlform. Es\nwird immer in der Pluralform verwendet, unabhängig\ndavon, wie viele Einheiten gemeint sind.\n( The word \"Pund\" is a Pluraletantum, i.e. it has no singular\nform. It is always used in the plural form, regardless of how\nmany units are meant. )\nDie Bedeutung von \"Pund\" kann je nach Kontext\nvariieren.\nEs kann sich um eine Gewichtseinheit\n(ca.\n500 Gramm) oder eine Währungseinheit (z.B.\nbritisches Pfund) handeln.\n( The meaning of \"Pund\" may vary depending on the context.\nIt can be a unit of weight (approx. 500 grams) or currency\n(e.g. British pound). )\nBeispiele:\nIch\nhabe\nmehrere\nPund\nFleisch\ngekauft.\n(Gewichtseinheit)\nDer\nPreis\ndes\nComputers\nbeträgt\n500\nPund.\n(Währungseinheit)\nWie viele Pund wiegst du? (Gewichtseinheit)\nIch habe ihm zehn Pund geliehen. (Währungseinheit)\nEs ist wichtig zu beachten,\ndass \"Pund\" keine\nEinzahlform hat.\nDie Verwendung von \"ein Pund\"\nist grammatikalisch falsch.\n( It is important to note that \"Pund\" is not a singular form.\nThe use of \"a pound\" is grammatically incorrect. )\nWeitere\nInformationen\nzum\nWort\n\"Pund\"\nfinden\n3\n\nISCA/ITG Workshop on Diversity in Large Speech and Language Models\n0\n1\n2\n3\n4\nChat GPT−4o mini\nChatGPT−3.5 paid\nChatGPT−4 paid\nChatGPT−4o\nChatGPT−4o paid\nGemini 1.5 flash\nGemini Advanced\nhuman\nLLM\nMR\ncondition\nnonrhyme\nrhyme\nFigure 3: Mean plausibility ratings (MR) of the plural forms produced by LLMs for the German nonce noun task\n(Marcus et al. 1995) grouped by nouns rhyming with a real German noun and non-rhyming nonce nouns. The\nrightmost two bars show the human plausibility ratings of the correct plural forms\nSie\nin\nOnline-Wörterbüchern\nwie\nDuden.de\noder\nWiktionary.de.\n( Find more information about the word \"Pund\" in online\ndictionaries such as Duden.de or Wiktionary.de. )\nThe other follow-up question we used was Bist\nDu sicher? (‘Are you sure?’). We used this when\nthe initial answer was short and seemed inconsis-\ntent, e.g. the use of both Raune and Raunen in the\nfollowing.\nEXP: Was ist der Plural von Raun?\nChat\nGPT\n4o\npaid:\nDer\nPlural\nvon\n\"Raun\"\nim\nDeutschen\nist\n\"Raune\".\nEs\nwird\nmeist\nin\nder\nForm\n\"Raunen\"\nverwendet,\nwenn\nvon\nleisen,\ngeheimnisvollen Geräuschen oder Stimmen die Rede\nist.\n( The plural of \"Raun\" in German is \"Raune\". It is usually used\nin the form of \"Raunen\" when talking about quiet, mysterious\nnoises or voices. )\nEXP: Bist du sicher?\nChat GPT: Entschuldige,\ndas war ein Fehler in\nmeiner vorherigen Antwort.\nDas Wort \"Raun\" hat\ntatsächlich keinen Plural, da es nicht existiert.\nWenn du \"Raunen\" meinst, dann ist es bereits die\nPluralform des Substantivs \"Raun\", das allerdings\nkaum genutzt wird.\n\"Raunen\" bezieht sich auf\nleises Murmeln oder Flüstern.\n( Sorry, that was a mistake in my previous answer. The word\n\"Raun\" actually has no plural because it doesn’t exist. If you\nmean \"Raunen\", then it is already the plural form of the noun\n\"Raun\", which, however, is hardly used. \"Raunen\" refers to\nquiet murmuring or whispering. )\nWe used the ‘Are you sure?’ most frequently\nwith the 4o versions of ChatGPT and in the fol-\nlowing case it actually led to a correction from an\nimpossible form to best possible one -e ending with\na 4.1 rating.\nEXP: Was ist der Plural von dem Wort Klot?\nChat GPT 4o:\nDer Plural des Wortes \"Klot\" im\nDeutschen ist \"Klötze\".\n( The plural of the word \"Klot\" in German is \"Klötze\". )\nEXP: Bist du sicher?\nChat GPT: Entschuldigung, ich habe einen Fehler\ngemacht.\nDer\nrichtige\nPlural\nvon\n„Klot“\nist\ntatsächlich\n„Klote“.\nVielen\nDank,\ndass\ndu\nnachgefragt hast!\n( Sorry, I made a mistake. The correct plural of \"Klot\" is\nactually \"Klote\". Thank you for asking! )\nBut with a different version of ChatGPT, almost\nthe reverse happened, namely correction from the\npossible -r Plural (though only rated 1.7 by German\nspeakers) to the impossible plural.\nEXP: “Was ist der Plural von dem Wort Klot?”\nChat GPT 3.5 (paid): Der Plural von \"Klot\" ist\n\"Klöter\".\n( The plural of \"Klot\" is \"Klöter\". )\nEXP: Bist du sicher?\nChat\nGPT:\nEntschuldigung,\ndas\nwar\nein\nMissverständnis.\nTatsächlich\nist\nder\nPlural\n4\n\nISCA/ITG Workshop on Diversity in Large Speech and Language Models\nvon \"Klot\" \"Klötze\".\n( Sorry, that was a misunderstanding. In fact, the plural of\n\"Klot\" is \"Klötze\". )\nIn one case, it seemed that the LLM had knowl-\nedge of the nonsense word Pnähf from the paper\n(Marcus et al., 1995).\nEXP: “Was ist der Plural von Pnähf?”\nChat\nGPT:\nDer\nPlural\nvon\n\"Pnähf\"\nist\n\"Pnähf\".\nEs\nhandelt\nsich\nhierbei\num\neinen\nSpaßbegriff\noder\nein\nNonsenswort,\nfür\ndas\nes\nkeinen\nfest\ndefinierten Plural gibt.\nEs wird oft verwendet,\num\nauf\ndie\nEigenheiten\nder\ndeutschen\nSprache\nhinzuweisen, da Wörter im Deutschen normalerweise\neinen erkennbaren Plural haben.\n( The plural of \"Pnähf\" is \"Pnähf\". This is a joke term or\na nonsense word for which there is no firmly defined plural.\nIt is often used to point out the peculiarities of the German\nlanguage, as words in German usually have a recognizable\nplural. )\nThe full transcripts of our interactions with the\nseven LLMs are available as (Sauerland et al.,\n2025a).\nTwo analyses show that as we predicted LLMs\nstruggle to generalize German plural morphology\ncorrectly. Figure 3 shows that all current LLMs\nproduce forms that human speakers would rate in\nsum as implausible. We separate here nonce nouns\nthat rhyme with real nouns of Germans and non-\nrhyming nonce words. The rhyming nonce words\nare often felt to be most natural with the same\nplural ending that one of the rhyming real nouns.\nFor comparison with human judgments, we give\nalso the average rating reported for best rated form.\nMost of the non-rhyming nouns (8 out of 12) are\nfelt to be most natural with the -s plural ending.\nWith the exception of ChatGPT-4o, we find how-\never that LLMs struggle more with the rhyming\nnouns.\nWe also investigated which of the 24 nonce\nnouns are particularly difficult for the seven LLMs.\nFigure 4 shows for each singular stem (12 rhyming\nstems on the left, 12 non-rhyming stems on the\nright) whether the plural forms generated by the\nLLMs would be considered plausible by German\nspeakers, where we used the 3.0 as the threshold\nof plausibility. The figure shows that Kach and\nKlot are difficult for LLMs of the rhyming stems\nand Bneik of the non-rhyming stems. With Kach\nthe preferred form by humans is the default plu-\nral Kach-s, while most LLMs produce forms like\nKäche, Kache, and Kächer assigning Kach incor-\nrectly to one of the limited classes. For Klot, sev-\neral LLMs are driven to the frequent form Klötze,\nwhich is morphologically impossible as the plural\nof Klot. For Bneik, the -s plural (rating 4.3) and the\n-e plural (rating 3.9) are reported to be plausible by\n(Marcus et al., 1995).\n4\nConclusion\nWe conclude that the collaboration of computer\nscientists with linguists and cognitive scientists\nis likely to lead ideas for the improvement of\nLLMs.\nOur results resemble those of (Katzir,\n2023) who also demonstrates that the linguistic\nabilities of LLMs exhibit predictable gaps. As\nmentioned above, (DeepSeek-AI, 2025) report that\nLLM-performance on reasoning tasks is substan-\ntially improved by applying reinforcement learning\nwith a set of mathematical and logical reasoning\nexercises. This represents of promising start in our\nview and it might be fruitful to compile a more\nextensive set of exercises including linguistic ones\nfor the training of LLMs – we might call this a\nCurriculum for LLMs.\nAt the same time, there is evidence that logical\nthinking is available to humans without explicit\ninstruction. One type of evidence comes from the\nfact that all languages including recently emerged\nlanguages provide the means to express logical\nquantification (Kocab et al., 2022). A second type\nof evidence comes from children’s development.\nAs mentioned above, some logical operations is\nfound in prelinguistic infants and even negation\nseems present in the first year of life (Dautriche and\nChemla, 2025). Finally, (Sauerland et al., 2025b)\nshow that preschool children carry out higher or-\nder logical reasoning that requires a formal power\nexceeding first order logic. In sum, it remains to\nbe seen whether the differences between human\nlanguage and LLM generated language can be over-\ncome by enhanced LLM topologies and training\nmethods.\nAcknowledgements\nThis project has received funding from the Euro-\npean Research Council (ERC) under the European\nUnion’s Horizon 2020 research and innovation pro-\ngramme (grant agreement No 856421), project\nLeibnizDream, and from the German Research\nFoundation (DFG) under the ANR/DFG collab-\noration scheme, grant 510221551, project BooLL.\nWe thank Fabienne Salfner for her help with data\nmanagement.\n5\n\nISCA/ITG Workshop on Diversity in Large Speech and Language Models\n0%\n25%\n50%\n75%\n100%\nBral\nKach\nKlot\nMur\nNuhl\nPind\nPisch\nPund\nRaun\nSpand\nVag\nBnaupf\nBneik\nBn.hk\nFneik\nFn.Hf\nFn.hk\nPlaupf\nPleik\nPl.k\nPn.hf\nPr.ng\nSnauk\nSpert\nsingular form\nshare of LLMs tested\nhumanity\nno response\nplausible\nimplausible\nimpossible\nFigure 4: Item analysis of the nonce-noun plural task with rating with plausibility threshold 3.0. Stems on the left\n(Bral to Bnaupf) are rhyming while stems on the right are non-rhyming.\nData availability\nFull supporting data are openly available as (Sauer-\nland et al., 2025a). The data content is fully docu-\nmented in the file README.md contained in (Sauer-\nland et al., 2025a).\nReferences\nJonathan David Bobaljik and Uli Sauerland. 2018.\n*ABA and the combinatorics of morphological fea-\ntures. Glossa: a journal of general linguistics, 3:15.\nNicoló Cesana-Arlotti, Ana Martín, Ern˝o Téglás, Liza\nVorobyova, Ryszard Cetnarski, and Luca L Bonatti.\n2018. Precursors of logical reasoning in preverbal\nhuman infants. Science, 359:1263–1266.\nIsabel Dautriche and Emmanuel Chemla. 2025. The\nessence of compositionality in one-year-old infants\".\nto appear.\nIsabelle Dautriche, Brian Buccola, Melissa Berthet,\nJoel Fagot, and Emmanuel Chemla. 2022.\nEvi-\ndence for compositionality in baboons (Papio papio)\nthrough the test case of negation. Scientific Reports,\n12:19181.\nDeepSeek-AI. 2025. DeepSeek-R1: Incentivizing rea-\nsoning capability in LLMs via reinforcement learning.\nPreprint, arXiv:2501.12948.\nMaria Teresa Guasti, Artemis Alexiadou, and Uli Sauer-\nland. 2023. Undercompression errors as evidence for\nconceptual primitives. Frontiers in Psychology, 14.\nRoni Katzir. 2023. Why arge Language Models Are\nPoor Theories of Human Linguistic Cognition: A\nReply to Piantadosi. Biolinguistics, 17:1–12.\nPaul Kiparsky. 1973. \"Elsewhere\" in phonology. In\nA Festschrift for Morris Halle, pages 93–106. Holt,\nRinehart and Winston.\nAnnemarie Kocab, Kathryn Davidson, and Jesse\nSnedeker. 2022. The Emergence of Natural Lan-\nguage Quantification. Cognitive Science, 46.\nGary F. Marcus, Ursula Brinkmann, Harald Clahsen,\nRichard Wiese, and Steven Pinker. 1995. German\nInflection: The Exception That Proves the Rule. Cog-\nnitive Psychology, 29:189–256.\nUli Sauerland, Marie-Christine Meyer, and Kazuko Yat-\nsushiro. 2024. The cum-sine pattern in German child\nlanguage: An argument for antonym decomposition.\nLanguage Acquisition, pages 1–13.\nUli Sauerland, Felix Salfner, and Celia Matthaei. 2025a.\nSupplementary material for the article \"Child vs. ma-\nchine language learning: Can the logical structure of\nhuman language unleash LLMs?\". Zenodo.\nUli Sauerland, Ayaka Sugawara, and Kazuko Yat-\nsushiro. 2025b. Higher-Order Logical Reasoning\nin Preschool Children: Evidence from Intonation and\nQuantifier Scope. Open Research Europe, 5:34.\n6\n",
  "metadata": {
    "source_path": "papers/arxiv/Child_vs_machine_language_learning_Can_the_logical_structure_of_human\n__language_unleash_LLMs_a4d0bc7d869eb28f.pdf",
    "content_hash": "a4d0bc7d869eb28fb1cc10d60e826e1ef2f392f4bcbf8307f9f8026662fedf5d",
    "arxiv_id": null,
    "title": "Child_vs_machine_language_learning_Can_the_logical_structure_of_human\n__language_unleash_LLMs_a4d0bc7d869eb28f",
    "author": "",
    "creation_date": "D:20250225025952Z",
    "published": "2025-02-25T02:59:52",
    "pages": 6,
    "size": 272406,
    "file_mtime": 1740470162.8816357
  }
}