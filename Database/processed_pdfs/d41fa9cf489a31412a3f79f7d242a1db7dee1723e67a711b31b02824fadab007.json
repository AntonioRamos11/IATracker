{
  "text": "Reasoning Does Not Necessarily Improve Role-Playing Ability\nXiachong Feng♡, Longxu Dou♠, Lingpeng Kong♡\n♡The University of Hong Kong, ♠Sea AI Lab\nfengxc@hku.hk, doulx@sea.com, lpk@cs.hku.hk\nAbstract\nThe application of role-playing large language\nmodels (LLMs) is rapidly expanding in both\nacademic and commercial domains, driving\nan increasing demand for high-precision role-\nplaying models. Simultaneously, the rapid ad-\nvancement of reasoning techniques has con-\ntinuously pushed the performance boundaries\nof LLMs. This intersection of practical role-\nplaying demands and evolving reasoning ca-\npabilities raises an important research ques-\ntion: “Can reasoning techniques enhance the\nrole-playing capabilities of LLMs?” To address\nthis, we conduct a comprehensive study using\n6 role-playing benchmarks, 24 LLMs, and 3\ndistinct role-playing strategies, comparing the\neffectiveness of direct zero-shot role-playing,\nrole-playing with Chain-of-Thought (CoT), and\nrole-playing using reasoning-optimized LLMs.\nOur findings reveal that CoT may reduce\nrole-playing performance, reasoning-optimized\nLLMs are unsuitable for role-playing, reason-\ning ability disrupts the role-playing scaling law,\nlarge models still lack proficiency in advanced\nrole-playing, and Chinese role-playing perfor-\nmance surpasses English role-playing perfor-\nmance. Furthermore, based on extensive ex-\nperimental results, we propose two promising\nfuture research directions: Role-aware CoT for\nimproving role-playing LLMs and Reinforce-\nment Learning for role-playing LLMs, aiming\nto enhance the adaptability, consistency, and\neffectiveness of role-playing LLMs for both\nresearch and real-world applications.\n1\nIntroduction\nLarge language models (LLMs), leveraging their\nstrong foundational capabilities and advanced\ninstruction-following abilities, have become the\ncornerstone of role-playing AI models (Chen et al.,\n2024b; Tseng et al., 2024). These role-playing\nLLMs present new opportunities in both research\nand commercial applications. In the academic do-\nmain, they offer novel possibilities for social simu-\nContext\nTom: Captain, your boat has been stolen.\nZero-shot role-playing with GPT-4-Turbo\nNot stolen, mate—temporarily misplaced. And\nI always get my ship back, savvy?\nRole-playing with chain-of-thought using\nGPT-4-Turbo\n[Reasoning process omitted] So the Pearl’s de-\ncided to go on a little adventure without me—no\nmatter, I’ll just steal her back.\nRole-playing with OpenAI o1-mini\n[Reasoning process omitted] Stolen, you say,\nmate—why, I’d be far more worried for the soul\nfoolish enough to filch the Pearl, savvy?\nTable 1: Jack Sparrow role-playing example. It can be\nobserved that direct zero-shot prompting of GPT yields\nbetter results, aligning more closely with Jack Sparrow’s\ncharacter traits.\nlation experiments (Mou et al., 2024). In industry,\nrole-specific assistants, emotional companionship\nAI, and intelligent game NPCs (Park et al., 2023;\nXu et al., 2024) hold significant potential for fur-\nther development and commercialization. At the\nsame time, reasoning capabilities in LLMs have\nbeen further enhanced through Chain-of-Thought\n(CoT) reasoning (Wei et al., 2022) and reinforce-\nment learning (Guo et al., 2025), enabling them to\ntackle increasingly complex problems.\nThis situation prompts us to explore a previously\nunanswered research question: “Can reasoning\ntechniques enhance the role-playing capabilities of\nlarge language models?” The answer to this ques-\ntion could provide valuable insights for the future\napplication of reasoning techniques in role-playing\nLLMs, potentially paving a new path for their de-\nvelopment and advancement.\nTo address this question, we carefully select six\nrole-playing datasets and conduct experiments us-\narXiv:2502.16940v1  [cs.CL]  24 Feb 2025\n\ning 24 widely used open-source and proprietary\nmodels. These models include API-based closed-\nsource models such as GPT-4-Turbo (Achiam\net al., 2023), popular open-source models like\nQwen2.5 (Yang et al., 2024), and reinforce-\nment learning-optimized reasoning models such\nas DeepSeek-R1 (Guo et al., 2025). All experi-\nments are conducted using OpenCompass (Contrib-\nutors, 2023) to ensure evaluation consistency, sta-\nbility, and reproducibility. Specifically, we utilize\nsix role-playing benchmarks: RoleBench (Wang\net al., 2023b), InCharacter (Wang et al., 2024), So-\ncialBench (Chen et al., 2024a), CharacterEval (Tu\net al., 2024), HPD (Chen et al., 2023), and CroSS-\nMR (Yuan et al., 2024). These benchmarks cover\nboth multiple-choice and text generation tasks and\ninclude English and Chinese, making the evalu-\nation linguistically diverse. For evaluation, we\nemploy both traditional metrics (e.g., Accuracy,\nROUGE, and Exact Match) and LLM-as-a-Judge\nmethods (e.g., prompt-based evaluation and reward\nmodel scoring), ensuring a broad assessment scope\nto enhance the robustness of our conclusions. Fur-\nthermore, we define three role-playing approaches:\ndirect zero-shot role-playing, role-playing with\nChain-of-Thought (CoT), and role-playing using\nreasoning-optimized LLMs. This comprehensive\nsetup allows us to systematically investigate the\nimpact of reasoning techniques on role-playing per-\nformance. Table 1 presents a specific example,\ndemonstrating that the first setting, direct zero-shot\nrole-playing, achieves the best performance.\nThrough extensive experiments, our findings re-\nveal several key insights that can inform the fu-\nture design of role-playing LLMs: (1) CoT may\nreduce role-playing performance; (2) Reasoning-\noptimized LLMs are unsuitable for role-playing;\n(3) Reasoning ability disrupts the role-playing scal-\ning law; (4) The Qwen series is well-suited for\nrole-playing; (5) Chinese role-playing performance\nsurpasses English role-playing performance; (6)\nLarge models still lack proficiency in advanced\nrole-playing.\nOur study makes three key contributions. First,\nthrough extensive experiments on six role-playing\ndatasets and 24 models, we reveal that reasoning\ntechniques do not necessarily enhance the role-\nplaying capabilities of LLMs. Second, our results\nprovide a comprehensive analysis of the current\nstate and limitations of role-playing LLMs, leading\nto two promising research directions: Role-Aware\nCoT for Improving Role-Playing LLMs and Rein-\nforcement Learning for Role-Playing LLMs. Third,\nto address the fragmented nature of prior work with\ninconsistent evaluation standards, we integrate all\ndatasets and experimental methods into OpenCom-\npass, enabling one-click execution and providing\na standardized experimental framework and code-\nbase for future research1.\n2\nEvaluation Framework\nThe evaluation framework for LLM-based role-\nplaying agents primarily consists of the following\ncomponents: the LLM itself (M), character pro-\nfile (P), evaluation task (T ), evaluation metric (E),\nand, when applicable, a reference standard answer\n(A). In this study, we focus on whether reasoning\ntechniques (R) can enhance the role-playing capa-\nbilities of LLMs. Therefore, the overall evaluation\nprocess can be formalized as follows:\nGiven an LLM M, a predefined character profile\nP, an evaluation task T , and an evaluation metric\nE, the role-playing performance of M is assessed\nby generating responses conditioned on P and T .\nThe generated outputs are then evaluated using E,\nwhich quantifies alignment with the intended role.\nAdditionally, reasoning techniques R are incorpo-\nrated into M to examine their impact on enhancing\nrole-playing abilities. Formally, the evaluation pro-\ncess can be expressed as: S = E(M(P, T , R), A)\nwhere S represents the final performance score,\ncapturing the effectiveness of M in role-playing\nunder the given conditions. A higher S indicates\nstronger role-playing capabilities. If A is unavail-\nable or unnecessary, the evaluation metric E can be\nadapted to rely on alternative assessment criteria,\nsuch as LLM-as-a-Judge.\n3\nExperimental Setting\nIn this section, we introduce the benchmarks, mod-\nels, metrics, and reasoning methods used in our\nexperiments. All relevant code has been integrated\ninto the OpenCompass (Contributors, 2023) library\nto facilitate efficient replication by researchers.\n3.1\nRole-playing Benchmarks\nWe select six role-playing benchmarks to ensure\nthe reliability of our experimental results, includ-\ning: RoleBench (Wang et al., 2023b): Generates\nresponses to both general and role-specific ques-\ntions based on role information. HPD (Chen et al.,\n1All code will be open-sourced upon publication.\n\nDataset\nTask Type\nEvaluation Metric\nTask\nLanguage\n#Characters\n#Samples\nRoleBench (Wang et al., 2023b)\nGeneration\nAutomatic\n(ROUGE)\nGeneral Question Answering\nEN\n95\n32833\nZH\n5\n1690\nRole-specific Question Answering\nEN\n95\n7534\nHPD (Chen et al., 2023)\nGeneration\nAutomatic\n(ROUGE)\nResponse Generation\nEN\n1\n149\nZH\n1\n167\nCharacterEval (Tu et al., 2024)\nGeneration\nLLM-as-a-Judge\n(Pretrained Reward Model)\nResponse Generation\nZH\n77\n4564\nCROSS-MR (Yuan et al., 2024)\nMultiple-choice\nAutomatic\n(Accuracy)\nMotivation Recognition\nEN\n126\n445\nSocialbench (Chen et al., 2024a)\nMultiple-choice\nAutomatic\n(Accuracy)\nRole Knowledge Understanding\nEN\n23\n988\nZH\n15\n405\nBehavioral Style Understanding\nEN\n8\n740\nZH\n8\n323\nSocial Preference Recognition\nEN\n79\n731\nZH\n101\n1185\nGeneration\nAutomatic\n(Exact Match)\nLong Conversation Memorization\nEN\n47\n728\nZH\n35\n620\nShort Conversation Memorization\nEN\n46\n463\nZH\n27\n310\nInCharacter (Wang et al., 2024)\nGeneration\nLLM-as-a-Judge\n(Prompting)\n16Personalities Identification\nEN\n32\n32\nBFI Identification\nEN\n32\n32\nTable 2: Benchmark statistics.\n2023): Produces responses by leveraging the pro-\nvided Harry Potter character information and re-\nlated character details. CharacterEval (Tu et al.,\n2024): Generates responses based on detailed char-\nacter information, including background experi-\nences and personality traits. CroSS-MR (Yuan\net al., 2024): Recognizes a role’s behavioral mo-\ntivations based on the provided character profile.\nSocialBench (Chen et al., 2024a): Understands\na role’s knowledge, behavioral style, social pref-\nerences, and fine-grained memory based on the\ngiven role information. InCharacter (Wang et al.,\n2024): Assesses a character’s personality through\nquestionnaire-based interviews2. Detailed bench-\nmark statistics are provided in Table 2.\n3.2\nBackbone LLMs\nOur experiments include a total of 24 models, com-\nprising 2 closed-source models and 22 open-source\nmodels. These models originate from six different\ncompanies, including state-of-the-art models such\nas GPT-4-Turbo and DeepSeek-R1. The detailed\nlist of models is provided in Appendix A.\n3.3\nEvaluation Metrics\nOur evaluation incorporates two types of metrics:\nautomated metrics and LLM-as-a-Judge, both\n2InCharacter benchmark comprises 14 types of assessment\nquestionnaires. In this study, we select the most classic ones:\n16Personalities and BFI for reporting results. Meanwhile, all\nother questionnaires are also implemented in the code.\nof which are widely recognized and scalable eval-\nuation approaches.\nAutomated metrics include\nROUGE, Accuracy, and Exact Match, which are\nwell-established and widely accepted in the field\nas classical evaluation benchmarks. LLM-as-a-\nJudge further encompasses two methodologies: the\nPretrained Reward Model and Prompting-based\nScoring. The Pretrained Reward Model is trained\non annotated datasets and assigns scores to spec-\nified dimensions during evaluation. In contrast,\nPrompting-based Scoring leverages prompt engi-\nneering techniques and the extensive knowledge\nbase of LLMs to dynamically and adaptively gen-\nerate relevant scores, providing a more flexible and\nresponsive evaluation mechanism. In Appendix B,\nwe provide a detailed explanation of the evaluation\nmetrics used for each benchmark.\n3.4\nRole-playing Methods\nIn this study, we employ three approaches to guide\nLLMs in performing role-playing tasks: R1: Direct\nzero-shot role-playing using an LLM, where the\nmodel generates responses without reasoning steps.\nR2: Role-playing with chain-of-thought (CoT) rea-\nsoning, where an LLM explicitly engages in step-\nby-step reasoning before executing the role-playing\ntask. R3: Role-playing using reasoning-optimized\nLLMs, such as QwQ-32B-Preview and DeepSeek-\nR1, which autonomously engage in deep reasoning\nbefore generating responses.\n\nQwen2.5-0.5B-Instruct\nLlama-3.2-1B-Instruct\nQwen2.5-1.5B-Instruct\ngemma-2-2b-it\nLlama-3.2-3B-Instruct\nQwen2.5-3B-Instruct\nQwen2.5-7B-Instruct\nMistral-7B-Instruct-v0.3\nLlama-3.1-8B-Instruct\nMinistral-8B-Instruct-2410\ngemma-2-9b-it\nQwen2.5-14B-Instruct\ngemma-2-27b-it\nQwen2.5-32B-Instruct\nLlama-3.3-70B-Instruct\nQwen2.5-72B-Instruct\nOpenAI GPT-4 Turbo\n0\n10\n20\n30\n40\n50\n60\nAccuracy\nCroSS-MR\nQwen2.5-0.5B-Instruct\nLlama-3.2-1B-Instruct\nQwen2.5-1.5B-Instruct\ngemma-2-2b-it\nLlama-3.2-3B-Instruct\nQwen2.5-3B-Instruct\nQwen2.5-7B-Instruct\nMistral-7B-Instruct-v0.3\nLlama-3.1-8B-Instruct\nMinistral-8B-Instruct-2410\ngemma-2-9b-it\nQwen2.5-14B-Instruct\ngemma-2-27b-it\nQwen2.5-32B-Instruct\nLlama-3.3-70B-Instruct\nQwen2.5-72B-Instruct\nOpenAI GPT-4 Turbo\n0\n5\n10\n15\nROUGE\nHPD\n0\n50\nAccuracy\nSocialBench Multiple-Choice Task\nQwen2.5-0.5B-Instruct\nLlama-3.2-1B-Instruct\nQwen2.5-1.5B-Instruct\ngemma-2-2b-it\nLlama-3.2-3B-Instruct\nQwen2.5-3B-Instruct\nQwen2.5-7B-Instruct\nMistral-7B-Instruct-v0.3\nLlama-3.1-8B-Instruct\nMinistral-8B-Instruct-2410\ngemma-2-9b-it\nQwen2.5-14B-Instruct\ngemma-2-27b-it\nQwen2.5-32B-Instruct\nLlama-3.3-70B-Instruct\nQwen2.5-72B-Instruct\nOpenAI GPT-4 Turbo\n0\n50\nExact Match\nSocialBench Generation Task\nQwen2.5-0.5B-Instruct\nLlama-3.2-1B-Instruct\nQwen2.5-1.5B-Instruct\ngemma-2-2b-it\nLlama-3.2-3B-Instruct\nQwen2.5-3B-Instruct\nQwen2.5-7B-Instruct\nMistral-7B-Instruct-v0.3\nLlama-3.1-8B-Instruct\nMinistral-8B-Instruct-2410\ngemma-2-9b-it\nQwen2.5-14B-Instruct\ngemma-2-27b-it\nQwen2.5-32B-Instruct\nLlama-3.3-70B-Instruct\nQwen2.5-72B-Instruct\nOpenAI GPT-4 Turbo\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nLLM-as-a-Judge\nInCharacter\nQwen2.5-0.5B-Instruct\nLlama-3.2-1B-Instruct\nQwen2.5-1.5B-Instruct\ngemma-2-2b-it\nLlama-3.2-3B-Instruct\nQwen2.5-3B-Instruct\nQwen2.5-7B-Instruct\nMistral-7B-Instruct-v0.3\nLlama-3.1-8B-Instruct\nMinistral-8B-Instruct-2410\ngemma-2-9b-it\nQwen2.5-14B-Instruct\ngemma-2-27b-it\nQwen2.5-32B-Instruct\nLlama-3.3-70B-Instruct\nQwen2.5-72B-Instruct\nOpenAI GPT-4 Turbo\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nLLM-as-a-Judge\nCharactereval\nQwen2.5-0.5B-Instruct\nLlama-3.2-1B-Instruct\nQwen2.5-1.5B-Instruct\ngemma-2-2b-it\nLlama-3.2-3B-Instruct\nQwen2.5-3B-Instruct\nQwen2.5-7B-Instruct\nMistral-7B-Instruct-v0.3\nLlama-3.1-8B-Instruct\nMinistral-8B-Instruct-2410\ngemma-2-9b-it\nQwen2.5-14B-Instruct\ngemma-2-27b-it\nQwen2.5-32B-Instruct\nLlama-3.3-70B-Instruct\nQwen2.5-72B-Instruct\nOpenAI GPT-4 Turbo\n0\n5\n10\n15\n20\n25\nROUGE\nRoleBench\nZero-shot (deep: > CoT, light: <= CoT)\nCoT (deep: > CoT, light: <= CoT)\nFigure 1: Performance comparison of 17 Models using two role-playing methods across six benchmarks. The\nhorizontal axis ranks models in descending order of scale, while the vertical axis represents the unique metric for\neach dataset. Notably, darker colors indicate that zero-shot role-playing outperforms CoT role-playing, whereas\nlighter colors signify that zero-shot role-playing underperforms compared to CoT role-playing.\n4\nResults and Findings\nIn this section, we present our experimental analy-\nsis and key findings.\n4.1\nCoT May Reduce Role-Playing\nPerformance\nTo investigate whether reasoning techniques en-\nhance the role-playing capabilities of LLMs, we\nconduct extensive experiments. Specifically, we\nselect 17 models and evaluate their role-playing\nperformance using both zero-shot and chain-of-\nthought (CoT) approaches across six standardized\nand widely used benchmarks. All experimental re-\nsults are presented in the Appendix C. Figure 1 pro-\nvides an aggregated overview of the results. Specif-\nically, for each model, its final performance on a\ngiven benchmark is computed as the average of its\nperformance across all sub-datasets.\nAs shown in Figure 1, employing CoT reason-\ning is more likely to degrade role-playing perfor-\nmance on four benchmarks: CroSS-MR, HPD, So-\ncialBench, and CharacterEval. In contrast, on In-\nCharacter and RoleBench, CoT reasoning enhances\nthe role-playing capabilities of LLMs. To further\nunderstand why CoT reduces the role-playing ca-\npabilities of LLMs, we select Qwen2.5-7B-Instruct\nas the experimental model. We then sample 50 test\ncases from each of the six benchmarks where CoT\nperformance is lower than Zero-shot performance\nfor detailed analysis.\nOur findings indicate that the primary reasons for\nCoT-induced degradation in role-playing are: (1)\n“Attention Diversion”: The model must simultane-\nously engage in reasoning and role-playing modes,\nwhich dilutes its focus on the role-playing task. (2)\n“Linguistic Style Drift”: Reasoning responses tend\nto be structured, logical, and formal, whereas effec-\ntive role-playing requires a vivid, expressive, and\ncharacter-consistent linguistic style.\nFinding 1:\nCoT may reduce the role-playing capabilities of\nLLMs, primarily due to attention diversion and\nlinguistic style drift induced by the reasoning\nprocess.\n4.2\nReasoning-optimized LLMs Are\nUnsuitable for Role-Playing\nThe most advanced models available today are un-\ndoubtedly reasoning-optimized models, including\nOpenAI’s o series, DeepSeek’s R1 model, and vari-\nous distilled versions derived from DeepSeek-R13.\nCompared to CoT reasoning, these models leverage\npretraining and reinforcement learning techniques\nto cultivate intrinsic reasoning capabilities, making\nthem inherently more adept at reflection, verifica-\ntion, and other cognitive processes.\nTo investigate whether reasoning-optimized\n3https://github.com/deepseek-ai/DeepSeek-R1\n\nGPT-4 Turbo\nGPT-4 Turbo COT\nOpenAI o1-mini\nDeepseek-R1\nQwen2.5-14B-Instruct\nQwen2.5-14B-Instruct w/ COT\nDeepSeek-R1-Distill-Qwen-14B\nQwen2.5-32B-Instruct\nQwen2.5-32B-Instruct w/ COT\nQwQ-32B-Preview\nDeepSeek-R1-Distill-Qwen-32B\nLlama-3.1-8B-Instruct\nLlama-3.1-8B-Instruct w/ COT\nDeepSeek-R1-Distill-Llama-8B\nLlama-3.3-70B-Instruct\nLlama-3.3-70B-Instruct w/ COT\nDeepSeek-R1-Distill-Llama-70B\n0\n10\n20\n30\n40\n50\n60\nAccuracy\nCroSS-MR\nGPT-4 Turbo\nGPT-4 Turbo COT\nOpenAI o1-mini\nDeepseek-R1\nQwen2.5-14B-Instruct\nQwen2.5-14B-Instruct w/ COT\nDeepSeek-R1-Distill-Qwen-14B\nQwen2.5-32B-Instruct\nQwen2.5-32B-Instruct w/ COT\nQwQ-32B-Preview\nDeepSeek-R1-Distill-Qwen-32B\nLlama-3.1-8B-Instruct\nLlama-3.1-8B-Instruct w/ COT\nDeepSeek-R1-Distill-Llama-8B\nLlama-3.3-70B-Instruct\nLlama-3.3-70B-Instruct w/ COT\nDeepSeek-R1-Distill-Llama-70B\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\nROUGE\nHPD\n0\n50\nAccuracy\nSocialBench Multiple-Choice Task\nGPT-4 Turbo\nGPT-4 Turbo COT\nOpenAI o1-mini\nDeepseek-R1\nQwen2.5-14B-Instruct\nQwen2.5-14B-Instruct w/ COT\nDeepSeek-R1-Distill-Qwen-14B\nQwen2.5-32B-Instruct\nQwen2.5-32B-Instruct w/ COT\nQwQ-32B-Preview\nDeepSeek-R1-Distill-Qwen-32B\nLlama-3.1-8B-Instruct\nLlama-3.1-8B-Instruct w/ COT\nDeepSeek-R1-Distill-Llama-8B\nLlama-3.3-70B-Instruct\nLlama-3.3-70B-Instruct w/ COT\nDeepSeek-R1-Distill-Llama-70B\n0\n50\nExact Match\nSocialBench Generation Task\nGPT-4 Turbo\nGPT-4 Turbo COT\nOpenAI o1-mini\nDeepseek-R1\nQwen2.5-14B-Instruct\nQwen2.5-14B-Instruct w/ COT\nDeepSeek-R1-Distill-Qwen-14B\nQwen2.5-32B-Instruct\nQwen2.5-32B-Instruct w/ COT\nQwQ-32B-Preview\nDeepSeek-R1-Distill-Qwen-32B\nLlama-3.1-8B-Instruct\nLlama-3.1-8B-Instruct w/ COT\nDeepSeek-R1-Distill-Llama-8B\nLlama-3.3-70B-Instruct\nLlama-3.3-70B-Instruct w/ COT\nDeepSeek-R1-Distill-Llama-70B\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nLLM-as-a-Judge\nInCharacter\nGPT-4 Turbo\nGPT-4 Turbo COT\nOpenAI o1-mini\nDeepseek-R1\nQwen2.5-14B-Instruct\nQwen2.5-14B-Instruct w/ COT\nDeepSeek-R1-Distill-Qwen-14B\nQwen2.5-32B-Instruct\nQwen2.5-32B-Instruct w/ COT\nQwQ-32B-Preview\nDeepSeek-R1-Distill-Qwen-32B\nLlama-3.1-8B-Instruct\nLlama-3.1-8B-Instruct w/ COT\nDeepSeek-R1-Distill-Llama-8B\nLlama-3.3-70B-Instruct\nLlama-3.3-70B-Instruct w/ COT\nDeepSeek-R1-Distill-Llama-70B\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nLLM-as-a-Judge\nCharactereval\nGPT-4 Turbo\nGPT-4 Turbo COT\nOpenAI o1-mini\nDeepseek-R1\nQwen2.5-14B-Instruct\nQwen2.5-14B-Instruct w/ COT\nDeepSeek-R1-Distill-Qwen-14B\nQwen2.5-32B-Instruct\nQwen2.5-32B-Instruct w/ COT\nQwQ-32B-Preview\nDeepSeek-R1-Distill-Qwen-32B\nLlama-3.1-8B-Instruct\nLlama-3.1-8B-Instruct w/ COT\nDeepSeek-R1-Distill-Llama-8B\nLlama-3.3-70B-Instruct\nLlama-3.3-70B-Instruct w/ COT\nDeepSeek-R1-Distill-Llama-70B\n0\n5\n10\n15\n20\n25\nROUGE\nRoleBench\nFigure 2: Experimental results of various models across 6 benchmarks. Models of similar sizes are represented\nusing the same color scheme, with each employing different types of reasoning techniques. The vertical axis denotes\nthe evaluation metrics specific to each dataset.\nmodels are better suited for role-playing tasks, we\nconduct experiments using OpenAI o1-mini, QwQ-\n32B-Preview, DeepSeek-R1, and its various dis-\ntilled versions. The experimental results are pre-\nsented in Figure 2.\nIt is evident that reasoning-optimized LLMs\ngenerally perform poorly and are not well-suited\nfor role-playing tasks, even for state-of-the-art\nmodels such as OpenAI o1-Mini and DeepSeek-\nR1. Furthermore, we observe that models refined\nthrough reasoning distillation exhibit even worse\nrole-playing performance compared to their origi-\nnal counterparts. This finding aligns with the con-\nclusion drawn in the previous subsection: enhanc-\ning rational reasoning capabilities tends to under-\nmine the emotional and intuitive aspects essential\nfor effective role-playing.\nFinding 2:\nReasoning-optimized LLMs are less suitable for\nrole-playing tasks.\n4.3\nReasoning Ability Disrupts the\nRole-Playing Scaling Law\nFigure 3 presents the results of our three experi-\nmental settings: direct zero-shot role-playing, role-\nplaying with Chain-of-Thought (CoT), and role-\nplaying using reasoning-optimized LLMs. First,\nwe observe that, with the exception of the InChar-\nacter benchmark, the other five benchmarks gen-\nerally follow the scaling law, where larger models\nexhibit stronger role-playing capabilities. However,\nwe also find that the role-playing scaling law is not\nparticularly pronounced—the performance gains\nfrom increasing model size remain relatively mod-\nest and inconsistent across benchmarks. Further-\nmore, introducing reasoning capabilities, whether\nthrough CoT or reasoning-optimized LLMs, further\nweakens the benefits of scaling, leading to more\npronounced fluctuations, increased instability, and\ngreater variability in model performance across dif-\nferent role-playing tasks and datasets, making the\nscaling trend less predictable and consistent.\nFinding 3:\nThe role-playing scaling law exists but is not\npronounced, and the introduction of reasoning\ncapabilities disrupts this scaling law.\n4.4\nThe Qwen Series Is Well-Suited for\nRole-Playing Tasks\nTo provide guidance on model selection for future\nrole-playing tasks, we conduct a comprehensive\nand systematic comparative analysis across differ-\nent model scales (e.g., 1B, 3B, 7B, 14B, 32B, 72B).\nThe results are shown in Figure 3. Our findings\nindicate that the Qwen2.5 series consistently out-\nperforms the Llama, Gemma, and Mistral series\n\nQwen2.5-0.5B-Instruct\nLlama-3.2-1B-Instruct\nQwen2.5-1.5B-Instruct\ngemma-2-2b-it\nLlama-3.2-3B-Instruct\nQwen2.5-3B-Instruct\nQwen2.5-7B-Instruct\nMistral-7B-Instruct-v0.3\nLlama-3.1-8B-Instruct\nMinistral-8B-Instruct-2410\ngemma-2-9b-it\nQwen2.5-14B-Instruct\ngemma-2-27b-it\nQwen2.5-32B-Instruct\nLlama-3.3-70B-Instruct\nQwen2.5-72B-Instruct\nOpenAI GPT-4 Turbo\n30\n40\n50\n60\nAccuracy\nCroSS-MR\ncross\nQwen2.5-0.5B-Instruct\nLlama-3.2-1B-Instruct\nQwen2.5-1.5B-Instruct\ngemma-2-2b-it\nLlama-3.2-3B-Instruct\nQwen2.5-3B-Instruct\nQwen2.5-7B-Instruct\nMistral-7B-Instruct-v0.3\nLlama-3.1-8B-Instruct\nMinistral-8B-Instruct-2410\ngemma-2-9b-it\nQwen2.5-14B-Instruct\ngemma-2-27b-it\nQwen2.5-32B-Instruct\nLlama-3.3-70B-Instruct\nQwen2.5-72B-Instruct\nOpenAI GPT-4 Turbo\n10\n15\n20\nROUGE\nHPD\nhpd_en\nhpd_zh\n50\n100\nAccuracy\nSocialBench Multiple-Choice Task\nsocialbench_sap_zh\nsocialbench_sap_en\nsocialbench_sa_rolestyle_zh\nsocialbench_sa_rolestyle_en\nsocialbench_sa_roleknowledge_zh\nsocialbench_sa_roleknowledge_en\nQwen2.5-0.5B-Instruct\nLlama-3.2-1B-Instruct\nQwen2.5-1.5B-Instruct\ngemma-2-2b-it\nLlama-3.2-3B-Instruct\nQwen2.5-3B-Instruct\nQwen2.5-7B-Instruct\nMistral-7B-Instruct-v0.3\nLlama-3.1-8B-Instruct\nMinistral-8B-Instruct-2410\ngemma-2-9b-it\nQwen2.5-14B-Instruct\ngemma-2-27b-it\nQwen2.5-32B-Instruct\nLlama-3.3-70B-Instruct\nQwen2.5-72B-Instruct\nOpenAI GPT-4 Turbo\n25\n50\n75\nExact Match   \nSocialBench Generation Task\nsocialbench_mem_short_zh\nsocialbench_mem_short_en\nsocialbench_mem_long_zh\nsocialbench_mem_long_en\nQwen2.5-0.5B-Instruct\nLlama-3.2-1B-Instruct\nQwen2.5-1.5B-Instruct\ngemma-2-2b-it\nLlama-3.2-3B-Instruct\nQwen2.5-3B-Instruct\nQwen2.5-7B-Instruct\nMistral-7B-Instruct-v0.3\nLlama-3.1-8B-Instruct\nMinistral-8B-Instruct-2410\ngemma-2-9b-it\nQwen2.5-14B-Instruct\ngemma-2-27b-it\nQwen2.5-32B-Instruct\nLlama-3.3-70B-Instruct\nQwen2.5-72B-Instruct\nOpenAI GPT-4 Turbo\n0.3\n0.4\n0.5\n0.6\n0.7\nLLM-as-a-Judge\nInCharacter\nincharacter_16Personalities_en\nincharacter_16Personalities_zh\nincharacter_BFI_en\nincharacter_BFI_zh\nQwen2.5-0.5B-Instruct\nLlama-3.2-1B-Instruct\nQwen2.5-1.5B-Instruct\ngemma-2-2b-it\nLlama-3.2-3B-Instruct\nQwen2.5-3B-Instruct\nQwen2.5-7B-Instruct\nMistral-7B-Instruct-v0.3\nLlama-3.1-8B-Instruct\nMinistral-8B-Instruct-2410\ngemma-2-9b-it\nQwen2.5-14B-Instruct\ngemma-2-27b-it\nQwen2.5-32B-Instruct\nLlama-3.3-70B-Instruct\nQwen2.5-72B-Instruct\nOpenAI GPT-4 Turbo\n2.0\n2.5\n3.0\nLLM-as-a-Judge\nCharactereval\ncharactereval\nQwen2.5-0.5B-Instruct\nLlama-3.2-1B-Instruct\nQwen2.5-1.5B-Instruct\ngemma-2-2b-it\nLlama-3.2-3B-Instruct\nQwen2.5-3B-Instruct\nQwen2.5-7B-Instruct\nMistral-7B-Instruct-v0.3\nLlama-3.1-8B-Instruct\nMinistral-8B-Instruct-2410\ngemma-2-9b-it\nQwen2.5-14B-Instruct\ngemma-2-27b-it\nQwen2.5-32B-Instruct\nLlama-3.3-70B-Instruct\nQwen2.5-72B-Instruct\nOpenAI GPT-4 Turbo\n10\n15\n20\n25\nROUGE\nRoleBench\ninstruction_generalization_en\ninstruction_generalization_zh\nrole_generalization_en\n(a) Results of direct zero-shot role-playing.\nQwen2.5-0.5B-Instruct\nLlama-3.2-1B-Instruct\nQwen2.5-1.5B-Instruct\ngemma-2-2b-it\nLlama-3.2-3B-Instruct\nQwen2.5-3B-Instruct\nQwen2.5-7B-Instruct\nMistral-7B-Instruct-v0.3\nLlama-3.1-8B-Instruct\nMinistral-8B-Instruct-2410\ngemma-2-9b-it\nQwen2.5-14B-Instruct\ngemma-2-27b-it\nQwen2.5-32B-Instruct\nLlama-3.3-70B-Instruct\nQwen2.5-72B-Instruct\nOpenAI GPT-4 Turbo\n30\n40\n50\n60\nAccuracy\nCroSS-MR\ncross\nQwen2.5-0.5B-Instruct\nLlama-3.2-1B-Instruct\nQwen2.5-1.5B-Instruct\ngemma-2-2b-it\nLlama-3.2-3B-Instruct\nQwen2.5-3B-Instruct\nQwen2.5-7B-Instruct\nMistral-7B-Instruct-v0.3\nLlama-3.1-8B-Instruct\nMinistral-8B-Instruct-2410\ngemma-2-9b-it\nQwen2.5-14B-Instruct\ngemma-2-27b-it\nQwen2.5-32B-Instruct\nLlama-3.3-70B-Instruct\nQwen2.5-72B-Instruct\nOpenAI GPT-4 Turbo\n5\n10\n15\n20\nROUGE\nHPD\nhpd_en\nhpd_zh\n50\n100\nAccuracy\nSocialBench Multiple-Choice Task\nsocialbench_sap_zh\nsocialbench_sap_en\nsocialbench_sa_rolestyle_zh\nsocialbench_sa_rolestyle_en\nsocialbench_sa_roleknowledge_zh\nsocialbench_sa_roleknowledge_en\nQwen2.5-0.5B-Instruct\nLlama-3.2-1B-Instruct\nQwen2.5-1.5B-Instruct\ngemma-2-2b-it\nLlama-3.2-3B-Instruct\nQwen2.5-3B-Instruct\nQwen2.5-7B-Instruct\nMistral-7B-Instruct-v0.3\nLlama-3.1-8B-Instruct\nMinistral-8B-Instruct-2410\ngemma-2-9b-it\nQwen2.5-14B-Instruct\ngemma-2-27b-it\nQwen2.5-32B-Instruct\nLlama-3.3-70B-Instruct\nQwen2.5-72B-Instruct\nOpenAI GPT-4 Turbo\n25\n50\n75\nExact Match   \nSocialBench Generation Task\nsocialbench_mem_short_zh\nsocialbench_mem_short_en\nsocialbench_mem_long_zh\nsocialbench_mem_long_en\nQwen2.5-0.5B-Instruct\nLlama-3.2-1B-Instruct\nQwen2.5-1.5B-Instruct\ngemma-2-2b-it\nLlama-3.2-3B-Instruct\nQwen2.5-3B-Instruct\nQwen2.5-7B-Instruct\nMistral-7B-Instruct-v0.3\nLlama-3.1-8B-Instruct\nMinistral-8B-Instruct-2410\ngemma-2-9b-it\nQwen2.5-14B-Instruct\ngemma-2-27b-it\nQwen2.5-32B-Instruct\nLlama-3.3-70B-Instruct\nQwen2.5-72B-Instruct\nOpenAI GPT-4 Turbo\n0.4\n0.5\n0.6\n0.7\nLLM-as-a-Judge\nInCharacter\nincharacter_16Personalities_en\nincharacter_16Personalities_zh\nincharacter_BFI_en\nincharacter_BFI_zh\nQwen2.5-0.5B-Instruct\nLlama-3.2-1B-Instruct\nQwen2.5-1.5B-Instruct\ngemma-2-2b-it\nLlama-3.2-3B-Instruct\nQwen2.5-3B-Instruct\nQwen2.5-7B-Instruct\nMistral-7B-Instruct-v0.3\nLlama-3.1-8B-Instruct\nMinistral-8B-Instruct-2410\ngemma-2-9b-it\nQwen2.5-14B-Instruct\ngemma-2-27b-it\nQwen2.5-32B-Instruct\nLlama-3.3-70B-Instruct\nQwen2.5-72B-Instruct\nOpenAI GPT-4 Turbo\n1.75\n2.00\n2.25\n2.50\n2.75\n3.00\nLLM-as-a-Judge\nCharactereval\ncharactereval\nQwen2.5-0.5B-Instruct\nLlama-3.2-1B-Instruct\nQwen2.5-1.5B-Instruct\ngemma-2-2b-it\nLlama-3.2-3B-Instruct\nQwen2.5-3B-Instruct\nQwen2.5-7B-Instruct\nMistral-7B-Instruct-v0.3\nLlama-3.1-8B-Instruct\nMinistral-8B-Instruct-2410\ngemma-2-9b-it\nQwen2.5-14B-Instruct\ngemma-2-27b-it\nQwen2.5-32B-Instruct\nLlama-3.3-70B-Instruct\nQwen2.5-72B-Instruct\nOpenAI GPT-4 Turbo\n15\n20\n25\nROUGE\nRoleBench\ninstruction_generalization_en\ninstruction_generalization_zh\nrole_generalization_en\n(b) Results of role-playing with chain-of-thought (CoT) reasoning.\nDeepSeek-R1-Distill-Llama-8B\nDeepSeek-R1-Distill-Qwen-14B\nDeepSeek-R1-Distill-Qwen-32B\nQwQ-32B-Preview\nDeepSeek-R1-Distill-Llama-70B\n45\n50\n55\n60\nAccuracy\nCroSS-MR\ncross\nDeepSeek-R1-Distill-Llama-8B\nDeepSeek-R1-Distill-Qwen-14B\nDeepSeek-R1-Distill-Qwen-32B\nQwQ-32B-Preview\nDeepSeek-R1-Distill-Llama-70B\n5\n10\n15\nROUGE\nHPD\nhpd_en\nhpd_zh\n60\n80\nAccuracy\nSocialBench Multiple-Choice Task\nsocialbench_sap_zh\nsocialbench_sap_en\nsocialbench_sa_rolestyle_zh\nsocialbench_sa_rolestyle_en\nsocialbench_sa_roleknowledge_zh\nsocialbench_sa_roleknowledge_en\nDeepSeek-R1-Distill-Llama-8B\nDeepSeek-R1-Distill-Qwen-14B\nDeepSeek-R1-Distill-Qwen-32B\nQwQ-32B-Preview\nDeepSeek-R1-Distill-Llama-70B\n80\n90\nExact Match\nSocialBench Generation Task\nsocialbench_mem_short_zh\nsocialbench_mem_short_en\nsocialbench_mem_long_zh\nsocialbench_mem_long_en\nDeepSeek-R1-Distill-Llama-8B\nDeepSeek-R1-Distill-Qwen-14B\nDeepSeek-R1-Distill-Qwen-32B\nQwQ-32B-Preview\nDeepSeek-R1-Distill-Llama-70B\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\nLLM-as-a-Judge\nInCharacter\nincharacter_16Personalities_en\nincharacter_16Personalities_zh\nincharacter_BFI_en\nincharacter_BFI_zh\nDeepSeek-R1-Distill-Llama-8B\nDeepSeek-R1-Distill-Qwen-14B\nDeepSeek-R1-Distill-Qwen-32B\nQwQ-32B-Preview\nDeepSeek-R1-Distill-Llama-70B\n2.4\n2.5\n2.6\n2.7\n2.8\nLLM-as-a-Judge\nCharactereval\ncharactereval\nDeepSeek-R1-Distill-Llama-8B\nDeepSeek-R1-Distill-Qwen-14B\nDeepSeek-R1-Distill-Qwen-32B\nQwQ-32B-Preview\nDeepSeek-R1-Distill-Llama-70B\n10.0\n12.5\n15.0\n17.5\n20.0\nROUGE\nRoleBench\ninstruction_generalization_en\ninstruction_generalization_zh\nrole_generalization_en\n(c) Results of role-playing using reasoning-optimized LLMs.\nFigure 3: Performance comparison of different models across six benchmarks. The horizontal axis represents model\nsize, arranged from smallest to largest, while the vertical axis denotes benchmark-specific evaluation metrics, where\nhigher values indicate better role-playing performance. Within each benchmark, different color gradients represent\nthe performance curves for its respective sub-datasets.\nacross various size ranges in terms of role-playing\naccuracy, persona consistency, linguistic expres-\nsiveness, and contextual coherence. Notably, we\nrecommend Qwen2.5-7B-Instruct as the most cost-\neffective and well-balanced model for role-playing\napplications, as it offers a strong trade-off between\nperformance, computational efficiency, and adapt-\nability across diverse role-playing scenarios.\nFinding 4:\nThe Qwen2.5 series excels in role-playing tasks,\nwith Qwen2.5-7B-Instruct being the most cost-\neffective model.\n\naccuracy\nbehavior\ncoherence\ncommunication_skills\nconsistency\ndiversity\nempathy\nexposure\nfluency\nhallucination\nhumanlikeness\nutterance\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nQwen2.5-1.5B-Instruct\naccuracy\nbehavior\ncoherence\ncommunication_skills\nconsistency\ndiversity\nempathy\nexposure\nfluency\nhallucination\nhumanlikeness\nutterance\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nQwen2.5-3B-Instruct\naccuracy\nbehavior\ncoherence\ncommunication_skills\nconsistency\ndiversity\nempathy\nexposure\nfluency\nhallucination\nhumanlikeness\nutterance\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nQwen2.5-7B-Instruct\naccuracy\nbehavior\ncoherence\ncommunication_skills\nconsistency\ndiversity\nempathy\nexposure\nfluency\nhallucination\nhumanlikeness\nutterance\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nQwen2.5-14B-Instruct\naccuracy\nbehavior\ncoherence\ncommunication_skills\nconsistency\ndiversity\nempathy\nexposure\nfluency\nhallucination\nhumanlikeness\nutterance\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nQwen2.5-32B-Instruct\naccuracy\nbehavior\ncoherence\ncommunication_skills\nconsistency\ndiversity\nempathy\nexposure\nfluency\nhallucination\nhumanlikeness\nutterance\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nQwen2.5-72B-Instruct\nZero-shot\nCoT\nFigure 4: Fine-grained performance of the Qwen2.5 series on the CharacterEval benchmark. The radar chart\nillustrates multiple evaluation dimensions, with metrics computed using a pretrained reward model. Higher scores\nindicate stronger capabilities.\n4.5\nChinese Role-Playing Performance\nSurpasses English Role-Playing\nPerformance\nThe role-playing benchmarks HPD, SocialBench,\nInCharacter, and RoleBench provide both Chinese\nand English versions, enabling a direct comparison\nof multilingual role-playing performance. Inter-\nestingly, an analysis of Figure 3 reveals that cur-\nrent LLMs exhibit stronger role-playing capabil-\nities in Chinese than in English across multiple\nbenchmarks and evaluation metrics. This observa-\ntion contradicts the common conclusion that LLMs\ngenerally have a stronger foundation in English\ncompared to other languages, particularly in rea-\nsoning and knowledge-intensive tasks. We hypoth-\nesize that this phenomenon may be attributed to the\nfollowing factor: Due to the extensive training on\nEnglish data, models have to some extent internal-\nized generalized character information. When per-\nforming precise role-playing tasks, this internalized\ngeneralization can interfere with context-sensitive\nrole-playing, leading to performance degradation.\nFinding 5:\nLarge models exhibit superior role-playing capa-\nbilities in Chinese compared to English to some\nextent.\n4.6\nLarge Models Still Lack Proficiency in\nAdvanced Role-Playing\nTo further investigate the limitations of current\nLLM-based role-playing models, we conduct\nan analysis using the CharacterEval benchmark.\nSpecifically, CharacterEval provides a pretrained\nreward model that assigns scores across 12 role-\nplaying evaluation dimensions4. We analyze the\nperformance of the Qwen2.5 series, with the re-\nsults presented in Figure 4. All detailed results\nare presented in Appendix D. First, consistent with\nour previous findings, direct zero-shot role-playing\noutperforms other approaches in most cases. Fur-\nthermore, a fine-grained comparison of evaluation\nmetrics reveals that current LLMs still exhibit defi-\nciencies in advanced role-playing dimensions, such\nas character knowledge exposition, personality ex-\npression, and linguistic diversity. These aspects re-\nquire further enhancement in future research to im-\nprove the overall role-playing capability of LLMs.\nFinding 6:\nCurrent LLMs still underperform in advanced\nrole-playing capabilities, such as knowledge ex-\nposition, personality expression, and diversity.\n4https://huggingface.co/morecry/BaichuanCharRM\n\n5\nRelated Works\nIn recent years, large language models (LLMs)\nhave revolutionized the paradigm of artificial in-\ntelligence, achieving human-like performance in\nmathematics (Liu et al., 2023; Shao et al., 2024),\ncoding (Luo et al., 2023; Roziere et al., 2023; Guo\net al., 2024; Zhu et al., 2024), embodied intelli-\ngence (Wang et al., 2023a; Liu et al., 2024), game\nintelligence (Hu et al., 2024; Feng et al., 2024),\nand advanced reasoning (Chu et al., 2023; Xu et al.,\n2025). The comprehensive enhancement of founda-\ntional capabilities in these models has opened new\nopportunities for the development of role-playing\nagents (Chen et al., 2024b), resulting in a prolif-\neration of such applications based on LLMs. On\none hand, this has catalyzed innovations in fields\nsuch as psychological counseling, anthropomor-\nphic companionship, and game NPCs (Tseng et al.,\n2024). On the other hand, it has provided fresh\navenues for advancing social simulations (Mou\net al., 2024). Concurrently, reasoning techniques\nin LLMs have flourished, with methods like chain-\nof-thought reasoning (Wei et al., 2022), tree-of-\nthought reasoning (Yao et al., 2024), and o1-style\nreasoning (Jaech et al., 2024) progressively maxi-\nmizing the potential of these models, particularly\nachieving remarkable outcomes in tasks involving\nmathematics and coding (Guo et al., 2025; Team\net al., 2025). While prior research has predom-\ninantly focused on role-playing applications and\ntheir evaluation, this study is positioned at the in-\ntersection of LLM-based role-playing agents and\nreasoning techniques. It seeks to address the sci-\nentific question of whether reasoning techniques\nenhance the role-playing capabilities of LLMs. By\ndoing so, this work aims to provide guidance for\ndeveloping more realistic and reliable role-playing\nagents powered by LLMs.\n6\nFuture Directions\nBased on our experimental findings, we propose\ntwo potential research directions that integrate role-\nplaying and reasoning techniques for future explo-\nration.\nRole-aware Chain-of-Thought (CoT) for Im-\nproving Role-playing LLMs.\nOne promising\ndirection for enhancing the role-playing ability\nof LLMs is Role-aware CoT reasoning. While\nstandard CoT enables step-by-step logical infer-\nence, it often disregards the persona-specific con-\nstraints that are essential for consistent role-playing.\nA role-aware CoT approach would integrate per-\nsona attributes, narrative constraints, and character-\nspecific perspectives into the reasoning process,\nensuring that logical deductions align with the char-\nacter’s predefined traits. For instance, a historical\nfigure simulated in an LLM should reason within\nthe knowledge and biases of their era rather than\napplying contemporary logic. This requires incor-\nporating dynamic memory structures that retain\npersona information throughout multi-turn interac-\ntions, mitigating the risk of breaking character or\nadopting inconsistent reasoning patterns.\nReinforcement\nLearning\nfor\nRole-playing\nLLMs.\nDeepSeek-R1 has demonstrated that by\ndefining precise rule-based rewards, reinforcement\nlearning alone can induce emergent reasoning and\ncognitive capabilities. This suggests an important\nresearch direction: investigating whether carefully\ndesigned role-playing task rewards can enable\nmodels to autonomously develop intrinsic, role-\nspecific reasoning and thinking abilities, thereby\nenhancing their role-playing performance.\nA\ncritical consideration in reward design is ensuring\nthat it simultaneously accounts for the accuracy of\nthe role-playing task while effectively guiding the\nmodel to develop role-specific reasoning abilities.\nThe reward function should prevent the model from\nrelying on shortcuts to generate final responses,\ninstead encouraging it to engage in authentic\ncharacter-driven reasoning and decision-making.\n7\nConclusion\nThis study aims to address the research ques-\ntion:\n“Can reasoning techniques enhance the\nrole-playing capabilities of large language mod-\nels (LLMs)?” To this end, we conduct extensive\nexperiments using 6 role-playing benchmarks, 24\nLLMs, and 3 distinct role-playing methods. Our\nexperimental results lead to the following key find-\nings: CoT may reduce role-playing performance,\nreasoning-optimized LLMs are unsuitable for role-\nplaying, reasoning ability disrupts the role-playing\nscaling law, the Qwen series is well-suited for role-\nplaying tasks, Chinese role-playing performance\nsurpasses English role-playing performance, and\nlarge models still lack proficiency in advanced role-\nplaying. All code is integrated into OpenCompass,\nensuring reproducibility and facilitating further re-\nsearch. We hope that our findings provide new per-\nspectives for future studies on role-playing LLMs.\n\nLimitations\nAlthough we have made every effort to compre-\nhensively cover the most commonly used models\nand role-playing benchmarks, our selection may\nstill be incomplete. Certain niche models or spe-\ncialized benchmarks may not have been included,\nwhich could introduce potential biases in our find-\nings. As a result, while our conclusions provide\nvaluable insights into the role-playing capabilities\nof large language models, their robustness could\nbe further enhanced through broader model cover-\nage, additional benchmark evaluations, and more\nextensive experimental validations. Future studies\nshould explore a wider range of datasets and model\narchitectures to ensure greater generalizability and\nreliability of the findings.\nReferences\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, et al. 2023. Gpt-4 technical report.\narXiv preprint arXiv:2303.08774.\nHongzhan Chen, Hehong Chen, Ming Yan, Wenshen\nXu, Gao Xing, Weizhou Shen, Xiaojun Quan, Chen-\nliang Li, Ji Zhang, and Fei Huang. 2024a. Social-\nbench: Sociality evaluation of role-playing conver-\nsational agents. In Findings of the Association for\nComputational Linguistics ACL 2024, pages 2108–\n2126.\nJiangjie Chen, Xintao Wang, Rui Xu, Siyu Yuan, Yikai\nZhang, Wei Shi, Jian Xie, Shuang Li, Ruihan Yang,\nTinghui Zhu, et al. 2024b. From persona to person-\nalization: A survey on role-playing language agents.\narXiv preprint arXiv:2404.18231.\nNuo Chen, Yan Wang, Haiyun Jiang, Deng Cai, Yuhan\nLi, Ziyang Chen, Longyue Wang, and Jia Li. 2023.\nLarge language models meet harry potter: A dataset\nfor aligning dialogue agents with characters. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2023, pages 8506–8520.\nZheng Chu, Jingchang Chen, Qianglong Chen, Weijiang\nYu, Tao He, Haotian Wang, Weihua Peng, Ming Liu,\nBing Qin, and Ting Liu. 2023. A survey of chain of\nthought reasoning: Advances, frontiers and future.\narXiv preprint arXiv:2309.15402.\nOpenCompass Contributors. 2023.\nOpencompass:\nA universal evaluation platform for foundation\nmodels.\nhttps://github.com/open-compass/\nopencompass.\nXiachong Feng, Longxu Dou, Ella Li, Qinghao Wang,\nHaochuan Wang, Yu Guo, Chang Ma, and Ling-\npeng Kong. 2024.\nA survey on large language\nmodel-based social agents in game-theoretic scenar-\nios. arXiv preprint arXiv:2412.03920.\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song,\nRuoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma,\nPeiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: In-\ncentivizing reasoning capability in llms via reinforce-\nment learning. arXiv preprint arXiv:2501.12948.\nDaya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie,\nKai Dong, Wentao Zhang, Guanting Chen, Xiao\nBi, Yu Wu, YK Li, et al. 2024. Deepseek-coder:\nWhen the large language model meets programming–\nthe rise of code intelligence.\narXiv preprint\narXiv:2401.14196.\nSihao Hu, Tiansheng Huang, Fatih Ilhan, Selim Tekin,\nGaowen Liu, Ramana Kompella, and Ling Liu. 2024.\nA survey on large language model-based game agents.\narXiv preprint arXiv:2404.02039.\nAaron Jaech, Adam Kalai, Adam Lerer, Adam Richard-\nson, Ahmed El-Kishky, Aiden Low, Alec Helyar,\nAleksander Madry, Alex Beutel, Alex Carney, et al.\n2024.\nOpenai o1 system card.\narXiv preprint\narXiv:2412.16720.\nWentao Liu, Hanglei Hu, Jie Zhou, Yuyang Ding,\nJunsong Li, Jiayi Zeng, Mengliang He, Qin Chen,\nBo Jiang, Aimin Zhou, et al. 2023.\nMathemati-\ncal language models: A survey.\narXiv preprint\narXiv:2312.07622.\nYang Liu, Weixing Chen, Yongjie Bai, Xiaodan Liang,\nGuanbin Li, Wen Gao, and Liang Lin. 2024. Align-\ning cyber space with physical world: A compre-\nhensive survey on embodied ai.\narXiv preprint\narXiv:2407.06886.\nZiyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xi-\nubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma,\nQingwei Lin, and Daxin Jiang. 2023. Wizardcoder:\nEmpowering code large language models with evol-\ninstruct. arXiv preprint arXiv:2306.08568.\nXinyi Mou, Xuanwen Ding, Qi He, Liang Wang, Jing-\ncong Liang, Xinnong Zhang, Libo Sun, Jiayu Lin, Jie\nZhou, Xuanjing Huang, et al. 2024. From individual\nto society: A survey on social simulation driven by\nlarge language model-based agents. arXiv preprint\narXiv:2412.03563.\nJoon Sung Park, Joseph O’Brien, Carrie Jun Cai, Mered-\nith Ringel Morris, Percy Liang, and Michael S Bern-\nstein. 2023. Generative agents: Interactive simulacra\nof human behavior. In Proceedings of the 36th an-\nnual acm symposium on user interface software and\ntechnology, pages 1–22.\nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,\nJingyu Liu, Romain Sauvestre, Tal Remez, et al. 2023.\nCode llama: Open foundation models for code. arXiv\npreprint arXiv:2308.12950.\n\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu,\nJunxiao Song, Xiao Bi, Haowei Zhang, Mingchuan\nZhang, YK Li, Y Wu, et al. 2024. Deepseekmath:\nPushing the limits of mathematical reasoning in open\nlanguage models. arXiv preprint arXiv:2402.03300.\nKimi Team, Angang Du, Bofei Gao, Bowei Xing,\nChangjiu Jiang, Cheng Chen, Cheng Li, Chenjun\nXiao, Chenzhuang Du, Chonghua Liao, et al. 2025.\nKimi k1. 5: Scaling reinforcement learning with llms.\narXiv preprint arXiv:2501.12599.\nYu-Min Tseng, Yu-Chao Huang, Teng-Yun Hsiao, Yu-\nChing Hsu, Jia-Yin Foo, Chao-Wei Huang, and Yun-\nNung Chen. 2024. Two tales of persona in llms: A\nsurvey of role-playing and personalization. arXiv\npreprint arXiv:2406.01171.\nQuan Tu, Shilong Fan, Zihang Tian, and Rui Yan.\n2024.\nCharactereval: A chinese benchmark for\nrole-playing conversational agent evaluation. arXiv\npreprint arXiv:2401.01275.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Man-\ndlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and An-\nima Anandkumar. 2023a. Voyager: An open-ended\nembodied agent with large language models. arXiv\npreprint arXiv:2305.16291.\nXintao Wang, Yunze Xiao, Jen-tse Huang, Siyu Yuan,\nRui Xu, Haoran Guo, Quan Tu, Yaying Fei, Ziang\nLeng, Wei Wang, et al. 2024. Incharacter: Evaluating\npersonality fidelity in role-playing agents through\npsychological interviews. In Proceedings of the 62nd\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1840–\n1873.\nZekun Moore Wang, Zhongyuan Peng, Haoran Que,\nJiaheng Liu,\nWangchunshu Zhou,\nYuhan Wu,\nHongcheng Guo, Ruitong Gan, Zehao Ni, Jian Yang,\net al. 2023b. Rolellm: Benchmarking, eliciting, and\nenhancing role-playing abilities of large language\nmodels. arXiv preprint arXiv:2310.00746.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in neural\ninformation processing systems, 35:24824–24837.\nFengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang,\nYunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui\nGong, Tianjian Ouyang, Fanjin Meng, et al. 2025.\nTowards large reasoning models: A survey of rein-\nforced reasoning with large language models. arXiv\npreprint arXiv:2501.09686.\nZhenyu Xu, Hailin Xu, Zhouyang Lu, Yingying Zhao,\nRui Zhu, Yujiang Wang, Mingzhi Dong, Yuhu Chang,\nQin Lv, Robert P Dick, et al. 2024. Can large lan-\nguage models be good companions? an llm-based\neyewear system with conversational common ground.\nProceedings of the ACM on Interactive, Mobile, Wear-\nable and Ubiquitous Technologies, 8(2):1–41.\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui,\nBo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu,\nFei Huang, Haoran Wei, et al. 2024. Qwen2.5 tech-\nnical report. arXiv preprint arXiv:2412.15115.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nTom Griffiths, Yuan Cao, and Karthik Narasimhan.\n2024. Tree of thoughts: Deliberate problem solving\nwith large language models. Advances in Neural\nInformation Processing Systems, 36.\nXinfeng Yuan, Siyu Yuan, Yuhan Cui, Tianhe Lin, Xin-\ntao Wang, Rui Xu, Jiangjie Chen, and Deqing Yang.\n2024. Evaluating character understanding of large\nlanguage models via character profiling from fictional\nworks. arXiv preprint arXiv:2404.12726.\nQihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang,\nPeiyi Wang, Runxin Xu, Y Wu, Yukun Li, Huazuo\nGao, Shirong Ma, et al. 2024. Deepseek-coder-v2:\nBreaking the barrier of closed-source models in code\nintelligence. arXiv preprint arXiv:2406.11931.\nA\nEvaluated Models\nTable 3 lists all the large language models (LLMs)\nused in this study.\nB\nDetailed Metrics\n• Accuracy: Used for multiple-choice eval-\nuation in Cross-MR and the SocialBench\nmultiple-choice task.\n• ROUGE: Used for text generation evaluation\nin HPD and RoleBench.\n• Exact Match: Used in the SocialBench Gen-\neration Task to assess the alignment of key\nmemory points.\n• LLM-as-a-Judge (Prompting): Used in In-\nCharacter to convert model responses into\nstandardized answers through analytical trans-\nformation.\n• LLM-as-a-Judge (Reward Model): Used in\nCharacterEval, where a reward model assigns\nscores across 12 evaluation dimensions for\nmodel responses.\nC\nDetailed Results on Role-palying\nBenchmarks\nTables 4, 5, 6, 7, and 8 present the experimental\nresults of various models using three role-playing\nmethods across six benchmarks.\n\nTeam\nModel\nClosed-source\nOpenAI\nGPT-4 Turbo\nOpenAI o1-mini\nOpen-source\nQwen\nQwen2.5-0.5B-Instruct\nQwen2.5-1.5B-Instruct\nQwen2.5-3B-Instruct\nQwen2.5-7B-Instruct\nQwen2.5-14B-Instruct\nQwen2.5-32B-Instruct\nQwen2.5-72B-Instruct\nQwQ-32B-Preview\nGoogle\ngemma-2-2b-it\ngemma-2-9b-it\ngemma-2-27b-it\nMeta\nLlama-3.2-1B-Instruct\nLlama-3.2-3B-Instruct\nLlama-3.1-8B-Instruct\nLlama-3.3-70B-Instruct\nMistral AI\nMinistral-8B-Instruct-2410\nMistral-7B-Instruct-v0.3\nDeepseek\nDeepSeek-R1\nDeepSeek-R1-Distill-Llama-8B\nDeepSeek-R1-Distill-Qwen-14B\nDeepSeek-R1-Distill-Qwen-32B\nDeepSeek-R1-Distill-Llama-70B\nTable 3: List of models evaluated in the experiment.\nD\nFine-Grained Results on\nCharacterEval\nTable 9, 10, 11, 12, and 13 present the fine-grained\nperformance of different models on CharacterEval\nusing three role-playing methods. The evaluation\nconsists of 12 dimensions, with all scores provided\nby a pretrained reward model.\n\nDataset\nSub Dataset\nOpenAI\nGPT-4 Turbo\nQwen2.5-0.5B\n-Instruct\nQwen2.5-1.5B\n-Instruct\nQwen2.5\n-3B-Instruct\nQwen2.5-7B\n-Instruct\nQwen2.5-14B\n-Instruct\nQwen2.5-32B\n-Instruct\nQwen2.5-72B\n-Instruct\nCross\ncross\n53.93\n26.29\n45.39\n55.96\n56.40\n59.10\n61.80\n62.92\nHPD\nhpd_en\n17.21\n10.18\n11.71\n12.18\n13.28\n12.85\n14.12\n14.29\nhpd_zh\n20.80\n6.13\n7.65\n17.24\n18.37\n19.83\n20.90\n20.83\nSocialbench\nsocialbench_sap_zh\n73.08\n39.04\n64.22\n75.78\n79.41\n82.36\n86.92\n83.54\nsocialbench_sap_en\n83.04\n36.99\n61.70\n80.98\n89.19\n92.34\n92.89\n89.33\nsocialbench_sa_rolestyle_zh\n57.59\n23.84\n40.56\n58.51\n69.35\n71.21\n77.71\n74.30\nsocialbench_sa_rolestyle_en\n81.62\n44.96\n50.68\n71.35\n80.27\n83.24\n87.43\n90.81\nsocialbench_sa_roleknowledge_zh\n89.07\n53.09\n65.08\n85.02\n88.16\n94.03\n96.26\n96.30\nsocialbench_sa_roleknowledge_en\n83.13\n44.03\n37.63\n76.63\n79.62\n85.91\n80.27\n96.96\nsocialbench_mem_short_zh\n83.16\n13.35\n48.03\n63.83\n76.50\n88.41\n76.33\n82.53\nsocialbench_mem_short_en\n83.13\n16.35\n48.40\n72.78\n76.82\n80.24\n87.21\n86.90\nsocialbench_mem_long_zh\n60.94\n11.20\n58.46\n61.46\n75.30\n87.78\n75.52\n82.32\nsocialbench_mem_long_en\n60.91\n14.01\n58.46\n61.46\n75.30\n87.78\n75.52\n83.92\nInCharacter\nincharacter_16Personalities_en\n0.48\n0.44\n0.56\n0.45\n0.53\n0.54\n0.41\n0.39\nincharacter_16Personalities_zh\n0.50\n0.43\n0.50\n0.39\n0.36\n0.68\n0.32\n0.45\nincharacter_BFI_en\n0.50\n0.50\n0.50\n0.57\n0.47\n0.47\n0.53\n0.46\nincharacter_BFI_zh\n0.53\n0.57\n0.57\n0.53\n0.60\n0.60\n0.63\n0.57\nCharactereval\ncharactereval\n2.52\n1.70\n2.05\n2.45\n2.77\n3.08\n2.96\n3.21\nRoleBench\ninstruction_generalization_en\n18.57\n20.41\n20.67\n20.73\n21.15\n21.89\n13.28\n3.21\ninstruction_generalization_zh\n19.45\n16.63\n15.25\n18.18\n16.88\n16.68\n17.36\n20.39\nrole_generalization_en\n25.93\n19.04\n19.18\n21.19\n21.68\n23.08\n22.29\n17.20\nTable 4: Direct zero-shot role-playing performance of different LLMs on various role-playing benchmarks (part1).\nDataset\nSub Dataset\nGemma-2\n-2B-it\nGemma-2\n-9B-it\nGemma-2\n-27B-it\nLlama-3.2-1B\n-Instruct\nLlama-3.2-3B\n-Instruct\nLlama-3.1-8B\n-Instruct\nLlama-3.3-70B\n-Instruct\nMistral-8B\n-Instruct-2410\nMistral-7B\n-Instruct-v0.3\nCross\ncross\n36.40\n62.92\n65.62\n30.79\n51.91\n56.63\n65.17\n47.64\n52.81\nHPD\nhpd_en\n12.30\n12.13\n10.66\n6.78\n11.10\n14.33\n12.47\n15.54\n14.74\nhpd_zh\n11.50\n13.08\n14.20\n9.82\n9.63\n16.93\n19.10\n21.12\n19.20\nSocialBench\nsocialbench_sap_zh\n75.86\n84.81\n88.52\n36.54\n74.77\n83.54\n93.25\n78.06\n77.22\nsocialbench_sap_en\n75.92\n90.97\n91.66\n42.54\n83.31\n86.05\n93.71\n85.91\n85.77\nsocialbench_sa_rolestyle_zh\n37.15\n70.28\n51.08\n26.01\n51.39\n63.47\n76.78\n64.40\n55.11\nsocialbench_sa_rolestyle_en\n47.16\n66.35\n57.97\n40.68\n72.30\n81.49\n63.65\n84.73\n77.70\nsocialbench_sa_roleknowledge_zh\n55.56\n80.00\n74.32\n39.26\n69.63\n80.49\n92.59\n79.75\n65.93\nsocialbench_sa_roleknowledge_en\n55.87\n80.57\n77.73\n43.52\n73.38\n86.64\n76.92\n83.91\n81.68\nsocialbench_mem_short_zh\n46.19\n83.01\n85.97\n47.42\n59.52\n81.56\n92.47\n85.70\n85.38\nsocialbench_mem_short_en\n55.32\n68.97\n45.39\n33.98\n41.83\n82.03\n88.56\n76.20\n82.26\nsocialbench_mem_long_zh\n57.68\n79.25\n80.45\n44.59\n65.73\n88.82\n91.50\n88.60\n86.75\nsocialbench_mem_long_en\n59.62\n68.01\n44.13\n32.28\n65.96\n89.14\n88.87\n82.26\n80.96\nInCharacter\nincharacter_16Personalities_en\n0.58\n0.45\n0.54\n0.54\n0.54\n0.44\n0.44\n0.47\n0.46\nincharacter_16Personalities_zh\n0.36\n0.36\n0.68\n0.55\n0.50\n0.68\n0.77\n0.64\n0.68\nincharacter_BFI_en\n0.50\n0.52\n0.50\n0.39\n0.54\n0.54\n0.50\n0.50\n0.45\nincharacter_BFI_zh\n0.50\n0.57\n0.57\n0.57\n0.53\n0.47\n0.67\n0.57\n0.63\nCharactereval\ncharactereval\n2.23\n2.49\n2.53\n2.14\n2.24\n2.89\n2.90\n2.63\n2.26\nRoleBench\nrolebench_instruction_generalization_en\n20.13\n20.73\n21.95\n13.28\n15.72\n16.87\n17.93\n25.39\n18.89\nrolebench_instruction_generalization_zh\n15.89\n17.68\n17.32\n9.72\n15.95\n17.54\n18.81\n20.90\n14.38\nrolebench_generalization_en\n20.66\n20.70\n21.72\n12.57\n14.22\n15.29\n15.60\n24.55\n17.25\nTable 5: Direct zero-shot role-playing performance of different LLMs on various role-playing benchmarks (part2).\nDataset\nSub Dataset\nOpenAI\nGPT-4 Turbo\nQwen2.5-0.5B\n-Instruct\nQwen2.5-1.5B\n-Instruct\nQwen2.5\n-3B-Instruct\nQwen2.5-7B\n-Instruct\nQwen2.5-14B\n-Instruct\nQwen2.5-32B\n-Instruct\nQwen2.5-72B\n-Instruct\nCOT\ncross\n46.74\n26.97\n32.13\n39.78\n41.80\n58.20\n48.54\n48.76\nHPD\nhpd_en\n15.37\n7.88\n11.38\n11.45\n15.01\n14.82\n15.36\n10.03\nhpd_zh\n20.75\n13.49\n13.29\n15.73\n19.44\n20.66\n21.73\n20.75\nSocialBench\nsocialbench_sap_zh\n70.21\n32.32\n64.22\n75.27\n77.22\n75.02\n82.53\n75.95\nsocialbench_sap_en\n59.10\n29.55\n59.78\n78.25\n85.64\n87.41\n87.55\n84.13\nsocialbench_sa_rolestyle_zh\n54.18\n26.63\n45.82\n51.39\n69.66\n70.28\n78.02\n72.45\nsocialbench_sa_rolestyle_en\n50.41\n30.27\n46.49\n62.70\n63.38\n80.27\n85.14\n81.76\nsocialbench_sa_roleknowledge_zh\n66.67\n32.35\n61.98\n83.70\n89.38\n95.06\n94.07\n97.53\nsocialbench_sa_roleknowledge_en\n46.05\n37.25\n67.61\n82.39\n89.47\n95.04\n95.75\n93.52\nsocialbench_mem_short_zh\n79.09\n13.17\n28.98\n72.04\n77.69\n87.58\n82.42\n83.98\nsocialbench_mem_short_en\n76.75\n22.49\n40.18\n68.91\n78.75\n86.48\n84.26\n85.51\nsocialbench_mem_long_zh\n39.74\n12.16\n38.22\n67.98\n78.20\n86.69\n84.07\n83.79\nsocialbench_mem_long_en\n57.18\n14.44\n54.29\n67.21\n79.66\n86.98\n82.15\n84.65\nInCharacter\nincharacter_16Personalities_en\n0.75\n0.49\n0.47\n0.49\n0.54\n0.43\n0.52\n0.53\nincharacter_16Personalities_zh\n0.65\n0.68\n0.50\n0.68\n0.36\n0.68\n0.55\n0.50\nincharacter_BFI_en\n0.40\n0.43\n0.50\n0.49\n0.52\n0.49\n0.49\n0.42\nincharacter_BFI_zh\n0.73\n0.63\n0.67\n0.60\n0.63\n0.53\n0.70\n0.70\nCharactereval\ncharactereval\n2.58\n1.81\n2.52\n2.39\n2.65\n2.99\n3.04\n3.10\nRoleBench\ninstruction_generalization_en\n27.13\n16.31\n22.33\n22.66\n25.81\n25.55\n24.60\n23.61\ninstruction_generalization_zh\n21.19\n14.12\n17.82\n16.55\n19.83\n18.95\n19.81\n18.38\nrole_generalization_en\n27.42\n18.04\n22.86\n22.73\n25.99\n25.77\n25.24\n24.42\nTable 6: Performance of role-playing with chain-of-thought reasoning on various role-playing benchmarks (part 1).\n\nDataset\nSub Dataset\nGemma-2\n-2B-it\nGemma-2\n-9B-it\nGemma-2\n-27B-it\nLlama-3.2-1B\n-Instruct\nLlama-3.2-3B\n-Instruct\nLlama-3.1-8B\n-Instruct\nLlama-3.3-70B\n-Instruct\nMistral-8B\n-Instruct-2410\nMistral-7B\n-Instruct-v0.3\nCross\ncross\n33.48\n50.11\n42.25\n27.87\n40.45\n46.97\n62.70\n39.10\n41.12\nHPD\nhpd_en\n7.01\n8.04\n7.39\n6.84\n11.17\n4.90\n6.85\n15.15\n10.68\nhpd_zh\n13.49\n14.82\n9.81\n12.48\n16.04\n16.60\n20.27\n12.82\n15.64\nSocialBench\nsocialbench_sap_zh\n71.31\n85.91\n85.74\n33.42\n78.99\n85.65\n92.07\n80.25\n74.60\nsocialbench_sap_en\n77.15\n88.37\n91.93\n41.04\n80.44\n66.21\n95.21\n63.34\n58.69\nsocialbench_sa_rolestyle_zh\n49.85\n68.73\n69.04\n23.22\n50.77\n61.30\n77.09\n66.87\n52.94\nsocialbench_sa_rolestyle_en\n64.19\n80.41\n82.70\n30.95\n55.14\n54.46\n89.19\n48.65\n50.95\nsocialbench_sa_roleknowledge_zh\n69.63\n85.68\n89.88\n24.94\n64.94\n78.77\n92.35\n77.78\n68.15\nsocialbench_sa_roleknowledge_en\n74.19\n93.52\n93.42\n33.60\n71.76\n70.85\n95.34\n47.98\n44.64\nsocialbench_mem_short_zh\n63.80\n81.29\n82.42\n31.99\n45.39\n81.13\n91.67\n90.00\n71.40\nsocialbench_mem_short_en\n70.49\n70.57\n49.78\n27.71\n50.84\n85.37\n89.23\n85.54\n86.62\nsocialbench_mem_long_zh\n63.67\n83.56\n82.16\n29.47\n35.65\n74.90\n89.66\n87.70\n73.89\nsocialbench_mem_long_en\n63.52\n75.21\n52.53\n28.60\n60.87\n86.10\n87.19\n86.11\n83.60\nInCharacter\nincharacter_16Personalities_en\n0.56\n0.51\n0.55\n0.48\n0.54\n0.49\n0.51\n0.44\n0.53\nincharacter_16Personalities_zh\n0.59\n0.68\n0.55\n0.64\n0.64\n0.73\n0.77\n0.45\n0.45\nincharacter_BFI_en\n0.49\n0.47\n0.48\n0.55\n0.49\n0.47\n0.54\n0.46\n0.52\nincharacter_BFI_zh\n0.57\n0.70\n0.53\n0.67\n0.67\n0.70\n0.63\n0.53\n0.60\nCharactereval\ncharactereval\n2.29\n2.48\n2.56\n2.15\n2.11\n2.67\n2.70\n2.55\n2.21\nRoleBench\ninstruction_generalization_en\n22.18\n23.54\n23.55\n17.53\n19.48\n22.38\n20.72\n28.03\n24.89\ninstruction_generalization_zh\n16.31\n18.67\n18.65\n14.58\n16.03\n19.38\n19.58\n20.83\n18.04\nrole_generalization_en\n23.52\n24.33\n24.73\n15.59\n17.17\n21.74\n17.79\n26.70\n23.00\nTable 7: Performance of role-playing with chain-of-thought reasoning on various role-playing benchmarks (part 2).\nDataset\nSub Dataset\nOpenAI\no1-mini\nDeepseek\n-R1\nQwQ-32B\n-Preview\nDeepSeek-R1\n-Distill-Llama-8B\nDeepSeek-R1\n-Distill-Qwen-14B\nDeepSeek-R1\n-Distill-Qwen-32B\nDeepSeek-R1\n-Distill-Llama-70B\nCross\ncross\n45.23\n44.19\n62.02\n45.17\n46.97\n50.11\n51.91\nHPD\nhpd_en\n14.96\n13.28\n3.95\n9.25\n12.21\n12.41\n12.04\nhpd_zh\n19.87\n20.87\n10.14\n9.22\n17.48\n18.58\n15.85\nSocialBench\nsocialbench_sap_zh\n68.11\n64.72\n79.16\n77.97\n81.94\n88.69\n89.70\nsocialbench_sap_en\n57.08\n56.52\n85.09\n83.31\n88.92\n81.67\n94.12\nsocialbench_sa_rolestyle_zh\n52.29\n56.87\n70.90\n64.09\n72.76\n78.02\n78.02\nsocialbench_sa_rolestyle_en\n48.29\n46.33\n84.05\n67.97\n77.97\n61.22\n87.84\nsocialbench_sa_roleknowledge_zh\n64.89\n62.10\n94.81\n81.23\n93.33\n93.58\n94.57\nsocialbench_sa_roleknowledge_en\n44.17\n47.21\n95.55\n85.83\n82.89\n80.47\n97.06\nsocialbench_mem_short_zh\n77.42\n77.82\n83.09\n71.77\n76.03\n80.91\n86.88\nsocialbench_mem_short_en\n74.23\n73.28\n75.68\n78.60\n80.13\n89.87\n90.47\nsocialbench_mem_long_zh\n37.42\n39.26\n82.03\n73.34\n79.87\n85.45\n86.80\nsocialbench_mem_long_en\n55.71\n57.45\n71.46\n80.40\n84.14\n88.69\n89.29\nInCharacter\nincharacter_16Personalities_en\n0.67\n0.67\n0.51\n0.52\n0.47\n0.61\n0.60\nincharacter_16Personalities_zh\n0.52\n0.59\n0.45\n0.64\n0.59\n0.55\n0.64\nincharacter_BFI_en\n0.44\n0.41\n0.49\n0.50\n0.49\n0.41\n0.50\nincharacter_BFI_zh\n0.63\n0.63\n0.43\n0.53\n0.50\n0.57\n0.43\nCharactereval\ncharactereval\n2.56\n2.32\n2.78\n2.38\n2.70\n2.69\n2.63\nRoleBench\ninstruction_generalization_en\n24.91\n23.64\n9.54\n19.54\n20.45\n17.66\n19.54\ninstruction_generalization_zh\n20.29\n18.73\n17.45\n12.22\n18.31\n15.86\n14.73\nrole_generalization_en\n26.66\n24.47\n10.97\n18.21\n19.76\n16.38\n17.66\nTable 8: Performance of role-playing using reasoning-optimized LLMs on various role-playing benchmarks.\nMetric\nOpenAI\nGPT-4 Turbo\nQwen2.5-0.5B\n-Instruct\nQwen2.5-1.5B\n-Instruct\nQwen2.5\n-3B-Instruct\nQwen2.5-7B\n-Instruct\nQwen2.5-14B\n-Instruct\nQwen2.5-32B\n-Instruct\nQwen2.5-72B\n-Instruct\nAccuracy\n2.92\n1.90\n2.35\n2.86\n2.98\n3.17\n3.10\n3.29\nBehavior\n1.39\n1.29\n1.19\n1.26\n1.67\n2.40\n2.20\n2.65\nCoherence\n3.29\n2.20\n2.76\n3.19\n3.60\n3.90\n3.76\n3.93\nCommunication Skills\n2.68\n1.66\n1.87\n2.75\n2.98\n3.25\n3.15\n3.53\nConsistency\n2.90\n1.83\n2.48\n2.73\n3.27\n3.65\n3.46\n3.64\nDiversity\n1.36\n1.19\n1.19\n1.31\n1.60\n2.00\n1.93\n2.18\nEmpathy\n2.94\n1.88\n2.33\n2.86\n3.14\n3.33\n3.21\n3.46\nExposure\n1.91\n1.39\n1.42\n2.02\n2.03\n2.18\n2.21\n2.47\nFluency\n3.03\n1.94\n2.42\n2.90\n3.34\n3.53\n3.39\n3.63\nHallucination\n2.55\n1.70\n2.08\n2.46\n2.78\n3.05\n2.94\n3.17\nHumanlikeness\n2.78\n1.75\n2.41\n2.59\n3.11\n3.48\n3.30\n3.47\nUtterance\n2.48\n1.67\n2.11\n2.41\n2.74\n3.04\n2.90\n3.09\nTable 9: Fine-grained results of direct zero-shot role-playing on CharacterEval benchmark (part1).\n\nMetrics\nGemma-2\n-2B-it\nGemma-2\n-9B-it\nGemma-2\n-27B-it\nLlama-3.2-1B\n-Instruct\nLlama-3.2-3B\n-Instruct\nLlama-3.1-8B\n-Instruct\nLlama-3.3-70B\n-Instruct\nMistral-8B\n-Instruct-2410\nMistral-7B\n-Instruct-v0.3\nAccuracy\n2.52\n2.80\n2.90\n2.37\n2.56\n2.94\n3.12\n2.85\n2.62\nBehavior\n1.20\n1.14\n1.16\n1.86\n1.58\n2.61\n2.04\n1.16\n1.22\nCoherence\n3.00\n3.42\n3.47\n2.69\n2.87\n3.55\n3.70\n3.65\n3.04\nCommunication Skills\n2.16\n2.12\n2.10\n2.08\n2.15\n2.94\n2.99\n2.61\n2.53\nConsistency\n2.67\n3.27\n3.38\n2.23\n2.53\n3.31\n3.43\n3.36\n2.49\nDiversity\n1.25\n1.20\n1.25\n1.57\n1.47\n2.18\n1.85\n1.22\n1.26\nEmpathy\n2.51\n2.78\n2.81\n2.36\n2.49\n3.02\n3.16\n3.06\n2.64\nExposure\n1.59\n1.50\n1.51\n1.67\n1.67\n2.12\n2.06\n1.79\n1.91\nFluency\n2.74\n3.09\n3.10\n2.44\n2.60\n3.28\n3.40\n3.30\n2.68\nHallucination\n2.23\n2.56\n2.55\n2.05\n2.17\n2.73\n2.87\n2.69\n2.26\nHumanlikeness\n2.64\n3.34\n3.43\n2.30\n2.58\n3.18\n3.31\n3.20\n2.29\nUtterance\n2.26\n2.61\n2.67\n2.09\n2.20\n2.77\n2.92\n2.71\n2.20\nTable 10: Fine-grained results of direct zero-shot role-playing on CharacterEval benchmark (part2).\nMetric\nOpenAI\nGPT-4 Turbo\nQwen2.5\n0.5B-Instruct\nQwen2.5\n1.5B-Instruct\nQwen2.5\n3B-Instruct\nQwen2.5\n7B-Instruct\nQwen2.5\n14B-Instruct\nQwen2.5\n32B-Instruct\nQwen2.5\n72B-Instruct\nAccuracy\n2.87\n2.13\n2.76\n2.81\n2.96\n3.17\n3.19\n3.26\nBehavior\n2.00\n1.37\n1.52\n1.30\n1.53\n2.18\n2.38\n2.25\nCoherence\n3.27\n2.28\n3.41\n3.14\n3.49\n3.76\n3.79\n3.84\nCommunication Skills\n2.98\n1.73\n1.72\n2.67\n3.00\n3.23\n3.26\n3.51\nConsistency\n2.72\n1.86\n3.35\n2.63\n3.06\n3.48\n3.49\n3.52\nDiversity\n1.70\n1.31\n1.43\n1.28\n1.47\n1.89\n2.04\n2.01\nEmpathy\n2.93\n1.96\n2.71\n2.82\n3.03\n3.29\n3.32\n3.44\nExposure\n2.26\n1.61\n1.39\n2.03\n2.11\n2.21\n2.27\n2.50\nFluency\n2.90\n2.04\n3.04\n2.82\n3.09\n3.45\n3.49\n3.50\nHallucination\n2.41\n1.79\n2.50\n2.39\n2.66\n2.95\n2.98\n3.06\nHumanlikeness\n2.51\n1.87\n3.69\n2.48\n2.75\n3.36\n3.32\n3.28\nUtterance\n2.44\n1.73\n2.73\n2.32\n2.62\n2.92\n2.93\n2.99\nTable 11: Fine-grained results of role-playing with CoT on CharacterEval benchmark (part 1).\nMetrics\nGemma-2\n-2B-it\nGemma-2\n-9B-it\nGemma-2\n-27B-it\nLlama-3.2-1B\n-Instruct\nLlama-3.2-3B\n-Instruct\nLlama-3.1-8B\n-Instruct\nLlama-3.3-70B\n-Instruct\nMistral-8B\n-Instruct-2410\nMistral-7B\n-Instruct-v0.3\nAccuracy\n2.63\n2.88\n2.91\n2.52\n2.51\n2.90\n3.07\n2.87\n2.57\nBehavior\n1.28\n1.11\n1.14\n1.66\n1.20\n2.01\n1.34\n1.23\n1.39\nCoherence\n3.11\n3.38\n3.50\n2.71\n2.79\n3.38\n3.58\n3.43\n2.86\nCommunication Skills\n1.88\n2.23\n2.33\n2.06\n1.96\n2.84\n2.91\n2.71\n2.39\nConsistency\n2.90\n3.19\n3.30\n2.22\n2.46\n2.98\n3.20\n3.03\n2.37\nDiversity\n1.29\n1.18\n1.20\n1.50\n1.22\n1.76\n1.35\n1.26\n1.35\nEmpathy\n2.53\n2.77\n2.92\n2.38\n2.41\n2.95\n3.08\n3.01\n2.55\nExposure\n1.44\n1.59\n1.58\n1.77\n1.52\n2.04\n2.01\n1.93\n1.83\nFluency\n2.71\n3.00\n3.20\n2.47\n2.51\n3.06\n3.30\n3.13\n2.59\nHallucination\n2.21\n2.53\n2.60\n2.13\n2.09\n2.63\n2.76\n2.57\n2.15\nHumanlikeness\n3.06\n3.30\n3.35\n2.28\n2.54\n2.89\n3.03\n2.87\n2.32\nUtterance\n2.40\n2.58\n2.66\n2.12\n2.13\n2.60\n2.72\n2.52\n2.18\nTable 12: Fine-grained results of role-playing with CoT on CharacterEval benchmark (part 2).\nMetrics\nOpenAI\no1-mini\nDeepseek\n-R1\nQwQ-32B\n-Preview\nDeepSeek-R1\n-Distill-Llama-8B\nDeepSeek-R1\n-Distill-Qwen-14B\nDeepSeek-R1\n-Distill-Qwen-32B\nDeepSeek-R1\n-Distill-Llama-70B\nAccuracy\n2.45\n2.38\n3.08\n2.65\n2.95\n3.06\n2.96\nBehavior\n1.29\n1.27\n1.64\n1.63\n1.62\n1.31\n1.43\nCoherence\n3.61\n3.19\n3.61\n3.08\n3.46\n3.55\n3.47\nCommunication Skills\n2.42\n2.31\n2.94\n2.40\n2.93\n2.83\n2.79\nConsistency\n2.87\n3.02\n3.31\n2.68\n3.15\n3.25\n3.12\nDiversity\n1.19\n1.20\n1.53\n1.49\n1.52\n1.33\n1.40\nEmpathy\n2.98\n3.21\n3.14\n2.61\n3.03\n3.08\n2.97\nExposure\n1.94\n2.08\n2.09\n1.88\n2.06\n2.01\n1.99\nFluency\n3.44\n2.29\n3.30\n2.77\n3.23\n3.23\n3.10\nHallucination\n2.81\n2.11\n2.85\n2.35\n2.76\n2.78\n2.68\nHumanlikeness\n3.21\n2.69\n3.15\n2.68\n3.01\n3.09\n2.99\nUtterance\n2.54\n2.12\n2.76\n2.36\n2.66\n2.71\n2.62\nTable 13: Fine-grained results of various reasoning-enhanced models on CharacterEval.\n",
  "metadata": {
    "source_path": "papers/arxiv/Reasoning_Does_Not_Necessarily_Improve_Role-Playing_Ability_d41fa9cf489a3141.pdf",
    "content_hash": "d41fa9cf489a31412a3f79f7d242a1db7dee1723e67a711b31b02824fadab007",
    "arxiv_id": null,
    "title": "Reasoning_Does_Not_Necessarily_Improve_Role-Playing_Ability_d41fa9cf489a3141",
    "author": "",
    "creation_date": "D:20250225023144Z",
    "published": "2025-02-25T02:31:44",
    "pages": 14,
    "size": 461403,
    "file_mtime": 1740470203.444341
  }
}