{
  "text": "LettuceDetect: A Hallucination Detection Framework for RAG\nApplications\nÁdám Kovács1, Gábor Recski1,2\n1 KR Labs\n2 TU Wien\nlastname@krlabs.eu\nfirstname.lastname@tuwien.ac.at\nAbstract\nRetrieval-Augmented Generation (RAG) sys-\ntems remain vulnerable to hallucinated an-\nswers despite incorporating external knowl-\nedge sources. We present LettuceDetect,\na framework that addresses two critical lim-\nitations in existing hallucination detection\nmethods: (1) the context window constraints\nof traditional encoder-based methods, and\n(2) the computational inefficiency of LLM-\nbased approaches.\nBuilding on Modern-\nBERT’s extended context capabilities (up to\n8k tokens) and trained on the RAGTruth\nbenchmark dataset, our approach outperforms\nall previous encoder-based models and most\nprompt-based models, while being approxi-\nmately 30 times smaller than the best models.\nLettuceDetect is a token-classification\nmodel that processes context-question-answer\ntriples, allowing for the identification of unsup-\nported claims at the token level. Evaluations\non the RAGTruth corpus demonstrate an F1\nscore of 79.22% for example-level detection,\nwhich is a 14.8% improvement over Luna, the\nprevious state-of-the-art encoder-based archi-\ntecture. Additionally, the system can process\n30 to 60 examples per second on a single GPU,\nmaking it more practical for real-world RAG\napplications.\n1\nIntroduction\nLarge Language Models (LLMs) have made sig-\nnificant progress in recent years in terms of their\nperformance (OpenAI et al., 2024; Grattafiori et al.,\n2024; Team et al., 2024). However, the biggest ob-\nstacle to their usage in real-world applications is\ntheir tendency to hallucinate (Kaddour et al., 2023;\nHuang et al., 2025). Retrieval-Augmented Gener-\nation (RAG) is a method that enhances LLMs by\nsupporting answers with context documents and re-\ntrieving knowledge from external sources, prompt-\ning the LLMs to ground their responses based on\nthis information (Gao et al., 2024). This technique\nis widely used to minimize hallucinations of LLMs.\nDespite the incorporation of context documents in\nRAG, LLMs continue to experience hallucinations\n(Niu et al., 2024).\nHallucinations are defined as outputs that are\nnonsensical, factually incorrect, or inconsistent\nwith the provided evidence (Ji et al., 2023). Ji\net al. (2023) categorizes these errors into two types:\nIntrinsic hallucinations, which arise from the\nmodel’s inherent knowledge, and Extrinsic hal-\nlucinations, which occur when responses fail to\nbe grounded in the provided context, such as in\nthe case of RAG hallucinations (Niu et al., 2024).\nWhile RAG can mitigate intrinsic hallucinations by\ngrounding LLMs in external knowledge, extrinsic\nhallucinations persist due to imperfect retrieval pro-\ncesses or the model’s tendency to prioritize its in-\ntrinsic knowledge over external context (Sun et al.,\n2025), leading to factual contradictions. As LLMs\nremain prone to hallucinations, their utilization in\nhigh-risk settings, such as medical or legal fields,\nmay be jeopardized (Lozano et al., 2023; Magesh\net al., 2024).\nWe present LettuceDetect, a hallucination\ndetection framework that utilizes ModernBERT\n(Warner et al., 2024). Our approach trains a token-\nclassification model to predict whether a token is\nsupported by context documents and a question,\ndetermining if it is hallucinated. We frame this\ntask as predicting tokens in the answers generated\nby large language models (LLMs), based on the\nprovided context documents and the posed ques-\ntion. Our models are trained using the RAGTruth\ndataset (Niu et al., 2024). The architecture we\nemploy is similar to Luna (Belyi et al., 2025), as\nwe train an encoder-based model for this task. A\ndemonstration of our web application is displayed\nin Figure 1.\narXiv:2502.17125v1  [cs.CL]  24 Feb 2025\n\nAll components of our system are released under\nan MIT license and can be accessed on GitHub1\nand via pip by installing the lettucedetect2\npackage.\nThe trained models are published on Hugging\nFace also under MIT licenses. We have made avail-\nable both a large model 3 and a base model 4.\nWe believe our contribution will be valuable to\nthe community, particularly since many effective\nhallucination detection methods are either under\nnon-permissive licenses or depend on larger LLM-\nbased models.\nThe remainder of this paper is structured as fol-\nlows: Section 2 reviews recent methods for halluci-\nnation detection. Section 4 details our method for\ntraining an encoder-based hallucination detection\nmodel built on ModernBERT. Section 5 presents\nour findings on the example and span-level tasks\nusing the RAGTruth dataset.\n2\nRelated work\nModernBERT\nBERT (Devlin et al., 2019) was\none of the first major successes of applying the\nTransformer architecture (Vaswani et al., 2017) to\nnatural language understanding. BERT uses only\nthe Transformer’s encoder blocks in a bidirectional\nfashion, allowing it to learn context from both di-\nrections. As a result, BERT quickly became the\nbackbone of many NLP pipelines for tasks like\nclassification, question answering, named entity\nrecognition, etc.\nBERT’s initial design included certain limita-\ntions, such as a maximum sequence length of 512\ntokens and less efficient attention mechanisms,\nleaving room for architectural upgrades and larger-\nscale training. Despite the current rise of popu-\nlarity of LLM-based architectures in NLP, such\nas GPT-4 (OpenAI et al., 2024), Mistral (Jiang\net al., 2023) or Llama-3 (Grattafiori et al., 2024),\nencoder-based models are still widely used in many\napplications, because of their much smaller size\nand better-suited inference requirements that make\nthem suitable for real-world applications.\nModernBERT (Warner et al., 2024) is a state-of-\n1https://github.com/KRLabsOrg/\nLettuceDetect\n2https://pypi.org/project/\nlettucedetect/\n3https://huggingface.co/KRLabsOrg/\nlettucedect-large-modernbert-en-v1\n4https://huggingface.co/KRLabsOrg/\nlettucedect-base-modernbert-en-v1\nFigure 1: A web demo of our application built in Stream-\nlit5. It features three input fields: question, context, and\nanswer. The output shows the highlighted hallucinated\nspans.\nthe-art encoder-only transformer architecture that\nincorporates several modern design improvements\nover the original BERT model. It utilizes several\nenhancements, including rotary positional embed-\ndings (RoPE) (Su et al., 2024) instead of traditional\nabsolute positional embeddings. Additionally, it\nfeatures an alternating local-global attention mech-\nanism as described in (Team et al., 2024), allowing\nit to efficiently manage sequences of up to 8,192\ntokens. This makes it significantly more effective\nfor long-context tasks, such as modern informa-\ntion retrieval (Nussbaum et al., 2025; Zhang et al.,\n2024). ModernBERT features a hardware-aware\ndesign and an expanded training corpus of 2 tril-\nlion tokens, including textual and code data. As a\nresult, it achieves superior performance on various\ndownstream benchmarks, such as GLUE for clas-\nsification and BEIR for retrieval (while also main-\ntaining faster inference speed) (Nussbaum et al.,\n2025; Zhang et al., 2024). Based on these findings,\nthe main part of our paper is to use the advance-\nments of ModernBERT in the hallucination detec-\ntion of LLMs in an RAG setting. In this domain,\n\nlong-context awareness is an inevitable feature.\nHallucination Detection\ncan vary in granular-\nity, ranging from example-based detection (which\nassesses if an answer contains hallucinations) to\ntoken, span, or sentence-level detection (Niu et al.,\n2024). The methods for detecting hallucinations\nalso differ based on the techniques employed.\nPrompt-based Techniques\ntypically utilize zero\nor few-shot large language models (LLMs) to iden-\ntify hallucinations in LLM-generated responses.\nFew-shot or fine-tuned evaluation frameworks,\nsuch as RAGAS (Es et al., 2024), Trulens6, and\nARES (Saad-Falcon et al., 2024), have emerged to\nprovide hallucination detection at scale using LLM\njudges. However, real-time prediction remains a\nchallenge for these methods. Other prompt-based\napproaches, like the zero-shot method SelfCheck-\nGPT (Manakul et al., 2023), employ stochastic\nsampling to identify inconsistencies across multi-\nple response variants. Rather than relying on a sin-\ngle prompt, Chainpoll (Friel and Sanyal, 2023) im-\nplements a series of verification steps to detect hal-\nlucinations. Cohen et al. (2023) presents a method\nof cross-examination between two LLMs to un-\ncover inconsistencies. Chang et al. (2024) utilized\nLLM-based classifiers trained on synthetic errors\nto detect both hallucinations and coverage errors\nin LLM-generated responses.\nFine-tuned LLM Judges\napproaches involve\ntraining LLMs on hallucination detection tasks us-\ning specific training data. Niu et al. (2024) not only\nintroduced the RagTruth data but also presented a\nfine-tuned Llama-2-13B LLM, which achieved\nstate-of-the-art performance on their test set, even\nsurpassing larger models like GPT-4. RAG-HAT\n(Song et al., 2024) introduced a novel approach\ncalled Hallucination Aware Tuning (HAT), which\ninvolves training models to generate detection la-\nbels and provide detailed descriptions of identified\nhallucinations. They created a preference dataset\nto facilitate Direct Preference Optimization (DPO)\ntraining. Fine-tuning through DPO results in SOTA\nperformance on the RAGTruth test set.\nEncoder-based Solutions\nfocus on address-\ning computational efficiency constraints through\ndomain-specific adaptations. RAGHalu (Zimmer-\nman et al., 2024) employs a two-tiered encoder\nmodel that utilizes binary classification at each\n6https://www.trulens.org/\nlayer, fine-tuning a Natural Language Inference\n(NLI) model based on DeBERTa (He et al., 2021).\nThe approach most similar to our work is Luna\n(Belyi et al., 2025), which also builds on De-\nBERTa and NLI to create a lightweight long-\ncontext hallucination detection system capable of\nmanaging longer contexts effectively. Luna draws\nconnections between detecting entailment in NLI\ntasks and identifying hallucinations. They fine-\ntuned on a large, cross-domain corpus of question-\nanswering-based RAG samples, with annotations\nprovided by GPT-4. During the inference phase,\nLuna conducts sentence- or token-level checks on\neach model’s response against the retrieved pas-\nsages, effectively flagging unsupported fragments.\nFACTOID (Rawte et al., 2024) introduces a Fac-\ntual Entailment (FE) framework, which represents\na new form of textual entailment aimed at locating\nhallucinations at the token or span level. Other\napproaches, such as ReDeEp (Sun et al., 2025), in-\ntroduce techniques to analyze internal model states\nfor hallucination detection.\n3\nData\nWe trained and evaluated our models using the\nRAGTruth dataset (Niu et al., 2024). RAGTruth\nis the first large-scale benchmark for evaluating\nhallucinations in RAG settings. The dataset con-\ntains 18,000 annotated examples at the span level\nacross three tasks: question answering, data-to-text\ngeneration, and news summarization.\nFor the question answering task, data was sam-\npled from the MS MARCO dataset (Bajaj et al.,\n2018), where each question had up to three cor-\nresponding contexts. The authors then prompted\nLLMs to generate answers based on the retrieved\npassages. In the data-to-text generation task, LLMs\nwere asked to generate reviews for sampled busi-\nnesses from the Yelp Open Dataset (Yelp, 2021).\nFor the news summarization task, randomly se-\nlected documents were taken from the training set\nof the CNN/Daily Mail dataset (See et al., 2017),\nand LLMs were prompted to create summaries.\nFor response generation, various LLMs were\nemployed, including GPT-4-0613 (OpenAI et al.,\n2024), Mistral-7B-Instruct (Jiang et al., 2023), and\nselections from the Llama models, such as Llama-\n2-7b-chat and Llama-2-13B-chat (Grattafiori et al.,\n2024). Each sample in the dataset includes one re-\nsponse from each model, resulting in six responses\nper sample in RAGTruth.\n\nThe entire dataset was annotated by human eval-\nuators, who marked annotations in the responses\nand provided rationales. RAGTruth categorizes\nhallucinations into types such as Evident Conflict,\nSubtle Conflict, Evident Introduction of Baseless\nInformation, and Subtle Introduction of Baseless\nInformation. For our training, we focused solely\non the binary task of recognizing hallucinations\nwithout considering these specific categories. An\nexample from the dataset can be seen in Table 1.\nWe tokenized RAGTruth and found that the\nmean token length is 801 tokens, with a median\nof 741 tokens, a minimum of 194, and a maxi-\nmum of 2632 tokens. This highlights the need\nfor long-context models, such as ModernBERT, to\neffectively detect hallucinations.\nQUESTION\nHow to explain quartiles?\nREFERENCE\nPassage 2: Second quartile (Q2) which is more commonly\nknown as median splits the data in half (50%). Median\ndivides the data into a lower half and an upper half. Third\nquartile (Q3), also known as upper quartile, splits lowest\n75% (or highest 25%) of data\n. ..\nRESPONSE\nThe first quartile (Q1) splits the lowest 25% of the data,\nwhile the second quartile (Q2) splits the data into two equal\nhalves, with the median being the middle value of the lower\nhalf. Finally, the third quartile (Q3) splits the highest 75%\nof the data.\n. ..\nANNOTATION\nSpan: highest 75%\nType: Evident Conflict\nReason: Original: “lowest 75% or highest 25%”\nGenerative: “highest 75%”\nTable 1: An example of RAGTruth data, including ques-\ntion, references, response, and annotations.\n4\nMethod\nWe trained ModernBERT-base and -large vari-\nants as token classifiers on the RAGTruth dataset.\nInput sequences were constructed by concate-\nnating context, question, and answer segments\nusing special tokens ([CLS] for context, [SEP]\nfor separation) and tokenized to a maximum\nlength of 4,096 tokens (in the current version we\nhaven’t utilized ModernBERT’s full 8,192 context\nlength). For handling tokenization, we’ve used\nthe AutoTokenizer (Wolf et al., 2020). Our\nmodels are based solely on the ModernBERT ar-\nchitecture and were not pretrained on the NLI task,\nunlike previous encoder-based architectures.\nThe architecture leveraged Hugging Face’s\nAutoModelForTokenClassification\n(Wolf et al., 2020) with ModernBERT as the\nbackbone, and a classification head on top.\nContext/question tokens were masked (label=-\n100), while answer tokens were labeled as\nFigure 2: The architecture of LettuceDetect. The fig-\nure illustrates an example of a Question, Context, and\nAnswer triplet as input to our architecture. After the tok-\nenization step, the tokens are fed into LettuceDetect for\ntoken-level classification. Tokens from both the ques-\ntion and the context are masked (indicated by the red\nline) for loss calculations. In the output of LettuceDe-\ntect, we provide probabilities for each answer token. If\nthe output type is span-level, we aggregate subsequent\ntokens that are hallucinated for the span-level output.\n0 (supported) or 1 (hallucinated).\nTraining\nused AdamW optimization (Loshchilov and\nHutter, 2019) (learning rate 1 × 10−5, weight\ndecay 0.01) for 6 epochs on an NVIDIA A100\nGPU. For data and batch handling, we’ve used\nPyTorch DataLoader (Paszke et al., 2019)\n(batch size=8, shuffling enabled). We evaluated\nmodels using token-level F1 score, saving the\nbest-performing checkpoint via safetensors.\nDynamic\npadding\nwas\nimplemented\nusing\nDataCollatorForTokenClassification\nto process variable-length sequences efficiently.\nThe final model predicts hallucination probabil-\nities for each answer token, with span-level out-\nputs generated by aggregating consecutive tokens\nexceeding a 0.5 confidence threshold. The best\nmodels are uploaded to huggingface. Our method\ncan be seen in Figure 2. We discuss the results in\nSection 5.\n5\nEvaluation\nWe evaluate our models using the RAGTruth test\ndata across all task types, including question an-\n\nQUESTION ANSWERING\nDATA-TO-TEXT WRITING\nSUMMARIZATION\nOVERALL\nMethod\nPrec.\nRec.\nF1\nPrec.\nRec.\nF1\nPrec.\nRec.\nF1\nPrec.\nRec.\nF1\nPromptgpt-3.5-turbo\n18.8\n84.4\n30.8\n65.1\n95.5\n77.4\n23.4\n89.2\n37.1\n37.1\n92.3\n52.9\nPromptgpt-4-turbo\n33.2\n90.6\n45.6\n64.3\n100.0\n78.3\n31.5\n97.6\n47.6\n46.9\n97.9\n63.4\nSelCheckGPTgpt-3.5-turbo\n35.0\n58.0\n43.7\n68.2\n82.8\n74.8\n31.1\n56.5\n40.1\n49.7\n71.9\n58.8\nLMvLMgpt-4-turbo\n18.7\n76.9\n30.1\n68.0\n76.7\n72.1\n23.2\n81.9\n36.2\n36.2\n77.8\n49.4\nFinetuned Llama-2-13B\n61.6\n76.3\n68.2\n85.4\n91.0\n88.1\n64.0\n54.9\n59.1\n76.9\n80.7\n78.7\nRAG-HAT\n76.5\n73.1\n74.8\n92.9\n90.3\n91.6\n77.7\n59.8\n67.6\n87.3\n80.8\n83.9\nChainPollgpt-3.5-turbo\n33.5\n51.3\n40.5\n84.6\n35.1\n49.6\n45.8\n48.0\n46.9\n54.8\n40.6\n46.7\nRAGAS Faithfulness\n31.2\n41.9\n35.7\n79.2\n50.8\n61.9\n64.2\n29.9\n40.8\n62.0\n44.8\n52.0\nTrulens Groundedness\n22.8\n92.5\n36.6\n66.9\n96.5\n79.0\n40.2\n50.0\n44.5\n46.5\n85.8\n60.4\nLuna\n37.8\n80.0\n51.3\n64.9\n91.2\n75.9\n40.0\n76.5\n52.5\n52.7\n86.1\n65.4\nlettucedetect-base-v1\n60.64\n71.25\n65.52\n89.30\n86.53\n87.89\n53.89\n47.55\n50.52\n76.64\n75.50\n76.07\nlettucedetect-large-v1\n65.93\n75.00\n70.18\n90.45\n86.70\n88.54\n64.04\n55.88\n59.69\n80.44\n78.05\n79.22\nTable 2: Performance comparison at the example level across various tasks. We compare our results with models\npresented in Luna (Belyi et al., 2025) and RAGTruth (Niu et al., 2024), as well as evaluation frameworks RAGAS\nand Trulens. The evaluation also includes a fine-tuned LLM from the RAG-HAT (Song et al., 2024) paper.\nQUESTION ANSWERING\nDATA-TO-TEXT WRITING\nSUMMARIZATION\nOVERALL\nMethod\nPrec.\nRec.\nF1\nPrec.\nRec.\nF1\nPrec.\nRec.\nF1\nPrec.\nRec.\nF1\nPrompt Baselinegpt-3.5-turbo\n7.9\n25.1\n12.1\n8.7\n45.1\n14.6\n6.1\n33.7\n10.3\n7.8\n35.3\n12.8\nPrompt Baselinegpt-4-turbo\n23.7\n52.0\n32.6\n17.9\n66.4\n28.2\n14.7\n65.4\n24.3\n18.4\n60.9\n28.3\nFinetuned Llama-2-13B\n55.8\n60.8\n58.2\n56.5\n50.7\n53.5\n52.4\n30.8\n38.6\n55.6\n50.2\n52.7\nlettucedetect-base-v1\n62.65\n60.40\n61.50\n58.24\n56.57\n57.39\n52.98\n28.08\n36.71\n59.36\n52.01\n55.44\nlettucedetect-large-v1\n66.85\n62.14\n64.41\n64.71\n55.99\n60.04\n60.17\n35.47\n44.63\n64.92\n53.96\n58.93\nTable 3: Performance comparison at the span level across different tasks. We compare our results with models\npresented in RAGTruth (Niu et al., 2024). We limit this comparison to these papers, as other studies have not\nevaluated their performance on the span level task.\nswering (QA), data-to-text, and summarization.\nFollowing the methodology outlined in (Niu et al.,\n2024), we report both example-level and span-level\ndetection performance, reporting precision, recall,\nand F1 score. Our models are compared against\nstate-of-the-art baselines presented in (Niu et al.,\n2024; Song et al., 2024; Belyi et al., 2025). This\nincludes comparisons with prompt-based methods,\nsuch as gpt-4-turbo and gpt-3.5-turbo, as well as\nfine-tuned LLMs that have shown state-of-the-art\nperformance on the RAGTruth data, including the\npreviously established state-of-the-art model in\n(Niu et al., 2024) (a fine-tuned Llama-2-13B) and\nthe current best result from (Song et al., 2024)\n(a fine-tuned LLM based on Llama-3-8B trained\nthrough DPO training). We also compare our mod-\nels with encoder-based approaches, similar to ours,\nincluding the token classifier method presented in\n(Belyi et al., 2025), which is based on DeBERTa.\nTable 2 illustrates our results on the example-\nlevel task. Our large model (lettucedetect-large-\nv1) outperforms all prompt-based methods (gpt-\n4-turbo achieved an overall F1 score of 63.4%\ncompared to lettucedetect-large-v1’s 79.22%). It\nalso surpasses the previous state-of-the-art encoder-\nbased model, Luna (65.4% vs. 79.22%), and the\npreviously established state-of-the-art fine-tuned\nLLM presented in (Niu et al., 2024) (fine-tuned\nLlama-2-13B with 78.7% vs. 79.22%). The only\nmodel that exceeds our large model’s performance\nis the current state-of-the-art fine-tuned LLM based\non Llama-3-8B presented in the RAG-HAT paper\n(Song et al., 2024) (83.9% vs. 79.22%). Our\nbase model (lettucedetect-base-v1) also demon-\nstrates strong performance across tasks while being\nhalf the size of the large model. Considering our\nmodel’s compact size (150M for the base model\nand 396M for the large model) and its optimized\narchitecture based on ModernBERT, it is capable\nof processing approximately 30 to 60 examples\nper second on a single GPU. Given this optimized\ninference speed, it only falls short compared to one\nlarger model (8B Llama). Overall, our models are\nhighly efficient while being about 30 times smaller\nin size.\nIn Table 3, we present our results on the span-\nlevel task. In this task, we evaluate the overlap\nbetween the gold spans and the predicted spans.\nFollowing the RAGTruth paper, we measured\ncharacter-level overlap and calculated precision,\nrecall, and F1 score. Our models achieved state-of-\nthe-art performance, with the Llama-2-13B model\nreaching an overall F1 score of 52.7%, while our\nlarge model achieved 58.93% F1 score. Please\n\nnote that we were unable to compare our results\nwith RAG-HAT on this task because they did not\nmeasure at this level. Additionally, RAGTruth did\nnot include this evaluation in their published code,\nso we relied on our own implementation for this\nanalysis.\n6\nConclusion\nWe present LettuceDetect, a lightweight and\nefficient framework for hallucination detection in\nRAG systems. By leveraging ModernBERT’s long-\ncontext capabilities, our baseline models achieve\nstrong performance on the RAGTruth benchmark\nwhile remaining highly efficient in inference set-\ntings. This work serves as a foundation for our\nfuture research, where we plan to expand the\nframework to include more datasets, additional\nlanguages, and enhanced architectures. Even in\nits current form, LettuceDetect demonstrates that\neffective hallucination detection can be achieved\nwith lean, purpose-built models.\n\nReferences\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder,\nAndrew McNamara, Bhaskar Mitra, Tri Nguyen,\nMir Rosenberg, Xia Song, Alina Stoica, Saurabh\nTiwary, and Tong Wang. 2018. Ms marco: A human\ngenerated machine reading comprehension dataset.\nPreprint, arXiv:1611.09268.\nMasha Belyi, Robert Friel, Shuai Shao, and Atindriyo\nSanyal. 2025. Luna: A lightweight evaluation model\nto catch language model hallucinations with high\naccuracy and low cost. In Proceedings of the 31st\nInternational Conference on Computational Linguis-\ntics: Industry Track, pages 398–409, Abu Dhabi,\nUAE. Association for Computational Linguistics.\nTyler A. Chang, Katrin Tomanek, Jessica Hoffmann,\nNithum Thain, Erin MacMurray van Liemt, Kathleen\nMeier-Hellstern, and Lucas Dixon. 2024. Detecting\nhallucination and coverage errors in retrieval aug-\nmented generation for controversial topics. In Pro-\nceedings of the 2024 Joint International Conference\non Computational Linguistics, Language Resources\nand Evaluation (LREC-COLING 2024), pages 4729–\n4743, Torino, Italia. ELRA and ICCL.\nRoi Cohen, May Hamri, Mor Geva, and Amir Glober-\nson. 2023. LM vs LM: Detecting factual errors via\ncross examination. In Proceedings of the 2023 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 12621–12640, Singapore. Associ-\nation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nShahul Es, Jithin James, Luis Espinosa Anke, and\nSteven Schockaert. 2024. RAGAs: Automated evalu-\nation of retrieval augmented generation. In Proceed-\nings of the 18th Conference of the European Chapter\nof the Association for Computational Linguistics:\nSystem Demonstrations, pages 150–158, St. Julians,\nMalta. Association for Computational Linguistics.\nRobert Friel and Atindriyo Sanyal. 2023. Chainpoll: A\nhigh efficacy method for llm hallucination detection.\nPreprint, arXiv:2310.18344.\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,\nJinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang,\nand Haofen Wang. 2024. Retrieval-augmented gener-\nation for large language models: A survey. Preprint,\narXiv:2312.10997.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,\nAbhinav Pandey, Abhishek Kadian, Ahmad Al-\nDahle, Aiesha Letman, Akhil Mathur, Alan Schelten,\nAlex Vaughan, Amy Yang, Angela Fan, Anirudh\nGoyal, Anthony Hartshorn, Aobo Yang, Archi Mi-\ntra, Archie Sravankumar, Artem Korenev, Arthur\nHinsvark, and 542 others. 2024. The llama 3 herd of\nmodels. Preprint, arXiv:2407.21783.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu\nChen.\n2021.\nDeberta:\nDecoding-\nenhanced bert with disentangled attention. Preprint,\narXiv:2006.03654.\nLei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,\nZhangyin Feng, Haotian Wang, Qianglong Chen,\nWeihua Peng, Xiaocheng Feng, Bing Qin, and Ting\nLiu. 2025. A survey on hallucination in large lan-\nguage models: Principles, taxonomy, challenges, and\nopen questions. ACM Transactions on Information\nSystems, 43(2):1–55.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput.\nSurv., 55(12).\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel,\nGuillaume Lample, Lucile Saulnier, Lélio Re-\nnard Lavaud, Marie-Anne Lachaux, Pierre Stock,\nTeven Le Scao, Thibaut Lavril, Thomas Wang, Tim-\nothée Lacroix, and William El Sayed. 2023. Mistral\n7b. Preprint, arXiv:2310.06825.\nJean Kaddour, Joshua Harris, Maximilian Mozes, Her-\nbie Bradley, Roberta Raileanu, and Robert McHardy.\n2023. Challenges and applications of large language\nmodels. Preprint, arXiv:2307.10169.\nIlya Loshchilov and Frank Hutter. 2019.\nDe-\ncoupled weight decay regularization.\nPreprint,\narXiv:1711.05101.\nAlejandro Lozano, Scott L Fleming, Chia-Chun Chiang,\nand Nigam Shah. 2023. Clinfo.ai: An open-source\nretrieval-augmented large language model system for\nanswering medical questions using scientific litera-\nture. Preprint, arXiv:2310.16146.\nVarun Magesh, Faiz Surani, Matthew Dahl, Mirac Suz-\ngun, Christopher D. Manning, and Daniel E. Ho.\n2024.\nHallucination-free? assessing the reliabil-\nity of leading ai legal research tools.\nPreprint,\narXiv:2405.20362.\nPotsawee Manakul, Adian Liusie, and Mark Gales.\n2023. SelfCheckGPT: Zero-resource black-box hal-\nlucination detection for generative large language\nmodels. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 9004–9017, Singapore. Association for\nComputational Linguistics.\nCheng Niu, Yuanhao Wu, Juno Zhu, Siliang Xu,\nKaShun Shum, Randy Zhong, Juntong Song, and\n\nTong Zhang. 2024. RAGTruth: A hallucination cor-\npus for developing trustworthy retrieval-augmented\nlanguage models. In Proceedings of the 62nd An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 10862–\n10878, Bangkok, Thailand. Association for Compu-\ntational Linguistics.\nZach Nussbaum, John X. Morris, Brandon Duderstadt,\nand Andriy Mulyar. 2025. Nomic embed: Training a\nreproducible long context text embedder. Preprint,\narXiv:2402.01613.\nOpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,\nLama Ahmad, Ilge Akkaya, Florencia Leoni Ale-\nman, Diogo Almeida, Janko Altenschmidt, Sam Alt-\nman, Shyamal Anadkat, Red Avila, Igor Babuschkin,\nSuchir Balaji, Valerie Balcom, Paul Baltescu, Haim-\ning Bao, Mohammad Bavarian, Jeff Belgum, and\n262 others. 2024. Gpt-4 technical report. Preprint,\narXiv:2303.08774.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Köpf, Edward\nYang, Zach DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nand 2 others. 2019. Pytorch: An imperative style,\nhigh-performance deep learning library. Preprint,\narXiv:1912.01703.\nVipula Rawte, S. M Towhidul Islam Tonmoy, Krishnav\nRajbangshi, Shravani Nag, Aman Chadha, Amit P.\nSheth, and Amitava Das. 2024. Factoid: Factual\nentailment for hallucination detection.\nPreprint,\narXiv:2403.19113.\nJon Saad-Falcon, Omar Khattab, Christopher Potts, and\nMatei Zaharia. 2024. Ares: An automated evalua-\ntion framework for retrieval-augmented generation\nsystems. Preprint, arXiv:2311.09476.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1073–\n1083, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nJuntong Song, Xingguang Wang, Juno Zhu, Yuanhao\nWu, Xuxin Cheng, Randy Zhong, and Cheng Niu.\n2024.\nRAG-HAT: A hallucination-aware tuning\npipeline for LLM in retrieval-augmented generation.\nIn Proceedings of the 2024 Conference on Empirical\nMethods in Natural Language Processing: Indus-\ntry Track, pages 1548–1558, Miami, Florida, US.\nAssociation for Computational Linguistics.\nJianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan,\nWen Bo, and Yunfeng Liu. 2024. Roformer: En-\nhanced transformer with rotary position embedding.\nNeurocomputing, 568:127063.\nZhongxiang Sun, Xiaoxue Zang, Kai Zheng, Yang Song,\nJun Xu, Xiao Zhang, Weijie Yu, Yang Song, and\nHan Li. 2025. Redeep: Detecting hallucination in\nretrieval-augmented generation via mechanistic in-\nterpretability. Preprint, arXiv:2410.11414.\nGemma Team, Morgane Riviere, Shreya Pathak,\nPier Giuseppe Sessa, Cassidy Hardin, Surya Bhupati-\nraju, Léonard Hussenot, Thomas Mesnard, Bobak\nShahriari, Alexandre Ramé, Johan Ferret, Peter Liu,\nPouya Tafti, Abe Friesen, Michelle Casbon, Sabela\nRamos, Ravin Kumar, Charline Le Lan, Sammy\nJerome, and 179 others. 2024. Gemma 2: Improving\nopen language models at a practical size. Preprint,\narXiv:2408.00118.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nBenjamin Warner, Antoine Chaffin, Benjamin Clavié,\nOrion Weller, Oskar Hallström, Said Taghadouini,\nAlexis Gallagher, Raja Biswas, Faisal Ladhak, Tom\nAarsen, Nathan Cooper, Griffin Adams, Jeremy\nHoward, and Iacopo Poli. 2024.\nSmarter, better,\nfaster, longer: A modern bidirectional encoder for\nfast, memory efficient, and long context finetuning\nand inference. Preprint, arXiv:2412.13663.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, and 3 others. 2020. Hugging-\nface’s transformers: State-of-the-art natural language\nprocessing. Preprint, arXiv:1910.03771.\nYelp. 2021. Yelp open dataset. Accessed: 2023-11-03.\nXin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie,\nZiqi Dai, Jialong Tang, Huan Lin, Baosong Yang,\nPengjun Xie, Fei Huang, Meishan Zhang, Wenjie\nLi, and Min Zhang. 2024. mGTE: Generalized long-\ncontext text representation and reranking models for\nmultilingual text retrieval. In Proceedings of the\n2024 Conference on Empirical Methods in Natural\nLanguage Processing: Industry Track, pages 1393–\n1412, Miami, Florida, US. Association for Computa-\ntional Linguistics.\nIlana Zimmerman, Jadin Tredup, Ethan Selfridge, and\nJoseph Bradley. 2024. Two-tiered encoder-based hal-\nlucination detection for retrieval-augmented genera-\ntion in the wild. In Proceedings of the 2024 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing: Industry Track, pages 8–22, Miami, Florida,\nUS. Association for Computational Linguistics.\n",
  "metadata": {
    "source_path": "papers/arxiv/LettuceDetect_A_Hallucination_Detection_Framework_for_RAG_Applications_937d2c1ed962da80.pdf",
    "content_hash": "937d2c1ed962da80e676724e9e04db384d5dcd69f16071c1cfc5df8783e36cd3",
    "arxiv_id": null,
    "title": "LettuceDetect_A_Hallucination_Detection_Framework_for_RAG_Applications_937d2c1ed962da80",
    "author": "",
    "creation_date": "D:20250225024719Z",
    "published": "2025-02-25T02:47:19",
    "pages": 8,
    "size": 936654,
    "file_mtime": 1740470181.0188446
  }
}