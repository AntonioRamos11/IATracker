{
  "text": "Order Matters: Investigate the Position Bias in Multi-constraint\nInstruction Following\nJie Zeng1, Qianyu He1, Qingyu Ren1,3, Jiaqing Liang2†, Yanghua Xiao1†\nWeikang Zhou3, Zeye Sun3, Fei Yu3\n1Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\n2School of Data Science, Fudan University 3Ant Group\n{jzeng23, qyhe21, qyren24}@m.fudan.edu.cn, {liangjiaqing, shawyh}@fudan.edu.cn\nAbstract\nReal-world instructions with multiple con-\nstraints pose a significant challenge to existing\nlarge language models (LLMs). An observa-\ntion is that the LLMs exhibit dramatic perfor-\nmance fluctuation when disturbing the order of\nthe incorporated constraints. Yet, none of the\nexisting works has systematically investigated\nthis position bias problem in the field of multi-\nconstraint instruction following. To bridge this\ngap, we design a probing task where we quan-\ntitatively measure the difficulty distribution of\nthe constraints by a novel Difficulty Distribu-\ntion Index (CDDI). Through the experimental\nresults, we find that LLMs are more perfor-\nmant when presented with the constraints in\na “hard-to-easy” order. This preference can\nbe generalized to LLMs with different archi-\ntecture or different sizes of parameters. Ad-\nditionally, we conduct an explanation study,\nproviding an intuitive insight into the corre-\nlation between the LLM’s attention and con-\nstraint orders. Our code and dataset are publicly\navailable at https://github.com/meowpass/\nPBIF.\n1\nIntroduction\nLarge language models (LLMs) have made im-\npressive progress in massive natural language\ntasks (Wan et al., 2024; Zhang et al., 2024b) and\nhave been applied to various real-world scenar-\nios (Bai et al., 2023; Bi et al., 2024). To achieve\nsatisfactory performance, it is crucial for LLMs\nto understand the user’s instructions and convey\ndesired outputs, which is known as the Instruction\nFollowing capacity of LLM (Yin et al., 2023; Xu\net al., 2024).\nIn practice, instructions are usually incorporated\nwith multiple constraints of different types, e.g.,\nformat constraint which limits the model’s output\nto a specific format. Nevertheless, existing LLMs\noften struggle to follow multi-constraint instruc-\ntions, making multi-constraint instruction follow-\nMake a short introduction and list a few popular songs from the album: Back To \nBlack. There should be exactly two paragraphs in your response. Do not say the \nword \"popular\" in the response and answer in lowercase letters only. The \nresponse should end with the phrase \"really love their song!\".\n1\n2\n3\n4\nSeed Inst\n(a) Single-round Inference \nSeed Inst\n3\n4\n2\n1\n2\namy winehouse's album back to black was \nreleased in 2006, marking a defining \nmoment in modern r&b and jazz music. …\\n\nsome of the standout tracks from the album \ninclude \"rehab,“…fans of the album really \nlove their song!\n Multi-Constraint Instruction\n  amy winehouse’s Back to Black is a timeless \nalbum that showcases her unique blend of \njazz, and R&B. released in 2006. Some of the \npopular tracks from the album include \n\"rehab,\" \"you know i'm no good,\"  … many \nfans really love their song!\n(b) Multi-round Inference \nSeed Inst\namy winehouse's back to \nblack is …these popular \nsongs …. fans really love \ntheir song!\n＋\n＋\n＋＋\nDifferent Constraint Order\n…\n…\nSeed Inst\n3\n4\n2\n1\n＋\n＋＋\n＋\nSeed Inst\n3\n4\n2\n1\n＋\n＋\n＋＋\nSeed Inst\n3\n4\n2\n1\n＋\n＋\n＋\n＋\n…\n4\n1\n2\nSeed Inst\n…\n…\n4\n1\n2\namy winehouse's back to \nblack is …\\n\ntracks like \"rehab“ … fans \nreally love their song!\nFigure 1: (a) In single-round inference, the LLM per-\nforms differently when handling the same instruction\nwith different constraint orders. (b) In multi-round in-\nference, the latter response is evitably affected by the\nformer context.\ning an obstacle to hinder LLMs’ real-world appli-\ncation (Wen et al., 2024; Yin et al., 2023).\nRecently, a lot of works have demonstrated that\nLLMs are sensitive to the position of the referred\ncontext in many tasks, such as multi-document\nquestion answering, text evaluation, and list-wise\nranking (Liu et al., 2024; Zheng et al., 2023; Tang\net al., 2024). Since there are usually multiple con-\nstraints coexisting in the complex instruction, the\nposition bias problem is also significant in multi-\nconstraint instructions. As shown in Fig. 1, in\nthe single-round scenario, the LLM’s performance\nvaries significantly when presented with instruc-\ntions that have different constraint orders, even\nthough the two instructions are semantically iden-\ntical. When it comes to the multi-round scenario,\ndifferent constraint orders impose different impacts\n1\narXiv:2502.17204v1  [cs.CL]  24 Feb 2025\n\nMulti-constraint Instruction Synthesis\nSeed Inst\nConstraint Reordering\nSingle-round \ninference\n3\n4\n2\nConstraint Taxonomy\nSource Data\nLength\nFormat\nKeyword\nInclude\n…\n…\nSeed Sampling\nConstraints Sampling\nSample\nDifficulty Ranking\nAnchor\nCDDI=1\nCDDI=0.6\nCDDI=-0.3\nCDDI=-1\nHard-to-easy\nEasy-to-hard\nThere should be two paragraphs…\nDo not say the word “popular”…\nanswer in lowercase letters only…\nend with the phrase “really love…\nSequential-sensitive Inference \n2\n3\n1\n4\nExclude\nend with the phrase \n\"really love their song!\"\n4\n2\n3\n1\n2\n4\n1\n1\n3\n1\n2\n3 4\nConstraint \nSeed Inst\n1\n2\n3\n4\n＞＞＞\nSeed Inst\n3\n2\n1\n4\n1\n2\n3 4\nSeed Inst\nSeed Inst\n…\nSeed Inst\nSeed Inst\nSeed Inst\nSingle-round \ninference\n4\n \nFigure 2: The procedure of the probing task. First, we synthesize the initial instructions by sampling seed instructions\nand corresponding constraints. Then, we obtain instructions with different constraint orders by reordering the\nincorporated constraints. Finally, we conduct model inference on single and multi-round settings.\non the intermediate responses, thus inevitably lead-\ning to a discrepancy in the quality of the final re-\nsponses.\nNevertheless, the position bias of constraint or-\nders in the multi-constraint instruction following\nremains an under-explored problem. Existing work\nmanually assigns difficulty to different constraints\nbased on a predefined rule and orders the con-\nstraints according to their difficulty. They empiri-\ncally demonstrate the existence of LLMs’ perfor-\nmance fluctuation brought by different constraint\norder (Chen et al., 2024). However, on the one\nhand, handcraft difficulty categorization fails to\nreflect the real difficulty disparity of different con-\nstraints (Dentella et al., 2024; Srivastava et al.,\n2023). On the other hand, they merely analyze\nthe constraint order in a qualitative way, lacking\na quantitative metric to measure the disparity of\nconstraint order. Additionally, none of the existing\nworks has provided an intuitive explanation for the\nposition bias in multi-constraint instructions. It re-\nmains unclear how the LLMs handle instructions\nwith different constraint orders.\nTo address all the problems above, we systemat-\nically investigate the position bias problem in the\nmulti-constraint instructions. First, we propose a\nnovel metric called the Constraint Difficulty Distri-\nbution Index (CDDI) to quantitatively describe the\ndisparity of constraint order from the perspective\nof constraint difficulty. We leverage the accuracy\nof the LLM to quantify the difficulty of different\nconstraints, thus precisely reflecting their disparity.\nThen, for a thorough study of the position bias prob-\nlem, we design a probing task. As shown in Fig. 2,\nwe construct a large number of multi-constraint in-\nstances with different constraint orders and explore\ntwo practical scenarios: single-round inference and\nmulti-round inference. Our experiments find ex-\nisting LLMs commonly perform better with the\n“hard-to-easy” constraint orders, i.e., possibly plac-\ning harder constraints in former positions. Finally,\nto make an intuitive explanation of our findings, we\nresort to a gradient-based method (Wu et al., 2023).\nWe visualize the importance of different constraints\nlocated in different positions. We observe that the\nconstraint order will affect how the LLM handle the\nconstraints and is highly correlated to the LLM’s\nperformance on a specific constraint.\nIn summary, our main contributions are as fol-\nlows: (1) We are the first to systematically investi-\n2\n\ngate the position bias problem in multi-constraint\ninstruction following.\n(2) We propose a novel\nCDDI metric to quantify the disparity of different\nconstraint orders in the multi-constraint instruc-\ntions. (3) Through extensive experiments, we find\nthat existing LLMs can achieve a better perfor-\nmance when presented with constraints in “hard-\nto-easy” orders. This finding can be generalized\nin both single-round and multi-round scenarios, re-\ngardless of the architecture of LLM, the size of\nLLM’s parameters and the number of constraints.\n(4) Our explanation study explores how the LLMs\nassign attention when provided with instructions in\ndifferent constraint orders and demonstrates the sig-\nnificant correlation between the attention patterns\nand the LLMs’ performance on specific constraints.\n2\nRelated Work\n2.1\nComplex Instruction Following\nRiding on the wave of the large language model, the\ninstruction following has attracted increasing atten-\ntion for it is easy to be perceived by the users (Zhou\net al., 2023a; Lou et al., 2024). Practical instruc-\ntions are complex, usually incorporated with mul-\ntiple constraints of different types (Zhou et al.,\n2023b; He et al., 2024). A lot of evaluation bench-\nmarks have found that multi-constraint instruction\nfollowing is nontrivial for the LLMs (Jiang et al.,\n2023b; Wen et al., 2024; Qin et al., 2024). Con-\nsequently, several works propose to improve the\nLLM’s complex instruction following capacity by\nintroducing additional instruction fine-tuning (Sun\net al., 2024; Cheng et al., 2024; Zhang et al.,\n2024a).\nDifferent from these works, we focus on the\ninference stage of the LLMs instead of model train-\ning. Especially, we aim to investigate the posi-\ntion bias problem brought by the constraint order,\nwhich poses an essential impact on the model per-\nformance.\n2.2\nPosition Bias in the LLM\nThe position bias problem is common in the var-\nious LLM tasks (Liu et al., 2024; Zheng et al.,\n2023; Zeng et al., 2023). Researchers fisrt find\nthat the LLM’s performance degrades dramatically\nby merely changing the order of relevant informa-\ntion in the long-context question answering. A lot\nof works have studied the position bias problem\nin the field of logical reasoning (Chen et al.; Liu\net al., 2023; Berglund et al., 2023). They find the\nLLM is sensitive to the order of premises, although\nsuch ordering actually does not alter the reasoning\ntask (Chen et al.; Liu et al., 2023).\nDespite so, none of these works has studied the\nposition bias problem in the field of instruction\nfollowing, especially multi-constraint instruction\nfollowing. SIFo (Chen et al., 2024) is the most re-\nlated work to ours. They manually differentiate the\nconstraints based on the context length they will\ninfluence and conduct an empirical study to verify\nwhether the model performance will be affected by\nthe constraint order. However, Their investigation\nof position bias is fairly qualitative. Different from\nthem, we are the first to make a systematical and\nthorough investigation on the position bias of con-\nstraints in multi-constraint instruction following.\n3\nMethod\n3.1\nBackground\nIn this paper, we mainly focus on the multi-\nconstraint instruction Ic. It can be formulated as a\nseed instruction incorporated with n constraints:\nIc = Is ⊕C1 ⊕... ⊕Cn,\n(1)\nwhere the seed instructions Is describe a task,\ne.g., write a story, while these constraints Pn\ni=1 Ci\nlimit the output from different aspects, e.g., format,\nlength, content, etc. ⊕stands for the concatenation\noperation.\n3.2\nProbing Task\n3.2.1\nTask Formulation\nTo investigate the impact of constraint order, we\nintroduce a probing task. In this task, the LLM is\ngiven multi-constraint instructions with constraints\narranged in various orders. The LLM’s task is to\ngenerate a response that follows all constraints. We\nevaluate the LLM in two practical scenarios: single-\nround and multi-round inference. The LLM’s re-\nsponses are then evaluated to determine its per-\nformance across various constraints. The overall\nprocedure is illustrated in Fig. 2. In the following\nsections, we will provide a detailed explanation.\n3.2.2\nMulti-constraint Instruction Synthesis\nTo ensure the generalizability of probing data, we\nconstruct the initial multi-constraint instructions\nwhich include a variety of tasks and diverse con-\nstraint combinations. The multi-constraint instruc-\ntion synthesis can be further divided into two parts:\nseed sampling and constraint sampling.\n3\n\nFor the seed sampling, we sample data from\nthree source datasets: (1) Natural Instructions\nV2 (Wang et al., 2022). It is an instruction col-\nlection covering more than 1600 NLP tasks. We\nfilter those tasks that are too easy and could po-\ntentially conflict with complex constraints, e.g.,\nobject classification and sentiment tagging. Then,\nwe randomly sample 52 instructions from the re-\nmaining tasks. (2) Self-Instruct (Wang et al., 2023).\nWe only sample 83 instances from their initial 175\nseed instructions which are formulated by humans.\n(3) Open Assistant (Köpf et al., 2024). Following\nthe strategy of Suri (Li et al., 2023), we filter out\nthe first turn of the conversation with the highest\nquality (marked as rank 0 in the dataset) and sam-\nple 65 instances from them. Overall, we obtain 200\nseed instructions, where the number of instructions\nis denoted as nseed.\nAs for the constraint sampling, we first cate-\ngorize the constraints into 8 groups with 25 fine-\ngrained types (Zhou et al., 2023a). For each type of\nconstraint, we employ 8 different expressions to de-\nscribe it1. Then, we sample n constraints from the\nconstraint taxonomy and use the predefined rules\nto avoid possible conflicts. To ensure diversity,\nwe repeat the sampling process to obtain ncc dis-\ntinct constraint combinations, deriving nseed × ncc\nmulti-constraint instructions.\n3.2.3\nConstraint Reordering\nTo quantitatively construct instructions with differ-\nent constraint orders, here are two questions that\nneed to be answered: (1) How do we distinguish\nthe disparity of different constraints? (2) After\nwe order the constraints based on their disparity,\nhow do we quantitatively describe the disparity of\nconstraint orders?\nAn appropriate solution for the first question is\nto categorize the constraints based on their diffi-\nculty (Chen et al., 2024). In this paper, we also\nsort the constraints based on their difficulty. How-\never, different from existing works which designate\nthe difficulty of the constraints based on handcraft\nrules, we measure the difficulty of a constraint via\nthe overall accuracy of following it in our probing\ndatasets. The formulation is as follows:\nDffCx = Softmax(1 −AccCx),\n(2)\nAccCx = 1\nNx\nNx\nX\ni=1\nci\nx.\n(3)\n1More details are shown in Appx. A.2\nThe Cx refers to a specific type of constraint, the\nNx stands for the total number of instructions cor-\nresponding to the constraint Cx, and the ci\nx is a\nbinary value to reflect whether the constraint Cx is\nfollowed in the ith instruction.\nTo quantitatively describe the disparity of con-\nstraint order, we propose a novel metric called the\nConstraint Difficulty Distribution Index (CDDI)\nwhich quantifies a specific constraint order based\non its difficulty distribution. Given the difficulty\nof different types of constraints, we can readily\nattain the difficulty distribution of the constraints\nincorporated in the multi-constraint instructions.\nSpecifically, for a multi-constraint instruction, we\nrank the incorporated constraints based on their\ndifficulty, from the hardest to the easiest. We set\nthis “hard-to-easy” constraint order as an anchor\nsince it depicts an extreme situation, i.e., we des-\nignate the CDDI = 1 when the constraints fall in\nthis order. Consequently, akin to the Kendall tau\ndistance (Cicirello, 2020), we measure the diffi-\nculty distribution of a specific constraint order o\nby comparing it with the “hard-to-easy” constraint\norder omax. The formula is shown as:\nCDDIo = Ncon −Ndis\nNpair\n= 2(Ncon −Ndis)\nn(n −1)\n. (4)\nwhere Ncon and Ndis represent the number of con-\ncordant and discordant distribution pairs of con-\nstraints between o and omax, respectively. The\nNpair is the total number of compared constraint\npairs. Overall, we select ndd different difficulty\ndistributions, finally comprising nseed × ncc × ndd\ninstances.\n3.2.4\nSequential-Sensitive Inference\nGiven the multi-constraint instructions with dif-\nferent constraint orders, we evaluate the model’s\nperformance in two common scenarios: single-\nround inference and multi-round inference.\nIn\nsingle-round inference, the LLM is directly given\nthe multi-constraint instructions with different con-\nstraint distributions. We argue that different con-\nstraint distributions could impose different levels of\ndifficulty on the LLM to handle. The multi-round\ninference introduces a more typical setting: the\nuser will first provide the LLM with the core in-\ntention (i.e., the seed instruction in this work), and\nthen iteratively put forward the constraints in order\nto obtain a final response.\nTo evaluate the model performance, apart from\nthe constraint following accuracy mentioned in\n4\n\nFigure 3: The statistic of different types of constraints\nin the probing data. The 7cons and 9cons stand for the\nsetting when n=7 and n=9, respectively.\nEq.(3), we also verify its constraint-level accuracy\nAcccons and instruction-level accuracy Accinst.\nCorresponding formulas are shown below:\nAcccons =\n1\nmn\nm\nX\ni=1\nn\nX\nj=1\ncj\ni, Accinst = 1\nm\nm\nX\ni=1\nn\nY\nj=1\ncj\ni.\n(5)\nwhere m and n refer to the number of instructions\nand constraints in the instruction, respectively. Sim-\nilar to Eq.(3), the cj\ni is a binary value which equals\n1 when the constraint is followed in the ith instruc-\ntion. All the evaluation is conducted by leveraging\nthe script introduced in (Zhou et al., 2023a). We\nonly evaluate the final responses produced by the\nLLMs.\n4\nEmpirical Study\n4.1\nExperiment Setup\nModels\nFor our probing task, to ensure the gen-\neralizability of our study, we conduct experiments\non both closed and open-source LLMs with vary-\ning architectures and parameter sizes. Specifically,\nwe introduce the following models: (1) LLaMA3-\n8B-Instruct and LLaMA3-70B-Instruct (Dubey\net al., 2024). (2) LLaMA2-13B-Chat (Touvron\net al., 2023). (3) Mistral-7B-Instruct (Jiang et al.,\n2023a).2 (4) Qwen2.5-7B-Instruct (Yang et al.,\n2024). (5) GPT4o-mini (Achiam et al., 2023).\nDatasets\nWe construct various multi-constraint\ninstructions\nwith\ndifferent\nconstraint\norders\n(Sec.3.2). We empirically set the number of con-\nstraints n to 7. To ensure the diversity and complex-\nity, we set the number of constraint combinations\nncc to 10 and the number of difficulty distributions\nndd to 12, finally obtaining 200 × 10 × 12 = 24K\nsamples. To verify the influence of constraint num-\nber, we also conduct experiments on the setting\n2We use the latest v0.3 version.\nwhen n = 9. The statistic of the data for the prob-\ning task is provided in Fig. 3.\n4.2\nResults\nLLMs prefer to “hard-to-easy” constraint distri-\nbution.\nAs shown in Fig. 4, most of the LLMs ex-\nhibit a dramatic performance fluctuation on instruc-\ntions with varying constraint distributions. When\nthe constraint number is set to 7, the LLaMA3-8B-\nInstruct and Qwen2.5-7B-Instruct show approxi-\nmately 7% and 5% performance disparity in ex-\ntreme situations. This indicates the vulnerability\nof existing LLMs to the position bias brought by\nthe constraint order. Also, the LLMs tend to be\nmore performant to instructions with higher CDDI\nvalues. Even the LLaMA3-70B-Instruct exposes a\nclear preference for higher CDDI value as the num-\nber of constraints increases to 9, demonstrating that\n“hard-to-easy” is a superior constraint distribution\nfor existing LLMs.\nMulti-round inference exhibits more severe po-\nsition bias compared with the single-round in-\nference.\nThe LLMs’ performance in multi-round\ninference is presented in the Fig. 5. Compared with\nthe results in the single-round inference, the per-\nformance gap becomes more prominent. All the\nLLMs gain approximately 10% improvement on\nC_level accuracy. Surprisingly, the LLaMA3-8B-\nInstruct and LLaMA3-70B-Instruct achieve approx-\nimately 25% performance improvement by chang-\ning the constraint distribution from “easy-to-hard”\n(CDDI=-1) to “hard-to-easy” (CDDI=1). This in-\ndicates that the LLMs are more sensitive to the\nposition bias problem in a multi-round scenario.\nLLMs perform better in multi-round inference\nwhen provided with the instructions in appro-\npriate constraint order\nComparing the results\nin single-round (Fig. 4) and multi-round inference\n(Fig. 5), we observe that the LLMs reach better\nperformance if the incorporated constraints are ar-\nranged in an appropriate order. Specifically, when\nthe CDDI value is negative, the performance of\nLLMs in multi-round inference lags behind that\nin single-round inference. Nevertheless, with the\nincrease of the CDDI value, the LLMs can achieve\nsuperior performance in multi-round inference and\nreach their best performance in CDDI=1. An excep-\ntion is the Mistral-7B-Instruct-v0.3. We attribute\nthis to its inferiority in processing multi-round in-\nformation (Chen et al., 2024).\n5\n\nGPT4o-mini\nLLaMA3-70B-Instruct\nQwen2.5-7B-Instruct\nLLaMA3-8B-Instruct\nMistral-7B-Instruct\nLLaMA2-13B-Chat\nFigure 4: The performance of different LLMs in the single-round inference. The left and right figures show the\nresults with the number of constraints n set to 7 and 9, respectively. With the increase of the CDDI, the constraint\norder changes from “easy-to-hard” to “hard-to-easy”.\nGPT4o-mini\nQwen2.5-7B-Instruct\nMistral-7B-Instruct\nLLaMA2-13B-Chat\nLLaMA3-70B-Instruct\nLLaMA3-8B-Instruct\nFigure 5: The performance of different LLMs in the multi-round inference. The left and right figures show the\nresults with the number of constraints n set to 7 and 9, respectively. With the increase of the CDDI, the constraint\norder changes from “easy-to-hard” to “hard-to-easy”.\nPosition bias varies in different types of con-\nstraints.\nWe present the performance of the\nLLaMA3-8B-Instruct across different types of con-\nstraints in Tab. 1. As observed, with the increase of\nthe CDDI value, the model’s performance across\nmost constraint types shows an upward trend ex-\ncept for Startend and Content, indicating that not all\nthe constraints can benefit from the “hard-to-easy”\nconstraint distribution in single-round inference.\nWe make a more comprehensive explanation study\nin Sec. 5.3 for further investigation. Regarding the\nmulti-round inference, the model’s performance\nonly exhibits a drop tendency in the Length type\nas the CDDI value increases, indicating that the\nLLMs struggle to generate a length-controlled final\nresponse when the length constraint is applied early\nin the multi-round inference (Yuan et al., 2024).\n4.3\nRobustness of CDDI\nSince the CDDI is calculated by comparing the con-\ncordant and discordant pairs of two different con-\nstraint orders, there are usually multiple constraint\norders sharing the same CDDI value. Therefore,\nwe conduct a testing experiment to assess whether\nthe LLM exhibits significant fluctuations across\ndifferent constraint orders with the same CDDI\nvalue. Specifically, we set the CDDI to -0.05, a\nvalue that includes the most constraint orders in\nour setting, and conduct single-round inference for\n3 times. The experiment results are shown in Tab 2.\nWe calculate the P-value of the data, finding that\nthe P-value is much larger than 0.05. This indicates\nthat the fluctuation of LLM’s performance is negli-\ngible among different constraint orders in the same\nCDDI value.\n5\nExplanation Study\n5.1\nExplanation Metric\nTo make an explanation for the influence brought\nby the constraints of different orders, we make an\nexplanation study on where the LLMs mainly focus\nwhen handling multi-constraint instructions via a\nfeature attribution-based explanation method (Li\n6\n\nCDDI\nLength\nLanguage\nPunctuation\nFormat\nKeywords\nChangeCase\nStartend\nContent\nC_level\nI_level\nSingle-round Inference\n-1\n27.50\n28.20\n23.30\n71.14\n68.58\n49.57\n62.92\n81.22\n53.30\n1.95\n-0.8\n28.23\n30.70\n23.90\n73.64\n68.46\n49.00\n63.50\n77.78\n53.60\n1.80\n-0.6\n28.53\n31.10\n26.60\n71.23\n69.25\n49.79\n60.92\n78.22\n53.56\n1.95\n-0.4\n28.53\n36.10\n30.70\n72.41\n71.58\n51.64\n62.33\n77.83\n55.05\n2.10\n-0.2\n29.33\n39.30\n35.30\n73.82\n72.08\n50.07\n60.75\n77.44\n55.74\n2.40\n-0.05\n30.27\n42.90\n36.80\n74.95\n73.46\n52.14\n60.50\n77.50\n56.91\n2.90\n0.05\n29.17\n46.70\n38.00\n72.68\n74.75\n50.79\n61.50\n75.33\n56.57\n2.75\n0.2\n28.17\n50.50\n43.30\n76.05\n75.92\n52.07\n61.42\n72.94\n57.55\n2.75\n0.4\n30.23\n54.50\n46.40\n76.64\n76.29\n54.07\n62.83\n74.17\n59.14\n2.65\n0.6\n29.83\n59.20\n49.70\n79.09\n77.42\n56.71\n58.58\n74.33\n60.12\n3.00\n0.8\n29.40\n60.50\n51.70\n77.91\n77.63\n58.07\n58.25\n73.89\n60.16\n3.05\n1\n30.03\n67.10\n53.10\n78.00\n77.21\n59.21\n57.42\n74.61\n60.95\n3.50\nMulti-round Inference\n-1\n62.60\n64.00\n54.20\n21.59\n57.79\n62.50\n10.83\n16.61\n44.47\n0.75\n-0.8\n59.63\n64.90\n61.50\n22.27\n62.46\n61.93\n13.17\n17.39\n45.57\n0.75\n-0.6\n54.65\n67.87\n65.67\n25.74\n67.83\n59.47\n22.75\n20.08\n47.40\n0.65\n-0.4\n52.77\n68.74\n64.46\n32.30\n69.57\n61.44\n30.78\n26.21\n49.98\n1.05\n-0.2\n48.73\n68.74\n62.42\n38.67\n74.67\n59.16\n39.00\n32.02\n52.07\n1.25\n-0.05\n46.48\n69.97\n67.17\n46.38\n76.04\n60.97\n48.79\n46.58\n56.35\n1.40\n0.05\n45.32\n70.08\n68.84\n51.19\n76.62\n62.18\n52.68\n50.47\n58.04\n1.80\n0.2\n44.81\n69.91\n66.73\n58.35\n80.20\n60.34\n62.41\n63.22\n61.79\n3.41\n0.4\n44.30\n72.50\n69.10\n64.50\n81.75\n63.50\n68.00\n73.56\n65.39\n5.60\n0.6\n43.71\n68.87\n68.20\n71.71\n83.87\n59.83\n71.98\n81.77\n67.47\n5.05\n0.8\n44.35\n68.37\n68.00\n75.94\n84.49\n61.54\n70.31\n84.88\n68.76\n6.00\n1\n44.07\n70.90\n69.60\n81.41\n85.08\n64.43\n72.58\n87.22\n70.74\n4.00\nTable 1: The overall performance of LLaMA3-8B-Instruct on multi-constraint instructions with different CDDI\nvalues. From left to right, we sort the constraint types from the hardest to the easiest.\nRound\nLength\nkeywords\nlanguage\nChangeCase\nFormat\nContent\nStartend\nPunctuation\nC_level\nI_level\n1\n29.93\n73.46\n44.40\n50.68\n76.59\n77.11\n59.92\n34.40\n56.01\n2.70\n2\n29.83\n73.29\n43.80\n50.79\n73.36\n78.17\n61.50\n32.60\n55.49\n2.65\n3\n30.27\n73.46\n42.90\n52.14\n74.95\n77.50\n60.50\n36.80\n56.91\n2.90\n30.01±0.23\n73.40±0.10\n43.70±0.75\n51.20±0.82\n74.97±1.61\n77.59±0.53\n60.64±0.80\n34.60±2.11\n56.14±0.72\n2.75±0.13\nTable 2: The performance of LLaMA3-8B-Instruct when given the multi-constraint instruction in different constraint\norders while sharing the same CDDI value. By calculation, we obtain the P-value=0.9979.\net al., 2016; Wu et al., 2020). Specifically, we lever-\nage the importance of the input tokens to measure\nthe LLMs’ attention to them. To obtain the impor-\ntance of a specific instruction token tx to a response\ntoken ty, we calculate the confidence change after\nthe removal of the tx, as formulated below:\nItx,ty = p(ty|Zy) −p(ty|Zy,/tx),\n(6)\nwhere p(·|·) is the conditional probability produced\nby the LLM f, Zy is the tokens before the ty and\nZy,/tx is the tokens of Zy after removing the token\ntx. To reduce the computation, we approximate the\nItx,ty with the first-order gradient ∂f(ty|Zy)\n∂E[tx]\n(Wu\net al., 2023), where E [tx] is the token embedding\nof tx. We normalize the importance Itx,ty and ob-\ntain the standard importance Stx,ty with the for-\nmula:\nStx,ty =\nL × Itx,ty\nmaxNX\ni=1Iti,ty\n,\n(7)\nwhere NX is the number of instruction tokens\nand L is a hyper-parameter which helps to filter\nthe noise brought by the first-order approximation.\nTo visualize the LLMs’ attention to different con-\nstraints, we calculate the importance weight of a\nspecific constraint Cx to the final response Y with\nthe formula:\nSCx,Y =\n1\nNY\nX\nty∈Y\nX\ntx∈Cx\nStx,ty,\n(8)\nwhere NY is the number of response tokens.\n5.2\nExperiment Set-up\nWe conduct our explanation study on the LLaMA3-\n8B-Instruct model. We set the hyper-parameter\nL to 10 in Eq.(7) and select three most typical\ndifficulty distributions: hard-to-easy (indicated by\nCDDI=1), easy-to-hard (indicated by CDDI=-1)\nand random (indicated by CDDI=-0.05) to con-\nduct our experiments. We randomly sample 200\n7\n\n(a)\n(b)\nFigure 6: (a) The importance weights assigned by the LLM when handling constraints in different positions. (b)\nThe total importance weights which designated to the constraint part in the multi-constraint instructions among\nthree different constraint distributions.\nFigure 7: The importance weights across different types\nof constraint in three different constraint distributions.\ninstances from the corresponding data which fall\nin the required CDDI value in the probing task to\nserve as the dataset.\n5.3\nResults\nHard-to-easy constraint order induces the LLM\nto pay more attention to the constraint part in\nthe multi-constraint instructions.\nWe visualize\nthe importance weights of the model on the con-\nstraints in different positions. As shown in Fig. 6\n(a), in the multi-constraint instruction following,\nthe model’s attention on different positions varies\nwith changes in the constraint orders.\nSpecifi-\ncally, when the constraints are randomly distributed\nacross different positions (represented by CDDI=-\n0.05), the model assigns similar attention to all\npositions. As the constraint order becomes more\nstructured (represented by CDDI=-1 and CDDI=1),\nthe model’s attention neither exhibits the “lost in\nthe middle” phenomenon observed in long-context\nprocessing (Liu et al., 2024), nor a simply sequen-\ntial distribution, but follows an iterative, laddered\norder. Then, in Fig. 6 (b), we present the total\nimportance weight the model assigns to the con-\nstraint part. We observe that the “hard-to-easy”\nconstraint order attracts the most attention from the\nmodel towards the constraint part, which provides\nan explanation for the superiority of this constraint\norder.\nThe LLM’s performance on various constraints\nis strongly correlated with its attention patterns.\nThe importance weights of the model on different\ntypes of constraints are presented in Fig. 7. Among\nthe three distinct difficulty distributions, the “hard-\nto-easy” (represented by CDDI = 1) assigns the\nhighest importance weights to various types of con-\nstraints except for the Content and Startend. It\nis worth noting that this is exactly in accord with\nquantitative results in Tab. 1, i.e., as the CDDI value\nincreases, the model’s performance on the Content\nand Startend constraints shows a decreasing trend\ninstead. Overall, the results show that the model’s\naccuracy in following a specific type of constraint\nis strongly correlated with the attention assigned to\nit by the model.\n6\nConclusion\nIn this paper, we systematically investigate the posi-\ntion bias problem in the multi-constraint instruction\nfollowing. To quantitatively measure the disparity\nof constraint order, we propose a novel Difficulty\nDistribution Index (CDDI). Based on the CDDI,\nwe design a probing task. First, we construct a\nlarge number of instructions consisting of differ-\nent constraint orders. Then, we conduct experi-\nments in two distinct scenarios. Extensive results\nreveal a clear preference of LLMs for “hard-to-\neasy” constraint orders. To further explore this,\nwe conduct an explanation study. We visualize\nthe importance of different constraints located in\ndifferent positions and demonstrate the strong cor-\nrelation between the model’s attention distribution\nand its performance.\n8\n\n7\nLimitations\nOur work mainly focuses on the position bias prob-\nlem in the multi-constraint instruction following.\nWe make a quantitative analysis of the influence\nbrought by different constraint orders in the instruc-\ntions. However, there are still some limitations.\nThe constraints in our work are usually parallel\nto each other, which means the order change will\nnot affect the semantic meaning of the instructions.\nThe position bias problem for for those sequential\nconstraints need to be further explored. Moreover,\nwe only investigate the phenomenon of position\nbias in existing LLM without offering a solution.\nIn further work, we will conduct a further prob-\ning task in sequential constraints to improve the\ngeneralization of our findings.\nReferences\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, et al. 2023. Gpt-4 technical report.\narXiv preprint arXiv:2303.08774.\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,\nXiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei\nHuang, et al. 2023. Qwen technical report. arXiv\npreprint arXiv:2309.16609.\nLukas Berglund, Meg Tong, Max Kaufmann, Mikita\nBalesni, Asa Cooper Stickland, Tomasz Korbak, and\nOwain Evans. 2023. The reversal curse: Llms trained\non\" a is b\" fail to learn\" b is a\".\narXiv preprint\narXiv:2309.12288.\nXiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen,\nDamai Dai, Chengqi Deng, Honghui Ding, Kai Dong,\nQiushi Du, Zhe Fu, et al. 2024. Deepseek llm: Scal-\ning open-source language models with longtermism.\narXiv preprint arXiv:2401.02954.\nXinyi Chen, Baohao Liao, Jirui Qi, Panagiotis Eustra-\ntiadis, C. Monz, Arianna Bisazza, and M. D. Ri-\njke. 2024. The SIFo Benchmark: Investigating the\nSequential Instruction Following Ability of Large\nLanguage Models.\nIn Conference on Empirical\nMethods in Natural Language Processing, volume\nabs/2406.19999, pages 1691–1706.\nXinyun Chen, Ryan Andrew Chi, Xuezhi Wang, and\nDenny Zhou. Premise order matters in reasoning\nwith large language models. In Forty-first Interna-\ntional Conference on Machine Learning.\nJiale Cheng, Xiao Liu, Cunxiang Wang, Xiaotao\nGu, Yida Lu, Dan Zhang, Yuxiao Dong, Jie Tang,\nHongning Wang, and Minlie Huang. 2024. Spar:\nSelf-Play with Tree-Search Refinement to Improve\nInstruction-Following in Large Language Models.\narXiv, abs/2412.11605.\nVincent A Cicirello. 2020. Kendall tau sequence dis-\ntance: Extending kendall tau from ranks to sequences.\nEAI Endorsed Transactions on Industrial Networks\nand Intelligent Systems, 7(23).\nVittoria Dentella, Fritz Günther, Elliot Murphy, Gary\nMarcus, and Evelina Leivada. 2024. Testing ai on\nlanguage comprehension tasks reveals insensitivity to\nunderlying meaning. Scientific Reports, 14(1):28083.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, et al. 2024. The llama 3 herd of models. arXiv\npreprint arXiv:2407.21783.\nQianyu He, Jie Zeng, Wenhao Huang, Lina Chen, Jin\nXiao, Qianxi He, Xunzhe Zhou, Jiaqing Liang, and\nYanghua Xiao. 2024. Can large language models\nunderstand real-world complex instructions? In Pro-\nceedings of the AAAI Conference on Artificial Intelli-\ngence, volume 38, pages 18188–18196.\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, et al. 2023a. Mistral\n7b. arXiv preprint arXiv:2310.06825.\nYuxin Jiang, Yufei Wang, Xingshan Zeng, Wanjun\nZhong, Liangyou Li, Fei Mi, Lifeng Shang, Xin\nJiang, Qun Liu, and Wei Wang. 2023b.\nFollow-\nbench: A multi-level fine-grained constraints follow-\ning benchmark for large language models. arXiv\npreprint arXiv:2310.20410.\nAndreas Köpf, Yannic Kilcher, Dimitri von Rütte,\nSotiris Anagnostidis, Zhi Rui Tam, Keith Stevens,\nAbdullah Barhoum, Duc Nguyen, Oliver Stan-\nley, Richárd Nagyfi, et al. 2024.\nOpenassistant\nconversations-democratizing large language model\nalignment. Advances in Neural Information Process-\ning Systems, 36.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph Gon-\nzalez, Hao Zhang, and Ion Stoica. 2023. Efficient\nmemory management for large language model serv-\ning with pagedattention. In Proceedings of the 29th\nSymposium on Operating Systems Principles, pages\n611–626.\nJiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.\n2016. Visualizing and understanding neural mod-\nels in nlp. In Proceedings of the 2016 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 681–691.\nXian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke\nZettlemoyer, Omer Levy, Jason Weston, and Mike\nLewis. 2023. Self-alignment with instruction back-\ntranslation. arXiv preprint arXiv:2308.06259.\n9\n\nJunjie Liu, Shaotian Yan, Chen Shen, Liang Xie, Wenx-\niao Wang, and Jieping Ye. 2023. Concise and or-\nganized perception facilitates reasoning in large lan-\nguage models. arXiv e-prints, pages arXiv–2310.\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paran-\njape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. 2024. Lost in the middle: How language mod-\nels use long contexts. Transactions of the Association\nfor Computational Linguistics, 12:157–173.\nRenze Lou, Kai Zhang, and Wenpeng Yin. 2024. Large\nlanguage model instruction following: A survey of\nprogresses and challenges. Computational Linguis-\ntics, pages 1–10.\nYiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao,\nSangwoo Cho, Xiaoyang Wang, Xuansheng Wu, Fei\nLiu, Pengfei Liu, and Dong Yu. 2024. Infobench:\nEvaluating instruction following ability in large lan-\nguage models. arXiv preprint arXiv:2401.03601.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Shoeb, Abubakar Abid, Adam Fisch,\nAdam R Brown, Adam Santoro, Aditya Gupta, Adri\nGarriga-Alonso, et al. 2023. Beyond the imitation\ngame: Quantifying and extrapolating the capabili-\nties of language models. Transactions on machine\nlearning research.\nHaoran Sun, Lixin Liu, Junjie Li, Fengyu Wang, Bao-\nhua Dong, Ran Lin, and Ruohui Huang. 2024.\nConifer: Improving complex constrained instruction-\nfollowing ability of large language models. arXiv\npreprint arXiv:2404.02823.\nRaphael Tang, Crystina Zhang, Xueguang Ma, Jimmy\nLin, and Ferhan Türe. 2024. Found in the middle:\nPermutation self-consistency improves listwise rank-\ning in large language models. In Proceedings of\nthe 2024 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies (Volume 1: Long Pa-\npers), pages 2327–2340.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023.\nLlama 2:\nOpen founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nMengting Wan, Tara Safavi, Sujay Kumar Jauhar, Yu-\njin Kim, Scott Counts, Jennifer Neville, Siddharth\nSuri, Chirag Shah, Ryen W White, Longqi Yang,\net al. 2024. Tnt-llm: Text mining at scale with large\nlanguage models. In Proceedings of the 30th ACM\nSIGKDD Conference on Knowledge Discovery and\nData Mining, pages 5836–5847.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2023. Self-instruct: Aligning language\nmodels with self-generated instructions. In The 61st\nAnnual Meeting Of The Association For Computa-\ntional Linguistics.\nYizhong Wang, Swaroop Mishra, Pegah Alipoor-\nmolabashi,\nYeganeh Kordi,\nAmirreza Mirzaei,\nAnjana Arunkumar, Arjun Ashok, Arut Selvan\nDhanasekaran, et al. 2022. Super-naturalinstructions:\nGeneralization via declarative instructions on 1600+\nnlp tasks. Preprint, arXiv:2204.07705.\nBosi Wen, Pei Ke, Xiaotao Gu, Lindong Wu, Hao\nHuang, Jinfeng Zhou, Wenchuang Li, Binxin Hu,\nWendy Gao, Jiaxing Xu, Yiming Liu, Jie Tang,\nHongning Wang, and Minlie Huang. 2024. Bench-\nmarking Complex Instruction-Following with Mul-\ntiple Constraints Composition.\nIn The Thirty-\neight Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track, volume\nabs/2407.03978.\nXuansheng Wu, Wenlin Yao, Jianshu Chen, Xiaoman\nPan, Xiaoyang Wang, Ninghao Liu, and Dong Yu.\n2023. From language modeling to instruction follow-\ning: Understanding the behavior shift in llms after\ninstruction tuning. arXiv preprint arXiv:2310.00492.\nZ Wu, Y Chen, CM Kao, and FUN Liu. 2020. Perturbed\nmasking: Parameter-free probing for analyzing and\ninterpreting bert.\nIn Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics (ACL). Association for Computational\nLinguistics.\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,\nPu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei\nLin, and Daxin Jiang. 2024. Wizardlm: Empowering\nlarge pre-trained language models to follow complex\ninstructions. In The Twelfth International Conference\non Learning Representations.\nTianyi Yan, Fei Wang, James Y. Huang, Wenxuan Zhou,\nFan Yin, A. Galstyan, Wenpeng Yin, and Muhao\nChen. 2024. Contrastive Instruction Tuning. In An-\nnual Meeting of the Association for Computational\nLinguistics, volume abs/2402.11138.\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui,\nBo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu,\nFei Huang, Haoran Wei, et al. 2024. Qwen2. 5 tech-\nnical report. arXiv preprint arXiv:2412.15115.\nWenpeng Yin, Qinyuan Ye, Pengfei Liu, Xiang Ren,\nand Hinrich Schütze. 2023. Llm-driven instruction\nfollowing: Progresses and concerns. In Proceedings\nof the 2023 Conference on Empirical Methods in\nNatural Language Processing: Tutorial Abstracts,\npages 19–25.\nWeizhe Yuan, Ilia Kulikov, Ping Yu, Kyunghyun Cho,\nSainbayar Sukhbaatar, Jason Weston, and Jing Xu.\n2024. Following length constraints in instructions.\narXiv preprint arXiv:2406.17744.\nZhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya\nGoyal, and Danqi Chen. 2023. Evaluating Large Lan-\nguage Models at Evaluating Instruction Following.\nIn NeurIPS Workshop on Instruction Tuning and In-\nstruction Following, volume abs/2310.07641.\n10\n\nXianren Zhang, Xianfeng Tang, Hui Liu, Zongyu Wu,\nQi He, Dongwon Lee, and Suhang Wang. 2024a.\nDivide-Verify-Refine: Aligning LLM Responses\nwith Complex Instructions. arXiv, abs/2410.12207.\nZhen Zhang, Yuhua Zhao, Hang Gao, and Mengting\nHu. 2024b. Linkner: Linking local named entity\nrecognition models to large language models using\nuncertainty.\nIn Proceedings of the ACM on Web\nConference 2024, pages 4047–4058.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric P Xing, et al. 2023.\nJudging llm-as-a-judge with mt-bench and chatbot\narena. In Proceedings of the 37th International Con-\nference on Neural Information Processing Systems,\npages 46595–46623.\nJeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha\nBrahma, Sujoy Basu, Yi Luan, Denny Zhou, and\nLe Hou. 2023a. Instruction-Following Evaluation for\nLarge Language Models. arXiv, abs/2311.07911.\nJeffrey Zhou, Tianjian Lu, Swaroop Mishra, Sid-\ndhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou,\nand Le Hou. 2023b.\nInstruction-following evalu-\nation for large language models.\narXiv preprint\narXiv:2311.07911.\nA\nAppendix\nA.1\nImplementation Details\nWe utilize 8 NVIDIA A800 80GB GPUs to con-\nduct all the experiments. We employ the vLLM\nframework (Kwon et al., 2023) to accelerate the\nmodel inference. For reproducibility, we employ\nthe greed search in the whole inference (i.e., setting\nthe “do_sample” to false.).\nA.2\nMore details for Comstraint Sampling\nIn this work, We categorize the constraints into\n8 different groups. The categorization is shown\nin the Tab. 3. For each group, there are multiple\ntypes of constraints. Specifically, the constraints\nare designated to: (1) Keyword constraints. These\nconstraints focus on controlling the inclusion or\nexclusion of specific words or phrases within the\nresponse. (2) Language constraints. Language\nconstraints govern the linguistic properties of the\nresponse, including the language in which the re-\nsponse is written (e.g., English). (3) Length con-\nstraints. These constraints focus on controlling the\noverall length of the response, including the num-\nber of paragraphs, words, and sentences. (4) Con-\ntent Constraints. Content-related constraints define\nadditional rules to ensure the response contains spe-\ncific elements. (5) Format constraints. Formatting\nconstraints focus on how the response is structured\nand styled. For example. (6) ChangeCase Con-\nstraints. These constraints focus on adjusting the\ncase of words in the response. They may require\nthe entire response to be in uppercase letters (e.g.,\nALL CAPS), or entirely in lowercase letters (e.g.,\nall lowercase). (7) StartEnd constraints. These con-\nstraints limit the very beginning or ending of the\nmodel outputs. (8) Punctuation constraints. These\nconstraints limit the appearance of specific com-\nmas.\nConsidering the LLM is vulnerable to different\ndescriptions of the constraints (Yan et al., 2024),\nwe employ the GPT4o-mini to generate differ-\nent descriptions of the same constraints. Specif-\nically, given a description example, we leverage\nthe prompt shown in the Tab. 4 to seven distinct\nvariants. Overall, we obtain 8 distinct descriptions\nfor a specific type of constraint.\n11\n\nConstraint Group\nConstraint\nDescription Example\nKeyword\nInclude Keywords\nInclude keywords [keyword1], [keyword2] in your re-\nsponse.\nExclude Keywords\nDo not include keywords [forbidden words] in the re-\nsponse.\nKeyword Frequency\nIn your response, the word should appear N times.\nLetter Frequency\nIn your response, the letter [letter] should appear [N]\ntimes.\nLanguage\nResponse Language\nYour ENTIRE response should be in [language], no other\nlanguage is allowed.\nLength\nNumber Paragraphs\nYour response should contain [N] paragraphs. You separate\nparagraphs using the markdown divider ***.\nNumber Words\nAnswer with at least/around/at most [N] words.\nNumber Sentences\nAnswer with at least/around/at most [N] sentences.\nNumber\nParagraphs\n+\nFirst Word in i-th Para-\ngraph\nThere should be [N] paragraphs. Paragraphs and only para-\ngraphs are separated with each other by two line breaks.\nThe [i]-th paragraph must start with [first_word].\nContent\nPostscript\nAt the end of your response, please add a postscript starting\nwith [postscript marker].\nNumber Placeholder\nThe response must contain at least [N] placeholders repre-\nsenting the word space brackets, such as [address].\nFormat\nNumber Bullets\nYour response must contain exactly [N] bullet points. Use\nthe markdown bullet points such as: * This is a pont.\nTitle\nYour answer must contain a title, wrapped in double angular\nbrackets, such as «option of joy».\nChoose From\nYour response should contain one of the following options:\n[options].\nMinimum Number High-\nlighted Section\nHighlight at least [N] sections in your answer with mark-\ndown, i.e. *highlighted section*.\nMultiple Sections\nYour response must have [N] sections. Mark the beginning\nof each section with [section_splitter] X.\nJSON Format\nEntire output should be wrapped in JSON format.\nChangeCase\nAll Uppercase\nYour entire response should be in English, capital letters\nonly.\nAll Lowercase\nYour response should be in English, and in all lowercase\nletters. No capital letters are allowed.\nFrequency of All-capital\nWords\nIn your response, words with all capital letters should appear\nat least [N] times.\nStartEnd\nEnd Checker\nYour response must finish with this phrase: <end_phrase>.\nQuotation\nWrap uour entire response with double marks.\nPunctuation\nNo Commas\nIn your entire response, refrain from the use of any commas.\nTable 3: The categorization for different constraints.\n12\n\n/* Task prompt */\nYou are provided with a <constraint> in an instruction. As a prompt engineer, your task is to rephrase the provided <constraint>\nto make it more diverse. You ought to provide five more variants of the <constraint>. Make sure your revision does not\nchange the meaning of the original <constraint>.\n/* Example */\n—INPUT—\n<constraint>:\nYour response should contain at least 3 sentences.\n—OUTPUT—\nvariants:\n1. Respond with at least three sentences\n2. Use at least 3 sentences in your reply\n3. Your entire response should include at least three sentences\n4. Organize your entire response in at least 3 sentences\n5. Please make sure the response is at least 3 sentences long\n/* Input */\n—INPUT—\n<constraint>:\n{Given_constraint}\n—OUTPUT—\nvariants:\nTable 4: The prompts for diversifying the descriptions of a given constraint. We utilize one-shot in-context learning\nto enhance the performance. The information that requires manual input is highlighted.\n13\n",
  "metadata": {
    "source_path": "papers/arxiv/Order_Matters_Investigate_the_Position_Bias_in_Multi-constraint\n__Instruction_Following_81bf59d6b1f2eee4.pdf",
    "content_hash": "81bf59d6b1f2eee4ba7ba91fd19fa50cb1518bf6dd1f7b61017561194365c5d6",
    "arxiv_id": null,
    "title": "Order_Matters_Investigate_the_Position_Bias_in_Multi-constraint\n__Instruction_Following_81bf59d6b1f2eee4",
    "author": "",
    "creation_date": "D:20250225025213Z",
    "published": "2025-02-25T02:52:13",
    "pages": 13,
    "size": 4789135,
    "file_mtime": 1740470169.5620809
  }
}