{
  "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n1\nMambaFlow: A Novel and Flow-guided State Space\nModel for Scene Flow Estimation\nJiehao Luo, Jintao Cheng, Xiaoyu Tang, Member, IEEE, Qingwen Zhang, Bohuan Xue, Rui Fan, Senior\nMember, IEEE\nAbstract—Scene flow estimation aims to predict 3D motion\nfrom consecutive point cloud frames, which is of great interest\nin autonomous driving field. Existing methods face challenges\nsuch as insufficient spatio-temporal modeling and inherent\nloss of fine-grained feature during voxelization. However, the\nsuccess of Mamba, a representative state space model (SSM)\nthat enables global modeling with linear complexity, provides\na promising solution. In this paper, we propose MambaFlow,\na novel scene flow estimation network with a mamba-based\ndecoder. It enables deep interaction and coupling of spatio-\ntemporal features using a well-designed backbone. Innovatively,\nwe steer the global attention modeling of voxel-based features\nwith point offset information using an efficient Mamba-based\ndecoder, learning voxel-to-point patterns that are used to de-\nvoxelize shared voxel representations into point-wise features.\nTo further enhance the model’s generalization capabilities across\ndiverse scenarios, we propose a novel scene-adaptive loss function\nthat automatically adapts to different motion patterns. Extensive\nexperiments on the Argoverse 2 benchmark demonstrate that\nMambaFlow achieves state-of-the-art performance with real-time\ninference speed among existing works, enabling accurate flow\nestimation in real-world urban scenarios. The code is available\nat https://github.com/SCNU-RISLAB/MambaFlow.\nIndex Terms—Scene Flow Estimation, State Space Model,\nSpatio-temporal Deep Coupling, Real-time Inference\nI. INTRODUCTION\nS\nCENE flow is a vector field that describes the motion\nof 3D points between two frames, representing the 3D\ncounterpart of optical flow [1] in 2D images. The scene\nflow estimation task takes consecutive point cloud frames\nManuscript created November 2024; This work was partially supported by\nthe National Natural Science Foundation of China under Grants 62473288\nand 62233013, Guangdong Basic and Applied Basic Research Foundation\n(2024A1515012126), the Science and Technology Commission of Shanghai\nMunicipal under Grant 22511104500, the Fundamental Research Funds for\nthe Central Universities, and Xiaomi Young Talents Program. (Corresponding\nauthor: Xiaoyu Tang.)\nJiehao Luo and Bohuan Xue are with the School of Data Science\nand Engineering, and Xingzhi College, South China Normal University,\nShanwei 516600, China. (e-mail: 20228132034@m.scnu.edu.cn;\nbxueaa@connect.ust.hk)\nJintao Cheng and Xiaoyu Tang are with the School of Electronics and In-\nformation Engineering, and Xingzhi College, South China Normal University,\nFoshan 528225, China. (e-mail: 20172332035@m.scnu.edu.cn;\ntangxy@scnu.edu.cn)\nQingwen Zhang is with the Division of Robotics, Perception, and Learning\n(RPL), KTH Royal Institute of Technology, Stockholm SE-114 28, Sweden.\n(e-mail: qingwen@kth.se)\nRui Fan is with the College of Electronics & Information Engineering,\nShanghai Research Institute for Intelligent Autonomous Systems, the State\nKey Laboratory of Intelligent Autonomous Systems, and Frontiers Science\nCenter for Intelligent Autonomous Systems, Tongji University, Shanghai\n201804, China. (e-mail: rfan@tongji.edu.cn)\nJiehao Luo and Jintao Cheng contribute equally to this work.\nFig. 1.\nComparison of voxelization and devoxelization for scene flow\nestimation. (a) Previous methods typically use two consecutive frames with\ncoarse devoxelization that assigns identical features to points in the same\nvoxel, causing inherent feature loss. (b) Our MambaFlow leverages N con-\nsecutive scans for richer temporal information, with refined devoxelization that\nlearns distinct voxel-to-point patterns for fine-grained feature representation.\nfrom sensors as input and output motion vectors for each\npoint, providing autonomous perception systems with essential\nmotion information for accurate environmental perception\nand decision-making [2], [3]. Moreover, real-time scene flow\nestimation is also well regarded as a promising solution\nfor enhancing the performance of downstream tasks such as\n3D object detection [4], depth estimation [5] and moving\nobject segmentation [6], thus largely enhancing the overall\nintelligence of autonomous systems.\nExisting scene flow estimation methods have made progress\nbut still face several challenges. Current approaches [7]–[13]\neither directly concatenate consecutive point cloud frames for\nimplicit temporal features extraction, leading to insufficient\nutilization of temporal information, or pursue efficiency by\nprojecting point clouds into 2D pseudo-images, resulting in\nspatial feature loss. Jund et al. [11] proposed an end-to-\nend learning framework that concatenates consecutive point\ncloud frames and employs a flow embedding layer for feature\nextraction. Zhang et al. [12] drew inspiration from optical\narXiv:2502.16907v1  [cs.CV]  24 Feb 2025\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n2\nflow approaches and introduced a Gated Recurrent Unit (GRU)\ndecoder for point cloud feature refinement. While these meth-\nods partially alleviate the feature loss, their reliance on only\ntwo consecutive frames for temporal information consistently\nlimits model performance.\nTemporal information plays a dominant role in scene flow\nestimation. This is effectively demonstrated by recent work\nFlow4D [14], which leverages more than two consecutive\nframes to achieve significantly improved performance on the\nArgoverse 2 benchmark. To reduce computational complexity,\nFlow4D decomposes 4D convolution into parallel temporal\nand spatial branches with 1D and 3D convolutions respectively.\nWhile this decomposition achieves computational efficiency,\nit sacrifices the holistic modeling of spatio-temporal correla-\ntions. The assumption that temporal and spatial features can\nbe processed independently ignores their inherent coupling\nin motion dynamics. Moreover, since temporal and spatial\ninformation characterizes fundamentally different aspects of\nmotion dynamics and geometric structures, such simplified\nprocessing prevents effective feature interaction and temporal\nguidance.\nTo address the above challenges, we propose a novel multi-\nbranch architecture for feature extraction that achieves deep\ncoupling of spatio-temporal features. Specifically, our method\nincorporates a learnable fusion mechanism that dynamically\nweights temporal and spatial features while prioritizing tem-\nporal cues. This enables effective modeling of complex motion\npatterns across diverse scenarios, significantly enhancing the\nlearning of motion dynamics from point clouds compared to\nexisting methods.\nAlthough our spatio-temporal coupling approach enables\ncomprehensive feature extraction, the adopted voxel-based\nencoding introduces inherent feature loss, as points within the\nsame voxel grid become indistinguishable. Recent works [15],\n[16] demonstrate that transformer architectures can effectively\nlearn point-wise correlations through global attention model-\ning. However, their quadratic inference complexity challenges\nreal-time requirements. Studies on SSM [17]–[19] suggest a\npromising alternative with linear-complexity global attention\nmodeling. Drawing inspiration from recent advances in SSM\nfor point cloud processing [20]–[22], we propose a Mamba-\nbased decoder that steers global attention modeling of voxel-\nbased features through point offset information. It achieves\nrefined devoxelization by adaptively learning voxel-to-point\npatterns, which enables effective transformation from shared\nvoxel representations to point-wise features, representing our\ncore contribution to scene flow estimation.\nAs observed in [11], [12], scene flow estimation methods\noften exhibit limited generalization in autonomous driving\nscenarios due to the severe imbalance between dynamic\nand static point distributions, where over 90% of the point\ncloud exhibits zero displacement. Drawing insights from self-\nsupervised approaches [11], [12], we propose a scene-adaptive\nloss function that leverages motion statistics derived from\npoint displacement distributions.\nThrough extensive experiments, we demonstrate that our\nproposed feature extraction architecture integrated with the\nMamba-based decoder, guided by scene-adaptive loss supervi-\nsion, significantly enhances the model’s dynamics-awareness\nand scene adaptability, achieving state-of-the-art performance\non the Argoverse 2 benchmark among published works while\nmaintaining real-time inference speed of 17.30 FPS. The main\ncontributions of this paper are:\n1) We propose MambaFlow, a novel SSM-based architec-\nture for scene flow estimation that addresses voxelization\nfeature loss through voxel-to-point pattern learning. To\nour knowledge, this represents the first application of\nstate space modeling to scene flow estimation.\n2) We propose a multi-branch backbone for deep coupling\nof spatio-temporal features, with an adaptive fusion strat-\negy that prioritizes temporal information for complex\nmotion patterns.\n3) We propose a scene-adaptive loss function that lever-\nages point displacement distributions to automatically\ndistinguish between static and dynamic points without\nempirical thresholds.\nWe provide our code at https://github.com/SCNU-RISLAB/\nMambaFlow.\nII. RELATED WORK\nA. Scene Flow Estimation Methods\nScene flow estimation in autonomous driving, while sharing\nsimilarities with object registration methods like DifFlow3D\n[23] and [24] that achieve millimeter-level precision on small-\nscale datasets like ShapeNet [25] and FlyingThings3D [26],\nfaces unique challenges when processing scenes from datasets\nlike Argoverse 2 [27] and Waymo [28] containing 80k-177k\npoints per frame, where substantial downsampling compro-\nmises practicality [11]. For such large-scale data, voxel-based\nencoding with efficient feature refinement emerges as the pre-\nferred approach to balance efficiency and feature preservation.\nMany existing methods adopt flow embedding-based ap-\nproaches [9], [11], [12], [29], establishing soft correspon-\ndences by concatenating spatial features from consecutive\nframes. However, this indirect feature correlation with two-\nframe input has been increasingly recognized as suboptimal for\ncapturing temporal dynamics. Following successful practices\nin 3D detection [30], scene flow estimation has evolved to-\nwards multi-frame approaches. For example, Flow4D [14] pro-\ncesses five frames for more robust estimation. Most recently,\nEulerFlow [31] reformulates scene flow as a continuous space-\ntime PDE. However, to effectively exploit temporal dynamics\nwhile avoiding the computational complexity of continuous\nstreams, we follow Flow4D’s framework by adopting five\nconsecutive frames as input with voxel-based encoding.\nRecently, self-supervised methods have gained popularity\ndue to the difficulty of obtaining scene flow ground truth. Se-\nFlow [10] addresses key challenges through efficient dynamic\nclassification and internal cluster consistency, while ICP-Flow\n[9] incorporates rigid-motion assumptions via histogram-based\ninitialization and ICP alignment. However, precise motion\nestimation remains critical for autonomous driving safety, and\nstate-of-the-art self-supervised methods like EulerFlow still\nshow limitations in static background estimation, which could\nbe catastrophic for autonomous driving systems. Although\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n3\nreducing annotation costs is appealing, the investment in\nlabeled datasets is justified and necessary given the safety-\ncritical nature of autonomous driving. We maintain a su-\npervised architecture while incorporating insights from self-\nsupervised approaches and proposed a scene-adaptive loss\nfunction, which automatically extracts motion statistics from\nvelocity histograms for better generalization without empirical\nthresholds.\nB. State Space Model\nSSMs [17]–[19] have gained attention as an efficient alter-\nnative to attention mechanisms and Transformer architectures,\nparticularly for capturing long-term dependencies through\nhidden states. The Structured State Space Sequence (S4)\n[17] efficiently models long-range dependencies through di-\nagonal parameterization, addressing computational bottlenecks\nin attention-based approaches. Building upon S4, researchers\ndeveloped enhanced architectures such as S5 [18] and H3 [19].\nNotably, Mamba [32] represents a significant breakthrough\nby introducing a data-dependent selective scan mechanism\nand integrating hardware-aware parallel scanning algorithms,\nestablishing a novel paradigm distinct from CNNs and Trans-\nformers while maintaining linear computational complexity.\nThe linear complexity capability of Mamba has inspired\nextensive exploration in both vision [33], [34] and point cloud\nprocessing [20]–[22] domains. In 3D point cloud domain, re-\ncent works have demonstrated significant advances in adapting\nMamba for various tasks. Mamba3D [20] introduced local\nnorm pooling for geometric features and a bidirectional SSM\ndesign for enhanced both local and global feature represen-\ntation. PointMamba [21] is one of the pioneers in applying\nstate space models to point cloud analysis by utilizing space-\nfilling curves for point tokenization with a non-hierarchical\nMamba encoder, establishing a simple yet effective baseline\nfor 3D vision applications. MambaMos [22] further explored\nSSM’s potential in point cloud sequence modeling by adapting\nMamba’s selective scan mechanism for motion understand-\ning, demonstrating that SSM’s strong contextual modeling\ncapabilities are particularly effective for capturing temporal\ncorrelations in moving object segmentation. These successes in\nachieving linear complexity while maintaining robust feature\nlearning suggest promising potential for achieving fine-grained\ndevoxelization in scene flow estimation.\nBuilding upon these insights, we try to integrate the Mamba\narchitecture into the the scene for estimation network for to\nmaintains real-time performance while avoiding the quadratic\ncomplexity of transformer-based approaches.\nIII. PROBLEM STATEMENT\nConsider two consecutive point cloud frames Pt and Pt+1\nacquired at time instants t and t + 1, with vehicle ego-\nmotion transformation matrix Tt,t+1. The scene flow esti-\nmation task aims to predict the motion vector ˆMt,t+1(p) =\n(∆x, ∆y, ∆z)T for each point p in Pt. We employ the\nEnd Point Error (EPE) as the evaluation metric, as defined\nin Eq. (1):\nEPE(p) = ∥ˆM(p) −Mgt(p)∥2\n(1)\nwhere ˆM(p) and Mgt(p) denote the predicted and ground\ntruth scene flow, respectively.\nThe core objective of scene flow estimation is to minimize\nthe average EPE, which can be expressed as Eq. (2):\nmin\n1\n|Pt|\nX\np∈Pt\n∥ˆ\nM (p) −Mgt (p) ∥2\n(2)\nwhere |Pt| denotes the number of points in Pt. A method\nmust address both dynamic objects in static environments and\nglobal scene changes induced by ego-motion. Such challenges\nnecessitate effective integration of local features and global\ncontext for accurate 3D motion pattern capture.\nIV. METHODOLOGY\nA. Preliminaries\nMamba [32] is a sequence modeling framework based on\nSSM that introduces a selective scan mechanism to model\ncomplex state spaces through time-varying characteristics.\nThrough a time scale parameter ∆, it makes the state transition\nmatrix A and input projection matrix B input-dependent for\nselective feature filtering. The continuous system is discretized\nvia Zero-Order Hold, which can be expressed as Eq. (3):\n¯A = e∆A\n¯B = (e∆A −I)A−1∆B\n(3)\nAfter discretization, the linear ordinary differential equa-\ntions representing an SSM can be rewritten as Eq. (4):\nht = ¯Aht−1 + ¯Bxt\nyt = Cht\n(4)\nwhere ht denotes the hidden state, xt and yt denote the input\nand output sequences respectively, and C is output projection\nmatrix.\nB. MambaFlow Pipeline\n1) Overall Architecture: As shown in Figure 2, Mam-\nbaFlow adopts an end-to-end architecture and generates scene\nflow estimation through three main stages: Spatio-temporal\nsampling and encoding, Spatio-temporal Deep Coupling Net-\nwork, and MambaFlow Decoder.\nIn the Spatio-temporal Sampling and Encoding stage, N\nconsecutive LiDAR scans are first transformed into 3D voxel\nrepresentations through a voxel feature encoder, with all\nframes warped to the coordinate system of the time step\nt + 1. The subsequent temporal fusion stage concatenates\nthese 3D voxel representations along the temporal dimension,\ngenerating a 4D spatio-temporal feature tensor.\nNext, the 4D tensor is processed by the Spatio-temporal\nCoupling Network. This network adopts a U-Net architec-\nture with stacked Spatio-temporal Deep Coupling Block,\nwhich progressively learns multi-scale feature representations\nthrough deep hierarchical modeling. The network consists of\na five-level encoder with stacking depths [2,2,2,2,2] and a\nfour-level decoder with stacking depths [1,1,1,1], coupling\nmulti-scale contextual information through sparse convolution\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n4\nFig. 2.\nOverall architecture of MambaFlow. The network first voxelizes and encodes five consecutive scans, forming 4D features by concatenating 3D\nvoxel representations along the temporal dimension. These features are processed by our spatio-temporal coupling network for multi-scale feature learning.\nThe decoder then learns voxel-to-point patterns through cascaded FlowSSM layers, enabling point-wise feature differentiation within the same voxel and\ngenerating the scene flow through an MLP layer.\noperations. The fused features are residually combined with\nthe input features to enhance feature representation.\nFor the devoxelization stage, the decoder is used to pro-\ncesses the extracted 3D voxel features from time step t + 1.\nThe voxel features are first serialized into sequences following\nspace-filling curves. Through multiple cascaded FlowSSM\nmodules, which incorporate discretized point offset informa-\ntion into state space for the global modeling of voxel-wise\nfeatures, the decoder progressively refines voxel-wise features\nto point-wise features. Finally, the refined feature sequence is\ndeserialized and fed into an MLP head to generate point-wise\nscene flow estimation.\n2) Input Representation and Voxelization: To balance pre-\ndiction performance and computational efficiency, we follow\nFlow4D [14] by sampling five consecutive point cloud frames\nas input.\nGiven consecutive point cloud frames Pτ that can be\nrepresented as Eq. (5):\nPτ = {pi ∈R3}Nτ −1\ni=0\n,\nτ ∈{t −3, t −2, t −1, t, t + 1}\n(5)\nwhere pi = (xi, yi, zi)T denotes the point coordinates and Nτ\ndenotes the number of points at time step τ.\nFollowing previous studies, we first warp all point clouds\nto the perspective of time step t + 1 using the known pose\ntransformation matrices. For each transformed point cloud, we\nfirst employ an MLP to extract point-wise features F τ\npoint,\nfollowed by a voxel feature encoder that aggregates these\nfeatures into voxel-wise features Fτ\nvoxel. To form spatio-\ntemporal representations, we extend F τ\nvoxel with a temporal\ndimension and concatenate them along the time axis, yielding\na 4D voxel tensor F 4D that encodes both spatial and temporal\ninformation.\n3) Serialization: Since our SSM-based decoder processes\nsequential data, we adopt Z-order space-filling curves as our\nserialization strategy inspired by [15]. We define a mapping\nfunction Ψ that projects 3D point cloud Po to sequence\nP′\no while preserving spatial locality. The serialization and\ndeserialization process can be expressed as Eq. (6):\nP′\no = Ψ(Po)\nPo = Ψ−1(P′\no)\n(6)\nC. Spatio-temporal Deep Coupling Block\n1) Feature Extraction Stage: As shown in Figure 3, the\nproposed spatio-temporal deep coupling block (STDCB) con-\nsists of multi sparse convolutions branches, incorporating a\ncascaded Soft Feature Selection Mechanism to achieve adap-\ntive feature interaction. Specifically, STDCB first sparsifies\nF 4D to obtain a sparse 4D tensor F sparse, which is then\nprocessed by three parallel branches. Drawing inspiration from\nthe design of Flow4D [14], we adopt a convolution with\nkernel size (3 × 3 × 3 × 1) to extract geometric structures,\nand two convolution with kernel size (1 × 1 × 1 × 3) to\ncapture local motion patterns and cross-timestep dependencies\nthrough different receptive fields. The process of initially\nfeature extraction can be formulated as Eq. (7):\nF spatial = Φ3×3×3×1(F sparse)\nF temporal = Φ1×1×1×3(F sparse)\nF ct\ntemporal = Φ1×1×1×3,dilation=1(F sparse)\n(7)\nwhere F spatial, F temporal and F ct\ntemporal denote the features\nextracted from spatial, local temporal, and cross-timestep\ntemporal branches respectively. Φ denotes the convolution ker-\nnel. For the cross-timestep, we employ a dilated convolution\noperation where a dilation factor of 1 is applied to expand the\nreceptive field.\nThe decomposed design significantly reduces computational\ncomplexity compared to full 4D convolutions while preserving\nthe ability to capture both spatial and temporal information.\n2) Soft Feature Selection Mechanism: Given two feature\ntensors as input, Soft Feature Selection Mechanism designates\none as the main branch F main and the other as the auxiliary\nbranch F aux according to their roles in the specific task. Their\nrelative importance is then computed through a Gated Unit.\nThe attention weights can be formulated as Eq. (8):\nα = σ(LR(BN(Φp(Concat(F main, F aux)))))\n(8)\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n5\nFig. 3. Architecture of the Spatio-temporal Deep Coupling Block. (a) The baseline Spatio-temporal Decomposition Block [14] processes features through\nrepeated convolutions at each stage. (b) Our proposed Spatio-temporal Deep Coupling Block achieves more efficient feature extraction by removing redundant\nconvolutions modules and introducing a cross-timestep branch. The right panel (c)-(e) shows the detailed structures of Soft Feature Selection Mechanism and\ngating mechanisms used in Spatio-temporal Deep Coupling Block.\nwhere σ denotes the sigmoid activation, LR denotes the\nLeakyReLU, BN denotes the batch normalization operation,\nΦp denotes the point-wise convolution, and Concat denotes\nfeature concatenation along the channel dimension.\nThe attention weights are then used to adaptively combine\nthe two branches, where α is multiplied with the main branch\nand (1−α) with the auxiliary branch. This weighting scheme\nallows flexible control of feature importance: when α is large,\nthe main branch features are emphasized, while smaller α\nvalues give more weight to the complementary features from\nthe auxiliary branch.\n3) Temporal Gated Block: After obtaining features from\nthree parallel branches, we first fuse temporal features by\nemploying Soft Feature Selection Mechanism (SFSM) with\nconsecutive temporal features as the main branch and cross-\ntimestep temporal features as the auxiliary branch to comple-\nment cross-step temporal information, which can be formu-\nlated as Eq. (9):\nF ′\ntemporal = SFSM(F temporal, F ct\ntemporal)\n(9)\nThe fused temporal features then guide spatial feature learn-\ning through the Temporal Gated Block, where the attention\nweights β are computed as Eq. (10):\nβ = σ(Φp(ReLU(Φp(F ′\ntemporal))))\n(10)\nThe spatial features are modulated by temporal attention\nthrough gating and residual connection, which can be formu-\nlated as Eq. (11):\nF ′\nspatial = F spatial ⊙(1 + β)\n(11)\nwhere ⊙denotes element-wise multiplication.\nFinally, F ′\nspatial is adaptively combined with F ′\ntemporal\nusing SFSM, followed by a residual connection with F sparse\nand a point-wise convolution to fuse the features. The final\noutput can be formulated as Eq. (12):\nF ′\nsparse = Φp(Concat(SFSM(F ′\ntemporal, F ′\nspatial); F sparse))\n(12)\nD. MambaFlow Decoder\nThe overall structure of the MambaFlow Decoder is shown\nin Figure 4. During the decoding stage, we first extract the\n3D voxel features F t\n3D at time step t from F ′\nsparse. For\neach point in a voxel grid, we assign the corresponding voxel\nfeatures as their initial point-wise representations, denoted as\nF t\ncoarse, where all points in the same voxel share identical\nfeature values.\nMeanwhile, we preserve both point-wise features F t\npoint\nand point offset information P t\noffset during encoding stage.\nConsidering that P t\noffset only have 3 channels, which may\ncause information imbalance when directly used as decoder\ninput, we extend their feature dimension to a matching dimen-\nsion of F t\n3D through a point offset encoder, yielding point-\nwise offset features F t\noffset. The concatenation of F t\n3D and\nF t\npoint along the channel dimension forms the initial input\nfeatures F 0\ncoarse to the first decoding layer, with F t\noffset\nserving as guiding information for feature refinement.\nThe key component of our decoder is FlowSSM, a variant\nof SSM. As shown in Algorithm 1, it incorporates discretized\npoint offset information into state space for global modeling\nof voxel-wise features. To enable sequence modeling, we first\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n6\nFig. 4.\nMambaFlow Decoder architecture. Point-wise features and voxel\nfeatures are first serialized through Z-order space-filling curves for spa-\ntial proximity preservation. The FlowSSM module consists of N cascaded\nFlowSSM layers, where point offset features guide the learning of voxel-\nto-point patterns in each layer for refined feature reconstruction. The final\noutput is obtained through deserialization and feature fusion with point offset\ninformation.\nserialize both coarse features and offset features using Z-order\nspace-filling curves, which can be formulated as Eq. (13):\nF ′\ncoarse = Ψ(F t\ncoarse)\nF ′\noffset = Ψ(Pt\noffset)\n(13)\nThe decoder then progressively refines these features\nthrough multiple cascaded FlowSSM layers, where each layer\nconditions the state space matrices on F ′\noffset for adaptive\nfeature refinement. After N iterations, the refined feature\nsequence is first deserialized to obtain refined point-wise\nfeatures F ′\nrefined, then concatenated with F ′\noffset. Finally,\nthe scene flow estimation\nˆMt,t+1(p) is generated through\ndeserialization and an MLP head.\nE. Scene-adaptive Loss\nIn scene flow estimation, supervised methods relying on\nmanually labeled data often show limited generalization ability\nto real-world scenes. Inspired by the the loss function design\nof FastFlow3D [11], DeFlow [12] divides points into three\nsubsets {P1, P2, P3} using velocity thresholds of 0.4 m/s and\n1.0 m/s, and computes the total loss by averaging the endpoint\nerrors within each subset, which can be formulated as Eq. (18):\nL =\n3\nX\ni=1\n1\n|Pi|\nX\np∈Pi\n∥∆ˆM(p) −∆Mgt(p)∥2\n(14)\nHowever, empirically determined velocity thresholds can\nmisclassify slow-moving objects as static points. Considering\nthe severe imbalance where over 90% of points exhibit zero\ndisplacement in autonomous driving scenarios, we propose a\nscene-adaptive loss function that automatically determines the\ndisplacement threshold for effective model training.\nSpecifically,\nwe\nfirst\ndivide\nthe\ndisplacement\nrange\n[rmin, rmax] of a point cloud frame into K equal-width bins,\nwhere r denotes the point displacement and rmin is set to 0\nconsidering that zero-displacement points are in the majority\nof all points. We define wj as the proportion of points whose\nAlgorithm 1\nFlowSSM Process\nRequire: Coarse flow features F i−1\ncoarse: (B, N, 2C)\nPoint offset features F ′\noffset: (B, N, C)\nHidden state Hi−1: (B, D, N)\nEnsure: Refined flow features F i\ncoarse: (B, N, C)\nUpdated hidden state Hi: (B, D, N)\n/* Parameterize data independent matrices */\nA: (D, N) ←Parameter\nD: (D,) ←Parameter\n/* Parameterize data dependent matrices via point offset\nfeatures */\n∆: (B, D), B: (B, N), C: (B, N) ←Linear(F ′\noffset)\n/* Discretize */\n¯A: (B, D, N) ←Exp(∆⊗A)\n¯B: (B, D, N) ←∆⊗B\n/* Running SSM */\nF i\ncoarse: (B, N, 2C), Hi: (B, D, N)\n←SSM(¯A, ¯B, C, D)(F i−1\ncoarse, Hi−1)\nReturn: F i\nrefind, Hi\ndisplacement magnitudes fall into the j-th bin bj, which can\nbe formulated as Eq. (15):\nwj = nj\nn\n(15)\nwhere nj denotes the number of points in j-th bin and n is\nthe total number of points.\nWe then adaptively select the first bin whose scale is below\n1/K as the displacement threshold α, which can be expressed\nas Eq. (16):\nα = min{j : wj < 1\nK }\n(16)\nBy setting K = 100, we can effectively capture the inherent\ndisplacement distribution of each scene and focus the loss\non the truly dynamic portion of the point cloud. With the\nthreshold α indicating the bin index, we divide the point p into\ntwo categories {PStatic, PDynamic} based on their displacement\nr(p) relative to the lower bound of the α-th bin rα, as defined\nin Eq. (17):\nPStatic = {p ∈P | r(p) ≤rα}\nPDynamic = {p ∈P | r(p) > rα}\n(17)\nThe total loss function is a weighted average of these two\ntypes of losses, which can be formulated as Eq. (18):\nLtotal =\n1\n|PStatic|\nX\np∈PStatic\n∥∆ˆM(p) −∆Mgt(p)∥2\n+\n1\n|PDynamic|\nX\np∈PDynamic\n∥∆ˆM(p) −∆Mgt(p)∥2\n(18)\nwhere |PStatic| and |PDynamic| denote the number of the static\nand dynamic point sets respectively, ∥·∥2 denotes the L2 norm.\nV. EXPERIMENT\nA. Experiment Setups\n1) Datasets: The proposed method is evaluated on the\nArgoverse 2 dataset [27], a large-scale autonomous driving\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n7\nTABLE I\nQUANTITATIVE COMPARISON ON THE ARGOVERSE 2 TEST SET. ’SUP.’ REPRESENTS SUPERVISED METHODS. ’FD’ REPRESENTS FOREGROUND\nDYNAMIC, ’BS’ REPRESENTS BACKGROUND STATIC, AND ’FS’ REPRESENTS FOREGROUND STATIC.\nMethod\nSup.\n3-way Endpoint Error (↓)\nBucketed Normalized Endpoint Error (↓)\nAvg.\nFD\nBS\nFS\nDynamic\nStatic\nmean\nmean\nCar\nOther\nvehicles\nPedes-\ntrian\nWheeled-\nvru\nNSFP\n0.0606\n0.1158\n0.0344\n0.0316\n0.4219\n0.2509\n0.3313\n0.7225\n0.3831\n0.0279\nFastNSF\n0.1118\n0.1844\n0.0907\n0.0814\n0.3826\n0.2901\n0.4126\n0.5002\n0.3215\n0.0736\nICP-Flow\n0.0650\n0.1369\n0.0250\n0.0332\n0.3309\n0.1945\n0.3314\n0.4353\n0.3626\n0.0271\nSeFlow\n0.0536\n0.1323\n0.0043\n0.0242\n0.3194\n0.2178\n0.3464\n0.4452\n0.2683\n0.0148\nEulerFlow\n0.0423\n0.0498\n0.0526\n0.0245\n0.1303\n0.0929\n0.1408\n0.1947\n0.0931\n0.0253\nFastFlow3D\n✓\n0.0735\n0.1917\n0.0027\n0.0262\n0.5323\n0.2429\n0.3908\n0.9818\n0.5139\n0.0182\nDeFlow\n✓\n0.0501\n0.1091\n0.0062\n0.0352\n0.3704\n0.1530\n0.3150\n0.6615\n0.3520\n0.0262\nTrackFlow\n✓\n0.0473\n0.1030\n0.0024\n0.0365\n0.2689\n0.1817\n0.3054\n0.3581\n0.2302\n0.0447\nFlow4D\n✓\n0.0224\n0.0494\n0.0047\n0.0130\n0.1454\n0.0871\n0.1505\n0.2165\n0.1272\n0.0106\nMambaFlow (Ours)\n✓\n0.0191\n0.0450\n0.0015\n0.0108\n0.1422\n0.0786\n0.1399\n0.2369\n0.1136\n0.0077\nTABLE II\nQUANTITATIVE COMPARISON ON THE ARGOVERSE 2 VALIDATION SET.\nMethod\nBucketed Normalized EPE (↓)\n3-way Endpoint Error (↓)\nDynamic IoU\n(↑)\nmean Dynamic\nmean Static\nAvg.\nFD\nBS\nFS\nFastFlow3D\n0.3975\n0.0113\n0.0461\n0.1218\n0.0028\n0.0136\n0.7169\nDeFlow\n0.3299\n0.0192\n0.0446\n0.1013\n0.0043\n0.0281\n0.7505\nFlow4D\n0.1775\n0.0092\n0.0206\n0.0468\n0.0033\n0.0117\n0.8137\nMambaFlow (Ours)\n0.1620\n0.0079\n0.0176\n0.0414\n0.0018\n0.0096\n0.8661\nbenchmark containing 1,000 diverse scenarios. Each sequence\nin this dataset spans 15 seconds and encompasses 30 distinct\nobject classes in complex urban environments. The dataset\nprovides comprehensive sensor data, including LiDAR scans,\ndetailed 3D annotations, and HD maps, making it particularly\nsuitable for evaluating scene flow estimation in real-world\nurban scenarios.\n2) Metrics: We employ two metrics to evaluate our method.\nFirst, we use the standard End Point Error (EPE), which\nmeasures the L2 distance between predicted and ground truth\nscene flow vectors. However, as EPE tends to be dominated\nby large objects and fails to reflect performance on smaller,\nsafety-critical objects like pedestrians, we also adopt Bucket\nNormalized EPE [13]. This metric evaluates each object cat-\negory separately and normalizes the error by object speed,\nenabling more balanced assessment across different object\ntypes and motion patterns.\n3) Implementation details: Our model is implemented in\nPyTorch with a two-phase training strategy. The first phase\nfocuses on training the backbone network with scene-adaptive\nloss without the decoder to learn robust feature representations.\nIn the second phase, we integrate the decoder and fine-tune\nthe entire architecture with a lower learning rate to preserve\nthe pre-trained backbone knowledge while optimizing decoder\nperformance.\nWe conduct distributed training on 8 NVIDIA RTX 4090\nGPUs. The first phase runs for 30 epochs with an initial\nlearning rate of 2e-4 and batch size of 5 per GPU. The second\nphase continues for 50 epochs with an initial learning rate of\n2e-6 and batch size of 3 per GPU, while maintaining other\nhyperparameters.\nB. Scene Flow Estimation Performance\n1) Quantitation Analysis: We compare our proposed Mam-\nbaFlow with various supervised and self-supervised scene flow\nmethods. Table I presents the comparative results evaluated\nthrough the official Argoverse 2 test server. The proposed\nmethod achieves state-of-the-art performance in all EPE met-\nrics , demonstrating the most accurate point-level motion esti-\nmation across all published works. Specifically, MambaFlow\nsurpasses Flow4D by significant margins in all three metrics,\nshowing a 14.7% improvement in average EPE, 8.9% in\nforeground dynamic estimation, and 68.1% in background\nstatic prediction.\nFor object-level evaluation using Bucketed Normalized\nEPE, our method achieves competitive performance in dy-\nnamic object estimation, particularly excelling in rigid objects\nsuch as cars and other vehicles. MambaFlow’s balanced perfor-\nmance across both static and dynamic scenarios, especially its\nsuperior point-level accuracy as demonstrated by 3-way EPE\nmetrics, validates its effectiveness as a comprehensive scene\nflow estimation solution.\nAs shown in Table II, we further compare our method\nwith supervised methods FastFlow3D, DeFlow, and Flow4D\non the Argoverse 2 validation set. MambaFlow achieves best\nperformance across all metrics, demonstrating an 8.7% lower\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n8\nFig. 5. Qualitative results on the Argoverse 2 validation set. From left to right: Ground Truth, DeFlow, Flow4D, and our proposed MambaFlow. The color\nlegend indicates both speed (shown by color intensity) and motion angle (2D), aligned with the vehicle’s forward direction. The highlighted regions (yellow\ncircles) demonstrate our method’s superior performance in capturing both static and dynamic object motions, especially for challenging cases with complex\nmotion patterns.\nmean Dynamic Normalized EPE, 14.6% lower 3-way mean\nEPE, and 6.4% higher Dynamic IoU compared to the previous\nstate-of-the-art Flow4D.\n2) Qualitative Analysis: As shown in Figure 5, we visual-\nize scene flow predictions from different methods alongside\nground truth across multiple scenarios, where our method\ndemonstrates superior discrimination between moving and\nstatic objects, producing minimal flow errors for various\nstructures including buildings, pedestrians, and parked vehicles\nacross multiple scenarios. This capability is crucial for reliable\nscene understanding in autonomous driving scenarios.\nFurthermore, our method exhibits remarkable capability\nin object-level scene flow estimation. For instance, in the\nfirst row (yellow circles), where two pedestrians are walking\nin opposite directions, MambaFlow accurately captures their\ndistinct motion patterns despite the lack of explicit ground\ntruth annotations for this scenario. In contrast, Flow4D shows\nonly minimal response to these opposing movements, while\nDeFlow fails to detect this complex interaction entirely. This\ndemonstrates our method’s superior sensitivity to fine-grained\nmotion patterns at the object level.\nC. Ablation Study\nThe component analysis in Table III demonstrates the\ncontribution of each proposed module. In method (i), for\nboth mean Dynamic EPE and mean Static EPE metrics,\nincorporating STDCB alone brings improvements of 5.1%\nand 5.4% respectively, validating its effectiveness in temporal-\nspatial feature coupling. The application of MambaFlow de-\ncoder yields improvements of 3.1% and 1.1% respectively,\nTABLE III\nPERFORMANCE COMPARISON OF DIFFERENT COMPONENT\nCONFIGURATIONS. ‘STDCB’ REPRESENTS THE SPATIO-TEMPORAL DEEP\nCOUPLING BLOCK, ‘M.F. DECODER’ REPRESENTS THE MAMBAFLOW\nDECODER AND ‘S.A. LOSS’ REPRESENTS THE SCENE-ADAPTIVE LOSS.\nMethod\nComponents\nB.N. EPE (↓)\nSTDCB\nM.F.\nDecoder\nS.A.\nLoss\nmean D.\nmean S.\nBaseline\n-\n-\n-\n0.1775\n0.0092\n(i)\n✓\n0.1685\n0.0087\n✓\n0.1720\n0.0091\n✓\n0.1744\n0.0078\n(ii)\n✓\n✓\n0.1646\n0.0090\n✓\n✓\n0.1645\n0.0080\n✓\n✓\n0.1695\n0.0085\nMambaFlow\n✓\n✓\n✓\n0.1620\n0.0079\nhighlighting its capability in feature recovery. Meanwhile,\nutilizing Scene-adaptive Loss alone for supervision training\nimproves mean Dynamic EPE and mean Static EPE by 1.7%\nand 15.2% respectively, demonstrating its robust scene adap-\ntation capability. In method (ii), when validating pairwise\ncombinations of components, the model’s performance shows\nsubstantial improvements, particularly with the synergistic\neffect of STDCB and scene-adaptive loss, which yields im-\nprovements of 7.3% and 13.0% in mean Dynamic EPE and\nmean Static EPE respectively. By combining all components,\nthe proposed MambaFlow achieves the best performance.\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n9\nTABLE IV\nABLATION STUDY ON FLOWSSM ITERATION COUNT. ’#ITR’\nREPRESENTS THE DIFFERENT NUMBERS OF ITERATIONS.\n#Itr\nBucketed Normalized EPE (↓)\nEfficiency\nMean Dynamic\nMean Static\nFPS\n1\n0.1620\n0.0079\n17.30\n2\n0.1645\n0.0079\n16.66\n3\n0.1697\n0.0081\n15.99\n4\n0.1725\n0.0096\n14.90\n5\n0.1655\n0.0092\n13.76\nTABLE V\nRESOURCE CONSUMPTION OF DIFFERENT METHODS.\nMethods\nFPS\nParams(M) Total Size(MB) GM(GiB)\nFastFlow3D\n20.35\n6.8\n27.262\n2.37\nDeFlow\n19.82\n6.9\n27.568\n2.42\nFlow4D\n14.68\n4.6\n18.412\n1.78\nMambaFlow 17.30\n3.1\n12.598\n2.04\nAs shown in Table IV, the results demonstrate that with\na single iteration, our method achieves an optimal balance\nbetween accuracy and computational speed. We also tested the\neffect of increasing the number of iterations, and the results\nshow that the increasing number of iterations not only fails to\nimprove performance but also leads to slight degradation in\ncomputational efficiency. Therefore, we adopt single iteration\nin our overall architecture. However, further investigation into\nthis phenomenon, particularly on larger-scale datasets and\ndiverse point cloud processing tasks, could yield valuable\ninsights into the optimal number of iterations for different\napplications.\nD. Evaluation of Consumption\nAs shown in Table V, our method achieves superior per-\nformance with only 3.1M parameters, representing a 32.6%\nreduction in parameter count compared to Flow4D. Instead\nof using STDB with redundant convolution operations as\nin Flow4D, STDCB eliminates duplicate feature extraction\nprocesses based on our observation and experimental valida-\ntion that features extracted in earlier stages already contain\nsufficient information for subsequent fusion. This architectural\noptimization also improves computational efficiency, enabling\nour method to achieve 17.30 FPS compared to Flow4D’s 14.68\nFPS.\nIn terms of memory usage, our method achieves a better\nbalance between performance and resource efficiency with\nmoderate memory usage of 2.04 GiB. This slight increase in\nmemory overhead is well compensated by the substantial per-\nformance improvements. With a more compact model size of\n12.598 MB compared to Flow4D’s 18.412 MB, MambaFlow\nis particularly suitable for deployment in resource-constrained\nscenarios while maintaining state-of-the-art performance.\nVI. CONCLUSIONS\nIn this paper, we present MambaFlow, a novel scene flow\nestimation approach based on SSM. Through deep coupling\nof temporal and spatial information, our method achieves\ncomprehensive spatio-temporal feature extraction with tem-\nporal guidance. Most importantly, we introduce a Mamba-\nbased decoder that enables refined devoxelization by learning\ndistinct voxel-to-point patterns, effectively preserving fine-\ngrained motion details. Extensive experiments demonstrate\nthat our approach achieves state-of-the-art performance on the\nArgoverse 2 benchmark while maintaining real-time inference\ncapability, validating the effectiveness of SSM for scene flow\nestimation.\nREFERENCES\n[1] Z. Yi, H. Shi, K. Yang, Q. Jiang, Y. Ye et al., “Focusflow: Boosting\nkey-points optical flow estimation for autonomous driving,” IEEE Trans-\nactions on Intelligent Vehicles, vol. 9, no. 1, pp. 2794–2807, 2024.\n[2] H. Liu, Z. Huang, X. Mo, and C. Lv, “Augmenting reinforcement learn-\ning with transformer-based scene representation learning for decision-\nmaking of autonomous driving,” IEEE Transactions on Intelligent Vehi-\ncles, vol. 9, no. 3, pp. 4405–4421, 2024.\n[3] H. Shi, Q. Jiang, K. Yang, X. Yin, H. Ni et al., “Beyond the field-\nof-view: Enhancing scene visibility and perception with clip-recurrent\ntransformer,” IEEE Transactions on Intelligent Vehicles, pp. 1–16, 2024.\n[4] H. Meng, C. Li, G. Chen, L. Chen, and A. Knoll, “Efficient 3d object\ndetection based on pseudo-lidar representation,” IEEE Transactions on\nIntelligent Vehicles, vol. 9, no. 1, pp. 1953–1964, 2024.\n[5] Y. Feng, Z. Guo, Q. Chen, and R. Fan, “Scipad: Incorporating spatial\nclues into unsupervised pose-depth joint learning,” IEEE Transactions\non Intelligent Vehicles, pp. 1–11, 2024.\n[6] S. Kim, C. Kim, and K. Jo, “Awv-mos-lio: Adaptive window visibility\nbased moving object segmentation with lidar inertial odometry,” IEEE\nTransactions on Intelligent Vehicles, pp. 1–16, 2024.\n[7] X. Li, J. Kaesemodel Pontes, and S. Lucey, “Neural scene flow prior,”\nAdvances in Neural Information Processing Systems, vol. 34, pp. 7838–\n7851, 2021.\n[8] X. Li, J. Zheng, F. Ferroni, J. K. Pontes, and S. Lucey, “Fast neural\nscene flow,” in Proceedings of the IEEE/CVF International Conference\non Computer Vision, 2023, pp. 9878–9890.\n[9] Y. Lin and H. Caesar, “Icp-flow: Lidar scene flow estimation with icp,”\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2024, pp. 15 501–15 511.\n[10] Q. Zhang, Y. Yang, P. Li, O. Andersson, and P. Jensfelt, “SeFlow: A\nself-supervised scene flow method in autonomous driving,” in European\nConference on Computer Vision (ECCV).\nSpringer, 2024, p. 353–369.\n[11] P. Jund, C. Sweeney, N. Abdo, Z. Chen, and J. Shlens, “Scalable\nscene flow from point clouds in the real world,” IEEE Robotics and\nAutomation Letters, p. 1589–1596, Apr 2022. [Online]. Available:\nhttp://dx.doi.org/10.1109/lra.2021.3139542\n[12] Q. Zhang, Y. Yang, H. Fang, R. Geng, and P. Jensfelt, “DeFlow:\nDecoder of scene flow network in autonomous driving,” in 2024 IEEE\nInternational Conference on Robotics and Automation (ICRA), 2024, pp.\n2105–2111.\n[13] I. Khatri, K. Vedder, N. Peri, D. Ramanan, and J. Hays, “I can’t believe\nit’s not scene flow!” in European Conference on Computer Vision.\nSpringer, 2025, pp. 242–257.\n[14] J. Kim, J. Woo, U. Shin, J. Oh, and S. Im, “Flow4d: Leveraging\n4d voxel network for lidar scene flow estimation,” arXiv preprint\narXiv:2407.07995, 2024.\n[15] X. Wu, L. Jiang, P.-S. Wang, Z. Liu, X. Liu et al., “Point transformer v3:\nSimpler faster stronger,” in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 2024, pp. 4840–4851.\n[16] J. Li, Y. Zhang, P. Yun, G. Zhou, Q. Chen et al., “Roadformer:\nDuplex transformer for rgb-normal semantic road scene parsing,” IEEE\nTransactions on Intelligent Vehicles, vol. 9, no. 7, pp. 5163–5172, 2024.\n[17] A. Gu, K. Goel, and C. R´e, “Efficiently modeling long sequences with\nstructured state spaces,” arXiv preprint arXiv:2111.00396, 2021.\n[18] J. T. Smith, A. Warrington, and S. W. Linderman, “Simplified state space\nlayers for sequence modeling,” arXiv preprint arXiv:2208.04933, 2022.\n[19] D. Y. Fu, T. Dao, K. K. Saab, A. W. Thomas, A. Rudra et al., “Hungry\nhungry hippos: Towards language modeling with state space models,”\narXiv preprint arXiv:2212.14052, 2022.\n[20] X. Han, Y. Tang, Z. Wang, and X. Li, “Mamba3d: Enhancing local\nfeatures for 3d point cloud analysis via state space model,” arXiv\npreprint arXiv:2404.14966, 2024.\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n10\n[21] D. Liang, X. Zhou, W. Xu, X. Zhu, Z. Zou et al., “Pointmamba:\nA simple state space model for point cloud analysis,” arXiv preprint\narXiv:2402.10739, 2024.\n[22] K. Zeng, H. Shi, J. Lin, S. Li, J. Cheng et al., “Mambamos: Lidar-based\n3d moving object segmentation with motion-aware state space model,”\n2024. [Online]. Available: https://arxiv.org/abs/2404.12794\n[23] J. Liu, G. Wang, W. Ye, C. Jiang, J. Han et al., “Difflow3d: Toward\nrobust uncertainty-aware scene flow estimation with iterative diffusion-\nbased refinement,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2024, pp. 15 109–15 119.\n[24] Z. Wang, Y. Wei, Y. Rao, J. Zhou, and J. Lu, “3d point-voxel correlation\nfields for scene flow estimation,” IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 2023.\n[25] A. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang et al.,\n“Shapenet: An information-rich 3d model repository,” arXiv: Graph-\nics,arXiv: Graphics, Dec 2015.\n[26] N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers et al., “A large\ndataset to train convolutional networks for disparity, optical flow, and\nscene flow estimation,” in Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 2016, pp. 4040–4048.\n[27] B.\nWilson,\nW.\nQi,\nT.\nAgarwal,\nJ.\nLambert,\nJ.\nSingh\net\nal.,\n“Argoverse 2: Next generation datasets for self-driving perception and\nforecasting,” in Proceedings of the Neural Information Processing\nSystems\nTrack\non\nDatasets\nand\nBenchmarks,\nJ.\nVanschoren\nand\nS.\nYeung,\nEds.,\nvol.\n1,\n2021.\n[Online].\nAvailable:\nhttps://datasets-benchmarks-proceedings.neurips.cc/paper files/paper/\n2021/file/4734ba6f3de83d861c3176a6273cac6d-Paper-round2.pdf\n[28] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik et al.,\n“Scalability in perception for autonomous driving: Waymo open dataset,”\nin Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition, 2020, pp. 2446–2454.\n[29] X. Liu, C. R. Qi, and L. J. Guibas, “Flownet3d: Learning scene flow\nin 3d point clouds,” in Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, 2019, pp. 529–537.\n[30] K. Vedder and E. Eaton, “Sparse pointpillars: Maintaining and exploiting\ninput sparsity to improve runtime on embedded systems,” in 2022\nIEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS).\nIEEE, 2022, pp. 2025–2031.\n[31] K. Vedder, N. Peri, I. Khatri, S. Li, E. Eaton et al., “Scene flow as a\npartial differential equation,” arXiv preprint arXiv:2410.02031, 2024.\n[32] A. Gu and T. Dao, “Mamba: Linear-time sequence modeling with\nselective state spaces,” arXiv preprint arXiv:2312.00752, 2023.\n[33] L. Zhu, B. Liao, Q. Zhang, X. Wang, W. Liu et al., “Vision mamba:\nEfficient visual representation learning with bidirectional state space\nmodel,” arXiv preprint arXiv:2401.09417, 2024.\n[34] X. Pei, T. Huang, and C. Xu, “Efficientvmamba: Atrous selective scan\nfor light weight visual mamba,” arXiv preprint arXiv:2403.09977, 2024.\nJiehao Luo (Student Member, IEEE) is currently conducting research under\nthe supervision of Xiaoyu Tang at the School of Data Science and Engineering,\nand Xingzhi College, South China Normal University. His research focuses\non computer vision and robotic perception.\nJintao Cheng received his bachelor’s degree from the School of Physics and\nTelecommunications Engineering, South China Normal University, in 2021.\nHis research includes computer vision, SLAM, and deep learning.\nXiaoyu Tang (Member, IEEE) received a B.S. degree from South China\nNormal University, Guangzhou, China, in 2003 and an M.S. degree from Sun\nYat-sen University, Guangzhou, China, in 2011. He is currently pursuing a\nPh.D. degree with South China Normal University. Mr. Tang works as an\nassociate professor, master supervisor, and deputy dean at Xingzhi College,\nSouth China Normal University. His research interests include image process-\ning and intelligent control, artificial intelligence, the Internet of Things, and\neducational informatization.\nQingwen Zhang (Student Member, IEEE) received the M.Phil. degree from\nThe Hong Kong University of Science and Technology in 2022. She is\ncurrently a Ph.D student at the KTH Royal Institute of Technology. Her\nresearch interests include dynamic awareness in point clouds.\nBohuan Xue received the B.Eng. degree in computer science and technology\nfrom College of Mobile Telecommunications, Chongqing University of Posts\nand and Telecom, Chongqing, China, in 2018, and the Ph.D. degree from the\nDepartment of Computer Science and Engineering, the Hong Kong University\nof Science and Technology, Hong Kong, China, in 2024. He is currently\nworking as research assistant in School of Data Science and Engineering, and\nXingzhi College, South China Normal University.\nRui Fan (Senior Member, IEEE) received the B.Eng. degree in Automation\nfrom the Harbin Institute of Technology in 2015 and the Ph.D. degree in\nElectrical and Electronic Engineering from the University of Bristol in 2018.\nHe worked as a Research Associate at the Hong Kong University of Science\nand Technology from 2018 to 2020 and a Postdoctoral Scholar-Employee at\nthe University of California San Diego between 2020 and 2021. He began his\nfaculty career as a Full Research Professor with the College of Electronics &\nInformation Engineering at Tongji University in 2021, and was then promoted\nto a Full Professor in the same college, as well as at the Shanghai Research\nInstitute for Intelligent Autonomous Systems in 2022.\nProf. Fan served as an associate editor for ICRA’23 and IROS’23/24,\nan area chair for ICIP’24, and a senior program committee member for\nAAAI’23/24/25. He is the general chair of the AVVision community and\norganized several impactful workshops and special sessions in conjunction\nwith WACV’21, ICIP’21/22/23, ICCV’21, and ECCV’22. He was honored\nby being included in the Stanford University List of Top 2% Scientists\nWorldwide between 2022 and 2024, recognized on the Forbes China List\nof 100 Outstanding Overseas Returnees in 2023, and acknowledged as one\nof Xiaomi Young Talents in 2023. His research interests include computer\nvision, deep learning, and robotics, with a specific focus on humanoid visual\nperception under the two-streams hypothesis.\n",
  "metadata": {
    "source_path": "papers/arxiv/MambaFlow_A_Novel_and_Flow-guided_State_Space_Model_for_Scene_Flow\n__Estimation_ef789c3074a0a981.pdf",
    "content_hash": "ef789c3074a0a9812b03400225c335e359f4a534c995a086e216315907d9b05a",
    "arxiv_id": null,
    "title": "MambaFlow_A_Novel_and_Flow-guided_State_Space_Model_for_Scene_Flow\n__Estimation_ef789c3074a0a981",
    "author": "",
    "creation_date": "D:20250225022922Z",
    "published": "2025-02-25T02:29:22",
    "pages": 10,
    "size": 11776582,
    "file_mtime": 1740470207.2925978
  }
}