{
  "text": "Published as a conference paper at ICLR 2025\nSTRENGTH ESTIMATION AND HUMAN-LIKE STRENGTH\nADJUSTMENT IN GAMES\nChun-Jung Chen1,2âˆ—, Chung-Chin Shih1âˆ—, Ti-Rong Wu1â€ \n1Institute of Information Science, Academia Sinica, Taiwan\n2Department of Computer Science, National Taiwan University, Taiwan\nABSTRACT\nStrength estimation and adjustment are crucial in designing human-AI interactions,\nparticularly in games where AI surpasses human players. This paper introduces a\nnovel strength system, including a strength estimator (SE) and an SE-based Monte\nCarlo tree search, denoted as SE-MCTS, which predicts strengths from games\nand offers different playing strengths with human styles. The strength estimator\ncalculates strength scores and predicts ranks from games without direct human\ninteraction. SE-MCTS utilizes the strength scores in a Monte Carlo tree search to\nadjust playing strength and style. We first conduct experiments in Go, a challenging\nboard game with a wide range of ranks. Our strength estimator significantly\nachieves over 80% accuracy in predicting ranks by observing 15 games only,\nwhereas the previous method reached 49% accuracy for 100 games. For strength\nadjustment, SE-MCTS successfully adjusts to designated ranks while achieving\na 51.33% accuracy in aligning to human actions, outperforming a previous state-\nof-the-art, with only 42.56% accuracy. To demonstrate the generality of our\nstrength system, we further apply SE and SE-MCTS to chess and obtain consistent\nresults. These results show a promising approach to strength estimation and\nadjustment, enhancing human-AI interactions in games. Our code is available at\nhttps://rlg.iis.sinica.edu.tw/papers/strength-estimator.\n1\nINTRODUCTION\nArtificial intelligence has achieved superhuman performance in various domains in recent years,\nespecially in games (Silver et al., 2018; Schrittwieser et al., 2020; Vinyals et al., 2019; OpenAI et al.,\n2019). These achievements have raised interests within the community in exploring AI programs for\nhuman interactions, particularly in estimating human playersâ€™ strengths and offering corresponding\nlevels to increase entertainment or improve skills (Demediuk et al., 2017; Fan et al., 2019; Moon &\nSeo, 2020; GusmÃ£o et al., 2015; Silva et al., 2015; Hunicke & Chapman, 2004). For example, since\nthe advent of AlphaZero, human players have attempted to train themselves by using AI programs.\nSubsequently, many researchers have explored several methods to adjust the playing strength of\nAlphaZero-like programs to provide appropriate difficulty levels for human players (Wu et al., 2019;\nLiu et al., 2020; Fujita, 2022).\nHowever, although these methods can provide strength adjustment, two issues have arisen. First,\nwhile these methods can offer different strengths, human players often need to play several games\nor manually choose AI playing strength, consuming time to find a suitable strength for themselves.\nSecond, the behaviors between AI programs and human players are quite different. This occurs\nbecause most strength adjustment methods mainly focus on adjusting AI strength by calibrating the\nwin rate to around 50% for specific strengths, without considering the human behaviors at those\nstrengths. The problem is further exacerbated when human players attempt to use AI programs\nto analyze games and learn from the better actions suggested by AI. Therefore, designing AI\nprograms that can accurately estimate a playerâ€™s strength, provide corresponding playing strengths,\nand simultaneously offer human-like behavior is crucial for using superhuman AI in human learning.\nâˆ—These authors contributed equally.\nâ€ Corresponding author: tirongwu@iis.sinica.edu.tw\n1\narXiv:2502.17109v1  [cs.AI]  24 Feb 2025\n\nPublished as a conference paper at ICLR 2025\nTo address this challenge, this paper proposes a novel strength system, including a strength estimator\nand an SE-based MCTS, denoted as an SE-MCTS, which can predict strengths from games and\nprovide different playing strengths with a human-like playing style. Specifically, we propose a\nstrength estimator, based on the Bradley-Terry model (Bradley & Terry, 1952), which estimates\na strength score of an action at a game state, with higher scores indicating stronger actions. The\nstrength score can be further used to predict the strength of any given game, providing strength\nestimation without direct human interaction. Next, we present a novel strength adjustment approach\nwith human-like styles, named SE-MCTS, by incorporating the strength estimator into the Monte\nCarlo tree search (MCTS). During the MCTS, the search is limited to exploring actions that closely\ncorrespond to a given targeted strength score. We conduct experiments in Go, a challenging board\ngame for human players with a wide range of ranks. The results show several advantages of using\nour approach. First, the strength estimator significantly achieves over 80% accuracy in predicting\nranks within 15 games, compared to the previous method only achieves 49% accuracy even after\nevaluating 100 games. Second, the experiments show that SE-MCTS can not only provide designated\nranks but also achieve a playing style with 51.33% accuracy in aligning to human playersâ€™ actions,\nwhile previous state-of-the-art only obtained 42.56% accuracy. Finally, the strength estimator can\nbe trained with limited rank data and still accurately predict ranks. Furthermore, to demonstrate the\ngenerality of the proposed method, we apply SE and SE-MCTS to chess, achieving consistent results\nand significantly outperforming the previous state-of-the-art approach in both strength estimation and\nadjustment. These results show a promising direction for enhancing human-AI interactions in games.\n2\nBACKGROUND\n2.1\nBRADLEY-TERRY MODEL\nThe Bradley-Terry model (Bradley & Terry, 1952) is often used for pairwise comparisons, allowing\nfor the estimation of outcomes between individuals based on their relative strengths. In a group of\nindividuals, the model calculates the probability that individual i defeats individual j as P(i â‰»j) =\nÎ»i\nÎ»i+Î»j , where Î»i and Î»j represent the positive values of individuals i and j, respectively. A higher Î»i\nindicates a stronger individual. In practice, Î»i is usually defined by an exponential score function as\nÎ»i = eÎ²i, where Î²i represents the strength score of individual i.\nThe Bradley-Terry model can be further generalized to include comparison among more than two\nindividuals (Huang et al., 2006). Consider a group consisting of k individuals, indexed from 1 to k.\nThe probability that individual i wins out over the other individuals in this group is calculated as\nP(i) =\nÎ»i\nÎ»1+Î»2+...+Î»k . Furthermore, the model can be adapted for team comparisons, where each\nteam comprises multiple individuals. For example, assume team a consists of individuals 1 and 2,\nteam b consists of individuals 2, 3, and 4, and team c consists of individuals 1, 3, and 5. Then, the\nprobability that team a win against team b and c is defined as P(team a) =\nÎ»1Î»2\nÎ»1Î»2+Î»2Î»3Î»4+Î»1Î»3Î»5 ,\nwhere the strength of each team is determined by the product of the strengths of its individual members.\nDue to its generalization and broader extension, the Bradley-Terry model has been widely used in\nvarious fields, such as games (Coulom, 2007a), sports (Cattelan et al., 2013), and recommendation\nsystems (Chen & Joachims, 2016).\n2.2\nMONTE-CARLO TREE SEARCH\nMonte Carlo tree search (MCTS) (Coulom, 2007b; Kocsis & SzepesvÃ¡ri, 2006) is a best-first search al-\ngorithm that has been successfully used by AlphaZero (Silver et al., 2018) and MuZero (Schrittwieser\net al., 2020) to master both board games and Atari games. In AlphaZero, each MCTS simulation\nbegins by traversing the tree from the root node to a leaf node using the PUCT (Rosin, 2011) formula:\naâˆ—= arg max\na\n(\nQ(s, a) + c Â· P(s, a) Â·\npP\nb N(s, b)\n1 + N(s, a)\n)\n,\n(1)\nwhere Q(s, a) represents the estimated Q-value for the state-action pair (s, a), N(s, a) is the visit\ncounts, P(s, a) is the prior heuristic value, and c is a coefficient to control the exploration. Next, the\nleaf node is expanded and evaluated by a two-head network, fÎ¸(s) = (p, v), where p represents the\npolicy distribution and v denotes the win rate. The policy distribution p serves as the prior heuristic\n2\n\nPublished as a conference paper at ICLR 2025\nvalue. The win rate v is used to update the estimated Q-value of each node, from the leaf node to\nits ancestors up to the root node. This process is repeated iteratively, with more MCTS simulations\nleading to better decision-making. Finally, the node with the largest simulation counts is decided.\n2.3\nMCTS-BASED STRENGTH ADJUSTMENT\nStrength adjustment (Hunicke & Chapman, 2004; Paulsen & FÃ¼rnkranz, 2010; Silva et al., 2015;\nMoon & Seo, 2020) is crucial in the design of human-AI interactions, especially since AlphaZero\nachieved superhuman performance in many games like Go, Chess, and Shogi. As MCTS is widely\nused in these games, various methods have been explored to adapt it for strength adjustment (Sephton\net al., 2015; Wu et al., 2019; Demediuk et al., 2017; Fan et al., 2019; Moon et al., 2022). For instance,\nSephton et al. (2015) proposes adjusting the playing strength by using a strength index z. After the\nsearch, MCTS decides the node based on the proportionality of their simulation counts, with the\nprobability of selecting node i calculated as\nNz\ni\nPn\nj=1 Nz\nj , where Ni represents the simulation counts\nfor node i. A larger z value indicates a tendency to select stronger actions, while a smaller z favors\nweaker actions. Wu et al. (2019) further improves this method by introducing a threshold R to filter\nout lower-quality actions, removing nodes j where Nj < R Ã— Nmax, where Nmax represents the\nnode with largest simulation counts. The approach is used to adjust the playing strength of ELF\nOpenGo (Tian et al., 2019), resulting in covering a range of 800 Elo ratings within the interval\nz âˆˆ[âˆ’2, 2]. However, both methods only change the final decision in MCTS without modifying the\nsearch tree, leaving the search trees identical for different strengths.\n2.4\nSTRENGTH ESTIMATION\nStrength estimation is another important technique related to strength adjustment. With accurate\nstrength estimation, the AI can first predict a playerâ€™s strength and subsequently provide an appropriate\nlevel of difficulty for human learning. Several methods (MoudË‡rÃ­k & Neruda, 2016; Liu et al., 2020;\nEgri-Nagy & Tormanen, 2020; Scheible & SchÃ¼tze, 2014) have been proposed to estimate player\nstrength in games. For example, Liu et al. (2020) proposes estimating a playerâ€™s strength by using the\nstrength index with MCTS, as described in the previous subsection, to play against human players.\nSpecifically, the strength index z is adjusted after each game according to the game outcomes. Their\nexperiments show that z generally converges after about 20 games. However, this method requires\nhuman players to play against the MCTS programs with multiple games to obtain an estimation\nof their playing strengths. On the other hand, MoudË‡rÃ­k & Neruda (2016) proposes an alternative\napproach that categorizes the Go players into three ranks â€“ strong, median, and weak â€“ and uses a\nneural network to classify player ranks based on a game position using supervised learning. After\ntraining, given a game position, the neural network predicts ranks for each position by selecting\nthe highest probability. Furthermore, it can aggregate predictions across multiple positions. Two\nmethods are presented: (a) sum, which sums probabilities of all positions and makes a prediction\nbased on the highest probability; and (b) vote, which predicts the rank of each position first and\nselects the most frequent rank. However, this approach does not consider multiple actions during\ntraining and the experiment was limited to only three ranks. In addition, strength estimation can be\nformulated as a ranking problem (Burges et al., 2005; Xia et al., 2008), but it differs in a key aspect.\nRanking problems often focus on ordering items based on a single query, whereas in games, strength\nis assessed as overall skills across multiple positions or games. This challenge requires aggregating\nrankings across various scenarios to capture a playerâ€™s ability.\n3\nMETHOD\n3.1\nSTRENGTH ESTIMATOR\nWe introduce the strength estimator (SE), which is designed to predict the strength of an action a\nat a given state s based on human game records. Each state-action pair, denoted as p = (s, a), is\nlabeled with a rank r that corresponds to the playerâ€™s strength. For simplicity, in this paper, ranks are\nordered in descending order where rank 1, denoted as r1, represents the strongest level of play, and\nprogressively higher numbers indicate weaker playing strength. Each rank corresponds to a group\nof players, as we assume that players with the same rank have equivalent strength. Ranks could be\n3\n\nPublished as a conference paper at ICLR 2025\ndetermined by Elo ratings; for example, players with an Elo between 2400 and 2599 are classified as\nr1, players with an Elo between 2200 and 2399 as r2, and so on.\nConsider a game collection D that consists of numerous state-action pairs p, each associated with\na distinct rank r. Suppose there are n ranks in D, represented by r1, r2, ..., rn. Next, given a state-\naction pair p sampled from D, the strength estimator, denoted as SE(p) = Î», is designed to predict\nthe strength Î» corresponding to action a in state s. Consider two state-action pairs, p1 = (s, a1)\nand p2 = (s, a2), where a1 and a2 are played by r1 and r2, respectively. The strength estimator is\nexpected to predict strength Î»1 for p1 and Î»2 for p2, such that Î»1 â‰¥Î»2. Note that Î»1 = Î»2 occurs\nwhen the actions a1 and a2 are identical or are of equal strength. Following the Bradley-Terry model,\nwe can calculate the probability that r1 wins against r2 as P(r1 â‰»r2) =\nÎ»1\nÎ»1+Î»2 . To generalize to n\nranks, consider n state-action pairs, p1, p2, ..., pn, corresponding to ranks r1, r2, ..., rn, respectively.\nThe strength estimator predicts Î»i for each pi. The probability that r1 wins against all other ranks is\nthen calculated as P(r1 â‰»{r2, r3, . . . , rn}) =\nÎ»1\nÎ»1+Î»2+...+Î»n .\nFurthermore, we extend the method to estimate the composite strength of a rank by incorporating\nmultiple state-action pairs, collectively conceptualizing them as a team. This approach allows us to\neffectively measure the overall capabilities of players within specific ranks by considering various\nactions across different scenarios. Consider m state-action pairs pi,1, pi,2, ..., pi,m, where pi,j\nrepresents the j-th state-action pairs associated with ri sampled from D. The strength estimator\npredicts the strength Î»i,1, Î»i,2, ..., Î»i,m for each state-action pair, respectively. We define the\ncomposite strength for ri by aggregating all individual strengths using the geometric mean. The\ncomposite strength, denoted as Î›i, is calculated as Î›i = (Î»i,1Î»i,2 . . . Î»1,m)1/m.\nThe geometric mean is used to ensure that the strength estimator provides stable estimations and\nreflects the rankâ€™s ability across different scenarios. Namely, Î›i should remain consistent regardless\nof the number of state-action pairs considered:\nÎ›i = (Î»i,1Î»i,2 . . . Î»1,m)1/m =\n\u0012 m\nY\nj=1\nÎ»i,j\n\u0013 1\nm\n=\n\u0012\nm\nY\nj=1\npjâˆ¼D\nSE(pj)\n\u0013 1\nm\n,\n(2)\nwhere the state-action pair pj is randomly sampled from the game collection D. We can further\ncalculate the probability that rank r1 wins against all other ranks by using the composite strength:\nP(r1 â‰»{r2, r3, . . . , rn}) =\nÎ›1\nÎ›1+Î›2+...+Î›n .\nNote that our proposed method of aggregating strength using the geometric mean for teams differs\nfrom the Bradley-Terry model, which utilizes product aggregation. However, the geometric mean is\nspecifically tailored to our scenarios to guarantee consistent measurement of individual performance\nacross different games, rather than focusing on team member interactions. This approach also\nhelps to mitigate the influence of outliers. Moreover, this modification preserves the integrity of the\nBradley-Terry model principles, ensuring that the order of teams is strictly followed.\n3.2\nTRAINING THE STRENGTH ESTIMATOR\nThis subsection introduces a methodology for training strength estimator. For simplicity, we propose\nto train a neural network, fÎ¸(p) = Î², as a strength estimator which predicts a strength score Î² instead\nof strength Î» for a given state-action pair p. This strength score, Î², serves as the exponent for strength\nÎ» = eÎ², as defined by the Bradley-Terry model. Then, the composite strength, Î›i, from the equation\n2 can be expressed by using Î² as follows:\nÎ›i =\n\u0012 m\nY\nj=1\nÎ»i,j\n\u0013 1\nm\n=\n\u0012 m\nY\nj=1\neÎ²i,j\n\u0013 1\nm\n= e\n1\nm\nPm\nj=1 Î²i,j = eÎ²i,\n(3)\nwhere Î²i represents the average strength scores of m state-action pairs, each with ri, sampled from\nD.\nNext, given n ranks in the game collection, the strength estimator is optimized by maximizing the\nlikelihood L according to the ranking order (Xia et al., 2008; Chen et al., 2009). The likelihood is\n4\n\nPublished as a conference paper at ICLR 2025\nâ€¦â€¦\nğ‘Ÿ1\nâ€¦â€¦\nğ‘Ÿ2\nğ‘Ÿğ‘›\nğ‘š\nğ‘š\nğ‘š\nğ‘š\nğ‘Ÿâˆ\nâ€¦â€¦\nğ›½1 > ğ›½2, â€¦ , ğ›½âˆ\nğ›½2 > ğ›½3, â€¦ , ğ›½âˆ\nğ›½ğ‘›> ğ›½âˆ\nâ€¦\nğ›½1\nâ€¦â€¦\nğ›½2\nğ›½ğ‘›\nğ›½âˆ\naverage\nSSS\nğ›½2,1\nğ‘š\nSSS\nğ›½1,1\nğ‘š\nSSS\nğ›½âˆ,1\nğ‘š\nSSS\nğ›½ğ‘›,1\nğ‘š\nStrength\nEstimator\nFigure 1: The training process of the strength estimator.\ndefined as follows:\nL = P(r1 â‰»{r2, r3, . . . , rn}) Ã— P(r2 â‰»{r3, r4, . . . , rn}) Ã— . . . Ã— P(rnâˆ’1 â‰»rn)\n=\nnâˆ’1\nY\ni=1\nP(ri â‰»{ri+1, ri+2, . . . , rn}) =\nnâˆ’1\nY\ni=1\nÎ›i\nÎ›i + Î›i+1 + . . . + Î›n\n=\nnâˆ’1\nY\ni=1\neÎ²i\nPn\nj=i eÎ²j .\n(4)\nMaximizing L ensures that the strength scores of r1, r2, ..., rn are strictly in descending order, such\nthat r1 â‰»r2 â‰»r3 â‰». . . rn. Then, the loss function L can be defined by log-likelihood:\nL = âˆ’log(L) = âˆ’log\n\u0012nâˆ’1\nY\ni=1\neÎ²i\nPn\nj=i eÎ²j\n\u0013\n= âˆ’\nnâˆ’1\nX\ni=1\nlog(\neÎ²i\nPn\nj=i eÎ²j ).\n(5)\nFigure 1 illustrates the training process of the strength estimator. Assume there are n ranks, r1, r2, ...,\nrn, in the game collection. Initially, for each ri, we sample m state-action pairs and evaluate them by\nthe strength estimator. The strength estimator then outputs the strength score Î²i,j corresponding to\neach state-action pair. Subsequently, we average all Î²i,j to obtain Î²i for each ri. Finally, using all\nstrength scores, Î²i, we sequentially minimize each softmax loss as defined by the equation 5.\nSince the state-action pairs are collected only from human games, the strength estimator may provide\nunpredictable estimations for out-of-distribution state-action pairs, which rarely appear in human\ngames. To address this issue, we introduce an additional rank, râˆinto our training process, as\ndepicted by the dashed rectangle in Figure 1. This rank, râˆ, is defined as the weakest among all\nranks, ensuring that ri â‰»râˆfor all ri. To generate the state-action pairs for râˆ, we first select\na state-action pair, pi = (si, ai) from any ri. Then, we disturb the state-action pair pi to pâˆby\nmodifying the action ai to a randomly chosen legal action, resulting in pâˆ= (si, aâˆ). Since a\nrandom action aâˆis highly likely to result in an inferior outcome, these actions are expected to\ncorrespond to the weakest rank. Note that although a random action may occasionally result in a\nstrong action, the impact of such outliers will be minimized by the average Î²âˆas the number of\nsamples, m, increases.\n3.3\nSTRENGTH ESTIMATOR BASED MCTS FOR STRENGTH ADJUSTMENT\nWe present a novel method that integrates a strength estimator with MCTS to adjust strength dynami-\ncally, named SE-MCTS. In previous strength adjustment approaches, as described in subsection 2.3,\nthe MCTS search tree is unmodified during the search, with only changing the final action decision\nafter the search is complete. In contrast, we propose inherently modifying the search based on a\ntarget strength score to ensure that the search aligns more closely with the desired strength of ranks.\nSpecifically, in MCTS each node is evaluated by the strength estimator to obtain a strength score,\nÎ²(s, a), which represents the strength score of action a at state s. We can calculate the composite\nstrength score Î²(s, a), by averaging all Î² from the nodes within the subtree of state s. This is similar\nto the method used to calculate estimated Q-values. Given a targeted rank r with strength score Î²t, we\ncalculate the absolute strength difference for each node, which is denoted as Î´(s, a) = |Î²(s, a) âˆ’Î²t|.\n5\n\nPublished as a conference paper at ICLR 2025\nHigher values of Î´ indicate that the action is unlikely chosen by a player of the target rank, while\nlower Î´ suggest that the actions are closer to the strength of the target rank.\nAs the values Î´(s, a) are unbounded and can be any non-negative number, we normalize all values\nusing the minimum and maximum values observed in the current search tree. This normalization\nensures that the difference values are bounded within the [0, 1] interval, similar to the approach used\nin MuZero (Schrittwieser et al., 2020). Then, the PUCT formula in MCTS selection is modified from\nequation 1 as follows:\naâˆ—= arg max\na\n(\nQ(s, a) + c Â·\n\u0010\nP(s, a) âˆ’c1 Â· Ë†Î´(s, a)\n\u0011\nÂ·\npP\nb N(s, b)\n1 + N(s, a)\n)\n,\n(6)\nwhere Ë†Î´(s, a) is the normalized difference values of Î´(s, a) and c1 is a hyperparameter used to control\nthe confidence of prior heuristic values and difference values. Note that we choose to use Ë†Î´(s, a) to\neliminate the prior heuristic value P(s, a), rather than incorporating additional values like the use of\nQ(s, a). This is because, during the MCTS search, the algorithm first prioritizes actions based on\nhigher prior heuristic values and gradually shifts the focus to actions with higher Q-values when the\nsimulation counts increase. By combining P(s, a) and Ë†Î´(s, a), we effectively adjust the prioritization\nof actions, thereby aligning the search more closely with the desired strengths.\n4\nEXPERIMENTS\n4.1\nEXPERIMENT SETUP\nWe conducted experiments using the MiniZero framework (Wu et al., 2024). The human games are\ncollected1 from FoxWeiqi (YeHu, 2024), which is the largest online Go platform in terms of users.\nThese games are collected from amateur 5 kyu to 9 dan2, and are ranked in order from the strongest\nto weakest as follows: 9 dan, 8 dan, ..., 2 dan, 1 dan, 1-2 kyu, and 3-5 kyu, corresponding to r1, r2, ...,\nr11. Namely, a total of n = 11 ranks are used. Note that for kyu, we classify 1 to 2 kyu as one rank\nand 3 to 5 kyu as another rank, and we exclude games played by players ranked lower than 5 kyu.\nThis is because kyu players are still mastering basic Go strategies, their ranks often change rapidly.\nConsequently, their games do not consistently correspond to their ranks. For the training dataset,\nwe collect a total of 495,000 games, with 45,000 games from each rank. We also prepare a separate\ntesting dataset, including a candidate and a query dataset. The candidate dataset is used to estimate\nan average strength score of each rank, including a total of 1,100 games, with 100 games per rank.\nThe query dataset is used for the strength estimator to predict the strength, containing a total of 9,900\ngames, with 900 games per rank.\nThe network architecture of the strength estimator is similar to the AlphaZero network, consisting\nof 20 residual blocks with 256 channels. Given a state-action pair, the network outputs a policy\ndistribution p, a value v, and a strength score Î². The training loss for the policy and value network\nfollows AlphaZero, while the loss for the strength estimator is defined by equation 5. During training,\nwe aggregate the composite strength score Î²i by randomly selecting m = 7 state-action pairs from ri.\nOther training details are provided in the appendix.\n4.2\nPREDICTING RANKS FROM GAMES\nThe strength estimator can be utilized to predict ranks in games where the rank is unknown. We\nfirst calculate Î²i for each ri by evaluating all games in the candidate dataset. Next, for games from\nthe same unknown rank, ru, in the query dataset, a composite score Î²u is calculated by the strength\nestimator. Finally, ru is then determined to be ri, where |Î²u âˆ’Î²i| is the smallest among all ri.\nWe train two strength estimator networks, SE and SEâˆ, where SE is trained with 11 ranks, and SEâˆ\nincludes an additional rank, râˆ, for a total of 12 ranks. In addition, for comparison, we train another\n1We downloaded Go games from the FoxWeiqi online platform using its public download links.\n2In the game of Go, Kyu represents the beginner to decent amateur level, ranging from 18 kyu to 1 kyu, with\nlower numbers indicating stronger kyu players; Dan denotes advanced amateur, ranging from 1 dan to 9 dan,\nwith higher numbers indicating stronger dan players.\n6\n\nPublished as a conference paper at ICLR 2025\nnetwork based on supervised learning (SL), SLsum and SLvote, as mentioned in subsection 2.4. Both\nSLsum and SLvote are trained to classify 11 ranks for a given state-action pair but with different\naggregation methods.\nThe evaluation is conducted as follows. For each ri from the query dataset, each network evaluates all\nstate-action pairs from N randomly selected games and then predicts a rank. We repeat this prediction\nprocess 500 times for each N to ensure a stable estimation.\n0\n20\n40\n60\n80\n100\nGames\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nSE\nSEâˆ\nSLsum\nSLvote\nSEÂ±1\nSEâˆÂ±1\nSLsumÂ±1\nSLvoteÂ±1\n(a) Accuracy of rank predictions by different\nnetworks.\n0\n20\n40\n60\n80\n100\nGames\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nr1 (9 dan)\nr2 (8 dan)\nr3 (7 dan)\nr4 (6 dan)\nr5 (5 dan)\nr6 (4 dan)\nr7 (3 dan)\nr8 (2 dan)\nr9 (1 dan)\nr10 (1-2 kyu)\nr11 (3-5 kyu)\naverage\n(b) Accuracy of each rank for SEâˆ.\nFigure 2: Accuracy of rank prediction in Go, with the shaded area representing the 95% confidence\ninterval.\nFigure 2a shows the accuracy of predicting the games from the query dataset. From the figure, the\ntwo strength estimator networks significantly outperform the supervised learning networks. Both\nSE and SEâˆperform nearly identical performance, achieving over 80% of accuracy with only 15\ngames and reaching an accuracy of 97.5% after evaluating 100 games. In contrast, both SLsum and\nSLvote reached an accuracy of 49%, even after evaluating 100 games, with SLsum performs slightly\nbetter than SLvote. Furthermore, as human players do not always perform consistently and may\noccasionally change their ranks by one rank either above or below their actual rank, the games within\nri might involve players whose actual ranks are riâˆ’1 or ri+1, leading to slight fluctuations in the\ndataset. Therefore, we incorporate a prediction tolerance that allows for a deviation of one rank.\nSpecifically, if the network predicts riâˆ’1 or ri+1 for ri, we consider this prediction accurate. The\nresults show that both strength estimator networks achieve nearly 80% accuracy by only evaluating a\nsingle game, and perfectly predict the rank with an accuracy of over 99% after only 6 games, while\nthe supervised learning networks still cannot predict the correct ranks after 100 games. This result\nindicates that the ranks predicted by the strength estimators are close to the actual ranks even when\nthe prediction is incorrect.\nFigure 2b depicts the accuracy of each rank prediction for SEâˆwith several interesting observations.\nFirst, r1 (9 dan) has the highest accuracy among all ranks. Since 9 dan is the highest rank on\nFoxWeiqi, so professional Go players and Go AIs are also classified at this level, thus creating a\nsignificant strength gap between r1 and r2, and leading to a clearer distinction. This phenomenon\nresults in some players, originally ranked at r1 (9 dan), who are relatively stronger compared to\nr2 (8 dan), being relegated to r2. Consequently, r2 shows the lowest accuracy among all ranks.\nSecond, the prediction for r7 (3 dan) is below the average. This is because new players on FoxWeiqi\ncan only select a maximum initial rank of 3 dan and must advance gradually. Therefore, many\nplayers at this rank are actually stronger than 3 dan. Finally, the prediction for r11 (3-5 kyu) is also\nbelow average, corroborating to common understanding that the strength of kyu players is usually\ninconsistent. In conclusion, these results allow our strength estimator to provide evaluations of a\ngameâ€™s ranking system, further offering developers to make adjustments. Details on the accuracy of\neach rank prediction for other networks are provided in the appendix.\n4.3\nADJUSTING STRENGTH WITH STRENGTH ESTIMATOR\nIn this section, we evaluate the performance of SE-MCTS, as described in subsection 3.3, by\nincorporating the two trained strength estimator networks into MCTS to adjust the playing strength\nfor game playing. We first calculate the composite strength score Î²i, by averaging all Î² from the\nstate-action pairs in ri from the candidate set. Although we assume that the strength score Î²i for any\nstate-action pair from ri should be similar, in practice, we observe that Î²i may vary across different\n7\n\nPublished as a conference paper at ICLR 2025\naction numbers, as shown in Figure 3. In the beginning, particularly for the first actions, all Î²i are\nestimated as the same score. This is likely because the action choices at the beginning of Go have\nless variety, and weaker players can easily remember and imitate the openings from stronger players.\nTherefore, in the SE-MCTS, we propose using Î²d\ni instead of Î²i as the target strength score for each\naction, where Î²d\ni represent the composite strength score at d-th action for ri.\n20\n40\n60\n80\n100\n120\n140\n160\n180\n200\n220\n240\n260\nMove\n10\n8\n6\n4\n2\n0\n2\n4\nStrength Score\nr1 (9 dan)\nr2 (8 dan)\nr3 (7 dan)\nr4 (6 dan)\nr5 (5 dan)\nr6 (4 dan)\nr7 (3 dan)\nr8 (2 dan)\nr9 (1 dan)\nr10 (1-2 kyu)\nr11 (3-5 kyu)\nFigure 3: The composite strength score from SEâˆfor\neach rank across different actions in games.\nMCTS\nSA-MCTS\nSE-MCTS\nSEâˆ-MCTS\nr1\nr2\nr3\nr4\nr5\nr6\nr7\nr8\nr9\nr10\nr11\n92.0 Â±3.4 75.6 Â±5.3\n4.4 Â±2.6\n73.6 Â±5.5\n92.0 Â±3.4 62.0 Â±6.0\n6.0 Â±3.0\n73.2 Â±5.5\n92.0 Â±3.4 50.8 Â±6.2\n1.2 Â±1.4\n62.4 Â±6.0\n92.0 Â±3.4 55.6 Â±6.2\n2.0 Â±1.7\n51.6 Â±6.2\n92.0 Â±3.4 48.4 Â±6.2\n0.0 Â±0.0\n50.0 Â±0.0\n92.0 Â±3.4 49.2 Â±6.2\n1.2 Â±1.4\n43.6 Â±6.2\n92.0 Â±3.4 42.0 Â±6.1\n1.2 Â±1.4\n43.2 Â±6.2\n92.0 Â±3.4 31.2 Â±6.1\n0.0 Â±0.0\n22.4 Â±5.2\n92.0 Â±3.4 29.6 Â±5.7\n0.0 Â±0.0\n20.8 Â±5.0\n92.0 Â±3.4 19.2 Â±4.9\n0.0 Â±0.0\n5.6 Â±2.9\n92.0 Â±3.4\n8.0 Â±3.4\n0.0 Â±0.0\n4.0 Â±2.4\n0\n20\n40\n60\n80\n100\nWin Rate (%)\nFigure 4: Win rate against\nSEâˆ-MCTS5 in Go.\nTo evaluate the performance, we select four MCTS programs: (a) MCTS, representing vanilla MCTS\nwithout any strength adjustment mechanism, (b) SA-MCTS, which utilizes the strength index z (Wu\net al., 2019), (c) SE-MCTS, which uses SE network, and (d) SEâˆ-MCTS, which uses SEâˆnetwork.\nExcept MCTS, the remaining three programs can be adjusted to different strengths across a total of 11\nranks. We use SA-MCTSi, SE-MCTSi, and SEâˆ-MCTSi to represent the strength of each program\ncorrespond to ri. For SE-MCTS and SEâˆ-MCTS, we calculate the target strength score from the\ncandidate dataset for strength adjustment. However, for SA-MCTS, since the strength index z does\nnot directly correspond to any specific rank, we adjust z for each ri to ensure that each SA-MCTSi\nand SEâˆ-MCTSi achieve a comparable win rate, i.e., approximately 50%. Each z is in Appendix C.\nFigure 4 shows the win rate for each program playing against a baseline program, where the baseline\nis chosen as SEâˆ-MCTSi with i = 5 (5 dan). Generally, the win rate for SA-MCTSi decreases as\ni increases, except from i = 3 to i = 6 where there is a slight fluctuation. This corroborates the\nexperiments in (Wu et al., 2019). Interestingly, although SE can accurately predict the strength,\nSE-MCTSi cannot adjust the strength effectively. This is due to the exploration in the MCTS search,\nwhich may inevitably explore actions that are rarely seen in human games. Without training using\nrâˆ, SE provides inaccurate strength scores for these unseen actions. In contrast, the win rate of\nSEâˆ-MCTSi consistently decreases as i increases, demonstrating an accurate strength adjustment\nusing strength scores. We also include an experiment with different baselines in Appendix C.1.\nMoreover, we are interested in whether these programs can align with the human playerâ€™s behavior,\ni.e., if they can choose the same actions as human players. Therefore, we sample 50 human games\nfrom the query dataset for each rank and use four MCTS programs to play on every state-action pair.\nThe accuracy is evaluated based on whether the programs choose the same actions as human players.\nThe results are shown in Table 1. From the table, generally, the accuracy of ri decreases when the\nnumber i increases for all four programs. This is because weaker players are more unpredictable\nTable 1: Accuracy of move prediction for MCTS programs to human players in Go.\nrank (dan/kyu)\nMCTS\nSA-MCTS\nSE-MCTS\nSEâˆ-MCTS\nr1 (9 dan)\n53.05% Â± 0.95%\n47.00% Â± 0.95%\n53.06% Â± 0.95%\n53.73% Â± 0.95%\nr2 (8 dan)\n53.79% Â± 0.97%\n45.83% Â± 0.97%\n53.96% Â± 0.97%\n54.30% Â± 0.97%\nr3 (7 dan)\n52.70% Â± 0.98%\n46.70% Â± 0.98%\n54.28% Â± 0.97%\n53.88% Â± 0.98%\nr4 (6 dan)\n52.50% Â± 0.92%\n45.86% Â± 0.92%\n54.25% Â± 0.92%\n53.58% Â± 0.92%\nr5 (5 dan)\n49.48% Â± 0.93%\n42.29% Â± 0.92%\n52.00% Â± 0.93%\n50.35% Â± 0.93%\nr6 (4 dan)\n49.44% Â± 0.91%\n42.72% Â± 0.90%\n53.11% Â± 0.90%\n50.87% Â± 0.91%\nr7 (3 dan)\n50.75% Â± 0.89%\n42.68% Â± 0.88%\n53.71% Â± 0.89%\n51.40% Â± 0.89%\nr8 (2 dan)\n50.17% Â± 0.93%\n40.94% Â± 0.92%\n53.21% Â± 0.93%\n50.99% Â± 0.93%\nr9 (1 dan)\n48.10% Â± 0.89%\n40.94% Â± 0.88%\n52.60% Â± 0.89%\n49.44% Â± 0.89%\nr10 (1-2 kyu)\n46.95% Â± 0.91%\n36.58% Â± 0.88%\n50.06% Â± 0.91%\n47.84% Â± 0.91%\nr11 (3-5 kyu)\n46.87% Â± 0.89%\n36.64% Â± 0.86%\n51.36% Â± 0.89%\n48.23% Â± 0.89%\naverage\n50.35% Â± 0.28%\n42.56% Â± 0.28%\n52.87% Â± 0.28%\n51.33% Â± 0.28%\n8\n\nPublished as a conference paper at ICLR 2025\nthan stronger players. For MCTS, it achieves a high accuracy, exceeding 50%, but it cannot adjust\nstrength as shown in Figure 4. In contrast, although SA-MCTS can adjust strengths, it achieves the\nlowest accuracy among all four programs. This is due to SA-MCTS selecting actions proportional to\nthe simulation counts by using strength index z. The randomness results in diverging from human\nplaying styles. On the other hand, both SE-MCTS and SEâˆ-MCTS achieve a high accuracy, even\nbetter than MCTS. This is because SE-MCTS directly modifies the search by the strength scores,\nguiding the search to better align with human behavior. In conclusion, among all four programs,\nSEâˆ-MCTS not only adjusts strengths to specific ranks but also provides playing styles that are\nclosely aligned with those of human players at specific ranks.\n4.4\nTRAINING STRENGTH ESTIMATOR WITH LIMITED DATA\nIn this subsection, we investigate training a strength estimator with limited data. Unlike the supervised\nlearning methods, SLsum and SLvote, which require data from each rank, our method can estimate a\nstrength score and use it to predict ranks that were not observed during training. In niche games or\nthose favored by a specific group of enthusiasts, ranking systems are often not fully established due\nto a limited number of game records, and some specific ranks may be sparsely populated with only a\nfew players. Therefore, it is intriguing to explore whether the strength estimator can generalize to\nthese unseen strengths.\n0\n20\n40\n60\n80\n100\nGames\n0.1\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nr1 (9 dan)\nr2 (8 dan)\nr3 (7 dan)\nr4 (6 dan)\nr5 (5 dan)\nr6 (4 dan)\nr7 (3 dan)\nr8 (2 dan)\nr9 (1 dan)\nr10 (1-2 kyu)\nr11 (3-5 kyu)\naverage\n(a) 2-rank dataset.\n0\n20\n40\n60\n80\n100\nGames\n0.1\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nr1 (9 dan)\nr2 (8 dan)\nr3 (7 dan)\nr4 (6 dan)\nr5 (5 dan)\nr6 (4 dan)\nr7 (3 dan)\nr8 (2 dan)\nr9 (1 dan)\nr10 (1-2 kyu)\nr11 (3-5 kyu)\naverage\n(b) 3-rank dataset.\nFigure 5: Accuracy of rank predictions on the limited dataset of Go.\nWe train SEâˆon two separate datasets: the 2-rank dataset, containing r1 (9 dan) and r11 (3-5 kyu),\nand the 3-rank dataset, which includes the same ranks as the 2-rank dataset, plus an additional rank,\nr6 (4 dan). During the evaluation, we utilize the same methods, as described in 4.2, by using the\nstrength estimator to calculate Î²i for each ri in both candidate and query datasets, and then perform\nthe predictions. Figure 5a and 5b show the accuracy of rank prediction on 2-rank and 3-rank datasets,\nrespectively. On average, the strength estimator achieves an accuracy of over 80% after evaluating\n38 games for the 2-rank dataset and 21 games for the 3-rank datasets, respectively. Although these\nnumbers are larger than 15 â€“ the number of games needed by a strength estimator trained with all 11\nranks to achieve over 80% accuracy â€“ it can still effectively predict ranks that were not seen during\ntraining. This suggests that the strength estimator can generalize across a spectrum of ranks with\nfew ranks. Note that if sufficient data for more ranks are available, the accuracy still improves. Our\nmethod provides a way to both generalize with limited data and enhance performance as more data\nbecomes available.\n4.5\nGENERALIZING TO OTHER GAMES\nWe further experiment in another game, chess, to demonstrate the generality of our SE and SE-MCTS\napproaches. Similar to Go, chess is also a popular game with abundant human game records. The\ngames were collected from Lichess3 (Lichess, 2024), which uses Elo ratings as its ranking system.\nWe collect games with Elo ratings ranging from 1,000 to 2,600 and categorize them into eight ranks,\nwith each rank covering 200 Elo points and 240,000 games, for a total of 1,920,000 games. For the\ntesting dataset, the candidate dataset consists of 960 games, with 120 games per rank, while the query\ndataset contains 9,600 games, with 1,200 games per rank. Then, we apply experiments to chess,\n3Lichess is one of the most popular online chess platforms, with millions of active users.\n9\n\nPublished as a conference paper at ICLR 2025\nsimilar to those in Go. Note that the training algorithms remain identical, except the input features of\nthe neural network changed from Go to chess.\nThe results are consistent with the findings in Go. First, as shown in Figure 6, SEâˆachieves over\n80% accuracy in predicting ranks with only 26 games, significantly outperforming both SLsum and\nSLvote, and reaches an accuracy of 93.38% after evaluating 100 games. Second, Figure 7 shows\nthe win rate for each SEâˆ-MCTSi when playing against SEâˆ-MCTS5. The win rate consistently\ndecreases as i increases, demonstrating SEâˆ-MCTS can adjust its strength in chess. Finally, Table 2\nshows the accuracy of predicting human moves, with SEâˆ-MCTS achieving an average of 47.25%\naccuracy in aligning with human actions, outperforming both MCTS (46.56%), SA-MCTS (40.21%),\nand SE-MCTS (38.93%). In conclusion, this experiment demonstrates the versatility of our approach.\n0\n20\n40\n60\n80\n100\nGames\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nSE\nSEâˆ\nSLsum\nSLvote\nSEÂ±1\nSEâˆÂ±1\nSLsumÂ±1\nSLvoteÂ±1\nFigure 6: Accuracy of rank prediction in\nchess.\nSE-MCTS\nSEâˆ-MCTS\nr1\nr2\nr3\nr4\nr5\nr6\nr7\nr8\n23.6 Â±5.5\n84.8 Â±5.2\n27.6 Â±5.7\n76.2 Â±6.1\n24.0 Â±5.5\n74.2 Â±6.0\n13.8 Â±4.6\n63.6 Â±6.3\n4.8 Â±3.0\n50.0 Â±0.0\n5.6 Â±3.3\n38.4 Â±6.2\n4.0 Â±2.9\n24.4 Â±5.6\n2.8 Â±2.4\n17.2 Â±5.0\n0\n20\n40\n60\n80\n100\nWin Rate (%)\nFigure 7: Win rate against\nSEâˆ-MCTS5 in chess.\nTable 2: Accuracy of move prediction for MCTS programs to human chess players.\nrank (Elo)\nMCTS\nSA-MCTS\nSE-MCTS\nSEâˆ-MCTS\nr1 (2400 âˆ’2599)\n51.97% Â±0.69%\n50.17% Â±0.69%\n41.05% Â±0.68%\n51.51% Â±0.69%\nr2 (2200 âˆ’2399)\n51.58% Â±0.69%\n47.49% Â±0.69%\n40.19% Â±0.68%\n51.14% Â±0.69%\nr3 (2000 âˆ’2199)\n49.23% Â±0.69%\n45.01% Â±0.69%\n41.40% Â±0.68%\n49.19% Â±0.69%\nr4 (1800 âˆ’1999)\n46.52% Â±0.69%\n41.78% Â±0.68%\n38.66% Â±0.67%\n47.26% Â±0.69%\nr5 (1600 âˆ’1799)\n45.45% Â±0.69%\n38.18% Â±0.67%\n36.76% Â±0.67%\n46.62% Â±0.69%\nr6 (1400 âˆ’1599)\n44.33% Â±0.69%\n36.84% Â±0.67%\n37.63% Â±0.67%\n46.12% Â±0.69%\nr7 (1200 âˆ’1399)\n41.54% Â±0.68%\n31.49% Â±0.64%\n37.65% Â±0.67%\n43.04% Â±0.68%\nr8 (1000 âˆ’1199)\n41.89% Â±0.68%\n30.72% Â±0.64%\n38.05% Â±0.67%\n43.08% Â±0.68%\naverage\n46.56% Â±0.24%\n40.21% Â±0.24%\n38.93%Â±0.24%\n47.25% Â±0.24%\n5\nDISCUSSION\nThis paper introduces a novel strength system, including a strength estimator for evaluating the\nstrength from game records without requiring direct interaction with human players, and an SE-MCTS\nfor adjusting the playing strength using strength scores provided by the strength estimator. When\npredicting ranks in the game of Go, our strength estimator significantly achieves over 80% accuracy\nby examining only 15 games, whereas the previous supervised learning method only reached 49%\naccuracy even after evaluating 100 games. The strength estimator can be trained with limited rank\ndata and still accurately predict unseen rank data, providing extensive generalizability. For strength\nadjustment, SE-MCTS successfully adjusts to designated ranks while providing a playing style that\naligns with human behavior, achieving an average accuracy of 51.33%, compared to the previous\nstate-of-the-art method that only reached 42.56% accuracy. Furthermore, we apply our method to the\ngame of chess and obtain consistent results to Go, demonstrating the generality of our approach.\nOne limitation of our work is that the strength estimator relies on human game records for training.\nHowever, this issue could potentially be addressed by using all models trained by AlphaZero, which\nmay serve as players of different playing strengths to generate games. Besides, the strength system\nalso provides several benefits for future directions. For example, game designers can use the strength\nestimator to evaluate their ranking systems. The strength estimator can evaluate a game by examining\nthe strength scores for each action, and use it to identify incorrect actions for human players or\nfor cheat detection (Alayed et al., 2013). Furthermore, we can extend our strength estimator by\nincorporating opponent-specific strength scores to address the Bradley-Terry modelâ€™s limitations in\ncapturing intransitivity (Balduzzi et al., 2018; Bertrand et al., 2023; Omidshafiei et al., 2019; Vadori\n& Savani, 2024). Finally, the search tree of SE-MCTS can offer the opportunity for explainability of\nAI actions in human learning.\n10\n\nPublished as a conference paper at ICLR 2025\nETHICS STATEMENT\nThis paper presents a method for estimating player strength and adjusting it to specific ranks, allowing\nagents to play in a human-like manner. A potential ethical concern is that our method could be\nexploited by human players to develop human-like agents for cheating in human competitions.\nHowever, we emphasize that all models trained in this paper are used strictly for research purposes\nand adhere to established ethical guidelines.\nREPRODUCIBILITY STATEMENT\nWe have provided detailed descriptions of the method, implementation, and training hyperpa-\nrameters in Section 3, Section 4, and Appendix A to facilitate the reproduction of our exper-\niments.\nThe source code, along with a README file containing instructions is available at\nhttps://rlg.iis.sinica.edu.tw/papers/strength-estimator.\nACKNOWLEDGEMENT\nThis research is partially supported by the National Science and Technology Council (NSTC) of\nthe Republic of China (Taiwan) under Grant Number NSTC 113-2221-E-001-009-MY3, NSTC\n113-2634-F-A49-004, and NSTC 113-2221-E-A49-127.\nREFERENCES\nHashem Alayed, Fotos Frangoudes, and Clifford Neuman. Behavioral-based cheating detection in\nonline first person shooters using machine learning techniques. In 2013 IEEE Conference on\nComputational Inteligence in Games (CIG), pp. 1â€“8, August 2013.\nDavid Balduzzi, Karl Tuyls, Julien Perolat, and Thore Graepel. Re-evaluating evaluation. Advances\nin Neural Information Processing Systems, 31, 2018.\nQuentin Bertrand, Wojciech Marian Czarnecki, and Gauthier Gidel. On the Limitations of the\nElo, Real-World Games are Transitive, not Additive. In Proceedings of The 26th International\nConference on Artificial Intelligence and Statistics, pp. 2905â€“2921. PMLR, April 2023.\nRalph Allan Bradley and Milton E. Terry. Rank Analysis of Incomplete Block Designs: I. The\nMethod of Paired Comparisons. Biometrika, 39(3/4):324â€“345, 1952.\nChris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg\nHullender. Learning to rank using gradient descent. In Proceedings of the 22nd International\nConference on Machine Learning, ICML â€™05, pp. 89â€“96, New York, NY, USA, August 2005.\nAssociation for Computing Machinery.\nManuela Cattelan, Cristiano Varin, and David Firth. Dynamic Bradleyâ€“Terry modelling of sports\ntournaments. Journal of the Royal Statistical Society : Series C, Volulme 62(Number 1):135â€“150,\nJanuary 2013.\nShuo Chen and Thorsten Joachims. Predicting Matchups and Preferences in Context. In Proceedings\nof the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,\nKDD â€™16, pp. 775â€“784, New York, NY, USA, 2016. Association for Computing Machinery.\nWei Chen, Tie-yan Liu, Yanyan Lan, Zhi-ming Ma, and Hang Li. Ranking Measures and Loss\nFunctions in Learning to Rank. In Advances in Neural Information Processing Systems, volume 22.\nCurran Associates, Inc., 2009.\nRÃ©mi Coulom. Computing Elo Ratings of Move Patterns in the Game of Go. ICGA Journal, 30(4):\n198â€“208, December 2007a.\nRÃ©mi Coulom. Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search. In Computers\nand Games, Lecture Notes in Computer Science, pp. 72â€“83, Berlin, Heidelberg, 2007b. Springer.\n11\n\nPublished as a conference paper at ICLR 2025\nSimon Demediuk, Marco Tamassia, William Raffe, Fabio Zambetta, Xiaodong Li, and Florian\nMueller. Monte Carlo tree search based algorithms for dynamic difficulty adjustment. In 2017\nIEEE Conference on Computational Intelligence and Games (CIG), pp. 53â€“59, August 2017.\nAttila Egri-Nagy and Antti Tormanen. Derived metrics for the game of Go â€“ intrinsic network\nstrength assessment and cheat-detection. 2020 Eighth International Symposium on Computing and\nNetworking (CANDAR), pp. 9â€“18, November 2020.\nTianwen Fan, Yuan Shi, Wanxiang Li, and Kokolo Ikeda. Position Control and Production of Various\nStrategies for Deep Learning Go Programs. In 2019 International Conference on Technologies\nand Applications of Artificial Intelligence (TAAI), pp. 1â€“6, January 2019.\nKazuhisa Fujita. AlphaDDA: Strategies for adjusting the playing strength of a fully trained AlphaZero\nsystem to a suitable human training partner. PeerJ Computer Science, 8:e1123, October 2022.\nRenÃª GusmÃ£o, Kennet Calixto, and Caetano Segundo. Dynamic difficulty adjustment through\nparameter manipulation for Space Shooter game. In 2015 14th Brazilian Symposium on Computer\nGames and Digital Entertainment (SBGames), September 2015.\nTzu-Kuo Huang, Ruby Weng, and Chih-Jen Lin. Generalized Bradley-Terry Models and Multi-Class\nProbability Estimates. Journal of Machine Learning Research, 7:85â€“115, January 2006.\nRobin Hunicke and Vernell Chapman. AI for dynamic difficulty adjustment in games. Challenges in\ngame artificial intelligence AAAI workshop, 2, January 2004.\nLevente Kocsis and Csaba SzepesvÃ¡ri. Bandit Based Monte-Carlo Planning. In European Conference\non Machine Learning and Principles and Practice of Knowledge Discovery in Databases, volume\n2006, pp. 282â€“293, September 2006.\nLichess.\nlichess.org open database, 2024.\nURL https://database.lichess.org/\n#standard_games. Accessed: 2024-09-29.\nAn-Jen Liu, Ti-Rong Wu, I-Chen Wu, Hung Guei, and Ting-Han Wei. Strength Adjustment and\nAssessment for MCTS-Based Programs [Research Frontier]. IEEE Computational Intelligence\nMagazine, 15(3):60â€“73, August 2020.\nHee-Seung Moon and Jiwon Seo. Dynamic Difficulty Adjustment via Fast User Adaptation. In Ad-\njunct Publication of the 33rd Annual ACM Symposium on User Interface Software and Technology,\npp. 13â€“15, October 2020.\nJaeYoung Moon, YouJin Choi, TaeHwa Park, JunDoo Choi, Jin-Hyuk Hong, and Kyung-Joong\nKim. Diversifying dynamic difficulty adjustment agent by integrating player state models into\nMonte-Carlo tree search. Expert Systems with Applications, 205:117677, November 2022.\nJosef MoudË‡rÃ­k and Roman Neruda. Determining Player Skill in the Game of Go with Deep Neural\nNetworks. In Theory and Practice of Natural Computing, pp. 188â€“195, Cham, 2016. Springer\nInternational Publishing.\nShayegan Omidshafiei, Christos Papadimitriou, Georgios Piliouras, Karl Tuyls, Mark Rowland,\nJean-Baptiste Lespiau, Wojciech M. Czarnecki, Marc Lanctot, Julien Perolat, and Remi Munos.\nÎ±-Rank: Multi-Agent Evaluation by Evolution. Scientific Reports, 9(1):9937, July 2019.\nOpenAI, Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, PrzemysÅ‚aw DË›ebiak,\nChristy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, Rafal JÃ³zefowicz,\nScott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique P. d O. Pinto, Jonathan\nRaiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang,\nFilip Wolski, and Susan Zhang. Dota 2 with Large Scale Deep Reinforcement Learning, December\n2019.\nPhilip Paulsen and Johannes FÃ¼rnkranz. A Moderately Successful Attempt to Train Chess Evaluation\nFunctions of Different Strengths. In Proceedings of the ICML-10 Workshop on Machine Learning\nand Games, 2010.\n12\n\nPublished as a conference paper at ICLR 2025\nChristopher D. Rosin. Multi-armed bandits with episode context. Annals of Mathematics and\nArtificial Intelligence, 61(3):203â€“230, March 2011.\nChristian Scheible and Hinrich SchÃ¼tze. Picking the Amateurâ€™s Mind - Predicting Chess Player\nStrength from Game Annotations. In Proceedings of COLING 2014, the 25th International\nConference on Computational Linguistics: Technical Papers, pp. 311â€“321, Dublin, Ireland, August\n2014. Dublin City University and Association for Computational Linguistics.\nJulian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon\nSchmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy Lillicrap, and\nDavid Silver. Mastering Atari, Go, chess and shogi by planning with a learned model. Nature, 588\n(7839):604â€“609, December 2020.\nNick Sephton, Peter I. Cowling, and Nicholas H. Slaven. An experimental study of action selection\nmechanisms to create an entertaining opponent. In 2015 IEEE Conference on Computational\nIntelligence and Games (CIG), pp. 122â€“129, Tainan, Taiwan, August 2015. IEEE.\nMirna Silva, Victor Silva, and Luiz Chaimowicz. Dynamic Difficulty Adjustment through an Adaptive\nAI. In 14th Brazilian Symposium on Computer Games and Digital Entertainment (SBGames), pp.\n173â€“182, November 2015.\nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur\nGuez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen\nSimonyan, and Demis Hassabis. A general reinforcement learning algorithm that masters chess,\nshogi, and Go through self-play. Science, 362(6419):1140â€“1144, December 2018.\nYuandong Tian, Jerry Ma, Qucheng Gong, Shubho Sengupta, Zhuoyuan Chen, James Pinkerton,\nand Larry Zitnick. ELF OpenGo: An analysis and open reimplementation of AlphaZero. In\nProceedings of the 36th International Conference on Machine Learning, pp. 6244â€“6253. PMLR,\nMay 2019.\nNelson Vadori and Rahul Savani. Ordinal Potential-based Player Rating. In Proceedings of The 27th\nInternational Conference on Artificial Intelligence and Statistics, pp. 118â€“126. PMLR, April 2024.\nOriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, MichaÃ«l Mathieu, Andrew Dudzik, Junyoung\nChung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan,\nManuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John P. Agapiou, Max\nJaderberg, Alexander S. Vezhnevets, RÃ©mi Leblond, Tobias Pohlen, Valentin Dalibard, David\nBudden, Yury Sulsky, James Molloy, Tom L. Paine, Caglar Gulcehre, Ziyu Wang, Tobias Pfaff,\nYuhuai Wu, Roman Ring, Dani Yogatama, Dario WÃ¼nsch, Katrina McKinney, Oliver Smith, Tom\nSchaul, Timothy Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps, and David Silver.\nGrandmaster level in StarCraft II using multi-agent reinforcement learning. Nature, 575(7782):\n350â€“354, November 2019.\nI.-Chen Wu, Ti-Rong Wu, An-Jen Liu, Hung Guei, and Tinghan Wei. On Strength Adjustment for\nMCTS-Based Programs. Proceedings of the AAAI Conference on Artificial Intelligence, 33(01):\n1222â€“1229, July 2019.\nTi-Rong Wu, Hung Guei, Pei-Chiun Peng, Po-Wei Huang, Ting Han Wei, Chung-Chin Shih, and\nYun-Jui Tsai. MiniZero: Comparative Analysis of AlphaZero and MuZero on Go, Othello, and\nAtari Games. IEEE Transactions on Games, pp. 1â€“13, 2024.\nFen Xia, Tie-Yan Liu, Jue Wang, Wensheng Zhang, and Hang Li. Listwise approach to learning to\nrank - Theory and algorithm. In Proceedings of the 25th International Conference on Machine\nLearning, pp. 1192â€“1199, January 2008.\nYeHu. Fox Weiqi, 2024. URL https://www.foxwq.com/. Accessed: 2024-09-29.\n13\n\nPublished as a conference paper at ICLR 2025\nA\nDETAILED TRAINING SETTINGS FOR THE STRENGTH ESTIMATOR\nThe feature design in the strength estimator of Go is similar to AlphaZero (Silver et al., 2018).\nSpecifically, we use 18 channels to represent a board position, where the first 16 channels are the\nboard configurations from the past eight moves for both black and white stones. The remaining\ntwo channels are binary indicators of the color of the next player, i.e., one channel for Black and\nWhite. For chess, the feature design also follows the same approach as AlphaZero, which includes\n119 input channels. During training, for each rank, we randomly select seven state-action pairs. We\nalso perform data augmentation to further enhance the diversity of the training data. The network\nis optimized using stochastic gradient descent (SGD), with the loss function specified in Equation\n5. It is important to note that when training SEâˆ, the policy and value loss for state-action pairs of\nrâˆare not calculated, since these heads should only consider actual human playersâ€™ actions. The\nlearning rate is initially set at 0.01 and is halved after 100,000 training steps. The entire training\nprocess encompasses 130,000 steps, consuming around 242 GPU hours for Go and 69 GPU hours for\nchess on an NVIDIA RTX A5000 graphics card. Other hyperparameters are listed in Table 3.\nTable 3: Hyperparameters for training strength estimators.\nParameter\nGo\nChess\nNumber of Blocks\n20\n20\nInput Channel\n18\n119\nHidden Channel\n256\n256\nLearning Rate\n0.01 to 0.005\n0.01 to 0.005\nTraining Steps\n130,000\n130,000\nOptimizer\nSGD\nSGD\nMain Memory\n384GB\nCentral Processing Unit (CPU)\nIntel Xeon Silver 4216 (2.1 GHz)\nGraphical Processing Unit (GPU)\nNVIDIA RTX A5000\nGPU Hours\n242\n69\nB\nIN-DEPTH ANALYSIS FOR STRENGTH ESTIMATOR IN GO\nWe conduct in-depth analyses for strength estimators in Go. First, we present detailed insights into\npredicting ranks from games, as detailed in Subsection B.1. Second, we demonstrate the outcomes of\nstrength prediction using fewer moves in a single game, discussed in Subsection B.2. Finally, we\nexplore predictions based solely on the first 50 actions or the last 50 actions in games, which are\nelaborated in Subsection B.3 and Subsection B.4, respectively.\nB.1\nPREDICTING RANKS FROM GAMES\nFigure 8 shows the accuracy of rank predictions for different networks. We observe that in Figure\n8a and Figure 8b, SLvote and SLsum can only distinguish on some ranks, such as 3-5 kyu. This is\nbecause these models do not contain sufficient information to differentiate all ranks based on a single\nstate-action during training (MoudË‡rÃ­k & Neruda, 2016). In Figure 8c and Figure 8d, even though\nwe incorporated a prediction tolerance for these two methods, they still cannot perfectly distinguish\nall ranks, even after 100 games. In Figure 8e and Figure 8f, although our models SE and SEâˆ\ncannot perfectly predict all ranks without incorporating a prediction tolerance, they still achieve high\nperformance across all ranks. In Figure 8g and Figure 8h, when we allow a prediction tolerance, we\nachieve 100% accuracy across all ranks. This result further indicates that our model can differentiate\nthe strength relationship across all ranks.\nB.2\nPREDICTING RANKS FROM GAME POSITIONS\nWe are also interested in whether we can predict the rank using only game positions. Specifically,\nonly one game position can be chosen for each game instead of all actions when predicting the\n14\n\nPublished as a conference paper at ICLR 2025\n0\n20\n40\n60\n80\n100\nGames\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nr1 (9 dan)\nr2 (8 dan)\nr3 (7 dan)\nr4 (6 dan)\nr5 (5 dan)\nr6 (4 dan)\nr7 (3 dan)\nr8 (2 dan)\nr9 (1 dan)\nr10 (1-2 kyu)\nr11 (3-5 kyu)\naverage\n(a) SLvote\n0\n20\n40\n60\n80\n100\nGames\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nr1 (9 dan)\nr2 (8 dan)\nr3 (7 dan)\nr4 (6 dan)\nr5 (5 dan)\nr6 (4 dan)\nr7 (3 dan)\nr8 (2 dan)\nr9 (1 dan)\nr10 (1-2 kyu)\nr11 (3-5 kyu)\naverage\n(b) SLsum\n0\n20\n40\n60\n80\n100\nGames\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nr1 (9 dan)\nr2 (8 dan)\nr3 (7 dan)\nr4 (6 dan)\nr5 (5 dan)\nr6 (4 dan)\nr7 (3 dan)\nr8 (2 dan)\nr9 (1 dan)\nr10 (1-2 kyu)\nr11 (3-5 kyu)\naverage\n(c) SLvoteÂ±1\n0\n20\n40\n60\n80\n100\nGames\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nr1 (9 dan)\nr2 (8 dan)\nr3 (7 dan)\nr4 (6 dan)\nr5 (5 dan)\nr6 (4 dan)\nr7 (3 dan)\nr8 (2 dan)\nr9 (1 dan)\nr10 (1-2 kyu)\nr11 (3-5 kyu)\naverage\n(d) SLsumÂ±1\n0\n20\n40\n60\n80\n100\nGames\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nr1 (9 dan)\nr2 (8 dan)\nr3 (7 dan)\nr4 (6 dan)\nr5 (5 dan)\nr6 (4 dan)\nr7 (3 dan)\nr8 (2 dan)\nr9 (1 dan)\nr10 (1-2 kyu)\nr11 (3-5 kyu)\naverage\n(e) SE\n0\n20\n40\n60\n80\n100\nGames\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nr1 (9 dan)\nr2 (8 dan)\nr3 (7 dan)\nr4 (6 dan)\nr5 (5 dan)\nr6 (4 dan)\nr7 (3 dan)\nr8 (2 dan)\nr9 (1 dan)\nr10 (1-2 kyu)\nr11 (3-5 kyu)\naverage\n(f) SEâˆ\n0\n20\n40\n60\n80\n100\nGames\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nr1 (9 dan)\nr2 (8 dan)\nr3 (7 dan)\nr4 (6 dan)\nr5 (5 dan)\nr6 (4 dan)\nr7 (3 dan)\nr8 (2 dan)\nr9 (1 dan)\nr10 (1-2 kyu)\nr11 (3-5 kyu)\naverage\n(g) SEÂ±1\n0\n20\n40\n60\n80\n100\nGames\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nr1 (9 dan)\nr2 (8 dan)\nr3 (7 dan)\nr4 (6 dan)\nr5 (5 dan)\nr6 (4 dan)\nr7 (3 dan)\nr8 (2 dan)\nr9 (1 dan)\nr10 (1-2 kyu)\nr11 (3-5 kyu)\naverage\n(h) SEâˆÂ±1\nFigure 8: Accuracy of rank prediction for different networks in Go.\nranks, shown in Figure 9. According to Figures 9a and 9b, both SLvote and SLsum show similar\nperformance as in the situation of using all actions. In our method, Figures 9c and 9d demonstrate\nthat across 20 games, the accuracy decreases from 80%, when predictions are based on all actions, to\napproximately 60% when using just one action. This is intuitive because using the information from\nthe entire game would help capture the playerâ€™s strength. However, achieving 60% accuracy with\nunrelated actions among 20 games indicates that our model can still predict accurate ranks based on\none action of different games.\nB.3\nPREDICTING RANKS FROM THE FIRST 50 ACTIONS IN THE GAME\nWe are interested in evaluating performance when using only the first 50 actions of a game, known as\nthe fuseki stage in Go. Figures 10a and 10b indicate that SLvote and SLsum, utilizing these initial\nactions, achieve similar performance to predictions made using all actions in the game. Figures\n10c and 10d present the prediction results by our methods when limited to the first 50 actions.\n15\n\nPublished as a conference paper at ICLR 2025\n0\n20\n40\n60\n80\n100\nGames\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nr1 (9 dan)\nr2 (8 dan)\nr3 (7 dan)\nr4 (6 dan)\nr5 (5 dan)\nr6 (4 dan)\nr7 (3 dan)\nr8 (2 dan)\nr9 (1 dan)\nr10 (1-2 kyu)\nr11 (3-5 kyu)\naverage\n(a) SLvote\n0\n20\n40\n60\n80\n100\nGames\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nr1 (9 dan)\nr2 (8 dan)\nr3 (7 dan)\nr4 (6 dan)\nr5 (5 dan)\nr6 (4 dan)\nr7 (3 dan)\nr8 (2 dan)\nr9 (1 dan)\nr10 (1-2 kyu)\nr11 (3-5 kyu)\naverage\n(b) SLsum\n0\n20\n40\n60\n80\n100\nGames\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nr1 (9 dan)\nr2 (8 dan)\nr3 (7 dan)\nr4 (6 dan)\nr5 (5 dan)\nr6 (4 dan)\nr7 (3 dan)\nr8 (2 dan)\nr9 (1 dan)\nr10 (1-2 kyu)\nr11 (3-5 kyu)\naverage\n(c) SE\n0\n20\n40\n60\n80\n100\nGames\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nr1 (9 dan)\nr2 (8 dan)\nr3 (7 dan)\nr4 (6 dan)\nr5 (5 dan)\nr6 (4 dan)\nr7 (3 dan)\nr8 (2 dan)\nr9 (1 dan)\nr10 (1-2 kyu)\nr11 (3-5 kyu)\naverage\n(d) SEâˆ\nFigure 9: Accuracy of rank prediction for different networks in the game positions of Go.\nClearly, the overall accuracy has declined. Notably, the accuracy for kyu players shows significant\ndownward trends. This is mainly because the actions in the fuseki stage at these ranks are similar,\nthus complicating accurate predictions. This suggests that enhancing performance during the fuseki\ncould be crucial for human players aiming to progress from kyu to dan rank.\nB.4\nPREDICTING RANKS FROM THE LAST 50 ACTIONS IN THE GAME\nSimilarly, we examine the performance when using only the last 50 actions of the game, referred to as\nthe yose stage in Go. Figures 10e and 10f show that SLvote and SLsum, employing these final actions,\nmaintain similar performance to predictions based on all actions in the game. In our method, Figures\n10g and 10h display the prediction results using only the last 50 actions. As before, the overall\naccuracy has declined. However, the accuracy for 9 dan players between predictions made using the\nentire game and just the last 50 actions does not differ significantly. This is likely because the yose\nstage involves complex calculations and judgments, areas where top players excel. Furthermore, in\nmost games, especially those between the highest-skilled players, the outcome is often determined\nbefore the yose stage. This leads to less practice and proficiency in this phase among players of lower\nranks. Additionally, we observe a significant drop in accuracy for 8 dan players when predictions are\nbased solely on yose stage. This could be because some 8 dan players have comparable yose skills to\nthose of 9 dan players, leading to some misclassifications of 8 dan players as 9 dan.\n16\n\nPublished as a conference paper at ICLR 2025\n0\n20\n40\n60\n80\n100\nGames\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nr1 (9 dan)\nr2 (8 dan)\nr3 (7 dan)\nr4 (6 dan)\nr5 (5 dan)\nr6 (4 dan)\nr7 (3 dan)\nr8 (2 dan)\nr9 (1 dan)\nr10 (1-2 kyu)\nr11 (3-5 kyu)\naverage\n(a) SLvote with first 50 actions\n0\n20\n40\n60\n80\n100\nGames\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nr1 (9 dan)\nr2 (8 dan)\nr3 (7 dan)\nr4 (6 dan)\nr5 (5 dan)\nr6 (4 dan)\nr7 (3 dan)\nr8 (2 dan)\nr9 (1 dan)\nr10 (1-2 kyu)\nr11 (3-5 kyu)\naverage\n(b) SLsum with first 50 actions\n0\n20\n40\n60\n80\n100\nGames\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nr1 (9 dan)\nr2 (8 dan)\nr3 (7 dan)\nr4 (6 dan)\nr5 (5 dan)\nr6 (4 dan)\nr7 (3 dan)\nr8 (2 dan)\nr9 (1 dan)\nr10 (1-2 kyu)\nr11 (3-5 kyu)\naverage\n(c) SE first 50 actions\n0\n20\n40\n60\n80\n100\nGames\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nr1 (9 dan)\nr2 (8 dan)\nr3 (7 dan)\nr4 (6 dan)\nr5 (5 dan)\nr6 (4 dan)\nr7 (3 dan)\nr8 (2 dan)\nr9 (1 dan)\nr10 (1-2 kyu)\nr11 (3-5 kyu)\naverage\n(d) SEâˆwith first 50 actions\n0\n20\n40\n60\n80\n100\nGames\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nr1 (9 dan)\nr2 (8 dan)\nr3 (7 dan)\nr4 (6 dan)\nr5 (5 dan)\nr6 (4 dan)\nr7 (3 dan)\nr8 (2 dan)\nr9 (1 dan)\nr10 (1-2 kyu)\nr11 (3-5 kyu)\naverage\n(e) SLvote with last 50 actions\n0\n20\n40\n60\n80\n100\nGames\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nr1 (9 dan)\nr2 (8 dan)\nr3 (7 dan)\nr4 (6 dan)\nr5 (5 dan)\nr6 (4 dan)\nr7 (3 dan)\nr8 (2 dan)\nr9 (1 dan)\nr10 (1-2 kyu)\nr11 (3-5 kyu)\naverage\n(f) SLsum with last 50 actions\n0\n20\n40\n60\n80\n100\nGames\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nr1 (9 dan)\nr2 (8 dan)\nr3 (7 dan)\nr4 (6 dan)\nr5 (5 dan)\nr6 (4 dan)\nr7 (3 dan)\nr8 (2 dan)\nr9 (1 dan)\nr10 (1-2 kyu)\nr11 (3-5 kyu)\naverage\n(g) SE last 50 actions\n0\n20\n40\n60\n80\n100\nGames\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nr1 (9 dan)\nr2 (8 dan)\nr3 (7 dan)\nr4 (6 dan)\nr5 (5 dan)\nr6 (4 dan)\nr7 (3 dan)\nr8 (2 dan)\nr9 (1 dan)\nr10 (1-2 kyu)\nr11 (3-5 kyu)\naverage\n(h) SEâˆwith last 50 actions\nFigure 10: Accuracy of rank prediction for different networks using only the first or the last 50\nactions of the game in Go.\n17\n\nPublished as a conference paper at ICLR 2025\nC\nDETAILED EXPERIMENTS FOR STRENGTH ADJUSTMENT\nTable 4 presents the value of z for SA-MCTSi used in subsection 4.3. To ensure that each SA-MCTSi\nand SEâˆ-MCTSi achieve a comparable win rate, all methods are tested using a fixed simulation\ncount of 800. For SA-MCTS, since the strength index z does not directly correspond to any specific\nrank, we adjust z for each ri to ensure that each SA-MCTSi and SEâˆ-MCTSi achieve a comparable\nwin rate. As shown in Table 4, z gradually decreases from r1 to r11, aligning with the results in the\noriginal paper, which indicate that a greater z corresponds to a higher strength.\nTable 4: z according to rank.\nRank\nz\nr1 (9 dan)\n0.6\nr2 (8 dan)\n0.5\nr3 (7 dan)\n0.35\nr4 (6 dan)\n0.3\nr5 (5 dan)\n0.2\nr6 (4 dan)\n0.15\nr7 (3 dan)\n0.05\nr8 (2 dan)\n-0.1\nr9 (1 dan)\n-0.2\nr10 (1-2 kyu)\n-0.6\nr11 (3-5 kyu)\n-1.0\nC.1\nADJUSTING STRENGTH WITH DIFFERENT BASELINES\nIn Figure 4, the baseline program is chosen as SEâˆ-MCTSi with i = 5 (5 dan). It would be interesting\nto examine whether the relative strength remains consistent when different baseline models are used.\nTo further investigate this, we conduct a round-robin tournament by selecting five ranks (r2, r4, r6,\nr8 and r10) and two representative methods (SA-MCTS and SEâˆ-MCTS, excluding SE-MCTS due\nto its ineffective strength adjustment. Each combination involves 250 games, requiring approximately\n100 GPU hours on an NVIDIA RTX A5000. Table 5 summarizes the results, with the win rates\nin each cell representing the performance of the y-axis player against the x-axis player. Moreover,\nwe compute the Elo rating of each model using this table. We initialize the rating at 1500 for each\nmodel and iteratively update the ratings to match the expected win rates with the observed pairwise\noutcomes. The rightmost column of Table 5 presents the resulting Elo ratings. In summary, the Elo\nratings confirm that higher-ranked models consistently achieve higher ratings, demonstrating the\nrobustness of our method across different baselines.\nTable 5: The round-robin tournament among ten MCTS programs: SA-MCTS2, SA-MCTS4,\nSA-MCTS6, SA-MCTS8, SA-MCTS10, SEâˆ-MCTS2, SEâˆ-MCTS4, SEâˆ-MCTS6, SEâˆ-MCTS8,\nSEâˆ-MCTS10. For simplicity, abbreviations SA-r2, SA-r4, etc. are used.\nSA-r2\nSA-r4\nSA-r6\nSA-r8\nSA-r10\nSEâˆ-r2\nSEâˆ-r4\nSEâˆ-r6\nSEâˆ-r8\nSEâˆ-r10\nAvg. Win Rate\nElo\nSA-r2\n-\n64.4% Â± 5.95%\n65.2% Â± 5.92%\n77.6% Â± 5.18%\n90.8% Â± 3.59%\n56.0% Â± 6.17%\n64.8% Â± 5.93%\n67.2% Â± 5.83%\n76.0% Â± 5.30%\n94.0% Â± 2.95%\n72.9% Â± 1.8%\n1685.62\nSA-r4\n35.6% Â± 5.95%\n-\n58.4% Â± 6.12%\n67.2% Â± 5.83%\n85.2% Â± 4.41%\n37.6% Â± 6.02%\n55.6% Â± 6.17%\n62.0% Â± 6.03%\n75.2% Â± 5.36%\n86.0% Â± 4.31%\n62.5% Â± 2.0%\n1605.22\nSA-r6\n34.8% Â± 5.92%\n41.6% Â± 6.12%\n-\n65.6% Â± 5.90%\n84.4% Â± 4.51%\n35.6% Â± 5.95%\n40.4% Â± 6.09%\n50.0% Â± 6.21%\n66.0% Â± 5.88%\n80.0% Â± 4.97%\n55.4% Â± 2.1%\n1550.24\nSA-r8\n22.4% Â± 5.18%\n32.8% Â± 5.83%\n34.4% Â± 5.90%\n-\n68.0% Â± 5.79%\n24.8% Â± 5.36%\n39.2% Â± 6.06%\n39.2% Â± 6.06%\n47.6% Â± 6.20%\n68.8% Â± 5.75%\n41.9% Â± 2.0%\n1452.23\nSA-r10\n9.2% Â± 3.59%\n14.8% Â± 4.41%\n15.6% Â± 4.51%\n32.0% Â± 5.79%\n-\n7.2% Â± 3.21%\n14.8% Â± 4.41%\n21.6% Â± 5.11%\n22.8% Â± 5.21%\n50.8% Â± 6.21%\n21.0% Â± 1.7%\n1279.80\nSEâˆ-r2\n44.0% Â± 6.17%\n62.4% Â± 6.02%\n64.4% Â± 5.95%\n75.2% Â± 5.36%\n92.8% Â± 3.21%\n-\n70.4% Â± 5.67%\n83.2% Â± 4.64%\n88.0% Â± 4.04%\n93.6% Â± 3.04%\n74.9% Â± 1.8%\n1706.28\nSEâˆ-r4\n35.2% Â± 5.93%\n44.4% Â± 6.17%\n59.6% Â± 6.09%\n60.8% Â± 6.06%\n85.2% Â± 4.41%\n29.6% Â± 5.67%\n-\n61.2% Â± 6.05%\n78.0% Â± 5.15%\n85.6% Â± 4.36%\n60.0% Â± 2.0%\n1586.26\nSEâˆ-r6\n32.8% Â± 5.83%\n38.0% Â± 6.03%\n50.0% Â± 6.21%\n60.8% Â± 6.06%\n78.4% Â± 5.11%\n16.8% Â± 4.64%\n38.8% Â± 6.05%\n-\n72.0% Â± 5.58%\n80.0% Â± 4.97%\n52.0% Â± 2.1%\n1526.51\nSEâˆ-r8\n24.0% Â± 5.30%\n24.8% Â± 5.36%\n34.0% Â± 5.88%\n52.4% Â± 6.20%\n77.2% Â± 5.21%\n12.0% Â± 4.04%\n22.0% Â± 5.15%\n28.0% Â± 5.58%\n-\n69.6% Â± 5.71%\n38.2% Â± 2.0%\n1423.82\nSEâˆ-r10\n6.0% Â± 2.95%\n14.0% Â± 4.31%\n20.0% Â± 4.97%\n31.2% Â± 5.75%\n49.2% Â± 6.21%\n6.4% Â± 3.04%\n14.4% Â± 4.36%\n20.0% Â± 4.97%\n30.4% Â± 5.71%\n-\n21.3% Â± 1.7%\n1283.25\n18\n",
  "metadata": {
    "source_path": "papers/arxiv/Strength_Estimation_and_Human-Like_Strength_Adjustment_in_Games_eb0fdd4f445ce7a8.pdf",
    "content_hash": "eb0fdd4f445ce7a8f31b9593f4dcda1e34e849a3513f334bd0e627ee632aa2d0",
    "arxiv_id": null,
    "title": "Strength_Estimation_and_Human-Like_Strength_Adjustment_in_Games_eb0fdd4f445ce7a8",
    "author": "",
    "creation_date": "D:20250225024558Z",
    "published": "2025-02-25T02:45:58",
    "pages": 18,
    "size": 1724575,
    "file_mtime": 1740470183.230992
  }
}