{
  "text": "Graphical Abstract\nSystematic Weight Evaluation for Pruning Large Language Models:\nEnhancing Performance and Sustainability\nAshhadul Islam, Samir Brahim Belhaouari, Amine Bermak\n‚Ä¢\nTrain model\n‚Ä¢\nRecord weight \nevolution\n‚Ä¢\nGenerate \nmask\nùêø\"\n‚Ä¶\nùêø#\nùêø$%\"\nWeight Importance Mask\nWeight Evolution\nForward Propagation\nUnstructured Pruning\nTheoretical Compression\nùêø\"\n&\n‚Ä¶\nFine-Pruning Mask\nRefined Pruning\nStructured Pruning\nActual Compression\nùêø#\n&\nùêø$%\"\n&\nForward Propagation (Individual weights)\n‚Ä¢\nIterative pruning\n‚Ä¢\nLayer by layer\n‚Ä¢\nAvoid sudden loss of accuracy\nRefined Fine Pruning (Neurons & Filters)\n‚Ä¢\nChange architecture\n‚Ä¢\nRemove connections completely\n‚Ä¢\nActual reduction in size of models\narXiv:2502.1707\n\nHighlights\nSystematic Weight Evaluation for Pruning Large Language Models:\nEnhancing Performance and Sustainability\nAshhadul Islam, Samir Brahim Belhaouari, Amine Bermak\n‚Ä¢ Efficient Pruning for Large Language Models: This research\nintroduces a novel pruning method that evaluates the importance of\nindividual weights throughout the training process, significantly opti-\nmizing model performance while reducing resource consumption.\n‚Ä¢ Impact of Compression on Model Performance: Through com-\nprehensive experiments, the study demonstrates that moderate prun-\ning can enhance model efficiency, but excessive compression leads to\nsubstantial performance degradation in both language and multimodal\nmodels.\n‚Ä¢ Sustainable AI Development: The findings emphasize the need for\noptimized AI models to reduce the environmental impact, addressing\ncritical issues like carbon footprint, electricity, and water consumption\nassociated with training and deploying large-scale AI systems.\n\nSystematic Weight Evaluation for Pruning Large\nLanguage Models: Enhancing Performance and\nSustainability\nAshhadul Islama, Samir Brahim Belhaouaria, Amine Bermaka\naCollege Of Science & Engineering, Hamad Bin Khalifa University, Education\nCity, Doha, 34110, Qatar\nAbstract\nThe exponential growth of large language models (LLMs) like ChatGPT\nhas revolutionized artificial intelligence, offering unprecedented capabilities\nin natural language processing. However, the extensive computational re-\nsources required for training these models have significant environmental im-\nplications, including high carbon emissions, energy consumption, and water\nusage. This research presents a novel approach to LLM pruning, focusing\non the systematic evaluation of individual weight importance throughout\nthe training process. By monitoring parameter evolution over time, we pro-\npose a method that effectively reduces model size without compromising\nperformance. Extensive experiments with both a scaled-down LLM and a\nlarge multimodal model reveal that moderate pruning enhances efficiency\nand reduces loss, while excessive pruning drastically deteriorates model per-\nformance. These findings highlight the critical need for optimized AI mod-\nels to ensure sustainable development, balancing technological advancement\nwith environmental responsibility.\nKeywords:\nLarge Language Models (LLMs), Weight Evaluation, Model\nPruning, Sustainable AI, Performance Optimization\n1. Introduction\nThe rapid advancement of artificial intelligence, particularly in the de-\nvelopment and deployment of large language models (LLMs) like ChatGPT,\nhas brought about significant benefits across various domains. These models,\nwith billions of parameters, have demonstrated unparalleled capabilities in\nPreprint submitted to Nuclear Physics B\nFebruary 25, 2025\n\ntasks such as natural language understanding, translation, and text genera-\ntion. However, the growing scale of these models comes with substantial chal-\nlenges, particularly in terms of computational cost, environmental impact,\nand resource consumption, which underscore the need for optimization.\nOne of the most pressing concerns associated with training and deploying\nLLMs is their considerable carbon footprint. The computational resources\nrequired to train these models are immense, resulting in significant CO2 emis-\nsions. For instance, the training of BERT, a model with 110 million param-\neters (Zhou), is estimated to have a carbon footprint comparable to a round\ntrip flight across the United States (Strubell et al., 2020). For larger models\nlike ChatGPT, which contains 137 billion parameters (Gooding, 2023), the\nenvironmental impact is even more pronounced, equating to the annual CO2\nemissions of 13,483 Americans (Ludvigsen, 2022).\nThis alarming scale of\nemissions highlights the urgency of developing more sustainable approaches\nto AI. In addition to carbon emissions, the electricity consumption associated\nwith LLMs is staggering. The daily global usage of ChatGPT alone consumes\nan estimated 12 million kilowatt-hours (KWh) of electricity, equivalent to the\nmonthly electricity consumption of 90,000 Danish households (Ludvigsen).\nThis energy demand not only poses a significant operational cost but also\ncontributes to the environmental burden, particularly in regions where elec-\ntricity is predominantly generated from fossil fuels. Another critical aspect is\nthe water footprint of AI models, a less commonly discussed but equally im-\nportant factor. Training large models like ChatGPT requires vast amounts of\nwater for cooling data centers, with estimates suggesting that training such\na model can consume over 700,000 liters of freshwater, enough for producing\n370 BMW cars or 320 Tesla electric vehicles (Li et al., 2023). Moreover, even\na simple conversation with ChatGPT, involving 20-50 questions and answers,\ncould necessitate the equivalent of a 500ml bottle of water (Li et al., 2023).\nThis substantial water usage raises concerns, especially in areas facing water\nscarcity. Given the finite nature of natural resources and the increasing size\nof AI models, it is evident that the current trajectory is detrimental to the\nenvironmental stability. The growing computational power required to train\nand deploy these models exacerbates the strain on global resources, neces-\nsitating a reevaluation of how these models are developed and maintained.\nOptimization techniques, such as model pruning and efficient resource man-\nagement, are crucial in addressing these challenges. By reducing the size of\nmodels without compromising their performance, we can mitigate the envi-\nronmental impact, reduce operational costs, and make AI more sustainable\n2\n\nin the long run.\nContribution\nIn this research, we introduce a novel approach to pruning large language\nmodels (LLMs) by systematically evaluating the performance of each indi-\nvidual weight throughout the training process. The remainder of the paper is\nstructured as follows: Section 2 reviews recent trends in LLM pruning, Sec-\ntion 3 details our proposed methodology, and Section 4 presents an overview\nof the performance of our approach. Finally, Section 5 concludes the paper\nby discussing the limitations of our work and potential directions for future\nresearch.\n2. Related Work\nMagnitude pruning (Han et al., 2015) is a standard technique to induce\nsparsity in neural networks by removing individual weights based on their\nmagnitudes, typically determined either locally within each layer or globally\nacross the entire network. Despite its simplicity, it has been effective in find-\ning extremely sparse networks (Frankle and Carbin, 2019) and is considered\na strong baseline approach (Blalock et al., 2020) for neural network sparsi-\nfication. Dettmers et al. (Dettmers et al., 2022) observed emergent large\nmagnitude features in Transformer-based large language models (LLMs),\nnoting that when LLMs reach around 6B parameters, a small set of hid-\nden state features emerges with significantly larger magnitudes than others,\nwhich are crucial for predictive performance. In the context of compress-\ning recent LLMs, methods like LLM-Pruner (Ma et al., 2023) and FLAP\n(An et al., 2024) narrow network width by pruning coupled structures, while\nSheared-LLaMA (Xia et al., 2023) reduces both network width and depth\nby removing entire layers. Although pruning methods that incorporate both\nwidth and depth aspects exist (Xia et al., 2022; Kurti¬¥c et al., 2024), there\nremains a need for detailed analysis comparing these factors‚Äô impact on LLM\ninference efficiency. Traditional pruning in Deep Neural Networks (DNNs)\nfaces unique challenges when applied to LLMs, which have a large number\nof parameters and require significant computational resources (Brown et al.,\n2020). Various pruning methods for LLMs fall into unstructured and struc-\ntured categories. Unstructured pruning methods (Dong et al., 2017; Chen\net al., 2020, 2021a) set unimportant individual weights to zero, maintaining\nperformance but resulting in sparse weight matrices that are less hardware-\nefficient. Methods like SparseGPT (Frantar and Alistarh, 2023) and Wanda\n3\n\n(Sun et al., 2023) use sophisticated weight updates and pruning without\nretraining, respectively, while PST (Li et al., 2022) combines unstructured\npruning with efficient fine-tuning. Structured pruning methods (Chen et al.,\n2021b, 2023) remove entire groups of parameters, maintaining dense weight\nmatrices and improving hardware efficiency. Techniques such as LLM-Pruner\n(Ma et al., 2023) and LoRAPrune (Zhang et al., 2023) focus on efficient de-\nployment and inference acceleration, with Sheared-LLaMA (Xia et al., 2023)\naiming to prune models to a target architecture and train them dynami-\ncally. Furthermore, the compression of language models has garnered signif-\nicant attention, leading to various methods like network pruning, knowledge\ndistillation, and quantization (Bai et al., 2020; Brown et al., 2020; Devlin,\n2018). Pruning, especially structural pruning, remains a crucial focus due\nto its hardware-friendly nature, with methods varying from l1-dependent\npruning (Zafrir et al., 2021) to more advanced techniques like the optimal\nbrain surgeon (LeCun et al., 1989). Efficient compression and low-resource\nstrategies are increasingly essential, with advancements in layer-wise optimal\nbrain surgeon and data-free pruning approaches aiming to optimize the bal-\nance between compression efficiency and training data availability (Kurti¬¥c\net al., 2024; Srinivas and Babu, 2015).\n3. Proposed Approach\nThe central element of the pruning method proposed in this research lies\nin monitoring the evolution of parameters over time. This involves systemat-\nically observing how parameter values shift throughout the training process\nacross multiple epochs. Figure 1 presents two visualizations of the network‚Äôs\nweight dynamics: one tracks the changes in randomly selected weights over\n100 epochs, while the other follows the trajectory of specific weights within\nthe same training epochs. The initial weight values are chosen randomly and\nthen adjusted during training as per the training algorithm. We note the\noverall shift of weights of the network, with some weights jumping high rad-\nically and some staying low over the training phase. This behavior is further\nillustrated in Figure 2, where the evolution of network weights across mul-\ntiple epochs is displayed. Specifically, in subfigure 2a, we observe a subset\nof weights that increase dramatically early in the training process. In con-\ntrast, subfigure 2b highlights weights that remain relatively stable through-\nout training. Subfigure 2c shows a mix of these behaviors, with some weights\nfluctuating while others stabilize.\n4\n\na\nb\nFigure 1: (a) Depicts the evolution of randomly selected weights over 100 training epochs.\n(b) Illustrates the progression of specific weights across the same 100 training epochs.\nSubfigure 2d demonstrates the long-term evolution, emphasizing the di-\nvergence between weights that become highly influential and those that re-\nmain low, indicating their potential for pruning. Furthermore, subfigures 2e\nand 2f offer additional insights into the final distribution of weights, illus-\ntrating both the stability of low-magnitude weights and the radical shifts in\nothers, further reinforcing the potential for targeted pruning to optimize the\nmodel.\nOur approach incorporates a system that assigns weights to the parameter\nmagnitudes, emphasizing those values closer to the end of the training epochs\nwhile still considering earlier data.\nThe importance of each parameter is\ncalculated by multiplying its magnitude by a corresponding weight and then\naveraging these values, which helps in constructing an importance vector that\nmaps the evolution of parameters throughout the training epochs.\nFor example, to calculate the weighted importance of a particular weight\nor filter, we compile a log of its magnitude recorded at each epoch during\ntraining. This log allows us to assess the weighted significance according to\nthe provided equation. The resulting score represents the importance of a\nparameter in terms of its magnitude and how it has changed throughout the\ntraining process. Table 1 displays the magnitude logs for various weights\nacross epochs. Using our method, the most significant weights were found to\nbe Weight 1 (with a score of 14), Weight 2 (with a score of 13), and Weight\n3 (with a score of 6.66).\nTo generalize this calculation, we define a vector for each weight or fil-\nter (vali = [vali1, vali2, vali3, ...valin]), where each entry corresponds to the\nweight‚Äôs magnitude at a specific epoch across the n training epochs. This\n5\n\na\nb\nc\nd\ne\nf\nFigure 2: Evolution of weights over training epochs. Fig (a) and (b) show weights that\nincrease dramatically, while (c) and (d) illustrate weights that fluctuate minimally. Fig (e)\nand (f) further demonstrate the divergence and stability of weights, emphasizing patterns\ncritical for effective pruning.\n6\n\nTable 1: To assess the significance of parameters over training epochs, we log the value\nof each parameter at the end of each epoch, organizing these values into columns. The\naggregated significance is obtained by performing a weighted sum of each weight‚Äôs mag-\nnitude. The multipliers in the bottom row show the extent to which each magnitude is\nconsidered in the calculation.\nWeight #\nEpoch\n1\nEpoch\n2\nEpoch\n...\nEpoch\nk\nAggregated\nImportance\n1\n4\n6\n...\n3\n14\n2\n8\n9\n...\n5\n13\n3\n6\n8\n...\n8\n6.66\n4\n2\n5\n...\n9\n4.66\nMultiplier\n‚àó1\n‚àó2\n...\n‚àók\nvector forms the basis for calculating the weighted significance using the\nfollowing equation, which prioritizes the most recent k epochs:\nImpi =\nPk\nL=0 vali(n‚àíL) ‚àó(n ‚àíL)\nPk\nL=0(n ‚àíL)\n(1)\nHere, L varies from 0 to k, with 0 representing the most recent epoch and\nk counting back from the final epoch. The resulting importance matrix be-\ncomes a crucial tool for assessing weight significance and guiding the network\npruning strategy.\n4. Experiment And Results\nTo check the consistency of our methods, two key experiments were con-\nducted. These experiments focused on evaluating the effects of pruning, a\nprocess that reduces the number of parameters in a model, on model perfor-\nmance. The first experiment tested a scaled-down LLM trained from scratch,\nwhile the second involved a large pre-trained multimodal model. Both exper-\niments aimed to determine how much compression could be applied to these\nmodels before significant performance degradation occurred. Before looking\nat the individual experiments, we take a look at the general procedure.\n4.1. Record Weighted Average\nIn addition to directly training the model, a cloned version is maintained\nalongside it. The parameters of this clone are updated through a weighted\n7\n\naverage method that integrates historical parameter values across the train-\ning epochs. Initially, the cloned model‚Äôs parameters are set to zero before\nthe training starts. After each training step, both the original model‚Äôs pa-\nrameters and the corresponding parameters in the clone are updated. The\nupdated values in the clone are computed as a weighted average, combining\nthe existing parameters with the new ones from the original model, based\non the current epoch. This approach ensures that recent updates are given\nmore significance in the clone. The weighted average process, which gradu-\nally incorporates the model‚Äôs parameter values over the epochs, is expressed\nas:\nqnew = qold √ó Sprev + p √ó (n + 1)\nS\n(2)\nWhere:\n‚Ä¢ qnew are the updated parameters in the cloned model.\n‚Ä¢ qold are the previous parameters in the cloned model.\n‚Ä¢ p are the current parameters in the original model.\n‚Ä¢ n is the current epoch number.\n‚Ä¢ Sprev is the sum of weights from epoch 1 to n (inclusive).\n‚Ä¢ S is the sum of weights from epoch 1 to n + 1 (inclusive).\n4.2. Model Training and Pruning\n‚Ä¢ Step I: The Transformer model is trained over 5000 epochs, with weight\nchanges recorded throughout the process.\n‚Ä¢ Step II: After training, the model undergoes pruning based on the\nweighted parameters, followed by an additional 50 epochs of fine-tuning\nto maintain effective compression:\n‚Äì The importance of each parameter is assessed by evaluating its\nweighted absolute value:\nWabs = |W|\n(3)\n8\n\n‚Äì A pruning threshold is determined by scaling the standard devia-\ntion of these absolute values with a specific rate:\nThreshold = œÉ(Wabs) √ó Prune Rate\n(4)\nHere, œÉ(Wabs) represents the standard deviation of Wabs, and\n‚ÄùPrune Rate‚Äù is a constant that dictates the extent of pruning.\n‚Äì Parameters falling below the pruning threshold are set to zero:\nP =\n(\n0\nif Wabs < Threshold\nP\notherwise\n(5)\nThe effectiveness of Step II is evaluated by varying the pruning rates and\nobserving the corresponding loss values.\n4.3. Experiment 1: Scaled-Down LLM\n4.3.1. Model and Dataset:\nThe first experiment utilized a scaled-down version of a ChatGPT-like\nLarge Language Model (LLM), consisting of 10,788,929 (10.7 million) ad-\njustable parameters.\nThis substantial language model is founded on the\nTransformer architecture, which is specialized in generating text for natu-\nral language processing tasks (Vaswani et al., 2017). The model comprises\nseveral Transformer blocks, each containing the following components:\n‚Ä¢ Embedding Layer: Input tokens are transformed into 384-sized vec-\ntors, forming the initial layer input.\n‚Ä¢ Multi-Head Self-Attention Mechanism: This mechanism, with 6\nheads, directs focus across different segments of the input sequence\nusing linear transformations without bias, scaling the attention scores\nby the square root of the embedding size (384).\n‚Ä¢ Positional Embeddings: Positional embeddings of the same size\n(384) are added to the input tokens to signify sequence positions, en-\nabling the model to recognize word order.\n‚Ä¢ Feed-Forward Network: Following the attention mechanism, each\nTransformer block includes a feed-forward network with two linear lay-\ners and a hidden layer size of 1536, activated by ReLU. Dropout is\napplied with a rate of 0.0 to reduce overfitting.\n9\n\n‚Ä¢ Layer Normalization: This process standardizes activations across\nfeatures and enhances learning, applied after both the self-attention\nand feed-forward network layers.\n‚Ä¢ Final Layer: The final layer performs a linear transformation, corre-\nlating output embeddings with the vocabulary size based on the unique\ninput text characters.\nThe complete model stacks 6 of these Transformer blocks, processing\nthe input independently to grasp varying semantic layers in the text. The\nmodel was trained from scratch using the complete works of Shakespeare,\nwhich served as the training dataset. The primary task for this model was\nnext-token prediction, a common benchmark task in language modeling. All\n10,788,929 trainable parameters were tracked throughout the training epochs.\n4.3.2. Training Procedure:\nThe model was subjected to a series of pruning tests, where the parameter\ncount was reduced incrementally. The compression levels ranged from 0% (no\npruning) to 94% (high pruning). The effect of each level of compression on\nmodel performance was monitored by tracking the loss associated with the\nnext-token prediction task.\n4.4. Experiment 2: Multimodal Model\n4.4.1. Model and Dataset:\nThe second experiment centered around a pre-trained Phi-3-vision model,\na large multimodal model with 4.2 billion parameters, designed for both lan-\nguage and vision tasks. The dataset for this experiment comprised a variety\nof Burberry products, categorized under different product types such as hats,\ngloves, and sunglasses. The dataset included both textual descriptions and\ncorresponding images of the products.\n4.4.2. Training Procedure:\nIn this experiment, the Phi-3-vision model (Abdin et al., 2024) was fine-\ntuned for 10 epochs using the Burberry dataset (huggingface, 2023). After\nfine-tuning, the model underwent pruning at various compression levels, sim-\nilar to the first experiment. The performance of the model was evaluated by\nmeasuring the Mean Absolute Error (MAE) at each level of compression\nto understand the impact of pruning on its ability to process and classify\nmultimodal inputs.\n10\n\n4.5. Results\n4.5.1. Results of Experiment 1\nThe results from the first experiment, as shown in Table 2, indicate that\nthe model could tolerate compression up to 60% without a significant increase\nin loss.\nAt this level, the compression loss was recorded at 1.656, which\nwas lower than the initial loss of 1.9.\nHowever, further compression led\nto an increase in loss, with a sharp rise observed at 70% compression and\nabove, where the loss eventually escalated to 3.098 at 94% compression. This\ndemonstrates that moderate pruning can lead to a reduction in loss, but\nexcessive pruning severely hampers model performance.\nCompression (%)\nLoss\n0\n1.900\n0.1\n1.977\n0.2\n1.932\n0.3\n1.864\n0.4\n1.782\n0.5\n1.693\n0.6\n1.656\n0.7\n1.747\n0.8\n2.133\n0.9\n2.703\n0.94\n3.098\nTable 2: Compression Loss in Experiment 1\nThe impact of compression on model loss is further visualized in Fig-\nure 3. The figure illustrates how the loss changes as the model undergoes\nincreasing levels of compression. As shown, the loss decreases slightly up\nto a compression level of around 60%, corroborating the trend observed in\nTable 2, where a reduction in loss was recorded at moderate compression\nlevels. However, beyond this point, the figure reveals a steep increase in loss,\nespecially as compression exceeds 70%, culminating in a significant rise at\n94% compression. This visual representation reinforces the findings from the\ntable, indicating that while some pruning can reduce loss, excessive prun-\ning significantly impairs the model‚Äôs ability to perform, leading to a sharp\nincrease in loss.\n11\n\n0.2\n0.4\n0.6\n0.8\nCompression\n1.6\n1.8\n2.0\n2.2\n2.4\n2.6\n2.8\n3.0\nLoss\npruned loss\noriginal loss\nFigure 3: Loss as a function of compression levels. The figure shows a decrease in loss\nup to 60% compression, after which a sharp increase is observed, particularly beyond 70%\ncompression, consistent with the trends detailed in Table 2.\n12\n\n4.5.2. Results of Experiment 2\nIn the second experiment, the Phi-3-vision model exhibited stable per-\nformance at lower compression levels, as detailed in Table 3. The MAE de-\ncreased from 439 at 0% compression to 374 at 10% compression, suggesting\nthat some level of pruning can improve the model‚Äôs performance, possibly\nby eliminating redundant parameters. However, at compression levels be-\nyond 25%, the MAE began to rise again, and a drastic increase was observed\nat 48% compression, where the MAE surged to 11,041. This indicates that\nwhile the model can handle moderate pruning, aggressive pruning beyond\n30% significantly deteriorates its performance.\nCompression (%)\nMAE\n0\n439\n5\n380\n10\n374\n12\n378\n15\n384\n20\n397\n25\n457\n30\n401\n37\n474\n48\n11041\n59\n950\n67\n961\n76\n961\nTable 3: MAE in Experiment 2\nThe relationship between compression levels and model error is further\nillustrated in Figure 4. The figure visualizes how the model‚Äôs price prediction\nerror varies with increasing compression levels. As shown, the model main-\ntains a relatively low error up to moderate compression levels, reinforcing the\nfindings in Table 3 where the MAE decreases slightly with initial pruning.\nHowever, as the compression surpasses 30%, the error begins to escalate,\nculminating in a significant spike at higher compression levels. This sharp\nincrease at 48% compression aligns with the drastic rise in MAE observed in\nthe table, indicating that the model‚Äôs capacity to handle pruning is limited\nand aggressive compression can severely impair its predictive performance.\n13\n\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nCompression\n400\n500\n600\n700\n800\n900\n1000\nPrice Error\nCompression vs. Price Error\nFigure 4: Price error as a function of compression levels. The figure demonstrates that\nwhile the model maintains a relatively low error up to moderate compression levels, the\nerror escalates sharply beyond 30% compression, consistent with the MAE trends observed\nin Table 3.\n14\n\n5. Limitations And Future Work\nWhile the research on pruning and optimizing Large Language Models\n(LLMs) shows promising results, it is essential to recognize certain limitations\nthat present challenges for future development.\n‚Ä¢ Specialized Use Cases: While fine-tuning LLMs for specific purposes\ncan yield high performance, it may limit the model‚Äôs applicability across\na broader range of tasks, requiring more adaptable solutions.\n‚Ä¢ Adaptation to Model Size: As LLMs grow larger, the proportion\nof parameters that can be effectively pruned decreases unless more\nadvanced techniques are applied, highlighting the need for methods\nthat can handle large-scale models efficiently.\n‚Ä¢ Memory Considerations: Managing the memory requirements for\nstoring a weighted average of parameters in models with millions or\nbillions of parameters presents a significant challenge, necessitating\nmemory-efficient strategies.\n5.1. Future Work\nLooking forward, the future work in this area is both hopeful and excit-\ning. There is a strong potential for tangible energy savings by optimizing\nLLMs more efficiently, making AI systems not only faster but also more sus-\ntainable. The research community is also poised to explore smarter pruning\nmethods that could overcome current limitations, enabling deeper compres-\nsion without sacrificing model accuracy or generalization capabilities. As we\ncontinue to push the boundaries of LLMs, the focus will be on balancing inno-\nvation with sustainability, ensuring that the advancements in AI contribute\npositively to both technological progress and environmental responsibility.\nReferences\nAbdin, M., Jacobs, S.A., Awan, A.A., Aneja, J., Awadallah, A., Awadalla,\nH., Bach, N., Bahree, A., Bakhtiari, A., Behl, H., et al., 2024.\nPhi-3\ntechnical report: A highly capable language model locally on your phone.\narXiv preprint arXiv:2404.14219 .\n15\n\nAn, Y., Zhao, X., Yu, T., Tang, M., Wang, J., 2024.\nFluctuation-based\nadaptive structured pruning for large language models, in: Proceedings of\nthe AAAI Conference on Artificial Intelligence, pp. 10865‚Äì10873.\nBai, H., Zhang, W., Hou, L., Shang, L., Jin, J., Jiang, X., Liu, Q., Lyu, M.,\nKing, I., 2020. Binarybert: Pushing the limit of bert quantization. arXiv\npreprint arXiv:2012.15701 .\nBlalock, D., Ortiz, J.J.G., Frankle, J., Guttag, J., 2020. What is the State\nof Neural Network Pruning? arXiv URL: http://arxiv.org/abs/2003.\n03033, arXiv:2003.03033.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P.,\nNeelakantan, A., Shyam, P., Sastry, G., Askell, A., et al., 2020. Language\nmodels are few-shot learners. Advances in neural information processing\nsystems 33, 1877‚Äì1901.\nChen, L., Chen, Y., Xi, J., Le, X., 2021a. Knowledge from the original net-\nwork: restore a better pruned network with knowledge distillation. Com-\nplex & Intelligent Systems , 1‚Äì10.\nChen, T., Ji, B., Ding, T., Fang, B., Wang, G., Zhu, Z., Liang, L., Shi, Y., Yi,\nS., Tu, X., 2021b. Only train once: A one-shot neural network training and\npruning framework. Advances in Neural Information Processing Systems\n34, 19637‚Äì19651.\nChen, T., Liang, L., Ding, T., Zhu, Z., Zharkov, I., 2023. Otov2: Automatic,\ngeneric, user-friendly. arXiv preprint arXiv:2303.06862 .\nChen, X., Zhu, J., Jiang, J., Tsui, C.Y., 2020. Tight compression: Com-\npressing cnn model tightly through unstructured pruning and simulated\nannealing based permutation, in: 2020 57th ACM/IEEE Design Automa-\ntion Conference (DAC), IEEE. pp. 1‚Äì6.\nDettmers, T., Lewis, M., Belkada, Y., Zettlemoyer, L., 2022. Gpt3. int8 ():\n8-bit matrix multiplication for transformers at scale. Advances in Neural\nInformation Processing Systems 35, 30318‚Äì30332.\nDevlin, J., 2018. Bert: Pre-training of deep bidirectional transformers for\nlanguage understanding. arXiv preprint arXiv:1810.04805 .\n16\n\nDong, X., Chen, S., Pan, S., 2017. Learning to prune deep neural networks\nvia layer-wise optimal brain surgeon. Advances in neural information pro-\ncessing systems 30.\nFrankle, J., Carbin, M., 2019. The lottery ticket hypothesis: Finding sparse,\ntrainable neural networks. 7th International Conference on Learning Rep-\nresentations, ICLR 2019 , 1‚Äì42arXiv:1803.03635.\nFrantar, E., Alistarh, D., 2023. Sparsegpt: Massive language models can be\naccurately pruned in one-shot, in: International Conference on Machine\nLearning, PMLR. pp. 10323‚Äì10337.\nGooding, M., 2023.\nGoogle takes on ChatGPT with new Bard chatbot\nand AI-powered search.\nURL: https://techmonitor.ai/technology/\nai-and-automation/bard-google-chatgpt-generative-ai-chatbot.\nHan, S., Pool, J., Tran, J., Dally, W., 2015.\nLearning both weights and\nconnections for efficient neural network. Advances in neural information\nprocessing systems 28.\nhuggingface, 2023. DBQ/Burberry.Product.prices.United.States ¬∑ Datasets\nat Hugging Face ‚Äî huggingface.co. https://bit.ly/3YCJ9Z7. [Accessed\n13-08-2024].\nKurti¬¥c, E., Frantar, E., Alistarh, D., 2024. Ziplm: Inference-aware structured\npruning of language models. Advances in Neural Information Processing\nSystems 36.\nLeCun, Y., Denker, J., Solla, S., 1989. Optimal brain damage, in: Touretzky,\nD. (Ed.), Advances in Neural Information Processing Systems, Morgan-\nKaufmann.\nLi, P., Yang, J., Islam, M.A., Ren, S., 2023. Making ai less ‚Äùthirsty‚Äù: Un-\ncovering and addressing the secret water footprint of ai models. arXiv\narXiv:2304.03271.\nLi, Y., Luo, F., Tan, C., Wang, M., Huang, S., Li, S., Bai, J., 2022.\nParameter-efficient sparsity for large language models fine-tuning. arXiv\npreprint arXiv:2205.11005 .\n17\n\nLudvigsen, K.G.A., . ChatGPT‚Äôs Electricity Consumption ‚Äî towardsdata-\nscience.com. https://bit.ly/3An9joz. [Accessed 13-08-2024].\nLudvigsen,\nK.G.A.,\n2022.\nThe\nCarbon\nFootprint\nof\nChatGPT.\nURL:\nhttps://towardsdatascience.com/\nthe-carbon-footprint-of-chatgpt-66932314627d.\nMa, X., Fang, G., Wang, X., 2023. Llm-pruner: On the structural pruning of\nlarge language models. Advances in neural information processing systems\n36, 21702‚Äì21720.\nSrinivas, S., Babu, R.V., 2015. Data-free parameter pruning for deep neural\nnetworks. arXiv preprint arXiv:1507.06149 .\nStrubell, E., Ganesh, A., McCallum, A., 2020. Energy and policy consider-\nations for modern deep learning research. AAAI 2020 - 34th AAAI Con-\nference on Artificial Intelligence , 1393‚Äì13696doi:10.1609/aaai.v34i09.\n7123, arXiv:arXiv:1906.02243v1.\nSun, M., Liu, Z., Bair, A., Kolter, J.Z., 2023. A simple and effective pruning\napproach for large language models. arXiv preprint arXiv:2306.11695 .\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,\nKaiser,  L., Polosukhin, I., 2017. Attention is all you need. Advances in\nNeural Information Processing Systems 30.\nXia, M., Gao, T., Zeng, Z., Chen, D., 2023.\nSheared llama: Accelerat-\ning language model pre-training via structured pruning. arXiv preprint\narXiv:2310.06694 .\nXia, M., Zhong, Z., Chen, D., 2022. Structured pruning learns compact and\naccurate models. arXiv preprint arXiv:2204.00408 .\nZafrir, O., Larey, A., Boudoukh, G., Shen, H., Wasserblat, M., 2021.\nPrune once for all: Sparse pre-trained language models. arXiv preprint\narXiv:2111.05754 .\nZhang, M., Chen, H., Shen, C., Yang, Z., Ou, L., Yu, X., Zhuang, B., 2023.\nLoraprune: Pruning meets low-rank parameter-efficient fine-tuning. arXiv\npreprint arXiv:2305.18403 .\nZhou, R., . Question Answering Models for SQuAD 2.0. Stanford Web , 1‚Äì7.\n18\n",
  "metadata": {
    "source_path": "papers/arxiv/Systematic_Weight_Evaluation_for_Pruning_Large_Language_Models\n__Enhancing_Performance_and_Sustainability_a01ecd7889f8c35c.pdf",
    "content_hash": "a01ecd7889f8c35cdbf0054dfdc02c04b4542247802365273c52b8e658cbc68d",
    "arxiv_id": null,
    "title": "Systematic Weight Evaluation for Pruning Large Language Models: Enhancing Performance and Sustainability",
    "author": "Ashhadul Islam; Samir Brahim Belhaouari; Amine Bermak; ",
    "creation_date": "D:20250225024200Z",
    "published": "2025-02-25T02:42:00",
    "pages": 20,
    "size": 1306555,
    "file_mtime": 1740470189.3233986
  }
}