{
  "text": "Class-Dependent Perturbation Effects in\nEvaluating Time Series Attributions\nGregor Baer1,2[0009−0002−9918−1376], Isel Grau1,2[0000−0002−8035−2887], Chao\nZhang2,3[0000−0001−9811−1881], and Pieter Van Gorp1,2[0000−0001−5197−3986]\n1 Information Systems, Eindhoven University of Technology, Eindhoven, The\nNetherlands\n2 Eindhoven Artificial Intelligence Systems Institute, Eindhoven University of\nTechnology, Eindhoven, The Netherlands\n3 Human-Technology Interaction, Eindhoven University of Technology, Eindhoven, The\nNetherlands\nAbstract. As machine learning models become increasingly prevalent in\ntime series applications, Explainable Artificial Intelligence (XAI) methods\nare essential for understanding their predictions. Within XAI, feature\nattribution methods aim to identify which input features contributed the\nmost to a model’s prediction, with their evaluation typically relying on\nperturbation-based metrics. Through empirical analysis across multiple\ndatasets, model architectures, and perturbation strategies, we identify\nimportant class-dependent effects in these metrics: they show varying\neffectiveness across classes, achieving strong results for some while remain-\ning less sensitive to others. In particular, we find that the most effective\nperturbation strategies often demonstrate the most pronounced class\ndifferences. Our analysis suggests that these effects arise from the learned\nbiases of classifiers, indicating that perturbation-based evaluation may\nreflect specific model behaviors rather than intrinsic attribution quality.\nWe propose an evaluation framework with a class-aware penalty term\nto help assess and account for these effects in evaluating feature attribu-\ntions. Although our analysis focuses on time series classification, these\nclass-dependent effects likely extend to other structured data domains\nwhere perturbation-based evaluation is common.4\nKeywords: Feature attribution · Perturbation analysis · XAI evaluation\n· Time series classification.\n1\nIntroduction\nExplainable Artificial Intelligence (XAI) has emerged as a critical paradigm\nfor understanding complex machine learning models, particularly in domains\nwhere trust and explainability are essential, such as finance or healthcare. Within\nXAI, feature attribution methods quantify how input features contribute to\n4 Code\nand\nresults\nare\navailable\nat\nhttps://github.com/gregorbaer/\nclass-perturbation-effects.\narXiv:2502.17022v1  [cs.LG]  24 Feb 2025\n\n2\nBaer et al.\nmodel predictions, with their often model-agnostic nature enabling application\nacross different architectures and data types. These methods are increasingly\nbeing applied to structured data domains, such as time series, where temporal\ndependencies pose unique challenges. In such contexts, ensuring reliable evaluation\nof attribution quality becomes crucial [22].\nThe evaluation of feature attribution methods faces a fundamental method-\nological challenge: the absence of a ground truth for explanations. Although\nhuman-centered evaluation offers a direct assessment of the utility of explana-\ntions [11], it suffers from scalability limitations and potential domain-specific\nbiases. Consequently, functional evaluation approaches have emerged as primary\nvalidation frameworks, with the aim of computationally verifying whether attri-\nbution methods satisfy certain desirable properties [2,9]. Perturbation analysis\nrepresents one such framework that evaluates attribution correctness by measur-\ning how modifying features impacts model predictions. This approach rests on a\nkey assumption: perturbing important features should yield proportional changes\nin model output.\nAlthough perturbation analysis has gained traction for evaluating attribu-\ntion methods in structured data domains like time series, previous work has\nmainly focused on aggregate performance metrics. Studies note that perturbation\neffectiveness can vary substantially with data characteristics, leading to recom-\nmendations to evaluate multiple ways of perturbing features [14,17]. However,\nhow this effectiveness varies with specific data characteristics remains largely\nunexplored. A closer examination of reported results reveals an intriguing pattern:\nsubstantial portions of datasets can remain unaffected by perturbation when\nusing a single strategy uniformly across all instances [14,18]. This observation sug-\ngests underlying methodological challenges that have not yet been systematically\ninvestigated.\nOur analysis of these empirical patterns points to an important methodological\nlimitation: the effectiveness of perturbation-based evaluation can vary substan-\ntially across different predicted classes, which we refer to as class-dependent\nperturbation effects. These effects manifest when perturbation strategies effec-\ntively validate feature attributions for some classes while showing limited or no\nsensitivity for others. We hypothesize that such behavior emerges from classifier\nbiases, where models learn to associate certain perturbation values with specific\nclasses, potentially compromising the reliability of current evaluation practices.\nOur research examines how class-dependent effects influence perturbation-\nbased evaluation of attributions. Through extensive empirical analysis, we show\nthat these effects appear more pronounced with perturbation strategies that\nshow strong aggregate performance, and persist across different perturbation\nstrategies, model architectures, and attribution methods. This asymmetry in\nperturbation effectiveness has important implications: data set imbalance may\ninfluence evaluation results, and evaluation metrics might reflect specific model\nbehaviors rather than attribution quality. Although our evidence stems from time\nseries classification, similar considerations may extend to other structured data\ndomains such as computer vision.\n\nClass-Dependent Perturbation Effects\n3\nThis paper makes several contributions to existing XAI evaluation methodolo-\ngies. First, we identify and characterize class-dependent effects in the evaluation\nof feature attributions with perturbation, supported by comprehensive empirical\nevidence across four commonly used benchmark datasets. Second, we introduce\na penalty term that can be applied to any aggregate XAI evaluation metric to\ninvestigate the extent of class-dependent effects. Third, we provide recommen-\ndations for evaluation protocols that consider these effects, including how to\nassess whether attribution methods are affected by class-specific perturbation\nbehaviors.\nThe remainder of this paper is organized as follows. Section 2 discusses related\nwork on perturbation analysis for time series classification. Section 3 introduces\nthe notation and a formal definition of perturbation analysis, as well as the\nmetrics used to measure explanation correctness and class differences. Section 4\npresents our experimental setup to investigate class-dependent perturbation\neffects. Section 5 analyzes our results and discusses their implications for XAI\nevaluation. Finally, Section 6 concludes with recommendations for future research\ndirections.\n2\nRelated Work\nThe evaluation of explanations remains a critical challenge in XAI research.\nTo address this, functional evaluation techniques have emerged as key com-\nputational methods for assessing the quality of explanations without human\nintervention [2,9].\nWe focus on perturbation analysis as a computational method to measure the\ncorrectness and sparsity of explanations. This approach was first introduced by\nSamek et al. [12] to evaluate feature attribution methods in the image domain.\nIt involves sequentially perturbing pixels in order of the most relevant features\nfirst by replacing them with noninformative values and observing the impact on\nmodel predictions. This process generates a perturbation curve that tracks these\nprediction changes, allowing the calculation of metrics such as the area under or\nover the curve to jointly measure the correctness and sparsity of explanations. The\nfundamental assumption underlying this approach is that perturbing important\nfeatures should degrade model predictions proportionally to their attributed\nimportance, while perturbing irrelevant features should have minimal effects on\nthe model output.\nWithin time series classification, there are various explanation methods,\ncategorized into approaches based on time points, subsequences, instances, and\nothers [22]. Our work focuses on feature attribution at the level of time points,\nexamining how each point within a time series contributes to model predictions.\nAs illustrated in Figure 1, these explanations identify the most influential parts\nof a time series for a prediction, visualized as a heatmap where darker regions\nindicate minimal contribution and lighter regions indicate stronger contribution\nto the prediction. The figure also demonstrates how different attribution methods\ncan yield varying explanations for the same instance, highlighting the need for\n\n4\nBaer et al.\nrobust evaluation methods that answer the question of which explanation is\ncorrect.\n0\n50\n100\n150\n200\n250\n300\n350\n400\n450\n-2.0\n-1.0\n0.0\n1.0\n2.0\nTime Step\n1.0\n0.8\n0.2\n0.4\n0.6\n0.0\nGradient SHAP\n0\n50\n100\n150\n200\n250\n300\n350\n400\n450\n-2.0\n-1.0\n0.0\n1.0\n2.0\nTime Step\n1.0\n0.8\n0.2\n0.4\n0.6\n0.0\nGradients \nFig. 1: Output from Gradients [19] and Gradient SHAP [7] attribution methods\nfor InceptionTime [6] classifier on one FordB dataset sample. The white line\nrepresents the input time series, while the heatmap indicates feature importance\nfor the predicted class over time, with lighter colors denoting higher importance.\nAttributions were normalized to [0,1].\nSchlegel et al. [13] were the first to apply perturbation analysis to time series\nclassification, evaluating the quality of attributions by measuring the average\nchange in accuracy over the perturbed samples with four perturbation strategies,\nincluding zero and mean value replacement. Mercier et al. [8] expanded on\nthis framework by evaluating attributions with additional metrics from image\nexplanations, such as sensitivity and infidelity, revealing that no single attribution\nmethod consistently outperforms others in all aspects of evaluation.\nAnother methodological advance came from Šimić et al. [18], who introduced\na metric that compares perturbations of features ordered by their attributed\nimportance. Their approach addressed a key limitation: perturbation strategies\ncan affect predictions regardless of the relevance and location of the feature. By\nmeasuring the difference between most and least relevant feature perturbations,\nthey provided a more robust assessment of attribution quality. Their metric builds\n\nClass-Dependent Perturbation Effects\n5\nupon the degradation score introduced for image explanations [16], adapting it\nwith cubic weighting to emphasize the early divergence between perturbation\norders.\nFurther developments focused on understanding the effectiveness of perturba-\ntions. Schlegel et al. [14] introduced a novel visualization method to qualitatively\nassess the effectiveness of perturbations by showing class distribution histograms\nand distances between the original and perturbed time series, among others. They\nalso benchmarked the effectiveness of 16 perturbation strategies by recording the\nnumber of flipped class labels. Building on this, Schlegel et al. [15] introduced\nthe AttributionStabilityIndicator, which incorporates the correlation between\noriginal and perturbed time series to ensure minimal data perturbations while\nmaintaining a significant prediction impact.\nRecent work explored additional methodological refinements. Turbé et al. [23]\nincorporated perturbations into model training to mitigate distribution shifts,\nwhile Nguyen et al. [10] developed a framework to recommend optimal explanation\nmethods based on aggregate accuracy loss across perturbed samples. Furthermore,\nSerramazza et al. [17] evaluated and extended InterpretTime [23] on various\nmultivariate time series classification tasks by averaging different perturbation\nstrategies and applying said strategies in chunks.\nHowever, an important question has remained unexplored: how do the char-\nacteristics of different classes affect perturbation-based evaluation methods?\nPerturbation strategies may be influenced by classifier biases. For example, if\na classifier learned to associate certain perturbation values (such as zero) with\nspecific classes, the effectiveness of perturbation-based evaluation could vary\nbetween different predicted classes. This consideration may help explain some\nfindings in the literature. For example, Schlegel et al. [14] evaluated 16 different\nperturbation approaches and found that for most datasets, only up to 60% of\nthe samples changed their predicted label under perturbation, regardless of the\nstrategy employed. Such results suggest that the effectiveness of perturbations\nmight be influenced by factors beyond the perturbation strategy itself. Our work\ninvestigates whether class-dependent effects could explain these observed patterns,\nexamining how the relationship between perturbation strategies and learned class\nrepresentations might affect evaluation outcomes.\n3\nClass-adjusted Perturbation Analysis\nFeature attribution methods for time series classification identify the time points\nthat influence a model’s predictions. Since there is usually no ground truth\nfor evaluating attributions, perturbation analysis is commonly used to assess\nattribution quality by modifying input features and observing the impact on\nmodel predictions. The assumption is that destroying information at important\ntime points should cause the predictions to change, while perturbing irrelevant\ntime points should have minimal impact.\nLet x = [x1, . . . , xN] represent a univariate time series of length N. A classifier\nf(x) outputs predicted probabilities over C classes, where qc denotes the proba-\n\n6\nBaer et al.\nbility of class c. An attribution method produces relevance scores r = [r1, . . . , rN]\nof the same length as x, where ri quantifies the importance of time point i\nto the model’s prediction. To evaluate whether attributions correctly identify\nrelevant features, we perturb the time series x using a perturbation strategy p.\nThe perturbed value at time point i is denoted as x′\ni. Common perturbation\nstrategies include replacing values with constants (x′\ni = 0), statistical aggregates\n(x′\ni = mean(x)), or transformations based on local statistics.\nWe evaluate attribution quality using the degradation score (DS) [18,16]. This\nmetric compares two perturbation sequences: most relevant features first (MoRF),\nwhere features are perturbed in descending order of attributed importance, and\nleast relevant features first (LeRF), where features are perturbed in ascending\norder. An effective attribution method should show strong prediction changes\nunder MoRF perturbation but minimal impact under LeRF perturbation. This\naligns with the intuition that modifying truly important features should signifi-\ncantly disrupt the model’s decision-making process, while perturbing irrelevant\nfeatures should leave the core signal intact. Figure 2 illustrates these expected\nbehaviors.\n0\n20\n40\n60\n80\n100\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTarget Class Probability\nPercentage of Perturbed Features (%)\nPerturbation Order\nMoRF\nLeRF\nFig. 2: Example of MoRF and LeRF perturbation curves. Here, we show a\ndesirable outcome, where PCMoRF drops quickly as a result of perturbation\nwhereas PCLeRF stays unaffected when perturbing the first 40% of least important\nfeatures, resulting in a high DS, or area between both perturbation curves.\nFormally, for a given instance x belonging to class c, a perturbation strategy p,\nand a number of perturbed features m, we track the prediction changes through\n\nClass-Dependent Perturbation Effects\n7\nthe perturbation curve, which is obtained from interpolating the vector:\nPCx(c, p, m) = [qcp1, . . . , qcpm] ,\n(1)\nwhere qcpi represents the predicted probability of class c after perturbing i features.\nIf all time points are perturbed, then m = N. Without loss of generality, we\ndenote the vectors PCx(c, p, m) describing the perturbation curves in the MoRF\nand LeRF order as PCMoRF and PCLeRF, respectively. Then, the DS measures\nthe sum of the signed difference between the LeRF and MoRF perturbation\ncurves at each perturbation step:\nDS = 1\nm\nm\nX\ni\n(PCLeRFi −PCMoRFi) .\n(2)\nThe DS ranges from -1 to 1, where positive values suggest correct feature attribu-\ntions (MoRF perturbations impact predictions more than LeRF), zero indicates\nnon-discriminative attributions (equal impact regardless of perturbation order),\nand negative values suggest flawed attributions (LeRF perturbations have a\nstronger impact than MoRF).\nTo evaluate attribution methods for multiple instances, we usually compute\nthe mean degradation score DS. However, this aggregate metric can mask im-\nportant class-specific behaviors. Therefore, we extend this evaluation framework\nby introducing the class-adjusted degradation score DSc that balances overall\nattribution accuracy with consistent performance across classes. This metric\nrewards attribution methods that achieve both high aggregate performance and\nuniform behavior across different classes. Formally, we define:\nDSc(α) = DS −α · ∆,\n(3)\nwhere α is a parameter in [0, 1] that controls the penalty strength. The penalty\nterm ∆quantifies the performance differences between classes. For example, for\nthe trivial case of binary classification, it measures the absolute difference in\nattribution performance between the two classes:\n∆= 1\n2|DS1 −DS0| .\n(4)\nFor multi-class problems, we extend this concept by computing the mean absolute\ndifference across all possible class pairs:\n∆= 1\n2\n1\n\u0000C\n2\n\u0001\nX\ni<j\n|DSi −DSj| =\n1\nC(C −1)\nX\ni<j\n|DSi −DSj| ,\n(5)\nwhere i, j are class indices and C is the number of classes. Since the maximum\npossible value of the mean absolute difference in all pairs is 2, we introduce a\nfactor 1\n2 that normalizes the penalty to the interval [0,1]. This ensures that the\nclass-adjusted degradation score DSc remains on the [-1,1] scale, conserving its\ninterpretation. Setting α = 1 assigns equal importance to overall attribution\n\n8\nBaer et al.\ncorrectness (DS) and consistency between classes (∆), ensuring that both aspects\nare weighted equally in the final evaluation metric. For example, in a binary\nclassification task where DS1 = 1 and DS0 = 0, then DS = 0.5, which can be\ninterpreted as a moderately good degradation score value. However, the MoRF\nand LeRF curves behave perfectly for all instances of class 1, while for class 0, no\ndifferences are observed in the curves on average. Assuming α = 1, the penalty\n∆= 0.5 allows us to account for this difference in class behavior, resulting in a\nmore strict class-adjusted degradation score DSc = 0.\nAlthough we apply this penalty framework to the mean DS, it generalizes to\nany evaluation metric. This allows a systematic investigation of class-dependent\neffects in attribution evaluation by comparing base metrics against their class-\nadjusted variants. We use this framework to analyze how perturbation-based\nevaluation methods may exhibit different behaviors across classes.\n4\nExperiment Set-Up\nWe design our experiments to systematically investigate the effectiveness of\nattribution methods for time series data, with a particular focus on class-specific\ndifferences in perturbation behavior. Our investigation addresses three key aspects:\n(1) the general effectiveness of attribution methods across architectures and\ndatasets, (2) the presence and characteristics of class-dependent perturbation\neffects, and (3) the relationship between perturbation strategy selection and these\neffects.\nWe use four univariate time series datasets from the UCR Time Series\nClassification Archive [1]: FordA, FordB, ElectricDevices (ElecDev) and Wafer.\nTable 1 presents the characteristics of these datasets along with the achieved\naccuracy of the classifiers used in this study. These datasets are among the largest\nin the UCR Archive and are commonly used to evaluate feature attributions with\nperturbations [13,18,8,14,15,23,8], making our results comparable to previous\nwork.\nTable 1: Dataset characteristics and model performance across architectures.\nAccuracy metrics (Train, Val, Test) are reported in decimal format.\nResNet\nInceptionTime\nDataset\nTrain\nTest\nLength\nClasses\nTrain\nVal\nTest\nTrain\nVal\nTest\nSize\nSize\nFordA\n3,601\n1,320\n500\n2\n0.999\n0.931\n0.937\n1.000\n0.945\n0.952\nFordB\n3,636\n810\n500\n2\n1.000\n0.930\n0.804\n0.997\n0.938\n0.849\nWafer\n1,000\n6,164\n152\n2\n1.000\n1.000\n0.993\n1.000\n1.000\n0.998\nElecDev\n8,926\n7,711\n96\n7\n0.981\n0.913\n0.716\n0.987\n0.895\n0.702\n\nClass-Dependent Perturbation Effects\n9\nFor model training, we select two distinct and widely-adopted deep learning\narchitectures: ResNet [24] and InceptionTime [6] for their strong performance\nbaselines [3] and different feature extraction approaches. Following the predefined\nUCR splits, we train each model with a batch size of 256 using the AdamW\noptimizer with cosine annealing learning rate scheduling for up to 500 epochs,\nimplementing early stopping with patience of 25 epochs to ensure stable model\nconvergence. The validation set for early stopping consists of 20% of randomly\nselected observations from the train set, stratified by the class label. As shown\nin Table 1, both architectures achieve performance comparable to previous\nperturbation studies [14,18] and approach the performance of state-of-the-art\ndeep learning models [3]. Although we observe some performance disparity\nbetween training and test sets, indicating potential model capacity for further\noptimization through hyperparameter tuning, the achieved performance levels\nare sufficient for our objective of evaluating feature attributions.\nTable 2: Perturbation strategies investigated in our experiments. Each strategy\ntransforms an input time series, represented as vector x = [x1, . . . , xN] where xi\nrepresents the value at time step i. The perturbed value is denoted as x′\ni. For\nstrategies involving subsequence length k, we set k = 0.1 in our experiments,\ncorresponding to 10% of the time series length.\nStrategy\nDescription\nModification procedure\nGauss\nRandom noise\nfrom distribution\nx′\ni ∼N(mean(x), std(x))\nUnif\nRandom values\nwithin range\nx′\ni ∼U(min(x), max(x))\nOpp\nFlip sign\nx′\ni = −xi\nInv\nInvert around\nmaximum\nx′\ni = max(x) −xi\nSubMean\nLocal\nsubsequence\naverage\nx′\ni =\n1\n|Wi|\nP\nj∈Wi xj, where\nWi = {j : max(0, i −k + 1) ≤j ≤i}\nZero\nReplace with\nzero\nx′\ni = 0\nConstant\nReplace with\npredefined values\nx′\ni = c where\nc ∈{−2, −1.5, −1, −0.5, 0, 0.5, 1, 1.5, 2}\nWe evaluate five widely adopted attribution methods.5 These include four\ngradient-based methods: Gradients (GR) [19], Integrated Gradients (IG) [21],\nSmoothGrad (SG) [20], and Gradient SHAP (GS) [7]. We also include a method\n5 We implement all attribution methods using the TSInterpret package [5].\n\n10\nBaer et al.\nbased on perturbation, Feature Occlusion (FO) [4]. To ensure balanced class rep-\nresentation while maintaining computational feasibility, we compute attributions\non a stratified sample of 300 instances per class from the test set. Attributions\nalways explain the predicted class label.\nOur experimental design incorporates six established perturbation strategies\nfrom previous work [18,14] and extends them with a systematic framework of\nconstant-value perturbations. The details of the different strategies are described\nin Table 2. Although previous studies primarily focus on mean and zero value\nsubstitution among other more sophisticated strategies, we also evaluate a com-\nprehensive grid of constant perturbation values ranging between −2 and 2. This\nextension provides an interpretable baseline for understanding class-dependent\nperturbation effects, especially given the normalization of the UCR datasets to\nzero mean and unit standard deviation.\nWe apply these perturbation strategies systematically following the MoRF\nand LeRF orders, as determined by the feature attributions. For both orders,\nfeatures are perturbed incrementally in steps of 2% of the time series length\n(rounded up) until 50% perturbation coverage is reached, recording the predicted\nprobabilities at each perturbation step. This bounded perturbation approach\nensures computational efficiency by excluding the latter half of features, which\ntypically exhibit minimal discriminative power.\nTo analyze both overall attribution quality and potential class-dependent\neffects, we employ two evaluation approaches. First, we compute the DS (Equa-\ntion 2) from these perturbation curves, providing a normalized measure between\n-1 and 1 for each sample that enables consistent comparison across datasets\nwith varying time series lengths and perturbation step sizes. We establish overall\nperformance by averaging DS metrics across all experimental conditions, allow-\ning comparison with previous perturbation studies. To capture class-dependent\neffects, we extend this analysis using class-adjusted penalties (Equations 4 and 5)\nto calculate the class-adjusted metric, DSc (Equation 3) with α = 1.\n5\nResults and Discussion\n5.1\nEvaluating Attribution Quality\nTo establish baseline performance and enable comparison with previous work, we\nbegin by evaluating the overall effectiveness of different attribution methods and\nperturbation strategies. Table 3 presents the mean DS metrics aggregated across\nall experimental conditions. The observed DS ranges align with previous findings\nby Šimić et al. [18], suggesting consistent behavior in different experimental\nsettings.\nExamining these results in detail, we find that the perturbation strategies ex-\nhibit varying levels of performance. Zero and SubMean perturbations consistently\nachieve superior results across most experimental configurations, ranking among\nthe top performers in terms of mean DS. This suggests that they are generally\neffective across different datasets and model architectures. However, this pattern\n\nClass-Dependent Perturbation Effects\n11\nTable 3: Mean degradation scores (DS) for named perturbation strategies across\ndatasets, models, and attribution methods. DS measures the average differential\nimpact between perturbing most and least relevant features. Positive values\nindicate correct feature identification, values near zero suggest non-discriminative\nattributions, and negative values indicate reversed feature importance. Bold and\nitalic values highlight the highest and second-highest values per dataset and\nmodel-attribution combination.\nResNet\nInceptionTime\nDataset\nPerturbation\nGR\nIG\nSG\nGS\nFO\nGR\nIG\nSG\nGS\nFO\nFordA\nGauss\n-0.01\n0.06\n0.02 -0.04 0.09\n-0.00\n0.11\n0.01 -0.02 0.14\nInv\n-0.00 0.01\n0.01\n0.01\n0.02 -0.00 0.02\n0.00\n0.01\n0.02\nOpp\n-0.03\n0.03\n0.01\n0.03\n0.04\n-0.05\n0.06\n0.02\n0.04\n0.06\nSubMean\n-0.04\n0.07\n0.03 0.07\n0.13\n-0.09\n0.17\n0.05 0.14\n0.20\nUnif\n-0.01\n0.01\n0.00\n0.01\n0.01\n-0.01\n0.02\n0.01\n0.02\n0.02\nZero\n-0.04\n0.07 0.03\n0.07 0.13\n-0.10\n0.17 0.05\n0.15 0.20\nFordB\nGauss\n-0.00\n0.02 -0.00 -0.03 0.03\n-0.03\n0.13 0.01 -0.03 0.18\nInv\n-0.00 0.00 -0.00 0.00\n0.00 -0.01 0.02\n0.01\n0.01\n0.02\nOpp\n-0.01\n0.02\n0.00\n0.02\n0.02\n-0.04\n0.03 -0.02 0.03\n0.03\nSubMean\n-0.02\n0.04 0.01\n0.03 0.06\n-0.07\n0.09 -0.07 0.08\n0.10\nUnif\n-0.00\n0.00\n0.00\n0.00\n0.01\n-0.01\n0.01\n0.00\n0.00\n0.01\nZero\n-0.02\n0.04\n0.01 0.03\n0.06\n-0.07\n0.09\n-0.07 0.08 0.10\nWafer\nGauss\n-0.06 -0.10 0.01\n0.08\n-0.16\n0.02\n-0.10 0.12 0.18 -0.20\nInv\n0.02\n0.06\n0.04\n0.04\n0.14\n-0.01\n0.09\n0.03\n0.07\n0.24\nOpp\n-0.02\n0.01\n0.01\n0.01\n0.02\n-0.01\n0.06\n0.04\n0.05\n0.13\nSubMean\n0.12\n0.20\n0.01\n0.17 0.33\n0.06\n0.05\n0.03\n0.07\n0.06\nUnif\n-0.01\n0.01\n0.03\n0.02\n0.03\n0.03\n0.02\n0.07\n0.02\n0.10\nZero\n-0.02\n0.06\n0.05\n0.05\n0.14\n0.02\n0.08\n0.04\n0.05\n0.28\nElecDev Gauss\n-0.00 -0.01 0.01\n0.01 -0.01\n0.03\n-0.00 0.08\n0.05 -0.04\nInv\n0.01\n0.06\n0.08\n0.05\n0.07\n-0.03\n0.06\n0.01\n0.06\n0.03\nOpp\n0.02\n0.18\n0.06\n0.14\n0.22\n0.07\n0.18\n0.09\n0.18\n0.20\nSubMean\n0.14\n0.24 0.22 0.22 0.27\n0.15\n0.22 0.11 0.20 0.29\nUnif\n0.05\n0.09\n0.05\n0.06\n0.16\n0.04\n0.10\n0.05\n0.07\n0.11\nZero\n0.04\n0.16\n0.08\n0.12\n0.27\n0.09\n0.19\n0.08\n0.16\n0.27\n\n12\nBaer et al.\nhas notable exceptions. For the InceptionTime classifier, Gauss perturbations\ndemonstrate superior performance in two specific cases. On the FordB dataset,\nthey achieve the highest performance with IG, SG, FO attributions (DS equal\nto 0.13, 0.01 and 0.18, respectively). Similarly, on the Wafer dataset, they excel\nwith SG and GS attributions (DS equal to 0.12 and 0.18, respectively). In all\nother configurations, Gauss perturbations remain less effective.\nTurning to attribution methods, we observe substantial performance variations.\nFO consistently achieves the highest DS values across datasets and perturbation\nstrategies, indicating superior feature importance identification. IG and GS\ndemonstrate comparable performance levels, though generally lower than FO.\nIn contrast, GR shows poor discriminative ability, frequently yielding negative\nor near-zero DS values, suggesting that its feature importance assignments are\noften not better than random ordering.\nAlthough these aggregate metrics provide valuable insights into overall method\neffectiveness, they potentially mask important variations in performance distribu-\ntions. To better understand these underlying patterns, we examine the distribution\nof DS metrics through a more detailed case study.\n5.2\nDistribution Patterns in Attribution Quality\nTo understand how attribution performance varies between individual instances,\nwe analyze the distributional characteristics of DS scores. We select the FordB\ndataset, InceptionTime architecture, and SubMean perturbation strategy as a\ncase study, as these demonstrate patterns typical of our broader findings. Figure 3\npresents these distributions, revealing both overall performance patterns and,\ncrucially, class-specific effects.\nThe aggregate distributions shown in Figure 3a reveal notable variation in\nattribution method effectiveness. IG, GS, and FO exhibit positively skewed\ndistributions with extended tails toward higher DS values, indicating superior\nattribution quality. In contrast, GR and SG demonstrate negative skewness\nwith tails extending toward lower DS values, suggesting less reliable feature\nidentification. A critical observation is the concentration of scores around zero\nacross all methods, with mean DS values confined to a narrow range of [-0.07,\n0.10] also observed in Table 3. This clustering implies limited discriminative\npower in feature ordering for many instances, even among the better-performing\nmethods.\nClass-stratified analysis, presented in Figure 3b, uncovers substantial hetero-\ngeneity in attribution quality across classes. For IG, GS, and FO, instances from\nclass 1 consistently achieve higher DS scores with pronounced positive tails and\nminimal negative values. However, these same methods show markedly different\nbehavior for class 0, where DS scores cluster tightly around zero, indicating\nminimal perturbation impact. GR and SG show an inverse pattern, with class 1\nshowing predominantly negative scores while class 0 maintains a more balanced\ndistribution centered near zero.\nThese pronounced class-dependent variations in attribution performance raise\nquestions about the generalizability of perturbation-based evaluation methods.\n\nClass-Dependent Perturbation Effects\n13\nGR\nIG\nSG\nGS\nFO\n-0.4\n-0.2\n0.0\n0.2\n0.4\n0.6\nDegradation Score (DS)\nAttribution Method\n(a) Overall DS distributions per attribution method.\nGR\nIG\nSG\nGS\nFO\n-0.4\n-0.2\n0.0\n0.2\n0.4\n0.6\nDegradation Score (DS)\nAttribution Method\nTrue Label\n0\n1\n(b) Class-specific DS distributions per attribution method.\nFig. 3: Distributions of DS for different attribution methods on the FordB dataset\n(classifier: InceptionTime, perturbation strategy: SubMean).\n\n14\nBaer et al.\nTo establish whether these observed patterns represent a systematic phenomenon\nbeyond this specific case study, we extend our investigation to examine class-\ndependent behavior across all experimental conditions.\n5.3\nClass-Dependent Effects in Perturbation Analysis\nTo systematically investigate the presence and extent of class-dependent effects\nin perturbation-based evaluation, we apply the proposed class-adjusted DS (DSc)\nmetric to all experimental conditions. Table 4 presents these adjusted scores,\nwhich balance average attribution correctness with consistency between classes.\nA notable pattern emerges: the incorporation of class consistency penalties\nsubstantially reduces performance metrics across most experimental conditions,\nparticularly for previously high-performing perturbation strategies.\nZero and SubMean perturbation strategies, which demonstrated superior\nperformance before, show marked degradation under class-adjusted evaluation.\nFor the FordA and FordB datasets, these strategies’ scores approach zero or\nbecome negative after applying class-adjusted penalties, suggesting that their\napparent effectiveness stems primarily from class-specific behavior rather than\ngenuine attribution quality over all instances. The impact varies by dataset.\nWafer and ElecDev show more resilience to class adjustment, though we still see\nreduced scores. For ElecDev, the interpretation of these results requires additional\ncontext: as the only multiclass dataset in our evaluation, the pairwise averaging\nof class differences may underestimate class-specific effects due to the higher\ndimensionality of the classification space.\nTo examine the mechanisms underlying these class-specific effects, Figure 4\npresents a detailed analysis of perturbation impacts across classes on the FordB\ndataset. Figure 4a, which focuses on named perturbation strategies with In-\nceptionTime and FO attribution, reveals systematic class-dependent behavior.\nClass 0 instances consistently demonstrate minimal response to perturbation\nacross all strategies, with DS scores tightly clustered around zero. In contrast,\nClass 1 instances show substantial variability in perturbation response, with\nGauss achieving highest effectiveness, followed by Zero and SubMean strategies.\nThis asymmetric response pattern explains the earlier observed degradation in\nclass-adjusted metrics: perturbation strategies succeed primarily by exploiting\nclass-specific model behaviors.\nAnalysing the constant-value perturbations in Figure 4b reinforces these\nfindings while providing additional information. The evaluation of perturbation\nvalues between -2 and 2 reveals optimal effectiveness at moderate positive values,\nparticularly around 0.5, challenging the conventional preference for zero-based\nperturbation. We now extend the investigation of the constant perturbation\nstrategies to all experimental conditions.\n\nClass-Dependent Perturbation Effects\n15\nTable 4: Class-adjusted mean degradation scores (DSc with α = 1) for named\nperturbation strategies across datasets, models, and attribution methods. DSc\nmeasures both attribution quality and cross-class consistency of perturbations.\nPositive values indicate good attribution quality with consistent behavior across\nclasses, while lower values suggest either poor attribution quality or inconsistent\nperformance between classes. Bold and italic values highlight the highest and\nsecond-highest values per dataset and model-attribution combination.\nResNet\nInceptionTime\nDataset Perturbation\nGR\nIG\nSG\nGS\nFO\nGR\nIG\nSG\nGS\nFO\nFordA\nGauss\n-0.01\n0.00 0.00 -0.08 0.00\n-0.01\n0.00 -0.00 -0.05 0.00\nInv\n-0.00 0.00\n0.00\n0.00\n0.00 -0.01 0.00\n-0.00\n0.00\n0.00\nOpp\n-0.05\n0.00\n0.00\n0.00\n0.00\n-0.10\n0.00\n-0.00\n0.00\n0.00\nSubMean\n-0.08\n0.00\n0.00\n0.00\n0.01 -0.19\n0.00\n0.00\n0.00 -0.00\nUnif\n-0.02\n0.00\n0.00\n0.00\n0.00\n-0.03 -0.00 -0.00 0.00 -0.00\nZero\n-0.08 0.00 0.00\n0.00 0.01\n-0.20\n0.00\n0.00 -0.00 -0.00\nFordB\nGauss\n-0.00\n0.00 -0.00 -0.05 0.01\n-0.07 0.01\n0.01 -0.05 0.03\nInv\n-0.00 0.00 -0.00 0.00\n0.00 -0.01 0.00\n0.00\n0.00\n0.00\nOpp\n-0.02\n0.00 0.00 0.00\n0.00\n-0.09\n0.00\n-0.04\n0.00\n0.01\nSubMean\n-0.04 0.01 0.00\n0.00\n0.01 -0.16\n0.01\n-0.13\n0.01\n0.01\nUnif\n-0.01\n0.00\n0.00\n0.00\n0.00\n-0.02\n0.00\n0.00\n0.00\n0.00\nZero\n-0.04\n0.01\n0.00\n0.01 0.01\n-0.16\n0.01\n-0.13 0.01 0.02\nWafer\nGauss\n-0.15 -0.21 0.00\n0.00 -0.37\n0.00\n-0.20 -0.00\n0.03 -0.42\nInv\n0.01\n0.00 -0.00 0.00\n0.00\n-0.03\n0.00\n-0.01\n0.01\n0.01\nOpp\n-0.04\n0.00 -0.00 0.00\n0.00\n-0.02\n0.00\n-0.01\n0.01\n0.01\nSubMean\n0.03\n0.02 -0.01 0.04 0.04\n0.01\n0.05 -0.00 0.02 0.03\nUnif\n-0.02\n0.00 0.00 0.00\n0.00\n0.00\n0.00\n-0.01\n0.01\n0.01\nZero\n-0.05\n0.01 -0.00 0.01\n0.01\n0.01\n0.01\n-0.02 0.04 0.02\nElecDev Gauss\n-0.04 -0.04 -0.02 -0.03 -0.04\n0.01\n-0.01\n0.04\n0.03 -0.09\nInv\n-0.02\n0.01\n0.05\n0.01\n0.03\n-0.08\n0.02\n-0.05\n0.03 -0.04\nOpp\n-0.03\n0.10\n0.00\n0.10\n0.16\n0.04\n0.14\n0.04\n0.12\n0.12\nSubMean\n0.08\n0.13 0.14 0.14 0.22\n0.07\n0.15\n0.08\n0.13 0.23\nUnif\n0.03\n0.03 -0.00 0.02\n0.08\n-0.00\n0.06\n0.03\n0.04\n0.02\nZero\n-0.01\n0.09\n0.03\n0.06 0.23\n0.05\n0.12\n0.04\n0.10\n0.16\n\n16\nBaer et al.\nUnif\nInv\nOpp\nZero\nSubMean\nGauss\n-0.1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nDegradation Score (DS)\nPerturbation Strategy\nTrue Label\n0\n1\n(a) DS for named perturbation strategies.\n-2\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\n2\n-0.2\n0.0\n0.2\n0.4\n0.6\n0.8\nDegradation Score (DS)\nPerturbation Strategy\nTrue Label\n0\n1\n(b) DS for constant perturbation strategies.\nFig. 4: Class-stratified analysis of DS for different perturbation strategies on the\nFordB dataset (classifier: InceptionTime, attribution method: FO). Thin lines\nshow individual observations, while thick lines indicate class means.\n\nClass-Dependent Perturbation Effects\n17\n5.4\nEvaluating Constant Perturbation Values\nTable 5 shows the class-adjusted DS for the constant perturbation strategies,\nrevealing some consistent patterns in the datasets while highlighting important\ndomain-specific variations.\nModerate positive perturbation values between 0 and 1 often achieve the best\nperformance, although effectiveness varies by dataset and classifier architecture.\nFordA and FordB demonstrate the best performance with 0.5 perturbation.\nWafer is inconsistent, with different perturbation constants working best for\ndifferent attribution methods. ElecDev exhibits the best performance with lower\nperturbation values between -0.5 and 0.5, though its multiclass nature introduces\nadditional complexity in interpreting class-adjusted metrics.\nInterestingly, negative perturbation values consistently produce negative class-\nadjusted DS scores for most datasets, indicating asymmetric class responses to\nthe perturbation direction. However, this effect varies by attribution method,\nclassifier, and dataset. For example, SG and GS on ElecDev only show positive\nadjusted DS scores. These results suggest that the effectiveness of perturbation-\nbased evaluation depends on the alignment between perturbation values and the\nlearned class representations.\n5.5\nSynthesis and Implications\nOur investigation reveals three key findings on perturbation-based evaluation\nof feature attributions. First, the effectiveness of perturbation strategies can\nvary substantially between classes, with performance often showing asymmetric\nimpacts. This pattern appears particularly in binary classification tasks, where\nperturbation strategies may validate attributions effectively for one class while\nshowing limited sensitivity to the other.\nSecond, we observe that perturbation strategies that show strong aggregate\nperformance often demonstrate the most pronounced class-dependent effects.\nWhen examining these strategies using our class-adjusted framework, we find\nthat much of their effectiveness stems from strong performance in specific classes\nrather than consistent behavior across all classes. This observation suggests the\nimportance of examining class-specific responses when evaluating perturbation\nstrategies.\nThird, our analysis of constant perturbation strategies shows that optimal\nperturbation values can vary across datasets and architectures, indicating that\nperturbation effectiveness may be influenced by the specific characteristics of\nlearned model representations. This finding suggests that the choice of perturba-\ntion values warrants careful consideration for each specific application context.\nThese findings have important implications for XAI evaluation methodology.\nThe observed class-dependent effects indicate that aggregate performance metrics\nbenefit from additional analysis that considers class-specific behaviors. When\ndesigning evaluation protocols, researchers may want to examine whether their\nchosen perturbation strategies show consistent effectiveness across classes, partic-\nularly in contexts with class imbalance or when working with new domains and\nmodel architectures.\n\n18\nBaer et al.\nTable 5: Class-adjusted mean degradation scores (DSc with α = 1) for constant\nperturbation strategies across datasets, models, and attribution methods. DSc\nmeasures both attribution quality and cross-class consistency of perturbations.\nPositive values indicate good attribution quality with consistent behavior across\nclasses, while lower values suggest either poor attribution quality or inconsistent\nperformance between classes. Bold and italic values highlight the highest and\nsecond-highest values per dataset and model-attribution combination.\nResNet\nInceptionTime\nDataset Perturbation\nGR\nIG\nSG\nGS\nFO\nGR\nIG\nSG\nGS\nFO\nFordA\n-2\n-0.05 -0.07 -0.01 -0.07 -0.08 -0.02 -0.08 -0.00 -0.04 -0.08\n-1.5\n-0.07 -0.09 -0.01 -0.09 -0.09 -0.03 -0.10 -0.00 -0.06 -0.11\n-1\n-0.10 -0.10 -0.01 -0.10 -0.09 -0.05 -0.09\n0.00\n-0.05 -0.09\n-0.5\n-0.13 -0.01 -0.00 -0.02 0.00\n-0.14\n0.00\n0.00\n-0.00 -0.00\n0\n-0.08\n0.00\n0.00\n0.00\n0.01\n-0.20\n0.00\n0.00 -0.00 -0.00\n0.5\n-0.03 0.00 0.00 0.00 0.01 -0.04 0.00\n0.00\n0.00 0.00\n1\n-0.01\n0.00\n0.00\n0.00\n0.00\n-0.01\n0.00\n-0.00\n0.00\n0.00\n1.5\n-0.01 0.00\n0.00\n0.00\n0.00\n-0.00\n0.00\n-0.00\n0.00\n0.00\n2\n-0.01\n0.00\n0.00\n0.00\n0.00 -0.00 0.00\n-0.00\n0.00\n0.00\nFordB\n-2\n-0.01 -0.04 0.00 -0.04 -0.02 -0.00 -0.05 -0.00 -0.04 -0.06\n-1.5\n-0.02 -0.05 0.00 -0.05 -0.02 -0.01 -0.06 -0.00 -0.05 -0.07\n-1\n-0.03 -0.03 0.00 -0.04 -0.01 -0.02 -0.06 -0.02 -0.06 -0.08\n-0.5\n-0.04\n0.00\n0.00 -0.00 0.00\n-0.05 -0.02 -0.06 -0.02 -0.02\n0\n-0.04\n0.01\n0.00\n0.01\n0.01\n-0.16\n0.01\n-0.13\n0.01\n0.02\n0.5\n-0.02 0.01 -0.00 0.01 0.02 -0.10 0.02 -0.02 0.02 0.04\n1\n-0.00\n0.00 -0.00 0.00\n0.01\n-0.07\n0.02\n0.01\n0.01\n0.03\n1.5\n-0.00\n0.00 -0.00 0.00\n0.00\n-0.05\n0.01\n0.01\n0.01\n0.01\n2\n-0.00 0.00 -0.00 0.00\n0.00\n-0.03\n0.00\n0.00\n0.00\n0.01\nWafer\n-2\n-0.03 -0.05 0.00 -0.05 -0.13\n0.00\n-0.12 -0.00 -0.15 -0.37\n-1.5\n-0.09 -0.17 0.00 -0.19 -0.29\n0.00\n-0.17 -0.00 -0.25 -0.42\n-1\n-0.20 -0.20 0.00 -0.27 -0.31\n0.01\n-0.23 -0.01 -0.31 -0.34\n-0.5\n-0.14 -0.05 -0.00 -0.08 -0.05 0.02 -0.18 -0.01 -0.22 0.02\n0\n-0.05\n0.01 -0.00 0.01 0.01\n0.01\n0.01\n-0.02\n0.04\n0.02\n0.5\n0.01\n0.01 0.00\n0.01\n0.02\n0.01\n0.01 -0.03 0.05 0.05\n1\n0.02\n0.00 0.00 0.00\n0.02\n-0.01\n0.00\n-0.03\n0.03 0.06\n1.5\n0.03 -0.00 -0.00 -0.00 0.01\n0.00\n0.00\n-0.02\n0.02\n0.04\n2\n0.02\n-0.01 -0.00 -0.00 -0.00 -0.00\n0.00\n-0.01\n0.01\n0.03\nElecDev -2\n-0.05 -0.04 -0.03 -0.04 -0.05\n0.00\n-0.03\n0.04\n0.01 -0.11\n-1.5\n-0.02 -0.04 -0.01 -0.03 -0.03\n0.02\n-0.01\n0.05\n0.02 -0.07\n-1\n0.02 -0.07 0.00 -0.03 -0.01\n0.02\n0.00\n0.05\n0.04 -0.02\n-0.5\n0.00\n-0.03 0.02\n0.01\n0.05\n0.05\n0.06\n0.08\n0.09\n0.10\n0\n-0.01\n0.09\n0.03\n0.06\n0.23\n0.05\n0.12\n0.04\n0.10\n0.16\n0.5\n0.00\n0.15 0.10 0.13 0.27\n0.01\n0.11\n0.02\n0.11 0.14\n1\n0.01\n0.11\n0.08\n0.10\n0.19\n0.01\n0.11\n0.03\n0.09\n0.12\n1.5\n0.01\n0.09\n0.08\n0.09\n0.14\n0.02\n0.10\n0.03\n0.07\n0.06\n2\n0.00\n0.07\n0.08\n0.07\n0.08\n0.02\n0.04\n0.04\n0.03 -0.00\n\nClass-Dependent Perturbation Effects\n19\n6\nConclusion and Future Work\nThis paper presents a systematic investigation of class-dependent effects in\nperturbation-based evaluation of feature attributions for time series classification.\nThrough empirical evaluation across four datasets, five attribution methods,\nand multiple perturbation strategies, we demonstrate that perturbation-based\nevaluation methods can exhibit class-specific behaviors that warrant careful\nconsideration in validation procedures.\nOur investigation yields three main findings. First, we show that perturbation\nstrategies can exhibit notable class-dependent effects, where attribution validation\nmay show varying effectiveness across different classes. Second, we demonstrate\nthat perturbation strategies that show strong aggregate performance may derive\ntheir effectiveness partially from class-specific effects rather than solely from\nattribution quality. When controlling for class-dependent behaviors through\nour proposed penalty framework, we see that most of their effects come from\nstrong performance for only one class. Third, our analysis reveals that the best\nperturbation values, including alternatives to the commonly used zero-value\nsubstitution, vary substantially across datasets and model architectures. This\nindicates that perturbation effectiveness can depend heavily on learned model\nrepresentations rather than intrinsic attribution quality.\nThese findings have important implications for XAI evaluation methodology.\nThe presence of class-dependent effects suggests that aggregate performance\nmetrics benefit from additional analysis, particularly in imbalanced datasets\nwhere dominant classes might influence overall performance metrics. Furthermore,\nthe variation in optimal perturbation values between different experimental\nconfigurations indicates that evaluation protocols may benefit from dataset-\nspecific calibration. To address these challenges, we introduce a class-adjusted\npenalty framework applicable to any aggregate XAI evaluation metric, enabling\nresearchers to investigate potential class-specific patterns without additional\ncomputational overhead. Beyond aggregate analysis, we recommend examining\nindividual instances with multiple perturbation strategies designed to push model\npredictions toward different classes. If an attribution’s effect can be validated\nusing at least one such strategy, this can suggest that the attribution itself is\nsound, even if other strategies prove ineffective due to class-specific behaviors.\nSeveral limitations of our study should be noted. Our investigation focuses\non only four datasets and two classifier architectures, and while we observe\nconsistent patterns across multiple perturbation strategies, these findings may\nnot generalize to all contexts. For instance, the observed patterns in the responses\nof the classifiers to perturbations may reflect specific characteristics of our\nexperimental setup, such as the way the classifiers were trained, rather than\ngeneral properties of the evaluation approach. However, our results align well\nwith previous studies in the literature, suggesting broader applicability of our\nmethodology. We encourage researchers to validate and extend these findings\nusing their own datasets and experimental configurations.\nFuture research directions emerge from these findings. One promising av-\nenue involves developing perturbation strategies that systematically account for\n\n20\nBaer et al.\nclass-specific model behaviors, particularly focusing on methods that can push\npredictions toward different classes. This approach could lead to more compre-\nhensive evaluation metrics that consider the directionality of perturbation effects.\nAdditionally, investigating methods to adaptively select perturbation strategies\nfor individual instances could improve evaluation effectiveness, as our results\nsuggest that different instances may require different perturbation approaches\nto effectively validate their attributions. Understanding this interaction between\ninstance characteristics and perturbation effectiveness could provide insight into\ndesigning more robust evaluation frameworks.\nAcknowledgments. This paper is supported by the European Union’s HORIZON\nResearch and Innovation Program under grant agreement No. 101120657, project\nENFIELD (European Lighthouse to Manifest Trustworthy and Green AI).\nDisclosure of Interests. All authors declare that they have no conflicts of interest.\nReferences\n1. Dau, H.A., Bagnall, A., Kamgar, K., Yeh, C.C.M., Zhu, Y., Gharghabi, S.,\nRatanamahatana, C.A., Keogh, E.: The UCR time series archive. IEEE/CAA\nJournal of Automatica Sinica 6, 1293–1305 (2019). https://doi.org/10.1109/JAS.\n2019.1911747\n2. Doshi-Velez, F., Kim, B.: Considerations for Evaluation and Generalization in\nInterpretable Machine Learning. In: Escalante, H.J., Escalera, S., Guyon, I., Baró,\nX., Güçlütürk, Y., Güçlü, U., van Gerven, M. (eds.) Explainable and Interpretable\nModels in Computer Vision and Machine Learning, pp. 3–17. Springer (2018).\nhttps://doi.org/10.1007/978-3-319-98131-4_1\n3. Fawaz, H.I., Forestier, G., Weber, J., Idoumghar, L., Muller, P.A.: Deep learning\nfor time series classification: A review. Data Mining and Knowledge Discovery 33,\n917–963 (2019). https://doi.org/10.1007/s10618-019-00619-1\n4. Fong, R.C., Vedaldi, A.: Interpretable Explanations of Black Boxes by Meaningful\nPerturbation. In: Proceedings of the IEEE International Conference on Computer\nVision. pp. 3429–3437 (2017), https://openaccess.thecvf.com/content_iccv_2017/\nhtml/Fong_Interpretable_Explanations_of_ICCV_2017_paper.html\n5. Höllig, J., Kulbach, C., Thoma, S.: TSInterpret: A python package for the inter-\npretability of time series classification. Journal of Open Source Software 8, 5220\n(2023). https://doi.org/10.21105/joss.05220\n6. Ismail Fawaz, H., Lucas, B., Forestier, G., Pelletier, C., Schmidt, D.F., Weber, J.,\nWebb, G.I., Idoumghar, L., Muller, P.A., Petitjean, F.: InceptionTime: Finding\nAlexNet for time series classification. Data Mining and Knowledge Discovery 34,\n1936–1962 (2020). https://doi.org/10.1007/s10618-020-00710-y\n7. Lundberg, S.M., Lee, S.I.: A Unified Approach to Interpreting Model Predic-\ntions. In: Guyon, I., Luxburg, U.V., Bengio, S., Wallach, H., Fergus, R., Vish-\nwanathan, S., Garnett, R. (eds.) Advances in Neural Information Processing Sys-\ntems. vol. 30 (2017), https://proceedings.neurips.cc/paper_files/paper/2017/file/\n8a20a8621978632d76c43dfd28b67767-Paper.pdf\n8. Mercier, D., Bhatt, J., Dengel, A., Ahmed, S.: Time to Focus: A Comprehensive\nBenchmark Using Time Series Attribution Methods (2022). https://doi.org/10.\n48550/arXiv.2202.03759\n\nClass-Dependent Perturbation Effects\n21\n9. Nauta, M., Trienes, J., Pathak, S., Nguyen, E., Peters, M., Schmitt, Y., Schlötterer,\nJ., Van Keulen, M., Seifert, C.: From Anecdotal Evidence to Quantitative Evaluation\nMethods: A Systematic Review on Evaluating Explainable AI. ACM Computing\nSurveys 55, 1–42 (2023). https://doi.org/10.1145/3583558\n10. Nguyen, T.T., Le Nguyen, T., Ifrim, G.: Robust explainer recommendation for time\nseries classification. Data Mining and Knowledge Discovery 38, 3372–3413 (2024).\nhttps://doi.org/10.1007/s10618-024-01045-8\n11. Rong, Y., Leemann, T., Nguyen, T.T., Fiedler, L., Qian, P., Unhelkar, V., Seidel, T.,\nKasneci, G., Kasneci, E.: Towards human-centered explainable ai: A survey of user\nstudies for model explanations. IEEE Transactions on Pattern Analysis and Machine\nIntelligence 46(4), 2104–2122 (2024). https://doi.org/10.1109/TPAMI.2023.3331846\n12. Samek, W., Binder, A., Montavon, G., Lapuschkin, S., Müller, K.R.: Evaluating\nthe Visualization of What a Deep Neural Network Has Learned. IEEE Transactions\non Neural Networks and Learning Systems 28, 2660–2673 (2017). https://doi.org/\n10.1109/TNNLS.2016.2599820\n13. Schlegel, U., Arnout, H., El-Assady, M., Oelke, D., Keim, D.A.: Towards a rigorous\nevaluation of XAI methods on time series. In: 2019 IEEE/CVF International\nConference on Computer Vision Workshop (ICCVW). pp. 4197–4201 (2019). https:\n//doi.org/10.1109/ICCVW.2019.00516\n14. Schlegel, U., Keim, D.A.: A Deep Dive into Perturbations as Evaluation Technique\nfor Time Series XAI. In: Longo, L. (ed.) Explainable Artificial Intelligence, vol. 1903,\npp. 165–180. Springer Nature Switzerland, Cham (2023). https://doi.org/10.1007/\n978-3-031-44070-0_9\n15. Schlegel, U., Keim, D.A.: Introducing the attribution stability indicator: A measure\nfor time series XAI attributions. In: ECML-PKDD Workshop XAI-TS: Explainable\nAI for Time Series: Advances and Applications (2023). https://doi.org/10.48550/\narXiv.2310.04178\n16. Schulz, K., Sixt, L., Tombari, F., Landgraf, T.: Restricting the flow: Information bot-\ntlenecks for attribution. In: International Conference on Learning Representations\n(2020), https://openreview.net/forum?id=S1xWh1rYwB\n17. Serramazza, D.I., Nguyen, T.L., Ifrim, G.: Improving the Evaluation and Action-\nability of Explanation Methods for Multivariate Time Series Classification. In: Bifet,\nA., Davis, J., Krilavičius, T., Kull, M., Ntoutsi, E., Žliobait˙e, I. (eds.) Machine\nLearning and Knowledge Discovery in Databases. Research Track. pp. 177–195\n(2024). https://doi.org/10.1007/978-3-031-70359-1_11\n18. Šimić, I., Sabol, V., Veas, E.: Perturbation effect: A metric to counter misleading\nvalidation of feature attribution. In: Proceedings of the 31st ACM International\nConference on Information & Knowledge Management. pp. 1798–1807 (2022).\nhttps://doi.org/10.1145/3511808.3557418\n19. Simonyan, K., Vedaldi, A., Zisserman, A.: Deep inside convolutional networks:\nVisualising image classification models and saliency maps. In: Proceedings of\nthe International Conference on Learning Representations (ICLR) (2014). https:\n//doi.org/10.48550/arXiv.1312.6034\n20. Smilkov, D., Thorat, N., Kim, B., Viégas, F., Wattenberg, M.: SmoothGrad: Re-\nmoving noise by adding noise (2017). https://doi.org/10.48550/arXiv.1706.03825\n21. Sundararajan, M., Taly, A., Yan, Q.: Axiomatic Attribution for Deep Networks.\nIn: Proceedings of the 34th International Conference on Machine Learning. pp.\n3319–3328 (2017), https://proceedings.mlr.press/v70/sundararajan17a.html\n22. Theissler, A., Spinnato, F., Schlegel, U., Guidotti, R.: Explainable AI for Time\nSeries Classification: A Review, Taxonomy and Research Directions. IEEE Access\n10, 100700–100724 (2022). https://doi.org/10.1109/ACCESS.2022.3207765\n\n22\nBaer et al.\n23. Turbé, H., Bjelogrlic, M., Lovis, C., Mengaldo, G.: Evaluation of post-hoc inter-\npretability methods in time-series classification. Nature Machine Intelligence 5,\n250–260 (2023). https://doi.org/10.1038/s42256-023-00620-w\n24. Wang, Z., Yan, W., Oates, T.: Time series classification from scratch with deep\nneural networks: A strong baseline. In: 2017 International Joint Conference on\nNeural Networks (IJCNN). pp. 1578–1585 (2017). https://doi.org/10.1109/IJCNN.\n2017.7966039\n",
  "metadata": {
    "source_path": "papers/arxiv/Class-Dependent_Perturbation_Effects_in_Evaluating_Time_Series\n__Attributions_f1fc63027d0d6d19.pdf",
    "content_hash": "f1fc63027d0d6d19e0f29844b73f6295801341a4895c255bf0a3b07d72b814d1",
    "arxiv_id": null,
    "title": "Class-Dependent_Perturbation_Effects_in_Evaluating_Time_Series\n__Attributions_f1fc63027d0d6d19",
    "author": "",
    "creation_date": "D:20250225023844Z",
    "published": "2025-02-25T02:38:44",
    "pages": 22,
    "size": 926663,
    "file_mtime": 1740470193.7396932
  }
}