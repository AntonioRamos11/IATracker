{
  "text": " \n \nExperimental validation of UAV search and detection system in real \nwilderness environment \nStella Dumenčića, Luka Lančaa, Karlo Jakaca and Stefan Ivića* \naFaculty of Engineering, University of Rijeka, Vukovarska 58, 51 000 Rijeka, Croatia \nCorrespondence: stefan.ivic@uniri.hr \nAbstract \nSearch and rescue (SAR) missions require reliable search methods to locate survivors, \nespecially in challenging or inaccessible environments. This is why introducing unmanned \naerial vehicles (UAVs) can be of great help to enhance the efficiency of SAR missions while \nsimultaneously increasing the safety of everyone involved in the mission. Motivated by this, we \ndesign and experiment with autonomous UAV search for humans in a Mediterranean karst \nenvironment. The UAVs are directed using Heat equation-driven area coverage (HEDAC) \nergodic control method according to known probability density and detection function. The \nimplemented sensing framework consists of a probabilistic search model, motion control \nsystem, and computer vision object detection. It enables calculation of the probability of the \ntarget being detected in the SAR mission, and this paper focuses on experimental validation of \nproposed probabilistic framework and UAV control. The uniform probability density to ensure \nthe even probability of finding the targets in the desired search area is achieved by assigning \nsuitably thought-out tasks to 78 volunteers. The detection model is based on YOLO and trained \nwith a previously collected ortho-photo image database. The experimental search is carefully \nplanned and conducted, while as many parameters as possible are recorded. The thorough \nanalysis consists of the motion control system, object detection, and the search validation. The \nassessment of the detection and search performance provides strong indication that the designed \ndetection model in the UAV control algorithm is aligned with real-world results. \nKeywords: UAV, search and rescue, experiment, YOLO, HEDAC \nSubject classification codes: include these here if the journal requires them \n \n\n \n \n1. Introduction \nUnmanned Aerial Vehicles (UAVs) have emerged as efficient tools in Search and \nRescue (SAR) missions due to their ability to rapidly access remote, often challenging \nor inaccessible areas enhancing the speed of locating individuals in distress, especially \nin situations involving natural hazards and risks. A critical aspect of this capability is \nperson detection from aerial imagery. However, obtaining images for research purposes \nfor this application is a challenging task since it requires access to real-world SAR \nscenarios, which have the focus set on the mission instead of collecting data for research \npurposes and often require complete anonymity. Experiments conducted in controlled \nand monitored conditions can therefore serve as a valuable alternative for generating \ndatasets and advancing research of SAR missions. Additionally, existing datasets and \nalgorithms often fall short in addressing the unique challenges posed by SAR scenarios, \nespecially when detecting small-sized objects, such as individuals captured from a top-\ndown perspective (Hong et al., 2021). Unlike typical object detection datasets, the visual \nrepresentation of people in such images deviates from conventional forms, emphasizing \nthe need for specialized datasets tailored to this task (Akshatha et al., 2023). \nBeyond computer vision detection, effective motion control is essential for \nUAVs to systematically and efficiently survey target areas. Ergodic search algorithms, \nsuch as the Heat equation-driven area coverage (HEDAC) method (Ivic, 2020) used in \nthis study, enhance the search performance by distributing the search efforts \nproportional to the likelihood of locating a target showing significant efficiency in SAR \nmissions. However, implementing such strategies in SAR mission environments \nintroduces challenges, including obstacle avoidance, real-time communication, and \ncoordinating multiple UAVs. This is why the robustness of the motion control system is \nof utmost importance. The UAV search framework (Lanča et al., 2024) used in this \nstudy, utilizes the ergodic HEDAC motion control system in combination with Model \nPredictive Control (MPC) to efficiently search a large area while performing aerial \nimagery. The underlying sensing model is based on the performance of the used \nYOLOv8 object detection model, meaning the UAV's motion is influenced by both the \ntarget probability density function and the detection model. As UAVs adjust their flight \nheight while searching complex terrains, the performance of YOLOv8 varies based on \nthe flight height impacting the ground sampling distance (GSD) (Petso et al., 2021), \n(Qingqing et al., 2020). Since existing person detection models often do not provide \nperformance metrics across extensive flight height and GSD ranges, the existing \npretrained model was additionally trained on our initial experiment data to fill this gap.  \nDespite recent advancements, SAR applications still face challenges in both, \ncomputer vision object detection and UAV motion control. Detection algorithms must \ncontend with varying environmental conditions, occlusions, and the inherently low \nresolution of humans in aerial imagery. Meanwhile, motion control demands adaptive \nstrategies capable of balancing efficiency and reliability in high-stakes operations. \n\n \n \nMotivated by these challenges, we conducted a simulated SAR scenario in controlled \nand monitored conditions on the Učka mountain in Croatia in 2024 resulting in the \nsearch and motion control experiment validation, as well as a dataset designed for \ndetecting individuals in SAR scenarios. This dataset offers different image contexts, \nscales, and orientations reflective of real-world conditions enhancing the robustness and \napplicability of detection algorithms that will be valuable for future SAR research.  \nThe paper is structured as follows: The Introduction 1 presents the problem and \nmotivation of using object detection in combination with UAV motion control. In \nsection 2 the related work is presented. In section 3 the motion control algorithm and the \ndetection methodology are described. In section 4 the experiment setup is described. \nThe results are shown in section 5. In 6 the presented methodology and its usage is \ndiscussed. The paper is concluded in section 7. \n \n2. Literature overview \nIn the following section the utilization of UAVs in SAR missions is explored focusing \non the ergodic motion control and other strategies for efficient search area coverage, as \nwell as the usage of object detection models to help detect individuals in distress.  \n2.1 UAV in SAR missions \nA detailed survey on the usage of UAVs in SAR missions is presented in  (Lyu et al., \n2023) giving an overview of different types of UAVs that can be used in SAR missions, \nas well as different operational scenarios of the UAVs in times of disasters. The \nadvantages that UAVs offer, such as accessing inaccessible and often dangerous areas \ninclude improved safety for human resources, cost-effective operations, and faster data \ncollection including the ability to gather high-resolution imagery or data used for \nresearch and monitoring. Equipped with advanced sensors, such as cameras including \nthermal cameras, multispectral cameras, and light detection and ranging (LiDAR), \nUAVs can be used to detect human body heat, identify structural damages, and map \ncomplex terrains. This is extremely important in situations of natural hazards and risk \nsuch \nas \navalanches \n(Silvagni \net \nal., \n2017), \n(Bejiga \net \nal., \n2017), \nalbrigtsen2016application} or earthquakes (Qi et al., 2016), (Calamoneri et al., 2022), \n(Nedjati et al., 2016). Additionally, UAVs are increasingly being integrated with \ncommunication systems and payload delivery mechanisms to expand their functional \nroles in SAR missions (Doherty & Rudol, 2007). This can include delivering critical \nsupplies, such as medical kits, food, and water, to individuals in inaccessible locations. \nUAVs can also act as airborne relay stations as detailed in (WU et al., 2019). This \n\n \n \nmethod can be used to establish communication links in areas where conventional \nnetworks are disrupted, ensuring coordination among rescue teams.  \nThe effectiveness of UAVs in SAR missions is further enhanced by \nadvancements in motion control and object detection technologies, which play a crucial \nrole in enabling efficient search missions while navigating complex environments and \nidentifying targets. Motion control systems enable UAVs to maintain stability and \nmanoeuvrability in challenging conditions, such as strong winds or obstructed terrains, \nensuring reliable performance during missions, as well as effective path planning to \nsearch the target area effectively. Similarly, object detection algorithms allow UAVs to \nidentify search targets, monitor hazards, and detect critical infrastructure, facilitating \ndecision-making processes. This can be done either on-board the UAV or offline on a \nground-based workstation. On-board processing enables real-time detection, providing \nimmediate results but demanding significant computational resources, which reduces \nthe battery life and limits the UAV’s operational duration. On the other hand, offline \nprocessing involves analyzing captured images on a dedicated workstation with mostly \nbetter computing power, allowing for faster and more efficient processing while \nconserving UAV battery life, thereby extending the overall search duration. \n2.2 UAV search and ergodic motion control \nThe ability to effectively control the motion of UAVs is crucial in a variety of \napplications, especially in situations that depend on the control efficiency such as SAR \nmissions. In these operations, UAVs can be deployed either independently or in \ncoordination with ground search teams to increase the search efforts as discussed in \n(Goodrich et al., 2008). Additionally, various search strategies have been explored to \noptimize UAV motion control such as the methods presented in (Lin & Goodrich, 2009) \nthat use straight paths in combination with 90° turns to enhance the coverage in SAR \nscenarios.  \nErgodic motion control has emerged as a promising solution due to its capability \nto efficiently guide the UAVs over a defined area. By leveraging the principles of \nergodicity, this approach ensures that the spatial distribution of the UAV's trajectory \naligns with the probability distribution of the target's presence, in particular, areas \nwithin the search domain. The benefits of using ergodic search are presented in (Miller \net al., 2015) suggesting the robustness of the method in different conditions and \nuncertainties. This has led to multiple ergodic motion control systems being developed. \nThe three widely recognized approaches for controlling single or multi-agent systems in \nergodic exploration are HEDAC, MPC, and Spectral Multiscale Coverage (SMC).  \nThe HEDAC method (Ivic et al., 2016) is based on the heat equation used to \ncreate a potential field enabling efficient directing of either one or multiple agents. The \nHEDAC method was later improved by incorporating agent sensing and detection (Ivic, \n\n \n \n2020). In (Ivić et al., 2022), the Finite Element Method (FEM) was employed to solve \nthe fundamental heat equation, enhancing its ability to handle irregularly shaped \ndomains and inter-domain obstacles without increasing computational resource needs. \nMPC, also known as Receding Horizon Control (RHC), is employed to generate \ntrajectories by optimizing a specific objective within defined constraints. In (Bircher et \nal., 2018), (Mavrommati et al., 2017), the MPC approach was applied to path planning \nin a 3D search space for both known and unknown environments, demonstrating strong \nscalability in the experimental validation. \nSMC, introduced in (Mathew & Mezić, 2011), leverages the difference between \nthe desired and actual trajectories to create multi-agent paths. In (Hubenko et al., 2011), \nthe Neyman-Pearson lemma was incorporated into this method for a 2D coverage task, \nleading to the development of Multiscale Adaptive Search (MAS), which was \nexperimentally tested with a single UAV in (Mathew et al., 2013), . \nThis study presents a comprehensive experimental validation presented in \n(Lanča et al., 2024) using the HEDAC algorithm for coverage control and potential field \ngeneration, integrating it with MPC to enhance the motion control by optimizing flight \nheight enabling the control strategy to improve overall system performance and flight \nefficiency in uneven environments.  \n2.3 UAV images object detection \nEven though recent advancements in computer vision algorithms have proven \nhighly beneficial in many fields, especially when large datasets are available for training \nand testing, the availability of large, annotated datasets for SAR-specific applications \nremains limited, hindering the development of more robust automated detection \nsystems. Some examples of existing datasets include (Akshatha et al., 2023), (Zhu et al., \n2021), (Barekatain et al., 2017). However, the person detection from aerial images has \nsome specific challenges such as the top-down perspective of person objects resulting in \ndifferent characteristics that the object detection model should recognize. The image \nquality can depend on the UAV velocity, especially in SAR missions where the trade-\noff between the mission speed and image quality needs to be considered. Additionally, \nthe person objects in the image are already small-scaled, but the convolutional neural \nnetwork (CNN) downsampling is reducing the feature representations even more \nresulting in a lack of context information. These challenges could be tackled by \nextending the existing number of publicly available UAV image datasets enabling the \nmodels to learn from more images containing even more different image contexts. \nTo effectively utilize these datasets, efficient computer vision techniques are \nrequired to detect and localize objects in UAV imagery despite their small size and \ncomplex backgrounds. Object detection plays a crucial role in this process, as it \n\n \n \ninvolves both identifying objects and determining their precise locations within an \nimage. One of the most popular methods to solve this task is the You Only Look Once \nalgorithm (YOLO) which is a one-stage detector dividing the image into a grid and \npredicting bounding boxes and their class probabilities enabling simultaneous \nestimation of localization and classification. The version used in this study is the \nYOLOv8 (Jocher et al., 2023) created based on incremental improvements of the earlier \nYOLO versions presented in (P. Jiang et al., 2022), (Terven et al., 2023), (Hussain, \n2023). \nThe usage of YOLO for object detection in nature environments has shown \npromising results in applications such as wildlife monitoring (Gonzalez et al., 2016) and \nagricultural inspection (Messina & Modica, 2020). The application of YOLO person \ndetection on thermal images has also been widely researched (Kristo et al., 2020), (C. \nJiang et al., 2020), (Kannadaguli, 2020), (Levin et al., 2016), (Teutsch et al., 2014), \n(Giitsidis et al., 2015), (Yeom, 2021). Additionally, in (Yeom, 2024) the tracking of \npeople using thermal images in simulated SAR situations is shown using YOLOv5. \nHowever, detecting small objects of interest, such as people in large-scale images, is \nchallenging due to their small scale (Hong et al., 2021). \n \n3. UAV Motion control and machine vision detection \nThe successful usage of autonomous UAVs in SAR missions depends on several factors \nsuch as the implemented motion control and detection. In this section, the used \nmethodology in terms of the probabilistic model of the search, the UAV motion control \nusing HEDAC and MPC, and the YOLO object detection model are described. \n3.1 Probabilistic model of the search \nThe main objective of the conducted search is to validate the search success. To achieve \nthis, the first step is to define the UAV’s field of view (FOV) as well as the terrain \nmodel needed to determine the UAV’s sensing. Since the experiment search is \nconducted in the mountain area, the terrain is uneven, hence the sensing may not capture \nthe whole FOV that would be visible on even terrain. This is why the terrain data as part \nof the geographic information system (GIS) needs to be introduced. The terrain data was \nobtained by digital elevation model (DEM) files from the Copernicus database \n(European Union space programme, 2024). The DEM data was integrated to provide the \ninformation needed for calculating relative heights in the motion control system, as well \nas the possible obstacles impacting the sensing. The relative flight height, defined as the \nheight of the UAV above the ground, is calculated using the starting point of all flights, \nnamely 45.2368° latitude and 14.2031° longitude, and the DEM data.  This calculated \n\n \n \nheight is used to enable the flight height optimization and defining the no-fly safety \nzone. In Figure 1 it can be seen how the terrain can impact the UAV’s FOV. \n \nFigure 1. The FOV of a single UAV is represented by a semi-transparent pyramid. The \ndetection probability in the UAV’s FOV is shown using a gradient showing higher \ndetection probability for points closer to the UAV’s orthogonal view according to the \nsensing model, while the undetectable points are shown in green color. \n \nTo check if the defined point 𝑝 can be sensed by the camera, the point \ncoordinates need to be transformed to local coordinates in relation to the UAV’s \ncoordinate system. By transforming the coordinates, the original x and y coordinates are \nused, while the z coordinate first needs to be calculated based on the UAV flight height \nand the terrain at the point 𝑍𝑇(𝑥, 𝑦).. Based on if the point is in the FOV, the detection \nprobability 𝜓 is defined as: \n \n     𝜓(𝑅) = {𝛤(∨𝑅∨),∧𝑖𝑓𝑅∈𝛺𝐹𝑂𝑉\n0, 𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒\n , \n(1) \nwhere 𝑅 is a 3D defined point relative to the UAV camera, 𝛤 is used to define the \ndetection probability. For each point that is in the FOV, the detection probability is \ncalculated by 𝛤, while points outside of the FOV have a 0 probability of detecting the \ntargets. \n \nDuring the whole duration of the flight, the coverage 𝑐 is calculated as the \naccumulated detection probability for all points in the domain visible from the camera’s \nposition 𝑋. This is accumulated to calculate the search coverage in space and time: \n \n     𝑐(𝑝, 𝑡) = ∫𝜓(𝑅(𝑋(𝑡), 𝑝))𝑑𝑡\n𝑡\n0\n , \n(2) \n\n \n \nThe probability of undetected target presence 𝑚 is initially described by the \nprobability distribution 𝑚0 at 𝑡= 0. Over time, 𝑚 decreases as the agents apply their \nsensing effects, which are characterized by the coverage 𝑐. It is calculated as follows: \n \n𝑚(𝑝, 𝑡) = 𝑚0(𝑝) ⋅𝑒−𝑐(𝑝,𝑡) . \n(3) \n \nTo calculate the overall detection probability 𝜂, the undetected targerget is integrated \nover the domain: \n \n𝜂(𝑡) = 1 −∫\n𝑚(𝑝, 𝑡)𝑑𝑝\n𝛺2𝐷\n . \n(4) \nThe detection probability is the key factor analyzed in this study since it is a measure of \nthe search effectiveness.  \n3.2 UAV motion control \nThe motion control system implementation used in the main experiment, was taken \nfrom (Lanča et al., 2024) and consists of the HEDAC algorithm for defining the motion \ncontrol in 2D space and MPC for optimizing the flight regime in 3D space adding the \nheight as an additional control variable, as well as the UAV velocity. Although the \nproposed motion control framework is designed to handle multiple UAVs, all search \nmissions were conducted as single-agent searches. \n \nThe motion control consists of three control variables set by the motion control \nalgorithm, namely the velocity intensity 𝜌(𝑡), the incline angle 𝜑(𝑡), and the yaw \nangular velocity 𝜔(𝑡). Using the velocity intensity and the incline angle, the horizontal \nand vertical velocities are calculated. In addition, 𝜔 regulates the UAV direction in \nwhich the horizontal velocity acts.  By this, the UAV state is defined using three \ncoordinates, namely the 𝑥, 𝑦, and 𝑧 coordinates, as well as one orientation state. \nThe horizontal search control is defined by the potential field 𝑢(𝑝, 𝑡) of the \nsearch area. The potential field is guiding the UAV towards the areas that have the \nhighest probability of containing undetected targets. It is calculated by solving the \ndifferential equation as follows: \n \n𝛼⋅𝛥𝑢(𝑝, 𝑡) = 𝛽⋅𝑢(𝑝, 𝑡) −𝑚(𝑝, 𝑡) , \n(5) \n \n\n \n \nwhere 𝛼 and 𝛽 are HEDAC parameters used to modify the search behavior by adjusting \nthe smoothness and stability, and 𝛥 is the laplace operator. Additionally, the following \ncondition has to be met: \n \n𝜕𝑢\n𝜕𝑛= 0 . \n(6) \nwhere 𝑛 represents the normal outward to the search domain boundary defined by \n𝜕𝛺2𝐷. Based on the gradient of the potential field, the direction of the UAV needs to be \nadjusted. This is calculated for each control step steering on the current direction \ntowards the wanted direction defined by the gradient. Additionally, the UAV ‘s \nmaximum angular velocity is defined by the maximal turning velocity or equivalently \nthe minimum turning radius. \nTo control the UAV’s flight height and velocity, MPC is introduced to optimize \ntwo objectives, namely maximizing the UAV velocity, while keeping the flight height \nas close to the height goal defined for each flight. The first constraint that needs to be \nsatisfied for the optimization to be feasible, is the need to fly above the minimum height \nrepresenting the no-fly zone set at 35 meters above the terrain obtained by the terrain \nmodel. The no-fly zone height takes into account the tree height, possible uncertainties \ncontained in the DEM data, as well as an additional safety factor to minimize the risk of \nany collision with possible obstacles. Additional constraints that need to be met are the \nminimum and maximum velocities defined by the UAV specifications, as well as \nminimum and maximum accelerations. \n3.3 Computer vision system for human detection \nTo collect all the necessary data and to test out the motion control and vision detection \nsystems needed for the success of the main experiment, an initial experiment with 28 \nparticipants was conducted on the mountain Učka on 07.07.2024. \nThe initial experiment dataset was obtained by manually operated flights using \nDJI Matrice 210 and DJI M30T UAVs, where the obtained images have a resolution of \n2970 × 5280 pixels for the DJI Matrice 210 and  a resolution of 3000 × 4000 pixels for \nthe DJI M30T. This is the initial dataset used to train the YOLO object detection model \nused later in the main experiment. All captured persons are manually labeled in all \nimages in the initial dataset. \nThe initial dataset consists of images in combination with the corresponding \nlabels in the YOLOv8 format representing detected individuals. The images are stored \nin JPG format and include metadata such as Global Positioning System (GPS) \ncoordinates, providing valuable context for analysis. The image preprocessing includes \n\n \n \ntiling the original image into smaller parts to ensure easier YOLO training and \nmodifying the existing labels to fit the new small-sized images. \nImage labelling \nThe Computer Vision Annotation Tool (CVAT) in a local environment was used for \nlabeling. Three independent annotators manually labeled the original-sized images \nidentifying individuals. After the initial labeling, two independent reviewers, who were \nnot involved in the labeling process, reviewed the annotations for accuracy and \nconsistency. The images were labeled in an iterative process where labels were \ncorrected to increase the accuracy. The used label format is YOLO, specifically, it was \ndownloaded as the YOLOv8 Detection label format available in CVAT. \nImage tiling \nThe process of dividing the original UAV images into smaller tiles was performed using \na custom Python script. Each high-resolution image was split into 512 × 512 pixel \nsections, ensuring an overlap between the tiles to maintain comprehensive coverage and \nprovide additional context for better analysis. The minimal overlap is experimentally \ndefined as 100 px. The tiling method is shown in Figure 2.  \nThis method enables the use of smaller square image segments preferred by the \nYOLO algorithm training. The newly created file names were generated based on the \ntile's position within the original image. This naming convention helps to ensure that \neach tile can be easily traced back to its location within the larger image, providing a \nstructured approach to organizing the dataset for further processing and analysis. Lastly, \nthe existing labels needed to be modified to fit the newly created images. \n \nFigure 2 The tiling method used to divide original sized images into images of 512 × \n512 pixels. The overlap ensures more context being available in the newly created \ndataset. \n\n \n \nGround sampling distance \nSince different cameras were used, the images were divided into GSD groups to \nenable the comparison between different flight height conditions. Essentially, GSD is \nthe actual distance in the UAV image represented by 1 px defining how much detail is \ncaptured in the image. A lower GSD means that each pixel in the UAV image is \nrepresenting a smaller ground area enabling the image to show more detail, while in \ncontrast, a higher GSD is representing a bigger area, thus having less details. Using the \nhorizontal ℎ𝐺𝑆𝐷 and vertical 𝑣𝐺𝑆𝐷 distance of the UAV’s camera FOV, the GSD in the \nhorizontal and vertical directions can be calculated using the relative UAV height ℎ of \neach image as follows: \n \nℎ𝐺𝑆𝐷= 100 ⋅\n2⋅ℎ⋅𝑡𝑎𝑛ℎ𝐹𝑂𝑉\n2\n𝑥𝑖𝑚𝑎𝑔𝑒\n , \n(7) \n \n𝑣𝐺𝑆𝐷= 100 ⋅\n2⋅ℎ⋅𝑡𝑎𝑛𝑣𝐹𝑂𝑉\n2\n𝑦𝑖𝑚𝑎𝑔𝑒\n. \n(8) \nBecause the horizontal and vertical distance of the FOV are calculated from the \naspect ratio and diagonal FOV, the vertical and horizontal GSD are the same. The GSD \nimage groups enabled us to compare the model at different GSD intervals. The recall \nmetric of the initial model for each GSD group is used for the motion control system. \nThe distribution of images in height and GSD groups is shown in Figure 3.  \n \nFigure 3. Image distribution based on the height and GSD. The UAV camera height is \ndirectly impacting the GSD value. \n \n \n\n \n \nObject detection \nThe used model is YOLOv8 released in 2023 by Ultralytics and is the result of \nincremental improvements implemented on previous versions (YOLOv5, YOLOv6, \nYOLOv7, ...). The name YOLO comes from the simultaneous estimation of localization \nand classification that is done in one look of the images. The simplified scheme of the \nYOLOv8 architecture is shown in Figure 4 and consists of four main blocks: the input \ndata, the backbone, the neck, and the head. \n \nFigure 4. Simplified YOLO architecture. The input data is sent to the backbone which \nextracts image features by propagating it through multiple layers. The neck is enhancing \nthe created feature maps by using different scales. The head is predicting bounding \nboxes. \nThe input data is the data provided to the model for training. The used images \nhave a resolution of 512 × 512 pixels. Different augmentation methods were used to \nintroduce new context improving the generalizability of the model. The augmentation \nmethods included in the training process were horizontal flip, vertical flip, rotation, hue, \nhsv, translate, scale, mosaic, erasing, and crop fraction. Most of these methods are set as \ndefault augmentation methods. The used model is the pre-trained YOLOv8 trained on \nthe COCO dataset. \nThe backbone network of the neural network is used to extract the features from \nthe images. Based on the extracted features, object classification and localization are \nperformed. The feature extraction is done in several layers. In our research, the \noriginally proposed custom CSPDarknet53 is used. \nIn the neck block, the extracted features are aggregated to form new features \nfrom different layers of the backbone network. To aggregate features, the original \nPANet was used. \n\n \n \nThe head is used for suggesting anchors bounding boxes of the detected objects, \nin our case persons. Additionally, the head is used to estimate the percentage of \ncertainty for detected objects. The used model head is the one presented in the original \nmodel, namely the YOLOv8 head.  \nInitial detection model performance \nThe sensing function is based on the YOLOv8 metrics, specifically the recall metric \nrepresenting the percentage of correctly identified objects in relation to the total number \nof actual objects in the dataset reflecting the model's effectiveness in detecting all \ninstances of a specific class. The recall used in the main experiment was obtained from \nthe initial experiment. \nSince the initial experiment flights were operated manually, there was no \noptimization of the flight height resulting in more GSD groups than in the optimized \nautonomous flight regime in the main experiment. The recall metric obtained by the \nvalidation on the initial dataset resulted in the recall metrics for each GSD shown in \nTable 1. It can be seen that the recall is generally getting lower for higher GSD \nintervals. \n \nTable 1. Recall for each GSD group in the initial experiment. \nGSD \nRecall \n0.5 - 1.0 \n0.95 \n1.0 - 1.5 \n0.977 \n1.5 - 2.0 \n0.956 \n2.0 - 2.5 \n0.953 \n2.5 - 3.0 \n0.897 \n3.0 - 3.5 \n0.881 \n3.5 - 4.0 \n0.781 \n4.0 - 4.5 \n0.796 \n4.5 - 5.0 \n0.719 \n5.0 - 5.5 \n0.699 \n5.5 - 6.0 \n0.621 \n6.0 - 6.5 \n0.142 \n \n\n \n \n4. Experiment setup \nThe motivation of the experiment can be divided into two main goals, namely (1) \nadditional experimental validation of the autonomous motion control system using \nHEDAC and MPC and (2) creating a dataset containing people in different natural \nenvironments used in future SAR research. The experiment was conducted on the Učka \nmountain, Croatia on 27.10.2024. with 84 volunteers including the organizers and \nconsists of a treasure hunt enabling the wanted motion behavior of the participants. \n4.1 Location, environment and equipment \nUčka mountain Nature Park in Croatia presents a complex and challenging environment \nwell-suited for the evaluation of simulated UAV-based SAR operations. The area is \ncharacterized by uneven terrain, different low vegetation, and elevation variations, \nmaking it an ideal setting for assessing the capabilities of autonomous UAV systems in \nlocating missing persons in real-world conditions. \nIn this experiment, two UAVs were used: DJI Matrice 210 v2 and DJI Mavic 2 \nEnterprise Dual. The UAVs characteristics are shown in Table 2. The specifications \nshow the 𝜑 parameter of optimization used for defining the UAV movement in relation \nto the horizontal plane, minimum and maximum horizontal and vertical velocity, \nminimum and maximum horizontal acceleration, maximum angular velocity, and the set \nMPC time steps. In Table 3 the camera specifications of three used cameras are shown.  \nTable 2. UAV specifications. \nUAV \nFull name \nUnit \nMatrice 210 v2 \nMavic 2 \nEnterprise Dual \n𝜑𝑚𝑖𝑛 \nMinimum incline \nangle \n° \n-90 \n-90 \n𝜑𝑚𝑎𝑥 \nMaximum incline \nangle \n° \n90 \n90 \n𝑣ℎ,𝑚𝑖𝑛 \nMinimum \nhorizontal \nvelocity \nm/s \n0 \n0 \n𝑣ℎ,𝑚𝑎𝑥 \nMaximum \nhorizontal \nvelocity \nm/s \n10 \n8 \n𝑣𝑣,𝑚𝑖𝑛 \nMinimum vertical \nvelocity \nm/s \n-3 \n-2 \n𝑣𝑣,𝑚𝑎𝑥 \nMaximum vertical \nvelocity \nm/s \n5 \n3 \n\n \n \n𝑎ℎ,𝑚𝑖𝑛 \nMinimum \nhorizontal \nacceleration \nm/s2 \n-3.6 \n-3.6 \n𝑎ℎ,𝑚𝑎𝑥 \nMaximum \nhorizontal \nacceleration \nm/s2 \n2 \n2 \n𝑎𝑣,𝑚𝑖𝑛 \nMinimum vertical \nacceleration \nm/s2 \n-2 \n-2 \n𝑎𝑣,𝑚𝑎𝑥 \nMaximal vertical \nacceleration \nm/s2 \n2.8 \n2.8 \n𝜔𝑚𝑎𝑥 \nMaximal angular \nvelocity \n°/s \n120 \n30 \nN (T) \nMPC horizon \ntimesteps \n(duration) \n- \n(s) \n5 (15) \n5 (15) \nTable 3. Camera specifications. \nCamera \nFOV c1 [°] \nFOV c2 [°] \nResolution [px] \nDJI Zenmuse X5S \n39.2 \n64.7 \n5280 × 2970 \nDJI Zenmuse Z30 \n33.9 \n56.9 \n1920 × 1080 \nMavic 2 Enterprise \nDual built-in camera \n57.58 \n72.5 \n4056 × 3040 \nThe final experiment setup is shown in Table 4. Flights 1, 2, and 3 are connected \nand represent a single search mission, while flights 4 and 5 each represent their own \nsearch mission resulting in a total of three search missions. All five flights were \noperated autonomously.  \nTable 4. Experiment setup settings \nFlight \n1 \n2 \n3 \n4 \n5 \nSearch \nmission \nMission 1 \nMission 2 \nMission 3 \nUAV \nM210 \nM210 \nM210 \nMavic \nM210 \nCamera \nX5S \nX5S \nZ30 \nMavic built-in \ncamera \nX5S \nMin/goal \naltitude [m] \n35/55 \n55/75 \n35/75 \n35/55 \n35/55 \nZone \nA, B, C \nA, B, C \nA, B, C \nB, C \nA \nStart time \n11:15 \n11:44 \n12:13 \n12:40 \n13:02 \nEnd time \n11:38 \n12:07 \n12:35 \n12:55 \n13:27 \n\n \n \n4.2 Design and preparation of the experiment \nEnsuring an even probability of human targets within the defined search zone involved \nthe strategic placement of markers that needed to be found. The experiment consisted of \n150 markers where 50 of them were placed in each of the three zones as shown in \nFigure 5 to ensure the dispersion of individuals in the search area, thereby simulating a \nuniform distribution of targets in the search domain. The specification of each zone is \nshown in Table 5. The search domain is defined using the zones and consists of either \none or more zones with an offset between 50 and 100 m allowing UAVs to avoid \ntouching the boundary of the domain. The target probability distribution function was \nuniform for each zone. Additionally, the sum of undetected target probability \nthroughout the entire search domain is 1. The uniform probability inside each zone is \ndetermined by the number of people searching in that zone divided by the area of that \nzone.  \n \nFigure 5. This figure shows three zones and their corresponding markers. Each zone \ncontains 50 markers for the treasure hunt with unique names to identify them. \n\n \n \nTable 5. Specification of each zone. \nZone \nArea [𝑚2] \nNum. markers \nNum. people \nA \n432 734 \n50 \n25 \nB \n470 233 \n50 \n27 \nC \n613 709 \n50 \n26 \nAll participants were informed about the experiment setup and motivation. Each \nindividual involved in the experiment provided signed consent to be photographed, with \nall data de-identified to protect personal information such as names. Additionally, each \nparticipant got an information flier containing all necessary information: a map with the \npath to the starting point, a map with defined zones, additional information, QR codes \nthat led to the starting point, as well as QR codes that showed the position of the \nparticipant inside the zone. The available GPS data allowed the participants to track \ntheir location at all times during the experiment to ensure they remain in the assigned \nzone. Each participant had to fill in the name and surname, jacket color or multiple \ncolors if they have taken off the jacket, the time at the starting point, start and end of the \nsearch inside the zone, and if they found markers, each marker should have been noted \nusing the marker number as well as the time when the marker was found. The English \nversion of the flier is shown in Figure 6. \n \nFigure 6. Information flier containing all important information for the participants as \nwell as the log section. The original version of the flier is in Croatian, but for the \npurpose of showing this flier, it has been translated into English. \n\n \n \n4.3 Conducting the experiment \nConducting field experiments requires detailed planning, coordination, and adaptability \nespecially since real-world experiments introduce numerous challenges, including \nlogistical constraints, regulatory requirements, and unpredictable environmental factors.  \nOne of these challenges is the unpredictable weather making long-term planning \nfor UAV-based SAR experiments challenging. While weather forecasts are monitored, \nsudden changes such as fog, wind, or rain can still occur, affecting UAV stability and \nvisibility. Despite this uncertainty, extensive logistical work must be completed in \nadvance, including inviting participants, coordinating with the nature park, obtaining \nflight and imaging permissions, and securing signed consent from all participants \ninvolved in the experiment. These preparations ensure regulatory compliance, \noperational feasibility, and safety. Even though the weather forecast seemed promising, \nthe weather on the experiment day was cloudy and foggy. \nManaging a relatively large number of participants also resulted in challenges. \nSome participants forgot to enter the required log data. Additionally, depending on the \nlocation inside the zones, internet connectivity issues prevented some of the participants \nfrom verifying their locations based on the GPS coordinates and maps provided in the \nflier, disrupting real-time decision-making resulting in some individuals straying \noutside of their assigned zones.  \nSome of the images taken on 27.10.2024. during the experiment are shown in \nFigure 7. Subfigures (a) and (b) show the used drones, namely Matrice 210 v2 and \nMavic 2 Enterprise Dual. In (c) the home point of all flights is shown. Subfigure (d) \nshows the participants while giving them the introduction and explaining the \nexperiment. In Subfigures (e) and (f) an example of a UAV image from the first mission \nis shown, as well as a tile of the image containing a person. \n\n \n \n \nFigure 7. Scenes from conducting the experiment. Subfigures (a) and (b) show the used \nUAVs. Subfigure (c) shows the preparation to begin the mission at the home point of \neach flight. In (d) the participants are shown while the introduction speech has been \ngiven. Subfigures (e) and (f) show examples of a UAV image taken in the Mission 1 \nwith one tile of the image. \n \n \n\n \n \n5. Results \nThe following section presents the results of the conducted experiment consisting of the \nanalysis of the UAV motion control, the performance of the computer vision-based \nhuman detection, and the validation of the search and detection process. \n5.1 Analysis of UAV motion control \nAs mentioned in the experiment setup section, the area containing markers where \npeople were expected to stay was divided into three zones. However, to capture the \nwhole zone, the UAV flight zone was larger than the defined search zones. The flight \ntrajectories of all flights during the three search missions are shown in Figure 8. \n \n\n \n \nFigure 8. All flight trajectories of all search missions. (a) is showing Search mission 1 \nconsisting of flights 1, 2, and 3. (b) is showing Search mission 2. (c) is showing Search \nmission 3. (d) is showing all search missions. \nThe resulting flight velocity, acceleration and height of the first flight in Search mission \n1 is shown in Figure 9. It can be seen that the UAV’s velocity and acceleration are \ninside the constraints defined in the MPC optimization suggesting a stable flight. The \nflight height is following the goal height set to 55 meters in a smoothed line allowing \nthe UAV to optimize the velocity.  \n \nFigure 9. The first flight of Search mission 1 with a MPC horizon length of 15 seconds \nduring a 1400 seconds flight. The UAV’s velocity and acceleration is inside the set \nconstraints. The goal flight height is set to 55 meters with the UAV maximizing the \nflight velocity, while minimizing the flight height resulting in a smoother line. \n5.2 Computer vision human detection \nThe images resulted in most of them having no people. The number of images and \nnumber of labels in each flight is shown in Figure 10. The flight 3 which has been used \nto search all three zones has resulted in having the most images containing people \nmeaning it has also the highest number of labels, averaging on two persons per image \ncontaining people. Following flight 3, flight 1 has the most images containing people, \nbut flight 2 has more detected people.  \n \n\n \n \n \nFigure 10. Number of images with labels and number of labels in all UAV flights. It can \nbe seen that the Flight 3 contains most images as well as labels. Flight 1 contains more \nimages with labels than Flight 2, but less labels. Even though Flight 5 consisted of only \none zone, it has the lowest number of images containing people and labels. \nThe images with people were taken from the locations shown in Figure 11. Since \nthe UAV flight zone is larger than the defined zones containing markers, the UAVs \ncaptured images of people that mistakenly went outside of the zone. It can be seen that \nin zones A and B if people went out of their zones, it is still near the zone border, while \nin zone C people went further outside. Most grouped images are taken at the UAV flight \nstation as expected, since there were two flight operators at all times that are in the \nstarting and ending images of each flight and many volunteers decided to visit the \noperators at the highest point of the area. \n\n \n \n \nFigure 11. All images contain people and their locations. The UAV flight zone is larger \nthan the defined zones containing markers. Most people were detected in the location of \nthe starting point. This is expected since two UAV flight operators were in this location \nat all times and took images at the UAV flight start and end of each flight. \nDetection model validation \nThe number of images for each GSD interval in each Search mission is shown in Figure \n12. It is important to note that this Figure shows the GSD of all taken images. However, \nfor validation, only original sized images containing people were tiled into subparts of \n512 × 512 pixels since most tiles do not contain people and would still be enough to \nrepresent images with no people. \n\n \n \n \nFigure 12. Number of images in each GSD interval for each Mission. Mission 1 was the \nlongest one and consisted of three flights resulting in the largest amount of images for \nmost GSD intervals. It is important to note that this represents all images, meaning that \nnot only images with labels are considered, but also images without them. \nThe newly obtained dataset used an optimized flight regime implementation, \nhence having less GSD values than in the initial dataset. However, since the data in the \ninitial dataset was taken using only one of UAVs used in this experiment, contained less \npeople, namely about 28, and mostly having them grouped on the same walking path, \ntherefore containing less different image contexts than in this presented experiment \ndataset, the recall values are lower than in the initial dataset. The resulting recall metrics \nfor each GSD is shown in Table 6.  \nTable 6. Recall for each GSD group in all search missions. \nGSD \nSearch mission 1 \nSearch mission 2 \nSearch mission 3 \n1.0 - 1.5 \n1 \n- \n1 \n1.5 - 2.0 \n0.71 \n0.51 \n0.67 \n2.0 - 2.5 \n0.68 \n0.37 \n0.38 \n2.5 - 3.0 \n0.8 \n- \n- \n3.0 - 3.5 \n0.37 \n- \n- \nThe recall in each GSD interval in comparison to the initial experiment is shown \nin Figure 13. As mentioned earlier, since the initial experiment flights were operated \nmanually, the flight height has not been optimized but purposely designed to gather \nimages from a broader altitude range, resulting in more GSD ranges. On the other hand, \nin the main experiment, the flights were operated autonomously resulting in a stable \nflight height. Additionally, in the initial experiment flights, two cameras were used of \n\n \n \nwhich only one is used in this experiment causing new context in the ML model \nvalidation. However, it can be seen that the recall generally decreases with increasing \nthe GSD following the trend of the initial experiment. \n \nFigure 13. Recall of images created as tiles of images containing people. The initial \nexperiment was operated manually with no height optimization resulting in more GSD \nintervals than the autonomously operated missions in the search experiment. \nNevertheless, it can be seen that the missions’ recall follows the general trend of the \nrecall declining with higher GSDs. \n5.3 Validation of the search and detection \nTo assess the motion control sensing predicted search accomplishment based on the \ninitial experiment recall, we have compared it to the YOLO recall obtained based on the \nmain experiment images and the first detection of the detected individuals. In image 14 \nthe comparison of the search accomplishment and the YOLO detection rate for the first \nflight of the first search mission is shown. The predicted search accomplishment and the \nYOLO detection rate follow the same expected trend of detecting more people through \ntime having a similar increase in detection.  \n \nUnfortunately, due to the increasing goal height and the camera resolution, it \nwas not possible to identify each individual in the second, third, and fourth flight to \nassess the success in the same way. The fifth flight doesn’t have enough labels to \nconsider the result reliable so it has been discarded for the same analysis. However, \neven though the individuals were not identified, the YOLO success can be evaluated \nbased on the number of detected persons shown in Figure 14. \n\n \n \n \nFigure 5.8. The detections and search accomplishment of the first flight in Search \nmission 1. (a) shows the recorded targets meaning the manually labelled individuals, the \nYOLO detections of detections with a confidence score higher than 0.5, the detections \nwith an IoU higher than 0.7 and the undetected targets. In (b) the predicted search \naccomplishment is shown in blue, the recorded manual identifications obtained by \nimages taken in the experiment are shown in orange, while the YOLO detection rate is \nshown in green. The points show timestamps of images detecting an individual for the \nfirst time. \nThe confusion matrix for search missions 1 and 2 is shown in Figure 15. Search \nmission 3 consisted of only seven labels, hence it was discarded for further detection \nanalysis. The results suggest that most of the manually detected labels have also been \npredicted.  \n \nFigure 15. Confusion matrices for all search missions. Most ground truth labels have \nbeen detected, while the background area in the ground truth could only be mistakenly \npredicted as a person causing the maximum percentage being detected as persons. It is \nimportant to note that the Search mission 3 only consisted of seven labels, making the \nresults not relevant. \n \n\n \n \n6. Discussion and drawbacks \nConducting \nUAV \nexperiments \npresents \nsignificant \nchallenges \ndue \nto \nthe \nunpredictability of environmental conditions. This variability makes it difficult to \ninform participants well in advance about the confirmed experiment date. Despite these \nuncertainties, the experiment was successfully carried out with 78 participants in the \nsearch and 6 staff members, even though the final date was set only five days before. \nWeather forecasts have consistently predicted sunny conditions with minimal chance of \nrain. However, on the experiment day, unexpected fog developed, followed by light rain \nafter the experiment concluded. Additionally, windy conditions can cause additional \ndifficulties, especially in UAV motion control. This has caused unpredictable low \nquality of the images taken in Search missions 2 and 3 where individuals cannot be \nidentified. In addition to that, the Z30 camera used in the third flight of the Search \nmission 1 has shown unexpected low quality making it almost impossible to even \nmanually detect individuals. \nManaging a considerable large number of participants presents an additional \nsource of logistical challenges. In this study, one of the primary encountered difficulties \nwas data collection, as it relied on participants completing the logs accurately. Despite \nclearly outlining the required information and providing instructions on how to fill out \nthe forms, analysis of the submitted logs revealed some missing details, such as the \nnames of two participants and the shirt color of multiple individuals. In this case, the \nmissing names did not pose a significant issue, as they could be verified using the \nparticipant registration list. However, the shirt color has been shown as a bigger \nproblem since even though multiple individuals have not written any shirt color, there \nwere individuals who have changed the shirt or taken off the jacket during the \nexperiment, but have written only one color. These issues prevent the identification (not \ndetection) of individuals needed to obtain a first detection of each person which is \ncomparable to the search effectiveness 𝜂 calculated in the control framework. \nAdditionally, multiple participants have reported bad signal impacting the real-time map \ncausing issues in tracking the position inside their assigned zone. \n \n7. Conclusion \nThis study presents the simulated SAR mission experiment conducted on Učka \nmountain, Croatia with the goal of validating the search model, motion control system, \nas well as gathering additional data for future SAR research. The used motion control \nconsists of the HEDAC algorithm creating a potential field guiding the UAV’s direction \nand MPC for optimizing the flight regime. \n\n \n \n \nThe experiment consisted of 150 markers being placed inside three zones \nensuring a uniform distribution of participants taking part in the treasure hunt for \nmarkers. During the search, three autonomously operated single-UAV search missions \nwere conducted, the first one consisted of three connected flights, while the other two \nconsisted of one flight each.  \n \nThe results suggest that the probabilistic model of the search has a predicted \nsearch accomplishment similar to the manually detected individuals, as well as the \nYOLO detection rate. By this, the search and motion control systems are validated and \nshow promising results for this method to be used in SAR missions. This study can \nfurther be expanded by conducting a search exploration of a simulated SAR mission \nusing multiple UAVs to enhance the efficiency of the search. \nAcknowledgment \nThis publication is supported by the Croatian Science Foundation under the project \nUIP-2020-02-5090. \nData availability \nThe data produced in this research is publicly available on Open Science Framework \nhttps://osf.io/kb9e7. \nReferences \nAkshatha, K. R., Karunakar, A. K., Satish, S. B., Phani, P. K., Chinmay, V. D., & \nJohnson, D. G. (2023). Manipal-UAV person detection dataset: A step towards \nbenchmarking dataset and algorithms for small object detection. ISPRS Journal of \nPhotogrammetry and Remote Sensing, 195. \nhttps://doi.org/10.1016/j.isprsjprs.2022.11.008 \nBarekatain, M., Marti, M., Shih, H. F., Murray, S., Nakayama, K., Matsuo, Y., & \nPrendinger, H. (2017). Okutama-Action: An Aerial View Video Dataset for \nConcurrent Human Action Detection. IEEE Computer Society Conference on \nComputer Vision and Pattern Recognition Workshops, 2017-July. \nhttps://doi.org/10.1109/CVPRW.2017.267 \nBejiga, M. B., Zeggada, A., Nouffidj, A., & Melgani, F. (2017). A convolutional neural \nnetwork approach for assisting avalanche search and rescue operations with UAV \nimagery. Remote Sensing, 9(2). https://doi.org/10.3390/rs9020100 \n\n \n \nBircher, A., Kamel, M., Alexis, K., Oleynikova, H., & Siegwart, R. (2018). Receding \nhorizon path planning for 3D exploration and surface inspection. Autonomous \nRobots, 42(2). https://doi.org/10.1007/s10514-016-9610-0 \nCalamoneri, T., Coro, F., & Mancini, S. (2022). A Realistic Model to Support Rescue \nOperations after an Earthquake via UAVs. IEEE Access, 10. \nhttps://doi.org/10.1109/ACCESS.2022.3141216 \nEuropean Union space programme. (2024, July 1). Copernicus. \nhttps://www.copernicus.eu/en \nDoherty, P., & Rudol, P. (2007). A UAV search and rescue scenario with human body \ndetection and geolocalization. Lecture Notes in Computer Science (Including \nSubseries Lecture Notes in Artificial Intelligence and Lecture Notes in \nBioinformatics), 4830 LNAI. https://doi.org/10.1007/978-3-540-76928-6_1 \nGiitsidis, T., Karakasis, E. G., Gasteratos, A., & Sirakoulis, G. C. (2015). Human and \nfire detection from high altitude UAV images. Proceedings - 23rd Euromicro \nInternational Conference on Parallel, Distributed, and Network-Based Processing, \nPDP 2015. https://doi.org/10.1109/PDP.2015.118 \nGonzalez, L. F., Montes, G. A., Puig, E., Johnson, S., Mengersen, K., & Gaston, K. J. \n(2016). Unmanned aerial vehicles (UAVs) and artificial intelligence \nrevolutionizing wildlife monitoring and conservation. Sensors (Switzerland), 16(1). \nhttps://doi.org/10.3390/s16010097 \nGoodrich, M. A., Morse, B. S., Gerhardt, D., Cooper, J. L., Quigley, M., Adams, J. A., \n& Humphrey, C. (2008). Supporting wilderness search and rescue using a camera-\nequipped mini UAV. Journal of Field Robotics, 25(1–2). \nhttps://doi.org/10.1002/rob.20226 \nHong, M., Li, S., Yang, Y., Zhu, F., Zhao, Q., & Lu, L. (2021). SSPNet: Scale Selection \nPyramid Network for Tiny Person Detection From UAV Images. IEEE Geoscience \nand Remote Sensing Letters, 19. https://doi.org/10.1109/LGRS.2021.3103069 \nHubenko, A., Fonoberov, V. A., Mathew, G., & Mezić, I. (2011). Multiscale adaptive \nsearch. IEEE Transactions on Systems, Man, and Cybernetics, Part B: \nCybernetics, 41(4). https://doi.org/10.1109/TSMCB.2011.2106207 \nHussain, M. (2023). YOLO-v1 to YOLO-v8, the Rise of YOLO and Its Complementary \nNature toward Digital Manufacturing and Industrial Defect Detection. In Machines \n(Vol. 11, Issue 7). https://doi.org/10.3390/machines11070677 \n\n \n \nIvic, S. (2020). Motion Control for Autonomous Heterogeneous Multiagent Area Search \nin Uncertain Conditions. IEEE Transactions on Cybernetics, 52(5). \nhttps://doi.org/10.1109/TCYB.2020.3022952 \nIvic, S., Crnkovic, B., & Mezic, I. (2016). Ergodicity-Based Cooperative Multiagent \nArea Coverage via a Potential Field. IEEE Transactions on Cybernetics, 47(8). \nhttps://doi.org/10.1109/TCYB.2016.2634400 \nIvić, S., Sikirica, A., & Crnković, B. (2022). Constrained multi-agent ergodic area \nsurveying control based on finite element approximation of the potential field. \nEngineering Applications of Artificial Intelligence, 116. \nhttps://doi.org/10.1016/j.engappai.2022.105441 \nJiang, C., Ren, H., Ye, X., Zhu, J., Zeng, H., Nan, Y., Sun, M., Ren, X., & Huo, H. \n(2020). Object detection from UAV thermal infrared images and videos using \nYOLO models. International Journal of Applied Earth Observation and \nGeoinformation, 112. https://doi.org/10.1016/j.jag.2022.102912 \nJiang, P., Ergu, D., Liu, F., Cai, Y., & Ma, B. (2022). A Review of Yolo Algorithm \nDevelopments. Procedia Computer Science, 199. \nhttps://doi.org/10.1016/j.procs.2022.01.135 \nJocher, G., Qiu, J., & Chaurasia, A. (2023). Ultralytics YOLO. \nhttps://github.com/ultralytics/ultralytics \nKannadaguli, P. (2020). YOLO v4 Based Human Detection System Using Aerial \nThermal Imaging for UAV Based Surveillance Applications. 2020 International \nConference on Decision Aid Sciences and Application, DASA 2020. \nhttps://doi.org/10.1109/DASA51403.2020.9317198 \nKristo, M., Ivasic-Kos, M., & Pobar, M. (2020). Thermal Object Detection in Difficult \nWeather Conditions Using YOLO. IEEE Access, 8. \nhttps://doi.org/10.1109/ACCESS.2020.3007481 \nLanča, L., Jakac, K., & Ivić, S. (2024). Model predictive altitude and velocity control in \nergodic potential field directed multi-UAV search. http://arxiv.org/abs/2401.02899 \nLevin, E., Zarnowski, A., McCarty, J. L., Bialas, J., Banaszek, A., & Banaszek, S. \n(2016). Feasibility study of inexpensive thermal sensors and small UAS \ndeployment for living human detection in rescue missions application scenarios. \nInternational Archives of the Photogrammetry, Remote Sensing and Spatial \nInformation Sciences - ISPRS Archives, 41. https://doi.org/10.5194/isprsarchives-\nXLI-B8-99-2016 \n\n \n \nLin, L., & Goodrich, M. A. (2009). UAV intelligent path planning for wilderness search \nand rescue. 2009 IEEE/RSJ International Conference on Intelligent Robots and \nSystems, IROS 2009. https://doi.org/10.1109/IROS.2009.5354455 \nLyu, M., Zhao, Y., Huang, C., & Huang, H. (2023). Unmanned Aerial Vehicles for \nSearch and Rescue: A Survey. In Remote Sensing (Vol. 15, Issue 13). \nhttps://doi.org/10.3390/rs15133266 \nMathew, G., Kannan, S., Surana, A., Bajekal, S., & Chevva, K. R. (2013). Experimental \nimplementation of spectral multiscale coverage and search algorithms for \nautonomous UAVs. AIAA Guidance, Navigation, and Control (GNC) Conference. \nhttps://doi.org/10.2514/6.2013-5182 \nMathew, G., & Mezić, I. (2011). Metrics for ergodicity and design of ergodic dynamics \nfor multi-agent systems. Physica D: Nonlinear Phenomena, 240(4–5). \nhttps://doi.org/10.1016/j.physd.2010.10.010 \nMavrommati, A., Tzorakoleftherakis, E., Abraham, I., & Murphey, T. D. (2017). Real-\nTime Area Coverage and Target Localization Using Receding-Horizon Ergodic \nExploration. IEEE Transactions on Robotics, 34(1). \nhttps://doi.org/10.1109/TRO.2017.2766265 \nMessina, G., & Modica, G. (2020). Applications of UAV thermal imagery in precision \nagriculture: State of the art and future research outlook. In Remote Sensing (Vol. \n12, Issue 9). https://doi.org/10.3390/RS12091491 \nMiller, L. M., Silverman, Y., MacIver, M. A., & Murphey, T. D. (2015). Ergodic \nExploration of Distributed Information. IEEE Transactions on Robotics, 32(1). \nhttps://doi.org/10.1109/TRO.2015.2500441 \nNedjati, A., Vizvari, B., & Izbirak, G. (2016). Post-earthquake response by small UAV \nhelicopters. Natural Hazards, 80(3). https://doi.org/10.1007/s11069-015-2046-6 \nPetso, T., Jamisola, R. S., Mpoeleng, D., & Mmereki, W. (2021). Individual Animal and \nHerd Identification Using Custom YOLO v3 and v4 with Images Taken from a \nUAV Camera at Different Altitudes. 2021 6th International Conference on Signal \nand Image Processing, ICSIP 2021. \nhttps://doi.org/10.1109/ICSIP52628.2021.9688827 \nQi, J., Song, D., Shang, H., Wang, N., Hua, C., Wu, C., Qi, X., & Han, J. (2016). Search \nand Rescue Rotary-Wing UAV and Its Application to the Lushan Ms 7.0 \nEarthquake. Journal of Field Robotics, 33(3). https://doi.org/10.1002/rob.21615 \n\n \n \nQingqing, L., Taipalmaa, J., Queralta, J. P., Gia, T. N., Gabbouj, M., Tenhunen, H., \nRaitoharju, J., & Westerlund, T. (2020). Towards Active Vision with UAVs in \nMarine Search and Rescue: Analyzing Human Detection at Variable Altitudes. \n2020 IEEE International Symposium on Safety, Security, and Rescue Robotics, \nSSRR 2020. https://doi.org/10.1109/SSRR50563.2020.9292596 \nSilvagni, M., Tonoli, A., Zenerino, E., & Chiaberge, M. (2017). Multipurpose UAV for \nsearch and rescue operations in mountain avalanche events. In Geomatics, Natural \nHazards and Risk (Vol. 8, Issue 1). \nhttps://doi.org/10.1080/19475705.2016.1238852 \nTerven, J., Córdova-Esparza, D. M., & Romero-González, J. A. (2023). A \nComprehensive Review of YOLO Architectures in Computer Vision: From \nYOLOv1 to YOLOv8 and YOLO-NAS. In Machine Learning and Knowledge \nExtraction (Vol. 5, Issue 4). https://doi.org/10.3390/make5040083 \nTeutsch, M., Mueller, T., Huber, M., & Beyerer, J. (2014). Low resolution person \ndetection with a moving thermal infrared camera by hot spot classification. IEEE \nComputer Society Conference on Computer Vision and Pattern Recognition \nWorkshops. https://doi.org/10.1109/CVPRW.2014.40 \nWU, G., GAO, X., Fu, X., WAN, K., & DI, R. (2019). Mobility control of unmanned \naerial vehicle as communication relay in airborne multi-user systems. Chinese \nJournal of Aeronautics, 32(6). https://doi.org/10.1016/j.cja.2019.02.010 \nYeom, S. (2021). Moving people tracking and false track removing with infrared \nthermal imaging by a multirotor. Drones, 5(3). \nhttps://doi.org/10.3390/drones5030065 \nYeom, S. (2024). Thermal Image Tracking for Search and Rescue Missions with a \nDrone. Drones, 8(2). https://doi.org/10.3390/drones8020053 \nZhu, P., Wen, L., Du, D., Bian, X., Fan, H., Hu, Q., & Ling, H. (2021). Detection and \nTracking Meet Drones Challenge. IEEE Transactions on Pattern Analysis and \nMachine Intelligence, 44(11). https://doi.org/10.1109/TPAMI.2021.3119563 \n  \n \n",
  "metadata": {
    "source_path": "papers/arxiv/Experimental_validation_of_UAV_search_and_detection_system_in_real\n__wilderness_environment_624fa782fbdd5d5b.pdf",
    "content_hash": "624fa782fbdd5d5b20fa65689d743d592bd0cd6c44c7f8559bf92a25e4a580b9",
    "arxiv_id": null,
    "title": "Experimental_validation_of_UAV_search_and_detection_system_in_real\n__wilderness_environment_624fa782fbdd5d5b",
    "author": "Stella",
    "creation_date": "D:20250224184329+01'00'",
    "published": "20250224184329+01'00'",
    "pages": 32,
    "size": 1743649,
    "file_mtime": 1740470154.6650882
  }
}