{
  "text": "Published as a conference paper at ICLR 2025\nMLLMS KNOW WHERE TO LOOK:\nTRAINING-FREE PERCEPTION OF\nSMALL VISUAL DETAILS WITH MULTIMODAL LLMS\nJiarui Zhang , Mahyar Khayatkhoei , Prateek Chhikara , Filip Ilievski\nUniversity of Southern California, USA\nVrije Universiteit Amsterdam, The Netherlands\nABSTRACT\nMultimodal Large Language Models (MLLMs) have experienced rapid progress in\nvisual recognition tasks in recent years. Given their potential integration into many\ncritical applications, it is important to understand the limitations of their visual\nperception. In this work, we study whether MLLMs can perceive small visual\ndetails as effectively as large ones when answering questions about images. We\nobserve that their performance is very sensitive to the size of the visual subject\nof the question, and further show that this effect is in fact causal by conducting\nan intervention study. Next, we study the attention patterns of MLLMs when\nanswering visual questions, and intriguingly find that they consistently know where\nto look, even when they provide the wrong answer. Based on these findings, we\nthen propose training-free visual intervention methods that leverage the internal\nknowledge of any MLLM itself, in the form of attention and gradient maps, to\nenhance its perception of small visual details. We evaluate our proposed methods\non two widely-used MLLMs and seven visual question answering benchmarks\nand show that they can significantly improve MLLMs’ accuracy without requiring\nany training. Our results elucidate the risk of applying MLLMs to visual recogni-\ntion tasks concerning small details and indicate that visual intervention using the\nmodel’s internal state is a promising direction to mitigate this risk.1\n1\nINTRODUCTION\nMultimodal large language models (MLLMs) (Hurst et al., 2024; Team et al., 2024; Anthropic, 2024;\nWang et al., 2024; Li et al., 2024a; Team et al., 2025; Chen et al., 2025) have greatly progressed the\nstate of multimodal reasoning and planning, and are rapidly being integrated into various downstream\napplications, ranging from robotics (Li et al., 2024b; Chen et al., 2024), biomedicine (Li et al., 2023a),\nautonomous driving (Xu et al., 2024b; Zhang et al., 2023a) to visual mathematical reasoning (Gao\net al., 2023; Zhang et al., 2024c;b) and even food recipe generation (Chhikara et al., 2024). Given\nthe rapidly growing application of MLLMs, especially in critical domains such as biomedicine and\nsecurity, it is crucial to study the limitations of their visual perception to elucidate the potential risks\nthat may affect their downstream applications.\nTo motivate the limitation that will be the focus of this work, we start by presenting three revealing vi-\nsual question answering examples in Fig. 1, in which we ask a popular MLLM BLIP-2 (FlanT5XL) (Li\net al., 2023b) to identify an object’s presence or type in each image as we vary the size of the object.\nIn the absence of any prior evidence, we might reasonably expect the MLLM’s answer to be invariant\nto the size of the object, because of the MLLM’s large representational capacity and pretraining on\na wide variety of images containing objects of various sizes. To the contrary, in Fig. 1 (left), we\nobserve that initially the model does not recognize the existence of a small street sign and assigns a\nlower probability to the correct answer; however, zooming into the image (via more focused visual\ncropping) towards the street sign gradually increases the probability assigned to the correct answer,\nsuggesting that the model gradually perceives more and more relevant details of the street sign.\n1Our code is available at https://github.com/saccharomycetes/mllms_know.\n1\narXiv:2502.17422v1  [cs.CV]  24 Feb 2025\n\nPublished as a conference paper at ICLR 2025\nQ: Are there any street signs in the picture?\nQ: What kind of bird is this?\nQ: What brand of clock is this?\nSmaller crop size\nSmaller crop size\nSmaller crop size\nFigure 1: The effect of visual cropping on the probability of answers predicted by BLIP-2 FlanT5XL\nzero-shot VQA model. The x-axis labels are indices to the respective cropped images displayed under\neach plot that the model sees at each step. The model gradually finds the correct answer.\nIn Fig. 1 (middle), we observe further evidence of this difficulty in perceiving small details: the\nmodel initially predicts white as the type of the bird, but when we zoom into the image towards the\nbird, without changing the question in any way, we observe that the model gradually assigns higher\nprobability to the correct bird type of egret. This suggests that the model was not making a semantic\nerror of misunderstanding what type means, rather it was unable to perceive sufficient details to\ndiscriminate egret from other white birds, which is mitigated by visual cropping. Similarly, in Fig. 1\n(right), we observe that the model’s initial answer is not entirely irrelevant (“ama” compared to the\ncorrect answer “moma”), suggesting that the model knows where to look based on the question but\ncannot accurately perceive the actual word, which is again mitigated by visual cropping.\nIn this work, we will study the limitation observed in Fig. 1 extensively, elucidate its cause, and\npropose potential solutions to mitigate its consequences. In Sec. 3, we quantitatively show that there\nindeed exists a difficulty in perceiving small visual concepts across various widely-used MLLMs. Our\nfindings are consistent with prior works on evaluating the text-image matching in vision-language\njoint embedding models, which have observed a reverse correlation between visual object size in\nimages and the text-image matching score (Zhao et al., 2022), but we further establish a causal\nconnection between visual concept size and MLLMs’ perception ability through an intervention study.\nIn Sec. 4, we study whether the MLLMs’ difficulty in perceiving small visual concepts stems from a\ndifficulty in perceiving visual details, or from a difficulty in locating the concept due to its small size.\nWe quantitatively show that MLLMs consistently know where to look, even when they fail to answer\nthe question correctly. In Sec. 5, we propose three automatic visual cropping methods—leveraging\nthe attention maps and gradients of the MLLM itself—as scalable and training-free solutions to\nthe visual perception limitation. Finally, in Sec. 6, we apply our proposed methods to two popular\nMLLMs and evaluate them on seven visual question answering (VQA) benchmarks, showing their\nefficacy in enhancing MLLMs’ accuracy, especially on detail-sensitive benchmarks.\n2\nRELATED WORKS\nMultimodal Large Language Models (MLLMs). MLLMs are foundation models capable of\nhandling diverse language and vision tasks. These models fall into two categories: end-to-end\npretrained and modular pretrained. End-to-end models process joint image-language data through\narchitectures such as dual-encoder (Radford et al., 2021), fusion-encoder (Li et al., 2021), encoder-\ndecoder (Cho et al., 2021), and unified transformer (Wang et al., 2022), using objectives like\nimage-text matching, contrastive learning, and masked language modeling. Modular pretrained\nmodels, which dominate recent state-of-the-art approaches, avoid costly full pretraining by adapting\nexisting components: BLIP2 (Li et al., 2023b) and InstructBLIP (Dai et al., 2023) train a Transformer-\nbased connector between a frozen pretrained ViT (Dosovitskiy et al., 2021) image encoder and a\nfrozen LLM, which transforms ViT output tokens into a fixed set of image tokens in the input space\nof the LLM; Qwen-VL (Bai et al., 2023), similarly uses a fixed-length token connector (a single\ncross-attention layer), but trains both the connector and the LLM; LLaVA (Liu et al., 2023b) and\nLLaVA-1.5 (Liu et al., 2023a) instead use a linear projection and a two-layer MLP as their connectors,\nrespectively, and train both. Our work will contribute to a better understanding of the perception\nlimitations of MLLM and improve their perception scalably and without training, offering orthogonal\nbenefits to existing approaches.\n2\n\nPublished as a conference paper at ICLR 2025\nVisual Localization Methods. Dedicated visual localization methods, such as YOLO (Redmon\net al., 2016), SAM (Kirillov et al., 2023), and GLIP (Li et al., 2022b), rely heavily on dense spatial\nannotations for identifying salient image regions. Native approaches, such as Grad-CAM (Selvaraju\net al., 2017), localize regions by analyzing gradients from classifier decisions without spatial supervi-\nsion. Prior work adapts Grad-CAM to BLIP (Li et al., 2022a) leveraging its dedicated image-text\nsimilarity computation neural network called the Image-Text Matching network (Tiong et al., 2022;\nGuo et al., 2023). In this work, we derived a more general way for localizing the attention of MLLMs\nto images, not relying on the specific BLIP architecture. Several recent works have explored ways\nto improve the visual localization capability of MLLMs for visual question answering, including\nchain-of-thought (Shao et al., 2024; Liu et al., 2024b), tool-using (Wu and Xie, 2023), and visual\nprogramming approaches (Surís et al., 2023; Gupta and Kembhavi, 2023). In contrast, we demonstrate\nthat MLLMs can often effectively localize the visual subject of a question in their internal states, and\npropose training-free methods to leverage their internal states for improving their visual perception.\nVisual Perception Limitations in MLLMs. The difficulty of answering questions about small objects\nin images has been observed by several prior and concurrent works (Zhang et al., 2023b; 2024a; Liu\net al., 2024a; Wu and Xie, 2023), which have explored mitigating solutions based on high-resolution\nfine-tuning (Liu et al., 2024a; Dehghani et al., 2023; Wang et al., 2024), multi-agent pipelines (Wu\nand Xie, 2023), and use of visual cropping (Zhang et al., 2023b). In this work, we provide more\nextensive evidence for this difficulty, establish its causal effect on MLLMs’ performance, and show\nthat it is rooted in a failure to observe small visual details as opposed to a failure to locate small\nobjects. Several works have also shown that MLLMs suffer from object hallucination (Li et al., 2023c;\nYu et al., 2024). Furthermore, Zhang et al. (2024a) have shown visual blind spots in MLLMs—i.e.,\nlocations on the image where the MLLMs’ perception degrades—as well as their sensitivity to visual\nquality, presence of visual distractors in the image, and even local object location perturbations.\n3\nMLLMS’ SENSITIVITY TO THE SIZE OF VISUAL CONCEPTS\nIn this section, our goal is to quantitatively study our qualitative observations in Fig. 1 that MLLMs\nstruggle with describing small visual details in images. To that end, we consider the TextVQA dataset,\nin which for each question we can find the image ground-truth bounding box that contains the correct\ntextual answer. We partition its validation set into three groups based on the relative size of the\nground-truth bounding box S =\nAbb\nAtotal , where Abb denotes the area of the ground-truth bounding box,\nand Atotal the total area of the image: 1) S < 0.005 (small) consisting of 773 question-image pairs,\n2) 0.005 ≤S < 0.05 (medium) consisting of 2411 question-image pairs, and 3) S ≥0.05 (large)\nconsisting of 1186 question-image pairs. We chose TextVQA for this study because it contains\nTable 1: Sensitivity of the accuracy of MLLMs to the size of visual concepts in TextVQA. As the\nrelative visual size of the answer decreases (right to left in each row), we observe a decline in the\naccuracy of the original models (no cropping) in answering questions, whereas visual cropping\n(human-CROP) significantly improves accuracy on smaller objects.\nModel\nMethod\nAnswer Bbox Size (S)\nsmall\nmedium\nlarge\nBLIP-2 (FlanT5XL)\nno cropping\n12.13\n19.57\n36.32\nhuman-CROP\n55.76\n52.02\n45.73\nInstructBLIP (Vicuna-7B)\nno cropping\n21.79\n30.58\n45.30\nhuman-CROP\n69.60\n61.56\n53.39\nLLaVA-1.5 (Vicuna-7B)\nno cropping\n39.38\n47.74\n50.65\nhuman-CROP\n69.95\n65.36\n56.96\nQwen-VL (Qwen-7B)\nno cropping\n56.42\n65.09\n68.60\nhuman-CROP\n70.35\n75.49\n71.05\nGPT-4o\nno cropping\n65.76\n72.81\n69.17\nhuman-CROP\n75.63\n81.32\n71.72\n3\n\nPublished as a conference paper at ICLR 2025\na significant number of questions about small visual concepts, and textual answers are harder for\nMLLMs to guess from other side information in the image (compared to object types and attributes).\nSensitivity Study. If a model’s perception is not sensitive to the size of visual concepts, we expect it\nto have similar accuracy in all three partitions. In Tab. 1, we observe that the accuracy of all MLLMs\ndeclines as the ground-truth bounding box becomes relatively smaller (right to left on the no cropping\nrows). BLIP-2 and InstructBLIP are not trained on TextVQA (i.e., are zero-shot models), and their\naccuracy declines by 24 and 23 absolute percentage points between the large and small partitions,\nrespectively. LLaVA-1.5 and Qwen-VL are trained on the training set of TextVQA, yet, their accuracy\nalso declines by 11 and 12 between the large and small partitions, respectively. Lastly, even the\nmost recent commercial GPT-4o, with an unknown training set that might include all of TextVQA, is\nsuffering from a 7 percentage point decline in accuracy between small and medium partitions. These\nfindings suggest that MLLMs have a bias against perceiving smaller visual concepts.\nIntervention Study. The perceptual limitation we observed above might be merely correlated with\nsize. To study whether this limitation is causally related to size, we conduct an intervention study\nwhere we provide the MLLMs with visually cropped images based on the ground-truth bounding\nboxes, denoted as human-CROP. More specifically, for each image-question pair and each MLLM,\nwe crop the smallest square-shaped region containing the ground-truth bounding box from the image,\nand resize it to the input image resolution of the MLLM (the square-shaped cropping prevents extreme\ndeformation of the cropped image when resizing). The cropped image is then provided to the MLLM\nin addition to the original image-question pair (see more details in Fig. 4). We observe in Tab. 1\nthat human-CROP significantly improves the accuracy of all MLLMs on the small and medium\npartitions, and to a lesser extent on the large partition. These findings show that the perception\nlimitation is indeed caused by the size of the visual concepts, and that visual cropping can be a\npromising direction to mitigate this limitation.\n4\nDO MLLMS KNOW WHERE TO LOOK?\nThe limitation in perceiving small visual concepts can have two primary reasons: 1) they are hard\nto locate in the larger image, and 2) their small details are hard to perceive correctly. In Fig. 1, we\nobserved that the MLLM’s incorrect answer may contain partially correct information, hinting that\nit might know where to look in the image. In this section, we quantitatively study that observation\nto answer whether MLLMs’ sensitivity to size is rooted in a perception limitation or a localization\nlimitation. To that end, we first utilize the attention maps computed inside the Transformer layers of\nan MLLM to quantify its spatial attention over the image and then compare the total amount of this\nattention inside the ground-truth bounding box to other bounding boxes of the same size.\nMLLMs’ Setup. The considered MLLMs process a given image-question pair (x, q) in four steps\n(depicted in Fig. 4): 1) the image is divided into N × N non-overlapping patches and processed by\nthe ViT image encoder into N × N output tokens; 2) the ViT output tokens are transformed into\nthe input space of the backbone LLM—by either an MLP (LLaVA-1.5) or a Transformer connector\n(BLIP-2, InstructBLIP and Qwen-VL)—which we refer to as image tokens; 3) the image tokens are\nthen prepended to the question tokens and a predefined starting answer token, and fed to the LLM; 4)\nthe LLM is sampled auto-regressively following the starting answer token (we use greedy sampling).\nQuantifying MLLMs’ Spatial Attention over the Image. We first measure how important each\nimage token is to the MLLM’s decision (answer-to-token attention) by extracting the softmax cross-\nattention of the starting answer token to all image tokens in all layers of the backbone LLM, resulting\nin Ast(x, q) ∈RL×H×1×T , where L, H are the number of layers and heads-per-layer in the LLM,\nand T is the number of image tokens provided to the LLM. We then take the average over attention\nheads to arrive at the answer-to-token attention ˆAst(x, q) = 1\nH\nPH\nh=1 Ast(x, q). Next, we measure\nhow important each image region is to each image token (token-to-image attention). For the MLLMs\nthat use a Transformer connector to resample ViT output tokens into a fixed number of image tokens\n(BLIP-2, InstructBLIP and Qwen-VL), we extract the softmax cross-attention of each image token to\nall ViT output tokens in all layers of the connector, resulting in Ati ∈RLc×Hc×T ×N 2, where Lc, Hc\nare the number of layers and heads-per-layer in the connector, T the number of learnable query\ntokens (that become input image tokens to the LLM), and N 2 the number of image patches of the ViT\nimage encoder. We then take the average over attention heads to arrive at the token-to-image attention\n4\n\nPublished as a conference paper at ICLR 2025\nQ: What player number is this football player?\nA: 21\nQ: What phone number can a person call?\nA: 202-555-2000\nQ: What is the color of the bicycle? (A) blue (B) \nwhite (C) silver (D) red    A: C\nQ: What is the number? A: 8\nQ: What number is next exit? A: 100\nQ: Is there a car in the image? A: No\n🌋LLaVA-1.5\nInstructBLIP\nFigure 2: Examples of MLLMs knowing where to look despite answering incorrectly. The right panel\nin each example displays relative attention to the image (defined in Sec. 4) of one layer in the MLLM.\nˆAti(x) =\n1\nHc\nPHc\nh=1 Ati(x). For LLaVA-1.5 which uses an MLP to transform ViT output tokens to\nimage tokens, we set ˆAti(x) to the identity tensor. Finally, we compute the answer-to-image attention\nby computing the tensor product of the answer-to-token and token-to-image attention, resulting in\nAsi(x, q) ∈RL×Lc×1×N2 where Amk\nsi (x, q) = ˆAm\nst(x, q) ˆAk\nti(x) (superscripts m and k denote layer\nindices on the LLM and the connector, respectively).\nRelative Attention. One issue with using the softmax cross-attention is that not all highly attended\ntokens are semantically relevant to the input question. For example, recent work has observed that\nTransformers may use several tokens as registers to aggregate global information (Darcet et al., 2023).\nTo emphasize semantically relevant attention, we propose to normalize the answer-to-image attention\nof an image-question pair (x, q) by its value on a generic instruction q′. Specifically, we consider a\nfixed instruction q′ =“Write a general description of the image.”, and compute relative attention as\nArel(x, q) = Asi(x,q)\nAsi(x,q′) under element-wise division. Fig. 2 shows examples of relative attention for\nLLaVA-1.5 and InstructBLIP (Amk\nrel at layers m = 14, k = 0 and m = 15, k = 2, respectively).\nDo MLLMs Know Where to Look? Equipped with relative attention, we now return to our question\nof whether MLLMs have a localization limitation or perception limitation. To that end, we consider\nthe validation set of TextVQA again. For each image-question pair, we first compute the relative\nattention. We then define attention ratio as the ratio of the total (sum) relative attention inside\nthe answer ground-truth bounding box to its average across all bounding boxes of the same size\n0\n24\n48\n72\n96\n120\n144\nIth Layer\n0.9\n1.0\n1.1\n1.2\n1.3\nAttention Ratio\nBLIP-2 (FlanT5XL)\n0\n32\n64\n96\n128\n160\n192\nIth Layer\n1.0\n1.5\n2.0\n2.5\n3.0\nAttention Ratio\nInstructBLIP (Vicuna-7B)\n0\n4\n8\n12\n16\n20\n24\n28\n32\nIth Layer\n2\n4\n6\nAttention Ratio\nLLaVA-1.5 (Vicuna-7B)\n0\n4\n8\n12\n16\n20\n24\n28\n32\nIth Layer\n1\n2\n3\nAttention Ratio\nQwen-VL (Qwen-7B)\nCorrectly Answered\nIncorrectly Answered\nFigure 3: MLLMs’ attention ratio across all layers (average with 95% CI over TextVQA). The\nattention ratio measures how significantly the MLLM is attending to the ground-truth bounding box\n(defined in Sec. 4). We observe that it is greater than 1 in most layers, showing that the MLLMs know\nwhere to look in the image even when they fail to answer correctly.\n5\n\nPublished as a conference paper at ICLR 2025\nas the ground-truth on the image. This ratio provides a measure of how significantly the MLLM\nis attending to the ground-truth bounding box (in the sense of Markov’s inequality). In Fig. 3, we\nplot the average (with 95% confidence interval) of the attention ratio, over the validation set of\nTextVQA for all layers in the considered MLLMs. The horizontal axis shows the combined layer\nindex l = m + k × L for m ∈{0 . . . L −1} spanning the number of cross-attention layers in\nthe backbone LLM, and k ∈{0 . . . Lc −1} spanning the number of cross-attention layers in the\nconnector (BLIP-2: L = 24, Lc = 6; InstructBLIP: L = 32, Lc = 6; Qwen-VL: L = 32, Lc = 1;\nLLaVA-1.5: L = 32, Lc = 1). In all MLLMs, we observe a significantly larger than 1 attention ratio\nin most layers, suggesting that the models are attending significantly to the ground-truth bounding\nbox region on the image. Intriguingly, the models show similarly strong attention to the correct region\nregardless of whether they can answer the question correctly or incorrectly. These observations show\nthat the MLLMs tend to know where to look, even when they answer incorrectly.\n5\nAUTOMATIC VISUAL CROPPING (VICROP)\nWe observed in Sec. 4 that the sensitivity of MLLMs to visual concept size is primarily a perception\nlimitation (rather than a localization limitation). Therefore, one solution to mitigate this limitation is\nto simply train MLLMs with a larger number of image patches while maintaining per-patch resolution\n(hence increasing the image resolution of MLLMs). However, increasing the input image resolution\nby a factor of α, increases the number of ViT input patches (and output tokens) from N 2 to α2N 2,\nwhich in turn increases the softmax attention computation complexity on the order of α4N 4. Given\nthat this solution is not scalable for current Transformer-based MLLMs, we choose to explore an\nalternative solution that does not require any training and is scalable to any image resolution.\nWe note that several concurrent works have explored the first direction of training MLLMs with\nhigher resolution image patches (Li et al., 2024c; Sun et al., 2024; Li et al., 2024d; McKinzie et al.,\n2024; Xu et al., 2024a; Luo et al., 2024), and notably LLaVA-Next (Liu et al., 2024a) has achieved\nthe VQA state-of-the-art in several VQA benchmarks at the time of writing. We believe our work\nis parallel to these efforts in the following sense: rather than training higher and higher resolution\nMLLMs to enable them to see all resolutions (which is inevitably upper bounded), we explore how\nto smartly adjust the input image towards what an MLLM already can see without any additional\ntraining. We provide evidence showing that our training-free method can provide orthogonal benefits\nto the training-based methods in Appendices D and E.\nInspired by our findings that MLLMs tend to know where to look (Sec. 4) and that visual cropping\ncan mitigate the perception limitation (Sec. 3), in this section we construct three automatic visual\ncropping methods in order to mitigate the perception limitation of MLLMs. These methods seek to\nuse the internal information of an MLLM itself—in the form of attention maps and gradients—to find\nthe approximate region of interest in images (i.e., the region containing the subject of a question),\nand then to zoom into that region via visual cropping. One potential drawback of visual cropping is\nthat some questions might need to have a global view of the image. To address this issue, we utilize\nthe fact that MLLMs typically convert the image into a series of tokens. This allows us to directly\nImage \nEncoder\nLLM\n…\nQuestion\nImage \nEncoder\n…\nAnswer\n…\nVisual \nCropping\nInstructBLIP\n𝑇\nS\nLLM\n…\nQuestion\nMLP\nAnswer\n…\n🌋LLaVA-1.5\n𝑁×𝑁\nS\nTransformer\n𝑇\n𝑁×𝑁\n𝑁×𝑁\nFigure 4: Illustration of the proposed visual cropping approach applied to two MLLMs.\n6\n\nPublished as a conference paper at ICLR 2025\nextend the original image tokens by concatenating the visually cropped image tokens, as illustrated\nin Fig. 4. We use this concatenation approach when applying all our methods to MLLMs.\nRelative Attention ViCrop (rel-att). In this method, we directly compute the relative attention\nArel(x, q) defined in Sec. 4 for each image-question pair (x, q). We then select a target layer (in LLM\nand connector) based on a small held-out set of samples in TextVQA and use its relative attention as\nthe importance map for visual cropping (discussed below). We ablate on the choice of layer in Sec. 6.\nGradient-Weighted Attention ViCrop (grad-att). The relative attention runs an additional\ngeneric instruction through the MLLM to normalize the answer-to-image attention and emphasize\nsemantically relevant attention. As an alternative that does not require a second forward pass, we\nconsider using the gradients to normalize attention, because the gradient of the model’s decision with\nrespect to an attention score shows how sensitive the decision is to changes in that attention, hence how\nsemantically relevant the attention is for answering the question. To get a differentiable representation\nof the model’s decision, we consider the logarithm of the maximum output probability at the starting\nanswer token, v = log softmax(z(x, q))t∗∈R, where z ∈RD is the output logit of the LLM at the\nstarting answer position, D the vocabulary size, and t∗= arg maxt zt. Then, following our notation\nin Sec. 4, we can compute the gradient-weighted versions of answer-to-token attention ˜Ast(x, q) =\nAst(x, q) ⊙σ(∇Astv(x, q)) and token-to-image attention ˜Ati(x, q) = Ati(x) ⊙σ(∇Ativ(x, q)),\nwhere ⊙is element-wise product and σ(w) = max(0, w). We remove negative gradients because\nthey correspond to tokens that if attended to will reduce the model certainty, hence often distracting\nlocations Selvaraju et al. (2017). Finally, we compute the gradient-weighted answer-to-image\nattention as the following tensor product ˜Asi(x, q) = ˜Ast(x, q) ⊗˜Ati(x, q) ∈RL×Lc×1×N 2. We\nwill select the same target layer identified in rel-att from ˜Asi(x, q) as the importance map for\nvisual cropping.\nInput Gradient ViCrop (pure-grad). In this method, we seek to find the relevant regions on the\nimage directly using the gradient of the MLLM’s decision with respect to the input image. Compared\nto the previous attention-based methods, pure-grad is more versatile because it does not rely on\nthe Transformer-based architecture. Specifically, for each image-question pair (x, q), we will compute\nG(x, q) = ∥∇xv(x, q)∥2, where v(x, q) is the logarithm of the maximum output probability of the\nLLM at the starting answer token (as defined in grad-att above), and the L2-norm is taken over the\nimage channel dimension. However, gradients sometimes show high values in entirely constant-color\nregions (e.g., blue skies). Given that these non-edge regions do not contain any visual details, we\nwill explicitly diminish them in G. To that end, we first apply a 3 × 3-size Gaussian high-pass filter\nto the image, followed by a median filter of the same size to reduce salt-and-pepper noise. The\nresulting filtered image is then thresholded at its spatial median value to become a binary mask and is\nelement-wise multiplied by G. Finally, the edge-emphasized G is spatially average-pooled into the\nN × N patches of the MLLM to become an importance map for visual cropping.\nBounding Box Selection for Visual Cropping. To convert the importance map (from each of the\nmethods described above) to a bounding box, we use sliding windows of different sizes inspired by\nobject detection literature Redmon et al. (2016). Specifically, for each MLLM, we define a set of\nwindows with sizes equal to a multiple of the input image resolution of the MLLM. The multiples are\nin {1, 1.2, . . . 2}. We slide each window over the image with a stride of 1 and find its best position\nwhere the sum of the importance map inside the window is maximized. After selecting the best\nposition per window, we choose the window whose internal sum has the largest difference from\nthe average internal sum of its adjacent positions. This latter step is a heuristic to avoid choosing\ntoo small or too large windows (notice that in both cases, moving the window slightly left/right or\nup/down will not change its internal sum significantly). The chosen window is then cropped from the\nimage, resized to the input image resolution of the MLLM, and provided to the MLLM in addition to\nthe image-question pair.\nHigh-Resolution Visual Cropping. In one of the benchmarks we consider in this work, V∗Wu\nand Xie (2023), the images are of very high resolution (always more than 1K) and consequently, the\nresized input image provided to the MLLM might completely lose the visual concept of interest for a\nquestion. To mitigate this, on this benchmark, we employ a two-stage strategy. In the first stage, we\ndivide images into non-overlapping blocks of smaller than 1024 × 1024 with an aspect ratio close\nto 1, compute the importance map separately for each block according to the ViCrop methods, and\nthen re-attach the blocks back together. In the second stage, we find the bounding box for visual\n7\n\nPublished as a conference paper at ICLR 2025\nQ: What is the last on the list the lady is pointing at?\n🌋 LLaVA 1.5: 10        🌋 LLaVA 1.5 (w/ ViCrop): Use numbers\nQ: What is the name of the player?\n     InstructBLIP: Rudolph            InstructBLIP (w/ ViCrop): Holland\nQ: What is the color of the clock? (A) black (B) yellow (C) green (D) red \n \n🌋 LLaVA 1.5: A         🌋 LLaVA 1.5 (w/ ViCrop): C\nFigure 5: Examples of rel-att helping MLLMs correct their mistakes (cyan-colored bounding\nbox shows cropped region by rel-att; zoom-in insets are displayed for better readability).\ncropping on this re-attached importance map exactly as described before and provide the original\nimage-question pair with the resized cropped image to the MLLM.\n6\nVICROP METHOD ANALYSIS\nIn this section, we apply our proposed visual cropping methods to two open-source SOTA MLLMs,\nInstructBLIP (Vicuna-7B) (Dai et al., 2023) and LLaVA-1.5 (Vicuna-7B) (Liu et al., 2023a). We\nevaluate their effectiveness in improving the perception of smaller visual concepts on 4 detail-\nsensitive datasets (TextVQA 2 (Singh et al., 2019), V∗(Wu and Xie, 2023), POPE (Li et al., 2023c),\nDocVQA (Mathew et al., 2021)), and their ability to maintain performance on larger visual concepts\nin 3 general-purpose datasets containing mostly large objects (GQA (Hudson and Manning, 2019),\nAOKVQA (Schwenk et al., 2022), VQAv2 (Goyal et al., 2017)). InstructBLIP uses the hyper-\nparameters N = 16, m = 15, k = 2 and input image resolution of 224 × 224. LLaVA-1.5 uses\nN = 24, m = 14 and input image resolution of 336 × 336. When reporting accuracy, we compute\nVQA-score3 for all benchmarks except GQA. For GQA, we compute accuracy using the official\ncode.4. See Appendices A to C for further details about implementation, datasets, and prompts.\nViCrop Improves VQA Accuracy. In Fig. 5, we show a few examples of the ViCrop helping the\nMLLM correct itself (more examples in Appendix G), and in Tab. 2, we report the accuracy of\nthe proposed ViCrop methods on the VQA benchmarks. We observe that all methods significantly\nimprove the accuracy of the original MLLMs (no cropping) on detail-sensitive benchmarks, without\nrequiring any training, while maintaining the MLLMs’ performance on benchmarks with larger\nvisual concepts. Thus, the accuracy gain on fine details (most notably in TextVQA and V∗) does not\nseem to come at the cost of accuracy on larger visual details and relations. We also observe that the\naccuracy gain for LLaVA-1.5 is more substantial than for InstructBLIP. This can be explained by the\n2†In TextVQA evaluation, we do not provide externally extracted OCR tokens to the MLLM since we want to\nmeasure its true perception, this differs from the setup in the original paper. See more discussion in Appendix A.\n3https://visualqa.org/evaluation.html\n4https://cs.stanford.edu/people/dorarad/gqa/evaluate.html\n8\n\nPublished as a conference paper at ICLR 2025\nTable 2: Accuracy of the proposed ViCrop methods on visual question answering benchmarks.\nModel\nSmaller Visual Concepts\nLarger Visual Concepts\nTextVQA†\nV*\nPOPE\nDocVQA\nAOKVQA\nGQA\nVQAv2\nLLaVA-1.5\nno cropping\n47.80\n42.41\n85.27\n15.97\n59.01\n60.48\n75.57\nrel-att\n55.17\n62.30\n87.25\n19.63\n60.66\n60.97\n76.51\ngrad-att\n56.06\n57.07\n87.03\n19.84\n59.94\n60.98\n76.06\npure-grad\n51.67\n46.07\n86.06\n17.70\n59.92\n60.54\n75.94\nInstructBLIP\nno cropping\n33.48\n35.60\n84.89\n9.20\n60.06\n49.41\n76.25\nrel-att\n45.44\n42.41\n86.64\n9.95\n61.28\n49.75\n76.84\ngrad-att\n45.71\n37.70\n86.99\n10.81\n61.77\n50.33\n76.08\npure-grad\n42.23\n37.17\n86.84\n8.99\n61.60\n50.08\n76.71\nTable 3: Ablation study on the choice of layer and the use of high-resolution visual cropping.\nModel\nChoice of Layer\nHigh-Resolution ViCrop\nSelective\nAverage\n∆\nw/ High-Res\nw/o High-Res\n∆\nLLaVA-1.5\nno cropping\n47.80\n–\n–\n42.41\n42.41\n–\nrel-att\n55.17\n55.45\n+0.28\n62.30\n47.64\n-14.66\ngrad-att\n56.06\n56.26\n+0.20\n57.07\n49.74\n-7.33\npure-grad\n51.67\n–\n–\n46.07\n45.03\n-1.04\nInstructBLIP\nno cropping\n33.48\n–\n–\n35.60\n35.60\n–\nrel-att\n45.44\n44.40\n-1.04\n42.41\n38.74\n-3.67\ngrad-att\n45.71\n44.98\n-0.73\n37.70\n42.41\n+4.71\npure-grad\n42.23\n–\n–\n37.17\n42.41\n+5.24\nfact that InstructBLIP only trains its connector and not its backbone LLM during tuning—the LLM\ndoes not adapt to use the image tokens, rather the image tokens are adapted to optimally prompt the\nLLM—and therefore the LLM cannot effectively use the additional (cropped) image tokens provided\nthrough visual cropping. Nonetheless, the results show that ViCrop can be effectively applied to\ndifferent MLLMs, and is a promising inference-time solution for mitigating the perception limitation\nobserved in Sec. 3.\nAblation Study on the Choice of Layer. To understand the importance of the choice of an informative\nlayer for rel-att and grad-att (as discussed in Sec. 5), in Tab. 3 we compare the accuracy\nof these methods when simply taking the average of all layers in Arel and ˜Asi, respectively, on\nTextVQA. We observe that rel-att is robust to this choice and grad-att declines about 3.5\npercentage points in accuracy. Importantly, both methods still improve the MLLMs’ accuracy even\nwhen using the layer average, suggesting that averaging is a suitable choice in the absence of any\ndata for selecting a layer.\nAblation Study on the High-Resolution ViCrop. In Sec. 5, we proposed a two-stage strategy for\nprocessing the very high-resolution images in the V∗benchmark. To see how effective this strategy\nis, in Tab. 3 we compare the accuracy of ViCrop methods with and without this high-resolution\nstrategy on V∗. We observe that while this strategy is very beneficial to LLaVA-1.5, it declines the\nperformance of grad-att and pure-grad for InstructBLIP. However, all methods, with and\nwithout this strategy, still improve the MLLMs’ accuracy.\nViCrop with External Tools. In addition to the internal ViCrop methods, we also considered the\nuse of external off-the-shelf models to find the region of interest in an image for visual cropping.\nSpecifically, we utilized SAM (Kirillov et al., 2023), YOLO (Redmon et al., 2016), and CLIP (Radford\net al., 2021) to find the most relevant part of an image to a given question (details of these external\nViCrop methods are provided in Appendix F). In Tab. 4, we compare the accuracy of external ViCrop\nmethods to the internal methods on TextVQA. While external models are also effective in improving\nthe accuracy of MLLMs, they are weaker than all the proposed internal ViCrop methods, thus we did\nnot explore them further.\n9\n\nPublished as a conference paper at ICLR 2025\nTable 4: Accuracy of ViCrop using external tools compared to attention/gradient (on TextVQA); and\nthe inference time overhead of ViCrop methods (in seconds). Original’s time is per answer token.\nModel\nOriginal\nSAM\nYOLO\nCLIP\nrel-att\ngrad-att\npure-grad\nAccuracy\n(TextVQA)\nLLaVA-1.5\n47.80\n49.42\n48.84\n48.55\n55.17\n56.06\n51.67\nInstructBLIP\n33.48\n39.23\n36.49\n39.61\n45.44\n45.71\n42.23\nCPU Time\nLLaVA-1.5\n2.26\n91.53\n0.97\n5.46\n14.43\n11.33\n29.86\nInstructBLIP\n0.66\n4.35\n3.78\n7.04\nGPU Time\nLLaVA-1.5\n0.17\n3.33\n0.35\n1.07\n1.16\n0.89\n2.36\nInstructBLIP\n0.06\n0.28\n0.29\n0.60\nInference Time Overhead. In Tab. 4, we report the average inference-time overhead of the proposed\nvisual cropping methods on GPU (NVIDIA RTX A6000) and CPU (Intel(R) Gold 5317 CPU @\n3.00GHz) and compare with the per-answer-token processing time of the MLLMs. We see that all\nproposed methods (except SAM) are reasonably fast (1 to 2 seconds overhead on GPU). For example,\ncomputing the visual cropping with rel-att takes the time of generating only 5 tokens by the\nMLLM. Note that our methods’ time overhead will not scale with the number of answer tokens\nand is constant regardless of how long the answer is because our external methods do not need\nany answer token, and internal methods only need the starting answer token (see Sec. 5). In contrast,\nMLLMs’ inference time scales approximately linearly with the number of answer tokens.\n7\nCONCLUSION\nIn this work, we qualitatively and quantitatively showed that there exists a perception bias against\nsmall visual details in widely-used MLLMs. Then we found that MLLMs often know where to look\neven if they fail to answer the question, indicating that the bias toward small visual details is rooted in\na perception limitation rather than a localization limitation. To mitigate this limitation, we proposed\nmultiple automatic visual localization methods as scalable and training-free solutions based on models’\ninternal dynamics while answering the visual questions. Through evaluation of multiple multimodal\nbenchmarks, we showed that our method can significantly improve MLLMs’ accuracy without\nrequiring any training, especially in detail-sensitive scenarios. Our findings suggest that MLLMs\nshould be used with caution in detail-sensitive applications, and that visual cropping/localization with\nthe model’s own knowledge is a promising direction to enhance their performance.\nLimitations and Future Work. The proposed ViCrop methods do not enhance all types of questions\nequally. We have observed that questions concerning relations and counting are particularly difficult\nfor ViCrop methods to help answer. This is expected as the proposed ViCrop can only focus on one\nregion in the image. We leave extending ViCrop to focus on multiple regions simultaneously for\nfuture work. Another limitation of the proposed methods is their time overhead and the addition of\nvisual tokens. While the overhead is reasonable (a few seconds), we believe it can be significantly\noptimized as an inference-time mechanism, for example by utilizing lower precision, and weight\nquantization. Furthermore, Matryoshka Query Transformer (MQT) (Hu et al., 2024) enables MLLMs\nto have varying visual context size during inference. In our current results, we have shown that our\nmethods can work with two different MLLMs with distinct visual context sizes, so it seems entirely\npossible that our method can still work with varying visual context size under MQT, which can further\nreduce our computational cost through rescaling the cropped image. We leave these inference cost\noptimizations to future works. Lastly, we have observed that the proposed methods tend to have some\ncomplementary benefits, and therefore exploring ways to combine them (for example based on the\nprediction uncertainty) is also a very interesting direction for future research.\n10\n\nPublished as a conference paper at ICLR 2025\nACKNOWLEDGMENTS\nWe thank Jinyi Hu and Joe Mathai for their very useful insights. We also express our gratitude to\nanonymous reviewers for their valuable feedback. This research was supported in part by the National\nScience Foundation under Contract No. IIS-2153546.\nREFERENCES\nAnthropic. The claude 3 model family: Opus, Sonnet, Haiku, March 2024. URL https://\nwww-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/\nModel_Card_Claude_3.pdf.\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou,\nand Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization,\ntext reading, and beyond. arXiv preprint arXiv:2308.12966, 2023.\nBoyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia.\nSpatialvlm: Endowing vision-language models with spatial reasoning capabilities. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 14455–\n14465, June 2024.\nLiang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci. R1-v: Reinforcing super generalization\nability in vision-language models with less than $3. https://github.com/Deep-Agent/\nR1-V, 2025. Accessed: 2025-02-02.\nPrateek Chhikara, Dhiraj Chaurasia, Yifan Jiang, Omkar Masur, and Filip Ilievski. Fire: Food image\nto recipe generation. In Proceedings of the IEEE/CVF Winter Conference on Applications of\nComputer Vision, pages 8184–8194, 2024.\nJaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying vision-and-language tasks via text\ngeneration. In International Conference on Machine Learning, pages 1931–1942. PMLR, 2021.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards general-purpose vision-language\nmodels with instruction tuning, 2023.\nTimothée Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need\nregisters. arXiv preprint arXiv:2309.16588, 2023.\nMostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde\nCaron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim M Alabdulmohsin, et al. Patch\nn’pack: Navit, a vision transformer for any aspect ratio and resolution. Advances in Neural\nInformation Processing Systems, 36:2252–2274, 2023.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image\nis worth 16x16 words: Transformers for image recognition at scale. In International Conference\non Learning Representations, 2021.\nJiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong,\nJianhua Han, Hang Xu, Zhenguo Li, et al. G-llava: Solving geometric problem with multi-modal\nlarge language model. arXiv preprint arXiv:2312.11370, 2023.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa\nmatter: Elevating the role of image understanding in visual question answering. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition, pages 6904–6913, 2017.\nJiaxian Guo, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Boyang Li, Dacheng Tao, and\nSteven Hoi. From images to textual prompts: Zero-shot visual question answering with frozen\nlarge language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 10867–10877, 2023.\n11\n\nPublished as a conference paper at ICLR 2025\nTanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning\nwithout training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 14953–14962, 2023.\nWenbo Hu, Zi-Yi Dou, Liunian Harold Li, Amita Kamath, Nanyun Peng, and Kai-Wei Chang.\nMatryoshka query transformer for large vision-language models. arXiv preprint arXiv:2405.19315,\n2024.\nDrew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning\nand compositional question answering. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 6700–6709, 2019.\nAaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Os-\ntrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint\narXiv:2410.21276, 2024.\nGlenn Jocher, Ayush Chaurasia, and Jing Qiu. YOLO by Ultralytics. January 2023. URL https:\n//github.com/ultralytics/ultralytics.\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete\nXiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint\narXiv:2304.02643, 2023.\nBo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan\nZhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint\narXiv:2408.03326, 2024a.\nChunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan\nNaumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training a large language-and-vision\nassistant for biomedicine in one day. arXiv preprint arXiv:2306.00890, 2023a.\nJunnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven\nChu Hong Hoi. Align before fuse: Vision and language representation learning with momentum\ndistillation. Advances in neural information processing systems, 34:9694–9705, 2021.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-\ntraining for unified vision-language understanding and generation. In International Conference on\nMachine Learning, pages 12888–12900. PMLR, 2022a.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597,\n2023b.\nLiunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong,\nLijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n10965–10975, 2022b.\nXiang Li, Cristina Mata, Jongwoo Park, Kumara Kahatapitiya, Yoo Sung Jang, Jinghuan Shang,\nKanchana Ranasinghe, Ryan Burgert, Mu Cai, Yong Jae Lee, et al. Llara: Supercharging robot\nlearning data for vision-language policy. arXiv preprint arXiv:2406.20095, 2024b.\nYanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng\nLiu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models.\narXiv preprint arXiv:2403.18814, 2024c.\nYifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object\nhallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023c.\nZhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and\nXiang Bai. Monkey: Image resolution and text label are important things for large multi-modal\nmodels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 26763–26773, 2024d.\n12\n\nPublished as a conference paper at ICLR 2025\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–\nECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740–755. Springer, 2014.\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction\ntuning. arXiv preprint arXiv:2310.03744, 2023a.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv\npreprint arXiv:2304.08485, 2023b.\nHaotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee.\nLlava-next: Improved reasoning, ocr, and world knowledge, January 2024a. URL https:\n//llava-vl.github.io/blog/2024-01-30-llava-next/.\nShilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang,\nHang Su, Jun Zhu, et al. Llava-plus: Learning to use tools for creating multimodal agents. In\nEuropean Conference on Computer Vision, pages 126–142. Springer, 2024b.\nGen Luo, Yiyi Zhou, Yuxin Zhang, Xiawu Zheng, Xiaoshuai Sun, and Rongrong Ji. Feast your\neyes: Mixture-of-resolution adaptation for multimodal large language models. arXiv preprint\narXiv:2403.03003, 2024.\nMinesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: A dataset for vqa on document\nimages. In Proceedings of the IEEE/CVF winter conference on applications of computer vision,\npages 2200–2209, 2021.\nBrandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter,\nDhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Methods, analysis & insights\nfrom multimodal llm pre-training. arXiv preprint arXiv:2403.09611, 2024.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pages\n8748–8763. PMLR, 2021.\nJoseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified,\nreal-time object detection. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 779–788, 2016.\nDustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi.\nA-okvqa: A benchmark for visual question answering using world knowledge. In Computer Vision–\nECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part\nVIII, pages 146–162. Springer, 2022.\nRamprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,\nand Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localiza-\ntion. In Proceedings of the IEEE international conference on computer vision, pages 618–626,\n2017.\nHao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and\nHongsheng Li. Visual cot: Unleashing chain-of-thought reasoning in multi-modal language\nmodels, 2024.\nAmanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and\nMarcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 8317–8326, 2019.\nHai-Long Sun, Da-Wei Zhou, Yang Li, Shiyin Lu, Chao Yi, Qing-Guo Chen, Zhao Xu, Weihua Luo,\nKaifu Zhang, De-Chuan Zhan, et al. Parrot: Multilingual visual instruction tuning. arXiv preprint\narXiv:2406.02539, 2024.\nDídac Surís, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for\nreasoning. arXiv preprint arXiv:2303.08128, 2023.\n13\n\nPublished as a conference paper at ICLR 2025\nGemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett\nTanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal\nunderstanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.\nKimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun\nXiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with\nllms. arXiv preprint arXiv:2501.12599, 2025.\nAnthony Meng Huat Tiong, Junnan Li, Boyang Li, Silvio Savarese, and Steven CH Hoi. Plug-and-\nplay vqa: Zero-shot vqa by conjoining large pretrained models with zero training. arXiv preprint\narXiv:2210.08773, 2022.\nPeng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu,\nJialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language model’s perception of the\nworld at any resolution. arXiv preprint arXiv:2409.12191, 2024.\nWenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal,\nOwais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language:\nBeit pretraining for all vision and vision-language tasks. arXiv preprint arXiv:2208.10442, 2022.\nPenghao Wu and Saining Xie. V*: Guided visual search as a core mechanism in multimodal llms.\narXiv preprint arXiv:2312.14135, 2023.\nRuyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin Ni, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu,\nMaosong Sun, and Gao Huang. Llava-uhd: an lmm perceiving any aspect ratio and high-resolution\nimages. arXiv preprint arXiv:2403.11703, 2024a.\nZhenhua Xu, Yujia Zhang, Enze Xie, Zhen Zhao, Yong Guo, Kwan-Yee K Wong, Zhenguo Li, and\nHengshuang Zhao. Drivegpt4: Interpretable end-to-end autonomous driving via large language\nmodel. IEEE Robotics and Automation Letters, 2024b.\nTianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu,\nHai-Tao Zheng, Maosong Sun, et al. Rlhf-v: Towards trustworthy mllms via behavior alignment\nfrom fine-grained correctional human feedback. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 13807–13816, 2024.\nJiarui Zhang, Filip Ilievski, Kaixin Ma, Aravinda Kollaa, Jonathan Francis, and Alessandro Oltramari.\nA study of situational reasoning for traffic understanding. KDD, 2023a.\nJiarui Zhang, Mahyar Khayatkhoei, Prateek Chhikara, and Filip Ilievski. Visual cropping improves\nzero-shot question answering of multimodal large language models. In R0-FoMo: Robustness of\nFew-shot and Zero-shot Learning in Large Foundation Models, 2023b.\nJiarui Zhang, Jinyi Hu, Mahyar Khayatkhoei, Filip Ilievski, and Maosong Sun. Exploring perceptual\nlimitation of multimodal large language models. arXiv preprint arXiv:2402.07384, 2024a.\nJiarui Zhang, Ollie Liu, Tianyu Yu, Jinyi Hu, and Willie Neiswanger. Euclid: Supercharging\nmultimodal llms with synthetic high-fidelity visual descriptions. arXiv preprint arXiv:2412.08737,\n2024b.\nRenrui Zhang, Xinyu Wei, Dongzhi Jiang, Ziyu Guo, Shicheng Li, Yichi Zhang, Chengzhuo Tong,\nJiaming Liu, Aojun Zhou, Bin Wei, et al. Mavis: Mathematical visual instruction tuning with an\nautomatic data engine. arXiv preprint arXiv:2407.08739, 2024c.\nTiancheng Zhao, Tianqi Zhang, Mingwei Zhu, Haozhan Shen, Kyusong Lee, Xiaopeng Lu, and\nJianwei Yin. Vl-checklist: Evaluating pre-trained vision-language models with objects, attributes\nand relations. arXiv preprint arXiv:2207.00221, 2022.\n14\n\nPublished as a conference paper at ICLR 2025\nA\nIMPLEMENTATION DETAILS\nWe use python 3.10.6, transformers 4.29.1 and torch 2.1.2 for all the experiments. Our environment\nconsists of an Intel(R) Gold 5317 CPU @ 3.00GHz with 48 cores and 756 GB of RAM, and we\nutilize NVIDIA RTX A6000 GPUs for our experiments. We use the huggingface implementations of\nall studied MLLMs with the recommended hyper-parameters according to the respective papers. For\nGPT-4o, we use the official public API, which is available at the time of submission.\nRegarding the evaluation setting of the TextVQA dataset in Tab. 2, our setting is slightly different\nfrom the one used by the LLaVA-1.5 original paper Liu et al. (2023a). They report accuracy\non TextVQA by using externally extracted OCR tokens to enrich its text prompt. This is a text-\nspecific trick that essentially out-sources the perception of text to an external OCR model. This text-\nspecific trick is not mentioned in their paper or supplementary material, but see their clarification in\nresponse to a GitHub issue here: https://github.com/haotian-liu/LLaVA/issues/\n515#issuecomment-1763779341. In contrast, we treat TextVQA the same as any other vision\ndataset in our experiments, that is, we do not provide any OCR extracted tokens to MLLMs when\napplying them to TextVQA (only image and question, in the evaluation prompt format specified\nin their respective papers). This results in a slightly lower accuracy compared to the one reported\nin the original paper, but instead, this number shows the true perception ability of LLaVA-1.5 on\nTextVQA, not confounded by the ability of an external OCR model. For completeness, we also\nmeasured TextVQA accuracy in the presence of OCR tokens, which results in 59.8 for LLaVA-1.5\nwithout any visual cropping, and 63.95 with rel-att, showing that our proposed visual cropping\ncan still be beneficial even when OCR tokens are provided to the MLLM.\nB\nDATASET STATISTICS\nIn this section, we present the details of the datasets used for evaluation in this paper. We report the\naverage height and weight of the images in the dataset. We also report the number of images and\nquestions in each dataset. For VQAv2, we run our experiment on a random 50K subset of the official\nvalidation set. We use the entire validation set in all other datasets.\nTable 5: Average width ( ¯W) and height ( ¯H) of images, number of images, and number of questions\non all datasets.\nV∗\nDocVQA\nTextVQA\nPOPE\nAOKVQA\nGQA\nVQAv2\n¯W\n2246\n1776\n954\n584\n581\n578\n577\n¯H\n1582\n2084\n818\n478\n480\n482\n485\n# Images\n191\n1286\n3166\n500\n1122\n398\n14206\n# Questions\n191\n5349\n5000\n8910\n1145\n10781\n50000\nFor our analysis presented in Table 1 and Figure 3, we focused on TextVQA dataset, which includes\nbounding box annotations for OCR-detected text within images. However, this dataset does not\nspecify which bounding boxes correspond to the regions where answers are located, necessitating\na manual annotation process. The TextVQA dataset comprises 5000 questions and 3166 images.\nWe manually annotated these question-image pairs, ensuring accurate bounding boxes over all the\nregions of interest where the answers could be found. This manual annotation process was essential\nfor our analysis, allowing us to provide precise and reliable ground-truth data for the study. Given\nthat some questions were associated with multiple bounding boxes in their corresponding images, we\nundertook a filtering process to isolate the question-image pairs. This effort resulted in a refined set\nof 4370 question-image pairs, where there is only one instance of the subject of the question in the\nimage. For example, if the question is “what type of drink is sold here?” and there are two different\ncans of drinks in the image, we remove this image-question pair.\n15\n\nPublished as a conference paper at ICLR 2025\nC\nPROMPT FORMAT FOR ZERO-SHOT INFERENCE\nIn this section, we provide details about the prompt format used in models for zero-shot inference.\nWe use a different prompt format for LLaVA and InstructBLIP which we adapt from the original\npapers, as shown below.\nLLaVA-1.5\n<image> USER:{question} Answer the question using a single\nword or phrase.\nASSISTANT:\nInstructBLIP\n<image> Question:{question} Short Answer:\nD\nORTHOGONAL BENEFITS TO LLAVA-NEXT\nWe apply our proposed rel-att visual cropping method to an additional newer MLLM – LLaVA-\nNeXT (Liu et al., 2024a) current SOTA in several VQA benchmarks – that has support for higher-\nresolution compared to LLaVA-1.5. In Tab. 6, we observe that our method can still boost the MLLM’s\nperformance, without requiring any training. This provides further evidence for the generalizability\nof our proposed visual cropping and its orthogonal benefits to training MLLMs with higher image\npatch resolution.\nTable 6: Orthogonal benefits of visual cropping when applied to LLaV-NeXT that is trained to adapt\nto processing high-resolution images.\nModel\nTextVQA\nV∗\nLLaVA-NeXT (Mistral-7B)\n65.17\n58.11\nLLaVA-NeXT (Mistral-7B) + rel-att\n68.65\n61.78\nE\nCOMPARISON WITH THE V* METHOD (SEAL)\nThe V* method (SEAL) (Wu and Xie, 2023) proposes a multi-agent fine-tuning approach to enhance\nthe ability of an underlying MLLM to answer questions about small visual concepts. However, SEAL\nrequires substantial training and finetuning of several neural networks, whereas our methods are\ncompletely training-free, so a direct comparison would not be fair. Nonetheless, to provide an idea of\nhow our method compares to SEAL in an “as-is” fashion (i.e. if a user just wants to pick one method\nas-is off-the-shelf), we report the accuracy of SEAL compared to LLaVA-1.5+rel-att in Tab. 7.\nWe observe that our method outperforms SEAL except on the V* benchmark. We think this might be\nbecause SEAL is designed and tuned specifically toward high-resolution images in its V* benchmark.\nWe also note that the inference time of SEAL is slower than our method (4.44s compared to 1.88s on\naverage per question, tested on the same random 100 TextVQA samples with one A6000 GPU). That\nbeing said, we note that our methods and SEAL can both help enhance MLLMs, and our methods\ncan be integrated into SEAL or other multi-agent pipelines.\nTable 7: Performance comparison between our rel-att applied on LLaVA-1.5 and SEAL (Wu and\nXie, 2023) across multiple vision-language benchmarks.\nModel\nTextVQA\nV∗\nPOPE\nDocVQA\nAOKVQA\nGQA\nVQAV2\nSEAL\n36.30\n75.30\n82.40\n5.31\n55.34\n50.18\n65.35\nLLaVA-1.5+rel-att\n55.17\n62.30\n87.25\n19.63\n60.66\n60.97\n76.29\n16\n\nPublished as a conference paper at ICLR 2025\nF\nEXTERNAL TOOLS VICROP\nIn this section, we present three automatic question-guided localization methods based on popular\noff-the-shelf vision-based models, namely CLIP Radford et al. (2021), YOLO Redmon et al. (2016),\nand SAM Kirillov et al. (2023). These three methods utilize external vision-based knowledge for\nthe localization process through multimodal encoding, object detection, and semantic segmentation,\nrespectively. See Tab. 4 for their results compared to internal ViCrop methods.\nCLIP ViCrop. The intuition of this method is to progressively refine the image towards the region of\nhighest relevance to a given question using CLIP Radford et al. (2021). CLIP consists of an image\nencoder and a text encoder, which are trained on a large dataset of image-caption pairs to map each\nimage (caption) close to its caption (image) and far from all other captions (images). The result is\nan aligned shared space where various images can be directly compared with various texts. To find\nthe region of interest, given an image-question pair, we first crop the image from the four sides (top,\nbottom, left, and right) at a cropping ratio of 0.9 to produce four overlapping cropped images. We\nthen use CLIP to assess the semantic similarity between these cropped images and the question. The\nhighest-scoring crop is chosen as the input for the next iteration. This process is repeated for 20\niterations, and the cropped image with the highest CLIP similarity to the question is selected for\nvisual cropping.\nYOLO ViCrop. Instead of a progressive approach to finding the region of interest, in this method we\nselect candidate regions based on a state-of-the-art object detection method: YOLOv8 (Jocher et al.,\n2023) pretrained on COCO Lin et al. (2014). Using YOLO, we filter out regions that contain no\nsalient objects – i.e., regions for which CLIP could mistakenly assign high similarity. More concretely,\nfor each question-image pair, we first use YOLO to collect bounding boxes for all predicted objects\nwith confidence higher than 0.25 (the recommended default).5 Then, for each predicted bounding\nbox, we crop its corresponding image and compute its similarity to the question using CLIP. Finally,\nthe bounding box with the highest similarity score is selected as the region of interest for visual\ncropping.\nSAM ViCrop. A limitation of YOLO is that it only provides bounding boxes corresponding to a fixed\nnumber of object classes. To overcome this issue, we use the segment anything model (SAM) Kirillov\net al. (2023), which has shown state-of-the-art zero-shot segmentation performance. SAM can provide\nan extensive set of segmentation masks for each image, thus providing a more granular set of salient\ncandidate regions compared to YOLO. More concretely, for each image-question pair, we feed the\nimage into SAM, which provides an extensive set of segmentation masks corresponding to all objects\nand object parts. Then, we translate these masks into bounding boxes by computing the smallest\nbounding box that covers each segmentation mask. Finally, the bounding box with the highest CLIP\nsimilarity to the question is selected as the region of interest for visual cropping.\nFinally, for each method, we crop the smallest covering square (so that the cropped image is not\ndeformed when resized to the input resolution of the MLLM), and provide it to the MLLM in addition\nto the original image-question pair (as depicted in Fig. 4).\n5https://docs.ultralytics.com/modes/predict\n17\n\nPublished as a conference paper at ICLR 2025\nG\nADDITIONAL EXAMPLES ON MODEL’S PREDICTIONS\nQ: What is the breed of \nthe dog? \n(A)  Husky \n(B)  corgi \n(C)  Dalmatian \n(D)  golden retriever\n🌋 LLaVA 1.5:     C\n🌋 LLaVA 1.5 \n(w/ ViCrop):       D\nQ: Is the blue truck on the \nleft or right side of the \nwhite vehicle? \n(A)  right \n(B)  left\n🌋 LLaVA 1.5:     B\n🌋 LLaVA 1.5 \n(w/ ViCrop):     \nA\nQ: What is the color of \nthe handbag? \n(A)  white \n(B)  red \n(C)  black \n(D)  yellow                  \n🌋 LLaVA 1.5:     C\n🌋 LLaVA 1.5 \n(w/ ViCrop):         D\nQ: What is the color of \nthe parachute?\n(A)  Blue   \n(B)  yellow\n(C)  Green \n(D)  red\n🌋 LLaVA 1.5:     B\n🌋 LLaVA 1.5 \n(w/ ViCrop):         A\nFigure 6: Success (first 3) and failure (last) examples of LLaVA-1.5 (rel-att) on the V∗benchmark\n(cyan-colored bounding box shows cropped region by rel-att; zoom-in insets are displayed for\nbetter readability).\n18\n\nPublished as a conference paper at ICLR 2025\nQ: what number is on the middle bike?\n🌋 LLaVA 1.5:     39\n🌋 LLaVA 1.5 (w/ ViCrop):     30\nQ: what number is on the man in whites jersey?\n🌋 LLaVA 1.5:     22\n🌋 LLaVA 1.5 (w/ ViCrop):     7\nQ: what does the sign at the crosswalk say?\n🌋 LLaVA 1.5:     Go\n🌋 LLaVA 1.5 (w/ ViCrop):     10av\nQ: what team is written on the baseball in the right \nhand corner?\n🌋 LLaVA 1.5:     Yankees\n🌋 LLaVA 1.5 (w/ ViCrop):     Riverdogs\nQ: what is the company that made this game / \ncharacter?\n🌋 LLaVA 1.5:     Crazy brick\n🌋 LLaVA 1.5 (w/ ViCrop):     Steve Jackson games\nQ: what is the brand of the monitor?\n🌋 LLaVA 1.5:     Postugo\n🌋 LLaVA 1.5 (w/ ViCrop):     Positivo\nQ: what country is miss universe from?\n🌋 LLaVA 1.5:     Usa\n🌋 LLaVA 1.5 (w/ ViCrop):     Canada\nQ: is city of ballard in europe?\n🌋 LLaVA 1.5:     no\n🌋 LLaVA 1.5 (w/ ViCrop):     yes\nQ: which brewery made this ale?\n🌋 LLaVA 1.5:     Perrys\n🌋 LLaVA 1.5 (w/ ViCrop):     Cottrell\nQ: what does the bubble text say for the woman?\n🌋 LLaVA 1.5:     Hold it boys!\n🌋 LLaVA 1.5 (w/ ViCrop):     Hold it\nQ: what is styped in the search bar?\n🌋 LLaVA 1.5:     Sushi\n🌋 LLaVA 1.5 (w/ ViCrop):     Buscar dierccion\nQ: what is the number of the player in the middle?\n🌋 LLaVA 1.5:     3\n🌋 LLaVA 1.5 (w/ ViCrop):     22\nQ: what is the wine brand?\n🌋 LLaVA 1.5:     Dog house\n🌋 LLaVA 1.5 (w/ ViCrop):     Masseto\nQ: what kind of exchange shows on the banner?\n🌋 LLaVA 1.5:     Policy\n🌋 LLaVA 1.5 (w/ ViCrop):     Sab miller\nQ: who is the picture of?\n🌋 LLaVA 1.5:     Charles grayson\n🌋 LLaVA 1.5 (w/ ViCrop):     Man\nFigure 7: Success (first 9) and failure (last 6) examples of LLaVA-1.5 (rel-att) on the TextVQA\nbenchmark (cyan-colored bounding box shows cropped region by rel-att).\n19\n\nPublished as a conference paper at ICLR 2025\nQ: what does the sign say above the wheelchair \nsymbol?\n     InstructBLIP:     Men\n     InstructBLIP (w/ ViCrop):     Restroom\nQ: what is the number at the bottom right corner?\n     InstructBLIP:     100\n     InstructBLIP (w/ ViCrop):     16\nQ: what is the name in the very top right hand corner \nof this page?\n     InstructBLIP:     Shakespeare\n     InstructBLIP (w/ ViCrop):     Newcome\nQ: what is the name of the group on the bottom right \nof the poster?\n     InstructBLIP:     Public\n     InstructBLIP (w/ ViCrop):     Les socialistes\nQ: more sharing what?\n     InstructBLIP:     Photo\n     InstructBLIP (w/ ViCrop):     Options\nQ: what brand of bike wheel is this?\n     InstructBLIP:     Pace\n     InstructBLIP (w/ ViCrop):     Pacenti\nQ: what is written on the milk carton?\n     InstructBLIP:     Milk\n     InstructBLIP (w/ ViCrop):     U.s. forced europe\nQ: what letter is on the mans ball cap?\n     InstructBLIP:     F\n     InstructBLIP (w/ ViCrop):     T\nQ: what degree angle has been drawn?\n     InstructBLIP:     45\n     InstructBLIP (w/ ViCrop):     90\nQ: what brand are the guy's shorts?\n     InstructBLIP:     Gators\n     InstructBLIP (w/ ViCrop):     Nike\nQ: who is the service provider?\n     InstructBLIP:     Mophie\n     InstructBLIP (w/ ViCrop):     Apple\nQ: what is the middle book title?\n     InstructBLIP:     Mess\n     InstructBLIP (w/ ViCrop):     Messily\nQ: what is the word under the state name?\n     InstructBLIP:     Karazy\n     InstructBLIP (w/ ViCrop):     California\nQ: what is the manufacturer of these bullets?\n     InstructBLIP:     Sears\n     InstructBLIP (w/ ViCrop):     Remington\nQ: what is the word written in the bottom of the box?\n     InstructBLIP:     Hardcast\n     InstructBLIP (w/ ViCrop):     Flexible\nFigure 8: Success (first 9) and failure (last 6) examples of InstructBLIP (rel-att) on the TextVQA\nbenchmark (cyan-colored bounding box shows cropped region by rel-att).\n20\n",
  "metadata": {
    "source_path": "papers/arxiv/MLLMs_Know_Where_to_Look_Training-free_Perception_of_Small_Visual\n__Details_with_Multimodal_LLMs_3286368b9e4612df.pdf",
    "content_hash": "3286368b9e4612df44ee9d94cabe4848430e9d6c724cf81e10d4fb517280fe9b",
    "arxiv_id": null,
    "title": "MLLMs_Know_Where_to_Look_Training-free_Perception_of_Small_Visual\n__Details_with_Multimodal_LLMs_3286368b9e4612df",
    "author": "",
    "creation_date": "D:20250225030908Z",
    "published": "2025-02-25T03:09:08",
    "pages": 20,
    "size": 18941381,
    "file_mtime": 1740470083.7763743
  }
}