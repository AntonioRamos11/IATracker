{
  "text": "Benchmarking Multimodal RAG through a Chart-based Document\nQuestion-Answering Generation Framework\nYuming Yang1, Jiang Zhong1*, Li Jin2*, Jingwang Huang1, Jingpeng Gao1\nQing Liu2, Yang Bai2, Jingyuan Zhang3, Rui Jiang1, Kaiwen Wei1*\n1 College of Computer Science, Chongqing University, China\n2 Aerospace Information Research Institute, Chinese Academy of Sciences, China\n3 Kuaishou Technology, Beijing, China\nymyang@cqu.edu.cn, zhongjiang@cqu.edu.cn, weikaiwen@cqu.edu.cn\njinlimails@gmail.com\nAbstract\nMultimodal Retrieval-Augmented Generation\n(MRAG) enhances reasoning capabilities by\nintegrating external knowledge. However, ex-\nisting benchmarks primarily focus on simple\nimage-text interactions, overlooking complex\nvisual formats like charts that are prevalent in\nreal-world applications. In this work, we in-\ntroduce a novel task, Chart-based MRAG, to\naddress this limitation. To semi-automatically\ngenerate high-quality evaluation samples, we\npropose CHARt-based document question-\nanswering GEneration (CHARGE), a frame-\nwork that produces evaluation data through\nstructured keypoint extraction, crossmodal ver-\nification, and keypoint-based generation. By\ncombining CHARGE with expert validation,\nwe construct Chart-MRAG Bench, a com-\nprehensive benchmark for chart-based MRAG\nevaluation, featuring 4,738 question-answering\npairs across 8 domains from real-world doc-\numents. Our evaluation reveals three critical\nlimitations in current approaches: (1) unified\nmultimodal embedding retrieval methods strug-\ngles in chart-based scenarios, (2) even with\nground-truth retrieval, state-of-the-art MLLMs\nachieve only 58.19% Correctness and 73.87%\nCoverage scores, and (3) MLLMs demonstrate\nconsistent text-over-visual modality bias during\nChart-based MRAG reasoning. The CHARGE\nand Chart-MRAG Bench are released at https:\n//github.com/Nomothings/CHARGE.git.\n1\nIntroduction\nMultimodal\nretrieval-augmented\ngeneration\n(MRAG) (Zhao et al., 2023) enhances multimodal\nreasoning by retrieving relevant external knowl-\nedge, and leveraging multimodal large language\nmodels (MLLMs) for informed response genera-\ntion (OpenAI, 2023; Zhang et al., 2024a). This\napproach substantially mitigates hallucinations and\nimproves factual grounding (Gao et al., 2023).\n*Corresponding author\nFigure 1: Comparison of two common MRAG scenarios,\nimage-only and text-image, and the proposed text-chart\ntask. In the text-chart MRAG scenario, models need to\ncapture intricate chart details and retrieve both chart and\ntext information to generate correct answers.\nEffectively evaluating MRAG systems requires\nhigh-quality benchmarks that assess both retrieval\nand generation.\nExisting benchmarks such as\nMRAG-Bench (Hu et al., 2024) and Dyn-VQA\n(Li et al., 2024b) have made strides in assess-\ning MRAG capabilities through manually curated\nquestion-answering (QA) pairs. However, as illus-\ntrated in Fig.1(a) and (b), these benchmarks primar-\nily focus on scenarios involving images or simple\ncombinations of images and text. Such settings\nfail to capture the complex interactions between\nvisual details and corresponding text, particularly\nwhen dealing dense and structured information like\ncharts, which are widely used in real-world appli-\ncations (Masry et al., 2022). This leaves a critical\ngap in MRAG evaluation.\nTo bridge this gap, we propose a new task:\nChart-based MRAG. For a given text query, this\ntask involves three RAG sub-tasks: (1) Text-Chart\narXiv:2502.14864v1  [cs.AI]  20 Feb 2025\n\nMRAG, as illustrated in Fig. 1(c), both textual and\nchart data must be jointly retrieved to generate cor-\nrect answers. In addition, to allow for the sepa-\nrate evaluation of each modality’s contributions, it\nalso provides (2) Text-only RAG, where answers\ncan only be found in textual information; and (3)\nChart-only MRAG, where answers depend exclu-\nsively on chart data. To comprehensively evalu-\nate these tasks, a major challenge is how to semi-\nautomatically generate high-quality QA pairs that\naccurately capture text-chart interactions.\nTo overcome this challenge, we propose CHARt-\nbased document question-answering GEneration\n(CHARGE), a framework for automatically gen-\nerating QA pairs from real-world chart-document\ndata. CHARGE follows a three-stage pipeline com-\nprising structured keypoint extraction from text\nand chart data, crossmodal verification for accu-\nracy, and keypoint-based generation to model com-\nplex multimodal interactions. Moreover, to further\nchallenge the chart-based MRAG task, MLLMs\nare employed to generate QA pairs that require\nmulti-hop reasoning based on intra-document or\ninter-document retrieval.\nBuilding on CHARGE, we introduce Chart-\nMRAG Bench, a high-quality, human-checked\nbenchmark tailored for Chart-based MRAG. With\nCHARGE, 5,866 qualified QA pairs were initially\ngenerated, after that, 4,738 (nearly 80%) were\nmeticulously selected through expert evaluation\nbased on clarity, accuracy, multimodal coherence,\nand ethical considerations. As shown in Table 1,\nChart-MRAG Bench comprises 267 documents\nspanning 8 domains, 8 types of questions, 1,283\nparagraphs, and 627 charts, capturing complex\ncrossmodal interactions in realistic scenarios.\nWe conducted a systematic evaluation of main-\nstream retrieval methods and MLLMs on Chart-\nMRAG Bench. In our evaluation, keypoint-based\nCorrectness and Coverage metrics were introduced\nto rigorously assess accuracy and comprehensive-\nness. The results reveal that unified multimodal em-\nbedding retrieval methods, which rely on a single\nvector store, perform poorly in high-density chart\nscenarios. Furthermore, even with ground-truth\nretrieval, the best-performing Claude-3.5 Sonnet\n(Team et al., 2024) only achieved 58.19 Correctness\nand 73.87 Coverage metrics, highlighting persis-\ntent challenges in text-chart multimodal reasoning.\nIn summary, the contributions of this paper are:\n1) We present Chart-based MRAG, the first ex-\ntension of MRAG to chart scenarios that introduces\nBenchmarks\nTarget\nTask\nRetrieval\nModality\nQuestion\nTypes\nHuman\nAnnotation\nOK-VQA (Marino et al., 2019)\nVQA\nVisual\n1\n✓\nMMQA (Talmor et al., 2021)\nVQA\nMultimodal\n16\n×\nPlotQA (Methani et al., 2020)\nVQA\nVisual\n1\n✓\nChartQA (Masry et al., 2022)\nVQA\nVisual\n1\n✓\nDocVQA (Mathew et al., 2021)\nVQA\nVisual\n9\n✓\nMRAG-Bench (Hu et al., 2024)\nMRAG\nVisual\n3\n✓\nSSMQG (Wu et al., 2024)\nMRAG\nMultimodal\n5\n×\nChart-MRAG (Ours)\nMRAG\nMultimodal\n8\n✓\nTable 1: Comparison between existing MRAG bench-\nmarks and the proposed Chart-MRAG Bench.\na new dimension for evaluating crossmodal reason-\ning in information-dense visual contexts.\n2) We propose CHARGE, an automated frame-\nwork for generating QA pairs in real-world sce-\nnarios through a structured pipeline of keypoint\nextraction, verification, and generation.\n3) We establish Chart-MRAG Bench based on\nCHARGE. It is a human-verified benchmark for\nchart-based MRAG, covering 8 scenarios, 8 ques-\ntion types, and 4,738 QA pairs, with a subset de-\nsigned for multi-hop reasoning.\n4) We introduce two robust evaluation metrics\nto assess MRAG quality. Extensive experiments\nhighlight the limitations of existing retrieval and\ngeneration methods in chart-centric tasks.\n2\nRelated Work\nMultimodal RAG Methods. Recent advances in\nRetrieval-Augmented Generation (RAG) (Izacard\net al., 2022; Zhang et al., 2024b) have success-\nfully extended to multimodal domains (Chen et al.,\n2022; Zhao et al., 2023, 2024), enabling cross-\nmodal tasks through MLLMs (Yao et al., 2024;\nTeam, 2024). While researchers have proposed var-\nious approaches (Ma et al., 2024a; Faysse et al.,\n2024; Yu et al., 2024; Methani et al., 2020; Mathew\net al., 2021) for crossmodal retrieval, current evalu-\nation methodologies predominantly rely on Visual\nQuestion Answering (VQA) datasets (Marino et al.,\n2019; Talmor et al., 2021; Schwenk et al., 2022;\nMasry et al., 2022). These evaluations fall short in\naddressing retrieval-specific challenges.\nMultimodal RAG Benchmarks. The effective-\nness of MRAG systems necessitates comprehen-\nsive evaluation benchmarks. While several bench-\nmarks (Hu et al., 2024; Li et al., 2024b; Zhou\net al., 2024) explore vision-based retrieval for ques-\ntion answering through manual annotation, they\nneglect the critical dimension of crossmodal col-\nlaborative generation. Some studies (Dong et al.,\n2025; Ma et al., 2024b; Ding et al., 2024) consider\n\nFigure 2: The proposed CHARGE framework for creating multimodal QA pairs from document-chart data,\nconsisting of three steps: (1) Extract keypoints from textual content and charts, (2) Perform crossmodal verification\nto validate keypoint modality uniqueness, (3) Generate diverse QA pairs through constrained keypoint retrieval.\nhybrid modality retrieval, yet they primarily rely\non manual question-answering. Furthermore, al-\nthough some studies (Es et al., 2023; Abaskohi\net al., 2024; Mathew et al., 2021; Li et al., 2024a;\nWu et al., 2024) have investigated automated pro-\ncesses for generating crossmodal QA pairs, their\nscope focus on simplistic natural images with sin-\ngular subjects, the chart-based scenarios largely\nunexplored. To bridge this gap, this paper intro-\nduce Chart-MRAG Bench. Table 1 illustrates the\ndifferences between existing MRAG benchmarks\nand Chart-MRAG Bench.\n3\nCHARGE Framework\nWe present CHARGE, a framework for generat-\ning multimodal multi-hop QA pairs from text-chart\ndocuments. CHARGE operates in three stages: (1)\nextracting self-contained keypoints from both tex-\ntual and visual content, (2) verifying the modality\nauthenticity of extracted keypoints through cross-\nmodal verification, and (3) generating diverse QA\npairs by combining related keypoints across docu-\nments and modalities.\n3.1\nExtract Keypoints\nAs illustrated in Fig 2, given multimodal docu-\nments D = {d1, ..., dn}, CHARGE process its tex-\ntual content into coherent chunks T = {t1, ..., tm}\nand charts as discrete units C = {c1, ..., ck}. We\ndefine keypoints as self-contained factual state-\nments that capture core information from these\nsource materials. These atomic units are extracted\nfrom both textual and visual content (e.g., \"33% of\nU.S. adults say they use TikTok\") through:\nK =\n(\nϕt(T)\nfor text\nϕc(C, ψ(C))\nfor chart,\n(1)\nwhere K = {k1, ..., kr} consists of structured in-\nformation units capturing factual statements, logi-\ncal inferences, or conclusive summaries. For tex-\ntual content, we utilize GPT-4o through function\nϕt. For visual content, we first extract numerical\nvalues using function ψ (implemented with Char-\ntOCR (Luo et al., 2021)), then employ GPT-4o\nthrough function ϕc to jointly process the charts C\nand the extracted values, ensuring both contextual\ncomprehension and numerical precision. Detailed\nworkflow is presented in Appendix A.\n3.2\nCrossmodal Verification\nTo ensure the reliability of extracted keypoints, we\ndevelop a crossmodal verification mechanism that\nvalidates whether information truly belongs to its\nclaimed modality. Our key insight is: Authentic\nmodality-specific keypoints should be retrievable\nfrom its source modality but not from the other.\nWe first categorize keypoints into two funda-\nmental types: (1) Text-based keypoints (KT ): in-\nformation exclusively present in textual form; (2)\nChart-only keypoints (KC): information uniquely\nextractable from chart visualization. While GPT-4o\nperforms initial classification, crossmodal Verifica-\ntion is crucial for complex reasoning tasks.\nThe verification process employs crossmodal\nquerying with GPT-4o serving as a judge to de-\ntermine whether the queried information exists in\n\nFigure 3: An inter-document multi-hop QA example\nfrom Chart-MRAG Bench, generated by CHARGE.\neach modality’s response. Taking text-based key-\npoint verification as an example, for a given key-\npoint kt\ni ∈KT , we query both its source text chunk\nti and the paired chart ci (with OCR information\nvi). Let ˜kt\ni and ˜kc\ni denote the model’s responses\nfrom text and chart modalities respectively. The\nverification criterion is formalized as:\nStatus(kt\ni)=\n(\nRetain if ˜kt\ni =kt\ni ∧˜kc\ni ̸=kt\ni\nDrop\notherwise.\n(2)\nThis automated process retains keypoints only\nwhen correctly retrieved from their source modal-\nity and absent in others. Detailed algorithms are\nprovided in Appendix B.\n3.3\nQuestion-Answer Pair Generation\nSince each keypoint represents a specific conclu-\nsion or data point, it can generate a correspond-\ning question-answer pair (commonly referred to as\nSingle-Point QA). In our CHARGE framework, we\nsupport this basic form of QA generation. Addition-\nally, recognizing that most real-world queries re-\nquire the integration of multiple knowledge points\nto be answered fully (known as Multi-hop QA), we\nfurther designed a multi-hop question answering\napproach: by combining semantically related key-\npoints to form a single question-answer pair. These\ntypes of questions cannot be completely answered\nusing just one keypoint; they require the retrieval of\nall information sources (text chunks or charts) con-\ntaining the constituent keypoints to be answered\ncorrectly. As illustrated in Fig 3, CHARGE gener-\nates Multi-hop QA that requires retrieving multiple\npieces of information by combining \"33% of U.S.\nadults say they use TikTok\" (from a chart) with\n\"62% of U.S. adults who use TikTok say a reason\nthey use the site is to look at product reviews or\nrecommendations\" (from text).\nThe generation of Multi-hop QA involves two\nkey steps: identifying semantically related key-\npoints and constructing QA pairs from their combi-\nnations. Motivated by the capacity of RAG, we gen-\nerate QA pairs through keypoint retrieval . Specifi-\ncally, we first randomly select a keypoint (termed\nselected keypoint) as the query, then retrieve can-\ndidate keypoints (termed retrieved keypoints) by\nE5-Large (Wang et al., 2022). By tracking the\nsource documents of these retrieved keypoints, we\ncategorize them into two types:\n• Intra-Document: retrieved keypoints originate\nfrom the same document as the selected keypoint.\n• Inter-Document: retrieved keypoints come from\ndifferent documents than the selected keypoint.\nAdditionally, by tracking whether keypoints are\nfrom text or chart, we categorize their combinations\ninto three types:\n• Text-only: both extracted from text paragraphs.\n• Chart-only: both extracted from charts.\n• Text-Chart: extracted from different modalities.\nFor each combination of keypoints, we employ\nGPT-4o to generate meaningful question-answer\npairs by combining selected keypoint and its re-\ntrieved keypoint. These questions are designed\nto require multi-hop reasoning across different\nsources or modalities for answering. As illustrated\nin Table 2, CHARGE naturally supports eight dis-\ntinct types of QA pairs based on the document\nsources and modality combinations. While Algo-\nrithm 1 demonstrates the generation process for\nAlgorithm 1: Cross-document Text-Chart QA\nInput\n:Document set D;\nChart keypoint set KC;\nText keypoint set KT\nOutput :Question-Answer pair (q, a)\n// Step 1: Select chart keypoint\n1 Select kc\ni ∈KC from document da ∈D\n2 (ci, vi) ←(c, ψ(c)) where c ∈da\n// Step 2: Retrieve relevant text keypoint\n3 KT\nr ←Retrieve(kc\ni , KT , k)\n// top-k retrieval\n4 Select kt\nj ∈KT\nr from document db ∈D\n5 tj ←corresponding text block in db\n// Step 3: Generate QA pair\n6 (q, a) ←MLLM(kc\ni , kt\nj, ci, vi, tj)\n7 return (q, a)\n\nStatistics\nReasoning Step\nNumber\nTotal questions\n–\n4,738\n- Single-Point Text-only\n1-hop\n499 (10.53%)\n- Single-Point Chart-only\n1-hop\n763 (16.10%)\n- Intra-Document Text-only\n2-hop\n666 (14.06%)\n- Intra-Document Chart-only\n2-hop\n587 (12.39%)\n- Intra-Document Text-Chart\n2-hop\n746 (15.74%)\n- Inter-Document Text-only\n2-hop\n547 (11.54%)\n- Inter-Document Chart-only\n2-hop\n472 (9.96%)\n- Inter-Document Text-Chart\n2-hop\n458 (9.67%)\nTable 2: Statistics of question types based on reasoning\ncomplexity and modality.\none specific type, comprehensive implementation\ndetails for all types are provided in Appendix C.\n4\nChart-MRAG Bench\nBy utilizing the CHARGE framework, we gener-\nated an initial pool of question-answer pairs. These\npairs underwent rigorous expert evaluation to en-\nsure high quality, culminating in the Chart-MRAG\nBench. This process was guided by 4 principles:\nAuthenticity and Diversity. The benchmark is\nbased on real-world data collected from the of-\nficial website1, a trusted source of high-quality\nsocial research. We collected data from Septem-\nber 2023 to September 2024, encompassing 267\ndocuments containing 1,283 text passages and 627\ncharts. As illustrated in Table 2 and Fig 4, Chart-\nMRAG Bench encompasses 8 distinct domains,\nintegrating over 10 chart types and 8 QA types.\nAnnotation Reliability. We engaged 12 expert an-\nnotators with Master’s degrees. All annotators were\nproficient in English, with an average TOEFL score\nof 92 or equivalent language proficiency. The an-\nnotation process took 34 working days to complete.\nOur annotation protocol involved three indepen-\ndent reviewers evaluating each sample, achieving\na Fleiss’s kappa (Fleiss and Cohen, 1973) of 0.82,\nindicating substantial inter-annotator agreement.\nRigorous Quality Control. Through meticulous\nmanual review, we refined the dataset from 9,600\ninitial candidates to 5,866 validated pairs by sys-\ntematically eliminating 2,631 samples with OCR\nerrors and 1,103 redundant samples. A consensus-\nbased sampling strategy required validation from at\nleast two reviewers, resulting in 4,738 high-quality\nsamples (nearly 80% of the validated pairs).\nHigh Information Complexity. Statistical anal-\nysis reveals the benchmark’s sophistication: ap-\nproximately 70% of charts contain more than 8\n1www.pewresearch.org\nFigure 4: Distribution of documents across 8 domains,\nrepresenting key areas of real-world applications.\ncritical information points (mean: 13.87), and over\n73% of text passages include more than 6 keypoints\n(mean: 8.31). This information-rich environment\nrigorously evaluates models’ capacity to process\nintricate and dense data representations.\nFor illustrative examples of Chart-MRAG Bench\nquestion-answer pairs across different domains and\nreasoning types, please refer to Appendix D.\n5\nExperiments\n5.1\nBaselines and Evaluation Metrics\nWe conduct comprehensive evaluations using 3\ndistinct retrieval methods and 8 diverse MLLMs.\nIncluding Multimodal Retrievers: CLIP (Rad-\nford et al., 2021), JINA (Koukounas et al., 2024),\nSigLIP (Zhai et al., 2023), BGE-M3-base/large\n(Chen et al., 2024) and E5-base/large (Wang et al.,\n2022). And Backbone MLLMs: GPT-4o (ver-\nsion 2024-11-20) (Radford et al., 2021), GPT-\n4-Vision (Radford et al., 2021), Gemini-1.5-Pro\n(Team et al., 2024), Claude-3.5-Sonnet (version\n2024-10-22) (Awadalla et al., 2023), SAIL-VL-2B\n(Team, 2024), Qwen2-VL-7B-instruct (Wang et al.,\n2024), MiniCPM-V-2.6 (8B) (Yao et al., 2024), and\nLlama-3.2-90B-Vision (Dubey et al., 2024).\nFollowing Wu et al. (2024), we evaluate multi-\nmodal retrieval models using Recall@5 (R@5) and\nRecall@10 (R@10). Please refer to Appendix E\nfor details of the retrieval setup and metrics. More-\nover, since chart-based MRAG is a newly proposed\ntask, existing evaluation metrics are inadequate.\nTherefore, we introduce Correctness and Coverage\nmetrics to assess the quality of responses.\nCorrectness.\nIt measures the exact match be-\ntween response and ground truth keypoints. Given\na question-answer pair {Q, A, Kgt} with ground\ntruth keypoints Kgt = {kgt\n1 , ..., kgt\nn }, we extract\nkeypoints Kr = {kr\n1, ..., kr\nm} from the model’s\n\nModel\nOverall\nSingle-Point\nIntra-Document\nInter-Document\nR@5\nR@10\nText-only\nChart-only\nText-only\nChart-only\nText-Chart\nText-only\nChart-only\nText-Chart\nR@5\nR@10\nR@5\nR@10\nR@5\nR@10\nR@5\nR@10\nR@5\nR@10\nR@5\nR@10\nR@5\nR@10\nR@5\nR@10\nMethod 1: Unified Multimodal Embedding and Single Vector Store\nSigLIP\n11.69\n15.95\n50.00\n57.07\n0.00\n0.00\n19.63\n30.74\n0.00\n0.00\n0.00\n0.00\n16.20\n27.31\n0.00\n0.00\n0.00\n0.00\nCLIP\n13.26\n19.07\n56.06\n65.15\n0.00\n0.00\n24.44\n42.22\n0.00\n0.00\n0.00\n0.00\n16.20\n28.70\n0.00\n0.00\n0.00\n0.00\nJINA\n23.14\n29.02\n77.78\n85.35\n0.00\n0.00\n47.04\n63.70\n0.00\n0.00\n0.00\n0.00\n41.20\n56.94\n0.00\n0.00\n0.00\n0.00\nMethod 2: Multimodal Embeddings and Combined Vector Stores (Caption generated by GPT-4-Vision)\nBGE-M3-base\n22.89\n31.21\n39.90\n47.47\n52.24\n62.09\n9.63\n18.52\n17.01\n31.29\n9.09\n15.51\n9.72\n15.28\n13.43\n19.40\n4.46\n11.61\nBM25\n27.02\n36.46\n51.01\n54.55\n52.24\n63.88\n16.67\n26.67\n12.93\n25.17\n7.49\n19.79\n23.15\n31.94\n10.45\n16.42\n12.50\n21.43\nBGE-M3-large\n27.64\n39.52\n64.65\n70.71\n43.58\n59.70\n29.26\n42.96\n10.20\n17.69\n8.02\n18.72\n18.98\n32.41\n5.97\n14.93\n8.93\n22.32\nE5-base\n35.27\n47.59\n67.17\n73.74\n66.27\n80.90\n21.48\n34.81\n23.13\n47.62\n15.51\n25.13\n20.37\n27.78\n20.90\n35.07\n14.29\n23.21\nE5-large\n41.53\n59.54\n72.73\n79.80\n64.78\n79.40\n38.89\n60.74\n23.13\n48.30\n18.18\n41.71\n35.65\n53.24\n20.90\n41.04\n22.32\n40.18\nMethod 3: Multimodal Embeddings and Separate Vector Stores\nJINA + BM25\n23.83\n33.90\n48.48\n53.03\n45.67\n59.10\n11.85\n20.37\n14.97\n28.57\n9.63\n18.72\n15.74\n26.39\n7.46\n19.40\n14.29\n21.43\nCLIP + BGE-M3-base\n25.64\n36.09\n34.34\n41.92\n66.57\n77.61\n5.93\n12.96\n24.49\n51.70\n10.70\n19.25\n6.48\n12.04\n14.93\n35.07\n11.61\n12.50\nCLIP + BGE-M3-large\n33.40\n46.97\n57.58\n68.18\n66.57\n77.61\n20.74\n34.07\n24.49\n51.70\n19.79\n32.09\n13.89\n24.07\n14.93\n35.07\n16.07\n25.89\nSigLIP + E5-base\n37.96\n52.47\n64.65\n69.70\n84.18\n93.73\n15.56\n29.63\n39.46\n74.15\n17.11\n28.34\n13.89\n24.54\n14.18\n44.78\n14.29\n28.57\nSigLIP + E5-large\n42.53\n61.10\n68.69\n75.76\n84.18\n93.73\n25.19\n47.41\n39.46\n74.15\n24.06\n41.71\n21.76\n43.06\n14.18\n44.78\n22.32\n40.18\nTable 3: Performance Comparison of Different Multimodal Retrieved Models (%) on Chart-MRAG benchmark,\nevaluating three strategies: Unified Multimodal Embedding with Single Vector Store, Multimodal Embeddings with\nCombined Vector Stores, and Multimodal Embeddings with Separate Vector Stores (best scores highlighted in blue).\nresponse using an LLM. The score is defined as:\nCorrectness(Kr, Kgt) = 1[Kr ≡Kgt],\n(3)\nwhere Kr\n≡Kgt implies complete keypoint\nmatching and equal cardinality. This binary metric\nrequires perfect accuracy, with zero tolerance for\nmissing information or errors.\nCoverage. It quantifies the proportion of correctly\ncaptured ground truth keypoints:\nCoverage(Kr, Kgt) = |Km|\n|Kgt|,\n(4)\nwhere Km represents matched ground truth key-\npoints. This continuous metric in [0,1] enables\ngranular evaluation.\n5.2\nRetrieval Performance Comparison\nTable 3 reveals significant challenges in multi-\nmodal retrieval.\nWhile existing retrievers ex-\nhibit strong single-modal performance (JINA-CLIP\nachieves 77.78% Recall@5 in text-only questions\nand SigLIP + E5 reaches 84.18% Recall@5 in\nchart-only tasks), Inter-Document Text-Chart ques-\ntions yielded only 22.32% retrieval accuracy. The\nkey findings demonstrate that storing and retrieving\ncharts and text separately in the database substan-\ntially improves performance, achieving recall rates\nof 42.53% and 61.10% at k=5 and k=10.\nUnified\nmultimodal\nembeddings\nfail\nin\nknowledge-intensive scenarios. While Method 1\noutperforms all other approaches in pure text-only\nQA, it achieves zero recall (0.00%) in chart-only\nQA and Text-Chart QA tasks. This phenomenon\nreveals a critical limitation: current unified mul-\ntimodal embedding models excel at representing\nknowledge-sparse content (e.g., identifying a dog\nin an image) but struggle with knowledge-intensive\nscenarios (e.g., retrieving specific numerical values\nfrom charts in a multimodal repository).\nChart captioning enables simple yet effective\nmultimodal retrieval. Methods 2 and 3 achieve\ncomparable performance (Recall@5: 41.53% vs\n42.53%), with differences primarily in chart re-\ntrieval due to the inherent limitations of text-based\nchart representations. However, considering the\nmaintenance overhead of separate modal stores,\ncaption-based retrieval provides a practical ap-\nproach that preserves effectiveness while signif-\nicantly reducing system complexity.\n5.3\nGenerative Performance Comparison\nTable 4 presents the comprehensive experimen-\ntal results of mainstream MLLMs, with retrieval\nmethod 3 consistently applied across all evalua-\ntions to ensure controlled comparison. The results\nreveal that state-of-the-art MLLMs achieve only\nmodest performance metrics (Correctness = 3.06\nand Coverage = 9.59) without multimodal RAG\nknowledge, highlighting Chart-MRAG Bench’s ex-\nceptional challenging nature that surpasses existing\nbenchmarks in knowledge leakage control.\nClaude-3.5-Sonnet demonstrates superior over-\nall performance. The experimental results validate\nour keypoint-based evaluation methodology. With\nground truth retrieval, Claude-3.5-Sonnet achieves\nCorrectness of 58.19% and Coverage of 73.87%,\n\nModel\nOverall\nSingle-Point\nIntra-Document\nInter-Document\nCorr.\nCov.\nText-only Chart-only\nText-only\nChart-only\nText-Chart\nText-only\nChart-only\nText-Chart\nCorr.\nCorr.\nCorr.\nCov.\nCorr.\nCov.\nCorr.\nCov.\nCorr.\nCov.\nCorr.\nCov.\nCorr.\nCov.\nOpen-Source VLMs\nSAIL-VL-2B\n0.38\n1.58\n1.52\n0.30\n0.74\n2.59\n0.00\n0.00\n0.00\n1.60\n0.00\n3.47\n0.00\n0.37\n0.00\n1.79\n+ RAG (k=5)\n3.88\n8.51\n19.19\n4.18\n1.48\n14.44\n0.00\n2.04\n0.00\n1.34\n2.78\n13.89\n0.00\n0.75\n0.00\n1.79\n+ RAG (k=10)\n3.19\n7.71\n14.65\n4.48\n2.22\n13.89\n0.00\n1.36\n0.00\n2.14\n0.46\n11.11\n0.00\n0.75\n0.00\n3.12\n+ RAG (GT)\n19.82\n29.44\n63.64\n9.85\n31.30\n57.22\n2.72\n6.80\n0.53\n5.61\n29.86\n54.17\n0.00\n3.36\n3.57\n12.05\nqwen2-VL-7B-instruct\n1.16\n4.45\n4.55\n2.09\n0.56\n7.22\n0.00\n1.19\n0.00\n3.48\n0.46\n7.41\n0.00\n1.49\n0.00\n5.36\n+ RAG (k=5)\n13.51\n23.55\n50.51\n4.78\n22.41\n46.73\n1.36\n3.40\n1.07\n8.82\n15.51\n41.67\n1.49\n2.99\n0.00\n9.82\n+ RAG (k=10)\n14.45\n23.57\n51.01\n3.28\n26.11\n49.81\n0.68\n2.38\n1.60\n7.75\n20.14\n43.75\n0.75\n1.49\n0.00\n8.48\n+ RAG (GT)\n33.15\n42.46\n78.28\n11.04\n64.26\n81.30\n2.04\n9.52\n5.88\n20.86\n62.73\n80.56\n2.99\n6.72\n9.82\n27.23\nMiniCPM-V-2.6-8B\n0.88\n4.05\n2.02\n2.69\n0.37\n6.67\n0.00\n1.70\n0.00\n3.74\n0.00\n6.71\n0.00\n2.24\n0.00\n4.46\n+ RAG (k=5)\n17.32\n31.32\n47.98\n25.67\n14.81\n42.59\n3.40\n13.61\n4.01\n20.59\n15.97\n43.52\n1.49\n11.07\n6.25\n22.77\n+ RAG (k=10)\n17.60\n31.51\n48.48\n19.70\n21.11\n50.43\n2.72\n12.24\n4.81\n19.79\n19.68\n46.30\n1.49\n11.07\n4.46\n23.21\n+ RAG (GT)\n46.94\n59.41\n79.29\n48.66\n65.37\n80.99\n12.93\n29.93\n22.46\n46.79\n69.68\n83.10\n14.18\n32.34\n20.98\n47.32\nLlama-3.2-90B-Vision\n1.22\n4.36\n5.56\n2.09\n0.37\n8.15\n0.00\n1.36\n0.00\n1.87\n0.23\n5.79\n0.00\n3.36\n0.00\n4.46\n+ RAG (k=5)\n20.42\n34.68\n50.51\n30.15\n21.85\n49.20\n5.44\n16.21\n4.81\n20.05\n17.82\n45.83\n1.49\n12.69\n8.04\n28.57\n+ RAG (k=10)\n23.11\n37.31\n53.54\n31.94\n26.67\n53.15\n4.76\n16.67\n5.88\n24.33\n23.38\n49.54\n4.48\n15.67\n8.93\n30.36\n+ RAG (GT)\n50.16\n64.03\n79.80\n58.81\n63.33\n81.36\n21.09\n40.95\n21.66\n48.13\n59.49\n78.24\n32.09\n46.27\n29.46\n57.59\nProprietary VLMs\nGPT-4o\n2.50\n8.32\n8.59\n5.97\n0.37\n13.64\n0.00\n2.38\n0.00\n5.08\n0.46\n12.27\n0.75\n2.61\n0.00\n8.48\n+ RAG (k=5)\n25.89\n36.63\n47.98\n45.97\n24.44\n43.89\n11.90\n20.75\n8.29\n22.99\n19.44\n37.96\n6.72\n13.06\n13.39\n30.36\n+ RAG (k=10)\n29.11\n41.07\n47.98\n47.76\n28.52\n51.85\n13.27\n23.47\n10.96\n26.74\n25.93\n43.98\n14.93\n23.88\n15.62\n34.38\n+ RAG (GT)\n52.25\n62.97\n70.20\n63.88\n64.07\n77.22\n24.83\n38.10\n22.19\n45.19\n62.96\n75.46\n36.57\n51.12\n41.52\n62.95\nGPT-4-Vision\n2.13\n7.82\n6.57\n4.78\n0.74\n11.91\n0.00\n3.06\n0.00\n5.61\n1.39\n15.51\n0.00\n2.99\n0.00\n6.25\n+ RAG (k=5)\n25.17\n35.96\n46.97\n45.37\n21.85\n42.04\n12.93\n22.45\n6.95\n22.46\n21.06\n38.66\n7.46\n14.55\n9.82\n26.34\n+ RAG (k=10)\n29.21\n41.21\n49.49\n48.06\n31.11\n51.20\n11.56\n23.47\n10.43\n27.01\n25.46\n46.30\n12.69\n23.13\n13.84\n33.93\n+ RAG (GT)\n53.85\n63.95\n71.72\n64.78\n67.78\n79.91\n25.17\n39.68\n25.67\n47.33\n67.13\n78.01\n38.06\n50.37\n33.93\n56.25\nGemini-1.5-Pro\n2.47\n8.11\n7.58\n3.58\n2.04\n15.25\n0.00\n2.72\n0.00\n4.55\n1.85\n13.66\n1.49\n5.97\n0.89\n4.91\n+ RAG (k=5)\n26.64\n43.63\n51.52\n45.97\n24.07\n52.04\n11.56\n30.95\n7.75\n31.82\n21.53\n46.76\n8.21\n27.74\n14.29\n38.39\n+ RAG (k=10)\n32.05\n50.15\n55.56\n51.34\n34.63\n60.43\n14.97\n38.10\n9.09\n37.70\n24.54\n51.16\n16.42\n38.68\n20.54\n48.66\n+ RAG (GT)\n57.94\n70.96\n76.01\n69.55\n70.74\n84.07\n31.29\n57.03\n22.99\n51.87\n71.30\n83.80\n51.49\n66.42\n35.71\n61.61\nClaude-3.5-Sonnet\n3.06\n9.59\n7.58\n5.67\n1.48\n14.20\n0.00\n5.78\n2.14\n9.63\n2.31\n12.96\n0.75\n6.72\n0.89\n10.71\n+ RAG (k=5)\n26.74\n47.55\n44.44\n54.93\n22.04\n54.51\n14.97\n35.94\n5.35\n37.70\n16.67\n47.69\n8.21\n32.09\n15.18\n43.75\n+ RAG (k=10)\n31.49\n52.52\n45.96\n58.81\n28.70\n60.25\n18.37\n44.78\n6.42\n39.84\n24.54\n53.94\n19.40\n41.04\n17.86\n44.20\n+ RAG (GT)\n58.19\n73.87\n83.33\n70.75\n69.81\n85.19\n32.65\n60.32\n20.32\n56.68\n68.98\n85.88\n48.51\n67.91\n35.71\n63.84\nTable 4: Performance Comparison of Different MLLMs (%) on Chart-MRAG benchmark. The optimal retrieval\nconfiguration (SigLIP + E5-large) is employed across all experiments to ensure controlled comparison (best scores\nfor open-source and proprietary models highlighted in blue and red, respectively).\noutperforming mainstream MLLMs across various\nretrieval scenarios. It only falls behind Gemini-\n1.5-pro in Correctness at k=10. While Claude-3.5-\nSonnet leads in aggregate scores, the narrow perfor-\nmance margins suggest potential in open-source al-\nternatives: its Correctness (58.19) exceeds Gemini-\n1.5-pro by just 0.25. Moreover, in single-point\ntext-only evaluation at recall k=5, qwen2-VL-7B-\ninstruct achieves higher Correctness (50.51) com-\npared to Claude-3.5-Sonnet (44.44).\nModel performance generally scales with pa-\nrameter count.\nAmong open-source MLLMs,\nLlama-3.2-90B-Vision consistently outperforms\nmodels with smaller parameters across various re-\ntrieval settings. Similarly, in proprietary MLLMs,\nGPT-4-Vision, with its presumably larger model\nsize, demonstrates marginally better than GPT-4o.\nArchitectural\noptimizations\ncan\nmitigate\nMLLMs’ parameter constraints. By incorpo-\nFigure 5: Impact of retrieval size k across different\nparameter scales, demonstrating that larger models con-\nsistently benefit from increased retrieval context while\nsmaller models show performance degradation.\nrating SigLip-400M and optimizing multi-image\nunderstanding,\nMiniCPM-V\n2.6\nachieves\na\nCorrectness of 46.94 and Coverage that surpasses\nits base model qwen2-VL-7B-instruct by 13.79\nand 16.95 respectively. Most notably, despite using\n\nFigure 6: Trade-off analysis between retrieval coverage\nand answer accuracy across different k settings, illustrat-\ning how larger retrieval windows increase recall while\ncompromising answer correctness.\nonly 7B parameters, it approaches the performance\nof Llama-3.2-90B-Vision, with gaps of 3.22 in\nCorrectness and 4.62 in Coverage, demonstrating\nthat thoughtful architecture design can largely\ncompensate for parameter constraints.\n5.4\nFurther Analysis\nIn this study, we examine the influence of retrieval\nrate (k) and modality bias of MLLMs in multi-\nmodal question answering. Our analysis shows:\nModel performance in multimodal retrieval sig-\nnificantly correlates with parameter scale. Em-\npirical analysis reveals a strong correlation between\nmodel scale and multimodal retrieval performance.\nWe evaluated eight models of varying parameter\nsizes under different retrieval settings (k = 2, 5,\n10, 15, 20), where retrieved items were balanced\nbetween images and text (split equally for even k,\nwith text receiving one additional item for odd k).\nFor each model, we selected 40 question-answer\npairs per category, totaling 320 pairs for compre-\nhensive evaluation, as shown in Fig 6. The results\ndemonstrate that larger models consistently achieve\nsuperior performance across all retrieval settings.\nIn contrast, smaller models show no significant im-\nprovement (even exhibit declining) in performance\nas the number of retrieved items increases.\nLarger retrieval windows lead to a non-trivial\ntrade-off between retrieval coverage and answer\nquality. To systematically investigate the impact\nof Top_k on response generation, we conducted\nextended experiments as visualized in Fig 5. With\nk=5, the system achieves a 42.53 Recall and 56.17\ncorrectness. When increasing k=10, although the\n61.10 Recall, the answer get 49.13 correctness. No-\ntably, while this adjustment results in an increase\nFigure 7: Analysis of modality preference in MLLMs\nwhen presented with redundant information across text\nand charts, revealing systematic modality bias.\nin absolute correct answers from 1,132 to 1,423,\nthe improvement sacrifices precision.\nMLLMs demonstrate consistent text-over-visual\nmodality bias. To investigate modality bias in\nMLLMs, we carefully curated 100 specialized\nquestion-answer pairs where answers could be de-\nrived from both textual and visual information si-\nmultaneously, but with varying levels of granularity\n(e.g., \"one third\" in text versus \"35.2%\" in charts).\nAs shown in Fig. 7, analysis reveals a consistent\npreference across models for text-only responses,\neven when charts contain more precise informa-\ntion. Notably, larger MLLMs demonstrate superior\nability in detecting information redundancy and\nactively acknowledge this in their responses. For\ninstance, GPT-4o proactively identified informa-\ntion redundancy in 23% of its responses. In con-\ntrast, smaller models (such as SAIL-VL-2B) show\nlimited sensitivity to such information redundancy.\nDetailed examples refer to Appendix F.\n6\nConclusion\nThis paper introduces Chart-based MRAG, a novel\ntask to bridge the evaluation gap for chart formats\nin MRAG systems. To support this, we propose\nCHARGE, an automated framework for generat-\ning Crossmodal evaluation samples with keypoint-\nbased metrics. Combining CHARGE with expert\nvalidation, we construct Chart-MRAG Bench, com-\nprising 4,738 high-quality question-answer pairs\nacross 8 domains. Our experiments reveal key lim-\nitations in current MRAG approaches, highlighting\nthe need for specialized architectures to better han-\ndle high-density visual interactions.\n\nLimitations\nWhile our work presents promising results, we ac-\nknowledge several limitations that warrant consid-\neration in future research.\nFirst, although we ensured the accuracy of chart\ninformation in Chart-MRAG Bench through man-\nual verification, the CHARGE framework would\nbenefit from more advanced OCR techniques to fur-\nther enhance the accuracy of question generation,\nespecially in handling complex chart layouts and\ndiverse visual elements.\nSecond, due to computational constraints, our\nevaluation was confined to a select set of MRAG\nmethods and MLLMs. A more comprehensive\nevaluation across diverse model architectures and\nframeworks would likely yield additional insights\ninto the generalizability of our findings and poten-\ntially reveal new directions for improvement.\nEthical Considerations\nThis research was conducted under the approval\nof our institution’s ethics review board. All proce-\ndures were designed to ensure participant welfare\nand data privacy throughout the study.\nParticipant Recruitment and Compensation.\nWe recruited expert annotators through Amazon, a\nprofessional data annotation platform. Annotators\nwere compensated at a rate of $28.5 per hour. This\nrate was determined by:\n• Conducting pilot studies with 5 annotators to\nestablish an average task completion time of 45\nminutes\n• Accounting for additional training time (30 min-\nutes) and regular breaks\n• Considering local living wage standards across\ndifferent regions\n• Adding a 20% premium for specialized expertise\nrequired\nFor a typical 8-hour workday including training, we\nensure fair payment while maintaining data quality.\nRegular feedback from annotators confirmed the\ncompensation was considered fair for the required\nexpertise and effort.\nInformed Consent and Instructions.\nAll anno-\ntators received comprehensive instructions detail-\ning the task requirements, data usage policies, and\npotential content exposure. The instruction pack-\nage included:\n• Task objectives and annotation guidelines\n• Examples of expected annotations\n• Data privacy and usage policies\n• Right to withdraw from participation\nAnnotators provided explicit consent for their con-\ntributions to be used in academic research and pub-\nlic datasets.\nAnnotator Demographics.\nOur annotation team\nconsisted of 12 professional annotators with back-\ngrounds in data science and visualization. The an-\nnotators represented diverse geographical locations\n(3 North America, 3 Europe, 6 Asia) and possessed\nrelevant domain expertise. All demographic infor-\nmation was self-reported during the recruitment\nprocess.\nData Collection and Privacy.\nThe datasets used\nin this study, including those for generating multi-\nmodal question-answer pairs, were collected and\nprocessed in compliance with GDPR and relevant\ndata privacy regulations. We ensured that:\n• No personally identifiable information was col-\nlected\n• All chart data was anonymized before annotation\n• Participants were informed about data usage and\nsharing plans\nBias Mitigation.\nWe implemented several mea-\nsures to minimize potential biases in our dataset\nand evaluation metrics:\n• Diverse annotator selection to ensure varied per-\nspectives\n• Regular quality checks for systematic biases in\nannotations\n• Balanced representation of different chart types\nand domains\nThe resulting benchmark will be made publicly\navailable for academic research purposes, accom-\npanied by detailed documentation of the collection\nprocess and annotator guidelines. All materials will\nbe released through established academic reposito-\nries to ensure transparency and reproducibility.\nAcknowledgements\nWe sincerely thank all the anonymous review-\ners.\nThe work is supported by the National\nNatural Science Foundation of China (62206267\n\nand 62176029), Chongqing Key Project of Tech-\nnological Innovation and Application Develop-\nment (CSTB2023TIAD-KPX0064), China Post-\ndoctoral Science Foundation Funded Project\n(2024M763867).\nReferences\nAmirhossein Abaskohi, Spandana Gella, Giuseppe\nCarenini, and Issam H Laradji. 2024. Fm2ds: Few-\nshot multimodal multihop data synthesis with knowl-\nedge distillation for question answering.\narXiv\npreprint arXiv:2412.07030.\nAnas Awadalla, Irena Gao, Josh Gardner, Jack Hes-\nsel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe,\nYonatan Bitton, Samir Gadre, Shiori Sagawa, et al.\n2023. Openflamingo: An open-source framework for\ntraining large autoregressive vision-language models.\narXiv preprint arXiv:2308.01390.\nJianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu\nLian, and Zheng Liu. 2024. Bge m3-embedding:\nMulti-lingual, multi-functionality, multi-granularity\ntext embeddings through self-knowledge distillation.\narXiv preprint arXiv:2402.03216.\nWenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and\nWilliam W Cohen. 2022.\nMurag:\nMultimodal\nretrieval-augmented generator for open question\nanswering over images and text.\narXiv preprint\narXiv:2210.02928.\nYihao Ding, Kaixuan Ren, Jiabin Huang, Siwen\nLuo, and Soyeon Caren Han. 2024.\nMvqa: A\ndataset for multimodal information retrieval in pdf-\nbased visual question answering.\narXiv preprint\narXiv:2404.12720.\nKuicai Dong, Yujing Chang, Xin Deik Goh, Dexun\nLi, Ruiming Tang, and Yong Liu. 2025. Mmdocir:\nBenchmarking multi-modal retrieval for long docu-\nments. arXiv preprint arXiv:2501.08828.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, et al. 2024. The llama 3 herd of models. arXiv\npreprint arXiv:2407.21783.\nShahul Es, Jithin James, Luis Espinosa-Anke, and\nSteven Schockaert. 2023. Ragas: Automated eval-\nuation of retrieval augmented generation.\narXiv\npreprint arXiv:2309.15217.\nManuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani,\nGautier Viaud, Céline Hudelot, and Pierre Colombo.\n2024. Colpali: Efficient document retrieval with vi-\nsion language models. Preprint, arXiv:2407.01449.\nJoseph L Fleiss and Jacob Cohen. 1973. The equiva-\nlence of weighted kappa and the intraclass correlation\ncoefficient as measures of reliability. Educational\nand psychological measurement, 33(3):613–619.\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,\nJinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen\nWang. 2023. Retrieval-augmented generation for\nlarge language models: A survey. arXiv preprint\narXiv:2312.10997.\nWenbo Hu, Jia-Chen Gu, Zi-Yi Dou, Mohsen Fayyaz,\nPan Lu, Kai-Wei Chang, and Nanyun Peng. 2024.\nMrag-bench: Vision-centric evaluation for retrieval-\naugmented multimodal models.\narXiv preprint\narXiv:2410.08182.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299, 1(2):4.\nAndreas Koukounas, Georgios Mastrapas, Michael Gün-\nther, Bo Wang, Scott Martens, Isabelle Mohr, Saba\nSturua, Mohammad Kalim Akram, Joan Fontanals\nMartínez, Saahil Ognawala, et al. 2024. Jina clip:\nYour clip model is also your text retriever. arXiv\npreprint arXiv:2405.20204.\nLei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong\nFeng, Lingpeng Kong, and Qi Liu. 2024a. Multi-\nmodal arxiv: A dataset for improving scientific com-\nprehension of large vision-language models. arXiv\npreprint arXiv:2403.00231.\nYangning Li, Yinghui Li, Xingyu Wang, Yong Jiang,\nZhen Zhang, Xinran Zheng, Hui Wang, Hai-Tao\nZheng, Philip S Yu, Fei Huang, et al. 2024b. Bench-\nmarking multimodal retrieval augmented generation\nwith dynamic vqa dataset and self-adaptive planning\nagent. arXiv preprint arXiv:2411.02937.\nJunyu Luo, Zekun Li, Jinpeng Wang, and Chin-Yew\nLin. 2021. Chartocr: Data extraction from charts\nimages via a deep hybrid framework. In Proceedings\nof the IEEE/CVF winter conference on applications\nof computer vision, pages 1917–1925.\nXueguang Ma, Sheng-Chieh Lin, Minghan Li, Wenhu\nChen, and Jimmy Lin. 2024a. Unifying multimodal\nretrieval via document screenshot embedding. In Pro-\nceedings of the 2024 Conference on Empirical Meth-\nods in Natural Language Processing, pages 6492–\n6505, Miami, Florida, USA. Association for Compu-\ntational Linguistics.\nYubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen,\nYizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan\nMa, Xiaoyi Dong, et al. 2024b.\nMmlongbench-\ndoc:\nBenchmarking long-context document un-\nderstanding with visualizations.\narXiv preprint\narXiv:2407.01523.\nKenneth Marino, Mohammad Rastegari, Ali Farhadi,\nand Roozbeh Mottaghi. 2019. Ok-vqa: A visual ques-\ntion answering benchmark requiring external knowl-\nedge. In Proceedings of the IEEE/cvf conference\non computer vision and pattern recognition, pages\n3195–3204.\n\nAhmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty,\nand Enamul Hoque. 2022. Chartqa: A benchmark\nfor question answering about charts with visual and\nlogical reasoning. arXiv preprint arXiv:2203.10244.\nMinesh Mathew, Dimosthenis Karatzas, and CV Jawa-\nhar. 2021. Docvqa: A dataset for vqa on document\nimages. In Proceedings of the IEEE/CVF winter con-\nference on applications of computer vision, pages\n2200–2209.\nNitesh Methani, Pritha Ganguly, Mitesh M. Khapra,\nand Pratyush Kumar. 2020. Plotqa: Reasoning over\nscientific plots. In The IEEE Winter Conference on\nApplications of Computer Vision (WACV).\nR OpenAI. 2023.\nGpt-4 technical report. arxiv\n2303.08774. View in Article, 2(5).\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from\nnatural language supervision. In International confer-\nence on machine learning, pages 8748–8763. PMLR.\nDustin Schwenk, Apoorv Khandelwal, Christopher\nClark, Kenneth Marino, and Roozbeh Mottaghi. 2022.\nA-okvqa: A benchmark for visual question answer-\ning using world knowledge. In European conference\non computer vision, pages 146–162. Springer.\nAlon Talmor, Ori Yoran, Amnon Catav, Dan Lahav,\nYizhong Wang, Akari Asai, Gabriel Ilharco, Han-\nnaneh Hajishirzi, and Jonathan Berant. 2021. Mul-\ntimodalqa: Complex question answering over text,\ntables and images. arXiv preprint arXiv:2104.06039.\nBytedance Douyin Content Team. 2024. Sail-vl: Scal-\nable vision language model training with high quality\ndata curation.\nGemini Team, Petko Georgiev, Ving Ian Lei, Ryan\nBurnell, Libin Bai, Anmol Gulati, Garrett Tanzer,\nDamien Vincent, Zhufeng Pan, Shibo Wang, et al.\n2024. Gemini 1.5: Unlocking multimodal under-\nstanding across millions of tokens of context. arXiv\npreprint arXiv:2403.05530.\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing\nJiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\nand Furu Wei. 2022. Text embeddings by weakly-\nsupervised contrastive pre-training. arXiv preprint\narXiv:2212.03533.\nPeng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhi-\nhao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin\nWang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei\nDu, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang\nZhou, Jingren Zhou, and Junyang Lin. 2024. Qwen2-\nvl: Enhancing vision-language model’s perception\nof the world at any resolution.\narXiv preprint\narXiv:2409.12191.\nIan Wu, Sravan Jayanthi, Vijay Viswanathan, Simon\nRosenberg, Sina Pakazad, Tongshuang Wu, and Gra-\nham Neubig. 2024. Synthetic multimodal question\ngeneration. arXiv preprint arXiv:2407.02233.\nYuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang,\nJunbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li,\nWeilin Zhao, Zhihui He, et al. 2024. Minicpm-v:\nA gpt-4v level mllm on your phone. arXiv preprint\narXiv:2408.01800.\nShi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao\nRan, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han,\nZhiyuan Liu, et al. 2024.\nVisrag: Vision-based\nretrieval-augmented generation on multi-modality\ndocuments. arXiv preprint arXiv:2410.10594.\nXiaohua Zhai, Basil Mustafa, Alexander Kolesnikov,\nand Lucas Beyer. 2023. Sigmoid loss for language\nimage pre-training. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages\n11975–11986.\nDuzhen Zhang, Yahan Yu, Jiahua Dong, Chenxing Li,\nDan Su, Chenhui Chu, and Dong Yu. 2024a. Mm-\nllms: Recent advances in multimodal large language\nmodels. arXiv preprint arXiv:2401.13601.\nTianjun Zhang, Shishir G Patil, Naman Jain, Sheng\nShen, Matei Zaharia, Ion Stoica, and Joseph E Gon-\nzalez. 2024b. Raft: Adapting language model to do-\nmain specific rag. arXiv preprint arXiv:2403.10131.\nPenghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren\nWang, Yunteng Geng, Fangcheng Fu, Ling Yang,\nWentao Zhang, and Bin Cui. 2024.\nRetrieval-\naugmented generation for ai-generated content: A\nsurvey. arXiv preprint arXiv:2402.19473.\nRuochen Zhao, Hailin Chen, Weishi Wang, Fangkai\nJiao, Xuan Long Do, Chengwei Qin, Bosheng\nDing, Xiaobao Guo, Minzhi Li, Xingxuan Li, et al.\n2023. Retrieving multimodal information for aug-\nmented generation:\nA survey.\narXiv preprint\narXiv:2303.10868.\nJunjie Zhou, Zheng Liu, Ze Liu, Shitao Xiao, Yueze\nWang, Bo Zhao, Chen Jason Zhang, Defu Lian, and\nYongping Xiong. 2024. Megapairs: Massive data\nsynthesis for universal multimodal retrieval. arXiv\npreprint arXiv:2412.14475.\n\nA\nKeypoints Extraction Details\nAs illustrated in Fig 8, we begin by extracting\nkeypoints from text-chart pairs. Each keypoint\nrepresents an atomic unit that encapsulates a spe-\ncific conclusive statement. The detailed extraction\nmethodology is described in Appendix G.\nFollowing the initial extraction, as shown in\nFig 9, we implement a two-stage filtering pro-\ncess (preliminary screening by GPT-4o followed\nby Crossmodal Verification) to categorize the key-\npoints into two distinct sets:\n• Text-only keypoints (KT ): information exclu-\nsively present in textual form\n• Chart-only\nkeypoints\n(KC):\ninformation\nuniquely extractable from chart visualization\nThe detailed filtering methodology is provided\nin prompts 20 and 21.\nB\nCrossmodal Vertification Algorithms\nAlgorithm 2 presents a robust verification mech-\nanism for text-only keypoint identification. The\nalgorithm validates keypoints through Crossmodal\nVertification, confirming a keypoint as text-only\nwhen text-only queries can yield the correct an-\nswer.\nAlgorithm 2: Text-only Keypoint Verification\nInput\n:KT : Text-only keypoints set, kt\ni ∈KT\nT: text chunks, ti ∈T\nC: chart set, ci ∈C\nV : chart info set, vi ∈V\nOutput :Updated KT with verification status\n1 for kt\ni ∈KT do\n2\nqt\ni ←LLM(kt\ni)\n3\n˜kt\ni ←LLM(qt\ni, ti)\n4\n˜kc\ni ←V LM(qt\ni, ci, vi)\n// Status determination\n5\nif ˜kt\ni = kt\ni and ˜kc\ni ̸= kt\ni then\n6\nStatus(kt\ni) ←Retain\n7\nelse\n8\nStatus(kt\ni) ←Drop\n9\nend\n10 end\n11 return KT\nAlgorithm 3 implements a symmetric verifica-\ntion approach for chart-only keypoint detection.\nThrough inverse validation logic, it confirms key-\npoints as chart-only when only chart-only queries\ncan produce the correct answer.\nAlgorithm 3: Chart-only Keypoint Verification\nInput\n:KC: Chart-only keypoints set, kc\ni ∈KC\nT: text chunks, ti ∈T\nC: chart set, ci ∈C\nV : chart info set, vi ∈V\nOutput :Updated KC with verification status\n1 for kc\ni ∈KC do\n2\nqc\ni ←LLM(kc\ni )\n3\n˜kt\ni ←LLM(qc\ni , ti)\n4\n˜kc\ni ←V LM(qc\ni , ci, vi)\n// Status determination\n5\nif ˜kc\ni = kc\ni and ˜kt\ni ̸= kc\ni then\n6\nStatus(kc\ni ) ←Retain\n7\nelse\n8\nStatus(kc\ni ) ←Drop\n9\nend\n10 end\n11 return KC\nC\nQuestion-Answering Pair Generation\nSingle-Point Text-only QA\nTo generate single-\npoint text-only question-answer pairs, we propose a\nsimplified variant of the cross-document QA gener-\nation process. As shown in Algorithm 4, the gener-\nation process consists of three main steps. First, we\nrandomly select a text keypoint from the document\ncollection that contains a complete, self-contained\nfact or statement. This approach focuses on validat-\ning discrete factual statements contained within a\nsingle text keypoint. The algorithm then leverages\nGPT-4o to generate appropriate question-answer\npairs based solely on the selected text keypoint and\nits context. This simplified approach ensures that\nthe generated QA pairs require only single-hop rea-\nsoning, making them suitable for evaluating basic\nreading comprehension and fact extraction capabil-\nities.\nAlgorithm 4: Single-Point Text-only QA\nInput\n:Document set D;\nText keypoint set KT\nOutput :Question-Answer pair (q, a)\n// Step 1: Select text keypoint\n1 Select kt\ni ∈KT from document da ∈D\n2 ti ←corresponding text block in da\n// Step 2: Validate single-point constraint\n3 Assert kt\ni contains complete fact or statement\n// Step 3: Generate QA pair\n4 (q, a) ←MLLM(kt\ni, ti)\n5 return (q, a)\nSingle-Point Chart-only QA\nFor generating\nchart-focused question-answer pairs, we introduce\na single-point variant that specializes in visual data\ncomprehension. Algorithm 5 begins by selecting\n\nFigure 8: Demonstration of extracting atomic information units from text-chart pairs using structured prompts.\nFigure 9: Illustration of the keypoints classification process using GPT-4o screening and Crossmodal Verification.\n\na chart keypoint that represents a discrete observa-\ntion from a visual element, such as a specific trend,\ncomparison, or data point. Unlike Algorithm 4\nwhich processes textual information, this approach\nextracts both the chart content and its correspond-\ning numerical value to capture the complete visual\ncontext. The algorithm employs GPT-4o to gener-\nate QA pairs that specifically test chart comprehen-\nsion skills, ensuring that each question-answer pair\nis grounded in visual data interpretation without\nrequiring cross-reference to textual content.\nAlgorithm 5: Single-Point Chart-only QA\nInput\n:Document set D;\nChart keypoint set KC\nOutput :Question-Answer pair (q, a)\n// Step 1: Select chart keypoint\n1 Select kc\ni ∈KC from document da ∈D\n2 ci ←chart content in da\n3 vi ←corresponding value in da\n// Step 2: Validate single-point constraint\n4 Assert kc\ni represents discrete chart observation\n// Step 3: Generate QA pair\n5 (q, a) ←MLLM(kc\ni , ci, vi)\n6 return (q, a)\nIntra-Document Text-only QA\nAs illustrated in\nAlgorithm 6 introduces a systematic approach to\nconstructing QA pairs that capture document-level\nsemantic relationships. The algorithm first selects\na primary text keypoint and its associated context,\nthen identifies another relevant text keypoint from\nthe same document to establish intra-document con-\nnections. This design ensures that the generated\nquestions necessitate the integration of informa-\ntion from multiple parts of the document, testing\na system’s ability to perform document-level rea-\nsoning and information synthesis. The final gen-\neration step employs GPT-4o to create questions\nthat effectively evaluate comprehensive document\nunderstanding capabilities.\nIntra-Document Chart-only QA\nBuilding upon\nour text-only approach, we propose an algorithm\nthat focuses on reasoning across multiple chart el-\nements within a single document. Algorithm 7\npresents a structured methodology for generating\nquestions that require the synthesis of information\nfrom related visual components. The algorithm\ninitiates by selecting a primary chart keypoint and\nextracts its corresponding visual features, then iden-\ntifies semantically related chart elements within\nthe same document to establish comprehensive vi-\nsual reasoning chains. This architecture enables\nAlgorithm 6: Intra-document Text-only QA\nInput\n:Document set D;\nText keypoint set KT\nOutput :Question-Answer pair (q, a)\n// Step 1: Select primary text keypoint\n1 Select kt\ni ∈KT from document da ∈D\n2 ti ←corresponding text block in da\n// Step 2: Retrieve intra-document text\nkeypoint\n3 KT\nr ←{kt ∈KT |kt from da}\n4 Select kt\nj ∈KT\nr\n5 tj ←corresponding text block in da\n// Step 3: Generate QA pair\n6 (q, a) ←MLLM(kt\ni, kt\nj, ti, tj)\n7 return (q, a)\nthe generation of questions that assess a system’s\ncapability to integrate and reason over multiple vi-\nsual representations while maintaining document-\nlevel consistency. The generation process leverages\nGPT-4o to construct questions that effectively eval-\nuate sophisticated chart comprehension and cross-\nreference abilities.\nAlgorithm 7: Intra-document Chart-only QA\nInput\n:Document set D;\nChart keypoint set KC\nOutput :Question-Answer pair (q, a)\n// Step 1: Select primary chart keypoint\n1 Select kc\ni ∈KC from document da ∈D\n2 (ci, vi) ←(c, ψ(c)) where c ∈da\n// Step 2: Retrieve intra-document chart\nkeypoint\n3 KC\nr ←{kc ∈KC|kc from da}\n4 Select kc\nj ∈KC\nr\n5 (cj, vj) ←(c, ψ(c)) where c ∈da\n// Step 3: Generate QA pair\n6 (q, a) ←MLLM(kc\ni , kc\nj, ci, vi, cj, vj)\n7 return (q, a)\nIntra-Document Text-Chart QA\nTo further en-\nhance the document-level reasoning capabilities,\nwe introduce an algorithm that bridges the gap be-\ntween textual and visual content within individual\ndocuments. Algorithm 8 establishes a novel ap-\nproach by first selecting chart-specific keypoints\nand then retrieving semantically related textual de-\nscriptions from the same document. This design\nfacilitates the generation of questions that require\njoint understanding of both modalities, particularly\nfocusing on how charts and their contextual textual\nexplanations complement each other. Through se-\nmantic retrieval between chart and text keypoints,\nthe algorithm ensures that the generated questions\ncapture authentic Crossmodal relationships while\nmaintaining document coherence. The generation\n\nprocess employs GPT-4o to synthesize questions\nthat evaluate systems’ ability to perform integrated\nreasoning over both visual and textual information\nsources.\nAlgorithm 8: Intra-document Text-Chart QA\nInput\n:Document set D;\nChart keypoint set KC;\nText keypoint set KT\nOutput :Question-Answer pair (q, a)\n// Step 1: Select chart keypoint\n1 Select kc\ni ∈KC from document da ∈D\n2 (ci, vi) ←(c, ψ(c)) where c ∈da\n// Step 2: Retrieve intra-document text\nkeypoint\n3 KT\nr ←{kt ∈KT |kt from da}\n4 kt\nj ←kt ∈KT\nr Similarity(kc\ni , kt)\n5 tj ←corresponding text block in da\n// Step 3: Generate QA pair\n6 (q, a) ←MLLM(kc\ni , kt\nj, ci, vi, tj)\n7 return (q, a)\nInter-Document Text-only QA\nTo extend our\nintra-document approach to a broader context, we\ndevelop an algorithm that enables reasoning across\ndifferent documents. Algorithm 9 introduces a sys-\ntematic methodology for generating questions that\nrequire the integration of information from multiple\nsource documents. The algorithm first selects a pri-\nmary text keypoint from one document, then identi-\nfies semantically related text content from a differ-\nent document, thereby establishing cross-document\nconnections. This design facilitates the generation\nof questions that assess a system’s ability to synthe-\nsize information across document boundaries while\nmaintaining logical coherence. The generation pro-\ncess employs GPT-4o to create questions that ef-\nfectively evaluate comprehensive cross-document\nunderstanding and reasoning capabilities.\nAlgorithm 9: Inter-document Text-only QA\nInput\n:Document set D;\nText keypoint set KT\nOutput :Question-Answer pair (q, a)\n// Step 1: Select primary text keypoint\n1 Select kt\ni ∈KT from document da ∈D\n2 ti ←corresponding text block in da\n// Step 2: Retrieve cross-document text\nkeypoint\n3 KT\nr ←{kt ∈KT |kt from db ∈D, db ̸= da}\n4 Select kt\nj ∈KT\nr\n5 tj ←corresponding text block in db\n// Step 3: Generate QA pair\n6 (q, a) ←MLLM(kt\ni, kt\nj, ti, tj)\n7 return (q, a)\nInter-Document Chart-only QA\nTo further ad-\nvance cross-document reasoning capabilities, we\npresent an algorithm that enables sophisticated\nanalysis across charts from different documents.\nAlgorithm 10 establishes a methodology for gener-\nating questions that require the synthesis of visual\ninformation spanning multiple documents. The\nalgorithm begins by selecting a primary chart key-\npoint and its visual features from one document,\nthen identifies related chart elements from a differ-\nent document to establish cross-document visual\nreasoning chains. This framework facilitates the\ngeneration of questions that evaluate a system’s\nability to integrate and reason over visual represen-\ntations across document boundaries while maintain-\ning semantic coherence. The generation process\nutilizes GPT-4o to create questions that effectively\nassess advanced chart comprehension and cross-\ndocument visual reasoning abilities.\nAlgorithm 10: Inter-document Chart-only QA\nInput\n:Document set D;\nChart keypoint set KC\nOutput :Question-Answer pair (q, a)\n// Step 1: Select primary chart keypoint\n1 Select kc\ni ∈KC from document da ∈D\n2 (ci, vi) ←(c, ψ(c)) where c ∈da\n// Step 2: Retrieve cross-document chart\nkeypoint\n3 KC\nr ←{kc ∈KC|kc from db ∈D, db ̸= da}\n4 Select kc\nj ∈KC\nr\n5 (cj, vj) ←(c, ψ(c)) where c ∈db\n// Step 3: Generate QA pair\n6 (q, a) ←MLLM(kc\ni , kc\nj, ci, vi, cj, vj)\n7 return (q, a)\nInter-Document Text-Chart QA\nExtending our\nCrossmodal reasoning framework beyond single\ndocuments, we propose an algorithm that enables\nsophisticated analysis across textual and visual con-\ntent from different documents. Algorithm 11 es-\ntablishes an advanced approach by selecting chart-\nspecific keypoints from one document and retriev-\ning semantically related textual descriptions from\nanother document. This architecture facilitates the\ngeneration of questions that require joint under-\nstanding of cross-document modalities, particularly\nexploring how charts and textual explanations from\ndifferent sources can be synthesized for compre-\nhensive reasoning. Through cross-document se-\nmantic retrieval between chart and text keypoints,\nthe algorithm generates questions that evaluate\nsystems’ ability to perform integrated reasoning\nacross both document boundaries and modality\n\ngaps. The generation process leverages GPT-4o\nto create questions that assess sophisticated cross-\ndocument visual-textual understanding capabili-\nties.\nAlgorithm 11: Inter-document Text-Chart QA\nInput\n:Document set D;\nChart keypoint set KC;\nText keypoint set KT\nOutput :Question-Answer pair (q, a)\n// Step 1: Select chart keypoint\n1 Select kc\ni ∈KC from document da ∈D\n2 (ci, vi) ←(c, ψ(c)) where c ∈da\n// Step 2: Retrieve cross-document text\nkeypoint\n3 KT\nr ←{kt ∈KT |kt from db ∈D, db ̸= da}\n4 kt\nj ←kt ∈KT\nr Similarity(kc\ni , kt)\n5 tj ←corresponding text block in db\n// Step 3: Generate QA pair\n6 (q, a) ←MLLM(kc\ni , kt\nj, ci, vi, tj)\n7 return (q, a)\nD\nChart-MRAG Bench Cases\nTo illustrate the diverse chart categories in Chart-\nMRAG Bench, we present representative examples\nas shown in Figure 10.\nWe categorize the question-answering pairs in\nChart-MRAG into eight distinct categories, as sum-\nmarized in Table 5, encompassing various combi-\nnations of single-point, intra-document, and inter-\ndocument scenarios across text-only, chart-only,\nand text-chart contexts. These categories are il-\nlustrated through representative examples: Single-\nPoint Text-Only QA (Fig. 12), Single-Point Chart-\nOnly QA (Fig. 13), Intra-Document Text-Only\nQA (Fig. 14), Intra-Document Chart-Only QA\n(Fig. 15), Intra-Document Text-Chart QA (Fig. 16),\nInter-Document Text-Only QA (Fig. 17), Inter-\nDocument Chart-Only QA (Fig. 18), and Inter-\nDocument Text-Chart QA (Fig. 19).\nE\nRetrieval Setup and Metrics\nE.1\nRetrieval Setup\nFor retrieval system, we designed three distinct\nconfigurations to evaluate different approaches to\nmultimodal information retrieval:\nUnified Multimodal Embedding and Single\nVector Store.\nWe directly embedded charts\nand text into a unified embedding space using\nvision-language models CLIP, JINA-CLIP, and\nSigLIP. This approach maps all content to same-\ndimensional vectors in a single vector store, en-\nabling cross-modal matching between queries and\ndocuments regardless of their original modality.\nMultimodal Embeddings and Combined Vector\nStores. In this approach, charts are first converted\nto text summaries using GPT-4o. Both these sum-\nmaries and PDF-extracted text are then embedded\nusing sparse BM25 and dense embedding models\nBGE-M3-base/large, E5-base/large into their re-\nspective vector stores. Similarity search in these\nembedding spaces retrieves relevant documents\nacross both modalities.\nMultimodal Embeddings and Separate Vector\nStores. This approach maintains distinct embed-\nding spaces for different modalities, leveraging spe-\ncialized models for optimal representation. Charts\nare encoded using vision-language models (CLIP,\nJINA-CLIP, SigLIP), while textual content is pro-\ncessed through both sparse retrieval (BM25) and\ndense embedding models (BGE-M3-base/large, E5-\nbase/large). The retrieval process operates in par-\nallel across separate vector stores, with the final\nresults aggregated using a weighted combination\nscheme.\nE.2\nRetrieval Metrics\nWe segment text into semantic chunks with an av-\nerage length of 24.97 words, while treating each\nchart as an individual retrieval unit. We employ Re-\ncall@5 and Recall@10 as primary retrieval metrics.\nTo ensure balanced representation, we implement a\ntext-to-chart ratio of 3:2 in the final retrieval results.\nGiven that Chart-MRAG bench primarily con-\nsists of multi-hop questions requiring both textual\nand visual information, the comprehensive retrieval\nof all relevant sources is crucial for accurate an-\nswers. We employ Recall@5 and Recall@10 to\nevaluate the effectiveness and efficiency of the re-\ntrieval stage.\nMultimodal Recall. We introduce a Multimodal\nRAG Retrieval Recall metric to evaluate the effec-\ntiveness of crossmodal retrieval process. For tex-\ntual content, we perform sentence-level retrieval,\nwhile for charts, we treat each visualization as an\nindividual reference unit. The Recall is formally\ndefined as\nRecall = 1\nn\nn\nX\ni=1\n1(M(Gi, R)),\n(5)\nwhere n is the total number of ground truth ref-\nerences (including both text chunks and charts),\nGi denotes the i-th ground truth reference, R =\n\nFigure 10: Representative visualization categories from Chart-MRAG Bench, showcasing temporal trend analysis\n(line charts), geospatial data visualization (choropleth maps), categorical comparisons (bar charts), compositional\nanalysis (stacked bars), and integrated text-chart representations. The diversity of these examples demonstrates\nthe comprehensive scope of Chart-MRAG Bench in representing complex statistical information across multiple\ndomains and visualization paradigms.\nSource-Constrained and Modality-Constrained Question-Answer Categories\nSingle-Point Text-Only\nQuestions that require reasoning about an individual textual keypoint (kt\ni ∈KT ), focusing\non discrete factual validation within a single text segment.\nSingle-Point Chart-Only\nQuestions centered on an individual chart-only keypoint (kc\ni ∈KC), examining specific\ndata points or visual elements within a single chart.\nIntra-Document Text-Only\nQuestions that necessitate integrative reasoning across multiple textual keypoints (kt\ni, kt\nj ∈\nKT ) within the same document (di ∈D).\nIntra-Document Chart-Only Questions requiring comparative analysis of multiple chart-only keypoints (kc\ni , kc\nj ∈KC)\nfrom a single document (di ∈D).\nIntra-Document Text-Chart\nQuestions involving cross-modal reasoning between textual and chart-only keypoints\n(kt\ni ∈KT , kc\nj ∈KC) within the same document (di ∈D).\nInter-Document Text-Only\nQuestions demanding associative reasoning between textual keypoints (kt\ni, kt\nj ∈KT )\nfrom distinct documents (di, dj ∈D, i ̸= j).\nInter-Document Chart-Only Questions requiring comparative analysis of chart-only keypoints (kc\ni , kc\nj ∈KC) across\ndifferent documents (di, dj ∈D, i ̸= j).\nInter-Document Text-Chart\nQuestions involving cross-modal and cross-document reasoning, integrating textual and\nchart keypoints (kt\ni ∈KT , kc\nj ∈KC) from different documents (di, dj ∈D, i ̸= j).\nTable 5: Taxonomy of question-answer pairs in Chart-MRAG, categorized by source constraints (Single-Point/Intra-\nDocument/Inter-Document) and modality constraints (Text-only/Chart-only/Text-Chart).\n\n{R1, R2, . . . , Rk} represents the set of retrieved\nreferences, M(Gi, R) is a boolean function that\nreturns true if (1) for textual references, all con-\nstituent sentences in Gi are found in at least one\nreference in R, or (2) for chart references, the ex-\nact chart is present in R, and 1(·) is the indicator\nfunction.\nThis metric assesses the crossmodal align-\nment between retrieved and ground truth refer-\nences, where successful retrieval is determined by\nmodality-specific criteria: sentence-level matching\nfor text and exact matching for charts.\nF\nText-Over-Visual Modality Bias Case\nSection 11 presents a comprehensive analysis of\nText-Over-Visual Modality Bias, revealing a sys-\ntematic preference for text-based processing across\ndifferent model scales. Our experiments demon-\nstrate that multimodal language models consis-\ntently favor text-only responses, even in scenarios\nwhere visual elements (particularly charts) contain\nmore precise and relevant information. This bias\nraises important questions about the effective in-\ntegration of multiple modalities in current AI sys-\ntems.\nNotably, our investigation reveals a clear cor-\nrelation between model scale and the ability to\nhandle multimodal information effectively. Larger\nMLLMs, particularly GPT-4o, demonstrate sophis-\nticated capabilities in detecting and managing infor-\nmation redundancy across modalities, proactively\nacknowledging such overlaps in 23% of their re-\nsponses. This behavior suggests a more nuanced\nunderstanding of the complementary nature of dif-\nferent information sources.\nIn contrast, smaller models exhibit significant\nlimitations in processing multimodal inputs. For in-\nstance, SAIL-VL-2B (2B parameters) shows a stark\ninability to integrate information across modali-\nties, highlighting the critical role of model scale in\nachieving effective multimodal reasoning.\nG\nModel Prompts\nCHARGE framework encompasses multiple stages,\neach guided by specific prompts designed to facili-\ntate different aspects of the process. We detail these\nprompts according to their respective stages:\nIn the Extract Keypoints stage, we employ two\nspecialized prompts: one for document keypoint\nextraction (Fig. 20) and another for chart keypoint\nextraction (Fig. 21). These prompts are designed\nto identify and extract crucial information points\nfrom both textual and visual components.\nThe Crossmodal Verification stage utilizes two\nkey prompts: a keypoint classification prompt\n(Fig. 22) and a cross-modal information verifica-\ntion protocol (Fig. 23). These prompts work in\ntandem to ensure the consistency and accuracy of\ninformation across different modalities.\nFor Question-Answer Pair Generation, we im-\nplement two distinct protocols: a single-point gen-\neration protocol (Fig. 24) for straightforward ques-\ntions, and a multi-hop generation protocol (Fig. 25)\nfor complex questions requiring multiple reasoning\nsteps.\nThe Response stage features two prompts: one\ndesigned for generating responses without retrieved\ninformation (Fig. 26), and another for responses\nincorporating retrieved information (Fig. 27). This\ndual approach enables flexible response generation\nbased on available context.\nFinally, the Evaluation stage employs two met-\nric calculation prompts: one for assessing correct-\nness (Fig. 28) and another for measuring coverage\n(Fig. 29). These prompts ensure comprehensive\nevaluation of the generated responses.\n\nFigure 11: A Sample Case of Text-Over-Visual Modality Bias\n\nFigure 12: A sample case of single-point text-only question answering.\n\nFigure 13: A sample case of single-point chart-only question answering.\n\nFigure 14: A sample case of intra-document text-only question answering.\n\nFigure 15: A sample case of intra-document chart-only question answering.\n\nFigure 16: A sample case of intra-document text-chart question answering.\n\nFigure 17: A sample case of inter-document text-only question answering.\n\nFigure 18: A sample case of inter-document chart-only question answering.\n\nFigure 19: A sample case of inter-document text-chart question answering.\n\nFigure 20: Document keypoints extraction prompt details.\n\nFigure 21: Chart keypoints extraction prompt details.\n\nFigure 22: Keypoint classification task prompt details.\n\nFigure 23: Cross-modal information verification protocol prompt details.\n\nFigure 24: Single-point question-answer generation protocol prompt details.\n\nFigure 25: Multi-hop question-answer generation protocol prompt details.\n\nFigure 26: Response without retrieved information prompt details.\nFigure 27: Response with retrieved information prompt details.\n\nFigure 28: Correctness metric calculation prompt details.\n\nFigure 29: Coverage metric calculation prompt details.\n",
  "metadata": {
    "source_path": "papers/arxiv/Benchmarking_Multimodal_RAG_through_a_Chart-based_Document\n__Question-Answering_Generation_Framework_c3898b16fa4f2e28.pdf",
    "content_hash": "c3898b16fa4f2e2828895749bdf7d2b0272a24bc701afbabc1325418a14a6f17",
    "arxiv_id": null,
    "title": "Benchmarking_Multimodal_RAG_through_a_Chart-based_Document\n__Question-Answering_Generation_Framework_c3898b16fa4f2e28",
    "author": "",
    "creation_date": "D:20250221020539Z",
    "published": "2025-02-21T02:05:39",
    "pages": 36,
    "size": 6391377,
    "file_mtime": 1740346980.9216769
  }
}