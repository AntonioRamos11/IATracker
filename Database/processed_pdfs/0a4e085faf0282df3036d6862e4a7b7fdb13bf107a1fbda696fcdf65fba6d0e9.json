{
  "text": "ReFocus: Reinforcing Mid-Frequency and Key-Frequency Modeling for\nMultivariate Time Series Forecasting\nGuoqi Yu 1 Yaoming Li 2 Juncheng Wang 1 Xiaoyu Guo 2 Angelica I. Aviles-Rivero 3 Tong Yang 2\nShujun Wang 1\nAbstract\nRecent advancements have progressively incorpo-\nrated frequency-based techniques into deep learn-\ning models, leading to notable improvements in\naccuracy and efficiency for time series analysis\ntasks. However, the Mid-Frequency Spectrum\nGap in the real-world time series, where the\nenergy is concentrated at the low-frequency re-\ngion while the middle-frequency band is negligi-\nble, hinders the ability of existing deep learning\nmodels to extract the crucial frequency informa-\ntion. Additionally, the shared Key-Frequency\nin multivariate time series, where different time\nseries share indistinguishable frequency patterns,\nis rarely exploited by existing literature. This\nwork introduces a novel module, Adaptive Mid-\nFrequency Energy Optimizer, based on convolu-\ntion and residual learning, to emphasize the signif-\nicance of mid-frequency bands. We also propose\nan Energy-based Key-Frequency Picking Block\nto capture shared Key-Frequency, which achieves\nsuperior inter-series modeling performance with\nfewer parameters. A novel Key-Frequency En-\nhanced Training strategy is employed to further\nenhance Key-Frequency modeling, where spec-\ntral information from other channels is randomly\nintroduced into each channel. Our approach ad-\nvanced multivariate time series forecasting on\nthe challenging Traffic, ECL, and Solar bench-\nmarks, reducing MSE by 4%, 6%, and 5% com-\npared to the previous SOTA iTransformer. Code\nis available at this GitHub Repository: https:\n//github.com/Levi-Ackman/ReFocus.\n1Department of Biomedical Engineering, The Hong Kong Poly-\ntechnic University, Hong Kong SAR, China 2Data Structures Lab-\noratory, School of Computer Science, Peking University, Beijing,\nChina. 3DAMTP, University of Cambridge, Cambridge, UK. Cor-\nrespondence to:Shujun Wang<shu-jun.wang@polyu.edu.hk>.\nPreliminary Work.\n1. Introduction\nAccurate forecasting of time series offers reference for\ndecision-making across various domains (Lim & Zohren,\n2021; Torres et al., 2021), including weather (Du et al.,\n2023), economics (Oreshkin et al., 2020), and energy (Dong\net al., 2023; Liu et al., 2022b). Especially, long-term multi-\nvariate time series forecasting (LMTSF) emerges as a promi-\nnent area of interest in academic research (Wang et al.,\n2024c; Wen et al., 2022) and industrial applications (Cirstea\net al., 2022), offering the advantage of capturing complex\ninterdependencies and trends across multiple variables.\nRecently, the powerful representation capabilities of neu-\nral networks, such as Multi-Layer perception (MLPs) (Yi\net al., 2023c; Han et al., 2024), Transformers (Zhou et al.,\n2022c; Nie et al., 2023), and Temporal Convolution Net-\nwork (TCNs) (Eldele et al., 2024; Liu et al., 2022a), have\nsignificantly advanced deep learning-based LMTSF. These\napproaches can be broadly categorized into two/three folds:\ntime-domain-based (Han et al., 2024; Nie et al., 2023; Liu\net al., 2022a) and frequency-domain-based (Yi et al., 2023c;\nZhou et al., 2022c; Eldele et al., 2024) methods, or mixed\ntime & frequency. Time-domain methods are intuitive, han-\ndling nonlinearity and non-periodic signals directly from\nthe raw sequence (Li et al., 2023) using Transformers (Zhou\net al., 2022a), TCN (Donghao & Xue, 2024), or MLP (Wang\net al., 2024a). The latest study (Yi et al., 2024) highlights\nthat time-domain forecasters face challenges such as vulner-\nability to high-frequency noise, and computational ineffi-\nciencies. While frequency-domain-based methods usually\ntransform the time-domain data to the frequency spectrum\nby Fast Fourier transform (FFT) (Yi et al., 2023a). Then\nother operations (Self-attention (Zhou et al., 2022c), Lin-\near mapping (Xu et al., 2024a; Yi et al., 2023c), etc.) are\nemployed to extract frequency information. These meth-\nods benefit from advantages such as computational effi-\nciency (Fan et al., 2024; Xu et al., 2024a), periodic patterns\nextracting (Wu et al., 2023; Dai et al., 2024), and energy\ncompaction (Yi et al., 2023c;b).\nHowever, existing frequency-domain-based forecasters usu-\nally face TWO significant challenges when dealing with\nreal-world long-term time series: the Mid-Frequency Spec-\n1\narXiv:2502.16890v1  [cs.LG]  24 Feb 2025\n\nReFocus: Reinforcing Mid-Frequency and Key-Frequency Modeling for Multivariate Time Series Forecasting\nFigure 1: The Mid-Frequency Spectrum Gap and the shared Key-Frequency (high similarity in frequency spectra across variables) on\nWeather dataset. VPmax means ‘Maximum Vapor Pressure’ and VPact means ‘Actual Vapor Pressure’.\ntrum Gap and the shared Key-Frequency modeling.\n• Mid-Frequency Spectrum Gap (Figure 1 Red box)\nrefers to a condition where the energy of the spec-\ntrum is concentrated in the low-frequency regions, re-\nsulting in the mid-frequency band being negligible.\nLow-frequency components capture long-term trends,\noften contributing to mean shifts when overly concen-\ntrated (Stock & Watson, 2002; Granger & Newbold,\n1974; Chatfield & Xing, 2019). So this Mid-Frequency\nSpectrum Gap will introduce Nonstationarity (Cheng\net al., 2015; Liu et al., 2022c), where the mean and\nvariance of time series change over time, and make\ntime series less predictable. Furthermore, such uneven\nenergy distribution challenges existing deep-learning\nmodels to extract critical patterns (Tishby & Zaslavsky,\n2015; Xu et al., 2024b; Rahaman et al., 2019). So,\naddressing this Mid-Frequency Spectrum Gap is cru-\ncial for enhancing the feature extraction capabilities of\ndeep learning-based forecasters (Park et al., 2019; Bai\net al., 2018; Guo et al., 2019). Currently, widely used\nmethods for processing spectra, such as Filters (As-\nselin, 1972), and RevIN (Kim et al., 2022; Liu et al.,\n2022c)—a technique previously applied to address non-\nstationarity—are not effective in resolving this issue.\nConversely, convolution with residual connections has\neffectively handled spectral information (Can & Timo-\nfte, 2018; Chakraborty & Trehan, 2021), providing a\npotential solution.\n• Meanwhile, the second challenge: the shared Key-\nFrequency Modeling (Figure 1 Pink box) has the\ndisadvantage that distinct time series can exhibit in-\ndistinguishable frequency patterns, potentially leading\nto challenges in accurately differentiating and analyz-\ning individual series within a multivariate context (Yu\net al., 2023; Piao et al., 2024). However, existing ap-\nproaches have largely overlooked this critical charac-\nteristic. Meanwhile, energy, which is the square of the\namplitude of the spectrum, is proven as an effective\ntool for identifying certain frequency patterns in the\nmultivariate case (B´ogalo et al., 2024; Chekroun &\nKondrashov, 2017; Sundararajan & Bruce, 2023).\nBased on the above observations, this work mainly addresses\ntwo critical questions: (1) How can the Mid-Frequency\nSpectrum Gap be resolved to achieve a more evenly dis-\npersed spectrum?\n(2) How can inter-series dependen-\ncies be efficiently modeled by leveraging the shared Key-\nFrequency? To tackle challenge 1, we propose the ‘Adaptive\nMid-Frequency Energy Optimizer’ (AMEO), a convolution-\nand residual learning-based solution. It adaptively scales\nthe frequency spectrum by assigning higher scaling factors\nto lower frequencies, thereby dispersing the spectrum. To\naddress challenge 2, For the second challenge, we introduce\nthe ‘Energy-based Key-Frequency Picking Block (EKPB)’,\nwhich features fewer parameters and faster inference speeds\ncompared to the Transformer Encoder (Liu et al., 2024b)\nand MLP-Mixer (Chen et al., 2023). EKPB extracts shared\nfrequency information across channels effectively. We also\npropose a ‘Key-Frequency Enhanced Training’ strategy\n(KET) which incorporates spectral information from other\nchannels during training to enhance extraction of shared\nKey-Frequency that may not be included in the training set.\nOur contributions are summarized as follows.\n• We theoretically and empirically demonstrate that exist-\ning RevIN and high/low-pass filters fail to address the\nMid-Frequency Spectrum Gap. We propose AMEO,\na novel approach based on convolution and residual\nlearning that significantly enhances mid-frequency fea-\nture extraction.\n• We propose EKPB to capture shared Key-Frequency\nacross channels, which achieves superior inter-series\nmodeling capacity with lower parameters.\n• We propose KET, where spectral information from\nother channels is randomly introduced into each chan-\nnel, to enhance the extraction of the shared Key-\nFrequency.\n• Our approach outperforms the previous SOTA iTrans-\nformer by reducing MSE by 4%, 6%, and 5% on the\nchallenging Traffic, ECL, and Solar datasets, respec-\ntively, establishing new benchmarks in multivariate\ntime series forecasting.\n2\n\nReFocus: Reinforcing Mid-Frequency and Key-Frequency Modeling for Multivariate Time Series Forecasting\n2. Related work\nAdvancement in Recent Deep Learning-based Time Se-\nries Forecasting\nRecent advancements in deep learning-\nbased time series forecasting can be broadly categorized\ninto three key areas: (1) the application of sequential mod-\nels to time series data, (2) the tokenization of time series,\nand (3) the exploration of intrinsic patterns within time se-\nries. Efforts in the first area have focused on deploying\nvarious architectures for time series forecasting, includ-\ning Transformer (Wu et al., 2021; Wang et al., 2024b),\nMamba (Ahamed & Cheng, 2024; Wang et al., 2024d),\nMLPs (Wang et al., 2024a; Das et al., 2023; Yu et al., 2024a),\nRNNs (Lin et al., 2023), Graph Neural Networks (Shang\net al., 2024), TCNs (Wang et al., 2023), and even Large Lan-\nguage Models (LLMs) (Jin et al., 2024; Liu et al., 2024d;c).\nThe second direction has witnessed groundbreaking devel-\nopments, particularly in Patch Embedding (Nie et al., 2023)\nand Variate Embedding (Liu et al., 2024b). The final area ex-\nplores modeling complex relationships, including the inter-\nseries dependencies (Ng et al., 2022; Chen et al., 2024),\nthe dynamic evolution within a sequence (Du et al., 2023;\nZhang et al., 2022), or both (Yu et al., 2024b; Liu et al.,\n2024a).\nTime Series Modeling with Frequency\nFrequency as\na key feature of time series data, has inspired numerous\nworks (Yi et al., 2023a). FITS (Xu et al., 2024a) employs a\nsimple frequency-domain linear, getting results comparable\nto SOTA models with 10K parameters. Autoformer (Wu\net al., 2021) introduces the auto-correlation mechanism,\nleveraging FFT to improve self-attention. FEDformer (Zhou\net al., 2022c) further calculates attention weights from the\nspectrum of queries and keys. FiLM (Zhou et al., 2022b)\napplies Fourier analysis to preserve historical information\nwhile filtering out noise. FreTS (Yi et al., 2023c) incorpo-\nrates frequency-domain MLP to model both channel and\ntemporal dependencies. TimesNet (Wu et al., 2023) uti-\nlizes FFT to extract periodic patterns. FilterNet (Yi et al.,\n2024) proposes a filter-based method from the perspective\nof signal processing.\nHowever, they do not address the Mid-Frequency Spectrum\nGap and shared Key-Frequency modeling. In contrast, our\nmethod employs ‘Adaptive Mid-Frequency Energy Opti-\nmizer’ to improve mid-frequency feature extraction and\nintroduces ‘Energy-based Key-Frequency Picking Block’\nwith ‘Key-Frequency Enhanced Training’ strategy to cap-\nture shared Key-Frequency across channels.\n3. Methodology\n3.1. Problem Definition\nGiven a multivariate time series input X ∈RC×T , multi-\nvariate time series forecasting tasks are designed to predict\nits future F time steps ˆY ∈RC×F using past T steps. C is\nthe number of variates or channels.\n3.2. Preliminary Analysis\nThis section presents why RevIN (Kim et al., 2022; Liu et al.,\n2022c), High-pass, and Low-pass filters fail to address the\nMid-Frequency Spectrum Gap. Let the input univariate time\nseries be x(t) with length T and target y(t) with length F.\nDefinition 3.1 (Frequency Spectral Energy). The Fourier\ntransform of x(t), X(f), and its spectral energy EX(f) is\ngiven by:\nX(f) =\nT −1\nX\nt=0\nx(t)e−i2πft/T −1,\nf = 0, 1, . . . , T −1\nEX(f) = |X(f)|2.\n(1)\nImpact of RevIN on Frequency Spectrum\nDefinition 3.2 (Reversible Instance Normalization). Given\na forecast model f : RT →RF that generates a forecast\nˆy(t) from a given input x(t), RevIN is defined as:\nˆx(t) = x(t) −µ\nσ\n,\nt = 0, 1, . . . , T −1\nˆy(t) = f(ˆx(t)),\nˆy(t)rev = ˆy(t) · σ + µ,\nµ = 1\nT\nT −1\nX\nt=0\nx(t),\nσ =\nv\nu\nu\nt 1\nT\nT −1\nX\nt=0\n(x(t) −µ)2.\n(2)\nTheorem 3.3 (Frequency Spectrum after RevIN). The spec-\ntral energy of ˆx(t) (transformed using RevIN):\nE ˆ\nX(0) = 0,\nf = 0,\nE ˆ\nX(f) =\n\u0012 1\nσ\n\u00132\n|X(f)|2,\nf = 1, 2, . . . , T −1.\n(3)\nThe proof is in Appendix A.1. Theorem 3.3 suggests that\nRevIN scales the absolute spectral energy by σ2 but does\nnot affect its relative distribution except E ˆ\nX(0) = 0. Thus,\nRevIN preserves the relative spectral energy distribution\nand leaves the Mid-Frequency Spectrum Gap unresolved.\nHowever, our experiments still employ RevIN to ensure a\nfair comparison with other baselines.\nImpact of High- and Low-pass filter\nWe still define ˆx(t)\nto be the filtered (processed) signal, obtained by applying a\nfilter H(f) (High/Low-pass filter). The filter H(f) is 1 in\nthe passband (High/Low frequency) and 0 in the stopband\n(Middle frequency). So E ˆ\nX(f) = 0,\nE ˆ\nX ≤EX(f) for\nmiddle frequencies, which creates even larger gap.\n3.3. Overall Structure of The Proposed ReFocus\nIn this section, we elucidate the overall architecture of Re-\nFocus, depicted in Figure 2. We define frequency domain\n3\n\nReFocus: Reinforcing Mid-Frequency and Key-Frequency Modeling for Multivariate Time Series Forecasting\nFigure 2: General structure of ReFocus. ‘Adaptive Mid-Frequency Energy Optimizer (AMEO)’ enhances mid-frequency\ncomponents modeling, and ‘Energy-based Key-Frequency Picking Block’ (EKPB) effectively captures shared Key-Frequency\nacross channels\nprojection as D1 →D2 representing a projection from\ndimension D1 to D2 in the frequency domain (Xu et al.,\n2024a). Initially, we apply AMEO to the input X ∈RC×T ,\nyielding the processed spectrum Xam ∈RC×T . Next, we\nuse a projection T →D to transform Xam into the Vari-\nate Embedding Xem ∈RC×D (Liu et al., 2024b). Then,\nXem go through N EKPB to generate representation HN+1,\nwhich is projected to obtain final prediction ˆY .\nAdaptive Mid-Frequency Energy Optimizer\nBuilding\nupon the Preliminary Analysis, we propose a convolution-\nand residual learning-based solution to address the Mid-\nFrequency Spectrum Gap, which we denoted as AMEO.\nDefinition 3.4 (Adaptive Mid-Frequency Energy Optimizer).\nAMEO is defined as:\nˆx(t) = x(t) −β\nK\nK−1\nX\nk=0\n˜x(t + K −1 −k),\n˜x(t) =\n(\nx(t −( K\n2 + 1)),\nif K\n2 + 1 ≤t < T + K\n2 + 1,\n0,\nif 0 ≤t < K\n2 + 1 or T + K\n2 + 1 ≤t < T + K.\n(4)\nIt is equivalent to x = x −β · Conv(x). Conv is a 1D\nconvolution (Zero-padding at both ends, stride s = 1, ker-\nnel size K, with values initialized as\n1\nK ). β ∈R1 is a\nhyperparameter.\nTheorem 3.5 (Frequency Spectrum after AMEO). The spec-\ntral energy of ˆx(t) obtained using AMEO:\nE ˆ\nX(f) = |X(f)|2\n\n\n\n\n\n\n\n\n\n\n\n1 −β · 1\nK\nK−1\nX\nk=0\nei2πf( 3K\n2 −k−2)/T −1\n|\n{z\n}\nG(f)\n\n\n\n\n\n\n\n\n\n\n\n2\n(5)\nThe proof is in Appendix A.2.\nWe have E ˆ\nX(f) =\n|X(f)|2(1 −β · G(f))2. Generally, G(f) behaves as a\ndecay function, gradually reducing its value from One to\nZero. Such decay behavior makes AMEO relatively en-\nhances mid-frequency components, thus addressing the Mid-\nFrequency Spectrum Gap.\nEnergy-based Key-Frequency Picking Block\nIn each\nEKPB, the input Hi ∈RC×D(H1 = Xem) is first pro-\ncessed through an MLP to generate Hk\ni ∈RC×Q. Then,\nFFT is applied to get Hf\ni\n∈RC×(Q/2+1). For Hf\ni , we\ncalculate its energy, denoted as He\ni ∈RC×(Q/2+1). A\ncross-channel softmax is then applied to He\ni per frequency\nto obtain a probability distribution Hsoft\ni\n∈RC×(Q/2+1).\n4\n\nReFocus: Reinforcing Mid-Frequency and Key-Frequency Modeling for Multivariate Time Series Forecasting\nFigure 3: General process of the Key-Frequency Enhanced Training strategy (KET), where spectral information from\nother channels is randomly introduced into each channel, to enhance the extraction of the shared Key-Frequency.\nUsing Hsoft\ni\n, we select values from Hf\ni across channels\nfor each frequency, resulting in Kf\ni ∈R1×(Q/2+1), which\nrepresents the Shared Key-Frequency across all channels.\nThen iFFT is performed on Kf\ni to get Ki ∈R1×Q, fol-\nlowed by projection Q →D and repeating (C times) to get\nˆKi ∈RC×D. This ˆKi is point-wisely added to ˆ\nHi ∈RC×D\n, which is the projection of Hi using projection D →D.\nThen, an MLP and Add&Norm is applied to the result\nHK ∈RC×D to fuse inter-series dependencies informa-\ntion, and another MLP and Add&Norm is used to capture\nintra-series variations (Liu et al., 2024b). The output of each\nEKPB is ˆOi ∈RC×D, where Hi+1 = ˆOi.\n3.4. Key-Frequency Enhanced Training strategy\nIn real-world time series, certain channels often exhibit\nspectral dependencies, which may not be fully captured in\nthe training set, and the specific channels with such depen-\ndencies are also unknown (Geweke, 1984; Zhao & Shen,\n2024). So this work borrows insight from recent advance-\nment of mix-up in time series (Zhou et al., 2023; Ansari\net al., 2024), randomly introducing spectral information\nfrom other channels into each channel, to enhance the ex-\ntraction of the shared Key-Frequency, as in Figure 3. Given\na multivariate time series input X ∈RC×T and its ground-\ntruth Y ∈RC×F , we generate a pseudo sample pair:\nX′ = iFFT(FFT(X) + α · FFT(X[perm, :])),\nY ′ = iFFT(FFT(Y ) + α · FFT(Y [perm, :])).\n(6)\nα ∈RC×1 is a weight vector sampled from a normal dis-\ntribution, perm is a reshuffled channel index. Since FFT\nand iFFT are linear operations, this mix-up process can be\nequivalently simplified in the Time Domain:\nX′ = X + α · X[perm, :],\nY ′ = Y + α · Y [perm, :]\n(7)\nWe alternate training between real and synthetic data to\npreserve the spectral dependencies in real samples. This\ncombines the advantages of data augmentation, such as im-\nproved generalization, while mitigating potential drawbacks\nlike over-smoothing and training instability (Ryu et al.,\n2024; Alkhalifah et al., 2022).\n4. Experiments\n4.1. Experimental Settings\nThis section first introduces the whole experiment settings\nunder a fair comparison. Secondly, we illustrate the exper-\niment results by comparing ReFocus with the TEN well-\nacknowledged baselines. Further, we conducted an ablation\nstudy to comprehensively investigate the effectiveness of\nthe ‘Adaptive Mid-Frequency Energy Optimizer’ (AMEO),\n‘Energy-based Key-Frequency Picking Block’ (EKPB), and\n‘Key-Frequency Enhanced Training strategy’ (KET).\nTable 1: The Statistics of the eight datasets used in our\nexperiments.\nDatasets\nETTh1&2\nETTm1&2\nTraffic\nElectricity\nSolar Energy\nWeather\nVariates\n7\n7\n862\n321\n137\n21\nTimesteps\n17,420\n69,680\n17,544\n26,304\n52,560\n52,696\nGranularity\n1 hour\n5 min\n1 hour\n1 hour\n10 min\n10 min\nDatasets\nWe conduct extensive experiments on selected\nEight widely-used real-world multivariate time series fore-\ncasting datasets, including Electricity Transformer Temper-\nature (ETTh1, ETTh2, ETTm1, and ETTm2), Electricity,\nTraffic, Weather used by Autoformer (Wu et al., 2021), and\nSolar Energy datasets proposed in LSTNet (Lai et al., 2018).\nFor a fair comparison, we follow the same standard proto-\ncol (Liu et al., 2024b) and split all forecasting datasets into\ntraining, validation, and test sets by the ratio of 6:2:2 for the\nETT dataset and 7:1:2 for the other datasets. The character-\nistics of these datasets are shown in Table 1 (More can be\nfound in the Appendix).\nEvaluation protocol\nFollowing TimesNet (Wu et al.,\n2023), we use Mean Squared Error (MSE) and Mean Abso-\nlute Error (MAE) for the evaluation. We follow the same\n5\n\nReFocus: Reinforcing Mid-Frequency and Key-Frequency Modeling for Multivariate Time Series Forecasting\nevaluation protocol, where the input length is set as T = 96\nand the forecasting lengths F ∈{96, 192, 336, 720}. All\nthe experiments are conducted on a single NVIDIA GeForce\nRTX 4090 with 24G VRAM. The MSE loss function is uti-\nlized for model optimization. To foster reproducibility, we\nmake our code, and training scripts available in this GitHub\nRepository1. Full implementation details and other infor-\nmation are in Appendix B.\nBaseline setting\nWe carefully choose TEN well-\nacknowledged forecasting models as our baselines, includ-\ning 1) Transformer-based methods: iTransformer (Liu et al.,\n2024b), Crossformer (Zhang & Yan, 2023), PatchTST (Nie\net al., 2023); 2) Linear-based methods: TSMixer (Chen\net al., 2023), DLinear (Zeng et al., 2023); 3) TCN-based\nmethods: TimesNet (Wu et al., 2023), ModernTCN (Dong-\nhao & Xue, 2024); 4)Recent cutting-edge frequency-based\nmethods that discussed earlier: FilterNet (Yi et al., 2024),\nFITS (Xu et al., 2024a), FreTS (Yi et al., 2023c). These\nmodels represent the latest advancements in multivariate\ntime series forecasting and encompass all mainstream pre-\ndiction model types. The results of ModernTCN, FilterNet,\nFITS, and FreTS are taken from FilterNet (Yi et al., 2024)\nand other results are taken from iTransformer (Liu et al.,\n2024b).\n4.2. Experiment Results\nQuantitative comparison\nComprehensive forecasting\nresults are listed in Table 2. We leave full forecasting re-\nsults in APPENDIX to save place. It is quite evident that\nReFocus has demonstrated superior predictive performance\nacross all datasets, significantly outperforming the second-\nbest method. Especially, Compared to the previous SOTA\niTransformer, we have reduced the MSE by 4%, 6%, and\n5% on the three most challenging benchmarks: Traffic,\nECL, and Solar, respectively, indicating a significant break-\nthrough. These significant improvements indicate that the\nReFocus model possesses robust performance and broad\napplicability in multivariate time series forecasting tasks,\nespecially in tasks with a large number of channels, such as\nthe Solar Energy dataset (137 channels), ECL dataset (321\nchannels), and Traffic dataset (862 channels).\n4.3. Model Analysis\nAblation study of AMEO and KET\nTo evaluate the\ncontributions of each module in ReFocus, we performed\nablation studies on the ‘Adaptive Mid-Frequency Energy\nOptimizer (AMEO)’ and the ‘Key-Frequency Enhanced\nTraining (KET)’ strategy. The results are summarized in\nTable 3. Notably, integrating both modules achieves the\nbest performance, highlighting the effectiveness of their\n1https://github.com/Levi-Ackman/ReFocus\nsynergy. Additionally, each module delivers substantial\nimprovements over baseline models in most cases.\nFurther study of KET\nWe conducted further ablation\nstudies on the KET to demonstrate the importance of al-\nternate training between real and synthetic data. The ex-\nperimental results in Table 4 reveal that while training on\npseudo samples can partially enhance the model’s general-\nization performance on the test set, it also tends to cause\nover-smoothing and training instability on more complex\ndatasets, such as Solar Energy. In contrast, training on real\nand synthetic data alternatively (KET) improves generaliza-\ntion and mitigates over-smoothing and training instability by\npreserving the spectral dependencies of real samples. More\nAnalyses are in Appendix C.\nAblation study of different Key-Frequency Picking strat-\negy\nWe conducted an ablation study on various key-\nfrequency selection strategies. The evaluated methods in-\nclude Maximum-based, Minimum-based, and Softmax-\nbased random sampling strategies. Our experimental re-\nsults in Table 5 reveal that purely relying on Maximum\nor Minimum-based strategies may overlook certain critical\nKey-Frequency. In contrast, the random sampling strat-\negy based on a Softmax probabilistic distribution consis-\ntently achieved the best overall performance, particularly\non datasets with a larger number of channels and higher\ncomplexity—key challenges in multivariate time series fore-\ncasting.\nOutstanding inter-series modeling ability of the EKPB\nWe compared ‘Energy-based Key-Frequency Picking Block’\n(EKPB) with several well-established backbones, includ-\ning iTransformer (Liu et al., 2024b), TSMixer (Chen et al.,\n2023), and Crossformer (Zhang & Yan, 2023), which have\ndemonstrated exceptional performance in modeling inter-\nseries dependencies. Additionally, we included FECAM\n(Jiang et al., 2023), a method also designed for modeling\ncross-channel frequency-domain dependencies. The results\npresented in Table 6 demonstrate that our EKPB outper-\nforms in modeling inter-series dependencies across mul-\ntiple datasets. Additionally, when comparing the number\nof parameters and inference time during prediction under\nidentical configurations on the ECL dataset, our EKPB\nmethod still outperforms other baselines by a significant\nmargin, as in Table 7. To illustrate EKPB’s functional-\nity, we visualize the series embeddings with and without\nits adjustment in Figure 5. The T-SNE visualization of\nthe series embeddings shows that without EKPB, using\nonly the channel-independent strategy (Nie et al., 2023),\nthe MSE is 0.171. After applying EKPB, channels sharing\nKey-Frequency (variates 2&3) are clustered, while others\n(variates 1&3) are separated. This adjustment improves the\nMSE from 0.171 to 0.145, a 15% reduction. These indicate\nthat EKPB not only achieves better predictive performance\n6\n\nReFocus: Reinforcing Mid-Frequency and Key-Frequency Modeling for Multivariate Time Series Forecasting\nTable 2: Multivariate forecasting results with prediction lengths F ∈{96, 192, 336, 720} and fixed lookback length T = 96. Results\nare averaged from all prediction lengths. The best is Red and the second is Blue. The Lower MSE/MAE indicates the better prediction\nresult. Full results are in Appendix D.\nModels\nReFocus\nFilterNet\niTransformer ModernTCN\nFITS\nPatchTST\nCrossformer\nTimesNet\nTSMixer\nDLinear\nFreTS\n(Ours)\n(2024)\n(2024b)\n(2024)\n(2024a)\n(2023)\n(2023)\n(2023)\n(2023)\n(2023)\n(2023c)\nMetric\nMSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\nETTm1\n0.387 0.394 0.392 0.401 0.407 0.410 0.389 0.402 0.415 0.408 0.387 0.400 0.513 0.496 0.400 0.406 0.398 0.407 0.403 0.407 0.408 0.416\nETTm2\n0.275 0.320 0.285 0.328 0.288 0.332 0.279 0.322 0.286 0.328 0.281 0.326 0.757 0.610 0.291 0.333 0.289 0.333 0.350 0.401 0.321 0.368\nETTh1\n0.434 0.433 0.441 0.439 0.454 0.447 0.446 0.433 0.451 0.440 0.469 0.454 0.529 0.522 0.458 0.450 0.463 0.452 0.456 0.452 0.475 0.463\nETTh2\n0.371 0.396 0.383 0.407 0.383 0.407 0.382 0.404 0.383 0.408 0.387 0.407 0.942 0.684 0.414 0.427 0.401 0.417 0.559 0.515 0.472 0.465\nECL\n0.168 0.262 0.173 0.268 0.178 0.270 0.197 0.282 0.217 0.295 0.205 0.290 0.244 0.334 0.192 0.295 0.186 0.287 0.212 0.300 0.189 0.278\nTraffic\n0.412 0.265 0.463 0.310 0.428 0.282 0.546 0.348 0.627 0.376 0.481 0.304 0.550 0.304 0.620 0.336 0.522 0.357 0.625 0.383 0.618 0.390\nWeather\n0.245 0.271 0.245 0.272 0.258 0.279 0.247 0.272 0.249 0.276 0.259 0.281 0.259 0.315 0.259 0.287 0.256 0.279 0.265 0.317 0.250 0.270\nSolar Energy 0.222 0.252 0.243 0.281 0.233 0.262 0.244 0.286 0.395 0.407 0.270 0.307 0.641 0.639 0.301 0.319 0.260 0.297 0.330 0.401 0.248 0.296\nTable 3: Ablation of ‘Adaptive Mid-Frequency Energy Optimizer (AMEO)’ and ‘Key-Frequency Enhanced Training strategy (KET)’.\nWe list the average results. Full results are in Appendix D.\nAMEO KET\nETTm1\nETTm2\nETTh1\nETTh2\nECL\nTraffic\nWeather\nSolar Energy\nMSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\n-\n-\n0.401 0.403 0.283 0.325 0.440 0.437 0.376 0.400 0.178 0.270 0.449 0.289 0.252 0.278 0.232 0.264\n-\n✓\n0.394 0.396 0.279 0.322 0.437 0.435 0.373 0.398 0.171 0.263 0.414 0.268 0.250 0.275 0.228 0.258\n✓\n-\n0.393 0.402 0.282 0.326 0.443 0.440 0.372 0.397 0.174 0.267 0.452 0.289 0.248 0.275 0.231 0.261\n✓\n✓\n0.387 0.394 0.275 0.320 0.434 0.433 0.371 0.396 0.168 0.262 0.412 0.265 0.245 0.271 0.222 0.252\nTable 4: Further ablation of ‘Key-Frequency Enhanced Training strategy (KET)’. ‘Real’ means KET is not performed, i.e. trained on\noriginal data. ‘Pseudo’ means trained on Pseudo samples. If both are used (Bottom Line), this means the model is trained on Real and\nPseudo samples alternatively, i.e. KET. We list the average results. Full results are in Appendix D.\nReal Pseudo\nETTm1\nETTm2\nETTh1\nETTh2\nECL\nTraffic\nWeather\nSolar Energy\nMSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\n✓\n-\n0.401 0.403 0.283 0.325 0.440 0.437 0.376 0.400 0.178 0.270 0.449 0.289 0.252 0.278 0.232 0.264\n-\n✓\n0.396 0.398 0.280 0.323 0.436 0.434 0.372 0.397 0.175 0.266 0.417 0.271 0.252 0.276 0.277 0.294\n✓\n✓\n0.394 0.396 0.279 0.322 0.437 0.435 0.373 0.398 0.171 0.263 0.414 0.268 0.250 0.275 0.228 0.258\nTable 5: Ablation study of different Key-Frequency Picking strategies. ‘Softmax’ means using softmax function to generate a probability\ndistribution and picking shared Key-Frequency using this distribution. ‘Max’ means always choosing the biggest energy. ‘Min’ means\nalways choosing the smallest energy. We list the average results. Full results are in Appendix D.\nPicking Strategy\nETTm1\nETTm2\nETTh1\nETTh2\nECL\nTraffic\nWeather\nSolar Energy\nMSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\nMin\n0.388 0.392 0.280 0.323 0.432 0.432 0.371 0.396 0.194 0.281 0.517 0.344 0.378 0.363 0.240 0.270\nMax\n0.392 0.395 0.279 0.322 0.437 0.435 0.374 0.398 0.172 0.265 0.422 0.273 0.351 0.343 0.230 0.260\nSoftmax\n0.387 0.394 0.275 0.320 0.434 0.433 0.371 0.396 0.168 0.262 0.412 0.265 0.346 0.339 0.222 0.252\nbut also offers a more resource-efficient solution than other\nbaselines.\nSuperiority of AMEO over RevIN and Filters\nWe\ninvestigated the roles of AMEO, RevIN, and Filters in ad-\ndressing the Mid-Frequency Spectrum Gap through time-\nfrequency domain visualization analysis. The results pre-\nsented in Figure 4 align perfectly with our theoretical anal-\nysis before. High-pass and low-pass filters fail to address\nthe Mid-Frequency Spectrum Gap and exacerbate this issue.\nRevIN, on the other hand, merely eliminates the energy of\nthe zero-frequency component while scaling other compo-\nnents using the variance σ2, which also does not effectively\nresolve the problem. In contrast, our AMEO successfully\namplifies the mid-frequency energy. Furthermore, compared\nto the original sequence and the sequence processed by\n7\n\nReFocus: Reinforcing Mid-Frequency and Key-Frequency Modeling for Multivariate Time Series Forecasting\nFigure 4: The time-frequency domain visualization of the original sequence (ETTm1, the last variate), the sequence processed by\nhigh-pass and low-pass filters, by RevIN, and by AMEO. We selected the input −96 −forecast −96 task.\nTable 6: Multivariate forecasting result of ‘Energy-based Key-\nFrequency Picking Block’ (EKPB) and other inter-series dependen-\ncies modeling backbones. * means ‘former.’ We list the average\nresults. Full results are in Appendix D.\nDataset\nEKPB\nTSMixer\niTrans*\nCross*\nFECAM\nMSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\nETTm2 0.282 0.324 0.289 0.333 0.288 0.332 0.757 0.610 0.297 0.348\nETTh2 0.374 0.399 0.401 0.417 0.383 0.407 0.942 0.684 0.383 0.407\nWeather 0.252 0.277 0.256 0.279 0.258 0.279 0.259 0.315 0.253 0.304\nECL\n0.176 0.268 0.186 0.287 0.178 0.270 0.244 0.334 0.199 0.288\nTable 7: Model efficiency analysis. We evaluated the parameter\ncount, and the inference time (average of 5 runs on a single\nNVIDIA 4090 24GB GPU) with batch size = 1 on ECL dataset.\nWe set the dimension of layer dim ∈{256, 512}, and the number\nof network layers N = 2. The task is input-96-forecast-720. *\nmeans ‘former.’ Para means ‘Parameter count(M).’ Time means\n‘inference time(ms).’\nDim\nEKPB\nCross*\niTrans*\nTSMixer\nFECAM\nParam Time Para Time Para Time\nPara\nTime Para Time\n256\n0.29 68.91 0.93 98.37 1.27 192.12 13.66 432.40 1.39 205.66\n512\n0.97 84.54 1.78 118.29 4.63 249.60 43.04 507.54 5.14 277.43\nTable 8: Experiment result of high-pass filter, low-pass filter,\nRevIN, and AMEO using a simple linear projection as the fore-\ncaster on Weather and ETTm1 dataset. We set the input length\nT = 96 and forecasting length F ∈{720, 96}.\nDataset Length\nAMEO\nRevIN\nLow\nHigh\nNone\nMSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\nETTm1\n96\n0.331 0.365 0.354 0.375 0.345 0.371 1.097 0.792 0.348 0.375\n720\n0.466 0.440 0.486 0.448 0.478 0.458 1.106 0.796 0.479 0.456\nWeather\n96\n0.164 0.236 0.194 0.234 0.198 0.258 0.636 0.608 0.198 0.258\n720\n0.331 0.370 0.365 0.353 0.353 0.387 0.638 0.611 0.352 0.386\nRevIN, we observe that the sequence processed by AMEO\nexhibits significantly higher stationarity with much more\nstable means and variance.\nFigure 5: T-SNE visualization of the series embeddings with and\nwithout ‘Energy-based Key-Frequency Picking Block’ (EKPB) on\nECL dataset. We choose the input −96 −forecast −96 task.\nThree example variates are highlighted: variates 2&3 shared a\ncommon Key-Frequency, while variate 1 does not.\nIn Table 8, the performance of AMEO on two prediction\ntasks across two datasets consistently surpasses the results\nachieved by methods based on RevIN and Filters. Further-\nmore, while Filters and RevIN occasionally lead to degraded\nperformance on certain datasets, AMEO consistently deliv-\ners results that outperform the original methods. These\nfindings further highlight the superiority of AMEO over\nalternative approaches.\n5. Conclusion\nThis work addresses two critical challenges in multivariate\ntime series forecasting: the Mid-Frequency Spectrum Gap\nand the efficient modeling of the shared Key-Frequency. We\npropose the ‘Adaptive Mid-Frequency Energy Optimizer’,\nwhich effectively enhances mid-frequency extraction, and\nthe ‘Energy-based Key-Frequency Picking Block’ with the\n‘Key-Frequency Enhanced Training’ strategy, which effi-\nciently captures shared frequency patterns. Extensive exper-\niments demonstrate the superiority of our approach, achiev-\ning up to 6% MSE reduction on challenging benchmarks,\nthus advancing the SOTA in frequency-domain forecasting.\n8\n\nReFocus: Reinforcing Mid-Frequency and Key-Frequency Modeling for Multivariate Time Series Forecasting\nReferences\nAhamed, M. A. and Cheng, Q.\nTimemachine: A time\nseries is worth 4 mambas for long-term forecasting.\narXiv preprint arXiv:2403.09898, 2024. URL https:\n//arxiv.org/abs/2403.09898.\nAlkhalifah, T., Wang, H., and Ovcharenko, O.\nMlreal:\nBridging the gap between training on synthetic data and\nreal data applications in machine learning. Artificial In-\ntelligence in Geosciences, 3:101–114, 2022.\nAnsari, A. F., Stella, L., Turkmen, C., Zhang, X., Mercado,\nP., Shen, H., Shchur, O., Rangapuram, S. S., Arango, S. P.,\nKapoor, S., et al. Chronos: Learning the language of time\nseries. arXiv preprint arXiv:2403.07815, 2024.\nAsselin, R. Frequency filter for time integrations. Monthly\nWeather Review, 100(6):487–490, 1972.\nBai, S., Kolter, J. Z., and Koltun, V. An empirical evalua-\ntion of generic convolutional and recurrent networks for\nsequence modeling. arXiv preprint arXiv:1803.01271,\n2018.\nB´ogalo, J., Poncela, P., and Senra, E. Understanding fluc-\ntuations through multivariate circulant singular spectrum\nanalysis. Expert Systems with Applications, 251:123827,\n2024.\nCan, Y. B. and Timofte, R.\nAn efficient cnn for spec-\ntral reconstruction from rgb images.\narXiv preprint\narXiv:1804.04647, 2018.\nChakraborty, T. and Trehan, U. Spectralnet: Exploring\nspatial-spectral waveletcnn for hyperspectral image clas-\nsification. arXiv preprint arXiv:2104.00341, 2021.\nChatfield, C. and Xing, H. The analysis of time series: an\nintroduction with R. Chapman and hall/CRC, 2019.\nChekroun, M. D. and Kondrashov, D.\nData-adaptive\nharmonic spectra and multilayer stuart-landau models.\nChaos: An Interdisciplinary Journal of Nonlinear Sci-\nence, 27(9), 2017.\nChen, J., Lenssen, J. E., Feng, A., Hu, W., Fey, M., Tas-\nsiulas, L., Leskovec, J., and Ying, R. From similarity\nto superiority: Channel clustering for time series fore-\ncasting. arXiv preprint arXiv:2404.01340, 2024. URL\nhttps://arxiv.org/abs/2404.01340.\nChen, S.-A., Li, C.-L., Arik, S. O., Yoder, N. C., and\nPfister, T. Tsmixer: An all-mlp architecture for time\nseries forecasting.\nTransactions on Machine Learn-\ning Research, 2023. ISSN 2835-8856. URL https:\n//openreview.net/forum?id=wbpxTuXgm0.\nCheng, C., Sa-Ngasoongsong, A., Beyca, O., Le, T., Yang,\nH., Kong, Z., and Bukkapatnam, S. T. Time series fore-\ncasting for nonlinear and non-stationary processes: a\nreview and comparative study. Iie Transactions, 47(10):\n1053–1071, 2015.\nCirstea, R.-G., Guo, C., Yang, B., Kieu, T., Dong, X., and\nPan, S. Triformer: Triangular, variable-specific attentions\nfor long sequence multivariate time series forecasting–\nfull version. arXiv preprint arXiv:2204.13767, 2022.\nDai, T., Wu, B., Liu, P., Li, N., Bao, J., Jiang, Y., and Xia, S.-\nT. Periodicity decoupling framework for long-term series\nforecasting. In The Twelfth International Conference on\nLearning Representations, 2024.\nDas, A., Kong, W., Leach, A., Mathur, S. K., Sen, R.,\nand Yu, R.\nLong-term forecasting with tide: Time-\nseries dense encoder. Transactions on Machine Learn-\ning Research, 2023. ISSN 2835-8856. URL https:\n//openreview.net/forum?id=pCbC3aQB5W.\nDong, J., Wu, H., Zhang, H., Zhang, L., Wang, J., and\nLong, M. Simmtm: A simple pre-training framework\nfor masked time-series modeling. In Proceedings of the\nThirty-seventh Conference on Neural Information Pro-\ncessing Systems, 2023. URL https://openreview.\nnet/forum?id=ginTcBUnL8.\nDonghao, L. and Xue, W. Moderntcn: A modern pure\nconvolution structure for general time series analysis.\nIn Proceedings of the Twelfth International Conference\non Learning Representations, 2024. URL https://\nopenreview.net/forum?id=vpJMJerXHU.\nDu, D., Su, B., and Wei, Z. Preformer: Predictive trans-\nformer with multi-scale segment-wise correlations for\nlong-term time series forecasting. In ICASSP 2023-2023\nIEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP). IEEE, 2023.\nEldele, E., Ragab, M., Chen, Z., Wu, M., and Li, X. Tslanet:\nRethinking transformers for time series representation\nlearning. In International Conference on Machine Learn-\ning, 2024.\nFan, W., Yi, K., Ye, H., Ning, Z., Zhang, Q., and An, N.\nDeep frequency derivative learning for non-stationary\ntime series forecasting. arXiv preprint arXiv:2407.00502,\n2024.\nGeweke, J. F. Measures of conditional linear dependence\nand feedback between time series. Journal of the Ameri-\ncan Statistical Association, 79(388):907–915, 1984.\nGranger, C. W. and Newbold, P. Spurious regressions in\neconometrics. Journal of econometrics, 2(2):111–120,\n1974.\n9\n\nReFocus: Reinforcing Mid-Frequency and Key-Frequency Modeling for Multivariate Time Series Forecasting\nGuo, H., Mao, Y., and Zhang, R. Mixup as locally linear\nout-of-manifold regularization. In Proceedings of the\nAAAI conference on artificial intelligence, volume 33, pp.\n3714–3722, 2019.\nHan, L., Chen, X.-Y., Ye, H.-J., and Zhan, D.-C. Softs:\nEfficient multivariate time series forecasting with series-\ncore fusion. arXiv preprint arXiv:2404.14197, 2024. URL\nhttps://arxiv.org/abs/2404.14197.\nJiang, M., Zeng, P., Wang, K., Liu, H., Chen, W., and Liu,\nH. Fecam: Frequency enhanced channel attention mecha-\nnism for time series forecasting. Advanced Engineering\nInformatics, 58:102158, 2023.\nJin, M., Wang, S., Ma, L., Chu, Z., Zhang, J. Y., Shi, X.,\nChen, P.-Y., Liang, Y., Li, Y.-F., Pan, S., and Wen, Q.\nTime-LLM: Time series forecasting by reprogramming\nlarge language models. In Proceedings of the Twelfth\nInternational Conference on Learning Representations\n(ICLR), 2024. URL https://openreview.net/\nforum?id=Unb5CVPtae.\nKim, T., Kim, J., Tae, Y., Park, C., Choi, J.-H., and\nChoo, J.\nReversible instance normalization for accu-\nrate time-series forecasting against distribution shift. In\nInternational Conference on Learning Representations,\n2022. URL https://openreview.net/forum?\nid=cGDAkQo1C0p.\nLai, G., Chang, W.-C., Yang, Y., and Liu, H. Modeling\nlong-and short-term temporal patterns with deep neural\nnetworks. In Proceedings of the 41st International ACM\nSIGIR Conference on Research & Development in Infor-\nmation Retrieval, pp. 95–104, 2018.\nLi, Z., Qi, S., Li, Y., and Xu, Z. Revisiting long-term time\nseries forecasting: An investigation on linear mapping.\narXiv preprint arXiv:2305.10721, 2023.\nLim, B. and Zohren, S. Time series forecasting with deep\nlearning: A survey. Philosophical Transactions of the\nRoyal Society A: Mathematical, Physical and Engineer-\ning Sciences, pp. 20200209, 2021. doi: 10.1098/rsta.\n2020.0209. URL http://dx.doi.org/10.1098/\nrsta.2020.0209.\nLin, S., Lin, W., Wu, W., Zhao, F., Mo, R., and\nZhang, H. Segrnn: Segment recurrent neural network\nfor long-term time series forecasting.\narXiv preprint\narXiv:2308.11200, 2023.\nURL https://arxiv.\norg/abs/2308.11200.\nLiu, J., Liu, C., Woo, G., Wang, Y., Hooi, B., Xiong, C.,\nand Sahoo, D. Unitst: Effectively modeling inter-series\nand intra-series dependencies for multivariate time series\nforecasting. arXiv preprint arXiv:2406.04975, 2024a.\nURL https://arxiv.org/abs/2406.04975.\nLiu, M., Zeng, A., Chen, M., Xu, Z., Lai, Q., Ma, L.,\nand Xu, Q.\nScinet: Time series modeling and fore-\ncasting with sample convolution and interaction. In Oh,\nA. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.),\nAdvances in Neural Information Processing Systems,\n2022a. URL https://openreview.net/forum?\nid=AyajSjTAzmg.\nLiu, S., Yu, H., Liao, C., Li, J., Lin, W., Liu, A. X., and Dust-\ndar, S. Pyraformer: Low-complexity pyramidal attention\nfor long-range time series modeling and forecasting. In\nInternational Conference on Learning Representations,\n2022b. URL https://openreview.net/forum?\nid=0EXmFzUn5I.\nLiu, Y., Wu, H., Wang, J., and Long, M. Non-stationary\ntransformers: Exploring the stationarity in time series\nforecasting. In Advances in Neural Information Process-\ning Systems, 2022c. URL https://openreview.\nnet/forum?id=ucNDIDRNjjv.\nLiu, Y., Hu, T., Zhang, H., Wu, H., Wang, S., and Long,\nM. itransformer: Inverted transformers are effective for\ntime series forecasting. In Proceedings of the Twelfth\nInternational Conference on Learning Representations,\n2024b. URL https://openreview.net/forum?\nid=JePfAI8fah.\nLiu, Y., Qin, G., Huang, X., Wang, J., and Long, M. Auto-\ntimes: Autoregressive time series forecasters via large lan-\nguage models. arXiv preprint arXiv:2402.02370, 2024c.\nLiu, Y., Qin, G., Huang, X., Wang, J., and Long, M. Auto-\ntimes: Autoregressive time series forecasters via large lan-\nguage models. arXiv preprint arXiv:2402.02370, 2024d.\nNg, W. T., Siu, K., Cheung, A. C., and Ng, M. K. Expressing\nmultivariate time series as graphs with time series atten-\ntion transformer. arXiv preprint arXiv:2208.09300, 2022.\nURL https://arxiv.org/abs/2208.09300.\nNie, Y., Nguyen, N. H., Sinthong, P., and Kalagnanam, J.\nA time series is worth 64 words: Long-term forecast-\ning with transformers. In Proceedings of the Eleventh\nInternational Conference on Learning Representations,\n2023. URL https://openreview.net/forum?\nid=Jbdc0vTOcol.\nOreshkin, B. N., Carpov, D., Chapados, N., and Bengio,\nY. N-beats: Neural basis expansion analysis for inter-\npretable time series forecasting. In International Confer-\nence on Learning Representations, 2020. URL https:\n//openreview.net/forum?id=r1ecqn4YwB.\nPark, D. S., Chan, W., Zhang, Y., Chiu, C.-C., Zoph, B.,\nCubuk, E. D., and Le, Q. V. Specaugment: A simple data\naugmentation method for automatic speech recognition.\narXiv preprint arXiv:1904.08779, 2019.\n10\n\nReFocus: Reinforcing Mid-Frequency and Key-Frequency Modeling for Multivariate Time Series Forecasting\nPiao, X., Chen, Z., Murayama, T., Matsubara, Y., and Saku-\nrai, Y. Fredformer: Frequency debiased transformer for\ntime series forecasting. In Proceedings of the 30th ACM\nSIGKDD Conference on Knowledge Discovery and Data\nMining, KDD ’24, 2024.\nRahaman, N., Baratin, A., Arpit, D., Draxler, F., Lin, M.,\nHamprecht, F., Bengio, Y., and Courville, A. On the spec-\ntral bias of neural networks. In International conference\non machine learning, pp. 5301–5310. PMLR, 2019.\nRyu, H., Yoon, S., Yoon, H. S., Yoon, E., and Yoo, C. D.\nSimpsi: A simple strategy to preserve spectral informa-\ntion in time series data augmentation. In Proceedings\nof the AAAI Conference on Artificial Intelligence, vol-\nume 38, pp. 14857–14865, 2024.\nShang, Z., Chen, L., Wu, B., and Cui, D. Ada-mshyper:\nAdaptive multi-scale hypergraph transformer for time se-\nries forecasting. In The Thirty-eighth Annual Conference\non Neural Information Processing Systems, 2024.\nStock, J. H. and Watson, M. W. Forecasting using principal\ncomponents from a large number of predictors. Journal of\nthe American statistical association, 97(460):1167–1179,\n2002.\nSundararajan, R. R. and Bruce, S. A.\nFrequency band\nanalysis of nonstationary multivariate time series. arXiv\npreprint arXiv:2301.03664, 2023.\nTishby, N. and Zaslavsky, N.\nDeep learning and the\ninformation bottleneck principle.\nIn 2015 IEEE In-\nformation Theory Workshop (ITW), Apr 2015.\ndoi:\n10.1109/itw.2015.7133169.\nURL http://dx.doi.\norg/10.1109/itw.2015.7133169.\nToner, W. and Darlow, L. N. An analysis of linear time series\nforecasting models. In Proceedings of the Forty-first\nInternational Conference on Machine Learning (ICML),\n2024. URL https://openreview.net/forum?\nid=xl82CcbYaT.\nTorres, J. F., Hadjout, D., Sebaa, A., Mart´ınez- ´Alvarez, F.,\nand Troncoso, A. Deep learning for time series forecast-\ning: a survey. Big Data, 9(1):3–21, 2021.\nWang, H., Peng, J., Huang, F., Wang, J., Chen, J., and Xiao,\nY. MICN: Multi-scale local and global context model-\ning for long-term series forecasting. In The Eleventh\nInternational Conference on Learning Representations,\n2023. URL https://openreview.net/forum?\nid=zt53IDUR1U.\nWang, S., Wu, H., Shi, X., Hu, T., Luo, H., Ma, L., Zhang,\nJ. Y., and ZHOU, J. Timemixer: Decomposable multi-\nscale mixing for time series forecasting. In International\nConference on Learning Representations (ICLR), 2024a.\nWang, X., Zhou, T., Wen, Q., Gao, J., Ding, B., and\nJin, R.\nCARD: Channel aligned robust blend trans-\nformer for time series forecasting. In The Twelfth In-\nternational Conference on Learning Representations,\n2024b. URL https://openreview.net/forum?\nid=MJksrOhurE.\nWang, Y., Wu, H., Dong, J., Liu, Y., Long, M., and Wang,\nJ. Deep time series models: A comprehensive survey\nand benchmark. arXiv preprint arXiv:2407.13278, 2024c.\nURL https://arxiv.org/abs/2407.13278.\nWang, Z., Kong, F., Feng, S., Wang, M., Zhao, H., Wang,\nD., and Zhang, Y. Is mamba effective for time series\nforecasting? arXiv preprint arXiv:2403.11144, 2024d.\nWen, Q., Zhou, T., Zhang, C., Chen, W., Ma, Z., Yan, J.,\nand Sun, L. Transformers in time series: A survey. arXiv\npreprint arXiv:2202.07125, 2022.\nWu, H., Xu, J., Wang, J., and Long, M. Autoformer: Decom-\nposition transformers with auto-correlation for long-term\nseries forecasting.\nIn Ranzato, M., Beygelzimer, A.,\nDauphin, Y., Liang, P., and Vaughan, J. W. (eds.),\nAdvances in Neural Information Processing Systems,\nvolume 34, pp. 22419–22430. Curran Associates, Inc.,\n2021.\nURL https://proceedings.neurips.\ncc/paper_files/paper/2021/file/\nbcc0d400288793e8bdcd7c19a8ac0c2b-Paper.\npdf.\nWu, H., Hu, T., Liu, Y., Zhou, H., Wang, J., and Long, M.\nTimesNet: Temporal 2d-variation modeling for general\ntime series analysis. In The Eleventh International Confer-\nence on Learning Representations, 2023. URL https:\n//openreview.net/forum?id=ju_Uqw384Oq.\nXu, Z., Zeng, A., and Xu, Q. Fits: Modeling time se-\nries with 10k parameters. In Proceedings of the Twelfth\nInternational Conference on Learning Representations,\n2024a. URL https://openreview.net/forum?\nid=bWcnvZ3qMb.\nXu, Z.-Q. J., Zhang, Y., and Luo, T. Overview frequency\nprinciple/spectral bias in deep learning. Communica-\ntions on Applied Mathematics and Computation, pp. 1–\n38, 2024b.\nYi, K., Zhang, Q., Cao, L., Wang, S., Long, G., Hu, L.,\nHe, H., Niu, Z., Fan, W., and Xiong, H. A survey on\ndeep learning based time series analysis with frequency\ntransformation. arXiv preprint arXiv:2302.02173, 2023a.\nYi, K., Zhang, Q., Fan, W., He, H., Hu, L., Wang, P., An,\nN., Cao, L., and Niu, Z. FourierGNN: Rethinking mul-\ntivariate time series forecasting from a pure graph per-\nspective. In Thirty-seventh Conference on Neural In-\n11\n\nReFocus: Reinforcing Mid-Frequency and Key-Frequency Modeling for Multivariate Time Series Forecasting\nformation Processing Systems, 2023b. URL https:\n//openreview.net/forum?id=bGs1qWQ1Fx.\nYi, K., Zhang, Q., Fan, W., Wang, S., Wang, P., He, H.,\nAn, N., Lian, D., Cao, L., and Niu, Z.\nFrequency-\ndomain MLPs are more effective learners in time series\nforecasting. In Thirty-seventh Conference on Neural In-\nformation Processing Systems, 2023c. URL https:\n//openreview.net/forum?id=iif9mGCTfy.\nYi, K., Fei, J., Zhang, Q., He, H., Hao, S., Lian, D., and Fan,\nW. Filternet: Harnessing frequency filters for time series\nforecasting. arXiv preprint arXiv:2411.01623, 2024.\nYu, C., Wang, F., Shao, Z., Sun, T., Wu, L., and Xu, Y. Ds-\nformer: A double sampling transformer for multivariate\ntime series long-term prediction. In Proceedings of the\n32nd ACM international conference on information and\nknowledge management, pp. 3062–3072, 2023.\nYu, G., Li, Y., Guo, X., Wang, D., Liu, Z., Wang, S., and\nYang, T. Lino: Advancing recursive residual decomposi-\ntion of linear and nonlinear patterns for robust time series\nforecasting. arXiv preprint arXiv:2410.17159, 2024a.\nYu, G., Zou, J., Hu, X., Aviles-Rivero, A. I., Qin, J., and\nWang, S.\nRevitalizing multivariate time series fore-\ncasting: Learnable decomposition with inter-series de-\npendencies and intra-series variations modeling.\nIn\nProceedings of the Forty-first International Conference\non Machine Learning (ICML), 2024b. URL https:\n//openreview.net/forum?id=87CYNyCGOo.\nZeng, A., Chen, M., Zhang, L., and Xu, Q.\nAre\ntransformers effective for time series forecasting?\nIn Proceedings of the AAAI Conference on Arti-\nficial Intelligence,\nvolume 37,\npp. 11121–11128,\n2023.\nURL https://ojs.aaai.org/index.\nphp/AAAI/article/view/26317/26089.\nZhang, T., Zhang, Y., Cao, W., Bian, J., Yi, X., Zheng, S.,\nand Li, J. Less is more: Fast multivariate time series\nforecasting with light sampling-oriented mlp structures.\narXiv preprint arXiv:2207.01186, 2022. URL https:\n//arxiv.org/abs/2207.01186.\nZhang, Y. and Yan, J. Crossformer: Transformer utilizing\ncross-dimension dependency for multivariate time series\nforecasting. In The Eleventh International Conference\non Learning Representations, 2023. URL https://\nopenreview.net/forum?id=vSVLM2j9eie.\nZhao, L. and Shen, Y. Rethinking channel dependence\nfor multivariate time series forecasting: Learning from\nleading indicators. In The Twelfth International Confer-\nence on Learning Representations, 2024. URL https:\n//openreview.net/forum?id=JiTVtCUOpS.\nZhou, H., Zhang, S., Peng, J., Zhang, S., Li, J., Xiong,\nH., and Zhang, W. Informer: Beyond efficient trans-\nformer for long sequence time-series forecasting. Pro-\nceedings of the AAAI Conference on Artificial In-\ntelligence, 35:11106–11115, 2022a.\ndoi:\n10.1609/\naaai.v35i12.17325. URL http://dx.doi.org/10.\n1609/aaai.v35i12.17325.\nZhou, T., MA, Z., wang, x., Wen, Q., Sun, L., Yao, T.,\nYin, W., and Jin, R. Film: Frequency improved legendre\nmemory model for long-term time series forecasting. In\nKoyejo, S., Mohamed, S., Agarwal, A., Belgrave, D.,\nCho, K., and Oh, A. (eds.), Advances in Neural Informa-\ntion Processing Systems, volume 35, pp. 12677–12690.\nCurran Associates, Inc., 2022b.\nZhou, T., Ma, Z., Wen, Q., Wang, X., Sun, L., and Jin,\nR. Fedformer: Frequency enhanced decomposed trans-\nformer for long-term series forecasting. In Proceedings of\nthe 39th International Conference on Machine Learning\n(ICML 2022), 2022c.\nZhou, Y., You, L., Zhu, W., and Xu, P. Improving time\nseries forecasting with mixup data augmentation. 2023.\n12\n\nReFocus: Reinforcing Mid-Frequency and Key-Frequency Modeling for Multivariate Time Series Forecasting\nA. Proof\nThis section is dedicated to proving Theorem 3.3 and Theo-\nrem 3.5.\nA.1. Impact of RevIN on Frequency Spectrum\nRevIN (Kim et al., 2022; Liu et al., 2022c) normalizes inputs\nusing sample-wise mean and variance, then reverts scaling\npost-prediction to ensure consistent distributions, mitigating\nnon-stationary effects in time series.\nLet the original time series be x(t) with length T. The series\nˆx(t) that processed by RevIN is given by:\nˆx(t) = x(t) −µ\nσ\n, t = 0, 1, . . . , T −1,\nµ = 1\nT\nT −1\nX\nt=0\nx(t),\nσ =\nv\nu\nu\nt 1\nT\nT −1\nX\nt=0\n(x(t) −µ)2.\n(8)\nThe Fourier transform of x(t) and ˆx(t) are:\nX(f) =\nT −1\nX\nt=0\nx(t)e−i2πft/T −1,\nf = 0, 1, . . . , T −1,\nˆX(f) =\nT −1\nX\nt=0\n\u0012x(t) −µ\nσ\n\u0013\ne−i2πft/T −1\n= 1\nσ\nT −1\nX\nt=0\nx(t)e−i2πft/T −1 −µ\nσ\nT −1\nX\nt=0\ne−i2πft/T −1.\n(9)\nThe spectral energy is computed as the squared magnitude\nof the Fourier transform. For x(t) and ˆx(t), we have:\nEX(f) = |X(f)|2,\nE ˆ\nX(f) = | ˆX(f)|2.\n(10)\nWhen f = 0, the exponential term e−i2πft/T −1 = 1, so:\nˆX(0) = 1\nσ\nT −1\nX\nt=0\nx(t) −µT\nσ\n= µT\nσ −µT\nσ\n= 0\n(11)\nSince µ\nσ is a constant, we have:\nµ\nσ ·\nT −1\nX\nt=0\ne−i2πft/T −1 = 0,\nf = 1, 2 . . . , T −1,\nˆX(f) = 1\nσ\nT −1\nX\nt=0\nx(t)e−i2πft/T −1 −µ\nσ\nT −1\nX\nt=0\ne−i2πft/T −1\n= 1\nσ X(f),\nE ˆ\nX(f) =\n\u0012 1\nσ\n\u00132\n|X(f)|2.\n(12)\nThis suggests that RevIN scales the spectral energy by\nσ2 but does not affect its relative distribution except\nˆX(0) = 0. Thus, RevIN preserves the relative spectral\nenergy distribution and leaves the Mid-Frequency Spectrum\nGap unresolved.\nA.2. Impact of AMEO on Frequency Spectrum\nReferring back to Definition 3.4, AMEO is defined as:\nˆx(t) = x(t) −β\nK\nK−1\nX\nk=0\n˜x(t + K −1 −k),\n˜x(t) =\n(\nx(t −( K\n2 + 1)),\nif K\n2 + 1 ≤t < T + K\n2 + 1\n0,\nif 0 ≤t < K\n2 + 1 or T + K\n2 + 1 ≤t < T + K\n(13)\nThe Fourier transform of ˆx(t) is:\nˆX(f) =\nT −1\nX\nt=0\n\"\nx(t) −β\nK\nK−1\nX\nk=0\n˜x(t + K −1 −k)\n#\ne−i2πft/T −1\n=\nT −1\nX\nt=0\nx(t)e−i2πft/T −1\n|\n{z\n}\nX(f)\n−β\nK\nK−1\nX\nk=0\nT −1\nX\nt=0\n˜x(t + K −1 −k)e−i2πft/T −1\n|\n{z\n}\nTk(f)\n.\n(14)\nFor Tk(f), given FFT{x(t −a)} = X(f)e−i2πfa/T −1,\nwe have:\nTk(f) =\nT −1\nX\nt=0\n˜x(t + K −1 −k)e−i2πft/T −1\n=\nT −1\nX\nt=0\nx(t + 3K\n2\n−k −2)e−i2πft/T −1\n= FFT{x(t + 3K\n2\n−k −2)}\n= X(f)ei2πf( 3K\n2 −k−2)/T −1\n(15)\n13\n\nReFocus: Reinforcing Mid-Frequency and Key-Frequency Modeling for Multivariate Time Series Forecasting\nSo, we have the Fourier transform of ˆx(t) and its spectral\nenergy:\nˆX(f) = X(f) −β\nK\nK−1\nX\nk=0\nX(f)ei2πf( 3K\n2 −k−2)/T −1\n= X(f)\n\n\n1 −β · 1\nK\nK−1\nX\nk=0\nei2πf( 3K\n2 −k−2)/T −1\n|\n{z\n}\nG(f)\n\n\n,\nE ˆ\nX(f) = |X(f)|2\n\n\n\n\n\n\n\n\n\n\n\n1 −β · 1\nK\nK−1\nX\nk=0\nei2πf( 3K\n2 −k−2)/T −1\n|\n{z\n}\nG(f)\n\n\n\n\n\n\n\n\n\n\n\n2\n= |X(f)|2(1 −β · G(f))2.\n(16)\nIn this paper, we set K = 25 (i.e.,T/4 + 1, T = 96), and\nthe function graph of G(f) is shown in Figure 6.\nFigure 6: The function G(f) is plotted for T = 96 and K = 25.\nDue to the symmetry of the FFT, we only need to plot the values\nfor f = 0, 1, . . . , 48.\nIt is evident that G(f) is a gradually decay function, with\nits values decreasing from 1 to 0.\nThis ensures that\nE ˆ\nX(f) = |X(f)|2(1 −β · G(f))2, where, relative to EX,\nthe low-frequency components are attenuated, and the mid-\nfrequency components are enhanced.\nB. EXPERIMENTAL DETAILS\nB.1. Dataset Statistics\nWe elaborate on the datasets employed in this study with\nthe following details.\n• ETT Dataset (Zhou et al., 2022a) comprises two sub-\ndatasets: ETTh and ETTm, which were collected\nfrom electricity transformers. Data were recorded at\n15-minute and 1-hour intervals for ETTm and ETTh,\nrespectively, spanning from July 2016 to July 2018.\n• Solar Energy (Lai et al., 2018) records the solar power\nproduction of 137 PV plants in 2006, which are sam-\npled every 10 minutes.\n• Electricity Dataset2 encompasses the electricity con-\nsumption data of 321 customers, recorded on an hourly\nbasis, covering the period from 2012 to 2014.\n• Traffic Dataset3 consists of hourly data from the Cali-\nfornia Department of Transportation. It describes road\noccupancy rates measured by various sensors on San\nFrancisco Bay area freeways.\n• Weather Dataset4 contains records of 21 meteorolog-\nical indicators, updated every 10 minutes throughout\nthe entire year of 2020.\nWe follow the same data processing and train-validation-\ntest set split protocol used in iTransformer (Liu et al.,\n2024b), where the train, validation, and test datasets are\nstrictly divided according to chronological order to make\nsure there are no data leakage issues. We fix the input\nlength as T = 96 for all datasets, and the forecasting length\nF ∈{96, 192, 336, 720}.\nB.2. Implementation Details and Model Parameters\nWe trained our ReFocus model using the MSE loss func-\ntion and employed the ADAM optimizer. For evaluation\npurposes, we used two key performance metrics: the mean\nsquare error (MSE) and the mean absolute error (MAE). We\ninitialized the random seed as rs = 2024 and set the hyper-\nparameter K = 25-kernel size of the convolution kernel in\nAMEO. The dimension of the Layer is set to D = 512 and\nQ = 128. The batch size bs = 32 for the Traffic dataset\ndue to its large channel will cause out of memory when\nemployed with large batch size, and bs = 128 for others.\nThe learning rate is searched from lr ∈{1e −5, 1e −4}\nexcept for the Traffic dataset (lr = 5e −4). The number\nof EKPB is searched from N ∈{1, 2, 3, 4}, and hyper-\nparameter β. which controls the scale magnitude, from\nβ ∈{0.01, 0.1, 0.5, 1.0}. Our implementation was carried\nout in PyTorch and executed on a single NVIDIA GeForce\nRTX 4090 with 24G VRAM. To foster reproducibility, we\nmake our code, and training scripts available in this GitHub\nRepository5.\nAll the compared multivariate forecasting baseline models\nthat we reproduced are implemented based on the bench-\nmark of Time series Lab (Wang et al., 2024c) Repository 6,\n2https://archive.ics.uci.edu/ml/datasets/\nElectricityLoadDiagrams20112014\n3https://pems.dot.ca.gov/\n4https://www.bgc-jena.mpg.de/wetter/\n5https://github.com/Levi-Ackman/ReFocus\n6https://github.com/thuml/\nTime-Series-Library\n14\n\nReFocus: Reinforcing Mid-Frequency and Key-Frequency Modeling for Multivariate Time Series Forecasting\nFigure 7: We select the input −96 −forecast −96 task on Traffic and visualize the validation loss and weight of our ReFocus model.\nLEFT: Visualization of the Validation Loss during 100 training epochs with (KET) and without KET (Raw). RIGHT: Visualization\nabout the Weight (Obtained using the approach outlined in Analysis of linear model (Toner & Darlow, 2024)) of the trained model. Two\nsignificant metrics for assessing the information richness of the weight matrix-the information Entropy and the Sum of Eigenvalues-are\ncalculated. Both indicate higher quality with greater values.\nwhich is fairly built on the configurations provided by each\nmodel’s original paper or official code. Those that have\nnot yet been included in Time series Lab are directly re-\nproduced from their official code repositories. It is worth\nnoting that both the baselines used in this paper and our\nReFocus have fixed a long-standing bug. This bug was\noriginally identified in Informer (Zhou et al., 2022a) (AAAI\n2021 Best Paper) and subsequently addressed by FITS (Xu\net al., 2024a). For specific details about the bug and its\nresolution, please refer to GitHub Repository7.\nC. Further Analysis of the proposed\nKey-Frequency Enhanced Training strategy\nTo further investigate the impact of the proposed ‘Key-\nFrequency Enhanced Training (KET) strategy’ on model\ntraining and forecasting ability, we visualize its training\nprocess regarding Validation Loss and the model weights\nobtained after training in Figure 7. We also compute the\nEntropy and the Sum of Eigenvalues of the weight matrix.\nThe results show that, in the absence of KET, the model\nquickly overfits around the 24th epoch, exhibiting poor gen-\neralization. In contrast, with the aid of KET, the model\nconsistently performs better on the validation set, converg-\ning smoothly without overfitting, and the training process\nbecomes more stable. Additionally, weight visualization\nresults indicate that the model trained with KET has higher\ninformation Entropy and a greater Sum of Eigenvalues,\nsuggesting that the trained model possesses a stronger ca-\npacity for feature representation extraction. The predictive\nresults further validate this, as our KET improves the MSE\nfrom 0.414 to 0.380, achieving an 8.2% reduction.\n7https://github.com/VEWOXIC/FITS\nD. Full Results\nThe full experiment results are provided in the following\nsection due to the space limitation of the main text.\nFull multivariate forecasting results\nTable 9 contains\nthe detailed results of Ten baselines and our ReFocus on\neight well-acknowledged forecasting benchmarks. ReFocus\nconsistently achieves the best overall performance across\nall datasets, especially in tasks with a large number of chan-\nnels, such as the Solar Energy dataset (137 channels), ECL\ndataset (321 channels), and Traffic dataset (862 channels).\nIt obtains the best performance in terms of MSE: 35 out\nof 40 tasks, and MAE: 38 out of 40 tasks. These results\ndemonstrate the outstanding performance of ReFocus in\nmultivariate time series forecasting tasks.\nFull results of ablation on AMEO and KET\nTable D\npresents the full results of the ablation study on ‘Adap-\ntive Mid-Frequency Energy Optimizer (AMEO)’ and ‘Key-\nFrequency Enhanced Training (KET)’. KET and AMEO\ncontribute significantly to the model’s performance, each\nproviding substantial improvements. Moreover, their com-\nbination further enhances the model, achieving peak per-\nformance. These results provide strong evidence of the\neffectiveness of both AMEO and KET.\nFull results of further ablation study on KET\nTable D\nexhibits the full results of a further ablation study on the\n‘Key-Frequency Enhanced Training (KET)’ strategy. In-\ntroducing Pseudo samples—obtained by randomly incor-\nporating spectral information from other channels into the\ncurrent channel—generally leads to performance improve-\nment. However, on more complex datasets, it results in\nperformance degradation. In contrast, alternating training\nbetween Real and Pseudo samples (Our KET) overcomes\n15\n\nReFocus: Reinforcing Mid-Frequency and Key-Frequency Modeling for Multivariate Time Series Forecasting\nthis issue, yielding a further and consistent enhancement in\nperformance.\nFull results of ablation study of different Key-Frequency\nPicking strategies\nTable D illustrates the complete results\nof the ablation study on various Key-Frequency Picking\nstrategies. Notably, our Softmax-based random sampling\nstrategy consistently achieves the best overall performance,\nparticularly on more complex datasets.\nFull results of EKPB and other inter-series dependencies\nmodeling backbone\nTable D presents the full results of\n‘Energy-based Key-Frequency Picking Block (EKPB)’ and\nother inter-series dependency modeling backbones on mul-\ntivariate time series forecasting tasks. The proposed EKPB\nachieves overall optimal performance, demonstrating excep-\ntional capability in modeling inter-series dependencies.\n16\n\nReFocus: Reinforcing Mid-Frequency and Key-Frequency Modeling for Multivariate Time Series Forecasting\nTable 9: Multivariate long-term forecasting result comparison. We use prediction lengths F ∈{96, 192, 336, 720}, and input length\nT = 96. The best results are in bold and the second bests are underlined.\nModel\nReFocus\nFilterNet\niTransformer ModernTCN\nFITS\nPatchTST Crossformer TimesNet\nTSMixer\nDLinear\nFreTS\n(Ours)\n(2024)\n(2024b)\n(2024)\n(2024a)\n(2023)\n(2023)\n(2023)\n(2023)\n(2023)\n(2023c)\nMetric\nMSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\nETTm1\n96\n0.321 0.360 0.321 0.361 0.334 0.368 0.317 0.362 0.355 0.375 0.329 0.367 0.404 0.426 0.338 0.375 0.323 0.363 0.345 0.372 0.335 0.372\n192\n0.365 0.379 0.367 0.387 0.377 0.391 0.366 0.389 0.392 0.393 0.367 0.385 0.450 0.451 0.374 0.387 0.376 0.392 0.380 0.389 0.388 0.401\n336\n0.398 0.400 0.401 0.409 0.426 0.420 0.407 0.412 0.424 0.414 0.399 0.410 0.532 0.515 0.410 0.411 0.407 0.413 0.413 0.413 0.421 0.426\n720\n0.463 0.437 0.477 0.448 0.491 0.459 0.466 0.443 0.487 0.449 0.454 0.439 0.666 0.589 0.478 0.450 0.485 0.459 0.474 0.453 0.486 0.465\nAvg\n0.387 0.394 0.392 0.401 0.407 0.410 0.389 0.402 0.415 0.408 0.387 0.400 0.513 0.496 0.400 0.406 0.398 0.407 0.403 0.407 0.408 0.416\nETTm2\n96\n0.173 0.255 0.175 0.258 0.180 0.264 0.173 0.255 0.183 0.266 0.175 0.259 0.287 0.366 0.187 0.267 0.182 0.266 0.193 0.292 0.189 0.277\n192\n0.237 0.297 0.240 0.301 0.250 0.309 0.235 0.296 0.247 0.305 0.241 0.302 0.414 0.492 0.249 0.309 0.249 0.309 0.284 0.362 0.258 0.326\n336\n0.295 0.334 0.311 0.347 0.311 0.348 0.308 0.344 0.307 0.342 0.305 0.343 0.597 0.542 0.321 0.351 0.309 0.347 0.369 0.427 0.343 0.390\n720\n0.395 0.392 0.414 0.405 0.412 0.407 0.398 0.394 0.407 0.399 0.402 0.400 1.730 1.042 0.408 0.403 0.416 0.408 0.554 0.522 0.495 0.480\nAvg\n0.275 0.320 0.285 0.328 0.288 0.332 0.279 0.322 0.286 0.328 0.281 0.326 0.757 0.610 0.291 0.333 0.289 0.333 0.350 0.401 0.321 0.368\nETTh1\n96\n0.376 0.394 0.382 0.402 0.386 0.405 0.386 0.394 0.386 0.396 0.414 0.419 0.423 0.448 0.384 0.402 0.401 0.412 0.386 0.400 0.395 0.407\n192\n0.428 0.422 0.430 0.429 0.441 0.436 0.436 0.423 0.436 0.423 0.460 0.445 0.471 0.474 0.436 0.429 0.452 0.442 0.437 0.432 0.448 0.440\n336\n0.462 0.442 0.472 0.451 0.487 0.458 0.479 0.445 0.478 0.444 0.501 0.466 0.570 0.546 0.491 0.469 0.492 0.463 0.481 0.459 0.499 0.472\n720\n0.470 0.474 0.481 0.473 0.503 0.491 0.481 0.469 0.502 0.495 0.500 0.488 0.653 0.621 0.521 0.500 0.507 0.490 0.519 0.516 0.558 0.532\nAvg\n0.434 0.433 0.441 0.439 0.454 0.447 0.446 0.433 0.451 0.440 0.469 0.454 0.529 0.522 0.458 0.450 0.463 0.452 0.456 0.452 0.475 0.463\nETTh2\n96\n0.288 0.337 0.293 0.343 0.297 0.349 0.292 0.340 0.295 0.350 0.302 0.348 0.745 0.584 0.340 0.374 0.319 0.361 0.333 0.387 0.309 0.364\n192\n0.371 0.390 0.374 0.396 0.380 0.400 0.377 0.395 0.381 0.396 0.388 0.400 0.877 0.656 0.402 0.414 0.402 0.410 0.477 0.476 0.395 0.425\n336\n0.409 0.421 0.417 0.430 0.428 0.432 0.424 0.434 0.426 0.438 0.426 0.433 1.043 0.731 0.452 0.452 0.444 0.446 0.594 0.541 0.462 0.467\n720\n0.417 0.436 0.449 0.460 0.427 0.445 0.433 0.448 0.431 0.446 0.431 0.446 1.104 0.763 0.462 0.468 0.441 0.450 0.831 0.657 0.721 0.604\nAvg\n0.371 0.396 0.383 0.407 0.383 0.407 0.382 0.404 0.383 0.408 0.387 0.407 0.942 0.684 0.414 0.427 0.401 0.417 0.559 0.515 0.472 0.465\nECL\n96\n0.143 0.238 0.147 0.245 0.148 0.240 0.173 0.260 0.200 0.278 0.181 0.270 0.219 0.314 0.168 0.272 0.157 0.260 0.197 0.282 0.176 0.258\n192\n0.158 0.252 0.160 0.250 0.162 0.253 0.181 0.267 0.200 0.280 0.188 0.274 0.231 0.322 0.184 0.289 0.173 0.274 0.196 0.285 0.175 0.262\n336\n0.172 0.267 0.173 0.267 0.178 0.269 0.196 0.283 0.214 0.295 0.204 0.293 0.246 0.337 0.198 0.300 0.192 0.295 0.209 0.301 0.185 0.278\n720\n0.198 0.290 0.210 0.309 0.225 0.317 0.238 0.316 0.255 0.327 0.246 0.324 0.280 0.363 0.220 0.320 0.223 0.318 0.245 0.333 0.220 0.315\nAvg\n0.168 0.262 0.173 0.268 0.178 0.270 0.197 0.282 0.217 0.295 0.205 0.290 0.244 0.334 0.192 0.295 0.186 0.287 0.212 0.300 0.189 0.278\nTraffic\n96\n0.380 0.248 0.430 0.294 0.395 0.268 0.550 0.355 0.651 0.391 0.462 0.295 0.522 0.290 0.593 0.321 0.493 0.336 0.650 0.396 0.593 0.378\n192\n0.403 0.259 0.452 0.307 0.417 0.276 0.527 0.337 0.602 0.363 0.466 0.296 0.530 0.293 0.617 0.336 0.497 0.351 0.598 0.370 0.595 0.377\n336\n0.419 0.267 0.470 0.316 0.433 0.283 0.537 0.342 0.609 0.366 0.482 0.304 0.558 0.305 0.629 0.336 0.528 0.361 0.605 0.373 0.609 0.385\n720\n0.446 0.287 0.498 0.323 0.467 0.302 0.570 0.359 0.647 0.385 0.514 0.322 0.589 0.328 0.640 0.350 0.569 0.380 0.645 0.394 0.673 0.418\nAvg\n0.412 0.265 0.463 0.310 0.428 0.282 0.546 0.348 0.627 0.376 0.481 0.304 0.550 0.304 0.620 0.336 0.522 0.357 0.625 0.383 0.618 0.390\nWeather\n96\n0.160 0.202 0.162 0.207 0.174 0.214 0.165 0.203 0.166 0.213 0.177 0.218 0.158 0.230 0.172 0.220 0.166 0.210 0.196 0.255 0.174 0.208\n192\n0.211 0.248 0.210 0.250 0.221 0.254 0.212 0.247 0.213 0.254 0.225 0.259 0.206 0.277 0.219 0.261 0.215 0.256 0.237 0.296 0.219 0.250\n336\n0.266 0.290 0.265 0.290 0.278 0.296 0.266 0.293 0.269 0.294 0.278 0.297 0.272 0.335 0.280 0.306 0.287 0.300 0.283 0.335 0.273 0.290\n720\n0.344 0.343 0.342 0.340 0.358 0.349 0.344 0.343 0.346 0.343 0.354 0.348 0.398 0.418 0.365 0.359 0.355 0.348 0.345 0.381 0.334 0.332\nAvg\n0.245 0.271 0.245 0.272 0.258 0.279 0.247 0.272 0.249 0.276 0.259 0.281 0.259 0.315 0.259 0.287 0.256 0.279 0.265 0.317 0.250 0.270\nSolar Energy\n96\n0.182 0.219 0.206 0.251 0.203 0.237 0.206 0.264 0.371 0.417 0.234 0.286 0.310 0.331 0.250 0.292 0.221 0.275 0.290 0.378 0.217 0.278\n192\n0.222 0.249 0.242 0.279 0.233 0.261 0.246 0.285 0.377 0.398 0.267 0.310 0.734 0.725 0.296 0.318 0.268 0.306 0.320 0.398 0.256 0.302\n336\n0.240 0.268 0.255 0.291 0.248 0.273 0.260 0.296 0.416 0.412 0.290 0.315 0.750 0.735 0.319 0.330 0.272 0.294 0.353 0.415 0.263 0.307\n720\n0.242 0.271 0.267 0.301 0.249 0.275 0.264 0.298 0.414 0.400 0.289 0.317 0.769 0.765 0.338 0.337 0.281 0.313 0.356 0.413 0.256 0.297\nAvg\n0.222 0.252 0.243 0.283 0.233 0.262 0.244 0.286 0.395 0.407 0.270 0.307 0.641 0.639 0.301 0.319 0.260 0.297 0.330 0.401 0.248 0.296\n1st Count\n34\n38\n2\n1\n0\n0\n1\n1\n0\n0\n1\n0\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n17\n\nReFocus: Reinforcing Mid-Frequency and Key-Frequency Modeling for Multivariate Time Series Forecasting\nTable 10: Full result of ablation study on the ‘Adaptive Mid-Frequency Energy Optimizer (AMEO)’ and the ‘Key-Frequency Enhanced\nTraining (KET)’ strategy. We use prediction lengths F ∈{96, 192, 336, 720}, and input length T = 96. The best results are in bold.\nModel Both (ReFocus) + AMEO\n+ KET\nNone\nMetric MSE\nMAE\nMSE MAE MSE MAE MSE MAE\nETTm1\n96\n0.321\n0.360\n0.331 0.368 0.331 0.363 0.339 0.367\n192 0.365\n0.379\n0.377 0.390 0.373 0.382 0.381 0.391\n336 0.398\n0.400\n0.403 0.407 0.403 0.402 0.414 0.413\n720 0.463\n0.437\n0.462 0.441 0.467 0.438 0.468 0.442\nAvg 0.387\n0.394\n0.393 0.402 0.394 0.396 0.401 0.403\nETTm2\n96\n0.173\n0.255\n0.179 0.262 0.178 0.260 0.180 0.262\n192 0.237\n0.297\n0.244 0.304 0.241 0.299 0.245 0.302\n336 0.295\n0.334\n0.304 0.340 0.300 0.337 0.304 0.340\n720 0.395\n0.392\n0.402 0.396 0.398 0.393 0.404 0.396\nAvg 0.275\n0.320\n0.282 0.326 0.279 0.322 0.283 0.325\nETTh1\n96\n0.376\n0.394\n0.382 0.398 0.378 0.395 0.383 0.395\n192 0.428\n0.422\n0.433 0.425 0.432 0.423 0.432 0.425\n336 0.462\n0.442\n0.468 0.450 0.469 0.447 0.469 0.449\n720 0.470\n0.474\n0.489 0.486 0.470 0.474 0.474 0.480\nAvg 0.434\n0.433\n0.443 0.440 0.437 0.435 0.440 0.437\nETTh2\n96\n0.288\n0.337\n0.285 0.336 0.289 0.339 0.288 0.338\n192 0.371\n0.390\n0.375 0.391 0.374 0.390 0.374 0.391\n336 0.409\n0.421\n0.405 0.420 0.412 0.425 0.419 0.428\n720 0.417\n0.436\n0.424 0.441 0.418 0.438 0.423 0.441\nAvg 0.371\n0.396\n0.372 0.397 0.373 0.398 0.376 0.400\nECL\n96\n0.143\n0.238\n0.146 0.241 0.145 0.239 0.147 0.242\n192 0.158\n0.252\n0.165 0.259 0.161 0.253 0.162 0.256\n336 0.172\n0.267\n0.177 0.272 0.176 0.269 0.180 0.274\n720 0.198\n0.290\n0.206 0.297 0.203 0.292 0.221 0.307\nAvg 0.168\n0.262\n0.174 0.267 0.171 0.263 0.178 0.270\nTraffic\n96\n0.380\n0.248\n0.414 0.274 0.380 0.250 0.414 0.278\n192 0.403\n0.259\n0.439 0.287 0.404 0.262 0.437 0.284\n336 0.419\n0.267\n0.449 0.288 0.421 0.270 0.449 0.288\n720 0.446\n0.287\n0.506 0.307 0.450 0.290 0.495 0.307\nAvg 0.412\n0.265\n0.452 0.289 0.414 0.268 0.449 0.289\nWeather\n96\n0.160\n0.202\n0.165 0.209 0.164 0.207 0.164 0.209\n192 0.211\n0.248\n0.210 0.252 0.215 0.252 0.216 0.256\n336 0.266\n0.290\n0.267 0.291 0.273 0.295 0.275 0.299\n720 0.344\n0.343\n0.350 0.346 0.349 0.345 0.353 0.349\nAvg 0.245\n0.271\n0.248 0.275 0.250 0.275 0.252 0.278\nSolar Energy\n96\n0.182\n0.219\n0.197 0.226 0.192 0.230 0.192 0.234\n192 0.222\n0.249\n0.236 0.269 0.231 0.255 0.235 0.265\n336 0.240\n0.268\n0.246 0.276 0.244 0.271 0.249 0.279\n720 0.242\n0.271\n0.245 0.274 0.245 0.274 0.250 0.278\nAvg 0.222\n0.252\n0.231 0.261 0.228 0.258 0.232 0.264\n18\n\nReFocus: Reinforcing Mid-Frequency and Key-Frequency Modeling for Multivariate Time Series Forecasting\nTable 11: Full result of further ablation study on the ‘Key-Frequency Enhanced Training (KET)’ strategy. We use prediction lengths\nF ∈{96, 192, 336, 720}, and input length T = 96. The best results are in bold.\nModel Both (KET)\nPseudo\nReal\nMetric MSE MAE MSE MAE MSE MAE\nETTm1\n96\n0.331 0.363 0.331 0.362 0.339 0.367\n192 0.373 0.382 0.375 0.384 0.381 0.391\n336 0.403 0.402 0.406 0.405 0.414 0.413\n720 0.467 0.438 0.471 0.440 0.468 0.442\nAvg 0.394 0.396 0.396 0.398 0.401 0.403\nETTm2\n96\n0.178 0.260 0.178 0.260 0.180 0.262\n192 0.241 0.299 0.242 0.299 0.245 0.302\n336 0.300 0.337 0.301 0.339 0.304 0.340\n720 0.398 0.393 0.399 0.393 0.404 0.396\nAvg 0.279 0.322 0.280 0.323 0.283 0.325\nETTh1\n96\n0.378 0.395 0.382 0.394 0.383 0.395\n192 0.432 0.423 0.429 0.423 0.432 0.425\n336 0.469 0.447 0.467 0.445 0.469 0.449\n720 0.470 0.474 0.467 0.472 0.474 0.480\nAvg 0.437 0.435 0.436 0.434 0.440 0.437\nETTh2\n96\n0.289 0.339 0.288 0.338 0.288 0.338\n192 0.374 0.390 0.370 0.390 0.374 0.391\n336 0.412 0.425 0.412 0.423 0.419 0.428\n720 0.418 0.438 0.418 0.437 0.423 0.441\nAvg 0.373 0.398 0.372 0.397 0.376 0.400\nECL\n96\n0.145 0.239 0.147 0.241 0.147 0.242\n192 0.161 0.253 0.165 0.257 0.162 0.256\n336 0.176 0.269 0.179 0.271 0.180 0.274\n720 0.203 0.292 0.209 0.296 0.221 0.307\nAvg 0.171 0.263 0.175 0.266 0.178 0.270\nTraffic\n96\n0.380 0.250 0.383 0.254 0.414 0.278\n192 0.404 0.262 0.406 0.265 0.437 0.284\n336 0.421 0.270 0.424 0.272 0.449 0.288\n720 0.450 0.290 0.454 0.293 0.495 0.307\nAvg 0.414 0.268 0.417 0.271 0.449 0.289\nWeather\n96\n0.164 0.207 0.166 0.207 0.164 0.209\n192 0.215 0.252 0.216 0.255 0.216 0.256\n336 0.273 0.295 0.275 0.297 0.275 0.299\n720 0.349 0.345 0.352 0.346 0.353 0.347\nAvg 0.250 0.275 0.253 0.276 0.252 0.278\nSolar Energy\n96\n0.192 0.230 0.235 0.263 0.192 0.234\n192 0.231 0.255 0.290 0.303 0.235 0.265\n336 0.244 0.271 0.287 0.301 0.249 0.279\n720 0.245 0.274 0.296 0.308 0.250 0.278\nAvg 0.228 0.258 0.277 0.294 0.232 0.264\n19\n\nReFocus: Reinforcing Mid-Frequency and Key-Frequency Modeling for Multivariate Time Series Forecasting\nTable 12:\nFull result about ablation study of different Key-Frequency Picking strategies.\nWe use prediction lengths F\n∈\n{96, 192, 336, 720}, and input length T = 96. The best results are in bold.\nModel\nSoftmax\nMax\nMin\nMetric MSE MAE MSE MAE MSE MAE\nETTm1\n96\n0.321 0.360 0.331 0.360 0.321 0.357\n192 0.365 0.379 0.370 0.380 0.366 0.377\n336 0.398 0.400 0.401 0.402 0.400 0.399\n720 0.463 0.437 0.467 0.438 0.464 0.436\nAvg 0.387 0.394 0.392 0.395 0.388 0.392\nETTm2\n96\n0.173 0.255 0.175 0.258 0.177 0.259\n192 0.237 0.297 0.240 0.300 0.242 0.300\n336 0.295 0.334 0.303 0.338 0.302 0.338\n720 0.395 0.392 0.396 0.392 0.398 0.394\nAvg 0.275 0.320 0.279 0.322 0.280 0.323\nETTh1\n96\n0.376 0.394 0.380 0.396 0.372 0.391\n192 0.428 0.422 0.430 0.423 0.426 0.423\n336 0.462 0.442 0.464 0.444 0.464 0.443\n720 0.470 0.474 0.473 0.478 0.467 0.472\nAvg 0.434 0.433 0.437 0.435 0.432 0.432\nETTh2\n96\n0.288 0.337 0.289 0.340 0.287 0.337\n192 0.371 0.390 0.373 0.391 0.366 0.388\n336 0.409 0.421 0.414 0.425 0.410 0.423\n720 0.417 0.436 0.418 0.437 0.419 0.437\nAvg 0.371 0.396 0.374 0.398 0.371 0.396\nECL\n96\n0.143 0.238 0.145 0.241 0.165 0.256\n192 0.158 0.252 0.162 0.256 0.176 0.267\n336 0.172 0.267 0.175 0.269 0.192 0.283\n720 0.198 0.290 0.204 0.293 0.242 0.318\nAvg 0.168 0.262 0.172 0.265 0.194 0.281\nTraffic\n96\n0.380 0.248 0.389 0.253 0.504 0.341\n192 0.403 0.259 0.413 0.268 0.505 0.338\n336 0.419 0.267 0.427 0.276 0.521 0.351\n720 0.446 0.287 0.457 0.296 0.536 0.347\nAvg 0.412 0.265 0.422 0.273 0.517 0.344\nWeather\n96\n0.160 0.202 0.166 0.207 0.164 0.205\n192 0.211 0.248 0.212 0.248 0.212 0.249\n336 0.266 0.290 0.268 0.291 0.269 0.290\n720 0.344 0.343 0.348 0.344 0.349 0.344\nAvg 0.245 0.271 0.249 0.273 0.249 0.272\nSolar Energy\n96\n0.182 0.219 0.189 0.228 0.208 0.247\n192 0.222 0.249 0.236 0.262 0.242 0.270\n336 0.240 0.268 0.245 0.273 0.256 0.283\n720 0.242 0.271 0.248 0.276 0.253 0.280\nAvg 0.222 0.252 0.230 0.260 0.240 0.270\n20\n\nReFocus: Reinforcing Mid-Frequency and Key-Frequency Modeling for Multivariate Time Series Forecasting\nTable 13: Multivariate forecasting result of ‘Energy-based Key-Frequency Picking Block’ (EKPB) and other inter-series dependencies\nmodeling backbones. We use prediction lengths F ∈{96, 192, 336, 720}, and input length T = 96. The best results are in bold.\nModel\nEKPB\niTransformer\nTSMixer\nCrossformer\nFECAM\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\nETTm2\n96\n0.179 0.260 0.180 0.264 0.182 0.266 0.287 0.366 0.188 0.275\n192 0.244 0.301 0.250 0.309 0.249 0.309 0.414 0.492 0.265 0.336\n336 0.303 0.339 0.311 0.348 0.309 0.347 0.597 0.542 0.318 0.362\n720 0.401 0.395 0.412 0.407 0.416 0.408 1.730 1.042 0.416 0.417\nAvg 0.282 0.324 0.288 0.332 0.289 0.333 0.757 0.610 0.297 0.348\nETTh2\n96\n0.288 0.338 0.297 0.349 0.319 0.361 0.745 0.584 0.298 0.345\n192 0.374 0.391 0.380 0.400 0.402 0.410 0.877 0.656 0.377 0.397\n336 0.414 0.426 0.428 0.432 0.444 0.446 1.043 0.731 0.425 0.434\n720 0.421 0.440 0.427 0.445 0.441 0.450 1.104 0.763 0.432 0.450\nAvg 0.374 0.399 0.383 0.407 0.401 0.417 0.942 0.684 0.383 0.407\nWeather\n96\n0.166 0.209 0.174 0.214 0.166 0.210 0.158 0.230 0.182 0.242\n192 0.216 0.256 0.221 0.254 0.215 0.256 0.206 0.277 0.223 0.281\n336 0.274 0.296 0.278 0.296 0.287 0.300 0.272 0.335 0.270 0.320\n720 0.351 0.346 0.358 0.349 0.355 0.348 0.398 0.418 0.338 0.374\nAvg 0.252 0.277 0.258 0.279 0.256 0.279 0.259 0.315 0.253 0.304\nECL\n96\n0.146 0.240 0.148 0.240 0.157 0.260 0.219 0.314 0.178 0.267\n192 0.161 0.254 0.162 0.253 0.173 0.274 0.231 0.322 0.185 0.273\n336 0.178 0.273 0.178 0.269 0.192 0.295 0.246 0.337 0.199 0.290\n720 0.220 0.306 0.225 0.317 0.223 0.318 0.280 0.363 0.235 0.323\nAvg 0.176 0.268 0.178 0.270 0.186 0.287 0.244 0.334 0.199 0.288\n21\n",
  "metadata": {
    "source_path": "papers/arxiv/ReFocus_Reinforcing_Mid-Frequency_and_Key-Frequency_Modeling_for\n__Multivariate_Time_Series_Forecasting_0a4e085faf0282df.pdf",
    "content_hash": "0a4e085faf0282df3036d6862e4a7b7fdb13bf107a1fbda696fcdf65fba6d0e9",
    "arxiv_id": null,
    "title": "ReFocus: Reinforcing Mid-Frequency and Key-Frequency Modeling for Multivariate Time Series Forecasting  ",
    "author": "Guoqi Yu, Yaoming Li, Juncheng Wang, Xiaoyu Guo, Angelica I. Aviles-Rivero, Tong Yang, Shujun Wang",
    "creation_date": "D:20250225022746Z",
    "published": "2025-02-25T02:27:46",
    "pages": 21,
    "size": 7034733,
    "file_mtime": 1740470214.9011059
  }
}