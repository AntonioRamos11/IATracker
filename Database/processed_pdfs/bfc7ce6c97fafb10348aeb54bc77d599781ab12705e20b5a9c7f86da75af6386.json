{
  "text": "CHEEMS: A Practical Guidance for Building and Evaluating\nChinese Reward Models from Scratch\n†Xueru Wen1,2, †Jie Lou3, †Zichao Li1,2, Yaojie Lu1, XingYu3, Yuqiu Ji3, Guohai Xu3,\nHongyu Lin1, Ben He1,2, Xianpei Han1, Le Sun1, Debing Zhang3\n1Chinese Information Processing Laboratory, Institute of Software,\nChinese Academy of Sciences, Beijing, China\n3University of Chinese Academy of Sciences, Beijing, China\n4Xiaohongshu Inc\n{wenxueru2022,lizichao2022}@iscas.ac.cn\n{luyaojie,hongyu,sunle,xianpei}@iscas.ac.cn\nbenhe@ucas.edu.cn loujie0822@gmail.com dengyang@xiaohongshu.com\nAbstract\nReward models (RMs) are crucial for align-\ning large language models (LLMs) with human\npreferences. However, most RM research is\ncentered on English and relies heavily on syn-\nthetic resources, which leads to limited and less\nreliable datasets and benchmarks for Chinese.\nTo address this gap, we introduce Cheems-\nBench, a fully human-annotated RM evalua-\ntion benchmark within Chinese contexts, and\nCheemsPreference, a large-scale and diverse\npreference dataset annotated through human-\nmachine collaboration to support Chinese RM\ntraining.\nWe systematically evaluate open-\nsource discriminative and generative RMs on\nCheemsBench and observe significant limita-\ntions in their ability to capture human prefer-\nences in Chinese scenarios. Additionally, based\non CheemsPreference, we construct an RM\nthat achieves state-of-the-art performance on\nCheemsBench, demonstrating the necessity of\nhuman supervision in RM training. Our find-\nings reveal that scaled AI-generated data strug-\ngles to fully capture human preferences, empha-\nsizing the importance of high-quality human\nsupervision in RM development.\n1\nIntroduction\nWith the rapid advancement of large language mod-\nels (Yang et al., 2024a; Grattafiori et al., 2024),\npost-training has emerged as a critical challenge to\nensure their safety, reliability, and alignment with\nhuman values (Hou et al., 2024; Lin et al., 2024).\nReward models (Palan et al., 2019; Ouyang et al.,\n2022), as core components of LLM post-training,\nplay a pivotal role in capturing human preferences\n†These authors contributed equally to this work.\nEnglish Instruction\nExisting RM Resources\nRejected\nChosen\nCheemsBench for Chinese RM\nWin\n>\nLose\n>\nStandalone annotation\nPairwise comparison \nChinese\nInstruction\n#1\n#2\n#3\n#4\n#5\nCross-validation annotation\n>\n,\n>\n,\n[\n]\n[\n]\nConflict-resolved Partial-ranking comparison\n#1\n#2\n#3\n#4\n#5\n#1>#2=#3\n#3>#2>#4\n#1>#5>#4\n#1>#2>#5\n...\nFigure 1: The differences in construction and usage\nbetween CheemsBench and the existing RM resources.\nand guiding models to adhere more closely to hu-\nman needs (Bai et al., 2022). By providing reward\nsignals, RMs can guide parameter optimization\nduring training (Ibarz et al., 2018; Ouyang et al.,\n2022) or directly intervene in outputs during decod-\ning(Khanov et al., 2024; Li et al., 2024a).\nDespite the crucial role of RMs in post-training,\ncurrent research is mainly focused on English. For\ninstance, Skywork-Reward (Liu et al., 2024a) and\nUltraRM (Cui et al., 2023) leverage high-quality\nEnglish preference datasets (Zheng et al., 2023;\nJi et al., 2024) and benchmarks (Lambert et al.,\n2024) to achieve superior performance. In contrast,\n1\narXiv:2502.17173v1  [cs.CL]  24 Feb 2025\n\nStatistics\nCheemsBench\nCheemsPreference\nOpen Prompt\nHuman Instruction\nGPT\nHuman\n# Prompts\n1,146\n1,346\n27,861\n3,260\n# Responses\n5\n5\n5.29\n5.07\n# Comparisons\n7,838\n9,762\n332,370\n37,618\nAvg. Char. of Prompt\n186.58\n197.04\n175.56\n164.08\nAvg. Char. of Chosen\n437.50\n436.96\n457.92\n440.18\nAvg. Char. of Rejected\n454.01\n446.43\n394.18\n432.84\nTable 1: Statistics of CheemsBench and CheemsPreference: Number of prompts, average responses per prompt,\ncomparisons (excluding ties), and average character lengths of prompts, chosen responses, and rejected responses.\nthe development of Chinese RMs faces significant\nchallenges due to a lack of large-scale, high-quality\npreference datasets and comprehensive evaluation\nbenchmarks. Existing Chinese resources are of-\nten small in scale (Huozi-Team, 2024; Yucheng,\n2023) and limited to specific domains (Yang, 2024;\nXinlu Lai, 2024; Xu et al., 2023), making them\ninsufficient for LLM post-training. Moreover, ex-\nisting RM mainly rely on synthetic data, which\nstruggles to accurately reflect human preferences.\nTo address this critical gap, this paper con-\nstructs a comprehensive and human-centric Chi-\nnese RM resource from scratch1. It consists of two\nkey datasets: (1) CheemsBench, a fully human-\nannotated and extensive Chinese RM evaluation\nbenchmark that verifies whether RMs accurately\ncapture and reflect human preferences; and (2)\nCheemsPreference, a large-scale, diverse Chinese\npreference dataset that provides supervised signals\nfor training Chinese RMs, enabling them to effec-\ntively learn and model human preferences.\nAs shown in Figure 1, unlike most RM re-\nsources that rely on machine-generated annotations\n(Zhou et al., 2024), CheemsBench and CheemsPref-\nerence are built on human supervision, thereby\nmore accurately capturing realistic human values.\nMoreover, while traditional RM benchmarks (Lam-\nbert et al., 2024) typically rely on pairwise com-\nparisons, recent studies (Wen et al., 2024) have\nhighlighted their limitations in reflecting down-\nstream performances. CheemsBench introduces\na multi-response evaluation mechanism, which\naligns closely with downstream tasks.\nIn CheemsBench, we combine open-source\nprompts and real-world human instructions with\na comprehensive taxonomy to evaluate RM per-\nformance To better align with downstream tasks\nand reduce preference-induced noise (Zhang et al.,\n1CHEEMS stands for C¯ h¯ inese re¯ ward mode¯ l benchm¯ ark\nand preference datas¯ et.\n2024a), we sample five responses from various\nopen- and closed-source LLMs for each prompt and\nconduct five rounds of human-driven triple-wise\ncomparisons. To address potential annotation con-\nflicts, we design a graph-based conflict-resolving\nalgorithm that generates unique and consistent par-\ntial rankings. Using CheemsBench, we assess the\nprogress of reward models and preference datasets\nin the Chinese context and identify considerable\nroom for improvement in Chinese RMs.\nFor CheemsPreference, we collect 27k human\ninstructions following a multi-tiered prompt taxon-\nomy and sample more than 5 responses per prompt\nfrom various LLMs, ensuring both prompt and re-\nsponse diversity. To alleviate inconsistencies and\nbiases in GPT annotations (Stureborg et al., 2024)\nwhile reducing human effort, we design a distant su-\npervision algorithm to improve data quality. Specif-\nically, human annotators first label a small golden\npreference dataset, which is then used to train an\nRM to filter a larger GPT-annotated dataset. The\ncombined human- and GPT-annotated data form\nCheemsPreference, achieving state-of-the-art re-\nsults on CheemsBench and performing well on the\nEnglish RewardBench (Lambert et al., 2024).\nOur contributions are summarized as follows:\n• We propose CheemsBench, the first large-\nscale and comprehensive benchmark designed\nspecifically for Chinese reward models.\n• We construct CheemsPreference, the first\nlarge-scale, diverse, and high-quality Chinese\npreference dataset.\n• We provide a comprehensive investigation\ninto Chinese RM training and evaluation.\n2\nRelated Works\nReinforcement Learning from Human Feed-\nback.\nReinforcement Learning from Human\n2\n\nHuman \nInstructions\nOpen-source\nPrompts\nLLaMA\nGPT\nQwen\nGLM\nIntern\nLanguage Model Pool\nChinese Instruction Set\n将下文翻译成文言文:门客对他说…\nTranslate into Classical Chinese: \nThe retainer said to him…\nResp. #1\nResp. #2\nResp. #3\nResp. #4\nResp. #5\n#1>#2=#3\n#1>#5>#4\n#1>#2>#5\n#1\n#2\n#3\n#4\n#5\n#1>[#2,#3]>[#4,#5]\nResponses\nEqual Edge\nPreference Edge\nConflict Edge\nTopological Edge\nUntopological Edge\n(1) Data Construction\n(2) Human Annotation\n(3) Conflict Resolving\n...\n...\nFigure 2: Chinese RM benchmark construction process. We utilize open-source prompts and human instructions\nand sample five responses from various models for each prompt. These responses then undergo five rounds of\ntriple-wise manual comparisons. Unique partial rankings are generated by conflict resolving algorithm.\nFeedback has been widely adopted for LLM align-\nment (Ouyang et al., 2022; Bai et al., 2022). Previ-\nous research mostly focuses on specific tasks like\nsummarization (Stiennon et al., 2022) and question\nanswering (Nakano et al., 2022). Recent studies\nhave expanded RLHF applications to broader do-\nmains (Hou et al., 2024; Lin et al., 2024; Yu et al.,\n2024), improving LLMs to be more helpful, hon-\nest, and harmless. RLHF enables models to align\nwith human expectations more closely by integrat-\ning human preferences captured by reward models\n(Ng and Russell, 2000; Brown and Niekum, 2019;\nPalan et al., 2019). Thus, a reward model that ac-\ncurately reflects human preferences is fundamental\nto the RLHF methodology.\nReward Model Training and Evaluation.\nTo\ndevelop a RM that captures human preferences, cur-\nrent works gather preference data through manual\nannotation (Bai et al., 2022; Zheng et al., 2023)\nor distilling advanced LLMs (Zhu et al., 2023;\nCui et al., 2023). These works mostly focus on\nEnglish, overlooking Chinese contexts. Existing\nChinese preference datasets are generally small\n(Huozi-Team, 2024; Yucheng, 2023) or limited to\nspecific tasks (Yang, 2024; Xinlu Lai, 2024; Xu\net al., 2023). Beyond the training data, RM evalu-\nation is also critical for post-training. The typical\nRM evaluation computes accuracy on a fixed test\ndataset (Lambert et al., 2024). Recent studies (Son\net al., 2024; Kim et al., 2024; Zhou et al., 2024; Liu\net al., 2024b; Frick et al., 2024; Gureja et al., 2024)\nhave attempted to strengthen the correlation with\ndownstream performance. However, these bench-\nmarks focus on English, raising questions about\ntheir applicability to Chinese contexts.\n3\nChinese RM Benchmark\nIn this section, we introduce CheemsBench, a\nbenchmark designed to comprehensively evaluate\nChinese RMs. Our benchmark is characterized by:\n(1) High coverage: We incorporate a wide range of\nprompts and sampling models, ensuring broad eval-\nuation across diverse scenarios. (2) High-quality\nannotation: We derive a reliable preference ranking\nthrough multiple rounds of manual triple-wise com-\nparisons and conflict resolving. Figure 2 illustrates\nthe overall construction process.\n3.1\nData Construction\nPrompt Collection.\nWe sample Chinese prompts\nfrom various open datasets, including Humaneval-\nXL (Peng et al., 2024), MathOctopus (Chen et al.,\n2024), GAOKAO-Bench (Zhang et al., 2024b), Hal-\nluQA (Cheng et al., 2023), Flames (Huang et al.,\n2023), CLiB (Lee, 2023), AlignBench (Liu et al.,\n2023), and COIG-CQIA (yuelin bai, 2023). We\nmanually map their original categories into a uni-\nfied system shown in Figure 8. We also include real-\nworld human instructions for out-of-distribution\nevaluation. To ensure thorough converge across\ndifferent scenarios, we build a comprehensive cate-\ngorization system as illustrated in Figure 9. In total,\nwe select 1,146 prompts from open-source datasets\nand 1,346 from human instructions.\nResponses Collection.\nTo ensure a wide range of\nresponse quality and distribution, we sample 5 re-\n3\n\nsponses per prompt from various models. (1) Open-\nsource models: Qwen2-7B/72B-Instruct (Yang\net al., 2024a), Meta-Llama-3.1-8B/70B-Instruct\n(Grattafiori et al., 2024),\nLlama3.1-8B/72B-\nChinese-Chat (Wang et al., 2024b), Internlm2-chat-\n1.8b (Cai et al., 2024), and GLM-4-9b-chat (GLM\net al., 2024); (2) Proprietary models: GPT-4 (Ope-\nnAI et al., 2024), GPT-3.5-turbo, GPT-4-turbo, and\nClaude-3-5-sonnet (Anthropic, 2024). We observe\nthat some open-source models demonstrate lim-\nited Chinese capabilities and tend to exhibit code-\nswitching or even significant garbling2. In such\ncases, we rely on human annotators to filter these re-\nsponses during the annotation process. Specifically,\nannotators are instructed to discard responses con-\ntaining substantial sections of meaningless content,\nwhile retaining those with minor code-switching\nthat do not compromise semantic meaning. This\nprocedure allows us to account for LLMs’ code-\nswitching behavior during RM evaluation.\n3.2\nBenchmark Labeling\nHuman Annotation.\nTo accurately capture hu-\nman preferences, CheemsBench relies entirely on\nhuman judgment for its annotation process. Given\na prompt and its corresponding 5 responses, we\npre-design five annotation tasks, each compris-\ning a triple-wise comparison of three adjacent re-\nsponses. These tasks are distributed to different\nannotators who perform preference comparisons\nindependently. All annotation results are then used\nto construct a ranked list of responses.\nConflict Resolving.\nHowever, conflicts may arise\ndue to the human preferences ambiguity and poten-\ntial annotation errors. To derive reliable results, we\ndevelop a dedicated conflict resolving algorithm, as\nshown in Algorithm 1. Specifically, we first trans-\nform the annotation results into a directed prefer-\nence graph, where responses and preferences repre-\nsent nodes and edges respectively. We then employ\ndepth-first search to identify cycles in the graph,\nwhich indicate conflicts. These cycles are merged\ninto larger nodes, and this process is repeated until\nno cycles remain in the graph. Finally, we perform\ntopological sorting to obtain a partial ranking3.\n2The LLaMA series shows a higher tendency for code-\nswitching and nonsensical output, possibly due to its tokenizer\nvocabulary and insufficient training on Chinese corpora.\n3Details about the algorithms and annotators are provided\nin Appendix C and Appendix D, respectively.\n3.3\nEvaluation Metrics\nGiven multiple responses per prompt, there are\nmany potential metrics for evaluation (Wen et al.,\n2024). We first convert a partial ranking into mul-\ntiple pair-wise comparisons and evaluate the accu-\nracy as in the typical setting (Lambert et al., 2024):\nAccuracy = 1\nN\nN\nX\ni=1\nI(ri\nw > ri\nl)\n(1)\nwhere N is the total number of pair-wise compar-\nisons after transformation, and the indicator func-\ntion I checks if the reward score for the preferred\nresponse ri\nw is greater than that of its counterpart ri\nl.\nAdditionally, the exact match rate can be employed,\nwhich measures the proportion of prompts where\nall pair-wise comparisons are correctly sorted:\nExact Match = 1\nM\nM\nX\nj=1\nI\n ^\nk\n(rj,k\nw > rj,k\nl )\n!\n(2)\nwhere M is the number of prompts, and the indica-\ntor function verifies if all comparisons are ordered\ncorrectly. We obtain the final result by averaging\nthe metrics from subsets of different categories.\n4\nChinese Preference Dataset\nIn this section, we present the construction of\nCheemsPreference, as depicted in Figure 3. Our\ndataset is characterized by: (1) Scale and diversity:\nWe amass 27k real human instructions, featuring\na comprehensive multi-tier categorization system,\nand sample multiple responses from a variety of\nmodels for each prompt. (2) High-quality annota-\ntion: We employ a distant supervision algorithm,\nwhich integrates both human annotations and GPT-\n4o to establish reliable partial preference ranks.\n4.1\nData Construction\nPrompt Collection.\nDiverse and high-quality in-\nstruction data are crucial for ensuring the robust-\nness of RMs. To this end, we collect 27,861 real-\nworld human instructions. To ensure extensive cov-\nerage of downstream scenarios, we develop a com-\nprehensive multi-tier categorization system, which\nencompasses eight main categories with dozens of\nrefined subcategories, as illustrated in Figure 10.\nResponse Collection.\nWe sample responses from\na broad range of models: (1) Open-source mod-\nels: Qwen2-7B/72B-Instruct (Yang et al., 2024a),\nQwen2.5-7B/14B/32B/72B-Instruct (Team, 2024),\n4\n\n门下客谓之曰…\nThe retainer said to him…\nChinese Instruction \n…\nSampling Responses\n门客问之曰…\nThe retainer asked him…\n座客与之曰…\nA seated guest said to him…\n将下文翻译成文言文:门客对他说…\nTranslate into Classical Chinese: \nThe retainer said to him…\nHuman \nSubset\nReward \nModel R(•) \nR(#1) = 0.4\nR(#5) = 0.2\nR(#2) = -0.1\nR(#3) = -0.5\nR(#4) = -0.2\nResponses\nInitial Edge\nConflict Edge\nConsistent Edge\nI[R(#i) > R(#j)] = 1\nDistant Supervision by R(•)\nGPT \nSubset\nHuman \nSubset\nCheems\nPreference\n...\nFigure 3: Chinese preference dataset construction process. Each prompt’s different responses and their annotation\nresults form a directed graph. Circles in this preference graph indicate conflicts. We utilize the reward model trained\non the human-annotated dataset to filter GPT annotations, thereby producing a directed acyclic graph.\nMeta-Llama-3.1-8B/70B-Instruct (Grattafiori et al.,\n2024), Llama3.1-8B/72B-Chinese-Chat (Wang\net al., 2024b), Internlm2-chat-1.8b (Cai et al.,\n2024), and GLM-4-9b-chat (GLM et al., 2024). (2)\nProprietary models: GPT-4 (OpenAI et al., 2024),\nGPT-3.5-turbo, GPT-4-turbo, GPT-4o, and Claude-\n3-5-sonnet (Anthropic, 2024). To guarantee the\nquality of responses, we implement rule-based\nmethods to detect responses that are abnormally\nlengthy or contain excessive non-Chinese symbols.\nAlthough this approach may have lower accuracy\nfor prompts involving math or code, we prioritize\na high recall rate to filter out more low-quality re-\nsponses. Finally, each prompt has more than 5\nresponses on average.\n4.2\nDistant Supervision\nThe quality of preference data (Gao et al., 2024)\nis essential for the training of RM. While human\nannotation ensures high quality, it is expensive\nand challenging to obtain in large quantities. Con-\nversely, GPT-based annotation is scalable but often\ninconsistent and biased (Stureborg et al., 2024). To\nconstruct large-scale, high-quality Chinese pref-\nerence data, we implement a distant supervision\nstrategy for annotation. We initially engage human\nannotators to label a small subset of data, follow-\ning the protocol detailed in Section 3.2. Subse-\nquently, GPT-4o is employed to annotate a larger\ndataset. For a set of N responses, GPT-4o per-\nforms C2\nN pair-wise comparisons between each\nresponse pairs4. To mitigate positional bias (Li\net al., 2024b), the order of responses in each com-\nparison is randomized. Although these GPT-4o\n4Annotation prompts can be found in Appendix B.\nannotations can exhibit inconsistencies, i.e., cy-\ncles in the preference graph, we employ an RM\ntrained on human-annotated data to filter these an-\nnotations and establish a consistent partial order.\nAdditionally, we propose a length-debias post-hoc\nfiltering strategy to alleviate length bias (Dubois\net al., 2024). This involves dividing the dataset into\ntwo groups, where the chosen response is longer or\nshorter than the rejected one, and downsampling\nthe larger group to balance the dataset.\n5\nChinese Reward Model\nIn this section, we introduce our reward model\ntraining methodology. In contrast to typical prefer-\nence datasets constructed by pair-wise comparisons\n(Cui et al., 2023; Ji et al., 2024), CheemsPreference\nhas two distinct characteristics: (1) each prompt is\nassociated with multiple responses, and (2) these re-\nsponses form only a partial preference chain. Thus,\nwe employ following loss according to Bradley-\nTerry Model (Bradley and Terry, 1952):\nL′ = −\nE\nx∼X\nyw,yl∼Yx\n[log (σ (r (x, yw) −r (x, yl)))]\n(3)\nwhere X stands for the distribution of the prompt\nx and Yx denotes the distribution of responses y\ngiven the prompt x. We employ a greedy sample-\nbased batch logic for calculating this loss. Specif-\nically, during each forward pass, we determine if\nall responses for a given prompt can be included\nin one batch. If feasible, they are added to the\nbatch; otherwise, any excess responses are allo-\ncated to subsequent batches. This method might\nbypass some pair comparisons, but it ensures that\nno response is duplicated across batches, thereby\n5\n\nModel Name\nRewardBench\nOpen Prompt\nHuman Instruction\nOverall\nAcc.\nExact.\nAcc.\nExact.\nGenerative Models as Reward Models\nSkywork-Critic-Llama-3.1-70B\n0.933\n0.755\n0.320\n0.731\n0.258\n0.516\nCompassJudger-1-14B-Instruct\n0.841\n0.745\n0.327\n0.692\n0.239\n0.501\nCompassJudger-1-32B-Instruct\n0.852\n0.742\n0.322\n0.685\n0.231\n0.495\nQwen2.5-72B-Instruct\n-\n0.734\n0.306\n0.678\n0.229\n0.487\nSkywork-Critic-Llama-3.1-8B\n0.890\n0.726\n0.288\n0.696\n0.229\n0.485\nGPT-4o\n0.846\n0.640\n0.163\n0.727\n0.300\n0.457\nDoubao-pro-128k\n-\n0.720\n0.280\n0.662\n0.164\n0.456\nQwen2.5-7B-Instruct\n-\n0.713\n0.262\n0.637\n0.163\n0.444\nDiscriminative Reward Models\nSkywork-Reward-Gemma-2-27B\n0.938\n0.754\n0.329\n0.748\n0.311\n0.535\nSkywork-Reward-Gemma-2-27B-v0.2\n0.943\n0.751\n0.321\n0.735\n0.294\n0.525\nLlama-3.1-Nemotron-70B-Reward-HF\n0.941\n0.750\n0.317\n0.722\n0.271\n0.515\nLlama-3-OffsetBias-RM-8B\n0.894\n0.734\n0.310\n0.689\n0.239\n0.493\nRM-Mistral-7B\n0.804\n0.721\n0.285\n0.700\n0.259\n0.491\nURM-LLaMa-3-8B\n0.899\n0.727\n0.310\n0.688\n0.230\n0.489\nArmoRM-Llama3-8B-v0.1\n0.904\n0.715\n0.308\n0.677\n0.246\n0.487\nSkywork-Reward-Llama-3.1-8B-v0.2\n0.931\n0.721\n0.283\n0.701\n0.237\n0.486\nCheemsRM (Ours)\n0.919\n0.857\n0.508\n0.832\n0.431\n0.657\nTable 2: Performance of discriminative and generative RMs on CheemsBench. The Overall metric is the average of\naccuracy (Acc.) and exact match (Exact.) across the Open Prompt and Human Instruction subsets. CheemsRM\nrefers to the RM trained on our CheemsPreference dataset.\nmitigating overfitting risks (Ouyang et al., 2022).\nMore importantly, this sample-based batch organi-\nzation enhances computational efficiency by reduc-\ning redundant forward passes. To further stabilize\ntraining, we integrate an additional regularization\nterm (Hou et al., 2024), imposing a Gaussian prior\non the distribution of reward scores:\nL = L′ +\nE\nx∼X,y∼Yx\n\u0002\nr2 (x, y)\n\u0003\n(4)\n6\nExperiments\nWe first assess the performance of open-source\nRMs and datasets on CheemsBench (Section 6.1).\nNext, we examine our benchmark’s correlation\nwith downstream tasks (Section 6.2). For Cheem-\nsPreference, we conduct an ablation study to\ndemonstrate its effectiveness (Section 6.3) and test\nthe scaling trend (Section 6.4).\n6.1\nBenchmark Results\nReward Models Evaluation\nWe thoroughly as-\nsess the performance of current RMs in the Chinese\ncontext, including discriminative reward models\nand generative models as reward models5 (Zheng\net al., 2023). Table 2 demonstrates the results of\ntop-ranked RMs on CheemsBench. We find that (1)\nThe accuracy of the leading models significantly\n5Comprehensive results and citations for models and\ndatasets are provided in the Appendix E.\ndrops when applied to CheemsBench. This per-\nformance gap indicates considerable room for im-\nprovement of RMs in Chinese settings. (2) These\nRMs perform better on open-source prompts\nthan on human instructions. This is expected, as\nour human instructions are collected from the real\nworld and thus can be more out-of-distribution than\nopen-source prompts. (3) For prompts with rela-\ntively deterministic answers, RM can assess the\nquality of the responses more accurately. Figure\n4 details the performance of these RMs on different\nsubcategories. On the open-source prompt subset,\nRMs show competence in \"Reasoning\" but strug-\ngle in other categories. On the human instruction\nsubset, models excel in \"Reasoning\" and \"Complex\nInstructions\" but perform poorly in tasks involving\n\"Understanding\". These observations emphasize\nthe need for targeted enhancements in these tasks.\nPreference Datasets Evaluation\nWe evaluate\nvarious Chinese and English preference datasets\non CheemsBench by training RMs6 based on\nQwen2.5-72B-Instruct (Team, 2024). The exper-\nimental results are presented in Table 3.\nNo-\ntably, among the Chinese datasets, \"Huozi\" (Huozi-\nTeam, 2024) performs best. Meanwhile, the \"Ultra-\nfeedback\" (Cui et al., 2023) leads among English\n6Details about hyperparameter settings for different exper-\niments are provided in Appendix F.\n6\n\nReasoning\nKnowledge\nLanguage\nSafety\n0.65\n0.7\n0.75\n0.8\n0.85\nReasoning\nKnowledge\nMathematics\nUnderstanding\nCreation\nComplex instruction\nSafety\n0.55 0.6 0.65 0.7 0.75 0.8\nSkywork-Reward-Gemma-2-27B\nSkywork-Reward-Gemma-2-27B-v0.2\nSkywork-Critic-Llama-3.1-70B\nLlama-3.1-Nemotron-70B-Reward-HF\nCompassJudger-1-14B-Instruct\nLlama-3-OffsetBias-RM-8B\nRM-Mistral-7B\nURM-LLaMa-3.1-8B\nArmoRM-Llama3-8B-v0.1\nFigure 4: Accuracy of top-ranked reward models on CheemsBench across subsets of different categories. The left\nand right sub-figures respectively show the results on open-source prompts and human instructions.\ndatasets. Comparisons of the top-performing En-\nglish and Chinese preference datasets on Cheems-\nBench reveal a critical gap between English and\nChinese preference datasets, which highlights a\nneed for better Chinese preference dataset.\nDataset\nOpen Prompt\nHuman Instruction\nAcc.\nExact.\nAcc.\nExact.\nChinese Preference Datasets\nHH-RLHF-cn\n0.704\n0.306\n0.646\n0.212\nHuozi\n0.728\n0.302\n0.682\n0.237\nKyara\n0.705\n0.258\n0.664\n0.198\nZhihu\n0.463\n0.105\n0.487\n0.080\nEnglish Preference Datasets\nChatbotArena\n0.745\n0.342\n0.718\n0.288\nHH-RLHF\n0.753\n0.351\n0.740\n0.299\nMathPreference\n0.566\n0.179\n0.502\n0.103\nNectar\n0.716\n0.288\n0.664\n0.222\nPKU-SafeRLHF\n0.737\n0.311\n0.678\n0.240\nSkywork\n0.757\n0.343\n0.749\n0.271\nMathStackExchange\n0.749\n0.340\n0.719\n0.256\nUltraFeedback\n0.768\n0.356\n0.748\n0.303\nHelpSteer2\n0.713\n0.279\n0.736\n0.292\nTable 3: Performance results of various datasets. Each\ndataset’s performance is evaluated under Open Prompt\nand Human Instruction subsets, with results presented\nin terms of accuracy (Acc.) and exact match (Exact.).\n6.2\nDownstream Correlation\nIn this section, we explore the correlation of\nCheemsBench with various downstream tasks by\nemploying a Best-of-32 sampling strategy for opti-\nmization on three tasks: Human Win-rate, MT-\nbench-zh (Huozi-Team, 2024), and MT-bench\n(Zheng et al., 2023). For the Human Win-rate task,\nwe use 87 unique Chinese instructions that are not\nincluded in CheemsBench. For each prompt, we\nobtain a fixed baseline response from Qwen2-72B-\nInstruct. Then we sample 32 responses from the\nsame model and have human annotators score each\none. They assign 1 if a response exceeds the base-\nline and −1 if it doesn’t, which allows us to com-\npute win rates. For MT-bench-zh and MT-bench,\nresponses are sampled from Qwen2-7B-Instruct,\nwith RMs performing Best-of-32 sampling on two-\nturn prompts, and GPT-4o is employed as the judge.\nWe select 26 distinct open RMs, differing in train-\ning data and structures, for correlation assessment.\nOur baselines include RewardBench (Lambert et al.,\n2024), RMB (Zhou et al., 2024), and alternatives\nof our benchmarks annotated by GPT-4o, named\nas Open Prompt GPT and Human Instruction GPT.\nThe results in Figure 5 illustrate that: (1) Our\nbenchmark exhibits significantly stronger cor-\nrelations with downstream tasks compared to\nother baselines, whether in Chinese or English\ntasks. (2) The benchmarks annotated by GPT\ndemonstrate suboptimal correlation, underscor-\ning the necessity of human judgment, which can\nachieve better generalization on downstream tasks.\n6.3\nDataset Construction Ablation\nWe conduct an ablation study to assess the effective-\nness of the dataset construction strategies outlined\nin Section 4.2. We train RMs based on Qwen2.5-\n72b-instruct (Team, 2024) to perform experiments\nand report performances in Table 4. The results\nreveal several key insights: (1) Neither Human\nnor GPT subsets alone are sufficient. The GPT\nsubset underperforms on our benchmark, indicat-\ning the inability of GPT-4o to fully capture human\npreferences. Conversely, the Human subset per-\nforms poorly on RewardBench, likely due to its\n7\n\nModel\nRewardBench\nOpen Prompt\nHuman Instruction\nOverall\nAcc.\nExact.\nAcc.\nExact.\nState-of-the-art Baselines\nRewardBench@1\n0.943\n0.751\n0.321\n0.735\n0.294\n0.525\nRewardBench@2\n0.941\n0.750\n0.317\n0.722\n0.271\n0.515\nModels trained using CheemsPreference\nHuman subset\n0.897\n0.852\n0.502\n0.823\n0.412\n0.647\nGPT subset\n0.822\n0.778\n0.373\n0.743\n0.303\n0.549\nw/ Length debiasing\n0.865\n0.790\n0.402\n0.768\n0.322\n0.571\nw/ Distant supervision\n0.909\n0.837\n0.464\n0.821\n0.404\n0.632\nw/ All strategies\n0.917\n0.837\n0.458\n0.826\n0.416\n0.634\nCheemsPreference\n0.919\n0.857\n0.508\n0.832\n0.431\n0.657\nTable 4: The performance of RMs trained on our datasets, along with ablation studies on different processing\nstrategies. CheemsPreference represents a combination of the fully processed GPT subset with the human subset.\nHuman Win-rate\nMT Bench-zh\nMT Bench\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSpearman's rank correlation\nOpen Prompt\nHuman Instruction\nOpen Prompt GPT\nHuman Instruction GPT\nRMB PairWise\nRMB BoN\nRewardBench\nFigure 5: Correlations between different RM benchmarks an performance on three downstream tasks.\nsmaller scale, which limits out-of-distribution per-\nformance. (2) Length-debias strategy enhances\nperformance. We investigate the biases of GPT\nand human annotators in Appendix D.3, highlight-\ning the necessity of a length-debias strategy. (3)\nDistant supervision strategy significantly im-\nproves performance, highlighting the importance\nof incorporating human supervision. (4) The inte-\ngration of all strategies performs the best, under-\nscoring the effectiveness of our approach.\n6.4\nScaling Trend\nWe validate scaling trends on CheemsPreference.\nFigure 6 shows that RM performance improves\nwith increased data volume on Open Prompt and\nHuman Instruction subsets, indicating that larger\ntraining dataset leads to superior performance.\nThis phenomenon also highlights the potential of\nour distant supervision approach. We then assess\nmodel scaling trending by training RM on differ-\nent sizes of Qwen-2.5 series models (Team, 2024).\nFigure 7 illustrates that increasing the model size\nfrom 0.5B to 72B significantly enhances perfor-\nmance, demonstrating that larger models capture\ncomplex preference patterns more effectively.\nMoreover, there is no significant difference when\nstarting training from pretrained or instruct models.\n104\n105\nData Size\n0.79\n0.80\n0.81\n0.82\n0.83\n0.84\n0.85\n0.86\nAccuracy\nSubset\nOpen Prompt\nHuman Instruction\nFigure 6: Impact of data size scaling measured by the\nnumber of pairs on accuracy.\n109\n1010\nModel Size\n0.78\n0.80\n0.82\n0.84\n0.86\nAccuracy\nSubset\nOpen Prompt\nHuman Instruction\nModel Type\nInstruct\nPretrain\nFigure 7: Impact of model size scaling on RM accuracy.\n8\n\n7\nConclusion\nIn this paper, we address the challenges of develop-\ning Chinese RMs by introducing CheemsBench, a\ncomprehensive RM benchmark, and CheemsPref-\nerence, a high-quality Chinese preference dataset.\nUsing these resources, we evaluate the progress of\nRMs in the Chinese context and validate the effec-\ntiveness of our dataset construction strategies. Our\nwork narrows the gap between English and Chinese\nRMs and sets the foundation for future research.\nLimitations\nThis work addresses the resource insufficiency in\nChinese reward models. However, by focusing pri-\nmarily on the Chinese language, the datasets may\nnot fully capture all regional variations, potentially\nintroducing language and cultural biases. Addition-\nally, while the importance of human annotations is\nevident, the subjective nature of human judgment\nand the particular group of annotators involved can\nlead to biased preferences. Moreover, our find-\nings, while tailored to the Chinese context, require\nfurther validation to ensure applicability beyond\nChinese and English languages.\nEthical Considerations\nSeveral ethical considerations are central to this\nwork. Firstly, by releasing real human instructions\nand responses from open-source models, there is\na risk of harmful content being present, necessi-\ntating careful filtering. Our annotation process is\nlargely focused on Chinese contexts, which may\nnot accurately capture preferences from various\ncultures and diverse populations, underscoring the\nneed for greater inclusivity. Furthermore, the re-\nward models, while designed to align with human\npreferences, may not fully capture true human val-\nues, which could lead to unintended consequences\nin downstream applications. We acknowledge these\npotential issues, noting that they are widespread in\nthe research community and require careful atten-\ntion. By highlighting these concerns, we hope to\nfoster more robust solutions in the field.\nReferences\nAnthropic. 2024. Claude 3.5 sonnet. https://www.\nanthropic.com/claude/sonnet.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda\nAskell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan,\nNicholas Joseph, Saurav Kadavath, Jackson Kernion,\nTom Conerly, Sheer El-Showk, Nelson Elhage, Zac\nHatfield-Dodds, Danny Hernandez, Tristan Hume,\nScott Johnston, Shauna Kravec, Liane Lovitt, Neel\nNanda, Catherine Olsson, Dario Amodei, Tom\nBrown, Jack Clark, Sam McCandlish, Chris Olah,\nBen Mann, and Jared Kaplan. 2022.\nTraining\na helpful and harmless assistant with reinforce-\nment learning from human feedback.\nPreprint,\narXiv:2204.05862.\nRalph Allan Bradley and Milton E. Terry. 1952. Rank\nanalysis of incomplete block designs: I. the method\nof paired comparisons. Biometrika, 39(3/4):324.\nDaniel S. Brown and Scott Niekum. 2019.\nDeep\nbayesian reward learning from preferences. Preprint,\narXiv:1912.04472.\nZheng Cai, Maosong Cao, Haojiong Chen, Kai Chen,\nKeyu Chen, Xin Chen, Xun Chen, Zehui Chen,\nZhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan,\nQi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya\nGu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo,\nConghui He, Yingfan Hu, Ting Huang, Tao Jiang,\nPenglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li,\nJingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yin-\ning Li, Hongwei Liu, Jiangning Liu, Jiawei Hong,\nKaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv,\nHaijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma,\nWenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan\nQu, Fukai Shang, Yunfan Shao, Demin Song, Zi-\nfan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze\nTang, Bin Wang, Guoteng Wang, Jiaqi Wang, Ji-\nayu Wang, Rui Wang, Yudong Wang, Ziyi Wang,\nXingjian Wei, Qizhen Weng, Fan Wu, Yingtong\nXiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong\nYan, Xiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia\nYu, Jing Yu, Yuhang Zang, Chuyu Zhang, Li Zhang,\nPan Zhang, Peng Zhang, Ruijie Zhang, Shuo Zhang,\nSongyang Zhang, Wenjian Zhang, Wenwei Zhang,\nXingcheng Zhang, Xinyue Zhang, Hui Zhao, Qian\nZhao, Xiaomeng Zhao, Fengzhe Zhou, Zaida Zhou,\nJingming Zhuo, Yicheng Zou, Xipeng Qiu, Yu Qiao,\nand Dahua Lin. 2024. Internlm2 technical report.\nPreprint, arXiv:2403.17297.\nMaosong Cao, Alexander Lam, Haodong Duan, Hong-\nwei Liu, Songyang Zhang, and Kai Chen. 2024.\nCompassjudger-1: All-in-one judge model helps\nmodel evaluation and evolution.\narXiv preprint\narXiv:2410.16256.\nNuo Chen, Zinan Zheng, Ning Wu, Ming Gong, Dong-\nmei Zhang, and Jia Li. 2024. Breaking language\nbarriers in multilingual mathematical reasoning: In-\nsights and observations. Preprint, arXiv:2310.20246.\nQinyuan Cheng, Tianxiang Sun, Wenwei Zhang, Siyin\nWang, Xiangyang Liu, Mozhi Zhang, Junliang He,\nMianqiu Huang, Zhangyue Yin, Kai Chen, and\nXipeng Qiu. 2023. Evaluating hallucinations in chi-\nnese large language models. CoRR, abs/2310.03368.\nGanqu Cui, Lifan Yuan, Ning Ding, Guanming Yao,\nWei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and\n9\n\nMaosong Sun. 2023. Ultrafeedback: Boosting lan-\nguage models with high-quality feedback. Preprint,\narXiv:2310.01377.\nHanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan,\nShizhe Diao, Jipeng Zhang, Kashun Shum, and\nTong Zhang. 2023. Raft: Reward ranked finetuning\nfor generative foundation model alignment. arXiv\npreprint arXiv:2304.06767.\nNicolai Dorka. 2024.\nQuantile regression for dis-\ntributional reward models in rlhf. arXiv preprint\narXiv:2409.10164.\nYann Dubois, Balázs Galambosi, Percy Liang, and Tat-\nsunori B. Hashimoto. 2024. Length-controlled al-\npacaeval: A simple way to debias automatic evalua-\ntors. Preprint, arXiv:2404.04475.\nEvan Frick, Tianle Li, Connor Chen, Wei-Lin Chiang,\nAnastasios N. Angelopoulos, Jiantao Jiao, Banghua\nZhu, Joseph E. Gonzalez, and Ion Stoica. 2024.\nHow to evaluate reward models for rlhf. Preprint,\narXiv:2410.14872.\nDeep Ganguli, Liane Lovitt, Jackson Kernion, Amanda\nAskell, Yuntao Bai, Saurav Kadavath, Ben Mann,\nEthan Perez, Nicholas Schiefer, Kamal Ndousse,\nAndy Jones, Sam Bowman, Anna Chen, Tom Con-\nerly, Nova DasSarma, Dawn Drain, Nelson Elhage,\nSheer El-Showk, Stanislav Fort, Zac Hatfield-Dodds,\nTom Henighan, Danny Hernandez, Tristan Hume,\nJosh Jacobson, Scott Johnston, Shauna Kravec,\nCatherine Olsson, Sam Ringer, Eli Tran-Johnson,\nDario Amodei, Tom Brown, Nicholas Joseph, Sam\nMcCandlish, Chris Olah, Jared Kaplan, and Jack\nClark. 2022. Red teaming language models to re-\nduce harms: Methods, scaling behaviors, and lessons\nlearned. Preprint, arXiv:2209.07858.\nYang Gao, Dana Alon, and Donald Metzler. 2024. Im-\npact of preference noise on the alignment perfor-\nmance of generative language models.\nPreprint,\narXiv:2404.09824.\nTeam GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chen-\nhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Han-\nlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Ji-\nadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie\nTang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu,\nLucen Zhong, Mingdao Liu, Minlie Huang, Peng\nZhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shu-\ndan Zhang, Shulin Cao, Shuxun Yang, Weng Lam\nTam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan\nZhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu,\nXinyue Yang, Xixuan Song, Xunkai Zhang, Yifan\nAn, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li,\nYushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang,\nZhen Yang, Zhengxiao Du, Zhenyu Hou, and Zihan\nWang. 2024. Chatglm: A family of large language\nmodels from glm-130b to glm-4 all tools. Preprint,\narXiv:2406.12793.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,\nAbhinav Pandey, Abhishek Kadian, Ahmad Al-\nDahle, Aiesha Letman, Akhil Mathur, Alan Schel-\nten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh\nGoyal, Anthony Hartshorn, Aobo Yang, Archi Mi-\ntra, Archie Sravankumar, Artem Korenev, Arthur\nHinsvark, Arun Rao, Aston Zhang, Aurelien Ro-\ndriguez, Austen Gregerson, Ava Spataru, Baptiste\nRoziere, Bethany Biron, Binh Tang, Bobbie Chern,\nCharlotte Caucheteux, Chaya Nayak, Chloe Bi,\nChris Marra, Chris McConnell, Christian Keller,\nChristophe Touret, Chunyang Wu, Corinne Wong,\nCristian Canton Ferrer, Cyrus Nikolaidis, Damien Al-\nlonsius, Daniel Song, Danielle Pintz, Danny Livshits,\nDanny Wyatt, David Esiobu, Dhruv Choudhary,\nDhruv Mahajan, Diego Garcia-Olano, Diego Perino,\nDieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy,\nElina Lobanova, Emily Dinan, Eric Michael Smith,\nFilip Radenovic, Francisco Guzmán, Frank Zhang,\nGabriel Synnaeve, Gabrielle Lee, Georgia Lewis An-\nderson, Govind Thattai, Graeme Nail, Gregoire Mi-\nalon, Guan Pang, Guillem Cucurell, Hailey Nguyen,\nHannah Korevaar, Hu Xu, Hugo Touvron, Iliyan\nZarov, Imanol Arrieta Ibarra, Isabel Kloumann, Is-\nhan Misra, Ivan Evtimov, Jack Zhang, Jade Copet,\nJaewon Lee, Jan Geffert, Jana Vranes, Jason Park,\nJay Mahadeokar, Jeet Shah, Jelmer van der Linde,\nJennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu,\nJianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang,\nJiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park,\nJoseph Rocca, Joshua Johnstun, Joshua Saxe, Jun-\nteng Jia, Kalyan Vasuden Alwala, Karthik Prasad,\nKartikeya Upasani, Kate Plawiak, Ke Li, Kenneth\nHeafield, Kevin Stone, Khalid El-Arini, Krithika Iyer,\nKshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal\nLakhotia, Lauren Rantala-Yeary, Laurens van der\nMaaten, Lawrence Chen, Liang Tan, Liz Jenkins,\nLouis Martin, Lovish Madaan, Lubo Malo, Lukas\nBlecher, Lukas Landzaat, Luke de Oliveira, Madeline\nMuzzi, Mahesh Pasupuleti, Mannat Singh, Manohar\nPaluri, Marcin Kardas, Maria Tsimpoukelli, Mathew\nOldham, Mathieu Rita, Maya Pavlova, Melanie Kam-\nbadur, Mike Lewis, Min Si, Mitesh Kumar Singh,\nMona Hassan, Naman Goyal, Narjes Torabi, Niko-\nlay Bashlykov, Nikolay Bogoychev, Niladri Chatterji,\nNing Zhang, Olivier Duchenne, Onur Çelebi, Patrick\nAlrassy, Pengchuan Zhang, Pengwei Li, Petar Va-\nsic, Peter Weng, Prajjwal Bhargava, Pratik Dubal,\nPraveen Krishnan, Punit Singh Koura, Puxin Xu,\nQing He, Qingxiao Dong, Ragavan Srinivasan, Raj\nGanapathy, Ramon Calderer, Ricardo Silveira Cabral,\nRobert Stojnic, Roberta Raileanu, Rohan Maheswari,\nRohit Girdhar, Rohit Patel, Romain Sauvestre, Ron-\nnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan\nSilva, Rui Hou, Rui Wang, Saghar Hosseini, Sa-\nhana Chennabasappa, Sanjay Singh, Sean Bell, Seo-\nhyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sha-\nran Narang, Sharath Raparthy, Sheng Shen, Shengye\nWan, Shruti Bhosale, Shun Zhang, Simon Van-\ndenhende, Soumya Batra, Spencer Whitman, Sten\nSootla, Stephane Collot, Suchin Gururangan, Syd-\nney Borodinsky, Tamar Herman, Tara Fowler, Tarek\nSheasha, Thomas Georgiou, Thomas Scialom, Tobias\nSpeckbacher, Todor Mihaylov, Tong Xiao, Ujjwal\nKarn, Vedanuj Goswami, Vibhor Gupta, Vignesh\nRamanathan, Viktor Kerkez, Vincent Gonguet, Vir-\n10\n\nginie Do, Vish Vogeti, Vítor Albiero, Vladan Petro-\nvic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whit-\nney Meers, Xavier Martinet, Xiaodong Wang, Xi-\naofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xin-\nfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Gold-\nschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen,\nYiwen Song, Yuchen Zhang, Yue Li, Yuning Mao,\nZacharie Delpierre Coudert, Zheng Yan, Zhengxing\nChen, Zoe Papakipos, Aaditya Singh, Aayushi Sri-\nvastava, Abha Jain, Adam Kelsey, Adam Shajnfeld,\nAdithya Gangidi, Adolfo Victoria, Ahuva Goldstand,\nAjay Menon, Ajay Sharma, Alex Boesenberg, Alexei\nBaevski, Allie Feinstein, Amanda Kallet, Amit San-\ngani, Amos Teo, Anam Yunus, Andrei Lupu, An-\ndres Alvarado, Andrew Caples, Andrew Gu, Andrew\nHo, Andrew Poulton, Andrew Ryan, Ankit Ramchan-\ndani, Annie Dong, Annie Franco, Anuj Goyal, Apara-\njita Saraf, Arkabandhu Chowdhury, Ashley Gabriel,\nAshwin Bharambe, Assaf Eisenman, Azadeh Yaz-\ndan, Beau James, Ben Maurer, Benjamin Leonhardi,\nBernie Huang, Beth Loyd, Beto De Paola, Bhargavi\nParanjape, Bing Liu, Bo Wu, Boyu Ni, Braden Han-\ncock, Bram Wasti, Brandon Spence, Brani Stojkovic,\nBrian Gamido, Britt Montalvo, Carl Parker, Carly\nBurton, Catalina Mejia, Ce Liu, Changhan Wang,\nChangkyu Kim, Chao Zhou, Chester Hu, Ching-\nHsiang Chu, Chris Cai, Chris Tindal, Christoph Fe-\nichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty,\nDaniel Kreymer, Daniel Li, David Adkins, David\nXu, Davide Testuggine, Delia David, Devi Parikh,\nDiana Liskovich, Didem Foss, Dingkang Wang, Duc\nLe, Dustin Holland, Edward Dowling, Eissa Jamil,\nElaine Montgomery, Eleonora Presani, Emily Hahn,\nEmily Wood, Eric-Tuan Le, Erik Brinkman, Este-\nban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun,\nFelix Kreuk, Feng Tian, Filippos Kokkinos, Firat\nOzgenel, Francesco Caggioni, Frank Kanayet, Frank\nSeide, Gabriela Medina Florez, Gabriella Schwarz,\nGada Badeer, Georgia Swee, Gil Halpern, Grant\nHerman, Grigory Sizov, Guangyi, Zhang, Guna\nLakshminarayanan, Hakan Inan, Hamid Shojanaz-\neri, Han Zou, Hannah Wang, Hanwen Zha, Haroun\nHabeeb, Harrison Rudolph, Helen Suk, Henry As-\npegren, Hunter Goldman, Hongyuan Zhan, Ibrahim\nDamlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis,\nIrina-Elena Veliche, Itai Gat, Jake Weissman, James\nGeboski, James Kohli, Janice Lam, Japhet Asher,\nJean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jen-\nnifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy\nTeboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe\nCummings, Jon Carvill, Jon Shepard, Jonathan Mc-\nPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang,\nKai Wu, Kam Hou U, Karan Saxena, Kartikay Khan-\ndelwal, Katayoun Zand, Kathy Matosich, Kaushik\nVeeraraghavan, Kelly Michelena, Keqian Li, Ki-\nran Jagadeesh, Kun Huang, Kunal Chawla, Kyle\nHuang, Lailin Chen, Lakshya Garg, Lavender A,\nLeandro Silva, Lee Bell, Lei Zhang, Liangpeng\nGuo, Licheng Yu, Liron Moshkovich, Luca Wehrst-\nedt, Madian Khabsa, Manav Avalani, Manish Bhatt,\nMartynas Mankus, Matan Hasson, Matthew Lennie,\nMatthias Reso, Maxim Groshev, Maxim Naumov,\nMaya Lathi, Meghan Keneally, Miao Liu, Michael L.\nSeltzer, Michal Valko, Michelle Restrepo, Mihir Pa-\ntel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark,\nMike Macey, Mike Wang, Miquel Jubert Hermoso,\nMo Metanat, Mohammad Rastegari, Munish Bansal,\nNandhini Santhanam, Natascha Parks, Natasha\nWhite, Navyata Bawa, Nayan Singhal, Nick Egebo,\nNicolas Usunier, Nikhil Mehta, Nikolay Pavlovich\nLaptev, Ning Dong, Norman Cheng, Oleg Chernoguz,\nOlivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin\nKent, Parth Parekh, Paul Saab, Pavan Balaji, Pe-\ndro Rittner, Philip Bontrager, Pierre Roux, Piotr\nDollar, Polina Zvyagina, Prashant Ratanchandani,\nPritish Yuvraj, Qian Liang, Rachad Alao, Rachel\nRodriguez, Rafi Ayub, Raghotham Murthy, Raghu\nNayani, Rahul Mitra, Rangaprabhu Parthasarathy,\nRaymond Li, Rebekkah Hogan, Robin Battey, Rocky\nWang, Russ Howes, Ruty Rinott, Sachin Mehta,\nSachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara\nChugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov,\nSatadru Pan, Saurabh Mahajan, Saurabh Verma,\nSeiji Yamamoto, Sharadh Ramaswamy, Shaun Lind-\nsay, Shaun Lindsay, Sheng Feng, Shenghao Lin,\nShengxin Cindy Zha, Shishir Patil, Shiva Shankar,\nShuqiang Zhang, Shuqiang Zhang, Sinong Wang,\nSneha Agarwal, Soji Sajuyigbe, Soumith Chintala,\nStephanie Max, Stephen Chen, Steve Kehoe, Steve\nSatterfield, Sudarshan Govindaprasad, Sumit Gupta,\nSummer Deng, Sungmin Cho, Sunny Virk, Suraj\nSubramanian, Sy Choudhury, Sydney Goldman, Tal\nRemez, Tamar Glaser, Tamara Best, Thilo Koehler,\nThomas Robinson, Tianhe Li, Tianjun Zhang, Tim\nMatthews, Timothy Chou, Tzook Shaked, Varun\nVontimitta, Victoria Ajayi, Victoria Montanez, Vijai\nMohan, Vinay Satish Kumar, Vishal Mangla, Vlad\nIonescu, Vlad Poenaru, Vlad Tiberiu Mihailescu,\nVladimir Ivanov, Wei Li, Wenchen Wang, Wen-\nwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng\nTang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo\nGao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia,\nYe Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi,\nYoungjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao,\nYundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary\nDeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang,\nZhiwei Zhao, and Zhiyu Ma. 2024. The llama 3 herd\nof models. Preprint, arXiv:2407.21783.\nSrishti Gureja, Lester James V. Miranda, Shayekh Bin\nIslam, Rishabh Maheshwary, Drishti Sharma, Gusti\nWinata, Nathan Lambert, Sebastian Ruder, Sara\nHooker, and Marzieh Fadaee. 2024. M-rewardbench:\nEvaluating reward models in multilingual settings.\nPreprint, arXiv:2410.15522.\nZhenyu Hou, Yilin Niu, Zhengxiao Du, Xiaohan\nZhang, Xiao Liu, Aohan Zeng, Qinkai Zheng, Minlie\nHuang, Hongning Wang, Jie Tang, and Yuxiao Dong.\n2024.\nChatglm-rlhf: Practices of aligning large\nlanguage models with human feedback. Preprint,\narXiv:2404.00934.\nKexin Huang, Xiangyang Liu, Qianyu Guo, Tianxi-\nang Sun, Jiawei Sun, Yaru Wang, Zeyang Zhou,\nYixu Wang, Yan Teng, Xipeng Qiu, Yingchun Wang,\nand Dahua Lin. 2023.\nFlames:\nBenchmarking\nvalue alignment of chinese large language models.\nPreprint, arXiv:2311.06899.\n11\n\nHuozi-Team. 2024. Huozi: Leveraging large language\nmodels for enhanced open-domain chatting. https:\n//github.com/HIT-SCIR/huozi.\nBorja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving,\nShane Legg, and Dario Amodei. 2018. Reward learn-\ning from human preferences and demonstrations in\natari. Preprint, arXiv:1811.06521.\nJiaming Ji, Donghai Hong, Borong Zhang, Boyuan\nChen, Josef Dai, Boren Zheng, Tianyi Qiu, Boxun\nLi, and Yaodong Yang. 2024. Pku-saferlhf: Towards\nmulti-level safety alignment for llms with human\npreference. arXiv preprint arXiv:2406.15513.\nMaxim Khanov, Jirayu Burapacheep, and Yixuan Li.\n2024. Args: Alignment as reward-guided search.\nPreprint, arXiv:2402.01694.\nSunghwan Kim, Dongjin Kang, Taeyoon Kwon,\nHyungjoo Chae, Jungsoo Won, Dongha Lee, and\nJinyoung Yeo. 2024. Evaluating robustness of re-\nward models for mathematical reasoning. Preprint,\narXiv:2410.01729.\nNathan Lambert, Valentina Pyatkin, Jacob Morrison,\nLJ Miranda, Bill Yuchen Lin, Khyathi Chandu,\nNouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi,\nNoah A. Smith, and Hannaneh Hajishirzi. 2024. Re-\nwardbench: Evaluating reward models for language\nmodeling. Preprint, arXiv:2403.13787.\nNathan Lambert, Lewis Tunstall, Nazneen Rajani, and\nTristan Thrush. 2023.\nHuggingface h4 stack ex-\nchange preference dataset.\nJein\nLee.\n2023.\nchinese-llm-benchmark.\nhttps://github.com/jeinlee1991/\nchinese-llm-benchmark.\nBolian Li, Yifan Wang, Ananth Grama, and Ruqi\nZhang. 2024a.\nCascade reward sampling for\nefficient decoding-time alignment.\nPreprint,\narXiv:2406.16306.\nZongjie Li, Chaozheng Wang, Pingchuan Ma, Daoyuan\nWu, Shuai Wang, Cuiyun Gao, and Yang Liu. 2024b.\nSplit and merge: Aligning position biases in llm-\nbased evaluators. Preprint, arXiv:2310.01432.\nMingan Lin, Fan Yang, Yanjun Shen, Haoze Sun, Tian-\npeng Li, Tao Zhang, Chenzheng Zhu, Tao Zhang,\nMiao Zheng, Xu Li, Yijie Zhou, Mingyang Chen,\nYanzhao Qin, Youquan Li, Hao Liang, Fei Li, Yadong\nLi, Mang Wang, Guosheng Dong, Kun Fang, Jian-\nhua Xu, Bin Cui, Wentao Zhang, Zenan Zhou, and\nWeipeng Chen. 2024. Baichuan alignment technical\nreport. Preprint, arXiv:2410.14940.\nChris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Ju-\njie He, Chaojie Wang, Shuicheng Yan, Yang Liu,\nand Yahui Zhou. 2024a.\nSkywork-reward: Bag\nof tricks for reward modeling in llms.\nPreprint,\narXiv:2410.18451.\nXiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang,\nZhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan\nXu, Weng Lam Tam, Xiaohan Zhang, Lichao Sun,\nHongning Wang, Jing Zhang, Minlie Huang, Yux-\niao Dong, and Jie Tang. 2023. Alignbench: Bench-\nmarking chinese alignment of large language models.\nPreprint, arXiv:2311.18743.\nYantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou,\nand Juanzi Li. 2024b. Rm-bench: Benchmarking\nreward models of language models with subtlety and\nstyle. Preprint, arXiv:2410.16184.\nXingzhou Lou, Dong Yan, Wei Shen, Yuzi Yan, Jian Xie,\nand Junge Zhang. 2024. Uncertainty-aware reward\nmodel: Teaching reward models to know what is\nunknown. arXiv preprint arXiv:2410.00847.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,\nXu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen\nKrueger, Kevin Button, Matthew Knight, Benjamin\nChess, and John Schulman. 2022. Webgpt: Browser-\nassisted question-answering with human feedback.\nPreprint, arXiv:2112.09332.\nAndrew Y. Ng and Stuart J. Russell. 2000. Algorithms\nfor inverse reinforcement learning. In Proceedings\nof the Seventeenth International Conference on Ma-\nchine Learning, ICML ’00, page 663–670, San Fran-\ncisco, CA, USA. Morgan Kaufmann Publishers Inc.\nOpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,\nLama Ahmad, Ilge Akkaya, Florencia Leoni Ale-\nman, Diogo Almeida, Janko Altenschmidt, Sam Alt-\nman, Shyamal Anadkat, Red Avila, Igor Babuschkin,\nSuchir Balaji, Valerie Balcom, Paul Baltescu, Haim-\ning Bao, Mohammad Bavarian, Jeff Belgum, Ir-\nwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro,\nChristopher Berner, Lenny Bogdonoff, Oleg Boiko,\nMadelaine Boyd, Anna-Luisa Brakman, Greg Brock-\nman, Tim Brooks, Miles Brundage, Kevin Button,\nTrevor Cai, Rosie Campbell, Andrew Cann, Brittany\nCarey, Chelsea Carlson, Rory Carmichael, Brooke\nChan, Che Chang, Fotis Chantzis, Derek Chen, Sully\nChen, Ruby Chen, Jason Chen, Mark Chen, Ben\nChess, Chester Cho, Casey Chu, Hyung Won Chung,\nDave Cummings, Jeremiah Currier, Yunxing Dai,\nCory Decareaux, Thomas Degry, Noah Deutsch,\nDamien Deville, Arka Dhar, David Dohan, Steve\nDowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti,\nTyna Eloundou, David Farhi, Liam Fedus, Niko Felix,\nSimón Posada Fishman, Juston Forte, Isabella Ful-\nford, Leo Gao, Elie Georges, Christian Gibson, Vik\nGoel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-\nLopes, Jonathan Gordon, Morgan Grafstein, Scott\nGray, Ryan Greene, Joshua Gross, Shixiang Shane\nGu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris,\nYuchen He, Mike Heaton, Johannes Heidecke, Chris\nHesse, Alan Hickey, Wade Hickey, Peter Hoeschele,\nBrandon Houghton, Kenny Hsu, Shengli Hu, Xin\nHu, Joost Huizinga, Shantanu Jain, Shawn Jain,\nJoanne Jang, Angela Jiang, Roger Jiang, Haozhun\n12\n\nJin, Denny Jin, Shino Jomoto, Billie Jonn, Hee-\nwoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Ka-\nmali, Ingmar Kanitscheider, Nitish Shirish Keskar,\nTabarak Khan, Logan Kilpatrick, Jong Wook Kim,\nChristina Kim, Yongjik Kim, Jan Hendrik Kirch-\nner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,\nŁukasz Kondraciuk, Andrew Kondrich, Aris Kon-\nstantinidis, Kyle Kosic, Gretchen Krueger, Vishal\nKuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike,\nJade Leung, Daniel Levy, Chak Ming Li, Rachel Lim,\nMolly Lin, Stephanie Lin, Mateusz Litwin, Theresa\nLopez, Ryan Lowe, Patricia Lue, Anna Makanju,\nKim Malfacini, Sam Manning, Todor Markov, Yaniv\nMarkovski, Bianca Martin, Katie Mayer, Andrew\nMayne, Bob McGrew, Scott Mayer McKinney, Chris-\ntine McLeavey, Paul McMillan, Jake McNeil, David\nMedina, Aalok Mehta, Jacob Menick, Luke Metz,\nAndrey Mishchenko, et al. 2024. Gpt-4 technical\nreport. Preprint, arXiv:2303.08774.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. Preprint, arXiv:2203.02155.\nMalayandi Palan, Nicholas C. Landolfi, Gleb Shevchuk,\nand Dorsa Sadigh. 2019.\nLearning reward func-\ntions by integrating human demonstrations and pref-\nerences. Preprint, arXiv:1906.08928.\nJunsoo Park, Seungyeon Jwa, Meiying Ren, Daeyoung\nKim, and Sanghyuk Choi. 2024. Offsetbias: Lever-\naging debiased data for tuning evaluators. Preprint,\narXiv:2407.06551.\nQiwei Peng, Yekun Chai, and Xuhong Li. 2024.\nHumaneval-xl:\nA multilingual code generation\nbenchmark for cross-lingual natural language gen-\neralization. Preprint, arXiv:2402.16694.\nGuijin Son, Hyunwoo Ko, Hoyoung Lee, Yewon Kim,\nand Seunghyeok Hong. 2024. Llm-as-a-judge & re-\nward model: What they can and cannot do. Preprint,\narXiv:2409.11239.\nNisan Stiennon, Long Ouyang, Jeff Wu, Daniel M.\nZiegler, Ryan Lowe, Chelsea Voss, Alec Radford,\nDario Amodei, and Paul Christiano. 2022. Learn-\ning to summarize from human feedback. Preprint,\narXiv:2009.01325.\nRickard Stureborg, Dimitris Alikaniotis, and Yoshi\nSuhara. 2024.\nLarge language models are\ninconsistent and biased evaluators.\nPreprint,\narXiv:2405.01724.\nQwen Team. 2024. Qwen2.5: A party of foundation\nmodels.\nHaoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao,\nand Tong Zhang. 2024a. Interpretable preferences\nvia multi-objective reward modeling and mixture-of-\nexperts. In EMNLP.\nShenzhi Wang, Yaowei Zheng, Guoyin Wang, Shiji\nSong, and Gao Huang. 2024b. Llama3.1-8b-chinese-\nchat.\nZhilin Wang, Alexander Bukharin, Olivier Delal-\nleau, Daniel Egert, Gerald Shen, Jiaqi Zeng, Olek-\nsii Kuchaiev, and Yi Dong. 2024c.\nHelpsteer2-\npreference: Complementing ratings with preferences.\nPreprint, arXiv:2410.01257.\nZhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi\nZeng, Gerald Shen, Daniel Egert, Jimmy J. Zhang,\nMakesh Narsimhan Sreedhar, and Oleksii Kuchaiev.\n2024d.\nHelpsteer2:\nOpen-source dataset for\ntraining top-performing reward models. Preprint,\narXiv:2406.08673.\nXueru Wen, Jie Lou, Yaojie Lu, Hongyu Lin, Xing\nYu, Xinyu Lu, Ben He, Xianpei Han, Debing Zhang,\nand Le Sun. 2024. Rethinking reward model evalu-\nation: Are we barking up the wrong tree? Preprint,\narXiv:2410.05584.\nshareAI Xinlu Lai. 2024. The dpo dataset for chinese\nand english with emoji. https://huggingface.co/\ndatasets/shareAI/DPO-zh-en-emoji.\nWei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang,\nHan Zhong, Heng Ji, Nan Jiang, and Tong Zhang.\n2024. Iterative preference learning from human feed-\nback: Bridging theory and practice for rlhf under\nkl-constraint. Preprint, arXiv:2312.11456.\nGuohai Xu, Jiayi Liu, Ming Yan, Haotian Xu, Jinghui\nSi, Zhuoran Zhou, Peng Yi, Xing Gao, Jitao Sang,\nRong Zhang, Ji Zhang, Chao Peng, Fei Huang, and\nJingren Zhou. 2023. Cvalues: Measuring the val-\nues of chinese large language models from safety to\nresponsibility. Preprint, arXiv:2307.09705.\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng,\nBowen Yu, Chang Zhou, Chengpeng Li, Chengyuan\nLi, Dayiheng Liu, Fei Huang, Guanting Dong, Hao-\nran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian\nYang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin\nXu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang\nLin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang,\nMei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng\nWang, Ru Peng, Rui Men, Ruize Gao, Runji Lin,\nShijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu,\nTianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng,\nXiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin\nWei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang\nZhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu\nCui, Zhenru Zhang, and Zhihao Fan. 2024a. Qwen2\ntechnical report. arXiv preprint arXiv:2407.10671.\nKai-Chou Yang. 2024. Kyara.\nRui Yang, Ruomeng Ding, Yong Lin, Huan Zhang, and\nTong Zhang. 2024b. Regularizing hidden states en-\nables learning generalizable reward model for llms.\narXiv preprint arXiv:2406.10216.\nHuimu Yu, Xing Wu, Weidong Yin, Debing Zhang, and\nSonglin Hu. 2024. Codepmp: Scalable preference\n13\n\nmodel pretraining for large language model reason-\ning. Preprint, arXiv:2410.02229.\nLi Yucheng. 2023. 3,000 chinese zhihu q&a preference\ndataset.\nhttps://huggingface.co/datasets/\nliyucheng/zhihu_rlhf_3k.\nyuelin bai. 2023. Coig-cqia: Quality is all you need for\nchinese instruction fine-tuning. https://github.\ncom/paralym/COIG-CQIA.\nMichael JQ Zhang, Zhilin Wang, Jena D. Hwang,\nYi Dong, Olivier Delalleau, Yejin Choi, Eunsol Choi,\nXiang Ren, and Valentina Pyatkin. 2024a. Diverging\npreferences: When do annotators disagree and do\nmodels know? Preprint, arXiv:2410.14632.\nXiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying,\nLiang He, and Xipeng Qiu. 2024b. Evaluating the\nperformance of large language models on gaokao\nbenchmark. Preprint, arXiv:2305.12474.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judg-\ning llm-as-a-judge with mt-bench and chatbot arena.\nPreprint, arXiv:2306.05685.\nEnyu Zhou, Guodong Zheng, Binghai Wang, Zhiheng\nXi, Shihan Dou, Rong Bao, Wei Shen, Limao Xiong,\nJessica Fan, Yurong Mou, Rui Zheng, Tao Gui,\nQi Zhang, and Xuanjing Huang. 2024. Rmb: Com-\nprehensively benchmarking reward models in llm\nalignment. Preprint, arXiv:2410.09893.\nBanghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu,\nand Jiantao Jiao. 2023. Starling-7b: Improving llm\nhelpfulness & harmlessness with rlaif.\nA\nPrompt Category\nOur instruction dataset is constructed using a dual-\nsource collection strategy. The primary source\ncomprises real human queries collected from pro-\nduction environments, ensuring authenticity and\npractical relevance. This is complemented by GPT-\nenhanced open-source data that undergoes rigorous\nhuman curation to maintain quality standards. To\nensure comprehensive coverage and diversity, we\ndeveloped a systematic taxonomy to guide our data\ncollection process. This taxonomy helps categorize\ninstructions across various dimensions, including\ntask types (e.g., comprehension, knowledge-based,\ncreative, reasoning, and mathematical), complexity\nlevels, and application scenarios. Each collected\nprompt is carefully reviewed and categorized ac-\ncording to this taxonomy, allowing us to maintain\na balanced distribution across different types of\ninstruction. The prompt category taxonomy for\nCheemsBench is illustrated in Figure 8 to 9, while\nthe promot category taxonomy for CheemsPrefer-\nence is illustrated in Figure 10.\nSafety\nReasoning\nKnowledge\nLanguage\nBias and Discrimination\nNon-Anthropomorphism\nSocial Norms Violation\nChinese Values\nProperty Safety\nHarmful Speech\nNot Environmentally Friendly\nPhysical Harm\nPsychological Harm\nPersonal Data\nIllegal Behavior\nSensitive Information\nLogical\nElementary Mathematics\nApplied Mathematics\nAdvanced Mathematics\nHistory\nGeography\nClassical Chinese\nBiomedical\nLiterature\nCommon Sense\nSports\nEducation\nIdiom\nFood\nMusic\nPseudoscience\nSuperstition\nAdvertising\nMythology\nOthers\nPoetry\nFilm\nParanormal Events\nPhysics\nScience\nScience Fiction\nScientific Knowledge\nArt\nCustoms\nEconomy\nHealth\nOther\nAstronomy\nCelebrities\nChemistry\nGames\nStereotype\nCultural Understanding\nMetaphor Understanding\nIdiom Explanation\nClassical Chinese Translation\nLiterary Reading\nInformation Extraction\nOther\nPractical Document Reading\nTable Q&A\nReading Comprehension\nWord Understanding\nCreative Writing\nClassical Poetry Continuation\nPractical Writing\nReal Life\nText Classification\nEmoji\nGames and Entertainment\nSeeking Advice\nTranslation\nContemporary Celebrities\nFunction\nOpinion Expression\nProfessional Writing\nCouplet\nEnglish Error Correction\nVirtual Romance\nFigure 8: Category system for open-source prompts,\nwhich are selected from various datasets and manually\nintegrated into this unified framework.\nComplex instruction\nCreation\nUnderstanding\nSafety\nKnowledge\nReasoning\nMathematics\nComposite ability\nRole playing\nString operation\nOriginal\nBrainstorming\nImitation writing\nRewrite\nPolishing\nAdaptation\nAmplification\nContinuation\nPagination\nSemantic understanding\nSyntax analysis\nSummarization\nText manipulation\nInformation extraction\nTranslation\nValues\nPopular science\nCommon sense\nLiterature knowledge\nFalse premise\nRefusal to answer\nCompound reasoning\nDeductive reasoning\nProblem solving\nTask planning\nTemporal reasoning\nInductive reasoning\nAlgebraic calculation\nPlane geometry\nNumber theory\nProbability and statistics\nEquation\nSequence\nEquation design and computation\nFunction\nSolid geometry\nIntegral\nLinear algebra\nFigure 9: Category system for human instructions. Due\nto the complexity of the full system, only the first two\ntiers of classification are displayed.\nB\nAnnotation Prompts\nIn this work, we leverage GPT-4o for constructing\nour preference dataset. We utilize the structured\njudge prompt presented in Figure 11 to assess re-\nsponse quality, emphasizing an objective and unbi-\nased comparison between different model outputs.\nEach prompt is assigned a specific criterion ac-\ncording to its category. These criteria ensure that\n14\n\nCreation\nCode\nMathematics\nKnowledge\nReasoning\nUnderstanding\nComplex instruction\nAmbiguous instruction\nSafety\nOriginal\nBrainstorming\nRewrite\nImitation writing\nContinuation\nPolishing\nAmplification\nAdaptation\nPagination\nCode understanding\nCode writing\nCode application\nOther categories\nAlgebraic calculation\nPlane geometry\nProbability and statistics\nNumber theory\nEquation\nFunction\nSequence\nEquation design and computation\nSolid geometry\nLinear algebra\nLiterature knowledge\nCommon sense\nFalse premise\nRefusal to answer\nCompound reasoning\nProblem solving\nDeductive reasoning\nTask planning\nCode\nTemporal reasoning\nInductive reasoning\nSemantic understanding\nText manipulation\nSyntax analysis\nInformation extraction\nSummarization\nTranslation\nComposite ability\nRole playing\nString operation\nValues\nPopular science\nFigure 10: Category system for prompts in the Chinese\nPreference Dataset. We only plot the first two-tier clas-\nsification due to the complexity of the complete system.\nthe evaluations are consistent and comprehensive\nacross different contexts. Figure 13 provides a de-\ntailed overview of the criteria in Chinese, covering\nlinguistic and logical aspects. It also accounts for\nthe safety and complexity of instructions. 7\nC\nConflict Resolving\nIn this section, we introduce an algorithm designed\nto address potential annotation conflicts that arise\nfrom human evaluations. The Conflict Resolving\nAlgorithm, as outlined in Algorithm 1, operates\nby systematically integrating conflicting responses\ninto larger nodes, based on the understanding that\nthese responses exhibit comparable quality. The al-\ngorithm begins by constructing a graph with nodes\nrepresenting individual responses. Directed edges\nare established based on preference relationships\nbetween responses. To handle cycles, which indi-\ncate conflicting annotations, the algorithm employs\na depth-first search (DFS) to detect and merge these\ncycles into super-nodes iteratively. This merging\nprocess helps conceptualize the similarity in quality\namong the involved responses. In the final step, a\ntopological sorting algorithm is applied to derive a\npartial ranking of responses. We report the conflict\nrate between human annotations and GPT annota-\ntions on the Open Prompts and Human Instruction\nsubsets in Table 5. The conflict rate is determined\n7The English versions of the judge prompt template and\ncriteria are displayed in Figure 12 and 14.\nby comparing the consistency between the original\nannotation results and the response rankings pro-\ncessed by the algorithm. We find that, overall, GPT\nis more inconsistent than human annotators. Addi-\ntionally, the conflict rate in the Human Instruction\nsubset is higher than in the Open Prompt subset,\nsuggesting that prompts in this subset may be more\nchallenging for preference annotation.\nTable 5: Conflict ratio of human annotations and GPT-\n4o annotations.\nDataset\nConflict Ratio\nOpen Prompt Human\n0.1999\nHuman Instruction Human\n0.2161\nOpen Prompt GPT\n0.2593\nHuman Instruction GPT\n0.3170\nD\nHuman Annotation\nWe employ a team of 29 professional annotators,\neach holding a bachelor’s degree, who work stan-\ndard business hours (8 hours of active annotation\ntime per day). On average, an annotator completes\napproximately 40 triple-wise comparisons per day,\nwith the flexibility to use any necessary tools and\nresources for fact-checking and verification.\nD.1\nAnnotation Pipeline\nOur prompt assignment system divides tasks ac-\ncording to the prompt category and distributes them\nto annotators based on their domain expertise and\nperformance history.\nTo ensure data quality, we implement a com-\nprehensive multi-stage verification process, which\nhas been tested and improved through more than\nsix months of practical applications in preference\ndataset production before being applied to the\nCheemsBench annotation process.\nSpecifically, each prompt first undergoes double-\nblind annotation where two independent annotators\nmust achieve 90% agreement. When discrepancies\noccur, annotators engage in alignment discussions\nto reach consensus based on established annotation\nguidelines rather than personal judgment. When\nsignificant disagreements cannot be resolved, the\ncases are forwarded to data delivery teams, data\noperations teams, and finally algorithm developers\nfor further review and guidance.\nFor quality assurance, we employ a cascading\nsingle-blind review system. First, data delivery\n15\n\nAlgorithm 1 Conflict Resolving Algorithm\n1: Input: responses, annotations\n2: Output: responseRanks\n3: G ←InitializeGraph()\n4: for each annotationi in annotations do\n▷Build Graph G\n5:\n(chosen_response, reject_response) ←annotationi\n6:\nr1 ←ComputeIdentifier(chosen_response)\n7:\nr2 ←ComputeIdentifier(reject_response)\n8:\nif r1 not in G then\n9:\nAddNode(r1, G)\n10:\nend if\n11:\nif r2 not in G then\n12:\nAddNode(r2, G)\n13:\nend if\n14:\nif IsEqual(annotationi) then\n▷In case chosen and reject is annotated as equal quality\n15:\nAddEdge(r1, r2, G)\n16:\nAddEdge(r2, r1, G)\n17:\nelse\n18:\nAddEdge(r1, r2, G)\n19:\nend if\n20: end for\n21: M ←InitializeMapping()\n▷Record mapping bewteen merged node and origin nodes\n22: repeat\n▷Detect and Merge Cycles\n23:\nconflict_ids ←DetectCycles(G)\n▷Cycles can be detected with Depth-first Search\n24:\nAddNode(rm,G)\n25:\nif len(conflict_ids) > 0 then\n26:\nrm, ←CreateRecordIdentifier(conflict_ids, M)\n27:\nfor ri in conflict_ids do\n28:\nfor e in FindEdgesEndswith(ri, G) do\n29:\nDeleteEdge(e)\n30:\nAddEdge(e[0], rm)\n31:\nend for\n32:\nfor e in FindEdgesStartswith(ri, G) do\n33:\nDeleteEdge(e)\n34:\nAddEdge(rm, e[−1])\n35:\nend for\n36:\nDeleteNode(ri)\n37:\nend for\n38:\nend if\n39: until len(conflict_ids) == 0\n40: Initialize an empty list\n41: while G is non-empty do\n▷Topological Sort\n42:\nR ←SelectNodesWithoutInEdges(G)\n43:\nAddRanksWithMapping(responseRanks,M,R)\n44:\nDeleteNodesEdges(G,R)\n45: end while\n46: Return responseRanks\n16\n\nJudge Prompt Template\n你是一个答案质量评估专家，擅长深度理解用户的问题，并以此为依据全面、深度\n地考察模型给出的答案的质量，并在比较后输出最佳答案。接下来，我会给你一个来\n自用户的问题「query」，参考答案「reference」和两个不同的模型回答「answerA」、\n「answerB」。\n除了query和两个answer之外，我还可能会提供「reference」，即关于该query的参考资\n料（它有可能是题目的参考回答，也可能是一些解题思路或者评价标准）。当存\n在reference时，你必须结合reference的内容对答案进行深度分析。当没有reference时，\n按照你自己的理解进行分析即可。\n请你参考全面、细致、深度考察以下关于该query的考察标准，综合比\n较answerA和answerB的质量，如果answerA更好，则在「conclusion」输出A；如\n果answerB更好，则在「conclusion」输出B；如果整体质量区分不明显，则输出C；\n{criteria}\n「query」：\n{query}\n{reference}\n「answerA」：\n{answer_a}\n「answerB」：\n{answer_b}\n请确保你清晰理解了评估流程，**避免任何位置偏见**，请确保回答的呈现顺序不影响\n您的判断。不要因回答的长度影响你的评估，**避免任何长度偏见**，不要偏袒，尽可\n能地客观。此外，我们现在是在中文场景，你应该考虑模型是否**正确使用了中文回\n复**，你在评价时也应该以中文视角进行评价。\n你只需要输出“A”，“B”或“C”，不需要输出中间思考过程。接下来回复结果：\nFigure 11: Template for AI annotation based on detailed criteria and ensuring objective comparison.\nteams verify 30% of the annotated data, which is\nthen passed to data operations teams for another\nindependent 30% verification. The final results\nare validated by research teams.\nTo ensure re-\nview quality under this single-blind setting, we\nhave developed a dynamic verification mechanism\nwhere ground truth samples are continuously es-\ntablished through collaborative alignment among\nteams and regularly embedded into review tasks.\nOur multi-stage process provides strong account-\nability, as each stage’s work is reviewed by sub-\nsequent stages, and approved annotations can be\nrejected in later reviews, which incentivizes thor-\nough independent assessment rather than simple\nagreement. We adopt the single-blind approach due\nto practical constraints: while our quality control\nreviewers are more experienced and highly qual-\nified, their limited number compared to regular\nannotators necessitates this approach to maximize\nquality check coverage.\nD.2\nAnnotation Guideline\nOur annotation guidelines are built upon three core\ndimensions as shown in Table 6. We ask annotators\nto score each response according to the criteria in\nTable 7 while conducting preference annotations.\nFor responses with identical scores, we require\nannotators to perform bucket-wise pairwise com-\nparisons for further ranking. In the comparison pro-\ncess, annotators are instructed to assign ‘g’ (good)\nif response A is preferred over B, ‘b’ (bad) if B is\npreferred over A, or ‘s’ (same) if both responses\nare considered equally good. The comparison is\nbased on overall user preference without detailed\nscoring criteria. After completing all comparisons,\nannotators are required to integrate their pairwise\njudgments to establish a complete ranking (e.g.,\nA>C>B=D>E). The annotators then cross-validate\nthis final ranking against their initial scoring to\nensure consistency and resolve any potential con-\ntradictions.\nBeyond the general guidelines, we also devel-\n17\n\nJudge Prompt Template\nYou are an answer quality assessment expert, skilled in deeply understanding user queries and\nthoroughly evaluating the quality of model responses based on that understanding, to output the\nbest answer after comparison. Below, I will provide you with a user query \"query\", a reference\nanswer \"reference\", and two different model responses \"answerA\" and \"answerB\".\nBesides the query and the two answers, I may also provide a \"reference\", which is additional\ninformation related to the query (it might be a reference answer to the question, or solution ideas\nor evaluation criteria). When there is a reference, you must perform an in-depth analysis of\nthe answers using the reference. When there is no reference, analyze them according to your\nunderstanding.\nPlease assess the following criteria comprehensively, meticulously, and deeply regarding the query,\nand compare the quality of answerA and answerB. If answerA is better, output \"A\" in \"conclusion\";\nif answerB is better, output \"B\"; if the overall quality difference is not significant, output \"C\";\n{criteria}\n\"query\":\n{query}\n{reference}\n\"answerA\":\n{answer_a}\n\"answerB\":\n{answer_b}\nEnsure that you clearly understand the assessment process, **avoid any positional bias**, and\nmake sure the presentation order of the answers does not affect your judgment. Do not let the\nlength of the answer affect your evaluation, **avoid any length bias**, and remain as objective\nas possible without showing favoritism. Furthermore, this is a Chinese context, and you should\nconsider whether the models have used Chinese appropriately in their responses, and you should\nevaluate from a Chinese perspective.\nYou only need to output \"A\", \"B\", or \"C\", without detailing the reasoning process. Please respond\nwith the result:\nFigure 12: Template for AI annotation translated into English.\noped and iteratively refined specific evaluation cri-\nteria for different types of prompts. These prompt-\nspecific guidelines elaborate on the above stan-\ndards, balance different evaluation metrics accord-\ning to task requirements, and provide detailed ex-\namples for annotators’ reference. Additionally, we\nestablished specific protocols for handling special\ncases such as garbled text, logically inconsistent\nresponses, and misinformation. Furthermore, anno-\ntators are required to highlight and identify specific\nproblematic sections within responses to pinpoint\nexact issues beyond preference annotation.\nD.3\nAnnotation Bias\nWe explore the preferences of both human and GPT\nannotators in terms of response length and position,\nas shown in Figure 15. It can be observed that GPT-\n4o generally prefers responses that are placed later,\nwhereas human annotators do not exhibit a signif-\nicant preference for position. Additionally, when\nthe response length difference is moderate, both\nhuman and GPT annotators tend to favor longer\nresponses. However, as the length difference be-\ncomes too large, humans tend to prefer shorter ones.\nOverall, the specific preferences of the annotators\nare not very pronounced.\nE\nBenchmark Results\nIn this section, we present comprehensive results\non CheemsBench.\nTable 8 reports the perfor-\nmance of both discriminative RMs and genera-\ntive models serving as RMs. The evaluated dis-\ncriminative RMs include Skywork-series (Liu\net al., 2024a), Llama-3.1-Nemotron-70B-Reward\n(Wang et al., 2024c), Llama-3-OffsetBias-RM-8B\n18\n\n(Park et al., 2024), RM-Mistral-7B (Xiong et al.,\n2024), URM-series (Lou et al., 2024), ArmoRM-\nLlama3-8B-v0.1 (Wang et al., 2024a), GRM-\nseries (Yang et al., 2024b), QRM-series (Dorka,\n2024), FsfairX-LLaMA3-RM-v0.1 (Dong et al.,\n2023), RM-Gemma-2/7B (Dong et al., 2023),\nIntermLM-series (Cai et al., 2024), BTRM-\nQwen2-7b-0613. The evaluated generative models\nas RMs include Skywork-Critic-series (Liu et al.,\n2024a), CompassJudger-Series (Cao et al., 2024),\nQwen2.5-Series (Team, 2024), Llama3.1-Series\n(Grattafiori et al., 2024), Llama-3-OffsetBias-8B\n(Park et al., 2024). For commercial models like\nGPT-4, GPT-3.5-turbo and Doubao-pro, we use\ntheir official APIs for evaluation.\nTable 3 re-\nports the performance of different datasets. The\nevaluated datasets include HH-RLHF-cn, Huozi\n(Huozi-Team, 2024), Kyara (Yang, 2024), Zhihu,\nChatbotArene (Zheng et al., 2023), HH-RLHF\n(Ganguli et al., 2022), MathPreference, Nectar\n(Zhu et al., 2023), PKU-SafeRLHF (Ji et al.,\n2024), Skywork (Liu et al., 2024a), MathStack-\nExchange (Lambert et al., 2023), UltraFeedback\n(Cui et al., 2023), HelpSteer2 (Wang et al., 2024d).\nF\nHyperparameter Settings\nWe present the key hyperparameters used in our ex-\nperiments in Table 9. Consistent settings are main-\ntained across all experiments except when training\nthe RM on the Human subset of CheemsPreference,\nwhere we use 2 epochs, as it yields the best results.\nWe report the experiment results for a single run.\nHyperparameter\nValue\nMax Sequence Length\n2048\nRegularization Coefficient\n0.1\nGradient Accumulation Steps\n4\nMicro Batch Size\n2\nGlobal Batch Size\n256\nEpochs\n2\nWarmup Ratio\n0.1\nLearning Rate Scheduler\nCosine\nLearning Rate\n5e-6\nTable 9: Hyperparameter settings.\nG\nUse of AI Assistants\nWe use AI to assist with grammar checks, sentence\npolish and coding.\n19\n\nAI Annotation Prompts and Corresponding Criteria in Chinese\nCriterion: 语言\n1. 符合基本要求：回答是否遵循用户意图，满足了用户提出问题的基本目的和需求，是\n否试图对问题进行恰当的回应。\n2. 创造性：回答是否具有创新性或独特性，是否提供了新颖的见解或解决方法。\n3. 语言与逻辑连贯性：语言表达是否流畅、逻辑是否清晰、各个部分是否有机组合在一\n起、层次是否分明。\n4. 事实正确性：回答中提供的信息是否准确无误，是否基于可信的事实和数据。\nCriterion: 知识/理解\n1. 符合基本要求：回答是否遵循用户意图，满足了用户提出问题的基本目的和需求，是\n否试图对问题进行恰当的回应。\n2. 事实正确性：回答中提供的信息是否准确无误，是否基于可信的事实和数据。\n3. 清晰度：回答是否表达清晰易懂，是否使用了简洁的语言和结构，以便用户可以轻松\n理解。\nCriterion: 推理/数学\n1. 符合基本要求：回答是否遵循用户意图，满足了用户提出问题的基本目的和需求，是\n否试图对问题进行恰当的回应。\n2. 推理准确性：回答是否能基于正确理解的信息进行有效推理，包括逻辑结构的正确\n性、应用推理规则的合理性。\n3. 结论合理性：回答得出的结论是否与现实世界的常识和已知事实一致，这是推理过程\n的重要验证步骤，确保回答在实际应用中是可靠的。\nCriterion: 代码\n1. 符合基本要求：回答是否遵循用户意图，满足了用户提出问题的基本目的和需求，是\n否试图对问题进行恰当的回应。\n2. 代码质量与效率：这个指标评估编写的代码质量，包括是否易于阅读（如命名规范、\n注释）、是否易于维护和扩展、是否符合编程最佳实践。同时，也包括代码的执行效\n率，如运行时间和内存消耗，以及算法的时间复杂度和空间复杂度。代码效率直接影响\n程序的性能，尤其是在处理大量数据或需要高性能计算时。\nCriterion: 安全\n1. 生成的内容应避免引发任何形式的伤害，包括但不限于误导、煽动、歧视或侮辱，同\n时也要尊重用户的隐私和个人信息。\n2. 生成的内容应基于事实，不应编造、歪曲或误导性地表达信息，同时也应尽可能地避\n免传播未经证实或可能引起误解的信息。\n3. 生成的内容应能提供实际价值，解答疑问或提供有用的信息，同时也应尽可能地满足\n用户的需求和期望。\nCriterion: 复杂指令\n1. 是否准确理解了用户的复杂指令，厘清了用户的复杂需求。\n2. 生成的内容应提供有用的信息，按照用户的期待执行了复杂任务，可能地满足用户的\n需求和期望。\n3. 回答是否表达清晰易懂，是否使用了简洁的语言和结构，以便用户可以轻松理解自己\n的复杂需求如何被满足.\nFigure 13: AI Annotation Prompts and Corresponding Criteria in Chinese.\n20\n\nAI Annotation Prompts and Corresponding Criteria in English\nCriterion: Language\n1. Meets Basic Requirements: Does the response follow the user’s intent and fulfill the basic\npurpose and needs of the user’s question? Does it attempt to appropriately address the question?\n2. Creativity: Is the response innovative or unique? Does it provide novel insights or solutions?\n3. Linguistic and Logical Coherence: Is the language used fluent? Is the logic clear? Are all parts\norganically integrated, and is there a clear hierarchy?\n4. Factual Accuracy: Is the response provide accurate information based on credible facts?\nCriterion: Knowledge/Understanding\n1. Meets Basic Requirements: Does the response follow the user’s intent and meet the basic\npurpose and needs of the user’s question? Does it attempt to appropriately address the question?\n2. Factual Accuracy: Is the information provided in the response accurate and based on credible\nfacts and data?\n3. Clarity: Is the response expressed clearly and understandably? Does it use concise language\nand structure for easy comprehension by the user?\nCriterion: Reasoning/Mathematics\n1. Meets Basic Requirements: Does the response follow the user’s intent and meet the basic\npurpose and needs of the user’s question? Does it attempt to appropriately address the question?\n2. Reasoning Accuracy: Can the response perform effective reasoning based on correctly under-\nstood information, including the correct logical structures and the reasoning rules application?\n3. Conclusion Reasonableness: Does the conclusion drawn align with common knowledge and\nknown facts about the real world? This is an important verification step in the reasoning process to\nensure the response is reliable in practical application.\nCriterion: Code\n1. Meets Basic Requirements: Does the response follow the user’s intent and meet the basic\npurpose and needs of the user’s question? Does it attempt to appropriately address the question?\n2. Code Quality and Efficiency: This criterion evaluates the quality of the written code, including\nreadability (e.g., naming conventions, comments), maintainability and extensibility, and adherence\nto coding best practices. It also considers the execution efficiency of the code, such as runtime and\nmemory usage, and the time and space complexity of algorithms. Code efficiency directly impacts\nperformance, especially when handling large data or requiring high-performance computing.\nCriterion: Safety\n1. The generated content should avoid causing any harm, including but not limited to misleading,\ninciting, discrimination, or insult. It should also respect users’ privacy and personal information.\n2. The generated content should be based on facts and should not fabricate, distort, or express\ninformation misleadingly. It should also strive to avoid spreading unverified or potentially mislead-\ning information as much as possible.\n3. The generated content should provide practical value, answer queries, or provide useful\ninformation, while striving to meet the user’s needs and expectations.\nCriterion: Complex Instructions\n1. Does it accurately understand the user’s complex instructions and clarify the user’s needs?\n2. The generated content should provide useful information and perform complex tasks according\nto the user’s expectations, to the fullest extent possible meet the user’s needs and expectations.\n3. Is the response expressed in a clear and understandable manner? Does it use concise language\nand structure to help the user easily understand how their complex needs are being met?\nFigure 14: AI Annotation Prompts and Corresponding Criteria translated into English.\n21\n\nDimension\nDefinition\nHarmlessness\nGenerated content must avoid any potential harm to individuals, devices,\nproperty, environment, or essential institutions. Specifically:\n• Avoid all forms of discrimination (racial, gender, religious, national,\nsexual orientation, age, socioeconomic status)\n• Adhere to core socialist values and social ethics\n• Exclude pornographic and violent content\n• Protect privacy and intellectual property rights\n• Avoid promoting harmful real-world advice or illegal activities\n• Respect all groups and avoid biased language\n• Exclude abusive, threatening, or offensive language\nTruthfulness\nGenerated content must contain accurate information and avoid misleading\nusers. Specifically:\n• Avoid providing false information, especially regarding important\ndecisions or sensitive topics\n• Exclude misleading or unverified information\n• Provide sources or evidence when possible to enhance credibility\n• Ensure accuracy in professional or technical information\n• Maintain fidelity to input information in summarization tasks\nHelpfulness\nGenerated content should follow user requirements and provide effective\nassistance. Specifically:\n• Use clear, understandable language and structure\n• Answer questions accurately, even when poorly formulated\n• Seek clarification for unclear instructions\n• Avoid excessive or redundant information\n• Make appropriate contextual assumptions only when implicitly re-\nquired by the task\nTable 6: Detailed evaluation dimensions and their definitions for annotation guidelines.\n22\n\nQuality\nScore\nCategory\nDetailed Description\nPoor\n0\nSevere Errors\nResponse contains severe mistakes with no practical value.\nExamples: harmful content, completely ignored instruc-\ntions, text collapse, wrong language (non-Chinese), severe\ncontent missing (truncated), blank, or error messages.\n1\nExtremely Low\nQuality\nResponse performs extremely poorly in all 3H dimensions,\nwith major/numerous errors in format, information, and\ntext. Cannot meet user needs; overall impression is ex-\ntremely poor and generally unusable.\nAverage\n2\nBelow Average\nResponse shows deficiencies in 3H dimensions with some\nobvious but non-fatal issues. Poor overall impression,\nmost content unusable, small portions might be usable\nafter adjustment.\n3\nModerate\nResponse shows average performance in 3H dimensions,\nbasically meets user needs. Contains minor errors with\nlimited impact. Acceptable impression, mediocre, partially\nadoptable but requires user adjustments.\nExcellent\n4\nGood\nResponse performs well in 3H dimensions, meets user\nneeds with no hard errors. Good overall impression with\nminimal flaws or none (but no highlights). Mostly directly\nusable, small portions need minor adjustments.\n5\nOutstanding\nResponse excels in all 3H dimensions, exceptional over-\nall impression with brilliant points/highlights. Perfectly\naddresses user needs; highly suitable for the scenario, en-\ntirely adoptable without changes.\nTable 7: Scoring criteria and detailed descriptions for response quality assessment.\n23\n\n0-200\n200-400\n400-600\n600-800\n800-1000\n1000-1200\n1200-1400\n1400-1600\n1600-1800\n1800-2000\n2000-2200\n2200-2400\nLength Difference Buckets\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nRatio of Answer A > Answer B\nBias Analysis Based on Answer Length Differences\n(a) Human Annotator - Length Bias.\n1-2\n1-3\n2-3\nPosition Pair\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nWin Rate of Position A\nWin Rates for Position Pairs\n(b) Human Annotator - Positional Bias.\n0-200\n200-400\n400-600\n600-800\n800-1000\n1000-1200\n1200-1400\n1400-1600\n1600-1800\n1800-2000\n2000-2200\n2200-2400\nLength Difference Buckets\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nRatio of Answer A > Answer B\nBias Analysis Based on Answer Length Differences\n(c) GPT Annotator - Length Bias.\n1\n2\nPosition\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nWin Rate\nWin Rates for Positions 1 and 2\n(d) GPT Annotator - Positional Bias.\nFigure 15: Comparison of Human and GPT Annotator Biases. For subfigures (a) and (c), the x-axis represents the\nlength difference between answer A and answer B, while the y-axis shows the proportion of cases where answer A\nis selected.\n24\n\nModel Name\nRewardBench\nOpen Prompt\nHuman Instruction\nOverall\nAcc.\nExact.\nAcc.\nExact.\nOpen-source Reward Models\nSkywork-Reward-Gemma-2-27B\n0.938\n0.754\n0.329\n0.748\n0.311\n0.535\nSkywork-Reward-Gemma-2-27B-v0.2\n0.943\n0.751\n0.321\n0.735\n0.294\n0.525\nLlama-3.1-Nemotron-70B-Reward-HF\n0.941\n0.750\n0.317\n0.722\n0.271\n0.515\nLlama-3-OffsetBias-RM-8B\n0.894\n0.734\n0.310\n0.689\n0.239\n0.493\nRM-Mistral-7B\n0.804\n0.721\n0.285\n0.700\n0.259\n0.491\nURM-LLaMa-3-8B\n0.899\n0.727\n0.310\n0.688\n0.230\n0.489\nArmoRM-Llama3-8B-v0.1\n0.904\n0.715\n0.308\n0.677\n0.246\n0.487\nSkywork-Reward-Llama-3.1-8B-v0.2\n0.931\n0.721\n0.283\n0.701\n0.237\n0.486\nURM-LLaMa-3.1-8B\n0.929\n0.722\n0.292\n0.696\n0.230\n0.485\nGRM-Llama3-8B-rewardmodel-ft\n0.915\n0.728\n0.281\n0.688\n0.229\n0.482\nQRM-Llama3.1-8B\n0.931\n0.722\n0.275\n0.691\n0.233\n0.480\nSkywork-Reward-Llama-3.1-8B\n0.931\n0.721\n0.273\n0.690\n0.230\n0.479\nFsfairX-LLaMA3-RM-v0.1\n0.844\n0.710\n0.286\n0.667\n0.224\n0.472\nRM-Gemma-7B\n0.695\n0.700\n0.273\n0.678\n0.235\n0.471\ninternlm2-20b-reward\n0.902\n0.714\n0.260\n0.652\n0.200\n0.457\ninternlm2-7b-reward\n0.876\n0.712\n0.262\n0.644\n0.187\n0.451\nBTRM-Qwen2-7b-0613\n0.832\n0.708\n0.259\n0.647\n0.186\n0.450\nRM-Gemma-2B\n0.654\n0.662\n0.222\n0.633\n0.205\n0.431\ninternlm2-1-8b-reward\n0.822\n0.642\n0.182\n0.619\n0.163\n0.402\nGRM-llama3-8B-distill\n0.862\n0.531\n0.123\n0.548\n0.127\n0.332\nGRM-Gemma-2B-rewardmodel-ft\n0.845\n0.509\n0.111\n0.470\n0.106\n0.299\nGemma-2B-rewardmodel-ft\n0.805\n0.494\n0.106\n0.473\n0.111\n0.296\nGRM-gemma2-2B-rewardmodel-ft\n0.884\n0.471\n0.093\n0.480\n0.110\n0.288\nGenerative Models as Reward Models\nSkywork-Critic-Llama-3.1-70B\n0.933\n0.755\n0.320\n0.731\n0.258\n0.516\nCompassJudger-1-14B-Instruct\n0.841\n0.745\n0.327\n0.692\n0.239\n0.501\nCompassJudger-1-32B-Instruct\n0.852\n0.742\n0.322\n0.685\n0.231\n0.495\nQwen2.5-72B-Instruct\n-\n0.734\n0.306\n0.678\n0.229\n0.487\nSkywork-Critic-Llama-3.1-8B\n0.890\n0.726\n0.288\n0.696\n0.229\n0.485\nGPT4o\n0.846\n0.727\n0.300\n0.667\n0.203\n0.457\nDoubao-pro-128k\n-\n0.720\n0.280\n0.662\n0.164\n0.456\nQwen2.5-7B-Instruct\n-\n0.713\n0.262\n0.637\n0.163\n0.444\nLlama-3-OffsetBias-8B\n0.840\n0.690\n0.243\n0.658\n0.180\n0.443\nLlama-3.1-70B-Instruct\n0.840\n0.685\n0.244\n0.610\n0.153\n0.423\nCompassJudger-1-1.5B-Instruct\n0.734\n0.660\n0.210\n0.594\n0.132\n0.399\nLlama-3.1-8B-Instruct\n0.657\n0.630\n0.158\n0.583\n0.116\n0.372\nGPT3.5-turbo\n0.653\n0.616\n0.143\n0.572\n0.113\n0.361\nTable 8: Performance of discriminative and generative RMs on CheemsBench. The Overall metric is the average of\naccuracy (Acc.) and exact match (Exact.) across the Open Prompt and Human Instruction subsets.\n25\n",
  "metadata": {
    "source_path": "papers/arxiv/Cheems_A_Practical_Guidance_for_Building_and_Evaluating_Chinese_Reward\n__Models_from_Scratch_bfc7ce6c97fafb10.pdf",
    "content_hash": "bfc7ce6c97fafb10348aeb54bc77d599781ab12705e20b5a9c7f86da75af6386",
    "arxiv_id": null,
    "title": "Cheems_A_Practical_Guidance_for_Building_and_Evaluating_Chinese_Reward\n__Models_from_Scratch_bfc7ce6c97fafb10",
    "author": "",
    "creation_date": "D:20250225025032Z",
    "published": "2025-02-25T02:50:32",
    "pages": 25,
    "size": 2452899,
    "file_mtime": 1740470174.006377
  }
}