{
  "text": " \nTitle \nRELICT: A Replica Detection Framework for \nMedical Image Generation  \nAuthors \nOrhun Utku Aydin*1, Alexander Koch*1, Adam Hilbert*1, Jana Rieger1, Felix Lohrke1, \nFujimaro Ishida2, Satoru Tanioka1,3, Dietmar Frey1,4  \n \n* Authors contributed equally to this work \n \nAffiliations \n1 CLAIM - Charité Lab for AI in Medicine, Charité – Universitätsmedizin Berlin, corporate \nmember of Freie Universität Berlin and Humboldt-Universität zu Berlin, Charitéplatz 1, \n101117, Berlin, Germany  \n \n2 Department of Neurosurgery, Mie Chuo Medical Center, 2158-5 Myojin-cho, 514-1101, \nHisai, Tsu, Japan  \n \n3 Department of Neurosurgery, Mie University Graduate School of Medicine, 2-174 \nEdobashi, 514-8507, Tsu, Japan  \n \n4 Department of Neurosurgery, Charité – Universitätsmedizin Berlin, corporate member of \nFreie Universität Berlin and Humboldt-Universität zu Berlin, Charitéplatz 1, 101117, Berlin, \nGermany \n​\nCorresponding Author Details​\nDietmar Frey \ndietmar.frey@charite.de \n \nKeywords​\nMedical image synthesis, generative models, memorization, privacy \n \n \n \n\n \nAbstract  \nBackground \nDespite the potential of synthetic medical data for augmenting and improving the \ngeneralizability of deep learning models, memorization in generative models can lead to \nunintended leakage of sensitive patient information and limit model utility. Thus, the use of \nmemorizing generative models in the medical domain can jeopardize patient privacy.  \n \nMethods \nWe propose a framework for identifying replicas, i.e. nearly identical copies of the training \ndata, in synthetic medical image datasets. Our REpLIca deteCTion (RELICT) framework for \nmedical image generative models evaluates image similarity using three complementary \napproaches: (1) voxel-level analysis, (2) feature-level analysis by a pretrained medical \nfoundation model, and (3) segmentation-level analysis. Two clinically relevant 3D generative \nmodelling use cases were investigated: non-contrast head CT with intracerebral hemorrhage \n(N=774) and time-of-flight MR angiography of the Circle of Willis (N=1,782). Expert visual \nscoring was used as the reference standard to assess the presence of nearly identical \nreplicas. We report the balanced accuracy at the optimal threshold to assess replica \nclassification performance of the analysed methods.  \n \nResults \nThe reference visual rating identified 45 of 50 and 5 of 50 generated images as replicas for \nthe NCCT and TOF-MRA use cases, respectively. Image-level and feature-level metrics \nperfectly classified replicas with a balanced accuracy of 1 when an optimal threshold was \nselected for the NCCT use case. A perfect classification of replicas for the TOF-MRA case \nwas not possible at any threshold, with the segmentation-level analysis achieving the highest \nbalanced accuracy of 0.79.  \n \nDiscussion \nReplica detection is a crucial but neglected validation step for the development of deep \ngenerative models in medical imaging. The proposed RELICT framework provides a \nstandardized, easy-to-use tool for replica detection and aims to facilitate responsible and \nethical medical image synthesis.  \n \n \n \n\n \n1. Introduction \nArtificial intelligence for medical imaging has the potential to transform diagnostic workflows \nand support critical treatment decisions in fields such as radiology (Saha et al. 2024), \nhistopathology (Dippel et al. 2024) and dermatology (Salinas et al. 2024). One of the key \nrequirements for developing robust and generalizable deep learning models is access to \nlarge, diverse and high-quality training and validation datasets (Schwabe et al. 2024). \nHowever, the curation of such medical imaging datasets is often constrained by the \nchallenges of sharing of sensitive medical data (Legido-Quigley et al. 2025), costs of data \nacquisition and limited availability of expert labeling.  \n \nTo address these challenges, synthetic medical images of various modalities, anatomical \nregions and pathological conditions have been successfully generated using generative \nmodels such as generative adversarial networks (GAN) (Ferreira et al. 2024) or \ndiffusion-based models (DM) (Kazerouni et al. 2023; Ibrahim et al. 2024). Synthetic data can \nthen be used to augment the training set for improved fairness, generalizability, and \nperformance of downstream deep learning models (Frid-Adar et al. 2018; Khader et al. 2023; \nKtena et al. 2024; Khosravi et al. 2024). Synthetic data should retain the predictive \nproperties of real data and provide high quality and resolution required for many clinical \napplications. Thus, synthetic images are expected to be indistinguishable from real images \nto ensure optimal generative quality and utility (Park et al. 2021).  \n \nHowever, a key challenge in generative modeling is the potential reproduction of real training \ndata at varying levels of similarity. While replication, i.e., synthesis of an identical copy of a \ntraining sample, is the major concern, synthetic images can also closely resemble training \ndata without being exact copies (Akbar et al. 2023; Carlini et al. 2023). Higher levels of \nsimilarity between synthetic and real images can reduce the privacy benefits of synthetic \ndata and decrease the added value of synthetic data augmentations by limiting diversity. \nThis raises ethical concerns, since data that is not initially publicly shared due to data \nprotection regulations can be unintentionally exposed by releasing a model or publishing \nsynthetic datasets.  \n \nThe use of generative models in the medical domain is a high-risk application that can \njeopardize patient privacy (Giuffrè and Shung 2023). Patient imaging data can serve as \nuniquely identifiable biometric information, similar to fingerprints (Packhäuser et al. 2022). \nThis vulnerability can be exploited in adversarial attacks, such as membership inference \nattacks, where information about the inclusion of an individual in a training dataset can be \nextracted (Kuppa et al. 2021; Paul et al. 2021). Despite the existence of data protection \nregulations, ethical guidelines and AI research checklists (Chen et al. 2024; Tejani et al. \n2024), empirical analysis of memorization in synthetic medical image generation remains \nlargely unexplored (Ibrahim et al. 2024). \n \nPrior works addressed content-based image retrieval (Gupta et al. 2023) and memorization \nin medical image generation using various image similarity measures (Dar et al. 2024).   \nA significant gap in the field is the absence of a standardized tool and methodology for \nreplica detection. Recent works either do not check for memorized replicas at all (Pinaya et \n \n\n \nal. 2022; Pan et al. 2023; Peng et al. 2023) or rely on task-specific, custom approaches for \nreplica detection tailored to individual image generation tasks. For instance, Fernandez et al. \ncalculated the overlap of real and generated labels using the Dice coefficient to find nearest \nneighbours (Fernandez et al. 2024), Dar et al trained a self-supervised model to project \nimages onto a lower dimensional embedding space and performed replica detection through \ncorrelation values (Dar et al. 2024), Packhäuser et al. train a siamese neural network to \ndetect memorized images (Packhäuser et al. 2022, 2023), Akbar et al used correlation of \npixel intensities (Akbar et al. 2023), Aydin et al. used a predefined threshold of l2 distance \nratio (Aydin et al. 2024). This non-uniformity in the literature warrants a standardized, \neasy-to-use solution to be used as a validation step in medical image generation research.  \n \nIn this study, we propose a framework for identifying replicas, near-identical copies of the \ntraining data, in synthetic medical image datasets. Our framework evaluates image similarity \nusing three complementary approaches: (1) image level comparison, (2) feature extraction \nby a medical foundation model, and (3) segmentation-level comparison. We demonstrate our \nframework on two clinically relevant use cases: generative modelling of non-contrast head \nCT scans with intracerebral hemorrhage and Circle of Willis arterial segments. By proposing \na standard, easy-to-use replica detection framework we aim to contribute to the safe, \nresponsible and ethical deployment of generative models in medical imaging.  \n \n \n\n \n2. Methods \nThe data collection for this retrospective study was approved by the local Ethics \nCommittees of following hospitals: Mie Chuo Medical Center institutional review \nboard [permit number: MCERB-202321], Matsusaka Chuo General Hospital \ninstitutional review board [permit number: 325], Suzuka Kaisei Hospital institutional \nreview board [permit number: 2020-05], and Mie University Hospital institutional \nreview board [permit number: T2023-7]. Written informed consent was waived due to \nthe retrospective nature of the analysis.  \n \n2.1. Data \nUse case 1: 3D NCCT with Intracerebral hemorrhage  \nIn use case 1, the image generation task was to synthesise 3D non contrast \ncomputed tomography (NCCT) data with intracerebral hemorrhage (ICH) as the \nleading pathology. The training dataset included 387 patients with baseline and \nfollow up NCCT imaging within 24 hours (in total 774 images) with the primary \ndiagnosis of ICH from 4 hospitals in Japan: Mie Chuo Medical Center, Matsusaka \nChuo General Hospital, Suzuka Kaisei Hospital and Mie University Hospital. The \nNCCT volumes were registered to the MNI space and all volumes had a shape of \n182x218x182 voxels with a voxel spacing of 1x1x1 millimeters. Detailed patient \ncharacteristics and dataset information have been previously reported (Tanioka et al. \n2024). \n​\nA latent diffusion model architecture was used for generative modelling. Latent \ncodes of size (8, 20, 24, 20) were produced using a vector-quantized autoencoder \nwith residual-vector quantization, resulting in 8x downsampling of the original data \ndimensions. The elucidated diffusion training method was used for training and \nsynthetic images were generated using a DPM-Solver++ for 100 sampling steps \n(Karras et al. 2022).  \n \nAn nnUnet segmentation model was trained on the training set of 774 scans with \nmanually segmented binary ICH labels (Isensee et al. 2021). The model was trained \nfor 100 epochs using the default nnUnet hyperparameters, with CT normalization \nusing 5 fold cross validation. \n \nThe open source implementation of the diffusion model used in this use case can be \nfound in the following github repository:  \n \nhttps://github.com/claim-berlin/relict/brain-ae​\n \n \n \n \n\n \nUse case 2: 3D TOF-MRA  \nIn use case 2, the image generation task was to synthesise healthy 3D time-of-flight \nmagnetic resonance angiography (TOF-MRA) data. A 3D adaptation of the \nStyleGANv2 architecture was used for generative modelling of the Circle of Willis.  \nThe training data was open source and consisted of 1782 3D TOF MRA volumes \nfrom 7 different datasets as detailed in a prior work (Aydin et al. 2024). Preprocessing \nsteps included registration to a custom TOF-MRA template, cropping to a region of \ninterested of size 128x128x32 and voxel spacing of 0.62x0.62x0.62 millimeters \ncentered around the Circle of Willis.  \n \nAn nnUnet segmentation model was trained on 50 patients from the TopCoW dataset \nto segment Circle of Willis artery segments in a multiclass setting (Isensee et al. 2021; \nYang et al. 2024). Paired artery segment labels were merged to create a single class. \nThis resulted in following artery segments: Internal carotid artery (ICA), basilar artery \n(BA), posterior communicating artery (Pcom), anterior communicating artery (Acom), \nthe posterior cerebral artery (PCA), anterior cerebral artery (ACA), and the first \nsegment of the middle cerebral artery (M1). The model was trained for 1000 epochs \nusing the default nnUnet hyperparameters, with MR normalization using 5 fold cross \nvalidation. \n \nThe details regarding the generative model architecture, hyperparameters and open \nsource implementation can be found in the following github repository: ​\n​\nhttps://github.com/claim-berlin/3D_StyleGAN_Circle_of_Willis \n \n \n2.2. Replica detection and study design \nThe definition of replica in scientific literature is ambiguous, can be task-dependent \nand subjective (Fernandez et al. 2023; Dar et al. 2025). In this work, we refer to a \nsynthetic image as a replica if it is a near identical copy of a real image in the training \nset and has no distinguishing anatomical or pathological image features compared to \nthe real image.  \n \nOur analysis using the proposed replica detection framework consists of the \nfollowing steps: 1) a generative model is trained on 3D medical imaging data, 2) the \ntrained generative model is used to generate a synthetic dataset, 3) for each \nsynthetic image, training images are ranked based on similarity to identify the closest \ntraining image, 4) an expert visual scoring is performed to score similarity of each \nsynthetic image and closest training image leading to a binary replica decision, 5) the \nvarious measures are tested at their optimal thresholds for their replica detection \nperformance. ​\n \nFor subsequent automatization of replica detection we propose following steps: \nsteps 1-3 remain the same, 4) ranking of synthetic images within the synthetic \n \n\n \ndataset by their distance ratios for each measure, 5) visual scoring of a small subset \nof highest likely replica images with lowest distance ratios 6) visual scoring results \nare used to optimize use case specific thresholds for automated replica detection in \nfuture datasets. The methodological overview of the RELICT framework is shown in \nFigure 1. \n \n \nFigure 1. Methodology of the replica detection framework. \n \n2.3 Replica detection methodology \nA measure of image similarity is required for robust identification of potential replicas \nin medical imaging. We assess the similarity between a synthetic and real training \nimage from three different analysis perspectives: 1) image-level analysis, 2) \nfeature-level analysis, 3) segmentation-level analysis. The individual measures used \nin the different analysis levels are described in Appendix 1.  \n \n2.3.1 Image-level Analysis  \nIn the image-level analysis voxels constituting the synthetic and real image are \ndirectly compared based on various measures to compute a distance or similarity \nscore. Image-level analysis constitutes a simple and explainable method to compare \nimages and is adopted in previous works for replica detection tasks (Carlini et al. \n2023; Yoon et al. 2023). In our work, we use the following measures for image-level \nanalysis: mean absolute error (MAE), root mean square error (RMSE) and structural \nsimilarity index measure (SSIM) (Wang et al. 2004).      \n \n \n \n \n \n\n \n2.3.2 Feature-level Analysis \nFeature-level analysis aims to reduce the dimensions of the original images by using \na pretrained encoder, and subsequently comparing feature representations instead \nof the original voxel or pixel values. The feature-level comparison has been shown to \nallow a more medically relevant evaluation of image similarity (Gupta et al. 2023; Jush \net al. 2024; Dar et al. 2025). In our work we use the pretrained Resnet-50 MedicalNet \nas a medical foundation model for feature extraction (Chen et al. 2019). The network \nhas been trained on 23 medical image segmentation datasets and has been used in \nother works to extract feature representations from medical images (Chen et al. 2019; \nTak et al. 2024). Feature-level analysis can be computationally faster especially if \nGPU resources are used for the feature extraction step, since only extracted features \nare compared instead of whole images. In our work, images are normalized using \nz-score normalization and encoded using the Resnet-50 pretrained MedicalNet \nmodel (He et al. 2015). The dimensions of the resulting feature map is further reduced \nusing an adaptive average pooling layer to a final size of (2048x4x4x4). This \nembedding is flattened to a feature vector and compared using the RMSE and the \ncosine similarity.  \n \n2.3.3 Segmentation-level Analysis \nDifferent medical imaging modalities contain target structures relevant for diagnosis \nand treatment decisions. For instance, the first use case of this study includes NCCT \nimages of patients with intracerebral hemorrhage. The hemorrhage lesions constitute \nthe region of interest and are more salient for the diagnosis and treatment decisions \ncompared to other parts of the image. Generative models trained on data containing \na significant region of interest (ROI) such as hemorrhage lesions should preserve \nand capture the predictive properties of the real images with respect to the \npathology. Therefore, segmentations of ROIs should play a role in replica detection \nframeworks. Similar ROIs can argue in favour of a generated image being a replica, \nalthough there might be considerable differences in the background. In our work, we \nuse the Dice coefficient (Zou et al. 2004) and the average surface distance (ASD) \n(Yeghiazaryan and Voiculescu 2018) from an open-source implementation to compare \nsegmentations (https://github.com/google-deepmind/surface-distance).  \n \n\n \n2.4  Distance ratio and replica decision \nIn our replica detection framework, we use the methodology introduced by Carlini et \nal. and Yoon et al. (Carlini et al. 2023; Yoon et al. 2023). This approach first computes a \nmeasure, such as RMSE, between the generated image under evaluation and all \nimages in the training set. Second, the training images are sorted from most similar \nto least similar and the closest training image is identified. Third, we use following \nequation to compute the distance ratio by comparing a synthetic image to real \nimages:  \n \n \n \nfor a given distance measure M, where    is the synthetic image under evaluation \n𝑥\nfor replica detection, x is the closest image in the training set, S   is the subset of n \n𝑥\nclosest images in the training set to . This equation computes the measure value for \n𝑥\nthe closest training image x and divides it by the mean value of the n closest training \nimages. This equation is modified from the work by Carlini et al. All experiments \nwere performed with n=50 closest training images for S  .  \n𝑥\n \nThe distance ratio provides information about how “abnormally close” the synthetic \nimage is to the closest training image (Carlini et al. 2023). A binary decision whether \nthe synthetic image is a replica can be made by thresholding the distance ratios. \nHere, the threshold T might depend on image properties (e.g. intensity range, image \nvariation within training set) and should ultimately reflect the user’s tolerance for \nresemblance: \n \n \n \nEquation 1 based on the distance ratio was used for the image-level and \nfeature-level analysis. To ensure consistency when applying threshold comparisons, \nsimilarity measures were converted into distance-based measures by first \nnormalizing the values between 0 and 1 and subtracting their values from 1 (e.g., a \nDice coefficient of 0.7 was transformed into a distance of 0.3).  \n \nFor the segmentation-level analysis only the absolute value of the segmentation \nevaluation result was used for replica detection instead of the ratio in Equation 1. \nThis decision was based on the consideration that the segmentation step already \nisolates the foreground region of interest and disregards background information \nsuccessfully.  \n \n \n \n\n \n \n \n2.5. Visual Scoring of Replicas \nThe respective generative models were used to generate 50 synthetic images for \neach use case. Since comparing all 50 synthetic images to each training image \nvisually would be infeasible, synthetic images were paired up with a single training \nimage as a preliminary step. For each synthetic image the most similar image from \nthe training dataset was identified based on the RMSE calculated between the \nsynthetic image and each image in the training set. This allows for a more detailed \nand reliable image comparison within feasible efforts of clinical raters.  \n \nEach pair of synthetic and real image was inspected by two senior raters \nindependently and classified based on predefined subjective visual scoring criteria \n(Table 1). The visual scoring was performed using ITK-SNAP by inspecting the pair \nof synthetic and real image side by side (Yushkevich et al. 2006). A 4 point Likert-type \nscale (Likert 1932) was chosen to avoid neutral decisions. Visual scores of 3 or 4 led \nto the classification of a synthetic image as a replica. In cases where the raters \ndisagreed on the replica classification decision, the images were re-evaluated by \neach rater individually using the same criteria. The visual scoring is illustrated with \nexamples in Figure 2.  \n \nScale \n1 \n2 \n3 \n4 \nPoint \nCertainly not \na Replica \nProbably not a \nreplica \nProbably a \nReplica \nCertainly  \nReplica \nDescription Two different \nimages, no \nresemblances \nImages are \ndifferent, with \nconsiderable \ndifferences in \nanatomy, \npathology or \nbackground   \nImages are very \nsimilar, with some \nminor differences in \nanatomy, pathology \nor background  \nImages are \nmostly identical \nTable 1. Subjective visual scoring for replica detection ground truth creation. The \nscoring is performed using a Likert-type scale where increasing scores reflect the \nrater's subjective confidence that an image is a replica. ​\n \n \n \n \n\n \n \nFigure 2. Subjective visual scoring examples for two pairs of real and generated \nimages.  \n \n2.6. Software ​\nThe framework is written in the Python programming language (Version: 3.10) and \nimplements the image comparison measures from open-source libraries. The \nauthors will make efforts to assess ongoing research in the field and aim to keep the \nrepository updated with successful replica detection methods as they are developed. \nAdditionally, we welcome contributions from the open-source community to expand \nthe framework.  \n \nComputation was performed on the HPC for research cluster of the Berlin Institute of \nHealth using 50 CPU cores and a single V100 GPU. The replica detection code is \navailable open-source in the following github repository: ​\n​\nhttps://github.com/claim-berlin/relict \n \n2.7 Evaluation \n \nFor each analysis, the replica detection thresholds were analyzed by systematically \nevaluating performance across 0.01 increments relative to the value of the measure. \nThe performance was reported using balanced accuracy to equally consider \nsensitivity and specificity. The runtime was reported in minutes for each analysis \nmethod, using a typical workstation computer.  \n \n \n \n \n \n \n \n\n \n3. Results \n3.1 Visual rating results \nIn the NCCT use case, raters identified 45 out of 50 synthetic images as replicas, with the \nremaining 5 classified as non-replicas. In the TOF-MRA use-case, 5 images were identified \nas replicas and 45 as non-replicas. The raters agreed in the replica detection decision in 46 \nout of 50 cases (92%) for the NCCT use case and 41 out of 50 cases (82%) for the \nTOF-MRA use case. The median visual score was 3 and 4 for the NCCT images and 1 and \n2 for the TOF-MRA images for the two raters respectively. Figures 2-4 present example pairs \nof real and synthetic images assessed by the raters.  \n \n \n \nFigure 2. NCCT images classified as non-replicas based on visual rating. \n \n \n​\nFigure 3. NCCT images classified as replicas based on visual rating. \n \n \n \n \n \n\n \n \nFigure 4. TOF-MRA images classified as replicas based on visual rating. A. Real TOF-MRA \ndata, B. synthetic TOF-MRA data, C. segmentations of real volumes from row A, D.   \nsegmentations of synthetic volumes from row B.  \n \n3.2 Replica detection results \nThe optimal threshold for replica detection varied based on the analysis-level and measure \nused (Figure 5, Appendix 1B). For the NCCT use case, all measures tested on the \nimage-level and feature-level analysis allowed for finding an optimal threshold to identify \nreplicas. This indicated a perfect alignment with the visual rating (balanced accuracy of 1). \nThe segmentation-level measures Dice and ASD had lower balanced accuracies of 0.96 and \n0.98 respectively. The closest images identified by RMSE based preselection were also \nidentified by all other measures as the closest image in 47 out of 50 images.  \n \nIn the TOF-MRA use case, the analysed measures ranked training images differently for all \n50 synthetic images and thus identified different images as closest, compared to the \npreselection method. An example synthetic image with different closest training images \nidentified by RMSE, feature cosine similarity and multiclass ASD are shown in Figure 6. The \nsegmentation-level ASD measure enabled the highest replica detection performance with a \nbalanced accuracy of 0.8 and 0.79 in binary and multiclass settings. All analysed image-level \nand feature-level measures resulted in a balanced accuracy lower than 0.72.  \n \nAn overview of all used measures with their respective analysis levels and runtimes can be \nfound in Table 2. \n \n\n \n \n \nFigure 5. Quantitative evaluation of replica detection performance for NCCT and TOF-MRA \nuse cases.  \n \n \nFigure 6. Closest training images of generated TOF images from the training dataset \nselected by various measures.  \n \n \n \n \n \n \n \n \n\n \nTable 2. Replica detection methods overview and runtime analysis.  \n \n \n \n \nMeasure  \nAnalysis Level  \nRuntime  \n \nNCCT \nTOF-MRA \nMean Absolute Error \n(MAE) \nImage-level \n16 mins \n2 mins \nRoot Mean Square \nError  (RMSE) \nImage-level \n16 mins \n2 mins \nSSIM \nImage-level \n76 mins \n \n \n11 mins \nEmbeddings \nRMSE \nFeature-level \n4 mins \n2 mins \nEmbeddings  \nCosine similarity \nFeature-level \n4 mins \n2 mins \nDice (binary) \nSegmentation-level \n23 mins \n3 mins \nDice (multiclass) \nSegmentation-level \n- \n4 mins \nAverage surface \ndistance (binary) \nSegmentation-level \n31 mins \n16 mins \nAverage surface \ndistance (multiclass) \nSegmentation-level \n- \n16 mins \n\n \nDiscussion \nWe propose a replica detection framework to be used in medical image generation research \nas a standard model validation step. Memorization in generative models and resulting \nreplicated images can be detected using image-level, feature-level or segmentation-level \ncomparisons. Our research confirms reports of memorization in medical and natural image \ngeneration studies, and aims to raise awareness about the significant risks posed by this \nunderexplored issue. \n \nBeyond confirming the threat of image replication in medical image generation, our analysis \nyielded more insights into the challenge. Visual rating revealed a clear discrepancy in the \npercentage of replicated images in the two use cases; 90 percent of all generated NCCTs \nwere identified as replicas in comparison to only 10 percent in the TOF-MRA use case. The \nimage-level and feature-level analysis agreed best with the visual scoring in the NCCT use \ncase, reflecting the near-identical appearance of the volume pairs even when evaluated on \nindividual axial slices. In contrast, for the TOF-MRA use case, the segmentation-level \nanalysis outperformed the image- and feature-level methods. This is likely due to images \nshowing more abstract similarities, i.e in vessel anatomy, bifurcations, that are not fully \ncaptured by image or feature level comparisons. Furthermore, we demonstrated utilization of \nthe provided ranking of synthetic images based on their distance ratios, enabling the \nidentification of high likely replica candidates automatically. Decision thresholds can be \ndefined and tuned by subjective visual assessment and desired criteria for similarity to serve \nautomated detection in subsequently generated synthetic datasets. \n \nMemorization is a major concern and has implications for model development, downstream \nuse of synthetic data, and data sharing. Memorization is being increasingly considered \nduring the development of generative models and factors causing it are being explored to \ndevelop mitigation strategies (Somepalli et al. 2023; Dutt et al. 2024; Dombrowski et al. \n2024). Research in natural image generation shows that models trained on smaller training \ndataset sizes are more prone to memorization. Additionally, it has been reported that larger, \nmore complex models memorize faster (Tirumala et al. 2022). In the NCCT use case, we \ntrained a large diffusion-based model on a small dataset of 774 images with high resolution \nin 3D. In addition, the dataset contained baseline and follow-up images of the same patients, \nwith a high degree of similarity of the images. This might make memorization more probable \nsince data duplication has also been reported to increase memorization (Carlini et al. 2023). \n \nRecently, differential privacy (DP) has been proposed as a method for generative image \nmodelling to share sensitive patient data with known guarantees about privacy-preservation. \nDifferential privacy is an active area of research and allows a tradeoff between data utility \nand preservation of privacy. By decreasing the parameters (epsilon, delta), it is possible to \nobtain a stronger privacy guarantee, at the cost of reducing the utility of the data. Thus, \npractitioners always have to decide between potentially leaking sensitive data but providing \nhigh utility or vice versa. Especially for diffusion based models the implementation of \ndifferential privacy is practically challenging (Dockhorn et al. 2023). We see this as an \nimportant example application of our replication detection pipeline as a filter to additionally \n \n\n \nsafeguard sharing data with potential replication of patient information. Since DP can not \nguarantee that no sensitive data is leaked, it is also not a panacea to avoid legal constraints \nin data sharing. \n \nGenerated data is increasingly being used for downstream data augmentation in medicine \n(Ktena et al. 2024). Memorized images are highly unlikely to benefit downstream tasks since \nthey do not provide additional diversity to a given training set. We hypothesize that our \nframework can be used as a filter to select valuable and unique images among many \ngenerated images. Data augmentation using more unique images can help address the data \ndiversity problem in medical image deep learning models (Hofmanninger et al. 2020). With \nthis proposed replica filter, the added value of generative models - even predominantly \nmemorizing models as in the NCCT use case - can be tested in downstream tasks.  \n \nBased on our findings we have several recommendations for using RELICT for replica \ndetection. First, the replica detection threshold varies by dataset and measure used to \ncompare images. Thus, instead of relying on a single threshold, we recommend using the \nproposed framework to identify pairs of synthetic and real data that are most likely to be \nreplicas. This information is an output of the framework where pairs are ranked based on \neither their distance ratios or segmentation measure values. In a separate step, the ranking \nof most likely memorized images should be manually inspected to optionally fine-tune a \nreplica detection threshold for a given dataset if automation is desired. Second, the training \nsetup of the generative model should be considered during replica detection. Especially if \nthe generative model was trained using data augmentations, RELICT cannot guarantee to \nfind the closest real image, since augmentations are not considered explicitly. In this case, \nfeature-level analyses can be explored such as training of self-supervised models using \ncontrastive learning for replica detection for individual datasets (Dar et al. 2024), although \nthese approaches require training of a dataset specific model. Alternatively, replica detection \nmethods that increase robustness towards variations caused by data augmentation could be \nexplored for medical imaging  (Somepalli et al. 2023). Third, if the training dataset is large, \nimage-level analyses can be computationally very expensive and therefore cosine similarity \nof embeddings using an established medical foundation model, such as MedicalNet should \nbe preferred. Future additions to the framework can be made using more recent foundation \nmodels such as BiomedParse (Zhao et al. 2025) or MedSAM (Ma et al. 2024). \n \nSeveral checklists and reporting guidelines aim to ensure reproducibility and transparency of \nAI research in medicine and to provide reliability of published scientific evidence (Tejani et al. \n2024; Lekadir et al. 2025). Current guidelines however, are mostly concerned by predictive \nAI performance and reporting and are not specifically adapted for potential shortcomings of \ngenerative models. Our results suggest that a standardised replica detection framework can \nbe used to reveal replicas in synthetic datasets and thus provides insights to reliability and \nquality of synthetic datasets. We aim to raise awareness and caution on sharing of synthetic \ndata, and advocate for inclusion of replica detection requirements as part of reporting \nguidelines on generative AI studies.  \n \nOur study has several limitations. First, the proposed replica detection framework could only \nbe tested in a limited number of radiological use cases, each using a single generative \nmodelling approach, hence the memorization could not be directly compared between model \n \n\n \narchitectures. This was due to the fact that successfully training a generative model requires \nextensive hyperparameter tuning and computational resources. Second, a limited number of \nimage comparison measures were used and only a single medical foundational model was \ntested due to the exploratory nature of our replica detection tool. Third, in the subjective \nvisual rating the two senior raters assessed only the closest image suggested by a single \nmeasure, RMSE, because pairwise comparison of all images in the training set was not \nfeasible due to time constraints.  \nConclusion \nReplica detection is an important, but often neglected quality assurance step for validation of \ngenerative models in medical imaging. Standardized replica detection methods need to be \ndeveloped and included in AI in radiology and medicine checklists. Replica detection \nmethods are a crucial element for detecting patient privacy violations of generative models. \nOur developed framework provides an important step towards standardized and rigorous \nvalidation practices of generative models with potential for safer sharing of synthetic medical \nimage data.   \n \n \n \n\n \nReferences  \nAkbar MU, Wang W, Eklund A (2023) Beware of Diffusion Models for Synthesizing Medical \nImages - a Comparison with Gans in Terms of Memorizing Brain MRI and Chest \nX-Ray Images. SSRN \nAydin OU, Hilbert A, Koch A, et al (2024) Generative Modeling of the Circle of Willis Using \n3D-StyleGAN. NeuroImage 120936. \nhttps://doi.org/10.1016/j.neuroimage.2024.120936 \nCarlini N, Hayes J, Nasr M, et al (2023) Extracting Training Data from Diffusion Models \nChen J, Zhu L, Mou W, et al (2024) STAGER checklist: Standardized testing and \nassessment guidelines for evaluating generative artificial intelligence reliability. \niMetaOmics 1:e7. https://doi.org/10.1002/imo2.7 \nChen S, Ma K, Zheng Y (2019) Med3D: Transfer Learning for 3D Medical Image Analysis \nDar SUH, Seyfarth M, Ayx I, et al (2025) Unconditional Latent Diffusion Models Memorize \nPatient Imaging Data: Implications for Openly Sharing Synthetic Data \nDar SUH, Seyfarth M, Kahmann J, et al (2024) Unconditional Latent Diffusion Models \nMemorize Patient Imaging Data: Implications for Openly Sharing Synthetic Data \nDippel J, Prenißl N, Hense J, et al (2024) AI-Based Anomaly Detection for Clinical-Grade \nHistopathological Diagnostics. NEJM AI 1:AIoa2400468. \nhttps://doi.org/10.1056/AIoa2400468 \nDockhorn T, Cao T, Vahdat A, Kreis K (2023) Differentially Private Diffusion Models \nDombrowski M, Zhang W, Cechnicka S, et al (2024) Image Generation Diversity Issues and \nHow to Tame Them \nDutt R, Sanchez P, Bohdal O, et al (2024) MemControl: Mitigating Memorization in Medical \nDiffusion Models via Automated Parameter Selection \nFeng Q, Guo C, Benitez-Quiroz F, Martinez A (2021) When do GANs replicate? On the \nchoice of dataset size. In: 2021 IEEE/CVF International Conference on Computer \nVision (ICCV). IEEE, Montreal, QC, Canada, pp 6681–6690 \nFernandez V, Pinaya WHL, Borges P, et al (2024) Generating multi-pathological and \nmulti-modal images and labels for brain MRI. Med Image Anal 97:103278. \nhttps://doi.org/10.1016/j.media.2024.103278 \nFernandez V, Sanchez P, Pinaya WHL, et al (2023) Privacy Distillation: Reducing \nRe-identification Risk of Multimodal Diffusion Models. \nhttps://doi.org/10.48550/arXiv.2306.01322 \nFerreira A, Li J, Pomykala KL, et al (2024) GAN-based generation of realistic 3D volumetric \ndata: A systematic review and taxonomy. Med Image Anal 93:103100. \nhttps://doi.org/10.1016/j.media.2024.103100 \nFrid-Adar M, Diamant I, Klang E, et al (2018) GAN-based synthetic medical image \naugmentation for increased CNN performance in liver lesion classification. \nNeurocomputing 321:321–331. https://doi.org/10.1016/j.neucom.2018.09.013 \nGiuffrè M, Shung DL (2023) Harnessing the power of synthetic data in healthcare: \ninnovation, application, and privacy. NPJ Digit Med 6:186. \nhttps://doi.org/10.1038/s41746-023-00927-3 \nGupta D, Loane R, Gayen S, Demner-Fushman D (2023) Medical image retrieval via nearest \nneighbor search on pre-trained image features. Knowl-Based Syst 278:110907. \nhttps://doi.org/10.1016/j.knosys.2023.110907 \nHe K, Zhang X, Ren S, Sun J (2015) Deep Residual Learning for Image Recognition \nHofmanninger J, Prayer F, Pan J, et al (2020) Automatic lung segmentation in routine \nimaging is primarily a data diversity problem, not a methodology problem. Eur Radiol \nExp 4:50. https://doi.org/10.1186/s41747-020-00173-2 \nIbrahim M, Khalil YA, Amirrajab S, et al (2024) Generative AI for Synthetic Data Across \nMultiple Medical Modalities: A Systematic Review of Recent Developments and \n \n\n \nChallenges \nIsensee F, Jaeger PF, Kohl SAA, et al (2021) nnU-Net: a self-configuring method for deep \nlearning-based biomedical image segmentation. Nat Methods 18:203–211. \nhttps://doi.org/10.1038/s41592-020-01008-z \nJush FK, Vogler S, Truong T, Lenga M (2024) Content-Based Image Retrieval for Multi-Class \nVolumetric Radiology Images: A Benchmark Study \nKarras T, Aittala M, Aila T, Laine S (2022) Elucidating the Design Space of Diffusion-Based \nGenerative Models \nKazerouni A, Aghdam EK, Heidari M, et al (2023) Diffusion models in medical imaging: A \ncomprehensive survey. Med Image Anal 88:102846. \nhttps://doi.org/10.1016/j.media.2023.102846 \nKhader F, Müller-Franzes G, Tayebi Arasteh S, et al (2023) Denoising diffusion probabilistic \nmodels for 3D medical image generation. Sci Rep 13:7303. \nhttps://doi.org/10.1038/s41598-023-34341-2 \nKhosravi B, Li F, Dapamede T, et al (2024) Synthetically enhanced: unveiling synthetic data’s \npotential in medical imaging research. eBioMedicine 104:105174. \nhttps://doi.org/10.1016/j.ebiom.2024.105174 \nKtena I, Wiles O, Albuquerque I, et al (2024) Generative models improve fairness of medical \nclassifiers under distribution shifts. Nat Med 30:1166–1173. \nhttps://doi.org/10.1038/s41591-024-02838-6 \nKuppa A, Aouad L, Le-Khac N-A (2021) Towards Improving Privacy of Synthetic DataSets. \nIn: Gruschka N, Antunes LFC, Rannenberg K, Drogkaris P (eds) Privacy \nTechnologies and Policy. Springer International Publishing, Cham, pp 106–119 \nLegido-Quigley C, Wewer Albrechtsen NJ, Bæk Blond M, et al (2025) Data sharing \nrestrictions are hampering precision health in the European Union. Nat Med 1–2. \nhttps://doi.org/10.1038/s41591-024-03437-1 \nLekadir K, Frangi AF, Porras AR, et al (2025) FUTURE-AI: international consensus guideline \nfor trustworthy and deployable artificial intelligence in healthcare. \nhttps://doi.org/10.1136/bmj-2024-081554 \nLikert R (1932) A technique for the measurement of attitudes. Arch Psychol 22  140:55–55 \nMa J, He Y, Li F, et al (2024) Segment anything in medical images. Nat Commun 15:654. \nhttps://doi.org/10.1038/s41467-024-44824-z \nPackhäuser K, Folle L, Thamm F, Maier A (2023) Generation of Anonymous Chest \nRadiographs Using Latent Diffusion Models for Training Thoracic Abnormality \nClassification Systems. In: 2023 IEEE 20th International Symposium on Biomedical \nImaging (ISBI). pp 1–5 \nPackhäuser K, Gündel S, Münster N, et al (2022) Deep Learning-based Patient \nRe-identification Is able to Exploit the Biometric Nature of Medical Chest X-ray Data. \nSci Rep 12:14851. https://doi.org/10.1038/s41598-022-19045-3 \nPan S, Wang T, Qiu RLJ, et al (2023) 2D medical image synthesis using transformer-based \ndenoising diffusion probabilistic model. Phys Med Biol 68:105004. \nhttps://doi.org/10.1088/1361-6560/acca5c \nPark HY, Bae H-J, Hong G-S, et al (2021) Realistic High-Resolution Body Computed \nTomography Image Synthesis by Using Progressive Growing Generative Adversarial \nNetwork: Visual Turing Test. JMIR Med Inform 9:e23328. \nhttps://doi.org/10.2196/23328 \nPaul W, Cao Y, Zhang M, Burlina P (2021) Defending Medical Image Diagnostics Against \nPrivacy Attacks Using Generative Methods: Application to Retinal Diagnostics. In: \nOyarzun Laura C, Cardoso MJ, Rosen-Zvi M, et al. (eds) Clinical Image-Based \nProcedures, Distributed and Collaborative Learning, Artificial Intelligence for \nCombating COVID-19 and Secure and Privacy-Preserving Machine Learning. \nSpringer International Publishing, Cham, pp 174–187 \nPeng J, Chen G, Saruta K, Terata Y (2023) 2D brain MRI image synthesis based on \nlightweight denoising diffusion probabilistic model. Med Imaging Process Technol 6:. \nhttps://doi.org/10.24294/mipt.v6i1.2518 \n \n\n \nPinaya WHL, Tudosiu P-D, Dafflon J, et al (2022) Brain Imaging Generation with Latent \nDiffusion Models \nSaha A, Bosma JS, Twilt JJ, et al (2024) Artificial intelligence and radiologists in prostate \ncancer detection on MRI (PI-CAI): an international, paired, non-inferiority, \nconfirmatory study. Lancet Oncol 25:879–887. \nhttps://doi.org/10.1016/S1470-2045(24)00220-1 \nSalinas MP, Sepúlveda J, Hidalgo L, et al (2024) A systematic review and meta-analysis of \nartificial intelligence versus clinicians for skin cancer diagnosis. Npj Digit Med 7:1–23. \nhttps://doi.org/10.1038/s41746-024-01103-x \nSchwabe D, Becker K, Seyferth M, et al (2024) The METRIC-framework for assessing data \nquality for trustworthy AI in medicine: a systematic review. Npj Digit Med 7:1–30. \nhttps://doi.org/10.1038/s41746-024-01196-4 \nSomepalli G, Singla V, Goldblum M, et al (2023) Understanding and Mitigating Copying in \nDiffusion Models \nTak D, Garomsa BA, Chaunzwa TL, et al (2024) A foundation model for generalized brain \nMRI analysis \nTanioka S, Aydin OU, Hilbert A, et al (2024) Prediction of hematoma expansion in \nspontaneous intracerebral hemorrhage using a multimodal neural network. Sci Rep \n14:16465. https://doi.org/10.1038/s41598-024-67365-3 \nTejani AS, Klontzas ME, Gatti AA, et al (2024) Checklist for Artificial Intelligence in Medical \nImaging (CLAIM): 2024 Update. Radiol Artif Intell. https://doi.org/10.1148/ryai.240300 \nTirumala K, Markosyan AH, Zettlemoyer L, Aghajanyan A (2022) Memorization Without \nOverfitting: Analyzing the Training Dynamics of Large Language Models \nWang Z, Bovik AC, Sheikh HR, Simoncelli EP (2004) Image Quality Assessment: From Error \nVisibility to Structural Similarity. IEEE Trans Image Process 13:600–612. \nhttps://doi.org/10.1109/TIP.2003.819861 \nWen Y, Liu Y, Chen C, Lyu L (2024) Detecting, Explaining, and Mitigating Memorization in \nDiffusion Models \nYang K, Musio F, Ma Y, et al (2024) TopCoW: Benchmarking Topology-Aware Anatomical \nSegmentation of the Circle of Willis (CoW) for CTA and MRA \nYeghiazaryan V, Voiculescu I (2018) Family of boundary overlap metrics for the evaluation of \nmedical image segmentation. J Med Imaging 5:1. \nhttps://doi.org/10.1117/1.JMI.5.1.015006 \nYoon T, Choi JY, Kwon S, Ryu EK (2023) Diffusion Probabilistic Models Generalize when \nThey Fail to Memorize \nYushkevich PA, Piven J, Hazlett HC, et al (2006) User-guided 3D active contour \nsegmentation of anatomical structures: significantly improved efficiency and reliability. \nNeuroImage 31:1116–1128. https://doi.org/10.1016/j.neuroimage.2006.01.015 \nZhao T, Gu Y, Yang J, et al (2025) A foundation model for joint segmentation, detection and \nrecognition of biomedical objects across nine modalities. Nat Methods 22:166–176. \nhttps://doi.org/10.1038/s41592-024-02499-w \nZhou Wang, Bovik AC (2009) Mean squared error: Love it or leave it? A new look at Signal \nFidelity Measures. IEEE Signal Process Mag 26:98–117. \nhttps://doi.org/10.1109/MSP.2008.930649 \nZou KH, Warfield SK, Bharatha A, et al (2004) Statistical Validation of Image Segmentation \nQuality Based on a Spatial Overlap Index. Acad Radiol 11:178–189. \nhttps://doi.org/10.1016/S1076-6332(03)00671-8 \n \n \n \n \n \n\n \nData availability statement \nData are available from the corresponding author upon reasonable request.  \nAcknowledgements \nComputation has been performed on the HPC for Research cluster of the Berlin Institute of \nHealth. \nDisclosures  \nThe authors declare that they have no conflicts of interest. \nFunding statement  \nThis work has received funding from the European Commission through Horizon Europe \ngrant VALIDATE (Grant No. 101057263, coordinator: DF) and Horizon Europe Grant \nCYLCOMED (Grant No.101095542). In addition, This work has received funding from the \nGerman Federal Ministry of Education and Research (ANONYMED Project 16KISA043).  \nAuthor contribution statement \nOrhun Utku Aydin: Writing – review & editing, Writing – original draft, Visualization, Software, \nMethodology, Investigation, Formal analysis, Data curation, Conceptualization. Alexander \nKoch: Writing – review & editing, Visualization, Software, Investigation, Formal analysis, \nData curation, Conceptualization. Adam Hilbert: Writing – review & editing, Visualization, \nSupervision, Software, Project administration, Methodology, Investigation, Funding \nacquisition, Formal analysis, Conceptualization. Jana Rieger: Writing – review & editing, \nVisualization, Software. Felix Lohrke: Writing – review & editing, Methodology, Software. \nSatoru Tanioka: Writing – review & editing, Writing – original draft, Visualization, Validation, \nFormal analysis, Data curation. Dietmar Frey: Writing – review & editing, Writing – original \ndraft, Validation, Supervision, Formal analysis, Resources, Project administration, \nInvestigation, Funding acquisition. \n \n \n\n \nSupplementary Materials \nAppendix 1.  \n \nA. Measures for image comparison \n \nImage-level analysis \n \nMean Absolute Error ​\n​\nThe Mean Absolute Error (MAE) is calculated as the average of the absolute \ndifferences between corresponding voxels in the generated and real volumes: ​\n \n \n​\nwhere N is the total number of voxels.  \n \nRoot Mean Squared Error​\nThe Root Mean Squared Error (RMSE) is calculated as the square root of the \naverage of the squared differences between corresponding voxels in the generated \nand real volumes: \n \n \n \nMean Structural Similarity Index Measure \n \nThe SSIM was calculated using the structural_similarity function of the \nskimage.metrics library based on the implementation by Wang et al using a gaussian \nweighting function with a standard deviation of 1.5 in the calculation of SSIM (Wang \net al. 2004). The SSIM considers the luminance, contrast and structure of the images. \nTo provide a single measure for comparison between a pair of real and generated \nimages the mean SSIM was used for replica detection (Zhou Wang and Bovik 2009).  \n \n \n \nWhere μx, μy are the pixel mean values, σx, σy are the standard deviations, σxy the \ncovariance of x and y, and C1, C2 are stabilisers. \n \n\n \n \nFeature-level analysis \nCosine similarity  \nThe cosine similarity was calculated after flattening the 3D images to 1-dimensional \nvectors. The numpy library was used for calculation with the following formula.  \n \n \n \nwhere x . y is the dot product and ||x|| denotes the euclidean norm.  \n \nSegmentation-level analysis \n \nDice coefficient \n \nThe Dice coefficient is arguably the most popular measure for segmentation \nperformance assessment. We use the Dice coefficient to perform a region of interest \nlevel analysis by comparing segmentations of the leading structures in generated \nand real images. The Dice coefficient can be calculated using the confusion matrix \nvalues of true positives (TP), false positives (FP) and false negatives (FN).  \n \n \n \nAverage Surface Distance \nThe average surface distance is the average distance of outline of the predicted \nsurface to the outline of the ground truth surface and vice versa. \nLet S1, S2 be two surfaces and let d(s, S) be the distance from voxel s to surface the \nS. The distance is defined as \n \n \n \nthen the average surface distance ASD can be defined as: \n \n \n \nIn the exception case for the multiclass evaluation, where one class has an empty \nsegmentation, the 95 th percentile Hausdorff distance of the whole image was used \nas the ASD value for that class.  \nFor our replica detection framework, we use an open source implementation of ASD \nfrom (https://github.com/google-deepmind/surface-distance) due to its popularity and \n \n\n \nease of integration. Other distance based measures might be used interchangeably \nto assess similarity between segmentations in the scope of replica detection. \n \n \nB. Full replica detection plots \n \n \n \n\n \n \n \n \n \n \n \n \n",
  "metadata": {
    "source_path": "papers/arxiv/RELICT_A_Replica_Detection_Framework_for_Medical_Image_Generation_a4df415acd6028cc.pdf",
    "content_hash": "a4df415acd6028cc2b5a48566af72dfb42d3f61384142ead95b1ad293bff9fa1",
    "arxiv_id": null,
    "title": "UNLINKED_PREPRINT: A Replica Detection Framework for Medical Image Generation ",
    "author": "",
    "creation_date": "",
    "published": "",
    "pages": 26,
    "size": 4483076,
    "file_mtime": 1740470156.3051975
  }
}