{
  "text": "Published as a conference paper at ICLR 2025\nREASONING\nWITH LATENT THOUGHTS:\nON\nTHE\nPOWER OF LOOPED TRANSFORMERS\nNikunj Saunshi1, Nishanth Dikkala1, Zhiyuan Li1,2, Sanjiv Kumar1, Sashank J. Reddi1\n{nsaunshi, nishanthd, lizhiyuan, sanjivk, sashank}@google.com\n1Google Research, 2Toyota Technological Institute at Chicago\nSubmitted on Oct. 1, 2024\nABSTRACT\nLarge language models have shown remarkable reasoning abilities and scaling\nlaws suggest that large parameter count, especially along the depth axis, is the pri-\nmary driver. In this work, we make a stronger claim — many reasoning problems\nrequire a large depth but not necessarily many parameters. This unlocks a novel\napplication of looped models for reasoning. Firstly, we show that for many syn-\nthetic reasoning problems like addition, p-hop induction, and math problems, a\nk-layer transformer looped L times nearly matches the performance of a kL-layer\nnon-looped model, and is significantly better than a k-layer model. This is further\ncorroborated by theoretical results showing that many such reasoning problems\ncan be solved via iterative algorithms, and thus, can be solved effectively using\nlooped models with nearly optimal depth. Perhaps surprisingly, these benefits also\ntranslate to practical settings of language modeling — on many downstream rea-\nsoning tasks, a language model with k-layers looped L times can be competitive\nto, if not better than, a kL-layer language model. In fact, our empirical analysis\nreveals an intriguing phenomenon: looped and non-looped models exhibit scaling\nbehavior that depends on their effective depth, akin to the inference-time scal-\ning of chain-of-thought (CoT) reasoning. We further elucidate the connection to\nCoT reasoning by proving that looped models implicitly generate latent thoughts\nand can simulate T steps of CoT with T loops. Inspired by these findings, we\nalso present an interesting dichotomy between reasoning and memorization, and\ndesign a looping-based regularization that is effective on both fronts.\n1\nINTRODUCTION\nLanguage models have shown a lot of promise in solving problems that require strong reasoning abil-\nities like math, coding, common sense reasoning and logical puzzles (Brown et al., 2020; Team et al.,\n2023). This has sparked interest in developing techniques to improve reasoning on harder problems\n(Wei et al., 2022b) and has inspired theoretical studies on how Transformers are able to perform\nreasoning (Feng et al., 2024; Sanford et al., 2024a). Reasoning abilities are often emergent in larger\nlanguage models (Wei et al., 2022a) – this aligns with various scaling laws (Kaplan et al., 2020; Hoff-\nmann et al., 2022; Allen-Zhu & Li, 2024) that show that the performance of language models is very\nstrongly dependent on the model size (i.e., number of parameters) and much lesser on other archi-\ntectural design choices. However, recent works have started to question this view. Ye et al. (2024)\nargue that scaling laws for reasoning are more subtle, and depth is very important in addition to\nparameter count – at the same parameter count, deeper but shallower models are better. This is a de-\nviation from the conventional scaling law wisdom, but it intuitively makes sense because reasoning\nproblems often requires multi-step compositional thinking, and thus, depth can play a crucial role.\nIn this work, we make a stronger claim – while depth is important, many reasoning problems do not\nnecessarily require a lot of parameters. How does one solve reasoning problems with large depth\nbut few parameters? We argue that looped models are perfectly suited for this, where the same\n1\narXiv:2502.17416v1  [cs.CL]  24 Feb 2025\n\nPublished as a conference paper at ICLR 2025\nFigure 1: Illustration of the simple and architecture agnostic looping mechanism that we consider.\nA k-layer block looped L times (middle) is denoted by (k ⊗L), which can essentially be viewed\nas a weighted shared model. The iso-param baseline, (k ⊗1), is a k-layer model with the same\nnumber of distinct parameters. The iso-FLOP baseline, (kL ⊗1), is a kL-layer model with the\nsame depth but L times more parameters. Middle looping is a strategy that is inspired from prior\nworks on model stacking (e.g. (Saunshi et al., 2024)).\nfunction, parameterized with few parameters, is iteratively applied on the input. This leads us to our\nfirst important claim:\nClaim 1: Many reasoning problems require depth but not necessarily parameters. That is, they can\nbe solved via looped models\nLooped models have been studied in the literature for parameter efficiency (Lan et al., 2020),\nadaptive compute (Dehghani et al., 2018), equilibrium models (Bai et al., 2019) and for in-context\nlearning (Yang et al., 2023; Gatmiry et al., 2024a). In this work, we initiate the study of looped\nmodels in the context of reasoning. Admittedly, reasoning is not very well-defined and can be\nof various forms (Sun et al., 2023). Acknowledging this hurdle, we focus on a non-exhaustive\nlist of problems that intuitively require reasoning and that are inspired by reasoning benchmarks.\nThroughout the paper, we use the notation (k ⊗L) to denote a k-layer model looped L times\n(precise definition in Section 2), which has the same number of parameters as a (k ⊗1) model\nand same flops as a (kL ⊗1) non-looped model (see Figure 1). As a first step towards connecting\nlooped models and reasoning, we empirically evaluate looped models on several simple reasoning\ntasks in the literature (e.g. Section 2). Perhaps surprisingly, we find that a (k ⊗L) looped models\ndoes almost as well as, if not better than, a non-looped model (kL ⊗1) that has the same effective\ndepth but L times more parameters on these reasoning tasks. The looped model is also significantly\nbetter than a (k ⊗1) model which has the same number of parameters. Our theoretical results on the\nexpressiveness of looped models in representing iterative algorithms with short description further\ncorroborate these empirical findings and provide strong support for our claim. This naturally raises\nan important question: do looped models benefit language modeling in a similar manner?\nClaim 2: For language modeling, looped models have an inductive bias towards good reasoning\ndespite having worse perplexity and memorization to an iso-flop non-looped model\nFor the above claim, we again train a (k ⊗L) looped model on causal language modeling and com-\npare it to the iso-param (k ⊗1) and iso-flop (kL⊗1) non-looped baselines. While the looped model\nimproves over the iso-param baseline, perhaps unsurprisingly, it ends up with worse perplexity than\niso-flop baseline, since perplexity depends strongly on number of parameters. However, the down-\nstream evaluations reveal an intriguing trend: looped models have a tendency to improve tasks that\nrequire reasoning a lot more than memorization tasks. Specifically, the looped model has reasoning\nperformance much closer to the iso-flop baseline, sometimes even exceeding it despite having L\ntimes fewer parameters and worse perplexity. This contrasting behavior between the pretraining and\ndownstream metrics has been a subject of study lately (Saunshi et al., 2022; Liu et al., 2023) and is\nattributed to the inductive biases introduced due to different architectures and training algorithms.\nOur empirical analysis also uncovers an interesting phenomenon: accuracy on downstream tasks\nscale as logarithm of the effective depth. In particular, more loops enhances performance, and the\nrelative benefit of loops is higher for tasks that require more reasoning. This is conceptually similar\nto inference time scaling discovered for CoT, but with looping as a central component. To further\nelucidate this interesting relationship of looped models with CoT, we present the following claim.\n2\n\nPublished as a conference paper at ICLR 2025\nClaim 3: Looped models generate latent thoughts and can, in theory, simulate CoT reasoning\nNote that CoT reasoning gives the model more time and compute by generating multiple thought\ntokens before the answer, and it has powered the recent paradigm of inference-time scaling for\n“thinking” models like O1 and DeepSeek’s R1 (Guo et al., 2025). We make an observation about\nCoT reasoning – it is essentially a looped model that generates 1 thought token in each iteration.\nHowever, looped models seem to be much more powerful, since they can generate multiple latent\nthoughts in each iteration. We translate this intuition into a theoretical result about how looped\nmodels can simulate CoT reasoning.\nMotivated by these findings, we propose a regularization scheme that aims to tap into the inductive\nbias of looped models towards reasoning. This leads us to our final claim:\nClaim 4: Looping-inspired regularization can leverage this inductive bias towards better reasoning\nWith the backdrop of these claims, we concretely present the contributions of the paper below:\n• In this paper we study looped models – multilayer models with weight sharing – and their role in\nreasoning. In particular, we compare a k-layer model looped L times, denoted by (k ⊗L), with\nan iso-param (k ⊗1) non-looped model with k layers and an iso-flop (kL ⊗1) model with kL\nlayers and L times more parameters.\n• We conduct experiments on synthetic reasoning tasks like addition, p-hop induction and GSM-\nstyle math word problems in Section 2. For these tasks, we surprisingly find that iso-flop looped\nmodels, despite having way fewer parameters, can nearly match or outperform a non-looped\nmodel. Supporting these experiments, in Section 5 we present theoretical results for why looped\nmodels can solve such problems with almost optimal depth.\n• In Section 3, we train looped models on causal language modeling at 1B parameter scale. Here,\nwe show that looped models have an inductive bias towards doing well on reasoning benchmarks,\ndespite having much worse perplexity. This finding is novel, since most prior work on looping\nfocused more on perplexity metrics rather than downstream reasoning tasks. We validate this in-\nductive bias by visualizing the perplexity vs downstream performance plots as training process.\nAdditionally, we show that looped models demonstrate good scaling behavior on various bench-\nmarks as the number of loops are increased, akin to CoT reasoning. Finally, we show that looped\nmodels, along with a scratchpad, can simulate chain-of-thought reasoning.\n• Inspired by this inductive bias, in Section 4, we propose a regularization that encourages layers\nto be more similar to each other. We find that training with such a regularization inherits the\ninductive bias of looped models towards reasoning without affecting perplexity.\n2\nLOOPED MODELS ON SIMPLE REASONING TASKS\nWe first explore our hypothesis of looped models helping reasoning tasks on a set of tasks con-\nstructed in a procedural manner. The illustrative reasoning tasks we consider are: n-ary addition,\np-hop induction head that tests the model’s ability to track back for p steps, and i-GSM which con-\nsists of synthetically constructed grade-school math problems. While these obviously do not cover\nthe whole spectrum of reasoning problems, they provide useful insights into looped models and\nprovide a basis for the theoretical results in Section 5.\nLooped models. While many variants of looped model have been proposed (Lan et al., 2020;\nDehghani et al., 2018; Giannou et al., 2023; Yang et al., 2023; Mohtashami et al., 2023), we use\nthe vanilla version for simplicity of our exploration. For any sequence-to-sequence function f, we\ndenote f (L) = f ◦f · · · ◦f to be the function that is f looped L times. In general, the looping\nmechanism is independent of architectural choices for f. For the rest of the paper, we typically use\nf to denote a k-layer Transformer backbone of a model. Thus, f looped L times is the same as a\nkL layer model with weight sharing between all L blocks of k consecutive layers. We denote such\nlooped models with the notation (k ⊗L). Please refer to Figure 1 for a succinct illustration of the\nlooping mechanism. Section 5.1 provides a more formal definition of looped transformers that is\nused for theoretical analysis.\n3\n\nPublished as a conference paper at ICLR 2025\nTable 1: Accuracy of looped and non-looped models on the addition problem (left) and p-hop\ninduction (right), as described in Section 2. Left. For addition, we report accuracies for different\nnumber of operands (n). For all budgets, a (k ⊗12/k) looped model is significantly better than the\niso-param (k⊗1) model and also nearly as good as the non-looped iso-flop (12⊗1) baseline model.\nRight. The findings are very similar for the p-hop problem for different values of p. Note that a\nrandom guessing baseline would get at least 25% accuracy (since only 4 choices for answer). This\nsuggests that depth via looping and small number of parameters is very effective for these problems.\nAddition of n numbers\nParams /\nn = 8\nn = 16\nn = 24\nn = 32\nFLOPs\nBase (12 ⊗1)\n12x / 12x\n100.0\n100.0\n100.0\n100.0\n1 layer model\nBase (1 ⊗1)\n1x / 1x\n0.1\n0.1\n0.1\n0.0\nLoop (1 ⊗12)\n1x / 12x\n99.9\n100.0\n99.9\n99.6\n2 layer model\nBase (2 ⊗1)\n2x / 2x\n85.8\n71.5\n49.3\n38.8\nLoop (2 ⊗6)\n2x / 12x\n100.0\n99.8\n99.7\n99.5\n3 layer model\nBase (3 ⊗1)\n3x / 3x\n97.2\n78.5\n69.2\n60.7\nLoop (3 ⊗4)\n3x / 12x\n100.0\n99.1\n97.0\n96.6\np-hop with n tokens\nParams /\np = 16\np = 32\nFLOPs\nn = 256\nn = 256\nBase (6 ⊗1)\n6x / 6x\n99.9\n99.6\n1 layer model\nBase (1 ⊗1)\n1x / 1x\n48.9\n49.0\nLoop (1 ⊗6)\n1x / 6x\n99.9\n99.5\n2 layer model\nBase (2 ⊗1)\n2x / 2x\n68.8\n59.4\nLoop (2 ⊗3)\n2x / 6x\n99.9\n99.8\n3 layer model\nBase (3 ⊗1)\n3x / 3x\n97.2\n73.0\nLoop (3 ⊗2)\n3x / 6x\n99.9\n99.5\n2.1\nEXPERIMENTS WITH SIMPLE REASONING PROBLEMS\nn-ary addition. We consider the problem of adding n numbers with 3 digits each. Addition is pop-\nular in the literature to study aspects of reasoning with Transformers, such as use of scratchpad (Nye\net al., 2021), chain of thought reasoning (Lee et al., 2024; Li et al., 2024) and length generalization\n(Cho et al., 2024). One reason for its popularity is that addition can have algorithmic solutions,\nwhich is a feature of many reasoning problems. For our experiments, we train on a uniform mixture\non numbers of operands n ∈{2, 4, 8, 16, 32} and sample each 3-digit operand uniformly at random\nbetween [0, 999]. We train all models directly on the input-output pair, without any chain-of-thought\nsteps. Following is an example for n = 4:\nInput: “315 + 120 + 045 + 824 =” ; Output = “1304”.\nWe train a standard Transformer-based baseline (12 ⊗1) model with 12 layers. Please refer to\nAppendix A.1 for details on the training setup. We also train (k ⊗12/k) looped model and an\niso-param (k ⊗1) baseline models for comparison, and vary k ∈{2, 3, 4, 6}. All trained models\nare finally evaluated separately on each of n ∈{8, 16, 24, 32} to measure accuracy on increasingly\ndifficult problems. Results are presented in Table 1.\nWe find that, while the shallower baselines (k⊗1) degrade with lower k, the looped model (k⊗12/k)\nperforms very well, and nearly matches the iso-flop (12⊗1) baseline. In fact, even a 1-layer network\nlooped 12 times is able to solve this, despite using merely 1/12th of the parameters of the baseline.\nThis suggests the addition problem primarily requires depth, but not necessarily more parameters.\np-hop induction.\nThe p-hop problem is a synthetic induction task studied in Sanford et al.\n(2024b), who were inspired by the analysis of induction heads from Elhage et al. (2021). Specif-\nically, given a sequence of letters v = (v1 . . . vn) from an alphabet Σ, an induction head tries to\nfind the penultimate occurrence of vn (from left to right) in the sequence and output the character\nimmediately succeeding it. The p-hop problem generalizes this idea to sequentially hop p times.\nIntuitively, the p-hop problem tests a model’s ability to recursively backtrack and retrieve the answer\nfor a given query. This is reminiscent of the reasoning abilities required to solve reading compre-\nhension kind of problems. We present the formal definition of the p-hop problem in Definition A.1.\nWe perform experiments with looped and non-looped models on the p-hop problem, with alphabet\nsize set to 4 and sequences of length 256, and vary p between 16 and 32 to control the problem\ndifficulty. Our observations are presented in Table 1. Similarly to our findings on the addition task,\nreasonably deep looped models perform as well as the baseline using much fewer parameters.\n4\n\nPublished as a conference paper at ICLR 2025\nTable 2: Left. Symbolic i-GSM problem and its solution. Right. Accuracy of looped and non-\nlooped models on the i-GSM task from Section 2. (k ⊗L) looped model is significantly better than\nthe iso-param (k ⊗1) model and performs as well as non-looped iso-flop (kL ⊗1) model.\nQuestion.\nE#I := 4. E#J := E#I. K#N\n:= I#N + J#O + F#K. F#K := E#J. J#O :=\nF#K + K#O + E#J. H#J := E#J + F#K. I#P\n:= L#M + I#N + K#O. I#M := J#O + J#P\n+ F#K. J#P := H#J - F#K. L#M := I#N +\nJ#P + F#K. I#N := 2 * J#P + H#J + E#I.\nK#O := J#P + I#N + E#J. I#P?\nAnswer with CoT.\nE#I = 4. =⇒E#I\n= 4. E#J = E#I. =⇒E#J = 4. F#K = E#J.\n=⇒F#K = 4. H#J = E#J+F#K. =⇒H#J\n= 1. J#P = H#J-F#K. =⇒J#P = 4. I#N\n= 2J#P+2H#J+2E#I. =⇒I#N = 4. L#M\n= I#N+J#P+F#K.\n=⇒\nL#M = 5. K#O\n= J#P+I#N+E#J.\n=⇒\nK#O = 5. I#P =\nL#M+I#N+K#O. =⇒I#P = 0.\nParams / FLOPs\nAccuracy\nBase (8 ⊗1)\n8x / 8x\n73.2\n1 layer model\nBase (1 ⊗1)\n1x / 1x\n24.5\nLoop (1 ⊗2)\n1x / 2x\n52.3\nLoop (1 ⊗4)\n1x / 4x\n69.9\nLoop (1 ⊗8)\n1x / 8x\n73.2\n2 layer model\nBase (2 ⊗1)\n2x / 2x\n54.0\nLoop (2 ⊗2)\n2x / 4x\n66.9\nLoop (2 ⊗4)\n2x / 8x\n73.6\n4 layer model\nBase (4 ⊗1)\n4x / 4x\n71.3\nLoop (4 ⊗2)\n4x / 8x\n71.6\ni-GSM (Synthetic Grade School Math Problems).\nInspired by Ye et al. (2024), we built our own\nversion of grade-school level math word problems. While we follow many of the design guidelines\nof Ye et al. (2024), we make a few simplifying changes. We generate the math problem as a DAG\nof arithmetic computations modulo 7, and restrict the depth of the graph to 4. For simplicity, we\nretain the problems in the symbolic form and do not map them to English (e.g., see Table 2) 1 We\ntrain models of depths 1, 2, 4 and 8 and compare them with different looped variants in Table 2. The\nanswer is computed modulo 7. Hence, a random guessing baseline would get at least 14% accuracy.\nAppendix A.1 has more details. Remarkably, we again observe that a depth k model looped L times\nmatches or outperforms a depth kL model and far outperforms a depth k non-looped model. This\nsuggests that even a more complex and realistic looking reasoning problem does not need too many\nparameters and can benefit from depth via looping.\n3\nLANGUAGE MODELING WITH LOOPED MODELS\nIn this section, we pretrain and evaluate looped models for causal language models. We train models\non 250B tokens of the Pile dataset (Gao et al., 2020) and use a 24-layer 1B parameter model for most\nexperiments, motivated by the setting in Tay et al. (2022) (refer to Appendix A.2 for more details).\n3.1\nEXPERIMENTS WITH 1B LANGUAGE MODELING\nFor causal language modeling, we pretrain various looped models on the standard GPT2-style next\ntoken prediction objective (Radford et al., 2019). We train models with different parameter budgets\nto make sure that the findings are robust. We remind the reader that the notation (k⊗L) corresponds\nto a k layer model looped L times. For each setting, we compare 3 models: (a) (24 ⊗1): 24-layer\n1B model, (b) (k ⊗1): k-layer model with the same configuration as the 24-layer model for other\ndimensions, (c) (k ⊗24/k): k-layer model looped 24/k times to match the parameter count of (b)\nand match the effective depth/FLOPs of (a). We run experiments for k ∈{4, 6, 8, 12} to ensure that\nthe findings are robust. After pretraining on Pile, we evaluate the models on validation perplexity\nand on downstream benchmarks using k-shot evaluations. Results are summarized in Table 3\nEvaluation metrics. We evaluate the models on perplexity metric after training is completed. Since\nthere is growing evidence that perplexity, although very useful for training, is a narrow measure of\nmodel quality, we also track more holistic downstream evaluations (Liang et al., 2023). Thus, we\nevaluate the model on 4 important slices: closed book QA, open book QA, math word problems\nand reasoning primitives. These comprise of 19 different tasks in total.\n1Similar to Ye et al. (2024), the simplified setting still allows for over 7 billion unique solution templates.\n5\n\nPublished as a conference paper at ICLR 2025\nTable 3: Downstream evaluations for language models trained on the Pile dataset. Comparisons in-\nclude a 24-layer 1B-parameter baseline model, iso-flop looped models (k⊗24/k) for various param-\neter budgets k, and the corresponding iso-param baselines (k⊗1). Downstream evaluations are aver-\naged over tasks within 4 task groups. We also include the % Gap metric for each k to measure the gap\nbetween the iso-param and iso-flop baselines that is covered by the looped model (see Equation (1)).\nOverall the looped models are worse on perplexity and closed book QA (memorization benchmarks),\nbut the % Gap is much higher for task groups that require reasoning (open book QA, math word\nproblems). In fact for reasoning primitives, which are purely testing for reasoning skills, the looped\nmodels are much better than the 1B baseline for all k, despite having 24/k× fewer parameters.\nParams /\nPerplexity (↓)\nClosed\nOpen\nMath Word\nAll Tasks\nReasoning\nFLOPs\n(validation)\nBook QA (↑)\nBook QA (↑)\nProblems (↑)\nAverage (↑)\nPrimitives (↑)\n(4 tasks)\n(5 tasks)\n(6 tasks)\n(15 tasks)\n(4 tasks)\n24 layers\nBaseline\n24x / 24x\n7.40\n11.2\n33.9\n29.3\n26.0\n47.5\n12 layers\nBase (12 ⊗1)\n12x / 12x\n8.16\n8.2\n26.9\n26.7\n21.8\n35.7\nLoop (12 ⊗2)\n12x / 24x\n7.90\n9.3\n30.8\n34.3\n26.5\n51.2\n% Gap\n34 %\n37 %\n56 %\n282 %\n110 %\n131 %\nMiddle Loop\n12x / 24x\n7.81\n11.0\n32.3\n28.3\n25.0\n56.5\n(4 ⊗1, 4, 1)\n% Gap\n46 %\n94 %\n78 %\n62 %\n95 %\n176 %\n8 layers\nBase (8 ⊗1)\n8x / 8x\n8.75\n6.3\n22.7\n17.1\n16.1\n33.0\nLoop (8 ⊗3)\n8x / 24x\n8.19\n8.5\n30.8\n28.4\n23.9\n55.3\n% Gap\n41 %\n44 %\n72 %\n92 %\n78 %\n153 %\n6 layers\nBase (6 ⊗1)\n6x / 6x\n9.25\n4.0\n19.3\n17.7\n14.6\n24.1\nLoop (6 ⊗4)\n6x / 24x\n8.42\n8.2\n28.7\n29.8\n23.7\n56.1\n% Gap\n44 %\n58 %\n64 %\n104 %\n80 %\n136 %\n4 layers\nBase (4 ⊗1)\n4x / 4x\n10.12\n1.8\n13.8\n9.7\n9.0\n19.4\nLoop (4 ⊗6)\n4x / 24x\n8.79\n6.7\n26.2\n24.8\n20.4\n56.9\n% Gap\n48 %\n52 %\n61 %\n77 %\n67 %\n133 %\n• Closed book QA: This includes tasks like TriviaQA (Joshi et al., 2017), TydiQA-NoContext\n(Clark et al., 2020), Natural Questions (Kwiatkowski et al., 2019) and Web Questions (Talmor\n& Berant, 2018) that test the model’s ability to answer questions without any context, and thus,\nprimarily measure the memorization abilities of language models.\n• Open book QA: This includes tasks like TydiQA-GoldP (Clark et al., 2020), SquadV2 (Ra-\njpurkar et al., 2018), Drop (Dua et al., 2019), QuAC (Choi et al., 2018), CoQA (Reddy et al.,\n2019) that evaluate the model’s ability to infer the answer to a question from the extra context\nthat is provided, akin to reading comprehension.\n• Math word problems: To evaluate the model’s ability to reason, we test them on math word\nproblems considered in (Wei et al., 2022b). This includes tasks like SVAMP (Patel et al., 2021),\nASDiv (Miao et al., 2020), the MAWPS benchmark (Koncel-Kedziorski et al., 2016). We report\n5-shot evaluation for the pretrained model on these tasks.\n• Reasoning primitives: Saunshi et al. (2024) introduced these datasets to study the inductive\nbias of stacking towards improving reasoning, by isolating simple reasoning abilities. One\nsuch primitive is depth-k variable assignment that requires the model to resolve a chain of\nassignments of length k. An example of depth-0 var-assign is a=1, b=2, c=6, b=?, and example\nfor depth-1 var-assign is a=1, b=2, c=a, d=b, d=?. We evaluate on the math and coding variants\nof the depth-0 and depth-1 problems using 5-shot evaluation.\nFor each task group G from above, in Table 3 we report the average accuracy for that task group,\ndenoted by AvgG. Furthermore, for each layer budget k, we report the % gap between the iso-param\nand iso-flop models that is covered by the looped model. More specifically\n% Gap = AvgG(k ⊗24/k) −AvgG(k ⊗1)\nAvgG(24 ⊗1) −AvgG(k ⊗1) .\n(1)\nThis measures how effectively looping can bridge the gap between iso-param and iso-flops\nbaselines. Implicitly, it measures how different task groups behave when a model only has a few\nparameters but is given depth through looping. % Gap being closer to 0% means that providing\n6\n\nPublished as a conference paper at ICLR 2025\nFigure 2: Downstream evaluation for various task groups on the x-axis, vs validation log perplexity\non the y-axis (reversed), as training proceeds. The top plots compare a 12-layer baseline model\n(12 ⊗1) and the looped model (12 ⊗2). The second row compares the baseline 24-layer model\nand the 24-layer model trained with regularization using block size k = 4 and λreg = 10 (See Equa-\ntion (4)). For both comparisons we have similar observations. For closed book QA (memorization)\ntasks looping has very similar trends to baseline. For open book QA tasks and math word problems,\nlooping has much better downstream performance at an equivalent log perplexity. This verifies the\ninductive bias of looping and regularization towards better reasoning abilities.\ndepth via looping does not benefit the task group, and number of parameters is the most important\nfactor. On the other hand, % Gap closer to 100% means parameter count matters much less for the\ntask group, and that depth via looping is more essential.\nPerplexity results. Firstly we notice that all (k ⊗24/k) looped models have better perplexity\ncompared to the iso-param (k ⊗1) baseline, but worse perplexity compared to the non-looped 24-\nlayer baseline. The looped models only covers up roughly 34 −50% of the perplexity gap between\nthe iso-param and iso-flop baselines for various values of k. This perplexity gap is not too surprising\nsince the looped model has 24/k times fewer parameters, and thus, lower capacity than the 24-layer\nbaseline. This was also been observed in prior works (Lan et al., 2020; Mohtashami et al., 2023)\nand is the primary reason looped models have been ignored for language modeling. However, as we\nshall see shortly, the downstream metrics paint a more interesting and favorable picture.\nResults on QA tasks. We first consider closed book and open book QA categories in Table 3.\nClosed book QA tasks are primarily testing the model’s memorization abilities. Open book QA\non the other hand tests the model’s ability to infer the answer from the additional context that is\nprovided. Thus, intuitively, open book QA tasks require more reasoning. Firstly we notice that the\n% Gap for closed book QA (memorization) is very similar to % Gap for perplexity. The % Gap\nfor open book QA, on the other hand, is much higher for all parameter budgets. This suggests that\nlooped models relatively improve contextual QA much more than memorization based QA.\nMath problems and reasoning primitves. We also present the % Gap for the math word problem in\nTable 3. Surprisingly, we find that (k ⊗24/k) looped model can almost match the baseline (24 ⊗1)\nmodel for k ≥6, despite having k times fewer parameters. In fact, the 12 layer model looped twice\nis even signficantly better (34.3) than the 24 layer baseline (29.3), despite having 50% of parameters\nand worse perplexity; suggesting that looping disproportionately improves mathematical reasoning.\nTo better understand the effect on reasoning, we direct the readers attention to the evaluations for\nreasoning primitives in Table 3. The results are quite remarkable: (k ⊗24/k) looped models are\nbetter than the iso-flop baseline (24 ⊗1) at reasoning primitives, for all values of k. This is\na priori very surprising, since these are synthetic generated tasks and have nothing to do with the\n7\n\nPublished as a conference paper at ICLR 2025\npretraining data or the model architecture. Thus, solving these tasks necessarily requires reasoning\nfrom context, and memorization abilities will not help here. These results clearly suggest that looped\nmodels have a bias towards improving reasoning, despite having worse perplexity and memorization\nabilities. Next, we formalize the inductive bias towards reasoning via isoplots.\n3.2\nINDUCTIVE BIAS TOWARDS REASONING\nIn this section, we formalize the inductive bias by plotting the perplexity vs downstream metric iso-\nplots, as introduced in Saunshi et al. (2022). Section 3.1 showed that looped models have higher\nthan expected performance on reasoning problems. However, since looped models are worse on\nperplexity, it is hard to make a direct comparison between various models. One way to bring parity\nbetween models is to look at their downstream performances at the same validation pretraining loss\n(Liu et al., 2023). Saunshi et al. (2024) proposed plotting pretraining loss vs downstream metrics\nas training proceeds, as a way to study the inductive bias of various methods. For each model, we\nevaluate the log perplexity and downstream metrics at every 20k steps, starting from 120k steps. We\nplot these values in a scatter plot and fit a linear function with log perplexity and the corresponding\ndownstream metric being input and output respectively. Please refer to Figures 2 and 7 for two sets\nof isoplots.\nFindings. For all values of k, we observe the following:\n• The isoplots for (k⊗L) looped model and (k⊗1) baseline are very aligned for closed book QA\ntasks (if extrapolated). This suggests that log perplexity is a very strong indicator of downstream\nperformance on memorization based tasks.\n• For open book QA and math word problems, the isoplot line for the looped model is always\nhigher than the baseline model. This suggests that at the same log perplexity, looped models\nwill tend to have higher evaluation on these tasks that require more reasoning.\n• For reasoning primitives, there is a stark difference between looped and baseline models. The\nlooped model seems to have good performance at most points in training.\nOverall this suggests a strong inductive bias of looped models towards improving reasoning.\n3.3\nMIDDLE LOOPING VARIANT AND RELATIONSHIP WITH GRADUAL STACKING\nRecently Saunshi et al. (2024) introduced a gradual stacking (Gong et al., 2019; Reddi et al., 2023)\napproach for training language models called MidAS. This approach gradually grows the model\ndepth as training proceeds by duplicating certain layers of the model in each stacking operation.\nSurprisingly, they found that MidAS not only speeds up pretraining, but also improves reasoning\nin the same sense as Figure 2 – better reasoning at the same perplexity. Furthermore, the paper\nestablished a strong connection between stacking via MidAS and looped models, owing to the\nlayer duplication operation, and conjectured that this is the reason for such an inductive bias. Our\nresults from the previous section provides a compelling evidence for this conjecture by showing that\nlooped models also show a very similar inductive bias, thus, further strengthening the connection\nbetween stacking and looped models. Why such an inductive bias occurs is still an open question,\nand we believe that understanding this is an important future direction.\nFurthermore, inspired by their findings, we explore middle looping (see Figure 1 for an illustration)\n— a variant of looping which maintains independent layers at the start and the end of the network,\nand perform looping on the middle block of layers. The high-level intuition from Saunshi et al.\n(2024) is that the first and last layers play a special role in the model and thus, should be treated\ndifferently from the middle layers. In Table 3, we report results for a version of middle looping that\nis iso-param with a (12 ⊗1) baseline and iso-flop with a (24 ⊗1) baseline, just like the (12 ⊗2)\nmodel. Overall, we find that middle looping has better perplexity and more uniform improvements\nthan the default looping of (12 ⊗2) (except for math word problems), and thus, might be a more\npractical looping approach. We leave the exploration of the best looping strategies for future work.\n8\n\nPublished as a conference paper at ICLR 2025\nFigure 3: Scaling behavior for various task group as the effective depth increases. The blue curve\nshows how performance scales as the number of loops increases, without increasing parameters, us-\ning models of the form (4⊗D/4) for various values of D. The orange curve visualizes the scaling be-\nhavior of (D⊗1) which increases the depth by adding fresh parameters. For reasoning primitives, the\nlooped model scales as well, or even better, than the baseline despite having D/4 fewer parameters.\n3.4\nSCALING BEHAVIOR OF LOOPING\nIn this section, we discuss an intriguing scaling behavior of looping, specifically the impact of the\nnumber of loops on various evaluation metrics. In particular, we are interested in scaling behavior\nof: (a) accuracy as a function of number of loops and (b) comparison of looped and non-looped\nbaseline of the same effective depth. To this end, we pretrain various looped language models of the\nform (4 ⊗L), i.e., 4-layer model looped L times, for L ∈{1, 2, 3, 6, 9, 12}. To enable iso-FLOPs\ncomparison, we also train baseline models of the form (4L⊗1), which has L times more parameters.\nFor each task group, we plot the average accuracy as a function of the effective depth, i.e. D = 4L.\nFrom the results presented in Figure 3, we observe the following.\n1. In both cases, we find that the accuracies for all task groups continue to increase with more\nloops/depth, although, unsurprisingly, the returns are diminishing with depth for both looped and\nnon-looped models. Interestingly, for both looped and non-looped models, we found that one can\nfit a simple scaling law of the following form:\nAcc = α log(D) + β,\n(2)\nwhere D is the effective depth and α measures the impact of depth on downstream performance.\n2. Furthermore, for each task group, we compute αloop/αbase to assess the relative impact of “depth\nvia looping” compared to “depth via additional parameters”. We find that more loops continue\nto help, and the relative benefit of loops is higher for reasoning tasks like open book QA and\nmath problems. Remarkably, the impact of loops is even higher (1.19x) than impact of depth for\nreasoning primitives, which further consolidates the benefit of looped models for reasoning.\nLatent thoughts and connections to CoT reasoning.\nWe end this discussion with an important\nquestion: why should we expect this interesting scaling behavior of looped models? We argue that\nlooped models have a strong connection to chain-of-thought (CoT) reasoning. Such a connection is\ninsightful because recent works on thinking have shown CoT can demonstrate inference-time scaling\nbehavior, where the accuracy on reasoning datasets continues to improve as the length of the models\nchain of thought increases; i.e., generating more thoughts leads to better reasoning.\nTo establish this connection, we make a simple observation about CoT reasoning – it is essentially\na looped model that generates a single thought token in each iteration. However, looped models can\nbe more powerful since they can generate multiple “latent thoughts” in each iteration. This can be\nvisualized in Figure 4. We further translate this intuition into a theoretical result (see Section 5.4)\n9\n\nPublished as a conference paper at ICLR 2025\nTable 4: Results for the 24-layer 1B model with and without the regularization introduced in\nSection 4. We try various block sizes k motivated by the looped model settings from Table 3.\nOverall, regularization helps retain the inductive bias towards reasoning, with notable improvements\non math word problems and reasoning primitives, without almost neutral perplexity.\nPerplexity (↓)\nClosed\nOpen\nMath Word\nAll Tasks\nReasoning\n(validation)\nBook QA (↑)\nBook QA (↑)\nProblems (↑)\nAverage (↑)\nPrimitives (↑)\n(4 tasks)\n(5 tasks)\n(6 tasks)\n(15 tasks)\n(4 tasks)\nBaseline\n7.40\n11.2\n33.9\n29.3\n26.0\n47.5\nRegularized (k = 4, λreg = 1)\n7.41\n11.2\n34.8\n31.6\n27.2\n42.5\nRegularized (k = 4, λreg = 10)\n7.38\n12.5\n36.2\n36.4\n30.0\n57.2\nRegularized (k = 6, λreg = 10)\n7.40\n12.0\n35.8\n31.0\n27.5\n55.8\nRegularized (k = 8, λreg = 10)\n7.43\n11.3\n34.4\n32.8\n27.6\n56.3\nRegularized (k = 12, λreg = 10)\n7.51\n10.1\n34.1\n32.3\n27.0\n50.7\nabout how looped models can in fact simulate CoT reasoning. Given this connection to CoT rea-\nsoning, it is believable that looping can scale well for harder reasoning. We believe that leveraging\nlooping explicitly for inference-time scaling is a very promising future direction.\n4\nLOOPING-INSPIRED REGULARIZATION\nIn the previous section, we observed the looped models can improve reasoning with worse perplex-\nity. Can we leverage this observation to improve reasoning without affecting perplexity? Here, we\npropose a simple approach: regularize the weights of the model to encourage them to be close to a\nlooped model. This could have two advantages, (a) the model still has free parameters to improve\nperplexity, (b) the closeness to looped model can inherit the desirable inductive bias. In particular,\nif an L-layer model is denoted as f0 ◦f1 . . . , ◦fL/k−1, where each fi is a block of k layers, we\nadd a regularization term that makes all the weights of the block fi close to fi+1 in terms of cosine\nsimilarity. For a parameter group G (e.g. first feed-forward layer, or query matrix in Transformer),\nwe use θ(0)\nG , θ(1)\nG , . . . , θ(L−1)\nG\nto denotes the weights in all layers. Then the regularization term is\nRG(k) =\n1\nL −k\nL\nk −2\nX\ni=0\nk−1\nX\nj=0\nCosine\n\u0010\nθ(ik+j)\nG\n, θ((i+1)k+j)\nG\n\u0011\n(3)\nThe final loss function is a sum of the standard cross-entropy loss and the regularization term aver-\naged over all groups, multiplied by a scalar hyperparameter. Let G denote the set of all parameter\ngroups; G = {Attn-Q, Attn-K, . . . , FFN-W2}\nL = Lxent + λreg|G|−1 X\nG∈G\nRG(k)\n(4)\nIn the above formulation, λreg = 0 would recover standard training and λreg →∞would converge\nto a fully looped model. Intermediate values of λreg will lead to “approximately” looped models.\nFor instance, to emulate the (4 ⊗6) looped model setting, we use pick k = 4, L = 24 and a large\nregularization strength like λreg = 10. All other hyperparameters are kept the same as baseline\ntraining for a fair comparison. We tried options other than cosine similarity, like ℓ2 norm, to bring\nthe weights closer but found that cosine was more robust and effective.\nCosine similarities. Firstly, we check if the regularization had the right effect by measuring the\ncosine similarities between the successive blocks of k layers at the end of training. We, indeed, find\nthat for all parameter groups, the cosine similarity around 0.98 or higher (see Figure 5).\nInductive bias. To confirm the inductive bias of the regularizer, we visualize the log perplexity vs\ndownstream isoplots for λreg = 10 and baseline models in Figure 2. While the plots are similar\nfor closed book QA, a strong inductive bias shows up for open book QA and reasoning problems.\nCrucially, the regularized model does well on reasoning without hurting perplexity (see Table 4).\n10\n\nPublished as a conference paper at ICLR 2025\n5\nTHEORETICAL ANALYSIS FOR LOOPED MODELS\nIn this section, we present theoretical results to understand the phenomenon from the previous\nsections – why can looped model with few parameters match an iso-flops non-looped baseline on\nreasoning problems? While a complete theory is challenging, since “reasoning” is a very broad\nconcept, the goal is to provide some intuition and formalization for the expressive power of looped\nmodels.\nFirst, we show that looped Transformers can effectively solve group composition (a\ngeneralization of the addition problem). Then we show a very general result on how a non-looped\nmodel with very few distinct layers can be simulated by a looped model with a small blowup in\nmodel size. This result is then used to solve the p-hop problem using a one-layer looped transformer.\nOur construction for group composition and p-hop problem are nearly optimal in terms of depth\nand much more efficient in terms of parameters compared to non-looped models.\n5.1\nPRELIMINARIES AND NOTATIONS\nWe first define the standard transformer architecture. Throughout the paper we will fix the dimension\nof the embedding to be d ∈N+, the vocabulary to be V and maximum sequence length to be nmax.\nWe will use id to denote the identity mapping. Here, we describe the high-level notation. Please\nrefer to Appendix B.1 for detailed notations. We use FF and MHA to denote the feed-forward and\nattention layers respectively, and θMHA and θFF denote the parameters in these layers.\nDefinition 5.1 (Transformer Block). Given number of layers L ∈N+ and parameter θTB =\n(θ(l)\nMHA, θ(l)\nFF)L−1\nl=0 , L-layer transformer block TBθTB : (Rd)n →(Rd)n for any n ∈N+ is defined as\nTBθTB ≜(id + FFθ(L−1)\nFF\n) ◦(id + MHAθ(L−1)\nMHA\n) ◦· · · (id + FFθ(0)\nFF ) ◦(id + MHAθ(0)\nMHA),\n(5)\nWe also denote EMBED and OUTPUT to be the input embedding and output softmax layers respec-\ntively. Please refer to Appendix B.1 for precise definitions. Finally, we define the entire transformer\nmodel that maps a sequence of tokens to a distribution over tokens: pθ : ∪n≤nmaxVn →∆|V|−1.\npθ ≜OUTPUTθOUTPUT ◦TBθTB ◦EMBEDθTE,θPE\n(6)\nwhere θ = (θTB, θTE, θPE, θOUTPUT) denote all the transformer parameter. In particular, we use\nTFθ(v1, . . . , vn) ≜arg maxv∈V pθ(v|v1, . . . , vn) to denote the deterministic version of the trans-\nformer model. We now define a looped Transformer model that also subsumes a non-looped model.\nDefinition 5.2 ((L⊗T) Looped Transformer). Given the number of loops T ∈N+, parameters θ =\n(θTB, θTE, θPE, θOUTPUT), where θTF = (θ(l)\nMHA, θ(l)\nFF)L−1\nl=0 , we define a (L ⊗T) looped Transformer\nas pθ,T ≜OUTPUTθOUTPUT ◦(TBθTB)T ◦EMBEDθTE,θPE.\n5.2\nGROUP COMPOSITION PROBLEM\nWe consider the problem of composing n elements from a group, and prove that a standard 1-layer\ntransformer looped O(log(n)) times can solve this problem. This is a generalization of the modular\naddition problem and has a long history (see Appendix B.2.2). Recently, Liu et al. (2022) show\nthat transformers with log2 n depth can compute composition over n group elements, regardless of\nwhether the group is solvable or not. However, the construction in Liu et al. (2022) uses different\nattention parameter for each layer. Here, we provide a more parameter efficient construction where\nwe solve this problem by looping a one-layer transformer log(n) times.\nTheorem 5.1. For any finite group G and every n ∈N+, there exists a constant-precision looped\ntransformer TFθ,T computing the composition of n elements from G with a 1-layer transformer\nblock, T = ⌈log2 n⌉loops, G ∪{#} being the vocabulary, d = 3 (⌈log2 |G|⌉+ ⌈log2 n + 1⌉)\nembedding size, dFF = |G|2 + 6⌈log2 |G|⌉hidden dimension in MLP, dATTN = ⌈log2 n⌉hid-\nden attention dimension, and 2 attention heads.\nMore specifically, for any g1, . . . , gn ∈G,\nTFθ(#, g1, . . . , gn) = g1 ◦· · · ◦gn.\nThe above result matches the depth upper bound shown for non-looped models by Liu et al. (2022),\nand is very close to the super constant lower bound shown there. Thus looped models can solve the\nproblem with best known depth.\n11\n\nPublished as a conference paper at ICLR 2025\n5.3\nLOOPED MODELS CAN SIMULATE NON-LOOPED MODELS\nOur second theoretical result shows that a non-looped transformer with repeated layers can be sim-\nulated by a looped transformer with fewer parameters and same depth.\nTheorem 5.2. For any transformer pθ with L layers, d embedding size, dFF hidden dimension for\nMLP, H attention heads with dATTN hidden dimension, at most R distinct transformer layers and\nbounded activation value, there is a looped transformer pθ′,L working on the same vocabulary V plus\na dummy token #, which loops a 1-layer transformer block for L times, with d + R + 2 embedding\nsize, RhFF + O(L) hidden dimension for MLP, RH attention heads with dATTN hidden dimension,\nsuch that for any string v1, . . . , vn ∈V, pθ(v1, . . . , vn) = pθ′,L(#, v1, . . . , vn).\nWe can also use the above theorem to convert the O(log2 n) depth transformer that simulates group\ncomposition into a 1-layer looped transformer, although it is less parameter efficient than the result\nfrom the previous section. Please refer to Appendix B.2.1 for the full proof.\nTheory for p-hop. The experiments on p-hop induction from Section 2.1 surprisingly show that a\nsmall model looped multiple times can solve it very effectively. We establish a theoretical basis for\nthis finding. More specifically, we show that a constant layer transformer with log(p) loops suffices\nto solve p-hop induction problem. This result, in fact, matches the lower bound for layers required\nfor non-looped models proved in Sanford et al. (2024b). The result follows from Theorem 5.2 and\nthe construction for non-looped models from Sanford et al. (2024b) (restated in Theorem B.12).\nCorollary 5.3. p-hop problem (Definition A.1) can be solved by looping a 1-layer transformer\n⌊log2 p⌋+ 2 times, which has O(log n) bits of precision, d = dFF = dATTN = O(1) embedding\nsize, hidden dimension for MLP and attention, and at most 3 attention heads.\n5.4\nLOOPED MODELS CAN SIMULATE COT REASONING\nIn Section 3.4, we discussed how looped models can be viewed as generating multiple latent\nthoughts in each iteration. The following theorem shows that one can use looped transformer with\nL loops to simulate L steps CoT of another transformer with similar sizes.\nTheorem 5.4 (Looped transformer simulates CoT). For any L-layer non-looped transformer TFθ\nwith fixed input length n and the number of CoT steps m, there exists a looped transformer TFθ′\nwith L + O(1) layers, Ω(log(n + m)), more embedding dimension and constantly many more\nattention heads, such that for any input v = (vi)n\ni=1, the output of non-looped transformer after m\nsteps of CoT, TFm\nθ (v), is the same as that of the looped transformer on input x concatenated by m\ndummy tokens with m loops, TFθ′,m(v, #m).\nBelow we sketch the high-level idea behind Theorem 5.4; the full proof can be found in Ap-\npendix B.3.\n• (Masking all-but-one) We maintain a counter t (Lemma B.10) at each position for the current\nnumber of loop and for the ith position, we will only ”update” the embedding if i−n ≥t and reset\nthe embedding to the input to the looped transformer layer otherwise. This can be implemented\nby MLP. So similar to CoT, the first n + i −1 embedding won’t be changed in the ith loop.\n• (Shift by one) We can use attention to obtain the output of the current loop at the previous position\nand use that as the input of the next loop at current position. (Lemma B.9)\n• (Token decoding) We show that we can use MLP with ReLU activation to simulate the encoding\nand decoding process of CoT. (Lemma B.7)\n6\nRELATED WORK\nReasoning is recognized as a core ability for intelligent and robustly model and has thus seen\nsignificant focus over the last few years. The synthetic reasoning problems we consider in this\nwork have all been used in the prior works of Sanford et al. (2024b); Ye et al. (2024); Sanford\net al. (2024a); Nogueira et al. (2021) to theoretically analyze the strengths and limitations of\nTransformers. There is also interest in the representation power of Transformers for computational\n12\n\nPublished as a conference paper at ICLR 2025\nFigure 4: Left. Chain-of-thought reasoning can be viewed as a looped model, where each iteration\nproduces one new thoughts token. The new tokens are highlighted in red. Right. A looped model\ncan instead generate multiple latent thoughts in parallel and, in theory, can simulate CoT reasoning\nmy masking the updates appropriately (see Theorem 5.4)\nproblems (Liu et al., 2022; Strobl et al., 2023) and for chain-of-thought reasoning (Merrill &\nSabharwal, 2023a; Feng et al., 2023; Li et al., 2024). The necessity of model depth for performance\nhas been remarked upon, for small models (Liu et al., 2024) and reasoning (Chen & Zou, 2024; Ye\net al., 2024; Petty et al., 2023). In this work, we make a finer observation that albeit larger depth is\nnecessary, this can be achieved with a limited parameter budget via looping.\nLooping in transformer models has been studied since the works (Dehghani et al., 2018; Lan et al.,\n2020) where they showed the benefits of looping for supervised learning tasks and BERT pretrain-\ning respectively. Looping also appears in (Schwarzschild et al., 2021; Bansal et al., 2022) that\nstudy the extrapolation properties of looping for certain algorithmic tasks. More recently Gian-\nnou et al. (2023); de Luca & Fountoulakis (2024) have studied the theoretical properties of looped\ndecoder models and show that looping can simulate arbitrary Turing machines. In addition Yang\net al. (2023); Gao et al. (2024); Gatmiry et al. (2024a;b) study looped models as a way to sim-\nulate iterative algorithms for in-context learning. Recently, Mohtashami et al. (2023) introduced\nCoTFormer, which tries to improve the perplexity of looped language models and (Liu et al., 2024)\nexplore latency-efficiency parameter sharing for on-device LLMs. In contrast, our work focuses on\nthe surprising inductive bias of looping to improve downstream reasoning tasks, and goes beyond\nalgorithmic and in-context learning tasks.\nDifferent training algorithms (e.g. gradient descent (Soudry et al., 2018)) and architectural choices\n(e.g. attention (Edelman et al., 2022)) have been shown to have certain implicit biases. There\nis increasing interest in such inductive biases during pretraining (Saunshi et al., 2022; Liu et al.,\n2023). More recently, Saunshi et al. (2024) showed an inductive bias of stacking (Reddi et al.,\n2023) towards improving reasoning and hypothesize that a connection of stacking to looped models\ncould be responsible for this. Our results provide further verification for this hypothesis.\n7\nCONCLUSIONS, LIMITATIONS AND FUTURE WORK\nThis work explores a new direction of “looped models for reasoning”. Not only are looped models\nable to solve many reasoning problems with very fewer parameters, they also have an inductive\nbias towards disproportionately improving the reasoning performance of language models, over\nmemorization. The theoretical results on the expressivity of looped models start to provide some\nhints into their depth-optimality. While we test looped models on a subset of reasoning problems,\na natural question is whether the results hold for many other forms of reasoning (e.g. multimodal\nand common-sense reasoning). In particular, a succinct formalization of reasoning problems itself\nis an interesting future direction.\nFurthermore, the inductive bias towards improved reasoning\nperformance at the same perplexity is very intriguing and deserves further exploration. We find the\nscaling behavior of looped models very fascinating, and the connections to latent thoughts and CoT\n13\n\nPublished as a conference paper at ICLR 2025\nreasoning start to provide hints into this behavior. We hope this inspires future exploration on using\nlooped models for more efficient inference-time scaling to aid with deeper reasoning.\nREFERENCES\nZeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.3, knowledge capacity\nscaling laws. arXiv preprint arXiv:2404.05405, 2024.\nShaojie Bai, J Zico Kolter, and Vladlen Koltun. Deep equilibrium models. Advances in neural\ninformation processing systems, 2019.\nArpit Bansal, Avi Schwarzschild, Eitan Borgnia, Zeyad Emam, Furong Huang, Micah Goldblum,\nand Tom Goldstein. End-to-end algorithm synthesis with recurrent networks: Extrapolation with-\nout overthinking. Advances in Neural Information Processing Systems, 2022.\nDavid A. Barrington. Bounded-width polynomial-size branching programs recognize exactly those\nlanguages in nc. pp. 1–5, 1986.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 2020.\nXingwu Chen and Difan Zou. What can transformer learn with varying depth? case studies on\nsequence learning tasks. arXiv preprint arXiv:2404.01601, 2024.\nHanseul Cho, Jaeyoung Cha, Pranjal Awasthi, Srinadh Bhojanapalli, Anupam Gupta, and Chulhee\nYun. Position coupling: Leveraging task structure for improved length generalization of trans-\nformers. arXiv preprint arXiv:2405.20671, 2024.\nEunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke\nZettlemoyer. QuAC: Question answering in context. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing, 2018.\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev,\nand Jennimaria Palomaki. TyDi QA: A benchmark for information-seeking question answering in\ntypologically diverse languages. Transactions of the Association for Computational Linguistics,\n2020.\nArtur Back de Luca and Kimon Fountoulakis. Simulation of graph algorithms with looped trans-\nformers. arXiv preprint arXiv:2402.01107, 2024.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal\ntransformers. In International Conference on Learning Representations, 2018.\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner.\nDROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In\nProc. of NAACL, 2019.\nBenjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and variable\ncreation in self-attention mechanisms. In International Conference on Machine Learning, pp.\n5793–5831. PMLR, 2022.\nNelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann,\nAmanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al. A mathematical framework for\ntransformer circuits. Transformer Circuits Thread, 1:1, 2021.\nGuhao Feng, Yuntian Gu, Bohang Zhang, Haotian Ye, Di He, and Liwei Wang. Towards revealing\nthe mystery behind chain of thought: a theoretical perspective. arXiv preprint arXiv:2305.15408,\n2023.\nGuhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, and Liwei Wang. Towards revealing\nthe mystery behind chain of thought: a theoretical perspective. Advances in Neural Information\nProcessing Systems, 36, 2024.\n14\n\nPublished as a conference paper at ICLR 2025\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\nPhang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text\nfor language modeling. arXiv preprint arXiv:2101.00027, 2020.\nYihang Gao, Chuanyang Zheng, Enze Xie, Han Shi, Tianyang Hu, Yu Li, Michael K Ng, Zhenguo\nLi, and Zhaoqiang Liu. On the expressive power of a variant of the looped transformer. arXiv\npreprint arXiv:2402.13572, 2024.\nKhashayar Gatmiry, Nikunj Saunshi, Sashank J Reddi, Stefanie Jegelka, and Sanjiv Kumar. Can\nlooped transformers learn to implement multi-step gradient descent for in-context learning? In\nForty-first International Conference on Machine Learning, 2024a.\nKhashayar Gatmiry, Nikunj Saunshi, Sashank J Reddi, Stefanie Jegelka, and Sanjiv Kumar. On\nthe role of depth and looping for in-context learning with task diversity.\narXiv preprint\narXiv:2410.21698, 2024b.\nAngeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D Lee, and Dimitris\nPapailiopoulos. Looped transformers as programmable computers. In International Conference\non Machine Learning. PMLR, 2023.\nLinyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, and Tieyan Liu. Efficient training of\nBERT by progressively stacking. In Proceedings of the 36th International Conference on Machine\nLearning, 2019.\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\nShirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms\nvia reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Train-\ning compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics (Volume 1: Long Papers), 2017.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language\nmodels. arXiv preprint arXiv:2001.08361, 2020.\nRik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi.\nMawps: A math word problem repository. In Proceedings of the 2016 conference of the north\namerican chapter of the association for computational linguistics: human language technologies,\n2016.\nKenneth Krohn and John Rhodes. Algebraic theory of machines. i. prime decomposition theorem\nfor finite semigroups and machines. Transactions of the American Mathematical Society, 116,\n1965.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion\nJones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav\nPetrov. Natural questions: A benchmark for question answering research. Transactions of the\nAssociation for Computational Linguistics, 2019.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Sori-\ncut. Albert: A lite bert for self-supervised learning of language representations. In International\nConference on Learning Representations, 2020.\nNayoung Lee, Kartik Sreenivasan, Jason D. Lee, Kangwook Lee, and Dimitris Papailiopoulos.\nTeaching arithmetic to small transformers. In The Twelfth International Conference on Learn-\ning Representations, 2024.\n15\n\nPublished as a conference paper at ICLR 2025\nZhiyuan Li, Hong Liu, Denny Zhou, and Tengyu Ma. Chain of thought empowers transformers to\nsolve inherently serial problems. In The Twelfth International Conference on Learning Represen-\ntations, 2024. URL https://openreview.net/forum?id=3EWTEy9MTM.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language\nmodels. Transactions on Machine Learning Research, 2023.\nBingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Transformers\nlearn shortcuts to automata. arXiv preprint arXiv:2210.10749, 2022.\nHong Liu, Sang Michael Xie, Zhiyuan Li, and Tengyu Ma. Same pre-training loss, better down-\nstream: Implicit bias matters for language models.\nIn International Conference on Machine\nLearning. PMLR, 2023.\nZechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang\nXiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, et al. Mobilellm: Optimizing\nsub-billion parameter language models for on-device use cases. arXiv preprint arXiv:2402.14905,\n2024.\nWilliam Merrill and Ashish Sabharwal.\nThe expresssive power of transformers with chain of\nthought. arXiv preprint arXiv:2310.07923, 2023a.\nWilliam Merrill and Ashish Sabharwal. The parallelism tradeoff: Limitations of log-precision trans-\nformers. Transactions of the Association for Computational Linguistics, 11:531–545, 2023b.\nShen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating and developing\nenglish math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pp. 975–984, 2020.\nAmirkeivan Mohtashami, Matteo Pagliardini, and Martin Jaggi. Cotformer: More tokens with at-\ntention make up for less depth. arXiv preprint arXiv:2310.10845, 2023.\nRodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. Investigating the limitations of transformers with\nsimple arithmetic tasks. arXiv preprint arXiv:2102.13019, 2021.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin,\nDavid Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al.\nShow\nyour work: Scratchpads for intermediate computation with language models.\narXiv preprint\narXiv:2112.00114, 2021.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve simple\nmath word problems? In Proceedings of the 2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies. Association for\nComputational Linguistics, 2021.\nJackson Petty, Sjoerd van Steenkiste, Ishita Dasgupta, Fei Sha, Dan Garrette, and Tal Linzen.\nThe impact of depth and width on transformer language model generalization. arXiv preprint\narXiv:2310.19956, 2023.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 2019.\nPranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions\nfor SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), Melbourne, Australia, 2018.\nSashank Reddi, Sobhan Miryoosefi, Stefani Karp, Shankar Krishnan, Satyen Kale, Seungyeon Kim,\nand Sanjiv Kumar. Efficient training of language models using few-shot learning. In Proceedings\nof the 40th International Conference on Machine Learning, 2023.\nSiva Reddy, Danqi Chen, and Christopher D. Manning. CoQA: A conversational question answering\nchallenge. Transactions of the Association for Computational Linguistics, 2019.\n16\n\nPublished as a conference paper at ICLR 2025\nClayton Sanford, Bahare Fatemi, Ethan Hall, Anton Tsitsulin, Mehran Kazemi, Jonathan Halcrow,\nBryan Perozzi, and Vahab Mirrokni. Understanding transformer reasoning capabilities via graph\nalgorithms. arXiv preprint arXiv:2405.18512, 2024a.\nClayton Sanford, Daniel Hsu, and Matus Telgarsky. Transformers, parallel computation, and loga-\nrithmic depth. arXiv preprint arXiv:2402.09268, 2024b.\nNikunj Saunshi, Jordan Ash, Surbhi Goel, Dipendra Misra, Cyril Zhang, Sanjeev Arora, Sham\nKakade, and Akshay Krishnamurthy. Understanding contrastive learning requires incorporating\ninductive biases. In Proceedings of the 39th International Conference on Machine Learning,\n2022.\nNikunj Saunshi, Stefani Karp, Shankar Krishnan, Sobhan Miryoosefi, Sashank J. Reddi, and San-\njiv Kumar.\nOn the inductive bias of stacking towards improving reasoning.\narXiv preprint\narXiv:2409.19044, 2024.\nAvi Schwarzschild, Eitan Borgnia, Arjun Gupta, Furong Huang, Uzi Vishkin, Micah Goldblum,\nand Tom Goldstein. Can you learn an algorithm? generalizing from easy to hard problems with\nrecurrent networks. Advances in Neural Information Processing Systems, 2021.\nNoam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost.\nIn International Conference on Machine Learning, pp. 4596–4604. PMLR, 2018.\nDaniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The im-\nplicit bias of gradient descent on separable data. Journal of Machine Learning Research, 2018.\nLena Strobl, William Merrill, Gail Weiss, David Chiang, and Dana Angluin. Transformers as rec-\nognizers of formal languages: A survey on expressivity. arXiv preprint arXiv:2311.00208, 2023.\nJiankai Sun, Chuanyang Zheng, Enze Xie, Zhengying Liu, Ruihang Chu, Jianing Qiu, Jiaqi Xu,\nMingyu Ding, Hongyang Li, Mengzhe Geng, et al. A survey of reasoning with foundation models.\narXiv preprint arXiv:2312.11562, 2023.\nAlon Talmor and Jonathan Berant. The web as a knowledge-base for answering complex questions.\nIn Proceedings of the 2018 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 1 (Long Papers), 2018.\nYi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won\nChung, Dara Bahri, Tal Schuster, Steven Zheng, et al.\nUl2: Unifying language learning\nparadigms. In The Eleventh International Conference on Learning Representations, 2022.\nGemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,\nRadu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly\ncapable multimodal models. arXiv preprint arXiv:2312.11805, 2023.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yo-\ngatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language\nmodels. arXiv preprint arXiv:2206.07682, 2022a.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou.\nChain of thought prompting elicits reasoning in large language models.\nAdvances in Neural\nInformation Processing Systems, 2022b.\nLiu Yang, Kangwook Lee, Robert Nowak, and Dimitris Papailiopoulos. Looped transformers are\nbetter at learning learning algorithms. arXiv preprint arXiv:2311.12424, 2023.\nTian Ye, Zicheng Xu, Yuanzhi Li, and Zeyuan Allen-Zhu. Physics of language models: Part 2.1,\ngrade-school math and the hidden reasoning process. arXiv preprint arXiv:2407.20311, 2024.\n17\n\nPublished as a conference paper at ICLR 2025\n(a) Attn:Q\n(b) FFN:W1\n(c) Attn:PostNorm\nFigure 5: Cosine similarities for different layers in the model trained with the regularization strength\nλreg = 10 for block size k = 4 (see Section 4 for details). The 24 layer model will have 6 such\nblocks of size 4. The heatmap above shows the cosine similarities between weights for all pairs\nof blocks. Overall we find the final cosine similarity to be very high, thus suggesting a strong\nconnection to looped models.\n(a) Attn:Q\n(b) FFN:W1\n(c) Attn:PostNorm\nFigure 6: Cosine similarities for different layers in the baseline model trained without any regular-\nization strength. Overall the cosine similarities are very low for large matrices, as expected for high\ndimensions matrices.\nA\nEXPERIMENTS\nA.1\nSIMPLE REASONING SETUP DETAILS\nn-ary addition.\nAll experiments are run on a standard Transformer architecture with input di-\nmension of 256, 8 heads and 1024 hidden dimension in the feed-forward layers. We train using\nAdafactor (Shazeer & Stern, 2018) employing a linear warmup coupled with a cosine decay sched-\nule for the learning rate. All runs use a batch size of 1024, learning rate of 0.005 and run for 200k\nsteps. This corresponds to 200M examples, which is insignificant compared to the total possible ex-\namples (> 10320). Thus memorization of the answer is not an issue. Since training is a bit noisy, for\neach setting, we run 3 different random seeds and pick the run with the maximum average accuracy.\nWe pick maximum instead of average because we care about expressivity power of these models.\np-hop induction.\nWe formally define the p-hop problem below:\nDefinition A.1 (p-hop, Sanford et al. (2024b)). For a finite alphabet Σ, define the map hopp : Σn →\nΣ ∪{⊥} as hopp(v) = vfindp(v,n) if findp(v, n) ̸= 0 else ⊥, where\nfind1(v, i) = max ({0} ∪{j ≤i, vj−1 = vi})\nfindp(v, i) = find1(v, findp−1(v, i)) for p ≥2.\nFor the p-hop problem we sample instances randomly while enforcing that there always exists p-\nhops present in the input sequence. We do this by first picking the sequence of p-hops randomly and\n18\n\nPublished as a conference paper at ICLR 2025\nFigure 7: Downstream evaluation for various task groups on the x-axis, vs validation log perplexity\non the y-axis (reversed), as training proceeds. The top plots compare a 8-layer baseline model\n(8 ⊗1) and the looped model (8 ⊗3). Similarly to Figure 2, for closed book QA (memorization)\ntasks looping has very similar trends to baseline. For open book QA tasks and math word problems,\nlooping has much better downstream performance at an equivalent log perplexity.\nthen shuffling them around in a sequence with filler tokens to be filled by the remaining characters.\nAfter the shuffle, we sample the remaining characters to occur in place of the filler tokens while\nrespecting the p-hop order. Our train set consists of 4M examples and our test and validation sets\nconsist of 262k examples each. For all models we train on this dataset, the model dimension is 128,\nhidden dimension is 512 and 8 attention heads are used. Rotary positional encodings are used as\nwell. We train using Adafactor for 200,000 steps with a batch size of 256 using a base learning rate\nof 10−3 and use a linear warmup coupled with a cosine decay schedule for the learning rate.\ni-GSM.\nWe describe how the i-GSM dataset is generated in more detail here. We start with a\nhierarchy of entities of depth 4 from which we build a randomly sampled structure graph where\ndirected edges connect entities in level i to those in level i + 1. Each edge in the structure graph\ndefines a instance parameter which is an integer (for e.g. an edge between city center and car parks\ndenotes the number of car parks in city center). We then construct a randomly sampled mathematical\ndependency graph which is a DAG over all the instance parameters by relating each to the others.\nFinally we pick one of the nodes of the dependency graph to query and the goal is to compute the\nnumerical value of this node modulo some prime number P. For more details on the sampling\nprocess for the structure and dependency graphs, we refer the reader to Ye et al. (2024). We make\n3 simplifications compared to Ye et al. (2024): we phrase our problems in a symbolic language\nwithout the English construction of sentences (see Table 2); we do not allow abstract parameters in\nour problems; we perform arithmetic modulo 7 as opposed to 23. Our train dataset consists of around\n4 million examples and we test on around 50k examples. Given the significantly larger number of\nunique solution templates possible, train-test overlap in the problem template space is going to be\nlimited with high probability. For all models we train on this dataset, the model dimension is 128,\nhidden dimension is 512 and 8 attention heads are used. Rotary positional encodings are used as\nwell. We train using Adafactor for 200,000 steps with a batch size of 256 using a base learning rate\nof 10−3 and use a linear warmup coupled with a cosine decay schedule for the learning rate.\nA.2\nLANGUAGE MODELING SETUP\nWe train on the Pile data using causal language modeling objective. The dataset is pre-processed\nand cached to ensure that all models are trained on exactly the same tokens in the same order. For\nall experiments, we use a batch size of 512 and sequence length of 1280. We use a cosine learning\nrate schedule decaying over 400k steps with a peak learning rate of 0.01, tuned based on the baseline\nmodel. The base model is a 1.5B parameter decoder only Transformer model, with 24 layers, model\ndimensions of 2048, hidden dimension 5120 and 32 heads. For the shallower baselines and looped\nmodels, we only change the number of layers and keep all other hyperparameters the same.\n19\n\nPublished as a conference paper at ICLR 2025\nTable 5: Downstream evaluations for the models in Table 3 for closed book QA tasks.\nParams / FLOPs\nTriviaQA\nTydiQA-\nNatural\nWeb\nAverage\nNoContext\nQuestions\nQuestions\nAverage\nBaseline\n24x / 24x\n24.5\n10.6\n4.3\n5.6\n11.2\nBase (12 ⊗1)\n12x / 12x\n15.9\n9.3\n2.4\n5.2\n8.2\nLoop (12 ⊗2)\n12x / 24x\n18.6\n9.6\n3.4\n5.8\n9.3\nRegularized (k = 12)\n24x / 24x\n20.9\n10.9\n3.6\n5.0\n10.1\nBase (8 ⊗1)\n8x / 8x\n12.3\n7.4\n1.8\n3.9\n6.3\nLoop (8 ⊗3)\n8x / 24x\n17.3\n8.8\n2.5\n5.2\n8.5\nRegularized (k = 8)\n24x / 24x\n23.6\n10.6\n4.1\n6.7\n11.3\nBase (6 ⊗1)\n6x / 6x\n7.4\n4.8\n1.1\n2.7\n4.0\nLoop (6 ⊗4)\n6x / 24x\n16.0\n9.3\n2.7\n4.9\n8.2\nRegularized (k = 6)\n24x / 24x\n25.8\n12.2\n4.5\n5.6\n12.0\nBase (4 ⊗1)\n4x / 4x\n3.3\n1.9\n0.6\n1.5\n1.8\nLoop (4 ⊗6)\n4x / 24x\n11.8\n8.8\n2.4\n3.5\n6.7\nRegularized (k = 4)\n24x / 24x\n26.1\n13.3\n4.5\n6.2\n12.5\nTable 6: Downstream evaluations for the models in Table 3 for open book QA tasks.\nParams / FLOPs\nTydiQA-NoContext\nSquadV2\nDROP\nQuAC\nCoQA\nAverage\nBaseline\n24x / 24x\n33.4\n41.3\n23.4\n18.6\n52.7\n33.9\nBase (12 ⊗1)\n12x / 12x\n21.8\n34.6\n20.0\n17.3\n41.1\n26.9\nLoop (12 ⊗2)\n12x / 24x\n27.7\n39.5\n22.3\n17.6\n46.8\n30.8\nRegularized (k = 12)\n24x / 24x\n33.0\n44.3\n23.8\n17.7\n51.9\n34.1\nBase (8 ⊗1)\n8x / 8x\n12.7\n33.8\n16.0\n15.3\n35.9\n22.7\nLoop (8 ⊗3)\n8x / 24x\n29.8\n38.1\n21.6\n17.6\n46.6\n30.8\nRegularized (k = 8)\n24x / 24x\n33.0\n41.7\n25.2\n19.3\n52.9\n34.4\nBase (6 ⊗1)\n6x / 6x\n8.2\n26.8\n14.8\n14.4\n32.5\n19.3\nLoop (6 ⊗4)\n6x / 24x\n26.6\n34.6\n20.8\n18.1\n43.7\n28.7\nRegularized (k = 6)\n24x / 24x\n34.5\n46.1\n26.2\n18.6\n53.4\n35.8\nBase (4 ⊗1)\n4x / 4x\n3.4\n19.0\n11.1\n13.1\n22.3\n13.8\nLoop (4 ⊗6)\n4x / 24x\n22.0\n32.4\n20.1\n15.8\n40.7\n26.2\nRegularized (k = 4)\n24x / 24x\n33.6\n49.4\n24.1\n19.8\n54.0\n36.2\nTable 7: Downstream evaluations for the models in Table 3 for math word problems.\nParams / FLOPs\nASDiv\nMAWPS-\nMAWPS-\nMAWPS-\nMAWPS-\nSVAMP\nAverage\nAddSub\nMultiArith\nSingleEq\nSingleOp\nBaseline\n24x / 24x\n26.9\n49.9\n2.7\n38.6\n40.2\n17.9\n29.3\nBase (12 ⊗1)\n12x / 12x\n23.1\n44.8\n2.0\n36.8\n37.9\n15.3\n26.7\nLoop (12 ⊗2)\n12x / 24x\n31.5\n55.9\n2.0\n47.4\n50.2\n18.5\n34.3\nRegularized (k = 12)\n24x / 24x\n27.4\n54.4\n2.7\n43.7\n45.9\n19.8\n32.3\nBase (8 ⊗1)\n8x / 8x\n12.9\n32.7\n2.0\n19.9\n21.9\n13.4\n17.1\nLoop (8 ⊗3)\n8x / 24x\n23.2\n54.9\n2.3\n36.2\n38.3\n15.6\n28.4\nRegularized (k = 8)\n24x / 24x\n31.0\n56.7\n3.5\n40.7\n47.5\n17.3\n32.8\nBase (6 ⊗1)\n6x / 6x\n15.4\n31.1\n1.3\n21.3\n26.3\n10.9\n17.7\nLoop (6 ⊗4)\n6x / 24x\n24.5\n51.6\n0.7\n38.0\n47.7\n16.3\n29.8\nRegularized (k = 6)\n24x / 24x\n32.0\n47.1\n3.5\n44.9\n42.0\n16.8\n31.0\nBase (4 ⊗1)\n4x / 4x\n7.9\n10.4\n1.5\n11.0\n19.0\n8.3\n9.7\nLoop (4 ⊗6)\n4x / 24x\n19.9\n49.1\n1.7\n29.3\n35.9\n12.6\n24.8\nRegularized (k = 4)\n24x / 24x\n35.0\n53.9\n3.2\n52.2\n50.9\n23.0\n36.4\nA.3\nRESULTS FOR EACH TASK GROUP\nIn Section 3 we discussed results for various task groups like closed book QA, math word problems\netc. Here we present results for each individual task for completeness. Detailed results for closed\nbook QA are in Table 5, open book QA in Table 6, math word problems in Table 7 and reasoning\nprimitives in Table 8. These tables include, both, looped models from Table 3 and the models trained\nwith regularization from Table 4.\n20\n\nPublished as a conference paper at ICLR 2025\nTable 8: Downstream evaluations for the models in Table 3 for reasoning primitives.\nParams / FLOPs\nCode\nMath\nCode\nMath\nAverage\nDepth-0\nDepth-0\nDepth-1\nDepth-1\nBaseline\n24x / 24x\n71.1\n72.5\n24.2\n22.3\n47.5\nBase (12 ⊗1)\n12x / 12x\n52.2\n51.6\n20.7\n18.3\n35.7\nLoop (12 ⊗2)\n12x / 24x\n73.5\n86.5\n21.3\n23.6\n51.2\nRegularized (k = 12)\n24x / 24x\n75.1\n74.9\n27.0\n25.7\n50.7\nBase (8 ⊗1)\n8x / 8x\n48.7\n42.0\n21.4\n19.8\n33.0\nLoop (8 ⊗3)\n8x / 24x\n88.4\n86.9\n23.1\n22.9\n55.3\nRegularized (k = 8)\n24x / 24x\n92.2\n86.7\n23.1\n23.3\n56.3\nBase (6 ⊗1)\n6x / 6x\n26.3\n29.7\n20.2\n20.1\n24.1\nLoop (6 ⊗4)\n6x / 24x\n90.6\n88.1\n24.1\n21.7\n56.1\nRegularized (k = 6)\n24x / 24x\n84.0\n79.7\n31.4\n28.3\n55.8\nBase (4 ⊗1)\n4x / 4x\n19.3\n23.3\n17.3\n17.6\n19.4\nLoop (4 ⊗6)\n4x / 24x\n87.9\n90.0\n24.8\n24.8\n56.9\nRegularized (k = 4)\n24x / 24x\n88.0\n86.9\n27.9\n25.8\n57.2\nB\nTHEORETICAL RESULTS\nB.1\nDETAILED NOTATIONS\nDefinition B.1 (Embedding Layer). Given a finite vocabulary V, embedding dimension d ∈N+,\ntoken embedding parameter θTE ∈Rd×|V| and position embedding parameter θPE ∈Rd×nmax, we\ndefine the embedding layer as a sequence-to-sequence map, denoted by EMBEDθTE,θPE : Vn →\n(Rd)n for any 1 ≤n ≤nmax, where\nEMBEDθTE,θPE(v1, . . . , vn) = (θTE(v1) + θPE(1), . . . , θTE(vn) + θPE(n)) .\n(7)\nMulti-Head\nSelf-Attention\nMechanism:\nGiven\nattention\nparameters\nθATTN\n=\n{WQ, WK, WV , WO}, where each W m\nQ , W m\nK , W m\nV , W m\nO\n∈\nRdATTN×d, we define the Self-\nAttention layer with a causal mask for a decoder-only transformer in Algorithm 1.\nWe also\ndefine a Multi-Head Attention layer as a collection of self-attention layer with non-shared pa-\nrameters θMHA = {θ(h)\nATTN}H\nh=1, and its output is the sum of the outputs from each head. That is,\nMHAθMHA = PH\nh=1 ATTNθ(h)\nATTN.2\nAlgorithm 1 Causal Self-Attention, ATTN\nInput: Parameter θATTN = (WQ, WK, WV , WO), Input embedding x1, . . . , xn) ∈\n\u0000Rd\u0001n.\nOutput: Output embedding x′ = (x′\n1, . . . , x′\nn) ≜ATTNθATTN(x1, . . . , xn).\n1: qi ≜WQxi, ki ≜WKxi, vi ≜WV xi, ∀i ∈[n]\n2: si ≜softmax(⟨qi, k1⟩, . . . , ⟨qi, ki⟩)∥(0, . . . , 0).\n3: h′\ni ≜W ⊤\nO\nPn\nj=1(si)jvj.\nFeed-Forward Network:\nGiven the parameters of the fully-connected feedforward network layer\nθFF = (W1, b1, W2, b2) ∈RxFF×d × RdFF × Rd×dFF × RdFF, we define the feedforward layer FFθFF :\nRd →Rd as FFθFF(h) ≜W2, relu(W1h + b1) + b2.\nDefinition B.2 (Output Layer). Given parameter θOUTPUT ∈Rd×|V|, we denote the output layer as\nOUTPUTθOUTPUT : (Rd)n →∆|V|−1, where\nOUTPUTθOUTPUT(x1, . . . , xn) ≜softmax(x⊤\nn θOUTPUT)\n(8)\nFinally, we define the entire transformer model pθ : ∪n≤nmaxVn →∆|V|−1 as\npθ ≜OUTPUTθOUTPUT ◦TBθTB ◦EMBEDθTE,θPE\n(9)\n2Though in this paper we focus on attention with casual mask, our definition of looped transformer gener-\nalizes to the cases with other attention masks.\n21\n\nPublished as a conference paper at ICLR 2025\nfor any θ = (θTB, θTE, θPE, θOUTPUT).For convenience, we also write [pθ(v1, . . . , vn)] (v) as pθ(v |\nv1, . . . , vn). In particular, we use TFθ(v1, . . . , vn) ≜arg maxv∈V pθ(v|v1, . . . , vn) to denote the\ndeterministic version of the transformer model.\nFinite-precision Modeling:\nIn this paper we assume the transformer is of finite precision. More\nspecifically, we follow the setting in Li et al. (2024) and use the shorthand Fs ≜{c · k · 2−s |\nc ∈{−1, 1}, 0 ≤k ≤22s −1, k ∈N} to denote fixed-point numbers of constant precision s and\nrounding operation [·]s : R →Fs to denote the correcting rounding, namely the mapping from R\nto the closest representable number in Fs. (We break the tie by picking the number with smaller\nabsolute value). We assume that (1). all the parameters of the transformer are in Fs and (2). correct\nrounding is performed after every binary operation in the forward pass of the transformer. We will\nrefer the readers to Li et al. (2024) for detailed discussion on such finite-precision modeling and\nonly list important notations and lemmas that will be used in this paper below.\nWe use 1s to denote all-one vectors of length s. Similarly we define ⟨·, ·⟩s, ×s, and softmaxs. We\nrecall that for any s ∈N+ and integer 0 ≤x ≤2s −1, we use bins(x) ∈{0, 1}s to denote the\nusual binary encoding of integer x using s binary bits in the sense that x = Ps\ni=1 2i(bins(x))i and\nsbins(x) ∈{−1, 1}s to denote the signed binary encoding, which is 2bins(x) −(1, . . . , 1). Finally\nwe define Bs = max Fs = 2s −2−s.\nLemma B.1. [Lemma E.1, (Li et al., 2024)] For any s ∈N+, it holds that [exp(−Bs)]s = 0.\nLemma B.2. [Lemma E.2, (Li et al., 2024)] For any s ∈N+, it holds that [exp(Bs)]s = Bs.\nLemma B.3. [Lemma E.5, (Li et al., 2024)] Unlimited-fanin AND, OR (resp. MAJORITY) :\n{0, 1}n →{0, 1} can be simulated by some 2-layer feedforward ReLU network with constant (resp.\nlog n) bits of precision constant hidden dimension and additional n constant inputs of value 1.\nMathematically, let FF[s(n)] be the set of functions C : {0, 1}n →{0, 1} which can be a two-\nlayer feedforward ReLU network with at most s(n) bits of precision and constant hidden dimension\nFFθ : {0, 1}2n →{0, 1}, FFθ(x′) = W2 ×s relu([W1 ×s x′ + b1]s), where θ = (W2, W1, b1), such\nthat for any x ∈{0, 1}n,\nFFθ(x1, 1, x2, 1, . . . , xn, 1) = C(x).\n(10)\nWe have unlimited-fanin AND, OR ∈FF[1] and MAJORITY ∈FF[log n].\nGiven two vectors x, y of the same length s, we use x⌢y to denote their interleaving, that is,\n(x⌢y)2i−1 = xi, (x⌢y)2i = yi for all i ∈[e].\nLemma B.4. [Lemma E.3, (Li et al., 2024)] For any s ∈N+, let qi = sbins(i)⌢1s and ki = Bs ·\n(sbins(i)⌢(−1s)) for all i ∈[2s−1], it holds that\n\u0002\nexp(⟨qi, kj⟩s)\n\u0003\ns = 1 [i = j] for all i, j ∈[2s−1].\nB.2\nPROOFS\nB.2.1\nLOOPED MODELS CAN SIMULATE NON-LOOPED MODELS\nProof of Theorem 5.2. We start by introduce some more notations. We will proof the theorem for\nany fixed sequence of v = (v1, . . . , vn). We use x(l) = (x(l)\ni )n\ni=1 to denote the intermediate\nembedding of pθ in the lth layer. More specifically, we define\nxl = (id + FFθ(l−1)\nFF\n) ◦(id + MHAθ(l−1)\nMHA ) ◦· · · (id + FFθ(0)\nFF ) ◦(id + MHAθ(0)\nMHA) ◦EMBEDθEMBED(v).\n(11)\nWe also use x(l+0.5) = (x(l+0.5)\ni\n)n\ni=1 to denote the intermediate embedding of pθ in the lth layer\nafter the attention layer.\nxl+0.5 = (id + MHAθ(l−1)\nMHA )(xl).\n(12)\n22\n\nPublished as a conference paper at ICLR 2025\nSimilarly, for the constructed looped transformer pθ,T , we use v′ = (#, v1, . . . , vn) to denote\nits input. For simplicity, we use the convention that # is at position 0. The proof still works\nif # starts at position 1 because we can just transfer the later tokens by 1 position. We define\nx′(l) = (x′(l)\n0 , x′(l)\n1 , . . . , x′(l)\nn ) as the intermediate embedding of pθ in the lth layer and x′(l+0.5) =\n(x′(l+0.5)\n0\n, x′(l+0.5)\n1\n, . . . , x′(l+0.5)\nn\n) as the intermediate embedding of pθ in the lth layer.\nBelow we first state the key properties that our construction will satisfy, which imply the correctness\nof the theorem and then we state our construction of pθ′,T and show the properties are actually\nsatisfied:\n• x′(l)\ni\n= (x(l)\ni , 1R −er(l), l, 1 [i = 0]).\n• x′(l+0.5)\ni\n= (x(l+0.5)\ni\n, 1R −er(l), l, 1 [i = 0]).\n• x(l)\n0\n= x(l+0.5)\n0\n= 0.3\nTo get the last coordinate l, which is a depth counter, we just need to add 1 more hidden dimension\nin MLP.\nNext we show we can use two-layer with L + 2 MLP to get the mapping from ℓ7→1R −er(l). Let\n(θ(i)\nFF, θ(i)\nMHA)R\ni=1 be the parameters of the R distinct layers in θ. We assume in the lth layer, r(l)’s\nparameters are used. This is because er(l) = PL\ni=1 er(i)0.5∗([l −i+1]+ −2[l −i]+ +[l −i−1]+).\nNow we explain how to deactivate the undesired MLP neurons. In other words, our construction of\nθ′\nFF is essentially concatenation of θ(i)\nFF for i ∈[r] in the hidden dimension of FF, with the additional\ncontrol that FFθ′\nFF((x(l)\ni , 1R −er(l), l), 1 [i = 0]) = PR\ni=1 FFθ(i)\nFF (x(l)\ni )1 [r(l) = i, i ̸= 0] at lth layer.\nThis control can be done by subtracting 1 −er(l) + 1 [i = 0] by a constant which is larger than the\nmaximum pre-activation in the hidden layer.\nFinally we explain how to deactivate the undesired attention. We will only use attention to update the\nfirst part of the embedding, which is x(l+0.5)\ni\n. A crucial step here is that we set the token embedding\nof # as 0 We construct keys and queries as follows:\n1. W ′(r′)\nQ (x′(l)\ni\n) = (W (r′)\nQ\nx(l)\ni , 1 −1 [r′ = r(l)] for r′ ∈[R] and i = 0, . . . , n\n2. W ′(r′)\nK (x′(l)\ni\n)\n=\n(W (r′)\nK\nx(l)\ni , −B1 [i = 0]) for r′\n∈\n[R] and i\n=\n0, . . . , n, where\nB\nis some constant larger than the maximum previous inner product in attention,\nmaxl∈[L],i,j\nD\n(WKx(l)\ni , (WQx(l)\ni\nE\n.\n3. W ′(r′)\nO W ′(r′)\nV\n(x′(l)\ni\n) = (W (r′)\nO\nW (r′)\nV\nx(l)\ni , 0, 0, 0).\nThis construction works because only the ‘desired’ attention head r = r(l) will be activated and\nbehave as in the non-looped case, because otherwise all position in that attention head will be com-\npletely attended to position 0 and returns a zero vector. (We can choose B to be large enough and\ndistribution calculated by the attention score is delta distribution) at position 0, which yields a zero\nvector as its value. This completes the proof.\nB.2.2\nGROUP COMPOSITION.\nThe landmark result in automata theory, Krohn-Rhodes Decompotision Theorem (Krohn & Rhodes,\n1965), shows that all semi-automaton with solvable transformation group (which includes compo-\nsition problem of solvable groups) can be simulated by a cascade of permutation-reset automata,\nwhich can be simulated by TC0 circuits. (Liu et al., 2022) further showed that such automaton\nwith solvable transformation group can be continuously simulated by constant-depth transformers.\nHowever, it is also shown (Barrington, 1986) that the composition problem of unsolvable groups are\n3Here we abuse the notation for simplicity of presentation. x(l)\n0\n= x(l+0.5)\n0\nare not defined in the original\nnon-looped transformer. The key point here is that they are 0 vectors throughout the forward pass.\n23\n\nPublished as a conference paper at ICLR 2025\nAlgorithm 2 Group Composition\nInput: Group elements g0, g1, . . . , gn ∈G, where g0 = e.\nOutput: g0 ◦g1 ◦. . . gn.\n1: g(0)\ni\n= gi, ∀0 ≤i ≤n.\n2: for l = 1 →⌈log2 n⌉do\n3:\na(l)\ni\n= g(l−1)\n[2i−n−1]+, b(l)\ni\n= g(l−1)\n[2i−n]+ ∀0 ≤i ≤n.\n4:\ng(l)\ni\n= a(l)\ni\n◦b(l)\ni , ∀0 ≤i ≤n.\n5: end for\n6: return g(⌈log2 n⌉)\nn\n.\nNC1-complete, for example, the composition of permutation group over 5 elements, S5. Under the\ncommon hardness assumption that NC1 ̸= TC0), constant depth transformer cannot solve composi-\ntion of S5 using a single forward pass (Merrill & Sabharwal, 2023b; Liu et al., 2022; Li et al., 2024).\nBut with CoT, very shallow transformers (depth equal to one or two) can simulate the composition\nproblem of any group(Li et al., 2024; Merrill & Sabharwal, 2023a).\nProof of Theorem 5.1. We will set the token embedding of # the same as that of e, which is the\nidentity of G. In the following proof, we will just treat # as e. We will construct the transformer\nsimulating group composition following the following algorithm Algorithm 2, which gives the high-\nlevel idea of the construction. The correctness of Algorithm 2 follows from the associativity of group\ncomposition. More concretely, we can verify by induction that gl\n0 ◦gl\n1 ◦. . . gl\nn is the same for all\nl = 0, . . . , ⌈log2 n⌉and in the final round, i.e., when l = ⌈log2 n⌉, g(l)\ni\n= e for all i < n.\nBelow we show how to construct a transformer of the given sizes to simulate the above Al-\ngorithm 2.\nWe will embed each g\n∈G as a different vector g\n∈{−1, 1}⌈log2 |G|⌉and\neach position 0 ≤i ≤n as its binary representation in i ∈{−1, 1}⌈log2 n+1⌉, which is\na shorthand for sbins(i) with s = ⌈log2 n + 1⌉.\nWe concatenate them to get {x(0)\ni }n\ni=0,\nthat is, x(0)\ni\n= (gi, i, [2i −n −1]+, [2i −n −1]+, 0⌈log2 |G|⌉, 0⌈log2 |G|⌉). For convenience, we\nwill drop the 0’s in the end (also in the other proofs of the paper) and write it as x(0)\ni\n=\n(gi, i, [2i −n −1]+, [2i −n −1]+). Below we show we can construct 1-layer transformer block\nwith parameter (θMHA, θFF) satisfying that\n1.\nh\nMHAθMHA\n\u0010\n(gi, i, [2i −n −1]+, [2i −n −1]+)n\ni=0\n\u0011i\nk = (0⌈log2 |G|⌉+3⌈log2 n+1⌉, g[2k−n−1]+, g[2k−n]+)\nfor all g0 = e, gi ∈G∀i ∈[n], k = 0, . . . , n;\n2. FFθFF(g, i, j, k, g′, g′′) = (g′ ◦g′′ −g, 03⌈log2 n+1⌉, −g′, −g′′), for all i, j, k\n= 0, . . . , n,\ng, g′, g′′ ∈G.\nThe first claim is because we can use two attention heads to retrieve g[2k−n−1]+ and g[2k−n]+ re-\nspectively, where both of them use k as the key and use −[2k −n −1]+ and −[2k −n]+ as queries\nrespectively. This is possible because all the required information are already in xi. We further make\nattention temperature low enough so the probability returned by attention is a one-hot distribution at\nthe position whose key is equal to the negative query after rounding.\nNow we turn to the second claim about MLP. We will use |G|2 neurons with ReLU activation and\nbias to simulate the product of g′ and g′′. We can index each neuron by (h, h′) for h, h′ ∈G and set\nits incoming weight [W1](h,h′),: = (h, h′) and set bias (b1)(h,h′) = −2⌈log2 |G|⌉+1, which ensures\nthat the activation of neuron (h, h′) will only be 1 when g′ = h, g′′ = h′ and be 0 otherwise. Then\nsetting the outgoing weight of neuron (h, h′) as h ◦h′ and the bias in the second layer to be 0 finishes\nthe construction for simulating the group composition. Finally we use the remaining 6⌈log2 |G|⌉to\nsimulate negative identity mapping x →−x for the remaining 3⌈log2 |G|⌉embedding dimension.\nThis completes the proof.\n24\n\nPublished as a conference paper at ICLR 2025\nB.3\nCONNECTION TO CHAIN-OF-THOUGHT REASONING\nIn this section, we establish a connection betwee looped models and CoT reasoning. We first define\nthe recursion for CoT reasoning as follows:\nTFi\nθ(v1, . . . , vn) ≜TFi−1\nθ\n(v1, . . . , vn, TFθ(v1, . . . , vn)),\nfor i, n ∈N+ satisfying i + n ≤nmax −1 along with the base case of TF1\nθ(v1, . . . , vn) ≜\nTFθ(v1, . . . , vn). For all 0 ≤i ≤nmax −n −1, the output with i steps of CoT is vn+i+1 =\nTFi+1\nθ\n(v1, . . . , vn) = TFθ(v1, . . . , vn, vn+1, . . . , vn+i).\nWe first give the formal statement below.\nTheorem B.5 (Looped transformer simulates CoT). For any L-layer non-looped transformer TFθ,\nthere exists a looped transformer TFθ′ with L + O(1) layers, constantly many more dimensions\nin embedding, MLP and attention layers and constantly many more attention heads, such that for\nany input v = (vi)n\ni=1 and integer m, the output of non-looped transformer after m steps of CoT,\nTFm\nθ (v), is the same as that of the looped transformer on input x concatenated by m dummy tokens\nwith m loops, TFθ′,m(v, #m).\nBelow are some helping lemmas towards showing Theorem B.5 is at least as powerful as CoT.\nLemma B.6 (Simulating arg max using MLP). For every d ∈N and precision s ∈N+, there exists\na 3-layer network with relu activation and d2 width f with s precision, such that for any x ∈Fd\ns, if\nthere is k ∈[d], such that xk > maxj̸=k,j∈[d] xj, f(x) = ek.\nProof of Lemma B.6. Define gi = 2s · relu(2−s −P\nj̸=i relu(xj −xi)) for each i ∈[n]. We claim\nthat if there is k ∈[d], such that xk −maxj̸=k,j∈[d] xj ≥2−s, gi = 1 iff i = k for all i ∈[d].\nFirst gk = 2s · relu(2−s) = 1. Next for i ̸= k, it clearly holds that P\nj̸=i relu(xj −xi) ≥2−s\nand thus gi ≤0. This construction can clearly be implemented by a 3-layer relu network with s\nprecision.\nLemma B.7 (Simulating Decoding and Embedding using MLP). Given any s-precision θTE ∈\nRd×Σ and θOUTPUT, there is a 5-layer network f : Rd →Rd with relu activation and max(|Σ|2)\nwidth with s-precision, such that for all s-precision x ∈Rd which admits unique arg max for\nv ≜arg maxo∈Σ(x⊤θOUTPUT)(o), it holds that\nf(x) = θTE(v).\nProof of Lemma B.7. This is a simple application of Lemma B.6.\nLemma B.8 (Control Gate). A 2-layer relu network with precision s can implement F : Fs × Fs ×\n{0, 1}, F(x, y, M) = Mx + (1 −M)y.\nProof of Lemma B.8. Note that F(x, y, M) = relu(x −2s · (1 −M)) −relu(−x −2s · (1 −M)) +\nrelu(y −2s · M) −relu(−y −2s · M). The proof is completed.\nDefinition B.3 (Transformer Block with Mask). Given number of layers L ∈N+, parameter θTB =\n(θ(l)\nMHA, θ(l)\nFF)L−1\nl=0 , and mask function M : N →{0, 1}, we define the L-layer transformer block with\nmaskTBθTB,M : (Rd)n →(Rd)n for any n ∈N+ as\n[TBθTB,M(x)]i ≜(1 −M(i))xi + M(i)[TBθTB(x)]i\n(13)\nDefinition B.4 (Looped Transformer with Mask). Given number of loops T\n∈\nN+, pa-\nrameters θ\n=\n(θTB, θTE, θPE, θOUTPUT),\nand mask functions {M t}T\nt=1,\nwhere θTF\n=\n(θ(l)\nMHA, θ(l)\nFF)L−1\nl=0 , we define the looped transformer with mask as pθ,T,M ≜OUTPUTθOUTPUT ◦\nTBθTB,M T ◦· · · TBθTB,M 1 ◦EMBEDθTE,θPE and the corresponding deterministic version as\nTFθ,T,M(v1, . . . , vn) ≜arg maxv∈V pθ,T,M(v|v1, . . . , vn).\n25\n\nPublished as a conference paper at ICLR 2025\nDefinition B.5 (Shifting Layer). We define the shifting layer SHIFT : (Rd)n →(Rd)n as the\nfollowing for any d, n ∈N+ and x1, . . . , xn ∈Rd:\nSHIFT(x1, x2, x3, . . . , xn) = (x1, x1, x2, x3, . . . , xn−1).\n(14)\nLemma B.9. For input sequence length up to some integer n, SHIFT could be implemented by\na attention layer by concatenating each embedding xi with (sbins(i), sbins(f(i))), where n =\n⌈log2 n + 1⌉.\nProof of Lemma B.9. It is equivalent to show we can construct an attention heads which computes\nxf(i) at each position i.\nTo do so, we just need to invoke Lemma B.6 and use that to set key and query, so position i attends\nto position f(i). We set value at position i to be xi. This completes the proof.\nLemma B.10. For any positive integer s > 0, there is a constant-depth MLP F with O(s) hidden\nneurons per layer and parameters in Fs, such that for any input bin(x) ∈{−1, +1}s, where 0 ≤\nx ≤2s −1, it holds that\nF(x) = bin(x + 1).\nProof of Lemma B.10. By Lemma B.3, it suffices to show that we can simulate bin(x) 7→bin(x)+1\nusing O(s) wide, constant-depth boolean circuits with AND, OR, NOT gates with unlimited fan-ins.\nThis is immediate by noting that\n[bin(x + 1)]i = [bin(x + 1)]i ⊕\ni−1\n^\nj=1\n[bin(x + 1)]j\n(15)\nLemma B.11. For any positive integer s > 0, there is a constant-depth MLP F with O(s) hidden\nneurons per layer and parameters in Fs, such that for any input (bin(x), bin(y)) ∈{−1, +1}s, where\n0 ≤x, y ≤2s −1, it holds that\nF(x, y) = 1 [x > y] .\nProof of Lemma B.11. By Lemma B.3, it suffices to show that we can simulate bin(x), bin(y) 7→\n1 [x > y] using O(s) wide, constant-depth boolean circuits with AND, OR, NOT gates with unlim-\nited fan-ins. This is immediate by noting that\n1 [x > y] =\n_\ni∈[s]\n\n([bin(x)]i = 1) ∧([bin(y)]i = 0) ∧\n^\n1≤j<i\n([bin(x)]j = [bin(y)]j)\n\n.\n(16)\nProof of Theorem B.5. We consider mask M t(i) = 1 [i −t ≥n], which we call it CoT masking.\nLet s = ⌊log(n + m) + 1⌋, we use i to denote sbins(i) for 1 ≤i ≤n + m for convenience.\nThe embedding size of our newly constructed transformer is larger than the target transformer to be\nsimulated by an additive constant of 3s. We denote the new embedding at position i after tth loop\nby (x(t)\ni , p(t)\ni ), where x(t)\ni\nwill be the original embedding of the transformer to be simulated, and\np(t)\ni\nis of dimension 3s and only depends on i and t. In particular, we can show that we can set\np(t)\ni\n≜(i, [i −2]+ + 1, n + t) to save information about position and the number of loops — p(0)\ni\nis\nfrom the positional embedding and the update is due to Lemma B.10. The size of hidden dimension\nof MLP and attention (key, query, value) will also be increased by O(s).\nThe proof contains two steps:\n1. To show that there is a transformer with CoT masks simulating the target transformer with m\nsteps of CoT and L layers by looping its own L + O(1) layer block m times.\n26\n\nPublished as a conference paper at ICLR 2025\n2. To show that the above looped transformer with CoT masks can be simulated by a standard\nlooped transformer without mask and with constantly many more layers.\nFor the first claim, starting from the same parameter of the transformers with CoT θ, we build a new\nlooped model with parameter θ′ with constantly many more layers in each transformer block and\nat most constantly many more heads per attention layer. First, we can add constantly many more\nlayers to use MLP to simulate the decoding-encoding process using Lemma B.7. Next, we can add\none more transformer layer in each block and use the attention layer to simulate the shifting layer\nby Lemma B.9, since we have the additional position embedding p(\nit). In particular, the embedding\nwe get at position n + t after t loops, x(t)\nn+t, now simulates the token embedding of n + t of the CoT\ntransformer. By the way we define CoT mask M, for every t ≥−n + 1, the embedding ˆx(t′)\nn+t will\nkeep the same for all t′ ≥max(t, 0). In tth loop, the only embedding update that matters happens\nat n + tth position, because no updates happen at earlier positions, and updates at later positions\nn + t′ for some t′ > t will be overwritten eventually in the future loops t′, by some value which\nis independent of their value at the current loop t. In the end, we know the embedding x(T )\ni\nin\nDefinition B.4 is exactly equal to that in CoT transformer, and so does the final output.\nFor the second claim, because CoT mask can be computed by a O(log(n + m)) wide, constant-\ndepth MLP (Lemma B.11), together with Lemma B.8, we know it suffices to increase the number of\nlayers per transformer block and embedding size and hidden dimensions by constant to simulate the\ntransformer with mask by a transformer without mask.\nTheorem B.12 (Restatement of Theorem 4.2, (Sanford et al., 2024b)). p-hop problem (Defini-\ntion A.1) can be solved by ⌊log2 p⌋+ 2-layer non-looped transformer with log n bits of precision, at\nmost 3 different layers, d = dFF = dATTN = O(1) embedding size, hidden dimension for MLP and\nattention, 1 attention head.\n27\n",
  "metadata": {
    "source_path": "papers/arxiv/Reasoning_with_Latent_Thoughts_On_the_Power_of_Looped_Transformers_a3aed7457446fa4c.pdf",
    "content_hash": "a3aed7457446fa4c11c5f6cac89c38725ca42d0a66c44cc1935e2f8b6376882e",
    "arxiv_id": null,
    "title": "Reasoning_with_Latent_Thoughts_On_the_Power_of_Looped_Transformers_a3aed7457446fa4c",
    "author": "",
    "creation_date": "D:20250225030837Z",
    "published": "2025-02-25T03:08:37",
    "pages": 27,
    "size": 2149490,
    "file_mtime": 1740470086.860579
  }
}