{
  "text": "CODESWIFT: Accelerating LLM Inference for Efficient Code Generation\nQianhui Zhao1, Li Zhang1, Fang Liu1‚àó, Xiaoli Lian1‚àó, Qiaoyuanhe Meng1,\nZiqian Jiao1, Zetong Zhou1, Borui Zhang1,Runlin Guo1, Jia Li2\n1School of Computer Science and Engineering,\nState Key Laboratory of Complex & Critical Software Environment,\nBeihang University\n2Key Lab of High Confidence Software Technology (Peking University)\n{zhaoqianhui, fangliu, lianxiaoli}@buaa.edu.cn\nAbstract\nCode generation is a latency-sensitive task that\ndemands high timeliness, but the autoregres-\nsive decoding mechanism of Large Language\nModels (LLMs) leads to poor inference ef-\nficiency.\nExisting LLM inference accelera-\ntion methods mainly focus on standalone func-\ntions using only built-in components. More-\nover, they treat code like natural language se-\nquences, ignoring its unique syntax and seman-\ntic characteristics. As a result, the effective-\nness of these approaches in code generation\ntasks remains limited and fails to align with\nreal-world programming scenarios. To allevi-\nate this issue, we propose CODESWIFT, a sim-\nple yet highly efficient inference acceleration\napproach specifically designed for code gen-\neration, without comprising the quality of the\noutput. CODESWIFT constructs a multi-source\ndatastore, providing access to both general and\nproject-specific knowledge, facilitating the re-\ntrieval of high-quality draft sequences. More-\nover, CODESWIFT reduces retrieval cost by\ncontrolling retrieval timing, and enhances effi-\nciency through parallel retrieval and a context-\nand LLM preference-aware cache. Experimen-\ntal results show that CODESWIFT can reach\nup to 2.53√ó and 2.54√ó speedup compared to\nautoregressive decoding in repository-level and\nstandalone code generation tasks, respectively,\noutperforming state-of-the-art inference accel-\neration approaches by up to 88%. Our code\nand data are available at https://anonymous.\n4open.science/r/CodeSwift.\n1\nIntroduction\nLarge Language Models (LLMs) such as GPT-4o\n(Achiam et al., 2023) and DeepSeek-Coder (Guo\net al., 2024) have demonstrated impressive perfor-\nmance in coding tasks, revolutionizing the land-\nscape of software development (Github, 2021; Li\n*Corresponding authors\net al., 2023). These models excel in code comple-\ntion and generation but face a challenge: the signif-\nicant inference time. LLMs use the autoregressive\ndecoding mechanism, where each new token is\ngenerated conditioned on the previously generated\ntokens and the given context. However, developers\ntypically hold high expectations regarding the re-\nsponsiveness of code recommendations (Liu et al.,\n2024a). If LLMs fail to deliver precise and effi-\ncient feedback, it may directly affect development\nefficiency and user experience.\nTo accelerate the inference process of LLMs,\nspeculative decoding (Chen et al., 2023a; Leviathan\net al., 2023) is regarded as one of the effective so-\nlutions, which employs a draft-verification frame-\nwork to minimize the number of forward steps.\nSpecifically, it utilizes a small language model as\na draft model to rapidly generate candidate output\ntokens, which are then verified for acceptability by\nthe target LLM through a single forward step while\nkeeping the output consistent with that decoded\nautoregressively by the target LLM itself. Based\non the draft-verification paradigm, many inference\nacceleration approaches have emerged (Chen et al.,\n2023b; Zhang et al., 2024; Zhao et al., 2024; Li\net al., 2024b; Miao et al., 2024), most of which rely\non an additional draft model, either selected from\nthe same model family or trained for specific use\ncases. However, identifying a suitable draft model\nremains challenging, as it requires striking a deli-\ncate balance between maintaining a small model\nsize and ensuring high output quality. Additionally,\nthe draft model must align with the vocabulary of\nthe target LLM, further complicating the selection\nprocess. More recently, researchers have explored\nreplacing the parametric draft model with a non-\nparametric retrieval system (He et al., 2024; Yang\net al., 2023), which can easily be ported to any\nLLM without introducing additional training costs\nand have be applied to code generation task.\nWhile some of the above approaches have\narXiv:2502.17139v1  [cs.AI]  24 Feb 2025\n\n(a) A standalone function\n(b) A repository-level function\nFigure 1: Examples of standalone and repository-level\nfunctions. Intra-file and cross-file dependencies are\nhighlighted in green and yellow, respectively.\ndemonstrated promising performance in code gen-\neration task (He et al., 2024; Zhao et al., 2024),\nthey primarily focus on standalone code functions\nthat solely rely on built-in components.\nHow-\never, in real-world software development, it is\ncrucial for developers to be aware of other files\nwithin the repository during programming (Zhang\net al., 2023), which gives rise to repository-level\ncode generation (more details in Appendix A). As\nshown in Figure 1, complex dependencies that span\nmultiple levels can exist in repository-level func-\ntions. Experimental results show that existing in-\nference acceleration approaches typically perform\nworse on repository-level code generation under\nthe same settings than standalone ones. For exam-\nple, Self-speculative decoding (Zhang et al., 2024)\ncan achieve over 1.5√ó acceleration compared to\nautoregressive decoding in standalone code gen-\neration (Figure 5), but falls short when applied to\nrepository-level tasks, offering virtually no speedup\nin comparison to autoregressive inference (Table 1).\nMoreover, existing approaches treat source code as\nsequences similar to natural language, without ac-\ncounting for code‚Äôs unique syntactic and semantic\ncharacteristics. As a result, the effects of existing\nLLM inference acceleration approaches on code\ngeneration tasks may be limited and fail to align\nwith real-world scenarios.\nTo alleviate this issue, in this paper, we primarily\nfocus on improving the inference speed of LLMs\non code generation task, covering both repository-\nlevel and standalone code, without comprising the\nquality of the output. We propose CODESWIFT,\na simple yet highly efficient approach to accel-\nerate the inference of LLMs through an efficient\nand effective retrieval strategy. Concretely, we first\nconstruct a multi-source datastore, providing ac-\ncess to both general and project-specific knowl-\nedge and enhancing the quality of draft sequences.\nThen, CODESWIFT reduces unnecessary retrieval\noverhead by controlling the retrieval timing. Be-\nsides, CODESWIFT improves retrieval efficiency\nthrough parallel retrieval and the maintenance of\na context- and LLM preference-aware cache. Fi-\nnally, tree attention is employed to avoid redun-\ndant computation caused by verifying multiple\ndraft sequences. Experimental results show that\nthe decoding speed of CODESWIFT surpasses ex-\nisting inference acceleration approaches substan-\ntially on both repository-level and standalone code\ngeneration tasks. For repository-level code genera-\ntion, CODESWIFT achieves up to 2.30√ó and 2.53√ó\nspeedup on DevEval (Li et al., 2024a) and RepoE-\nval (Zhang et al., 2023), respectively. CODESWIFT\ncan also achieve up to 2.54√ó acceleration on stan-\ndalone code generation dataset, HumanEval (Chen\net al., 2021). It is worth noting that incorporat-\ning project-specific knowledge enables the genera-\ntion of high-quality drafts, reducing the verification\ntime and, consequently, the inference time of our\nmodel for repository-level code generation. How-\never, this knowledge can be omitted in standalone\ncode generation where such context is unnecessary.\nOur contributions can be summarized as follows:\n‚Ä¢ We identify limitations of current LLM infer-\nence acceleration approaches within the con-\ntext of real-world code generation and provide\ninsights for potential improvements.\n‚Ä¢ We propose CODESWIFT, a simple yet effi-\ncient approach to accelerate LLM inference\nfor code generation by leveraging effective\nretrieval and verification mechanisms.\n‚Ä¢ We conduct a comprehensive evaluation of\nCODESWIFT and results show that it achieves\nstate-of-the-art results in both repository-level\nand standalone code generation tasks.\n2\nRelated Work\nAutoregressive decoding generates tokens sequen-\ntially, leading to slow and costly decoding. To ac-\ncelerate this process, draft-verification approaches\n(Chen et al., 2023a; Miao et al., 2024; He et al.,\n2024) have gained popularity recently as they en-\nhance speed without compromising performance,\n\nwhich fall into generation-based and retrieval-\nbased categories based on their draft generation\ntechniques (more information in Appendix B).\nGeneration-based approaches. Draft tokens can\nbe generated either by a smaller model or by the\ntarget model itself. Speculative decoding (Chen\net al., 2023a; Leviathan et al., 2023) employs a\nsmaller model for drafting and uses the target LLM\nfor efficient parallel verification. Ouroboros (Zhao\net al., 2024) generates draft phrases to enhance par-\nallelism and extend drafts. Alternatively, the target\nLLM itself can be utilized to efficiently draft (Stern\net al., 2018; Li et al., 2024b; Fu et al., 2024), which\nreduces system complexity and selection difficul-\nties. Medusa (Cai et al., 2024) introduces multiple\nheads to predict multiple draft tokens in parallel.\nSelf-speculative decoding (Zhang et al., 2024) em-\nploys the target model with selectively certain in-\ntermediate layers skipped as the draft model.\nRetrieval-based approaches. The retrieval-based\ndraft generation approach replaces the model gener-\nation with a search in a retrieval datastore to obtain\ncandidate sequences. These approaches avoid extra\ntraining and can reduce computational overhead.\nLLMA (Yang et al., 2023) is an inference-with-\nreference decoding mechanism by exploiting the\noverlap between the output and the reference of an\nLLM. REST (He et al., 2024) replaces the para-\nmetric draft model with a non-parametric retrieval\ndatastore.\n3\nPreliminaries\n3.1\nRetrieval-based Speculative Decoding\nBuilding upon the draft-verification framework\nintroduced by speculative decoding (Chen et al.,\n2023a; Leviathan et al., 2023), retrieval-based de-\ncoding acceleration approaches leverage a retrieval\nmechanism to generate draft tokens (He et al., 2024;\nYang et al., 2023), which can eliminate the chal-\nlenge of selecting an appropriate draft model and\navoid additional training costs. A notable example\nis Retrieval-Based Speculative Decoding (REST)\n(He et al., 2024), which has proven to be effec-\ntive in standalone function generation task (Chen\net al., 2021). Below is an explanation of how it\nworks. Pre-built from a code corpus, the datastore\nof D = {(ci, ti)} serves as the source for the draft\ntoken sequence, where ci represents a context and\nti represents the corresponding continuation of ci.\nAs an alternative to the draft model, the objective of\nretrieval is to identify the most likely continuations\nFigure 2: Localness of source code.\n(a)\n(b)\nFigure 3: Heatmaps of (a) retrieval performance and (b)\nwhitespace distribution with token positions in REST.\nThe maximum token index is selected based on the\naverage token number per line (12).\nof the current context from the datastore D using\na suffix match (Manber and Myers, 1993). Specif-\nically, given a context s = (x1, ..., xt), it aims to\nfind contexts in D that match the longest suffix of\ns. Starting from a pre-defined match length upper\nlimit nmax (measured in the number of tokens), for\neach suffix length n, it extracts the suffix of s with\nn tokens, denoted as q, and obtains all contexts ci\nthat match q as a suffix. If at least one context in D\nmatches q, the corresponding context continuation\npairs are returned as the retrieval result S; other-\nwise, the match length n is decreased by one to at-\ntempt matching a shorter suffix. Subsequently, the\ntop k high-frequency prefixes in S are selected as\nthe draft sequences for later verification. Inspired\nby REST, CODESWIFT also incorporates a similar\nsuffix-match-based retrieval algorithm, leveraging\nits advantages in time and memory efficiency.\n3.2\nMotivating Examples\nTo identify the limitations of current inference ac-\nceleration methods in code generation, we present\nmotivating examples that highlight the localness\nof source code and the retrieval performance in\nretrieval-based approaches.\nLocalness of source code. Human-written pro-\ngrams are typically localized (Tu et al., 2014), with\nprogram entities (token sequences) defined or used\nin the preceding snippets frequently being reused\nin the subsequent code snippets within the same\n\ncode file. As shown in Figure 2, user_id_file_path\nis a user-defined variable within the current code\nsegment, which does not exist in the datastore but\nappears multiple times in subsequent code snip-\npets. Additionally, the blue-highlighted statements\ndemonstrate the repetition of token sequences. By\neffectively leveraging these frequently occurring to-\nken sequences within the code file, such as storing\nthem in a cache for subsequent retrieval, the accep-\ntance length for draft validation can be increased,\nthereby enhancing the inference speed.\nRetrieval is not always essential. Current work\nperforms retrieval operation at every position,\nwhich may bring unnecessary cost. To investi-\ngate the relationship between retrieval performance\nand token position in code generation, we ran-\ndomly selected 200 samples from DevEval (Li\net al., 2024a), a repository-level code generation\nbenchmark, and employed DeepSeek-Coder-6.7B\n(Guo et al., 2024) for evaluation. For each token,\nwe recorded whether it was: (a) retrieved from the\ndatastore rather than generated by the model, and\n(b) a whitespace character (e.g., spaces or new-\nline characters). Results are presented as heatmaps\nin Figure 3. As seen from Figure 3(a), retrieval\nfailures are frequent, with a particularly notable\npattern: the second token in each line has the low-\nest probability of being successfully retrieved. A\ncomparison with the whitespace rate heatmap sug-\ngests that this phenomenon may stem from the\nfact that the second token is typically the first non-\nwhitespace character at the beginning of a line. The\nfirst non-whitespace token in each line dictates the\ndirection of the line, making it more variable and\nconsequently more challenging to retrieve. Thus,\nskipping retrieval or reducing the retrieval proba-\nbility at such positions may improve performance.\n4\nMethod\nThe architecture of CODESWIFT is shown in Figure\n4. In this section, we first describe the construction\nof datastore and cache, and then provide a detailed\nexplanation of retrieval and verification process.\n4.1\nMulti-source Datastore Construction\nThe quality of the retrieval datastore, which serves\nas the source of draft sequences, critically deter-\nmines the acceleration potential. A larger data-\nstore may enhance the probability of result ac-\nceptance, but it also correspondingly increases re-\ntrieval time, making the trade-off between the two\ncritically important. To achieve optimal perfor-\nmance with a compact datastore and facilitate effec-\ntive retrieval, CODESWIFT incorporates a smaller\nrepository-related datastore Dr and a larger com-\nmon code datastore Dc to construct a comprehen-\nsive retrieval datastore D. This design supports par-\nallel retrieval, providing access to both general and\nproject-specific knowledge. To enable fast retrieval\nwith minimal overhead, we organize the datastore\ninto context-continuation pairs, facilitating a rapid\nexact-match method for context search.\nRepository-related datastore Dr. During soft-\nware development, developers often reference\ncross-file elements such as classes and methods,\nmaking intra-repository files highly relevant to the\ngenerated code. Additionally, repository-specific\nfactors, including domain variations and coding\nconventions, lead to distinct patterns of idiomatic\nexpressions. For instance, web development repos-\nitories frequently involve HTTP request-response\nhandling, while data science repositories focus on\ndata processing and modeling tasks. To this end,\nwe collect the code files from current repository\n(with the portions to be generated excluded) and\nform repository-related datastore Dr.\nCommon datastore Dc. To ensure that common\nprogramming operations are also retrievable, a sub-\nset of data from commonly used pre-trained code\ndatasets (Kocetkov et al., 2022) is used to form Dc,\nwhich serves as another component of datastore D.\nDatastore organization. For efficient retrieval, the\ndatastore is organized as contexts and the corre-\nsponding continuations following He et al. (2024).\nSpecifically, for each code file utilized in construct-\ning the datastore, the content preceding every posi-\ntion will constitute a context, whereas the content\nsubsequent to that position is the corresponding\ncontinuation. The datastore D of CODESWIFT can\nbe summarized as:\nD = (Dr, Dc)\n(1)\n(Dr, Dc) = ({(ci, ti)}|Dr|\ni=1 , {(cj, tj)}|Dc|\nj=1)\n(2)\nwhere ci (cj) represents the context, ti (tj) rep-\nresents the corresponding continuation of ci (cj),\n|Dr| (|Dc|) is the number of samples in Dr (Dc).\nFor standalone code generation, Dr can be omitted.\n4.2\nContext- and LLM Preference-aware\nCaching\nTo reduce retrieval costs and improve the alignment\nof retrieved results with LLM preferences‚Äîthereby\nincreasing both the accepted sequence length and\n\nDatastore\ncommon\ncode\nùê∑c\nCache\nrepo\ncode\nùê∑ùëü\nRetriever\nLLM\nif _user_id_file_is_old (user_id_file_path\n1\ntiming\nselection\n2\nname = self.                                from ùê∑ùíì\n_id = (                                           from ùê∑ùíÑ\n_id_file\nfrom ùê∑ùëü\n√ó\n√ó\n‚àö\n3\nLLM output sequences\nverified retrieved sequences\nif _user_id_\nfile_is_old(\nuser_id_file_path\nconstruct weighted Trie\nautoregressive\ndecoding\nconstruct \ntree attention\nverify\n4\nupdate\ntiming\nselection\nRetriever\nCache\ncache is available\ncache is not\navailable\nDatastore\nùê∑c\nùê∑ùëü\nparallel \nsearch\nùëπ\nretrieved sequences\nno results\nùëÖùëü\nùëÖùëê\nmissing table\nùë¥\nfind whether ùë† in\nend retrieval\nyes\nno\nùë†ends with ùë°ùëúùëòùëíùëõùë†ùëòùëñùëù?\nno\nyes\nwith probability 1 ‚àíùëù\nwith probability ùëù\nROOT\n_\nid\n_\nfile\n=\n(\nname\n=\nself\n.\ncontext ùíî\nmatched suffix\nFigure 4: Architecture of CODESWIFT. The left part illustrates an overview, and the right part offers a detailed\ndepiction of timing selection and retrieval operation.\ninference speed‚Äîwe design a context- and LLM\npreference-aware caching strategy to cache the ver-\nified retrieved sequences and LLM generated se-\nquences. Specifically, based on the observations in\nSection 3.2, program entities (token sequences) de-\nfined or used in preceding snippets are often reused\nin the subsequent code snippets. Consequently, if\nthe draft sequence r = (y1, ..., yj), retrieved by the\ncontext s = (x1, ..., xt), is verified by the LLM,\nwe concatenate them as (x1, ..., xt, y1, ..., yj) and\nadd it into CACHE. Moreover, since the datastore\nD is static, the draft sequences retrieved for the\nidentical context s remain consistent. However,\ndifferent LLMs exhibit distinct generation prefer-\nences, leading to varied decoding outputs after draft\nverification. Additionally, earlier decoding outputs\nmust maintain contextual relevance and coherence\nwith subsequent outputs. Therefore, we also incor-\nporate the verified decoding output sequence into\nCACHE for future use.\nTo maintain the CACHE, we assess whether the\ntwo aforementioned update conditions are satisfied\nafter each forward step of the LLM. If the number\nof sequences inside the CACHE exceeds the pre-\ndefined threshold l, it is accessible and will remain\nactive throughout the entire inference process.\n4.3\nDynamic and Efficient Retrieval Strategy\nAlgorithm 1 illustrates the complete retrieval pro-\ncess of CODESWIFT. Before each forward step,\ngiven current context s, CODESWIFT initially ver-\nifies the availability of CACHE. If the CACHE is\naccessible, that is, the number of sequences inside\nexceeds l, retrieval is prioritized from CACHE. If\nCACHE is unavailable or fails to yield valid (non-\nempty) results, CODESWIFT utilizes a dynamic and\nefficient retrieval strategy to minimize unnecessary\nAlgorithm 1: Retrieval Algorithm\nInput: current context s, datastore D, retrieval cache\nCACHE, minimum activation size l, missing\ntable M, skip token tokenskip, retrieval\nprobability p\nOutput: Retrieved sequences R\n1 if CACHE.size ‚â•l then\n2\n// retrieval from cache\n3\nR ‚Üêsearch(CACHE)\n4 if CACHE.size < l or R = ‚àÖthen\n5\n// retrieval timing selection\n6\nif s ‚ààM then\n7\npass\n8\nelse if s ends with tokenskip then\n9\nif random number < p then\n10\n// parallel retrieval from datastore\n11\nRr, Rc ‚Üêpar_search(Dr, Dc)\n12\nR ‚Üê(Rr, Rc)\n13 if R = ‚àÖthen\n14\n// update missing table\n15\nM ‚ÜêM ‚à™{s}\n16 else\n17\nupdate CACHE\n18 return R;\nretrieval cost. Specifically, CODESWIFT optimizes\nretrieval timing by addressing two key considera-\ntions as follows.\nSkip token. As mentioned in Section 3.2, the in-\ntrinsic characteristics of code lead to a low retrieval\nsuccess rate at the first non-whitespace character\nof each line. Since obvious patterns are not found\nin other positions, and the introduction of intricate\njudgment processes may incur additional compu-\ntational overhead, we set the first non-whitespace\ncharacter of each line as the skip token. We strategi-\ncally reduce the retrieval probability of skip token\nthrough a control parameter p, which refers to the\nretrieval probability at these positions.\nMissing table. When utilizing the current context\n\ns to retrieve its continuations from datastore D, it\nmay fail to yield any valid results in some cases.\nTo prevent time wastage resulting from invalid re-\ntrieval, we maintain a missing table M = {smi}\nthat stores suffixes smi for which no valid results\ncan be retrieved from the datastore D. Thus, when\nsmi is encountered again during the subsequent in-\nference, CODESWIFT will bypass the retrieval and\ndirectly utilize the LLM to generate the next token.\nIf CODESWIFT decides to proceed with retrieval\naccording to the above strategy, parallel retrieval\nis conducted from repository-related datastore Dr\nand common datastore Dc to further boost the re-\ntrieval efficiency, and the results refer to Rr and\nRc, separately. Specifically, if Rr and Rc are both\nempty, s will be denoted as sm and added into the\nmissing table M. Otherwise, relevant sequences\nare employed to update the CACHE.\n4.4\nDraft Construction and Verification with\nWeighted Prefix Optimization\nThe retrieval results R = (Rr, Rc) contain poten-\ntial continuations of the current context s, often\nsharing the same prefix. To reduce the cost brought\nby verification each ri ‚ààR one by one, we con-\nstruct the draft sequences using a Trie, where the\nunique path from a node to the root node corre-\nsponds to a prefix of the retrieval results, aiming to\nreduce the repeated verification of shared prefixes\nin R. We use following equation to assign a weight\nfor each node:\nNweight = Œ± ¬∑ tr + Œ≤ ¬∑ tc\n(3)\nwhere tr and tc represents the times that the node\noccurs in Rr and Rc respectively, and Œ± and Œ≤\nrefers to the corresponding coefficient. By control-\nling the values of Œ± and Œ≤, the preference of draft\nsequences can be adjusted to accommodate differ-\nent scenarios. We select top-k sequences from the\nTrie, ordered by their weights from highest to low-\nest, as the draft sequences. Subsequently, the draft\nsequences are verified by LLM using tree attention\n(Spector and Re, 2023; Miao et al., 2024). As our\nobjective is to accelerate the inference without com-\npromising model performance, all correct tokens\nfrom the beginning will be accepted, while the draft\ntokens following the first error will be rejected.\n5\nExperiments\n5.1\nExperimental Setup\nDatasets.\nWe conduct experiments on both\nrepository-level and standalone code generation\nbenchmarks. For repository-level code generation,\nwe choose two widely-used benchmarks, DevEval\n(Li et al., 2024a) and RepoEval (Zhang et al., 2023).\nDevEval comprises 1,825 testing samples from 115\nrepositories, covering 10 popular domains. It aligns\nwith real-world repositories in code distributions\nand dependency distributions. RepoEval is con-\nstructed using the high-quality repositories sourced\nfrom GitHub. We use the function-level subset\nfor evaluation, which contains 455 testing samples.\nFor standalone code generation, we conduct experi-\nments on HumanEval (Chen et al., 2021), a widely-\nused standalone code generation dataset including\n164 human-written programming problems.\nBackbone Models. We use the 1.3B and 6.7B\nconfigurations of Deepseek-Coder-base (Guo et al.,\n2024), as well as 7B and 13B configurations of\nCodeLlama-Python (Roziere et al., 2023) for eval-\nuation, which are popular and well-performing\nLLMs in code generation.\nBaselines. We compare CODESWIFT with vanilla\nautoregressive decoding and several state-of-the-\nart inference acceleration approaches that follow\nthe draft-verification framework and have demon-\nstrated effectiveness in code generation, includ-\ning Self-speculative decoding (Zhang et al., 2024),\nOuroboros (Zhao et al., 2024), and REST (He et al.,\n2024). Self-speculative decoding requires several\nhours to identify skipped layers in the target LLM\nfor draft model construction. Ouroboros demands\nmanual selection of a suitable draft model for the\ntarget LLM. REST is draft model-free but suffers\nfrom misalignment between retrieval sequences\nand the LLM output.\nEvaluation Metrics. We report the decoding speed\n(ms/token) and the speedup ratio compared with\nvanilla autoregressive decoding. We also compare\nthe average acceptance length, defined as the av-\nerage number of tokens accepted per forward step\nby the target LLM, which reflects the upper bound\nof achievable acceleration. Since CODESWIFT and\nbaselines do not compromise model performance,\nthe correctness of the generated code is not evalu-\nated.\nImplementation Details. To provide essential con-\ntextual information, we prepend preceding code\nsnippets from the same file as context for DevEval\nand RepoEval. All results are obtained with a maxi-\nmum input length of 2k and a maximum generation\nlength of 512 under greedy decoding. We focus on\ngreedy decoding results as baseline approaches per-\nform optimally with greedy decoding and compara-\n\nTable 1: Decoding speed and speedup ratio on repository-level code generation datasets.\nDataset\nApproach\nDeepseek-Coder-1.3B\nDeepseek-Coder-6.7B\nCodeLlama-7B\nCodeLlama-13B\nms/token\nSpeedup\nms/token\nSpeedup\nms/token\nSpeedup\nms/token\nSpeedup\nDevEval\nAutoregressive\n20.00\n1.00√ó\n26.15\n1.00√ó\n26.29\n1.00√ó\n46.35\n1.00√ó\nSelf-speculative\n18.72\n1.07√ó\n22.55\n1.16√ó\n25.10\n1.05√ó\n42.74\n1.08√ó\nOuroboros\n-\n-\n15.69\n1.67√ó\n29.14\n0.90√ó\n39.73\n1.17√ó\nREST\n12.10\n1.65√ó\n15.28\n1.71√ó\n15.57\n1.69√ó\n43.38\n1.07√ó\nCODESWIFT\n8.71\n2.30√ó\n11.69\n2.24√ó\n12.17\n2.16√ó\n21.56\n2.15√ó\nRepoEval\nAutoregressive\n19.91\n1.00√ó\n25.75\n1.00√ó\n26.21\n1.00√ó\n47.86\n1.00√ó\nSelf-speculative\n19.63\n1.02√ó\n22.48\n1.16√ó\n24.36\n1.08√ó\n42.09\n1.14√ó\nOuroboros\n-\n-\n14.56\n1.77√ó\n33.12\n0.79√ó\n35.60\n1.34√ó\nREST\n12.09\n1.65√ó\n15.46\n1.67√ó\n15.43\n1.70√ó\n44.59\n1.04√ó\nCODESWIFT\n7.88\n2.53√ó\n10.83\n2.38√ó\n10.80\n2.43√ó\n19.02\n2.52√ó\n1.51x\n1.57x\n1.43x\n1.55x\n2.30x\n1.93x\n1.81x\n1.88x\n2.02x\n1.90x\n1.30x\n2.38x\n2.45x\n2.41x\n2.54x\n0\n10\n20\n30\n40\n50\nDeepseek-Coder-1.3B\nDeepseek-Coder-6.7B\nCodeLlama-7B\nCodeLlama-13B\nms/token\nAutoregressive\nSelf-speculative\nOuroboros\nREST\nCodeSwift\nFigure 5: Decoding speed and speedup ratio on HumanEval.\nbly to other sampling strategies. Dc is constructed\nfrom a subset of Python pre-training code in The\nStack (Kocetkov et al., 2022), taking approximately\n9 minutes and yielding a 0.9GB datastore. Dr\nranges from 60KB and 289MB across repositories,\ntaking an average of 10 seconds. Hyper-parameters\ninclude l = 50, p = 0.5, Œ± = Œ≤ = 1, with LLM\noutput truncated every 20 tokens and added to the\nCACHE. Following He et al. (2024), for retrieval,\nthe starting context suffix length nmax = 16, and\na maximum of 64 draft tokens of the top-k se-\nquences are selected in the Trie. Baseline imple-\nmentation details are in Appendix C. Experiments\nfor Deepseep-Coder and CodeLlama-7B use a sin-\ngle NVIDIA 4090 GPU and 28 CPU cores, and\nCodeLlama-13B experiments use a single NVIDIA\nA6000 GPU and 12 CPU cores.\n5.2\nMain Results\n5.2.1\nRepository-level Code Generation\nThe comparison results between CODESWIFT and\nbaselines are shown in Table 1.\nCODESWIFT\nachieves up to 2.30√ó and 2.53√ó speedup on\nDevEval and RepoEval, respectively, outper-\nforming state-of-the-art approaches by up to\n88%. CODESWIFT consistently maintains a sta-\nble speedup of more than 2√ó across a variety of\nbackbone models and datasets, and repositories\nspanning various topics (Appendix D), demonstrat-\ning its robustness.\nCompared to the substantial speedups gained\nby CODESWIFT, baseline approaches achieve lim-\nited accelerations. As a retrieval-based approach,\nthe datastore utilized by REST is approximately 8\ntimes the size of the one employed by CODESWIFT.\nREST exhibits the optimal speedup of around 1.7√ó\nin most cases, but it performs poorly in experiments\nof CodeLlama-13B. This may be attributed to the\nfact that the significant CPU resource demands\nposed by both the 13B model inference and the re-\ntrieval of data from a large datastore in REST, lead-\ning to decreased performance. Besides, Ouroboros\ndemonstrates comparable performance to REST\non Deepseek-Coder-6.7B, yet its generation speed\nis even slower than autoregressive decoding on\nCodeLlama-7B, indicating that its efficacy is sub-\nject to considerable fluctuations influenced by fac-\ntors such as model selection. Self-speculative de-\ncoding consistently maintains a stable yet modest\nacceleration. On the contrast, CODESWIFT does\nnot require a draft model or additional training,\nyet it can maintain a stable speedup ratio even\nunder resource-constrained conditions.\n5.2.2\nStandalone Code Generation\nFor CODESWIFT, we remove Dr from the datas-\ntore and retain Dc, which is the same as the one\nused in the previous experiments. The results are\nshown in Figure 5. Even without the benefit of the\nmulti-source datastore, CODESWIFT still out-\nperforms the baselines, further demonstrating the\neffectiveness of the retrieval strategy and caching\nmodules. Additionally, we observe that the base-\nlines consistently perform better on HumanEval\ncompared to repository-level datasets. This may\nbe affected by the difficulty difference between\n\nstandalone and repository-level code generation\ntasks. For instance, Deepseek-Coder-1.3B achieves\npass@1 scores of 34.8 on HumanEval and 18.2 on\nDevEval. Thus, for approaches such as Ouroboros\nand Self-speculative which require a draft model,\nthe performance in repository-level code genera-\ntion may be negatively affected by the poor perfor-\nmance of the draft model. For REST, HumanEval\ninvolves no project-specific knowledge, and the\ncommon datastore may adequately satisfy retrieval\nrequirements. The performance differences of ex-\nisting approaches on the two types of code gener-\nation tasks also highlight that evaluations based\nsolely on standalone datasets may fail to reflect\nperformance in real-world application scenarios.\n5.3\nAblation Study\nTo analyze the effectiveness of each component\nwithin CODESWIFT, we conduct an ablation study\nwith the results presented in Table 2. Each compo-\nnent is found to contribute to a speedup gain. The\nmulti-source datastore provides richer and more\ninterrelated retrieval content, not only enhancing\nthe average acceptance length but also minimizing\nthe external retrieval cost through parallel search.\nThe retrieval strategy accelerates the inference by\nreducing unnecessary retrieval operations (4.02%\nof the total count of retrieval), with negligible im-\npact on the average acceptance length. The CACHE\nis the most effective component, which provides\nan additional increase in average acceptance length\nof over 30% compared to the baseline. Statistical\nanalysis shows that, although the CACHE contains\nonly 174 sequences at most for DevEval, 33.13%\nof all retrieval operations can successfully obtain\nvalid results directly from the CACHE. The average\nretrieval time from the cache is 0.2ms, which is\napproximately 15% of the retrieval time from the\ndatastore. A case study is shown in Appendix F.\nTable 2: Ablation study results of CODESWIFT on De-\nvEval using Deepseek-Coder-6.7B. Each component is\nincrementally added. The baseline results are obtained\nusing REST with Dc as the datastore. AccLen refers to\naverage acceptance length.\nAccLen\nms/token\nSpeedup\nBaseline\n1.89\n15.86\n1.65√ó\n+ multi-source datastore\n2.28\n14.82\n1.76√ó\n+ retrieval strategy\n2.28\n14.19\n1.84√ó\n+ CACHE\n2.85\n11.69\n2.24√ó\n5.4\nAnalysis of Acceptance Length\nWe compare the acceptance length between\nCODESWIFT and REST (the best performing base-\nline), which represents the upper bound of achiev-\nable acceleration. The results is shown in Figure\n6(a) (more results in Appendix E). CODESWIFT\nexhibits a longer acceptance length across all\ndatasets, with an increase exceeding 50% com-\npared to REST on RepoEval. Although the size\nof REST‚Äôs datastore is approximately 8 times that\nof CODESWIFT, CODESWIFT achieves a higher\nacceleration upper bound. As REST‚Äôs performance\nimproves with the increasing size of the datastore\nwhen resources are sufficient (He et al., 2024),\nwe do not claim that CODESWIFT can outper-\nform REST under all circumstances. Nonetheless,\nCODESWIFT provides a more lightweight and effi-\ncient inference acceleration approach.\n2.04\n2.04\n2.38\n2.97\n3.21\n2.87\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\nAverage Acceptance Length\n2.06\n2.08\n2.38\n2.85\n3.05\n2.92\nREST\nCodeSwift\nDeepSeek-Coder-1.3B\nDeepSeek-Coder-6.7B\n(a)\n(b)\nFigure 6: (a) Acceptance length of CODESWIFT and\nREST; (b) Retrieval performance of CODESWIFT.\n5.5\nHeatmap of Retrieval Performance\nTo explicitly illustrate CODESWIFT‚Äôs effectiveness,\nwe depict its retrieval performance heatmap in Fig-\nure 6(b), with all settings aligned with Figure 3(a).\nA clear observation is that the overall color inten-\nsity of Figure 6(b) is markedly darker compared\nto Figure 3(a), indicating a significant increase in\nthe probability of CODESWIFT retrieving valid re-\nsults. This improvement underscores the enhanced\nretrieval efficacy of CODESWIFT.\n6\nConclusion\nIn this paper, we propose CODESWIFT, a sim-\nple and efficient LLM inference acceleration ap-\nproach for code generation without compromis-\ning generation quality. CODESWIFT leverages a\nmulti-source datastore and a context- and LLM\npreference- aware cache to improve the acceptance\nlength of the retrieved draft while minimizing re-\ndundant retrieval operations through a dynamic and\nefficient retrieval strategy. Experimental results\ndemonstrate that CODESWIFT outperforms state-\n\nof-the-art inference approaches in decoding speed\nfor both standalone and repository-level code gener-\nation tasks. Requiring no draft model or additional\ntraining, CODESWIFT provides a lightweight and\npractical solution for LLM inference acceleration\nin code generation.\nLimitations\nAlthough CODESWIFT offers advantages in accel-\nerating LLM inference for code generation, it also\nhas limitations that need to be taken into account.\nFirstly, we only present the experimental results\non code generation benchmarks written in Python.\nNevertheless, CODESWIFT is designed to be uni-\nversally applicable and can be seamlessly extended\nto other programming languages. Additionally, in\nthe process of integrating repository code into the\ndatastore, CODESWIFT directly utilizes the entire\ncode files. However, the development of an effec-\ntive method for extracting high-frequency expres-\nsions from repositories could potentially enhance\nperformance.\nEthical Considerations\nWe emphasize that the entirety of our research is\nbased on open-source datasets, models, and tools.\nOur method has no potential risk since it is training-\nfree and has no impact on the generation results.\nReferences\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, et al. 2023. Gpt-4 technical report.\narXiv preprint arXiv:2303.08774.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten\nBosma, Henryk Michalewski, David Dohan, Ellen\nJiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732.\nTianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng,\nJason D Lee, Deming Chen, and Tri Dao. 2024.\nMedusa: Simple llm inference acceleration frame-\nwork with multiple decoding heads. arXiv preprint\narXiv:2401.10774.\nCharlie Chen, Sebastian Borgeaud, Geoffrey Irving,\nJean-Baptiste Lespiau, Laurent Sifre, and John\nJumper. 2023a. Accelerating large language model\ndecoding with speculative sampling. arXiv preprint\narXiv:2302.01318.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarri Edwards, Yuri Burda, Nicholas Joseph, Greg\nBrockman, and et al. 2021. Evaluating large lan-\nguage models trained on code.\nZiyi Chen, Xiaocong Yang, Jiacheng Lin, Chenkai Sun,\nKevin Chen-Chuan Chang, and Jie Huang. 2023b.\nCascade speculative drafting for even faster llm infer-\nence. arXiv preprint arXiv:2312.11462.\nYangruibo Ding, Zijian Wang, Wasi Ahmad, Hantian\nDing, Ming Tan, Nihal Jain, Murali Krishna Ra-\nmanathan, Ramesh Nallapati, Parminder Bhatia, Dan\nRoth, et al. 2024. Crosscodeeval: A diverse and mul-\ntilingual benchmark for cross-file code completion.\nAdvances in Neural Information Processing Systems,\n36.\nYichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang.\n2024. Break the sequential dependency of llm in-\nference using lookahead decoding. In International\nConference on Machine Learning.\nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and\nLuke Zettlemoyer. 2019. Mask-predict: Parallel de-\ncoding of conditional masked language models. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 6112‚Äì\n6121, Hong Kong, China. Association for Computa-\ntional Linguistics.\nGithub. 2021. Github copilot.\nDaya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie,\nKai Dong, Wentao Zhang, Guanting Chen, Xiao\nBi, Yu Wu, YK Li, et al. 2024. Deepseek-coder:\nWhen the large language model meets programming‚Äì\nthe rise of code intelligence.\narXiv preprint\narXiv:2401.14196.\nZhenyu He, Zexuan Zhong, Tianle Cai, Jason Lee, and\nDi He. 2024. Rest: Retrieval-based speculative de-\ncoding. In Proceedings of the 2024 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies (Volume 1: Long Papers), pages 1582‚Äì1595.\nDenis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li,\nChenghao Mou, Carlos Mu√±oz Ferrandis, Yacine Jer-\nnite, Margaret Mitchell, Sean Hughes, Thomas Wolf,\net al. 2022. The stack: 3 tb of permissively licensed\nsource code. arXiv preprint arXiv:2211.15533.\nYaniv Leviathan, Matan Kalman, and Yossi Matias.\n2023. Fast inference from transformers via spec-\nulative decoding. In International Conference on\nMachine Learning, pages 19274‚Äì19286. PMLR.\nJia Li, Ge Li, Yunfei Zhao, Yongmin Li, Huanyu Liu,\nHao Zhu, Lecheng Wang, Kaibo Liu, Zheng Fang,\nLanshen Wang, Jiazheng Ding, Xuanming Zhang,\nYuqi Zhu, Yihong Dong, Zhi Jin, Binhua Li, Fei\nHuang, Yongbin Li, Bin Gu, and Mengfei Yang.\n\n2024a. Deveval: A manually-annotated code genera-\ntion benchmark aligned with real-world code reposi-\ntories. In ACL (Findings), pages 3603‚Äì3614. Associ-\nation for Computational Linguistics.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas\nMuennighoff, Denis Kocetkov, Chenghao Mou, Marc\nMarone, Christopher Akiki, Jia Li, Jenny Chim, et al.\n2023. Starcoder: may the source be with you! arXiv\npreprint arXiv:2305.06161.\nYuhui Li, Fangyun Wei, Chao Zhang, and Hongyang\nZhang. 2024b. Eagle: Speculative sampling requires\nrethinking feature uncertainty. In International Con-\nference on Machine Learning.\nMing Liang, Xiaoheng Xie, Gehao Zhang, Xunjin\nZheng, Peng Di, Hongwei Chen, Chengpeng Wang,\nGang Fan, et al. 2024. Repofuse: Repository-level\ncode completion with fused dual context.\narXiv\npreprint arXiv:2402.14323.\nFang Liu, Zhiyi Fu, Ge Li, Zhi Jin, Hui Liu, Yiyang Hao,\nand Li Zhang. 2024a. Non-autoregressive line-level\ncode completion. ACM Transactions on Software\nEngineering and Methodology.\nTianyang Liu, Canwen Xu, and Julian McAuley. 2024b.\nRepobench: Benchmarking repository-level code\nauto-completion systems. In The Twelfth Interna-\ntional Conference on Learning Representations.\nUdi Manber and Gene Myers. 1993. Suffix arrays: a\nnew method for on-line string searches. siam Journal\non Computing, 22(5):935‚Äì948.\nXupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao\nCheng, Zeyu Wang, Zhengxin Zhang, Rae Ying Yee\nWong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, et al.\n2024. Specinfer: Accelerating large language model\nserving with tree-based speculative inference and\nverification. In Proceedings of the 29th ACM Interna-\ntional Conference on Architectural Support for Pro-\ngramming Languages and Operating Systems, Vol-\nume 3, pages 932‚Äì949.\nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,\nJingyu Liu, Romain Sauvestre, Tal Remez, et al. 2023.\nCode llama: Open foundation models for code. arXiv\npreprint arXiv:2308.12950.\nBenjamin Frederick Spector and Christopher Re. 2023.\nAccelerating llm inference with staged speculative\ndecoding. In Workshop on Efficient Systems for Foun-\ndation Models@ ICML2023.\nMitchell Stern, Noam Shazeer, and Jakob Uszkoreit.\n2018. Blockwise parallel decoding for deep autore-\ngressive models. In Advances in Neural Information\nProcessing Systems, volume 31. Curran Associates,\nInc.\nZhaopeng Tu, Zhendong Su, and Premkumar Devanbu.\n2014. On the localness of software. In Proceedings\nof the 22nd ACM SIGSOFT International Symposium\non Foundations of Software Engineering, pages 269‚Äì\n280.\nDi Wu, Wasi Uddin Ahmad, Dejiao Zhang, Murali Kr-\nishna Ramanathan, and Xiaofei Ma. 2024. Repo-\nformer: Selective retrieval for repository-level code\ncompletion. In International Conference on Machine\nLearning.\nNan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin\nJiang, Linjun Yang, Rangan Majumder, and Furu\nWei. 2023. Inference with reference: Lossless ac-\nceleration of large language models. arXiv preprint\narXiv:2304.04487.\nHao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang,\nYuchi Ma, Guangtai Liang, Ying Li, Qianxiang Wang,\nand Tao Xie. 2024. Codereval: A benchmark of prag-\nmatic code generation with generative pre-trained\nmodels. In Proceedings of the 46th IEEE/ACM Inter-\nnational Conference on Software Engineering, pages\n1‚Äì12.\nFengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin\nLiu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and\nWeizhu Chen. 2023. Repocoder: Repository-level\ncode completion through iterative retrieval and gen-\neration. arXiv preprint arXiv:2303.12570.\nJun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen,\nGang Chen, and Sharad Mehrotra. 2024. Draft &\nverify: Lossless large language model acceleration\nvia self-speculative decoding. In Proceedings of the\n62nd Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n11263 ‚Äì 11282.\nWeilin Zhao, Yuxiang Huang, Xu Han, Wang Xu,\nChaojun Xiao, Xinrong Zhang, Yewei Fang, Kai-\nhuo Zhang, Zhiyuan Liu, and Maosong Sun. 2024.\nOuroboros:\nGenerating longer drafts phrase by\nphrase for faster speculative decoding. In Proceed-\nings of the 2024 Conference on Empirical Methods in\nNatural Language Processing, pages 13378‚Äì13393.\n\nA\nRepository-level Code Generation\nCode generation refers to the generation of code\nsnippets that meet requirements based on natural\nlanguage requirements. Most previous researches,\nsuch as the widely used datasets HumanEval(Chen\net al., 2021) and MBPP (Austin et al., 2021), focus\non standalone scenarios, which means the gener-\nated functions may invoke or access only built-in\nfunctions and standard libraries.\nResearches on standalone code generation of-\nten diverges from the complexities of real-world\nprogramming tasks. In practical development set-\ntings, developers typically work within project en-\nvironments, where the code to be generated is fre-\nquently intertwined with external contexts, such\nas API calls. This code often relies on the meth-\nods and properties defined in other files. These\nnon-standalone functions constitute more than 70%\nof the functions in popular open-source projects,\nand evaluating models‚Äô effectiveness on standalone\nfunctions cannot reflect these models‚Äô effectiveness\non pragmatic code generation scenarios (i.e., code\ngeneration for real settings of open source or propri-\netary code) (Yu et al., 2024). Consequently, there\nhas been growing interest in repository-level code\ngeneration (Liu et al., 2024b; Wu et al., 2024; Liang\net al., 2024), which refers to leveraging repository-\nlevel context during code generation tasks, rather\nthan restricting the context to the file under devel-\nopment. Code files within a repository are often\ninterdependent, featuring cross-module API calls,\nshared global snippets, and other forms of linkage.\nResearchers have introduced benchmark datasets\nsuch as RepoEval (Zhang et al., 2023), CoderEval\n(Yu et al., 2024), CrossCodeEval (Ding et al., 2024)\nand DevEval (Li et al., 2024a). These datasets pro-\nvide structured means for assessing the quality and\nrelevance of generated code in realistic scenarios.\nB\nLLM inference acceleration\napproaches\nAutoregressive decoding generates tokens in a\nstep-by-step manner and results in a slow and\ncostly decoding process. In order to accelerate\ndecoding, non-autoregressive decoding approaches\n(Ghazvininejad et al., 2019; Liu et al., 2024a) that\ncan generate multiple tokens in parallel have been\nproposed. While improving decoding speed, these\napproaches typically affect the model performance.\nTherefore, draft-verification decoding acceleration\napproaches (Chen et al., 2023a; Miao et al., 2024;\nHe et al., 2024) have been widely adopted recently,\nwhich do not comprise the model performance.\nThese approaches can be further categorized into\ngeneration-based and retrieval-based, depending\non the technique used for draft generation.\nB.1\nGeneration-based Approaches\nThe draft token can be generated either by the target\nLLM itself or by a small model. Using the target\nLLM itself to directly generate the token may get a\nhigher acceptance rate, while using a small model\nis more likely to have a faster generation speed.\nUsing a small model.\nSpeculative decoding\n(Chen et al., 2023a; Leviathan et al., 2023) is one\nof the effective acceleration approaches that min-\nimize the target LLM forward steps by using an\nsmaller model for drafting and then employing the\ntarget LLM to verify the draft in a low-cost paral-\nlel manner. Ouroboros (Zhao et al., 2024) gener-\nates draft phrases to parallelize the drafting process\nand lengthen drafts. Specinfer (Miao et al., 2024)\nuses many draft models obtained from distillation,\nquantization, and pruning to conduct speculations\ntogether.\nUsing the target LLM itself.\nIdentifying an ap-\npropriate draft model continues to pose significant\nchallenges, as it must align with the vocabulary of\nthe target LLM and achieve a delicate balance be-\ntween keeping quick decoding speed and ensuring\noutput quality. Thus, researchers have investigated\nutilizing the target LLM itself to generate efficient\ndraft sequences. Blockwise Decoding (Stern et al.,\n2018) installs multiple heads on the transformer\ndecoder, enabling parallel generation of multiple\ntokens per step. Medusa (Cai et al., 2024) intro-\nduces multiple heads to predict multiple draft to-\nkens in parallel. Lookahead decoding (Fu et al.,\n2024) uses a n-gram pool to cache the historical\nn-grams generated so far. Eagle (Li et al., 2024b)\nconducts the drafting process at the more structured\nfeature level. Self-speculative decoding (Zhang\net al., 2024)) employs the target LLM with selec-\ntively certain intermediate layers skipped as the\ndraft model.\nB.2\nRetrieval-based Approaches\nThe retrieval-based draft generation approach re-\nplaces the model generation with a search in a\nretrieval datastore to obtain candidate sequences.\nThese approaches can avoid extra training and\nreduce computational overhead.\nLLMA (Yang\n\nTable 3: The skipped layers utilized in draft models for Self-speculative decoding.\nIndex of Skipped Attention Layers\nIndex of Skipped MLP Layers\nDeepseek-Coder-1.3B\n[3, 6, 8, 9, 10, 13, 14, 15, 16, 18, 21, 22]\n[4, 6, 9, 10, 20]\nDeepseek-Coder-6.7B\n[2, 5, 7, 8, 11, 12, 16, 18, 19, 20, 22, 23, 24, 25, 26, 28]\n[2, 5, 6, 12, 15, 25, 26, 27, 28]\nCodeLlama-7B\n[4, 5, 7, 10, 11, 12, 13, 14, 18, 20, 21, 22, 27, 29, 31]\n[8, 11, 13, 22, 23, 25, 27, 28, 31]\nCodeLlama-13B\n[5, 6, 9, 10, 11, 14, 15, 16, 21, 23, 24, 26, 27, 28, 29, 30, 31, 32, 34, 35, 36, 37]\n[10, 11, 12, 14, 15, 25, 26, 27, 30, 32, 33, 34]\net al., 2023) is an inference-with-reference decod-\ning mechanism by exploiting the overlap between\nthe output and the reference of an LLM. It pro-\nvides generic speedup through speculative retrieval\nand batched verification. REST (He et al., 2024)\nreplaces the parametric draft model with a non-\nparametric retrieval datastore.\nAs many subse-\nquences during generation likely appear in the data-\nstore, it can frequently generate multiple correct\ntokens per step.\nC\nImplementation Details of Baselines\nSelf-speculative decoding.\nFor the selection of\nskipped layers, we adopt the results provided by\nthe authors (Zhang et al., 2024) for CodeLlama-\n13B. As for DeepSeek-Coder and CodeLlama-7B,\nfor which the authors did not provide skipped layer\nconfigurations, we utilize Bayesian optimization on\n4 samples from The Stack (Kocetkov et al., 2022)\nto determine the layers to skip during the drafting\nstage. The results can be seen in Table 3. Other\nsettings remain consistent with the original paper.\nOuroboros.\nThis approach requires a draft model\nfor the target LLM, and our selection is illustrated\nin Table 4. We prioritize the selection of a smaller\nmodel from the same series as the target LLM\nto serve as the draft model. For CodeLlama-7B,\nwhich is the smallest model in its series, we opt for\nTinyLlama-1.1B as the draft model due to its shared\narchitecture and tokenizer compatibility. For the\nconfiguration of hyper-parameters, we used Œ≥ = 11\nfor DeepSeek-Coder and Œ≥ = 4 for CodeLlama,\nfollowing the recommendations provided in the\noriginal paper.\nTable 4: Draft model selection for Ouroboros.\nTarget Model\nDraft Model\nDeepseek-Coder-base-6.7B\nDeepseek-Coder-base-1.3B\nCodeLlama-Python-7B\nTinyLlama-1.1B-v1_math_code\nCodeLlama-Python-13B\nCodeLlama-Python-7B\nREST.\nTo construct the datastore, we select the\nfirst 10 files out of the 145 files in The Stack dataset\n(Kocetkov et al., 2022), resulting in a datastore\nof approximately 8.7 GB in size. The results of\nREST demonstrate that its performance on the Hu-\nmanEval dataset improves as the size of the data-\nstore increases (He et al., 2024). However, due to\nhardware limitations, we have chosen the largest\nfeasible datastore that could be operated under the\ngiven constraints. The values of the other hyper-\nparameters are consistent with those in the original\npaper. Specifically, when performing exact match\nin the datastore, the starting context suffix length,\nnmax, is set to 16. The maximum number of se-\nlected draft tokens in the constructed Trie is set to\n64.\nD\nPerformance on Different Code Topics\nAs DevEval includes code repositories spanning\n10 distinct topics, we present the results of\nCODESWIFT using Deepseek-Coder-6.7B for code\ngeneration separately for each topic. As shown in\nFigure 7, CODESWIFT demonstrates consistent and\nsubstantial acceleration in code generation across\nall topics, highlighting its robustness and effective-\nness in diverse contexts.\nFigure 7: Performance of CODESWIFT on different\ncode topics.\nE\nComparison of Acceptance Length\nWe compare the acceptance length between\nCODESWIFT and REST on both DeepSeek-Coder\nand CodeLlama. The results are shown in Table 5.\nCODESWIFT exhibits a longer acceptance length\nacross all datasets and backbone models.\n\nimport functools\nfrom typing import Any, Dict, List, MutableMapping, Tuple, Union\nimport numpy as np\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport transformers\ntry:\n  from opendelta import (\n    AdapterModel,\n    BitFitModel,\n    LoraModel,\n    PrefixModel,\n    SoftPromptModel,\n  )\n  HAS_OPENDELTA = True\nexcept ModuleNotFoundError:\n  HAS_OPENDELTA = False\ndef make_head(n_embd: int, out: int, dtype: type = torch.float32) -> nn.Sequential:\n  \"\"\"Returns a generic sequential MLP head.\"\"\"\n    return nn.Sequential(\nnn.Linear(n_embd, n_embd, dtype=dtype),\n    nn.GELU(),\n   \nnn.Linear(n_embd, out, dtype=dtype),\n \n)\ndef make_head_with_dropout(\n  n_embd: int, out: int, dropout: float, dtype: type = torch.float32\n) -> nn.Sequential:\n\"\"\"Returns a generic sequential MLP head with dropout.\"\"\"\nreturn nn.Sequential(\nnn.Linear(n_embd, n_embd, dtype=dtype),\n   \nnn.GELU(),\n   \nnn.Dropout(dropout),\nnn.Linear(n_embd, out, dtype=dtype),\n)\ndef make_head_with_dropout_and_layer_norm(\nn_embd: int, out: int, dropout: float, dtype: type = torch.float32\n) -> nn.Sequential:\n\"\"\"Returns a generic sequential MLP head with dropout and layer norm.\"\"\"\n \nreturn nn.Sequential(\nnn.LayerNorm(n_embd, dtype=dtype),\nnn.Linear(n_embd, n_embd, dtype=dtype),\n   \nnn.GELU(),\n   \nnn.Dropout(dropout),\n nn.Linear(n_embd, out, dtype=dtype),\n)\nPrompt\nLLM output\ntokens retrieved from datastore\ntokens retrieved from cache\n          tokens that cannot be retrieved by baseline\nFigure 8: Case study of CODESWIFT‚Äôs retrieval performance.\nTable 5:\nAcceptance length comparison between\nCODESWIFT and REST. DC and CL are abbreviations\nfor Deepseek-Coder and CodeLlama, respectively.\nDevEval\nRepoEval\nHumanEval\nREST\nCODESWIFT\nREST\nCODESWIFT\nREST\nCODESWIFT\nDC-1.3B\n2.04\n2.97\n2.04\n3.21\n2.38\n2.87\nDC-6.7B\n2.06\n2.85\n2.08\n3.05\n2.38\n2.92\nCL-7B\n2.05\n2.77\n2.07\n3.06\n2.27\n2.79\nCL-13B\n2.06\n2.75\n2.06\n2.99\n2.25\n2.63\nF\nCase Study\nTo demonstrate the effectiveness of CODESWIFT,\nwe conduct a case study.\nAs shown in Figure\n8, we use different background colors to high-\nlight the sources of the accepted draft tokens. Ad-\nditionally, the tokens enclosed in red boxes are\nthose that can be retrieved by CODESWIFT but not\nby the baseline (REST with Dc as the datastore).\nWhen generating the earlier parts of the sequence,\nthe CACHE remains unavailable due to an insuf-\nficient accumulation of sequences. Nonetheless,\nlots of repository-related tokens can be addition-\nally retrieved by CODESWIFT benefiting from the\nmulti-source datastore. When the CACHE is avail-\nable, a larger number of consecutive tokens be-\ncomes retrievable, thereby enhancing the inference\nspeed through the extension of acceptable sequence\nlengths and the reduction of retrieval overhead.\n",
  "metadata": {
    "source_path": "papers/arxiv/CodeSwift_Accelerating_LLM_Inference_for_Efficient_Code_Generation_3dcf4182802d996f.pdf",
    "content_hash": "3dcf4182802d996fd7437a2df5cbedbd6714388aa9eb33a892c8ef500632def1",
    "arxiv_id": null,
    "title": "CodeSwift_Accelerating_LLM_Inference_for_Efficient_Code_Generation_3dcf4182802d996f",
    "author": "",
    "creation_date": "D:20250225024813Z",
    "published": "2025-02-25T02:48:13",
    "pages": 13,
    "size": 1500322,
    "file_mtime": 1740470178.7226915
  }
}