{
  "text": "All You Need for Counterfactual Explainability\nIs Principled and Reliable Estimate of Aleatoric and Epistemic Uncertainty\nKacper Sokol 1 2 Eyke H¨ullermeier 2\nAbstract\nThis position paper argues that, to its detriment,\ntransparency research overlooks many founda-\ntional concepts of artificial intelligence. Here, we\nfocus on uncertainty quantification – in the con-\ntext of ante-hoc interpretability and counterfactual\nexplainability – showing how its adoption could\naddress key challenges in the field. First, we posit\nthat uncertainty and ante-hoc interpretability of-\nfer complementary views of the same underlying\nidea; second, we assert that uncertainty provides a\nprincipled unifying framework for counterfactual\nexplainability. Consequently, inherently transpar-\nent models can benefit from human-centred ex-\nplanatory insights – like counterfactuals – which\nare otherwise missing. At a higher level, integrat-\ning artificial intelligence fundamentals into trans-\nparency research promises to yield more reliable,\nrobust and understandable predictive models.\n1. Uncertainty and Transparency\nArtificial intelligence (AI) models can achieve impressive\nresults across many domains, but their deployment is often\nstymied by their opaqueness, unreliability and lack of ro-\nbustness [Rudin, 2019]. Consequently, two paradigms have\nemerged to alleviate such issues: ante-hoc interpretabil-\nity envisages building inherently transparent models whose\nfunctioning adheres to domain-specific constraints, and post-\nhoc explainability offers tools that elucidate the operation of\npredictive models through independent explanatory mech-\nanisms [Sokol & Flach, 2020a]. Additionally, uncertainty\nquantification has been proposed to improve the account-\nability of AI by looking beyond predictive performance; it\naims to provide truthful representation of models’ aleatoric\n– inherent to data generating processes – and epistemic –\ndue to model non-uniqueness – uncertainty [H¨ullermeier &\nWaegeman, 2021].\nBut methods for reliably estimating uncertainty remain\n1Department of Computer Science, ETH Zurich, Switzerland\n2Institute of Informatics, LMU Munich, Germany. Correspon-\ndence to: Kacper Sokol <kacper.sokol@inf.ethz.ch>.\nfairly underdeveloped, with such considerations often be-\ning neglected. Moreover, while post-hoc approaches are\nat the forefront of human-centred explainable AI (XAI) –\nenabling diverse audiences, both with and without tech-\nnical expertise, to peer inside predictive models [Miller,\n2019] – these techniques are usually unable to faithfully\ncapture the operation of AI systems, offering misleading\ninsights [Rudin, 2019]. As a result, ante-hoc interpretable\nmodels are preferred in (high stakes) real-world applica-\ntions even though their functioning remains largely opaque\nto non-technical stakeholders, who nonetheless tend to be\ntheir primary users [Sokol & Vogt, 2023].\nIn this position paper we argue that AI transparency\ntechniques are largely oblivious to various notions of\nuncertainty despite the two being fundamentally inter-\nconnected. More broadly, we assert that XAI research\ntends to neglect the rich tapestry of foundational AI con-\ncepts, which can lead to reinventing what already exists;\nadmittedly, this sentiment is more true for post-hoc than\nante-hoc approaches since the latter draw upon decades of\nwork on classic AI models [Rudin et al., 2022]. We support\nthese arguments by demonstrating that connecting AI trans-\nparency and uncertainty quantification can address many\nopen challenges in XAI. Specifically, we use the example of\ncounterfactual explanations given that they are considered\nthe gold standard of human-centred XAI [Miller, 2019].\nTo this end, we first review relevant topics in Section 2:\n(§2.1) counterfactual explainability, (§2.2) uncertainty quan-\ntification and (§2.3) their intersection. Next, in Section 3,\nwe present our arguments. We begin by (§3.1) summarising\nopen challenges: misguided or overlooked modelling as-\nsumptions; ad-hoc fixes and unreliable proxies used in lieu\nof principled uncertainty handling; overemphasis on crisp\nclassification and inadequate XAI evaluation practice; domi-\nnance of neural models applied to unstructured data leading\nto neglect of inherently transparent AI; and largely missing\nhuman-centred perspective in ante-hoc interpretability.\nBuilding upon these observations, we posit that: (§3.2) un-\ncertainty quantification and ante-hoc interpretability\nare fundamentally two facets of the same concept; and\n(§3.3) uncertainty quantification provides a principled\nunifying framework for generating state-of-the-art coun-\n1\narXiv:2502.17007v1  [cs.LG]  24 Feb 2025\n\nAll You Need for Counterfactual Explainability Is Principled and Reliable Estimate of Aleatoric and Epistemic Uncertainty\nterfactuals. Consequently, we argue that making inher-\nently interpretable models uncertainty-aware not only\nimproves their reliability and robustness but also pro-\nvides a systematic pathway for integration of human-\ncentred explanatory insights such as counterfactuals. We\nthen (§3.4) review alternative perspectives, thereby strength-\nening our position. We conclude in Section 4 by summaris-\ning our arguments and outlining future research directions.\n2. Background\nLet us first overview latest research in counterfactual ex-\nplainability (§2.1) and uncertainty quantification (§2.2) as\nwell as work at the intersection of these two fields (§2.3).\n2.1. Counterfactual Explainability\nCounterfactuals capture hypothetical situations where the\noutcome of interest is different (usually, more desirable)\nthan what has been observed. They are considered the gold\nstandard of human-centred XAI given their strong social\nsciences foundations, natural familiarity to lay and expert\naudiences, and regulatory compliance [Wachter et al., 2017;\nMiller, 2019; Guidotti, 2022]. In their simplest form, they\nare generated by minimising the distance in the feature space\nbetween the current instance and a hypothetical data point\nthat is assigned the desired prediction, i.e., finding the most\nsimilar (distance-wise) instance of the selected class. This\nprocess optimises for the following properties:\nValidity guarantees that the counterfactual instance is clas-\nsified by the explained model with the desired class.\nSimilarity places the counterfactual instance as close as\npossible to the factual data point in the feature space\n(according to the chosen distance metric), making it\nhighly relevant and easy to reach.\nSparsity minimises the number of features changed be-\ntween the factual and counterfactual instances, enhanc-\ning human comprehensibility of the explanation.\nIn practice, however, this strategy often yields counter-\nfactuals that are perceptually closer to adversarial exam-\nples [Goodfellow et al., 2015] than meaningful explanations,\ni.e., the recommended change is either unrealistic or carries\nno intrinsic meaning [Freiesleben, 2022]. As a solution,\na collection of new social, technical and sociotechnical\ndesiderata was proposed [Pawelczyk et al., 2020; Delaney\net al., 2021; Schut et al., 2021; Guidotti, 2022]:\nPlausibility requires the counterfactual state to come from\nthe data manifold, thus be achievable in real life.\nConnectedness extends plausibility by ensuring that the\ncounterfactual instance is supported by (i.e., is close\nin the feature space to) another data point assigned to\nthe counterfactual class (with high probability) by the\nexplained model such that there exists a direct line (or a\nsequence of steps through known data points) between\nthe two that does not cross a decision boundary.\nDiscriminativeness makes the counterfactual instance un-\nambiguous, i.e., clearly distinguishable from similar\ninstances (refer back to similarity) that are not of the\ncounterfactual class, to avoid confusing humans with\nthe like of adversarial examples.\nRobustness prevents realistic data shifts and model\nchanges from invalidating counterfactual explanations.\nStability ensures that the neighbours of the explained data\npoint receive similar (or identical) counterfactuals.\nActionability guarantees that counterfactuals can be imple-\nmented in real life, as some features may be immutable,\ne.g., ethnicity, while others must obey monotonicity,\ne.g., age, simultaneously maintaining the congruity of\nthese changes given that they may be incompatible.\nNotably, all these properties – illustrated in Figure 1 – per-\ntain to the counterfactual instance itself. Additional desider-\nata apply to collections of such explanations, used because\na single counterfactual is unlikely to satisfy the distinct\nneeds of different explainees [Sokol & Flach, 2020c]. In\nthis context, diversity ensures that a set of counterfactuals is\nrepresentative of the available explanations while remaining\nminimal. From the model selection perspective, counterfac-\ntual availability can also be considered [Kanamori et al.,\n2024]; it maximises the number of individuals for whom at\nleast one acceptable explanation can be generated, which is\nimportant when multiple models (from a single class) with\ncomparable performance exist [Sokol et al., 2024].\nA prominent extension of counterfactuals builds action se-\nquences guiding an explainee step-by-step to the chosen\noutcome [Poyiadzi et al., 2020]. Here, additional desider-\nata are introduced to formalise the properties of the path\nconnecting the factual and counterfactual instances. Feasi-\nbility entails building such a link by following pre-existing\ndata points, thus ensuring plausibility, connectedness and\nstability. Affinity, branching and divergence imbue these\npaths with spatial awareness by capturing the geometry\nof the feature space density [Sokol et al., 2023]. These\ndesiderata account for the order in which feature changes\nneed to be implemented, group counterfactuals based on\nthe (directional) similarity of their paths, and differentiate\nincompatible explanations, hence offer a richer perspective\non counterfactual diversity and actionability. These extra\nproperties are shown in Figure 2.\nAdhering to the desiderata listed above allows to retrieve\nstate-of-the-art counterfactuals without modelling the causal\nstructure of the data generating processes [Karimi et al.,\n2022], which in real life is often infeasible.1 Such expla-\n1The causal perspective is outside of this paper’s scope, but\n2\n\nAll You Need for Counterfactual Explainability Is Principled and Reliable Estimate of Aleatoric and Epistemic Uncertainty\nD\nC\nA\nB\n(a) Visualisation of the three fundamental\ncounterfactual desiderata. Explanation A\nis invalid; explanation B lacks sparsity in\ncomparison to explanation C as the former\nrequires changing two features whereas the\nlatter only one; explanation D lacks sim-\nilarity when compared to explanation C\ngiven that it is farther away.\nB\nA\nC\nD\n(b) Visualisation of the six extended counterfactual desiderata. Explanation A is implau-\nsible as it lies outside of the data manifold; explanation B lacks connectedness as its\ncloses neighbour is classified negative; explanation C is not discriminative as it lies in\na region where classes overlap; explanations A, B & C may lack robustness as more\ndata points are collected to train the model and it converges; all the explanations are\nconsidered stable given the simplicity of this example; finally, if feature x1 (horizontal\naxis) is assumed actionable but feature x2 (vertical axis) not, only explanation D is\nactionable as the other counterfactuals require changing both features x1 and x2.\nFigure 1: Visualisation of the (a) fundamental – validity, similarity and sparsity – and (b) extended – plausibility, connected-\nness, discriminativeness, robustness, stability and actionability – properties of human-centred counterfactual explanations.\nThe question mark indicates the explained instance and the check mark represents a counterfactual data point.\nF\nC\nA\nB\nD\nE\n(a) Generating a diverse (representative\nand minimal) collection of counterfactuals\nhelps to satisfy disparate needs of different\nexplainees. Explanation availability is not\na problem in this example, especially that\nthe depicted linear model is near-optimal\nand unlikely to change significantly.\n(b) A counterfactual explanation – captured\nby the green vector – communicates the en-\ntire feature set change as a single step; its\npath-based alternative – depicted by the\nblue segments – splits it into three simpler\nsteps. The latter approach becomes partic-\nularly useful for high-dimensional data.\nC\nA\nD\nB\n(c) The feasibility of the shown path-based\ncounterfactuals (which also entails their\nplausibility, connectedness and stability)\nis ensured by constructing them upon pre-\nexisting instances. Accounting for the spa-\ntial properties allows to capture the paths’\naffinity, branching and divergence.\nFigure 2: Visualisation of counterfactuals in relation to (a) explanation groups and model selection; (b) path-based\nexplainability; and (c) geometry of the feature space density. The question mark indicates the explained instance, the star\nsymbolises an intermediate step of a counterfactual path, and the check mark represents a counterfactual data point.\n(a) The optimal model may remain aleatorically uncertain about a prediction, e.g., the\nquestion mark, because of the intrinsic class overlap. This type of uncertainty cannot\nbe (easily) reduced. Note, however, that collecting additional information (rather than\nobservations), e.g., an extra feature, could allow separating the overlapping instances\nin the added dimension at the expense of increased epistemic uncertainty.\n(b) The lack of knowledge about the optimal\nmodel may result in epistemic uncertainty\nabout a prediction, e.g., the question mark.\nThis type of uncertainty can usually be re-\nduced by collecting more data – see Panel (a).\nFigure 3: Demonstration of (a) aleatoric and (b) epistemic uncertainty for a two-dimensional toy data set with the model\nclass restricted to linear classifiers [H¨ullermeier & Waegeman, 2021].\n3\n\nAll You Need for Counterfactual Explainability Is Principled and Reliable Estimate of Aleatoric and Epistemic Uncertainty\n(a) Given the linear model class, the point represented by the\nquestion mark will be classified as positive with high certainty\n(despite the lack of data observed in its neighbourhood that could\ndirectly support this prediction).\n(b) When using the quadratic model class – which is more flexible\nand expressive than linear models – the point represented by the\nquestion mark cannot be classified as positive or negative with high\ncertainty (unless more data are collected).\nFigure 4: Demonstration of how uncertainty quantification depends on the assumed model class for (a) linear and (b)\nquadratic classifiers using a two-dimensional toy data set [H¨ullermeier & Waegeman, 2021].\nnations, nonetheless, may still lack reliability, hence be\nunsuitable for high stakes domains, if they do not fully ac-\ncount for the inner workings of the underlying predictive\nmodel [Rudin, 2019]. In XAI, a distinction is made between\nante- and post-hoc techniques. Post-hoc methods generate\nexplanations using mechanisms that operate independently\nof predictive models, which makes them portable but also\nprevents them from accurately capturing how these models\nmake decisions. Ante-hoc interpretability, in contrast, is\nachieved by building inherently transparent models that ad-\nhere to domain-specific constraints, which guarantees their\nreliability, but often at the expense of limiting their com-\nprehensibility to AI experts, with the explanatory needs of\nother stakeholders left unaddressed [Sokol & Vogt, 2023].\nWhile counterfactuals are ideal for this purpose, most rele-\nvant explainers are post-hoc [Guidotti, 2022], leaving this\nproblem unresolved.\n2.2. Uncertainty Quantification\nIn addition to transparency, other critical properties of data-\ndriven models – such as their fairness, robustness, reliability,\ntrustworthiness and accountability – have gained promi-\nnence with the proliferation of AI. The foundational aspect\nof these desiderata is predictive uncertainty given its crucial\nrole in enhancing AI systems’ awareness of their limita-\ntions [H¨ullermeier & Waegeman, 2021]. Primarily, this in-\nvolves distinguishing between aleatoric uncertainty, which\narises due to the randomness inherent to the data generating\nprocess, and epistemic uncertainty, which stems from the\nlearning algorithm’s ignorance of the true underlying model.\nAleatoric uncertainty is irreducible in the sense that more\nobservations of a given phenomenon will not improve our\nability to predict its outcome; a prototypical example is a\ncoin toss – no matter how many data points we collect, we\ncannot reliably predict how the coin will land next given\nproperties such as actionability and plausibility can act as its proxy.\nthe intrinsic variability of this process. In certain cases,\nhowever, collecting more information (as opposed to more\nobservations), e.g., an extra feature, can help to alleviate\naleatoric uncertainty, e.g., by separating thus far overlap-\nping data points in the added dimension. Such an approach\nreduces aleatoric uncertainty often at the expense of its epis-\ntemic counterpart as fitting a model in higher dimensions\nis more difficult and requires a larger data set. Epistemic\nuncertainty, on the other hand, is reducible as additional\nobservations of a given phenomenon can help us to more\nreliably predict its outcome; revisiting the coin toss example,\nthe larger our set of observations is, the more precisely we\ncan estimate the coin’s bias. Figure 3 demonstrates these\ntwo types of uncertainty for a two-dimensional toy example.\nMethods to reliably quantify aleatoric and epistemic un-\ncertainty, such as calibration, are still in early stages of\ndevelopment [Silva Filho et al., 2023]. By and large, we\nlack AI models that are truly uncertainty-aware, which cur-\ntails the use of this technology in high stakes domains. De-\nspite often being overlooked, uncertainty estimation is never\nassumption-free. It is influenced by the underlying, and\nsometimes implicit, data modelling assumptions, among\nwhich the model class choice is particularly consequential.\nNotably, the degree of predictive uncertainty is determined\nby the flexibility of the model class – see Figure 4. More\nrestrictive families of functions result in lower uncertainty\nand vice versa, with those deemed universal approximators,\ne.g., neural networks, impacted the most [H¨ullermeier &\nWaegeman, 2021]. Other, model-specific, assumptions also\ninfluence uncertainty quantification. For example, linear\nmodels – see Figure 4a – presuppose that the farther a data\npoint is placed from the decision boundary, the less uncer-\ntain its prediction is. This holds even if such an instance\ncomes from a sparse data region. While this property is\ninherent to linear models, its consequences may be unde-\nsirable and necessitate special handling, e.g., tweaking the\n4\n\nAll You Need for Counterfactual Explainability Is Principled and Reliable Estimate of Aleatoric and Epistemic Uncertainty\nuncertainty estimates post-hoc to convey low confidence\noutside of the data manifold [Perello-Nieto et al., 2016].\nIn addition to aleatoric and epistemic, various other sources\nof uncertainty exist [H¨ullermeier & Waegeman, 2021].\nModel uncertainty comes from incorrect model class choice,\nwhich is common in practice; it cannot be easily detected\nand addressed, but when the model class is highly expres-\nsive (or considered a universal approximator), this type of\nuncertainty is inconsequential. Approximation uncertainty\narises when the model learnt from data does not align with\nthe true or optimal model; it contributes to epistemic uncer-\ntainty and is particularly pronounced for highly expressive\nmodel classes and sparse training data. In general, uncer-\ntainty quantification is premised on the closed/stable world\nassumption, violating which makes this process consider-\nably more difficult [Gigerenzer, 2023]. Relevant examples\nare non-stationary data generating processes or data distribu-\ntion shifts, emergence of new and disappearance of known\ntarget classes, and noisy or imprecise data features (e.g., due\nto measurement errors).\nAnother challenge comes from representing lack of knowl-\nedge. The common approach of modelling total ignorance\nwith the uniform distribution is unable to distinguish be-\ntween precise knowledge of an inherently random process,\ne.g., a fair coin toss, and complete lack of knowledge about\nit, e.g., a possible bias of the coin. Overcoming this prob-\nlem requires working with second-order uncertainty given\nthat a single distribution cannot capture uncertain knowl-\nedge [Shafer, 1976; Dubois & Prade, 1988; Walley, 1991;\nSmets & Kennes, 1994].\n2.3. Uncertainty and Counterfactual Explainability\nIntegrating uncertainty quantification with XAI has recently\ngained interest, albeit with much left to be explored [Bhatt\net al., 2021]. Notably, reliable uncertainty estimation re-\nmains a key challenge in building ante-hoc interpretable\nmodels [Rudin et al., 2022]. More broadly, explaining why\nan AI system is uncertain can help humans to understand\nits limitations and act to reduce its uncertainty; conversely,\npresenting the uncertainty of an explanation can improve\nhumans’ ability to make responsible AI-aided decisions.\nIn this space, Antor´an et al. [2021] proposed a counterfac-\ntual explainer that shows how to reduce uncertainty of a\nprediction. Delaney et al. [2021] used uncertainty to eval-\nuate the quality of counterfactuals, expanding the validity\ndesideratum from crisp to probabilistic classification. To\nthis end, they proposed trust scores, which link (epistemic)\nuncertainty to counterfactual plausibility; others adapted\npre-existing outlier detection methods for this purpose [Ro-\nmashov et al., 2022]. Similarly, Thiagarajan et al. [2022]\ndesigned an explainer that generates high-confidence coun-\nterfactual data points by minimising their uncertainty.\nDuell et al. [2024] expanded the notion of counterfactual\nplausibility by computing uncertainty of the vector con-\nnecting the factual and counterfactual instances. Kanamori\net al. [2024] explored the implicit link between epistemic\nuncertainty and model multiplicity – a phenomenon where a\ngroup of models has comparable predictive performance de-\nspite intrinsic differences (see Figures 3b and 4), sometimes\ncalled the Rash¯omon effect of statistics [Breiman, 2001;\nRudin et al., 2024]; they did so in the context of counter-\nfactual robustness and availability. Schut et al. [2021] di-\nrectly coupled counterfactual explainability and uncertainty\nby connecting aleatoric and epistemic uncertainty respec-\ntively to counterfactual discriminativeness (which they call\nunambiguity) and plausibility (called realism) to generate\nhigh-quality explanations.\n3. Building Bridges: Uncertainty-aware XAI\nDespite latent connections between uncertainty and various\nforms of AI interpretability and explainability, (§3.1) many\nsuch links remain unexplored with the underlying challenges\nunaddressed – XAI is thus largely uncertainty-unaware.\nThis section focuses on two critical gaps at the intersection\nof these fields. One, (§3.2) we argue that (reliable) uncer-\ntainty quantification and ante-hoc interpretability are tightly\ncoupled and synergistic since the former reinforces AI trans-\nparency and accountability while the latter provides well-\ndefined model forms benefiting the former. Making ante-hoc\ninterpretable models uncertainty-aware additionally creates\na systematic pathway for adoption of human-centred devel-\nopments from across XAI, expanding the comprehensibil-\nity of such AI systems to non-technical stakeholders, e.g.,\nthrough counterfactual insights as we demonstrate next.\nTwo, (§3.3) we argue that uncertainty quantification offers a\nprincipled unifying framework for generating state-of-the-\nart (non-causal) counterfactuals, superseding many ad-hoc\nproxies and criteria currently used to this end. Of particular\nrelevance are the distinction between aleatoric and epistemic\nuncertainty, which allows to overcome the limitations im-\nposed by employing its aggregate measure, and the proper-\nties of the entire path connecting factual and counterfactual\ninstances, rather than of the latter point alone. We finish this\nsection by (§3.4) reviewing alternative perspectives.\n3.1. Open Challenges\nSection 2.3 showed that uncertainty has primarily been ap-\nplied, albeit often indirectly, to evaluate counterfactuals and,\ntoo a much lesser extent, guide their generation. Most ex-\nplainers overlook data modelling assumptions that are foun-\ndational to robust uncertainty quantification and use ad-hoc\nfixes when uncertainty estimates are unavailable. They em-\nploy proxies like operating within previously observed data\npoints, measuring distance to the nearest instance, adapting\n5\n\nAll You Need for Counterfactual Explainability Is Principled and Reliable Estimate of Aleatoric and Epistemic Uncertainty\npre-existing outlier detection methods, approximating the\ndata manifold in latent space, or defining custom metrics\nsuch as the trust scores. While these approaches help to\nensure plausibility, connectedness and actionability – as\nwell as other desiderata – they rely on surrogate measures\nof uncertainty that can be unreliable, e.g., when they are\nincompatible with the explained class of models.\nXAI’s blindness to uncertainty thus not only undermines the\ntruthfulness of explanations, but more critically it side-steps\nthe challenge of building high quality models AI [Schut\net al., 2021]. The latter is symptomatic of a broader prob-\nlem, where the quality of models and their explanations\ntends to be assessed independently (especially for post-hoc\nmethods) [Sokol & Vogt, 2024]. Another consequence is\nthat most (counterfactual) explainers operate on crisp pre-\ndictions, which prevents them from relying on the (relative)\nuncertainty of each class, possibly exacerbating explana-\ntion ambiguity [Sokol & Flach, 2020b]. More broadly,\nXAI rarely ventures beyond deterministic models; while\nit sometimes considers first-order probabilistic predictions,\nsecond-order uncertainty remains alien [Wood et al., 2024].\nMoreover, research at the intersection of XAI and uncer-\ntainty quantification is predominantly concerned with differ-\nentiable as well as neural model classes applied to images\nand text, overlooking other areas of AI despite the preva-\nlence of tabular data in real-life applications [Shwartz-Ziv\n& Armon, 2022]. Consequently, (ante-hoc) inherently inter-\npretable models – which tend to offer superior performance\nfor structured data [Rudin, 2019] – are neglected. They not\nonly lack methods to imbue them with reliable aleatoric\nand epistemic uncertainty estimates [Rudin et al., 2022], but\nalso that can produce human-centred explanatory insights\n(e.g., counterfactuals) into their operation. Since ante-hoc\ninterpretability in itself is often insufficient to engender un-\nderstanding in non-technical stakeholders, such techniques\nare of paramount importance [Sokol & Vogt, 2023].\n3.2. Uncertainty Quantification and Ante-hoc\nInterpretability are Fundamentally Intertwined\nGiven their technological nature, AI models should not\nbe accepted based on the trust they engender, e.g., after\nprolonged interactions, but rather due to their operational\nreliability and robustness [Ryan, 2020]. Their sound techni-\ncal design and functioning are especially important in high\nstakes domains [Rudin, 2019]. These principles are embod-\nied by (ante-hoc) interpretability and uncertainty quantifica-\ntion, which allow humans to develop correct understanding\nof models’ capabilities and limitations [Schut et al., 2021],\nleading to accountability and trust [Tomsett et al., 2020].\nWhile uncertainty is generally used to capture the quality of\nAI models and their predictions, it is important not to over-\nlook its decomposition into the aleatoric and epistemic parts\ngiven that they convey fundamentally distinct information.\nBased on our review of uncertainty and ante-hoc inter-\npretability, we posit that these concepts are not only comple-\nmentary [Schut et al., 2021] but rather constitute different\nviews of the same underlying idea – with the former being\nnecessary for a strong notion of the latter, and the latter en-\nhancing the former. While Rudin et al. [2022] have briefly\nnoted the link between these two concepts, there is much\nleft to be explored. Recall that ante-hoc interpretability\noffers inherently robust, accountable and transparent AI\nmodels by constraining their form [Rudin, 2019]. Speci-\nfying the modelling assumptions explicitly, in turn, allows\nfor reliable uncertainty quantification and decomposition\ngiven how sensitive these processes can be (see Figure 4).\nThis shared foundation is critical as post-hoc (and model-\nagnostic) uncertainty estimation can yield incorrect insights\ndue to its possible incompatibility with (implicit) data mod-\nelling assumptions – akin to how post-hoc explainers may\nnot truthfully capture the functioning of AI models.\nCurrent XAI research, nonetheless, often overlooks the qual-\nity (including predictive power) of a model as well as the\nimportance and implications of a model class choice; this is\nespecially true for post-hoc methods but also affects, albeit\nto a much lesser extent, ante-hoc interpretability. For ex-\nample, consider the fundamental differences in the decision\nboundary shape learnt by linear models and decision trees,\nand their impact on predictions and uncertainty estimates.\nFor the former, these properties rely largely on the distance\nfrom the decision boundary; whereas for the latter, the re-\nlation is more nuanced and depends on the hyper-rectangle\npartition of the feature space as well as the quantity and class\ndistribution of the training data used to learn it. Reducing\nepistemic uncertainty of models considered to be universal\napproximators is particularly challenging exactly because\nof their inherent expressiveness. Here, a popular solution is\nto introduce strong model form regularisation – an approach\nthat bears the hallmarks of ante-hoc interpretability.\nA slight modification to a model class – e.g., bounding the\nhyper-rectangles of a decision tree from all sides – could\nyield drastically different predictions and uncertainty esti-\nmates. Similarly, some model classes capture the training\ndata distribution, while other do not, leading to different\nhandling of instances located in sparse data regions. While\npost-processing techniques can enhance a particular model\nclass in this respect [Perello-Nieto et al., 2016], such an\napproach introduces asymmetry between the modelling as-\nsumptions and properties of the final predictions. If the latter\nare used in downstream tasks, this may be undesirable; more\nbroadly, it signals that another model class fulfilling the de-\nsired requirements ought to be considered instead [Rudin\net al., 2024].\nNotably, access to reliable uncertainty estimates promises to\n6\n\nAll You Need for Counterfactual Explainability Is Principled and Reliable Estimate of Aleatoric and Epistemic Uncertainty\n(a) Linear model.\n(b) Linear model with manifold.\nFigure 5: Illustration of uncertainty-driven instance- (green)\nand path-based (blue) counterfactuals for (a) a standard\nlinear model and (b) one enhanced with a background\nclass [Perello-Nieto et al., 2016]. In the former case, follow-\ning the direction perpendicular to the decision boundary is\nthe optimal approach; in the latter case, the counterfactual\nvector and path are skewed since epistemic uncertainty ad-\nditionally captures the data manifold shape.\nenable generation of a broad range of explanatory insights;\nwe show this for counterfactuals below (§3.3). Guiding this\nprocess with other, ad-hoc mechanisms may yield incorrect\nexplanations, and would be especially harmful, and counter-\nproductive, for ante-hoc interpretable models. In essence,\nsuch explainers need to compensate for the shortcomings or\nlack of (native) uncertainty estimates post-hoc, which not\nonly increases their complexity but also tends to introduce\n(implicit) assumptions that are likely incompatible with the\noperation of many AI models, thus degrading explanation\nquality. Uncertainty-guided approaches that produce human-\ncentred explanatory insights are of particular relevance for\nante-hoc interpretable models given their incomprehensibil-\nity to non-technical stakeholders, which likely harms their\nreal-life adoption.\n3.3. Uncertainty Quantification is a Unifying\nFramework for Counterfactual Explainability\nGiven the appeal of counterfactuals, improving their re-\ntrieval is crucial. We posit that uncertainty quantification is\nuniquely suited to this end and provides a principled unify-\ning framework for counterfactual explainability, subsuming\nthe desiderata listed in Section 2.1. Specifically, expressing\nthem through constraints on aleatoric and epistemic uncer-\ntainty, instead of its aggregate measure, and working with\nthe path-based conceptualisation of counterfactuals, rather\nthan the counterfactual instances alone, offer many benefits.\nChief among them is explanation consistency across models\n(whether from the same or different model classes) given\nthat modelling assumptions are directly accounted for in\nexplanation generation. This is more desirable than expla-\nnation consistency with respect to an explainer (applied to\ndistinct model classes) since its operational characteristics\nare unlikely to be compatible with a broad range of unique\nmodelling assumptions. Optimal, in terms of uncertainty,\nexplanations may thus necessarily be dissimilar in absolute\nterms across distinct modelling scenarios – e.g., different\ncounterfactual instances – but at the same time consistent\nwith regard to their high-level desiderata as shown in Fig-\nure 5. Consequently, building models that are uncertainty-\naware, reliable and robust because they are fundamentally\nsuitable for a particular application appears more important\nthan creating “universal”, thus overly complex, explainers.\nSimilarity & Sparsity\nThese observations offer an alter-\nnative perspective on counterfactual similarity, allowing us\nto compare explanations to each other through their uncer-\ntainty in addition to their separation in the feature space as\nwell as distance from the explained instance. Otherwise,\nthe classic interpretation of counterfactual similarity and\nsparsity positions these properties as complementary to the\nnotion of uncertainty, hence they can be implemented inde-\npendently of it, e.g., by minimising the Manhattan and Eu-\nclidean distance metrics. For path-based explanations, these\ntwo properties should additionally account for the quantity\nand size of individual steps as well as the number of features\naffected by each segment; note that these characteristics can,\nto a degree, be reflected in uncertainty estimates, e.g., when\nthe data manifold shape captures feature (inter)dependence.\nValidity & Actionability\nMore directly, aleatoric and\nepistemic uncertainty estimates offer a comprehensive per-\nspective on the validity of (instance-based) explanations.\nGoing beyond crisp classification allows to better differenti-\nate explanations, which can be especially insightful when\ndealing with more than two classes, where the interplay\nof their individual probabilities may be nuanced [Sokol\n& Flach, 2020b].\nFurthermore, lack of validity due to\naleatoric uncertainty clearly communicates different infor-\nmation about an explanation than when the uncertainty is\nof epistemic nature. For example, qualifying the validity\nof a counterfactual though its actionability usually requires\nconfining it to features that are deemed mutable and ac-\ncounting for the directionality, monotonicity and rate of\ntheir change as well as between-feature compatibility and\nratio. However, uncertainty estimates – their epistemic com-\nponent in particular – can offer useful insights with this\nregard. These validity and actionability perspectives can be\neasily extended to path-based counterfactuals by inspecting\nuncertainty of the points constituting their individual steps.\nA desirable uncertainty profile of a counterfactual path can\nthus be characterised by low epistemic uncertainty along its\nentirety, low aleatoric uncertainty towards its end, and de-\ncision boundary crossings in regions of increased aleatoric\nuncertainty or a balanced mixture of both uncertainty types.\nRobustness & Stability\nQuantifying the variability of\naleatoric and epistemic uncertainty within a hyper-sphere\n7\n\nAll You Need for Counterfactual Explainability Is Principled and Reliable Estimate of Aleatoric and Epistemic Uncertainty\nplaced around the factual and counterfactual instances, or\nslid along the vector or path linking them, can further im-\nprove explanation robustness and stability, e.g., by reducing\nits sensitivity to model shifts. It can also alleviate the ad-\nverse effects of noisy or imprecise human implementation\nof counterfactuals, which may be out of their control [Xuan\net al., 2024]. Avoiding sharp changes in per-class uncer-\ntainty measured along the vector or path and ensuring that it\nobeys monotonicity constraints such that a decision bound-\nary between any two classes is only crossed once can be\nhighly beneficial as well.\nFeasibility, Plausibility & Connectedness\nValidity ex-\npressed in terms of (epistemic) uncertainty also implicitly\ncaptures explanation feasibility; as a bonus, this approach\nrelaxes the strong assumption that atypical observations are\nimpossible, instead quantifying their improbability. Plau-\nsibility and connectedness can be implemented through un-\ncertainty like this as well – in lieu of direct reliance on\ndata observed before – which also improves explanation\nprivacy [Small et al., 2023].\nDiscriminativeness\nValidity viewed through (aleatoric)\nuncertainty additionally captures and improves explana-\ntion discriminativeness given that low aleatoric uncertainty\naround a counterfactual curbs its ambiguity. Notably, the\nuncertainty perspective naturally generalises this property,\nallowing to explicitly differentiate counterfactual instances\nfrom all the other classes and not only the factual one. Dis-\ncriminativeness reinforces explanation connectedness too,\nmaking counterfactuals not only appealing but also realistic.\nDiversity, Spatiality & Availability\nSimilarity naturally\nlends itself to generating diverse counterfactuals. Addition-\nally, path-based explanations determine the feature change\norder, offering another mechanism for diversity as well as\nimplementation of spatially-aware desiderata. For exam-\nple, uncertainty can indicate steps where paths are likely to\nbranch (aleatoric) and robust decision boundary crossings\n(epistemic). Availability can be viewed via a similar lens.\n3.4. Alternative Views\nReliable uncertainty quantification is undoubtedly challeng-\ning, and in some cases even impossible. Using it as the\nfoundation of XAI may thus curtail the progress of this field,\nat least short-term. Embracing this perspective will also\nlikely require a fundamental shift in how explanations are\nformalised – e.g., they might become probabilistic [Chau\net al., 2023], which can be seen in Figure 4a for a coun-\nterfactual placed in the span of possible models (top-right)\n– increasing the technical complexity of XAI to the detri-\nment of lay explainees. With this in mind, for many (low\nstakes) domains, crisp (ante-hoc interpretable) modelling\nmay be sufficient, making uncertainty estimation redundant.\nHowever, simply considering it can catalyse important in-\nsights even if it is ultimately deemed unnecessary. When\nAI models are provided as inaccessible oracles, i.e., black\nboxes, our perspective is also inapplicable; here, uncertainty\nproxies and post-hoc explainability are largely unavoidable.\nOur strong focus on ante-hoc interpretability is rooted in\nits rich history of success and reported superiority for struc-\ntured data; while the latter claim can be challenged [Wang\n& Lin, 2021], dismissing inherently transparent AI alto-\ngether without further evidence seems premature. One can\nadditionally argue that using auxiliary explanatory mecha-\nnisms to inspect ante-hoc interpretable models inadvertently\nbreaks this property. But in our case this position can be\neasily refuted since the focus remains on reliable data mod-\nelling, with counterfactuals simply being a by-product of\naccess to uncertainty estimates. Crucially, counterfactual\nexplainability can be viewed through frameworks other than\nuncertainty quantification, most notably causality. While\nthis perspective promises more reliable explanations and is\ncompatible with ante-hoc interpretability, causal modelling\nis often unrealistic in practice as mentioned earlier.\n4. Conclusion and Future Work\nIn this paper we argued that uncertainty quantification\nand ante-hoc interpretability are fundamentally intertwined;\nrecognising this synergistic relation promises to benefit com-\nprehensibility of predictive models as well as improve their\naleatoric and epistemic uncertainty. We also asserted that\nuncertainty quantification offers a sound unifying framework\nfor counterfactual explainability; this connection facilitates\nrigorous generation of reliable counterfactuals for different\nmodel classes. Consequently, we observed that counter-\nfactual explainers tend to be overcomplicated since they\nattempt to compensate for various shortcomings of AI mod-\nels. Grounding these tools in uncertainty could allow them\nto remain relatively simple, instead shifting the engineering\nefforts towards building more principled predictive mod-\nels that are ante-hoc interpretable and uncertainty-aware.\nTogether, our two tenets opened a pathway for bringing\nhuman-centred explanatory insights, like counterfactuals,\nto ante-hoc interpretable models, whose comprehensibility\nhas thus far been largely limited to technical experts.\nGiven their prevalence, this paper focused on counter-\nfactuals, nonetheless in future work we will expand our\nuncertainty-centred perspective to other explanation types.\nSince our approach is premised on access to reliable\naleatoric and epistemic uncertainty estimates, we will also\ninvestigate latest uncertainty quantification and probabil-\nity calibration methods, focusing on ante-hoc interpretable\nmodels given their strong connection to this topic. We addi-\ntionally plan to examine the role that other types and sources\n8\n\nAll You Need for Counterfactual Explainability Is Principled and Reliable Estimate of Aleatoric and Epistemic Uncertainty\nof uncertainty play in AI transparency, driving further explo-\nration of uncertainty-aware XAI; in particular, we will look\ninto second-order uncertainty and suitable (probabilistic)\nformalisations of explanations. Such a broad perspective\ncan, among others, improve the robustness and stability of\npredictions and explanations, benefiting (high stakes) do-\nmains like healthcare, where decisions are often made based\non incomplete and uncertain information. More broadly,\nwe will study other fundamental AI concepts that are over-\nlooked in XAI research.\nAcknowledgements\nThis research was supported by the 2024 DAAD Postdoc-\nNeT-AI – Short-term Research Stay programme.\nReferences\nAntor´an, J., Bhatt, U., Adel, T., Weller, A., and Hern´andez-\nLobato, J. M. Getting a CLUE: A method for explaining\nuncertainty estimates. In Proceedings of the International\nConference on Learning Representations, 2021.\nBhatt, U., Antor´an, J., Zhang, Y., Liao, Q. V., Sattigeri,\nP., Fogliato, R., Melanc¸on, G., Krishnan, R., Stanley, J.,\nTickoo, O., Nachman, L., Chunara, R., Srikumar, M.,\nWeller, A., and Xiang, A. Uncertainty as a form of trans-\nparency: Measuring, communicating, and using uncer-\ntainty. In Proceedings of the AAAI/ACM Conference on\nAI, Ethics, and Society, pp. 401–413, 2021.\nBreiman, L. Statistical modeling: The two cultures. Statisti-\ncal Science, 16(3):199–231, 2001.\nChau, S. L., Muandet, K., and Sejdinovic, D. Explaining the\nuncertain: Stochastic Shapley values for Gaussian process\nmodels.\nAdvances in Neural Information Processing\nSystems, 36:50769–50795, 2023.\nDelaney, E., Greene, D., and Keane, M. T. Uncertainty esti-\nmation and out-of-distribution detection for counterfac-\ntual explanations: Pitfalls and solutions. ICML Workshop\non Algorithmic Recourse, 2021.\nDubois, D. and Prade, H. Possibility Theory. Plenum Press,\n1988.\nDuell, J., Fu, H., Seisenberger, M., and Fan, X. QUCE: The\nminimisation and quantification of path-based uncertainty\nfor generative counterfactual explanations. arXiv preprint\narXiv:2402.17516, 2024.\nFreiesleben, T. The intriguing relation between counterfac-\ntual explanations and adversarial examples. Minds and\nMachines, 32(1):77–109, 2022.\nGigerenzer, G. Psychological AI: Designing algorithms\ninformed by human psychology. Perspectives on Psycho-\nlogical Science, pp. 17456916231180597, 2023.\nGoodfellow, I. J., Shlens, J., and Szegedy, C. Explaining and\nharnessing adversarial examples. In Proceedings of the\nInternational Conference on Learning Representations,\n2015.\nGuidotti, R. Counterfactual explanations and how to find\nthem: Literature review and benchmarking. Data Mining\nand Knowledge Discovery, pp. 1–55, 2022.\nH¨ullermeier, E. and Waegeman, W. Aleatoric and epistemic\nuncertainty in machine learning: An introduction to con-\ncepts and methods. Machine Learning, 110(3):457–506,\n2021.\nKanamori, K., Takagi, T., Kobayashi, K., and Ike, Y. Learn-\ning decision trees and forests with algorithmic recourse.\narXiv preprint arXiv:2406.01098, 2024.\nKarimi, A.-H., Barthe, G., Sch¨olkopf, B., and Valera, I. A\nsurvey of algorithmic recourse: Contrastive explanations\nand consequential recommendations. ACM Computing\nSurveys, 55(5):1–29, 2022.\nMiller, T. Explanation in artificial intelligence: Insights\nfrom the social sciences. Artificial Intelligence, 267:1–38,\n2019.\nPawelczyk, M., Broelemann, K., and Kasneci, G. Learning\nmodel-agnostic counterfactual explanations for tabular\ndata. In Proceedings of the Web Conference, pp. 3126–\n3132, 2020.\nPerello-Nieto, M., Silva Filho, T., Kull, M., and Flach, P.\nBackground check: A general technique to build more\nreliable and versatile classifiers. In 16th International\nConference on Data Mining, pp. 1143–1148. IEEE, 2016.\nPoyiadzi, R., Sokol, K., Santos-Rodriguez, R., De Bie, T.,\nand Flach, P. FACE: Feasible and actionable counterfac-\ntual explanations. In Proceedings of the AAAI/ACM Con-\nference on AI, Ethics, and Society, pp. 344–350, 2020.\nRomashov, P., Gjoreski, M., Sokol, K., Martinez, M. V.,\nand Langheinrich, M. BayCon: Model-agnostic Bayesian\ncounterfactual generator. In IJCAI, pp. 740–746, 2022.\nRudin, C. Stop explaining black box machine learning\nmodels for high stakes decisions and use interpretable\nmodels instead. Nature Machine Intelligence, 1(5):206–\n215, 2019.\nRudin, C., Chen, C., Chen, Z., Huang, H., Semenova, L.,\nand Zhong, C. Interpretable machine learning: Funda-\nmental principles and 10 grand challenges. Statistic Sur-\nveys, 16:1–85, 2022.\n9\n\nAll You Need for Counterfactual Explainability Is Principled and Reliable Estimate of Aleatoric and Epistemic Uncertainty\nRudin, C., Zhong, C., Semenova, L., Seltzer, M., Parr, R.,\nLiu, J., Katta, S., Donnelly, J., Chen, H., and Boner,\nZ. Position: Amazing things come from having many\ngood models. In International Conference on Machine\nLearning, pp. 42783–42795. PMLR, 2024.\nRyan, M. In AI we trust: Ethics, artificial intelligence,\nand reliability. Science and Engineering Ethics, 26(5):\n2749–2767, 2020.\nSchut, L., Key, O., Mc Grath, R., Costabello, L., Sacaleanu,\nB., and Gal, Y. Generating interpretable counterfactual\nexplanations by implicit minimisation of epistemic and\naleatoric uncertainties. In International Conference on Ar-\ntificial Intelligence and Statistics, pp. 1756–1764. PMLR,\n2021.\nShafer, G. A Mathematical Theory of Evidence. Princeton\nUniversity Press, 1976.\nShwartz-Ziv, R. and Armon, A. Tabular data: Deep learning\nis not all you need. Information Fusion, 81:84–90, 2022.\nSilva Filho, T., Song, H., Perello-Nieto, M., Santos-\nRodriguez, R., Kull, M., and Flach, P. Classifier cal-\nibration: A survey on how to assess and improve pre-\ndicted class probabilities. Machine Learning, 112(9):\n3211–3260, 2023.\nSmall, E. A., Clark, J. N., McWilliams, C. J., Sokol, K.,\nChan, J., Salim, F. D., and Santos-Rodriguez, R. Coun-\nterfactual explanations via locally-guided sequential al-\ngorithmic recourse. arXiv preprint arXiv:2309.04211,\n2023.\nSmets, P. and Kennes, R. The transferable belief model.\nArtificial Intelligence, 66(2):191–234, 1994.\nSokol, K. and Flach, P.\nExplainability fact sheets: A\nframework for systematic assessment of explainable ap-\nproaches. In Proceedings of the Conference on Fairness,\nAccountability, and Transparency, pp. 56–67, 2020a.\nSokol, K. and Flach, P.\nLIMEtree:\nConsistent and\nfaithful multi-class explanations.\narXiv preprint\narXiv:2005.01427, 2020b.\nSokol, K. and Flach, P. One explanation does not fit all: The\npromise of interactive explanations for machine learning\ntransparency. KI-K¨unstliche Intelligenz, 34(2):235–250,\n2020c.\nSokol, K. and Vogt, J. E. (Un)reasonable allure of ante-hoc\ninterpretability for high-stakes domains: Transparency\nis necessary but insufficient for comprehensibility. In\nICML 3rd Workshop on Interpretable Machine Learning\nin Healthcare, 2023.\nSokol, K. and Vogt, J. E. What does evaluation of explain-\nable artificial intelligence actually tell us? A case for\ncompositional and contextual validation of XAI building\nblocks. In Extended Abstracts of the CHI Conference on\nHuman Factors in Computing Systems, pp. 1–8, 2024.\nSokol, K., Small, E., and Xuan, Y. Navigating explanatory\nmultiverse through counterfactual path geometry. ICML\nWorkshop on Counterfactuals in Minds and Machines,\n2023.\nSokol, K., Kull, M., Chan, J., and Salim, F. Cross-model fair-\nness: Empirical study of fairness and ethics under model\nmultiplicity. ACM Journal on Responsible Computing, 1\n(3):1–27, 2024.\nThiagarajan, J. J., Thopalli, K., Rajan, D., and Turaga, P.\nTraining calibration-based counterfactual explainers for\ndeep learning models in medical image analysis. Scien-\ntific Reports, 12(1):597, 2022.\nTomsett,\nR.,\nPreece,\nA.,\nBraines,\nD.,\nCerutti,\nF.,\nChakraborty, S., Srivastava, M., Pearson, G., and Ka-\nplan, L. Rapid trust calibration through interpretable and\nuncertainty-aware AI. Patterns, 1(4), 2020.\nWachter, S., Mittelstadt, B., and Russell, C. Counterfactual\nexplanations without opening the black box: Automated\ndecisions and the GDPR. Harvard Journal of Law &\nTechnology, 31:841, 2017.\nWalley, P. Statistical Reasoning with Imprecise Probabil-\nities. Monographs on Statistics & Applied Probability.\nChapman & Hall, 1991.\nWang, T. and Lin, Q. Hybrid predictive models: When an\ninterpretable model collaborates with a black-box model.\nJournal of Machine Learning Research, 22(137):1–38,\n2021.\nWood, D., Papamarkou, T., Benatan, M., and Allmendinger,\nR. Model-agnostic variable importance for predictive\nuncertainty: An entropy-based approach. Data Mining\nand Knowledge Discovery, 38(6):4184–4216, 2024.\nXuan, Y., Sokol, K., Sanderson, M., and Chan, J. Perfect\ncounterfactuals in imperfect worlds: Modelling noisy\nimplementation of actions in sequential algorithmic re-\ncourse. arXiv preprint arXiv:2410.02273, 2024.\n10\n",
  "metadata": {
    "source_path": "papers/arxiv/All_You_Need_for_Counterfactual_Explainability_Is_Principled_and\n__Reliable_Estimate_of_Aleatoric_and_Epistemic_Uncertainty_f8f45ae82c531639.pdf",
    "content_hash": "f8f45ae82c531639459196c0d8f3b849b3ab5937df92fb0df744c2b236124107",
    "arxiv_id": null,
    "title": "All You Need for Counterfactual Explainability Is Principled and Reliable Estimate of Aleatoric and Epistemic Uncertainty",
    "author": "Kacper Sokol, Eyke Hüllermeier",
    "creation_date": "D:20250225023636Z",
    "published": "2025-02-25T02:36:36",
    "pages": 10,
    "size": 2515601,
    "file_mtime": 1740470198.2239923
  }
}