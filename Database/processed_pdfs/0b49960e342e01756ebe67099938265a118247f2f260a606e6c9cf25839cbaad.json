{
  "text": "WILDFRAME: Comparing Framing in Humans and LLMs\non Naturally Occurring Texts\nGili Lior\nLiron Naccache\nGabriel Stanovsky\nThe Hebrew University of Jerusalem\n{gili.lior,gabriel.stanovsky}@mail.huji.ac.il\nAbstract\nHumans are influenced by how information is\npresented, a phenomenon known as the fram-\ning effect. Previous work has shown that LLMs\nmay also be susceptible to framing but has done\nso on synthetic data and did not compare to\nhuman behavior. We introduce WILDFRAME,\na dataset for evaluating LLM responses to\npositive and negative framing, in naturally-\noccurring sentences, and compare humans on\nthe same data. WILDFRAME consists of 1,000\ntexts, first selecting real-world statements with\nclear sentiment, then reframing them in either\npositive or negative light, and lastly, collecting\nhuman sentiment annotations. By evaluating\neight state-of-the-art LLMs on WILDFRAME,\nwe find that all models exhibit framing effects\nsimilar to humans (r ≥0.57), with both hu-\nmans and models being more influenced by\npositive rather than negative reframing. Our\nfindings benefit model developers, who can ei-\nther harness framing or mitigate its effects, de-\npending on the downstream application.\n1\nIntroduction\nThe framing effect is a well-known cognitive phe-\nnomenon, where different presentations of the same\nunderlying facts affect human perception towards\nthem (Tversky and Kahneman, 1981). For exam-\nple, presenting an economic policy as only creating\n50,000 new jobs, versus also reporting that it would\ncost 2B USD, can dramatically shift public opin-\nion (Sniderman and Theriault, 2004).\nPrevious research has shown that LLMs exhibit\nvarious cognitive biases, including the framing ef-\nfect (Lorè and Heydari, 2024; Shaikh et al., 2024;\nMalberg et al., 2024; Echterhoff et al., 2024). How-\never, these either rely on synthetic datasets or eval-\nuate LLMs on different data from what humans\nwere tested on. In addition, comparisons between\nmodels and humans typically treat human perfor-\nmance as a baseline rather than comparing patterns\nin human behavior.\nFigure 1: The WILDFRAME data construction process.\nIn step (a) we extract statements based on their syntactic\nstructure, aiming for statements with clear negative or\npositive sentiment. Next, in (b), we reframe the state-\nment by adding a suffix or prefix, conveying the opposite\nsentiment. Finally, in (c), five annotators mark the sen-\ntiment of the reframed statement, counting how many\nannotators shift sentiment, i.e., the reframed statement\nsentiment is opposite to the base sentiment. The red\nparts in the figure represent negative parts of statement,\nwhile green represents positive parts.\nIn this work, we evaluate LLMs on real-world\ndata. Rather than measuring model performance\nin terms of accuracy, we analyze how closely their\nresponses align with human annotations. Further-\nmore, while previous studies have examined the\neffect of framing on decision making, we extend\nthis analysis to sentiment analysis, as sentiment\nperception plays a key explanatory role in decision-\nmaking (Lerner et al., 2015).\nTo better understand the framing effect in LLMs\nin comparison to human behavior, we introduce\nthe WILDFRAME dataset (Section 2), comprising\n1,000 statements, constructed through a three-step\nprocess, as shown in Figure 1. First, we collect\na set of real-world statements that express a clear\nnegative or positive sentiment (e.g., “I won the\n1\narXiv:2502.17091v1  [cs.CL]  24 Feb 2025\n\nhighest prize”). Second, we reframe the text by\nadding a prefix or suffix with an opposite sentiment\n(e.g., “I won the highest prize, although I lost all\nmy friends on the way”). Finally, we collect hu-\nman annotations by asking different participants\nif they consider the reframed statement to be over-\nall positive or negative. We choose to annotate\nAmazon reviews, where sentiment is more robust,\ncompared to e.g., the news domain which intro-\nduces confounding variables such as prior political\nleaning (Druckman, 2004).\nIn Section 3, we evaluate eight state-of-the-art\nLLMs on the WILDFRAME dataset and compare\nthem against human annotations.\nWe find that\nLLMs are influenced by framing, somewhat similar\nto human behavior. All models show a strong corre-\nlation (r > 0.57) with human behavior. Moreover,\nwe find that both humans and LLMs are more in-\nfluenced by positive reframing rather than negative\nreframing. We also find that larger models tend to\nbe more correlated with human behavior. Interest-\ningly, GPT-4o shows the lowest correlation with\nhuman behavior. This raises questions about how\narchitectural or training differences might influence\nsusceptibility to framing.\nThis work contributes to understanding the par-\nallels between LLM and human cognition, offering\ninsights into how cognitive mechanisms such as\nthe framing effect emerge in LLMs.1\n2\nThe WILDFRAME Dataset\nOur dataset curation consists of three steps, as de-\npicted in Figure 1. First, we collect natural, real-\nworld statements, with some clear sentiment, either\npositive or negative (§2.1; e.g., “I won the highest\nprize” as positive). Next, we reframe each state-\nment by adding a prefix or suffix conveying the\nopposite sentiment (§2.2; e.g., “I won the highest\nprize, although I lost all my friends on the way”).\nFinally, we collect large-scale human annotations\nvia crowdsourcing, to label the sentiment shifts\nwhen wrapping the statements with the opposite\nframing (§2.3; e.g., labeling “negative” the state-\nment “I won the highest prize, although I lost all\nmy friends on the way”).\nThe complete dataset consists of 1000 state-\nments, in which 500 are statements that their base\nform has positive sentiment, and 500 are base neg-\native statements.\n1WILDFRAME data available at https://huggingface.\nco/datasets/gililior/WildFrame\nCode: https://github.com/SLAB-NLP/WildFrame-Eval\nFigure 2: Distribution of sentiment scores before and\nafter applying opposite-sentiment framing, as detailed\nin Section 2.2. Prior to framing, base sentences exhibit a\nclear polarity (positive or negative), whereas the applica-\ntion of opposite framing introduces ambiguity, shifting\nthe sentiment scores toward a less distinct polarity.\n2.1\nCollecting Base Statements\nFirst, we collect base statements, which convey a\nclear sentiment, either clearly positive or clearly\nnegative statements. We use SPIKE – an extractive\nsearch system, which allows to extract statements\nfrom real-world datasets (Taub Tabib et al., 2020).\nSpecifically, we collect statements from Amazon\nReviews dataset, which are naturally occurring,\nsentiment-rich, texts but are less likely to trigger\nstrong preexisting biases or emotional reactions,\nwhich may be a confound for our experiment.2\nUsing SPIKE, we extract ∼6k statements that\nfulfilled our designated queries, which we found\ncorrelated with clear sentiment. We designed the\nqueries to capture positive or negative verbs that\ndescribe actions with some clear sentiment (e.g.,\n“enjoy” or “waste”), or statements with positive or\nnegative adjective, describing an outcome with a\nclear sentiment (e.g., “good” or “nasty”). The pat-\nterns and queries used for extraction are detailed\nin Appendix A. Next, we run in-house annotations\nto label and filter the extracted statements, to han-\ndle negations or other cases where the statement\ndoes not convey a clear sentiment. The filtering\nprocess results in 1, 301 positive statements, and\n1, 229 negative statements.\n2.2\nAdding Framing\nTo reframe the statements in our dataset, we use\nGPT-4 (Achiam et al., 2023).3 The input prompt\nincludes a 1-shot example, followed by a task de-\n2\nhttps://spike.apps.allenai.org/datasets/\nreviews\n3We used the gpt-4-0613 version.\n2\n\nscription “Add a <SENTIMENT> suffix or prefix\nto the given statement. Don’t change the original\nstatement.”, where SENTIMENT is either “posi-\ntive” or “negative”, opposite to the base statement\nsentiment (i.e., positive framing for negative base\nstatement, and vice versa).\nUnlike the base statement, the conveying senti-\nment of reframed statements is more ambiguous\nand there is no one clear label, as shown in Fig-\nure 2.4 The exhibeted ambiguity in sentiment al-\nlows us to measure to what extent LLMs’ shifting\nsentiment after framing, and how correlated it is to\nhuman behavior.\n2.3\nCollecting Human Annotations\nIn the final step, we collect human annotations\nthrough Amazon Mechanical Turk to evaluate the\nframing effect in WILDFRAME over human partic-\nipants, providing a reference for comparison with\nLLMs.5 Details about the annotation platform are\nelaborated in Appendix C.\nThe complete dataset includes 1K statements,\neach annotated by five different annotators. Given\nour budget, we preferred to collect five annotations\nper statement, resulting in less statements, but pro-\nviding a more robust scoring for the ambiguity of a\nstatement.\nFor the annotation process, each statement in\nour dataset is presented in its reframed version\n(i.e., positive base statements with negative framing\nand vice versa), to five different annotators. This\nsetup generates, for each dataset instance, a score\nranging from 0 to 5, representing the number of\nannotators that votes for the sentiment that aligns\nwith the opposite framing, which means that the\noverall sentiment of the reframed statement has\nshifted from its base sentiment. For example, in\nFigure 1, the statement “I won the highest prize,\nalthough I lost all my friends on the way” is shown\nto have two annotators voting “negative”, which\naligns with the sentiment of the framing and not\nthe base statement, so the label for that instance in\nWILDFRAME would be 2 (sentiment shifts).\nInstances with score near 0 indicate that anno-\ntators agree that the overall sentiment remains un-\nchanged despite the opposite framing. Score closer\nto 5 indicates that annotators agree that reframing\nshifted the perceived sentiment, while score around\n4Scores in Figure 2 are given by a fine-tuned sentiment\nanalysis model\nhttps://huggingface.co/cardiffnlp/\ntwitter-roberta-base-sentiment-latest\n5https://www.mturk.com\nFigure 3: Percentage of reframed statements that results\nin sentiment shift positive to negative or vice versa).\nRed represents negative-base statements reframed as\npositive, and green represents positive-base statements\nreframed as negative. Horizontal lines show the mean\nacross models. We find that both LLMs and humans are\ninfluenced by opposite framing, with a stronger effect\nfor positive reframing.\n2-3 suggests that the opposite framing makes the\nsentiment ambiguous.\n3\nResults\nWe evaluate 8 models on the WILDFRAME dataset,\ncomprising both open and closed-source mod-\nels. These included GPT-4o, Llama-3 (8B and\n70B), Mistral-v0.3 (7B), Mixtral-v0.1 (8x7B and\n8x22B), and Gemma-2 (9B and 27B). We test the\ninstruction-tuned version of these models.\nHumans and LLMs are largely more influenced\nby positive framing applied to negative state-\nments than by negative framing applied to posi-\ntive statements.\nThis trend is evident in Figure 3.\nFor LLMs, all models except GPT-4o exhibit higher\nratios of sentiment shifts for statements that were\noriginally negative (red bars) compared to those\nthat were originally positive (green bars). For hu-\nmans, we count the cases in which the majority\nvoted for a sentiment shift (3 annotators or more).\nThis finding aligns with Tong et al. (2021), which\ndemonstrated that positive framing is more effec-\ntive on humans when spatial distance is minimal.\nIn WILDFRAME, all statements are presented in\na first-person perspective (e.g., “I won the high-\nest prize”), creating zero spatial distance from the\nannotator’s perception and thereby amplifying the\nimpact of positive framing.\nModel size partially correlates with similarity to\nhuman behavior under opposite framings.\nIn\n3\n\nFigure 4, we present the correlation between each\nmodel’s behavior and human responses. For the\nLlama-3 and Mistral model families, we observe\na trend where larger models with more parameters\nexhibit higher correlation with human behavior re-\ngarding the framing effect. However, the Gemma-2\nfamily shows the opposite trend. This discrepancy\nhighlights the need for further investigation into\nthe relationship between model size, the framing\neffect, and its alignment with human behavior.\nOur results imply that annotators did not rely\non LLMs to complete their tasks.\nRecent dis-\ncussions have raised concerns that annotations ob-\ntained via crowdsourcing platforms like Mechani-\ncal Turk might not reflect genuine human input, as\nworkers could potentially use LLM-generated re-\nsponses to complete their HITs (Veselovsky et al.,\n2023). However, as shown in Figure 4, human\nannotations are not perfectly correlated with ei-\nther open-source or closed-source LLM outputs.\nThis strongly supports the conclusion that the an-\nnotations in WILDFRAME are genuine human re-\nsponses, reflecting actual human behavior.\nGPT-4o has the weakest correlation with other\nmodels.\nIn Figure 7 in the Appendix we show\nthe correlation between each pair of models. While\nmost models exhibit correlation coefficients greater\nthan 0.5 with each other, GPT-4o falls below this\nthreshold with many models, resulting in the lowest\naverage correlation to others. This discrepancy\nmay stem from the fact that GPT-4o is a closed-\nsource model, potentially developed independently\nof the open-source models that often influence each\nFigure 4: Pearson correlation coefficients between\nhuman sentiment shifts and predictions from vari-\nous LLMs after applying opposite sentiment framing.\nHigher values indicate stronger alignment between the\nmodel’s behavior and human annotations.\nother’s design. However, the exact reasons remain\nunclear due to the proprietary nature of GPT-4o’s\ntraining process.\nGPT-4o exhibits the lowest correlation with hu-\nman behavior.\nIn Figure 4 we find that GPT-4o\nexhibits the lowest correlation with human behav-\nior. We find this result surprising, given that GPT-\n4o is considered SOTA compared to open-source\nmodels. This finding highlights a fundamental\nquestion about the objectives in LLM development –\nspecifically, diffrentiate when human-like behavior\nis desired, and when other factors take precedence.\n4\nDiscussion and Future Work: When\nshould LLMs Imitate Humans?\nWe introduced WILDFRAME, a dataset designed to\nevaluate how LLMs are affected by different fram-\nings, in comparison to humans. The statements in\nthe dataset are start from a clear positive or neg-\native base statement, reframed with an opposite-\nsentiment suffix or prefix. By collecting human\nannotations, we quantify the strength of the fram-\ning effect for each instance and assess how LLMs\nreact to the same reframing, focusing on sentiment\nshifts – cases where the perceived sentiment aligns\nwith the opposite framing sentiment.\nAs LLMs become increasingly integrated into\ndecision-making systems, it becomes crucial to\nunderstand how framing influences their outputs.\nIn some applications, e.g., in virtual companions,\nframing can be harnessed to produce human-like\nbehavior leading to better engagement. In contrast,\nin other applications, such as financial or legal ad-\nvice, mitigating the framing effect can lead to less\nbiased decisions. In both cases, a better under-\nstanding of framing in LLMs can help develop\napplication-appropriate strategies.\nWe find that LLMs and humans exhibit similar\nbehavior on WILDFRAME, with all tested models\nachieving a strong correlation (r ≥0.57) with hu-\nman responses. Notably, GPT-4o showed the weak-\nest correlation to humans, despite being widely\nregarded as a very capable model in other contexts.\nWe hope this work encourages research into\ndistinguishing between scenarios where human-\nlike behavior is desirable and those where models\nshould surpass human limitations to achieve above-\nhuman performance. These insights are essential\nfor developing LLMs that are both interpretable\nand optimized for their intended applications.\n4\n\n5\nLimitations\nIn this work, we address a cognitive bias, and as\nwith any research involving human participants,\nour study has several limitations.\nFirst, our framing experiment is conducted\nwithin a single domain – Amazon reviews – and\nfocuses on a specific type of statement. Some of\nour findings may be artifacts of this dataset rather\nthan generalizable patterns.\nAdditionally, our approach to framing is highly\nspecific. We only manipulate statements by adding\na prefix or suffix, whereas reframing can take many\nother forms, such as restructuring sentences or alter-\ning word choices to convey ambiguous sentiment.\nThis may limit the generalizability of our results.\nFurthermore, our study focuses solely on senti-\nment analysis. Other downstream tasks influenced\nby framing, such as question answering or decision-\nmaking, may exhibit different patterns of sensitiv-\nity. Investigating these tasks could provide further\ninsights into the broader impact of framing on LLM\nbehavior in real-world applications.\nReferences\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, et al. 2023. Gpt-4 technical report.\narXiv preprint arXiv:2303.08774.\nJames N Druckman. 2004. Political preference forma-\ntion: Competition, deliberation, and the (ir) relevance\nof framing effects. American political science review,\n98(4):671–686.\nJessica Maria Echterhoff, Yao Liu, Abeer Alessa, Ju-\nlian McAuley, and Zexue He. 2024. Cognitive bias\nin decision-making with LLMs. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2024, pages 12640–12653, Miami, Florida, USA.\nAssociation for Computational Linguistics.\nJennifer S Lerner, Ye Li, Piercarlo Valdesolo, and\nKarim S Kassam. 2015. Emotion and decision mak-\ning. Annual review of psychology, 66(1):799–823.\nNunzio Lorè and Babak Heydari. 2024. Strategic be-\nhavior of large language models and the role of game\nstructure versus contextual framing. Scientific Re-\nports, 14(1):18490.\nSimon Malberg, Roman Poletukhin, Carolin M Schus-\nter, and Georg Groh. 2024. A comprehensive eval-\nuation of cognitive biases in llms. arXiv preprint\narXiv:2410.15413.\nAmmar Shaikh, Raj Abhijit Dandekar, Sreedath Panat,\nand Rajat Dandekar. 2024. Cbeval: A framework for\nevaluating and interpreting cognitive biases in llms.\narXiv preprint arXiv:2412.03605.\nPaul M Sniderman and Sean M Theriault. 2004. The\nstructure of political argument and the logic of issue\nframing. Studies in public opinion: Attitudes, nonatti-\ntudes, measurement error, and change, 3(03):133–65.\nHillel Taub Tabib, Micah Shlain, Shoval Sadde, Dan\nLahav, Matan Eyal, Yaara Cohen, and Yoav Gold-\nberg. 2020. Interactive extractive search over biomed-\nical corpora.\nIn Proceedings of the 19th SIG-\nBioMed Workshop on Biomedical Language Process-\ning, pages 28–37, Online. Association for Computa-\ntional Linguistics.\nZelin Tong, Diyi Liu, Fang Ma, and Xiaobing Xu. 2021.\nGood news or bad news? how message framing influ-\nences consumers’ willingness to buy green products.\nFrontiers in Psychology, 11:568586.\nAmos Tversky and Daniel Kahneman. 1981. The fram-\ning of decisions and the psychology of choice. sci-\nence, 211(4481):453–458.\nVeniamin Veselovsky, Manoel Horta Ribeiro, and\nRobert West. 2023. Artificial artificial artificial intel-\nligence: Crowd workers widely use large language\nmodels for text production tasks.\narXiv preprint\narXiv:2306.07899.\nA\nExtracting data with SPIKE\nWe found two patterns of statements, which can\nconvey a clear sentiment, and built queries upon\nthese patterns to extract statements from SPIKE.\nExamples for all types of statements are presented\nin Table 1.\nFirst, are statements in which the verb in the\nstatements is a verb with clear sentiment, that of-\nten implies the sentiment of the entire statement.\nE.g., ‘wastes’, ‘rejects’, ‘fails’ are negative verbs,\nwhile verbs like ‘enjoys’, ‘succeeds’, ‘empowers’,\nconveys positive statements.\nThe second pattern of statements that we found\nsuitable for conveying a clear sentiment, are state-\nments which describe some event/action, and\nits consequences, where often the adjective that\ndescribes the consequences holds information\nwhether it is positive or negative.\nNext, we needed to label and filter them due to\ntwo main issues. First, we needed to handle the\ncases in which negation words appear in the state-\nment and flips the sentiment. For example, a state-\nment like “We did not enjoy the show” includes\na positive verb (enjoy), but the negation flips its\nsentiment to be a negative statement. Another issue\nwe encountered is that there are many statements\nwhich are irrelevant to our case, even though they\n5\n\nmatch the positive/negative patterns, for example\n“I couldn’t sympathize with the shopping aspect of\nthe book since I hate to shop .” does not convey any\nclear sentiment, despite the use of the verb ‘hate’.\nA.1\nSPIKE Queries\n1. :something :[pos/neg verbs]develops\n2. :something :[pos/neg adjectives]badly :[cause\nsynonym]causes :something\nA.2\nWord Lists\nPositive verbs.\nachieve, admire, affirm, appreci-\nate, aspire, awe, bless, blossom, celebrate, cherish,\ncomfort, contribute, delight, donate, elevate, em-\npower, enchant, encourage, energize, engage, en-\njoy, enrich, enthuse, excel, fervor, flourish, fortify,\nglisten, glow, gratitude, grow, harmonize, heal, illu-\nminate, innovate, inspire, invigorate, laugh, learn,\nliberate, love, motivate, nourish, nurture, praise,\nprosper, radiate, rally, refresh, rejoice, renew, revel,\nrevere, revitalize, savor, shine, smile, soar, spark,\nsparkle, stimulate, strengthen, succeed, support,\nsynergize, thrive, unite, uplift, volunteer, adore,\namaze, boost, captivate, win.\nNegative verbs.\nabandon, abuse, accuse, alien-\nate, begrudge, betray, bewilder, blame, collapse,\ncomplain, condemn, confuse, contradict, criticize,\ndecay, deceive, decline, defeat, demoralize, deny,\ndespair, destroy, deteriorate, devalue, discourage,\ndiscriminate, dishearten, dismantle, dismiss, dis-\nsolve, doubt, exploit, fail, falter, fear, frustrate,\ngrieve, harass, hate, hurt, ignore, inhibit, intimidate,\nlose, mock, overlook, overwhelm, pollute, punish,\nregress, reject, repress, resent, sabotage, shatter,\nsicken, stifle, suffer, suffocate, suppress, terrorize,\ntorment, undermine, violate, waste, weaken, whine,\nwithdraw, withhold, worry.\nPositive adjectives.\nadmirable, lucky, enjoyable,\nmagnificent, enthusiastic, marvelous, euphoric,\namazing, excellent, exceptional, amused, excited,\namusing, extraordinary, nice, noble, outstanding,\nappreciative, fabulous, overjoyed, astonishing, fan-\ntastic, benevolent, fortunate, pleasant, blissful, plea-\nsurable, brilliant, positive, glad, prominent, good,\nproud, charming, cheerful, reliable, gracious, grate-\nful, clever, great, happy, superb, superior, terrific,\nincredible, tremendous, inspirational, delighted,\ndelightful, joyful, joyous, uplifting, wonderful,\nlovely.\nNegative adjectives.\nsad, angry, upset, disgust-\ning, boring, disappointing, frustrating, annoying,\nmiserable, terrible, deppressing, unhappy, melan-\ncolic, heartbreaking, Furious, iritating, emberess-\ning, horrible, stupid, unlucky, negative, bad.\n“Causes” synonym.\ncauses, creates, generates,\nprompts, produces, induces, yields, affects, in-\nvokes, effectuates, results, encourages, promotes,\nintroduces, begets, engenders, occasions, devel-\nops, starts, contributes, initiates, inaugurates, es-\ntablishes, begins, cultivates, acquires, provides,\nlaunches.\nB\nAdding Framing\nExample for statements after framing are presented\nin in Table 2.\nB.1\nFraming Prompts\n1. “Here is an example of a base statement with\na negative sentiment: I failed my math test to-\nday. Here is the same statement, after adding\na positive framing: I failed my math test today,\nhowever I see it as an opportunity to learn and\nimprove in the future. Here is a negative state-\nment: <statement> Like the example, add a\npositive suffix or prefix to it. Don’t change\nthe original statement.”\n2. “Here is an example of a base statement with\na positive sentiment: I got an A on my math\ntest. Here is the same statement, after adding\na negative framing: I got an A on my math\ntest. I think I spent too much time learning to\nit though. Here is a positive statement: <state-\nment>. Like the example, add a negative suf-\nfix or prefix to it. Don’t change the original\nstatement.”\nC\nAnnotation Platform\nWe select a pool of 10 qualified workers who suc-\ncessfully passed our qualification test, which con-\nsisted of 20 base statements (unframed), for which\nannotators were expected to achieve perfect accu-\nracy. The estimated hourly wage for the entire\nexperiment was approximately 14USD per hour.\nScreenshot of the annotation platform is pre-\nsented in Figure 5.\n6\n\nCategory\nExample Sentence\nPositive Verb\n“To my surprise I did enjoy the book and the characters .”\nNegative Verb\n“This dock has done nothing but provide frustration and waste a great deal of my time trying to get it to work properly .”\nPositive Outcome\n“This bag provides good protection for my snare drum at a really good price .”\nNegative Outcome\n“For me , Aspartame causes bad memory loss and nasty gastrointestinal distress .”\nTable 1: Examples for base statements collected using SPIKE. The words that inflect the sentiment are in bold.\nBase Sentence\nBase Sentiment\nOpposite Framing Sentence\n“To my surprise I did enjoy the book and the characters .”\nPositive\n“To my surprise I did enjoy the book and the characters, even though\nit had a disappointing ending. ”\n“For me , Aspartame causes bad memory loss and nasty\ngastrointestinal distress .”\nNegative\n“For me, Aspartame causes bad memory loss and nasty gastrointestinal\ndistress, but this has encouraged me to seek out healthier, natural\nalternatives and cultivate a balanced diet .”\nTable 2: Sentences after framing. Positive sentences are added with negative framing, and vice-versa. The opposite\nframing is in bold.\nD\nModels\nWe ran the open models via together-ai API.6 The\nlist of models we used are:\n• \"google/gemma-2-9b-it\"\n• \"google/gemma-2-27b-it\"\n• \"mistralai/Mistral-7B-Instruct-v0.3\"\n• \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n• \"mistralai/Mixtral-8x22B-Instruct-v0.1\"\n• \"meta-llama/Llama-3-8b-chat-hf\"\n• \"meta-llama/Llama-3-70b-chat-hf\"\nFor GPT-4o, we used the OpenAI api, with \"gpt-\n4o-2024-08-06\".7\n6https://www.together.ai\n7https://platform.openai.com/docs/overview\n7\n\nFigure 5: Screenshot of the annotation platform.\n(a) Sentences that are negative in their original form.\n(b) Sentences that are positive in their original form.\nFigure 6: Proportion of sentences for which LLMs flipped sentiment, became neutral, or retained the original\nsentiment when presented with opposite sentiment framing. For example, this measures the percentage of sentences\noriginally labeled as positive, that were labeled as negative after applying negative framing (and vice versa).\n8\n\nFigure 7: Pairwise Pearson correlation coefficients be-\ntween predictions from different models, indicating the\ndegree of similarity in their behavior under opposite\nsentiment framing scenarios.\nFigure 8: Proportions of sentences where annotators\nagreed on the extent of sentiment shift after applying\nopposite sentiment framing. The bars represent the\npercentage of sentences with 0 to 5 annotators agreeing\non a sentiment shift.\n9\n",
  "metadata": {
    "source_path": "papers/arxiv/WildFrame_Comparing_Framing_in_Humans_and_LLMs_on_Naturally_Occurring\n__Texts_0b49960e342e0175.pdf",
    "content_hash": "0b49960e342e01756ebe67099938265a118247f2f260a606e6c9cf25839cbaad",
    "arxiv_id": null,
    "title": "WildFrame_Comparing_Framing_in_Humans_and_LLMs_on_Naturally_Occurring\n__Texts_0b49960e342e0175",
    "author": "",
    "creation_date": "D:20250225024328Z",
    "published": "2025-02-25T02:43:28",
    "pages": 9,
    "size": 633219,
    "file_mtime": 1740470187.683289
  }
}