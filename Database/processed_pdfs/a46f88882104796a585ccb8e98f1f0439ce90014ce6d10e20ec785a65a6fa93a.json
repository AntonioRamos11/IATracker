{
  "text": "Language Model Re-rankers are Steered by Lexical Similarities\nLovisa Hagström1,2 Ercong Nie3,4 Ruben Halifa5\nHelmut Schmid3 Richard Johansson1,2 Alexander Junge5\n1Chalmers University of Technology\n2University of Gothenburg\n3LMU Munich\n4Munich Center for Machine Learning\n5amass technologies\nlovhag@chalmers.se\nAbstract\nLanguage model (LM) re-rankers are used to\nrefine retrieval results for retrieval-augmented\ngeneration (RAG). They are more expensive\nthan lexical matching methods like BM25 but\nassumed to better process semantic information.\nTo understand whether LM re-rankers always\nlive up to this assumption, we evaluate 6 dif-\nferent LM re-rankers on the NQ, LitQA2 and\nDRUID datasets. Our results show that LM re-\nrankers struggle to outperform a simple BM25\nre-ranker on DRUID. Leveraging a novel sep-\naration metric based on BM25 scores, we ex-\nplain and identify re-ranker errors stemming\nfrom lexical dissimilarities. We also investigate\ndifferent methods to improve LM re-ranker per-\nformance and find these methods mainly useful\nfor NQ. Taken together, our work identifies\nand explains weaknesses of LM re-rankers and\npoints to the need for more adversarial and re-\nalistic datasets for their evaluation.\n1\nIntroduction\nRetrieval-augmented generation (RAG) is used to\nalleviate problems arising from imperfect paramet-\nric knowledge of language models (LMs) (Gao\net al., 2024; Vu et al., 2024). However, the effi-\nciency of RAG hinges on the retrieval of useful\ninformation (Wang et al., 2024b). To this end, LM\nre-rankers are increasingly used to provide more\naccurate retrieval results for RAG, superseding sim-\npler methods based on keyword matching, such\nas BM25 (see Figure 1). While there are many\nbenchmark results for LM re-rankers (Thakur et al.,\n2021; Petroni et al., 2021), few extensive inspec-\ntions of LM re-rankers have been performed. Little\nis known about when the computationally expen-\nsive LM re-rankers are worth the cost and whether\nthey always can be expected to outperform simpler\nmethods.\nIn this paper, we evaluate LM re-rankers to better\nunderstand when they work well and when they\nFigure 1: An overview of a RAG pipeline.\nfail to outperform less expensive alternatives. The\ncontributions of this paper are as follows:\n• We evaluate 6 LM re-rankers of varying design\non the NQ, LitQA2 and DRUID datasets to com-\npare re-ranker performance for scenarios of vary-\ning aspects of difficulty and domain.\n• We explain variations in LM re-ranker perfor-\nmance using passage-query similarities, leverag-\ning BM25 scores and our novel separation metric\nDS. All LM re-rankers underperform on samples\ncorresponding to low DS values and we tie these\nto high rates of distractors (non-gold passages\nwith high lexical similarity to the query) and lack\nof document context.\n• We evaluate a set of methods for improving LM\nre-ranker performance, such as adding contextual\ninformation. Our results show that while most\nmethods work well on NQ, they are less effective\nfor LitQA2 and DRUID.\nTaken together, our paper identifies and measures\nnovel aspects of difficulty for LM re-rankers; dis-\ntractors and lack of contextual information. These\naspects are likely to occur in real-world scenar-\nios relying on e.g. information retrieval from the\nInternet. Our work points to the need of more ad-\nversarial and real-world aligned evaluation datasets\nto better understand and address LM re-ranker fal-\nlacies related to the identified aspects of difficulty.\n1\narXiv:2502.17036v1  [cs.CL]  24 Feb 2025\n\n2\nRelated Work\nThe goal of using a re-ranker in an information\nretrieval context is to refine the outputs of an ini-\ntial retrieval step based on a lexicographical or se-\nmantic database search. LM-based re-rankers are\nmore expensive to run compared to simpler meth-\nods based on lexical matching, like BM25, but\nare expected to increase the performance of the\noverall retrieval system thanks to their semantic\nunderstanding (Glass et al., 2022; Li et al., 2023).\nSun et al. (2023) also showed how standard LLMs,\nlike GPT-4, can be used as re-rankers.\nTwo popular benchmarks for re-rankers are the\nBEIR and KILT benchmarks by Thakur et al.\n(2021); Petroni et al. (2021). Compared to our\nwork, these benchmarks focus on high-level re-\nranker performance and do not consider fine-\ngrained aspects of difficulty for re-rankers.\nSimilarly to our work, Sturua et al. (2024) iden-\ntify and investigate fine-grained aspects of diffi-\nculty for their jina models, of which one is mis-\nleading syntactic similarities. This describes the\ncase when passages with high syntactic similarity\nto the query are favoured over gold documents with\nlower syntactic overlap. Henceforth referred to as\ndistractors. Wang et al. (2024a) instead consider\nan aspect of difficulty related to missing document\ncontext, for which a re-ranker may fail to identify a\ngold passage if its identification hinges on knowing\nthat the passage comes from a relevant document\nor webpage. By prepending page titles to passages\nthey were able to alleviate this issue on NQ.\nIn contrast to these works, we expand on the\nanalysis of distractors and missing document con-\ntext to include multiple SOTA re-rankers, datasets\nfrom diverse domains and better tuned metrics. We\nalso tie these aspects of difficulty to a more fun-\ndamental question of whether LM re-rankers are\nsteered by lexical similarities. To measure this, we\ndevelop a new metric which allows us to identify\nproblematic samples.\n3\nMethod\nThis section describes the re-rankers, datasets, met-\nrics and alleviation methods investigated.\n3.1\nRe-rankers\nWe evaluate a wide cohort of LM re-rankers to\nenable comprehensive comparisons between dif-\nferent model types and sizes. Three closed-source\nLM re-rankers are evaluated: The industrial grade\nre-ranker Cohere1 (Cohere), the LLM-based re-\nranker GPT-4o (GPT-4o) and the lightweight LLM\nre-ranker GPT-4o mini (GPT-4o m) (Sun et al.,\n2023) (Appendix E).2\nWe\nalso\nevaluate\nthree\nopen-source\nre-\nrankers from Hugging Face:\nThe large-scale\nLM\nre-ranker\nbge-reranker-v2-gemma\n(BGE),\nthe\nlightweight\nre-ranker\njina-reranker-v1-turbo-en (Jina turbo) and\njina-reranker-v2-base-multilingual (Jina\nbase), a larger re-ranker from the same model\nfamily. Our baseline is a re-ranker based on BM25\nscores that leverages lexical matching, similar\nto TF-IDF (Lù, 2024). See Appendix D to get a\nrough estimate of the runtime of each re-ranker.\n3.2\nEvaluation datasets\nWe evaluate the re-rankers on three datasets repre-\nsentative of different domains and aspects of diffi-\nculty: NQ, LitQA2 and DRUID. Natural Questions\n(NQ) is a popular dataset for re-ranker evaluations\nwith passages from Wikipedia pages (Kwiatkowski\net al., 2019). LitQA2 measures the ability of a sys-\ntem to extract information from scientific literature\n(Laurent et al., 2024). The dataset contains a high\nrate of domain-specific biomedical language and\ncan be expected to test the robustness to domain-\nshifts of LM re-rankers. DRUID (Dataset of Re-\ntrieved Unreliable, Insufficient and Difficult-to-\nunderstand contexts) contains fact-checked claims\nand corresponding potential evidence automatically\nretrieved from the Internet (Hagström et al., 2024).\nIt can be expected to contain more noisy passages\nand to test the capability of re-rankers to identify\nrelevant information for fact-checking. More de-\ntails and examples can be found in Appendix C.\n3.3\nEvaluation metrics\nWe mainly use Precision@1 (P@1) for our re-\nranker evaluations to accommodate the small num-\nber of passages available in DRUID.3 To under-\nstand when LM re-rankers fail to outperform sim-\npler methods, we also compare to alignment with\nBM25 relevance scores. This is measured as fol-\nlows.\n∆P@1(R) = P@1(R) −P@1BM25(R)\n(1)\n1rerank-english-v3.0 from https://docs.cohere.\ncom/v2/docs/models#rerank\n2gpt-4o-2024-08-06 and gpt-4o-mini-2024-07-18.\n3Metrics are defined by TREC in https://trec.nist.\ngov/pubs/trec16/appendices/measures.pdf.\n2\n\nGiven re-ranker predictions R, P@1(R) denotes\nthe score measured when document relevance is\ngiven by gold labels (default) and P@1BM25(R)\nwhen relevance is given by BM25 scores. Lever-\naging this metric, we can investigate whether re-\nrankers align with gold labels over BM25 scores,\nwhich corresponds to positive ∆P@1 values and\nvice versa for negative values.\n3.4\nGold from similar separation metric\nTo better understand why and when re-rankers fail\nto identify gold passages in a document, we define a\ngold-from-similar separation metric DS for a given\ntext similarity measure S. Given a query q, a set\nof passages p = {p1, ..., pn} and corresponding\ngold labels y indicating whether a passage pi is\ngold (yi = 1) or not (yi = 0), we compute the\nmetric DS by subtracting the maximal similarity of\nthe non-gold standard passages from the maximal\nsimilarity of the gold standard passages:\nDS(q, p, y) = max\ni: yi=1 S(q, pi) −max\ni: yi=0 S(q, pi)\n(2)\nThis metric indicates whether the most similar gold\nstandard passage is more or less similar to the query\nthan the most similar non-gold standard passage.\nWe assume there to exist at least one gold pas-\nsage per (q, p) sample. The similarity measure S\ncan be any measure of choice that takes two docu-\nments as input. A larger value of S should signify\ngreater similarity between the two documents.\n3.5\nAlleviation methods\nWe investigate two known methods previously\nshown to improve re-ranker performance: prepend-\ning page titles (Prepend titles) (Wang et al.,\n2024a) and incorporating contextual informa-\ntion generated by GPT-4o mini (Incorporate\ncontext).4 Prepending titles is quite straightfor-\nward for NQ and LitQA2, while the more noisy\nwebpage text in DRUID yields low-quality titles,\nwith missing values and inaccuracies. We were\nalso unable to obtain contextual information for\nthe DRUID passages due to computational limi-\ntations. Lastly, we also experiment with adjust-\ning the re-ranker prompt to better suit the fact-\nchecking setting represented by DRUID (Prompt)\n(Appendix F).\n4https://www.anthropic.com/news/contextual-r\netrieval\n4\nResults\nThe zero-shot performance of the re-rankers consid-\nered in this paper are shown in Table 1. Additional\nresults can be found in Appendix G. Based on these\nresults, we reach the following conclusions.\nRe-ranker\nNQ\nLitQA2\nDRUID\nStandard mode\nCohere\n0.65 (0.13)\n0.76 (0.08)\n0.68 (−0.21)\nBGE\n0.68 (0.17)\n0.78 (0.10)\n0.73 (−0.15)\nJina turbo\n0.56 (0.08)\n0.61 (0.03)\n0.69 (−0.20)\nJina base\n0.68 (0.15)\n0.65 (0.06)\n0.65 (−0.20)\nGPT-4o m\n0.83 (0.37)\n0.51 (0.10)\n0.72 (−0.10)\nGPT-4o\n0.85 (0.40)\n0.50 (0.10)\n0.73 (−0.10)\nBM25\n0.46\n0.67\n0.66\nPrepend titles\nCohere\n0.77 (0.23)\n0.79 (0.09)\n0.71 (−0.17)\nBGE\n0.76 (0.23)\n0.80 (0.10)\n0.74 (−0.14)\nJina turbo\n0.69 (0.16)\n0.66 (0.02)\n0.71 (−0.17)\nJina base\n0.78 (0.24)\n0.77 (0.07)\n0.69 (−0.18)\nGPT-4o m\n0.85 (0.34)\n0.50 (0.08)\n0.72 (−0.08)\nGPT-4o\n0.85 (0.36)\n0.51 (0.07)\n0.74 (−0.06)\nBM25\n0.50\n0.70\n0.68\nIncorporate context\nPrompt\nCohere\n0.72 (0.24)\n0.69 (0.06)\n0.69 (−0.18)\nBGE\n0.72 (0.26)\n0.70 (0.05)\n0.77 (−0.06)\nJina turbo\n0.62 (0.15)\n0.60 (−0.04)\n0.72 (−0.17)\nJina base\n0.74 (0.27)\n0.63 (0.09)\n0.72 (−0.14)\nGPT-4o m\n0.80 (0.37)\n0.47 (0.12)\n0.79 (−0.02)\nGPT-4o\n0.81 (0.38)\n0.46 (0.11)\n0.83 (0.05)\nBM25\n0.44\n0.58\n0.68\nTable 1: P@1 of all re-rankers. Values in (parenthesis)\nindicate ∆P@1 (Equation (1)). Values in bold indicate\ntop scores.\nLitQA2 is generally easier and NQ generally\nmore difficult.\nThe majority of the LM re-\nrankers perform best on LitQA2, followed by\nDRUID and NQ. The only exceptions are the Jina\nmodels and GPT-4o models. The GPT-4o models\nlikely struggle on LitQA2 due to token limitations.\nLarge LM re-rankers struggle to outperform a\nBM25 baseline on DRUID.\nThe best-performing\nre-ranker (BGE) outperforms the BM25 re-ranker\nby 10% on DRUID. This is smaller than the 46% on\nNQ (for GPT-4o) and 15% on LitQA2 (for BGE).\nWe also note that the smaller Jina LM re-rankers\nclearly outperform the BM25 re-ranker on NQ\nwhile they perform worse than or equal to BM25\non LitQA2 and DRUID.\nLM re-rankers align more with BM25 scores\nthan gold labels on DRUID.\nThe ∆P@1 values\nare negative for all LM re-rankers on DRUID in\n3\n\n0.00\n0.25\nDensity\nCorrect\nIncorrect\n(a) NQ\n0.00\n0.25\nDensity\n(b) LitQA2\n0\n5\n10\n15\n20\n25\n30\nDBM25\n0.00\n0.25\nDensity\n(c) DRUID\nFigure 2: Distribution of DBM25 (Equation (2)) for NQ,\nLitQA2 and DRUID. Correctness is based on P@1 of\nthe BGE re-ranker. The dashed vertical lines indicate\nthe mean values.\nTable 1, indicating that the re-rankers align more\nwith BM25 scores than gold labels on DRUID.\nWe note that while DRUID is easier compared\nto NQ with respect to LM re-ranker accuracy, it is\nharder with respect to how LM re-rankers struggle\nto outperform simpler methods like BM25. We hy-\npothesise that DRUID provides a greater challenge\nin this sense as it contains passages from the Inter-\nnet and popular claims that may have seen frequent\ndiscussion, increasing the rate of distractors.\n4.1\nQuery-passage similarities\nTo understand why LM re-rankers struggle to out-\nperform BM25 on DRUID, we apply our separa-\ntion metric DS to the passages in NQ, LitQA2\nand DRUID and make comparisons to re-ranker\nprecision. DBM25 results are found in Figure 2 (re-\nsults for other similarity metrics can be found in\nAppendix G). A summary of the distribution and\ncorresponding re-ranker performance can be found\nin Table 7. To better understand the re-ranker per-\nformance on DRUID we also partition the dataset\nby DBM25 value and report the re-ranker scores in\nTable 8. Our conclusion is as follows.\nLM re-rankers struggle to identify gold samples\nwith markedly low BM25 scores.\nThe results\nin Figure 2 show that LM re-rankers are generally\ngood at identifying gold samples if they are suffi-\nciently similar to the query. However, if the gold\npassage is too dissimilar to the query (correspond-\ning to low DBM25 values), the LM re-rankers are\nprone to make mistakes.\nWe see how NQ and DRUID pose a greater chal-\nlenge by including gold passages that are relatively\ndissimilar to the query. An inspection of some sam-\nples with low DBM25 scores in Appendix H reveals\na high rate of distractors and gold passages lacking\ndocument context. LitQA2 samples, on the other\nhand, have generally high DBM25 values and we\nhypothesise this makes the dataset easier for LM\nre-rankers. Seemingly, the domain-specific queries\nand passages of LitQA2 are less of a challenge com-\npared to the lexical dissimilarities between gold\npassage and query in the other datasets.\n4.2\nAlleviation methods\nTable 1 report the results from the investigations\ndescribed in Section 3.5. We reach the following\nconclusions.\nPrepending page titles yields the greatest effects\non NQ.\nPrepending page titles to the passages\nyields performance improvements for large LM\nre-rankers on NQ and unchanged performance on\nLitQA2 and DRUID. For LitQA2, this could be\ncaused by the more distracting details from the\nscientific paper titles (Wang et al., 2024a). For\nDRUID it likely stems from the noisy webpage\ntitles. Seemingly, the method of prepending page\ntitles is more suitable for nicely formatted datasets.\nWe also observe that the method of incorporating\ncontexts is inferior to prepending page titles.\nAdjusting the prompt yields significantly im-\nproved results for GPT-4o on DRUID.\nTable 1\nshows how GPT-4o benefits the most from an ad-\njusted prompt, indicating significance of prompt\nfor the performance of LLMs as re-rankers.\n5\nConclusion\nOur paper identifies and explores an important\nweakness of LM re-rankers: They struggle to\nidentify gold samples with markedly low BM25\nscores. We hypothesise that real-world datasets\nlike DRUID, with passages from the Internet, con-\ntain more distractors, resulting in gold samples with\nlow BM25 scores. However, most current datasets\nfor re-ranker evaluation fail to capture this aspect\nof difficulty and methods for improving LM re-\nranker performance are less effective for the noisier\nLitQA2 and DRUID samples. Our work points to\nthe need of more adversarial and real-world aligned\ndatasets to better understand LM re-rankers and\ntheir weaknesses in realistic settings.\n4\n\nLimitations\nThe datasets used in this study were not specifically\ndesigned to measure the preference of re-ranking\nmodels for similar over gold passages. A dataset\nspecifically curated for this purpose, potentially\ncomplemented by synthetically generated samples,\nwould allow a deeper analysis of our research ques-\ntions. We leave this for future work.\nOur work only investigated a subset of the alle-\nviation methods that exist for improving re-ranker\nperformance. For example, there are also meth-\nods focused on adapting chunk sizes, and methods\navoiding chunking all together. It would be inter-\nesting to also expand our analysis to incorporate\nadditional alleviation methods.\nEthical Considerations\nThere are no major ethical concerns related to our\nwork on LM re-ranker performance. The datasets\nused and methods investigated are not associated\nwith any ethical concerns.\nAcknowledgments\nThis work was supported by the Wallenberg AI, Au-\ntonomous Systems and Software Program (WASP)\nfunded by the Knut and Alice Wallenberg Founda-\ntion. The computations were enabled by resources\nprovided by the National Academic Infrastructure\nfor Supercomputing in Sweden (NAISS) at Alvis\npartially funded by the Swedish Research Council\nthrough grant agreement no. 2022-06725.\nReferences\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,\nJinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang,\nand Haofen Wang. 2024. Retrieval-augmented gener-\nation for large language models: A survey. Preprint,\narXiv:2312.10997.\nMichael Glass, Gaetano Rossiello, Md Faisal Mahbub\nChowdhury, Ankita Naik, Pengshan Cai, and Alfio\nGliozzo. 2022. Re2G: Retrieve, rerank, generate.\nIn Proceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 2701–2715, Seattle, United States. Association\nfor Computational Linguistics.\nLovisa Hagström, Sara Vera Marjanovi´c, Haeun Yu,\nArnav Arora, Christina Lioma, Maria Maistro, Pepa\nAtanasova, and Isabelle Augenstein. 2024. A reality\ncheck on context utilisation for retrieval-augmented\ngeneration. Preprint, arXiv:2412.17031.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: A benchmark for question answering\nresearch. Transactions of the Association for Compu-\ntational Linguistics, 7:452–466.\nJon M. Laurent, Joseph D. Janizek, Michael Ruzo,\nMichaela M. Hinks, Michael J. Hammerling, Sid-\ndharth Narayanan, Manvitha Ponnapati, Andrew D.\nWhite, and Samuel G. Rodriques. 2024. Lab-bench:\nMeasuring capabilities of language models for biol-\nogy research. Preprint, arXiv:2407.10362.\nChaofan Li, Zheng Liu, Shitao Xiao, and Yingxia Shao.\n2023. Making large language models a better founda-\ntion for dense retrieval. Preprint, arXiv:2312.15503.\nXing Han Lù. 2024. Bm25s: Orders of magnitude faster\nlexical search via eager sparse scoring. Preprint,\narXiv:2407.03618.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\nVassilis Plachouras, Tim Rocktäschel, and Sebastian\nRiedel. 2021. KILT: a benchmark for knowledge\nintensive language tasks. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2523–2544, Online.\nAssociation for Computational Linguistics.\nSaba Sturua, Isabelle Mohr, Mohammad Kalim Akram,\nMichael Günther, Bo Wang, Markus Krimmel, Feng\nWang, Georgios Mastrapas, Andreas Koukounas,\nNan Wang, et al. 2024. jina-embeddings-v3: Mul-\ntilingual embeddings with task lora. arXiv preprint\narXiv:2409.10173.\nWeiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang\nWang, Pengjie Ren, Zhumin Chen, Dawei Yin, and\nZhaochun Ren. 2023. Is ChatGPT good at search?\ninvestigating large language models as re-ranking\nagents. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 14918–14937, Singapore. Association for\nComputational Linguistics.\nNandan Thakur, Nils Reimers, Andreas Rücklé, Ab-\nhishek Srivastava, and Iryna Gurevych. 2021. BEIR:\nA heterogeneous benchmark for zero-shot evaluation\nof information retrieval models. In Thirty-fifth Con-\nference on Neural Information Processing Systems\nDatasets and Benchmarks Track (Round 2).\nTu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry\nWei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny\nZhou, Quoc Le, and Thang Luong. 2024. Fresh-\nLLMs: Refreshing large language models with search\nengine augmentation. In Findings of the Association\nfor Computational Linguistics: ACL 2024, pages\n5\n\n13697–13720, Bangkok, Thailand. Association for\nComputational Linguistics.\nKexin Wang, Nils Reimers, and Iryna Gurevych. 2024a.\nDAPR: A benchmark on document-aware passage\nretrieval. In Proceedings of the 62nd Annual Meeting\nof the Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 4313–4330, Bangkok,\nThailand. Association for Computational Linguistics.\nXiaohua Wang, Zhenghua Wang, Xuan Gao, Feiran\nZhang,\nYixin Wu,\nZhibo Xu,\nTianyuan Shi,\nZhengyuan Wang, Shizheng Li, Qi Qian, Ruicheng\nYin, Changze Lv, Xiaoqing Zheng, and Xuanjing\nHuang. 2024b.\nSearching for best practices in\nretrieval-augmented generation. In Proceedings of\nthe 2024 Conference on Empirical Methods in Natu-\nral Language Processing, pages 17716–17736, Mi-\nami, Florida, USA. Association for Computational\nLinguistics.\nA\nComputational resources\nAll open-source re-rankers are evaluated without\nfine-tuning on one T4, V100 or A100 Nvidia GPU\nper evaluation. The choice of GPU type depended\non the model size (see Table 6 for detailed informa-\ntion on what GPU type was used for what model).\nThe closed-source models were accessed via APIs\nso it is unclear as to exactly what GPU devices\nwere involved. The total computational budget for\nthe evaluations was about 50 GPU hours.\nB\nUse of AI assistants\nAI assistants like Copilot and ChatGPT were in-\ntermittently used to generate template code and\nrephrase sentences in the paper, etc. However, no\ncomplete paper sections or code scripts have been\ngenerated by an AI assistant. All generated text\ncontent has been inspected and verified by the au-\nthors. ChatGPT was also used and evaluated as a\nre-ranker in this work.\nC\nEvaluation datasets\nThe evaluation datasets are described in further de-\ntail below. High-level statistics for the datasets can\nbe found in Table 2 and examples of samples from\neach dataset can be found in Tables 3 to 5. From\neach dataset we extract a set of questions, corre-\nsponding passages to choose between and corre-\nsponding gold labels indicating whether a passage\ncontains the answer to the given question or not.\nC.1\nNatural Questions\nNatural Questions (NQ) by Kwiatkowski et al.\n(2019) is a popular dataset for re-ranker evalua-\ntions that contains real search engine queries and\nDataset\n#samples\n#passages/sample\n#gold\nmean\nmin\nmax\n/sample\nNQ\n3,759\n16\n4\n244\n1\nLitQA2\n124\n145\n33\n359\n1\nDRUID\n875\n4\n2\n5\n2\nTable 2: Statistics for the evaluation datasets.\nEx-\nactly one gold passage is found per sample for NQ\nand LitQA2. DRUID samples may contain more than\none gold passage.\ncorresponding Wikipedia pages with the gold pas-\nsage annotated. The gold passage annotators were\ninstructed to identify the first paragraph on the\nWikipedia page that contains the answer to the\nquery, which means that there may be multiple\nunidentified gold passages for each query. To avoid\nissues stemming from this, we only retain all pas-\nsages up to and including the gold passage as the\nretrieval corpora.\nChunking approach\nThe chunking is based on\nhtml elements, for which each passage is made\nout of one html element (e.g. a table <Table> or\nparagraph <P>), similarly to the approach used by\nthe NQ authors to annotate gold passages. These\npassages are then matched to the annotated gold\nlabels based on token indices.\nC.2\nLitQA2\nLitQA2 by Laurent et al. (2024) measures the abil-\nity of a system to extract information from scien-\ntific literature. The dataset contains a high rate\nof domain-specific biomedical language compared\nto the more generic queries of NQ and can be ex-\npected to test the robustness to domain-shifts of LM\nre-rankers. The dataset consists of multiple-choice\nquestions that are intended to be only answerable\nbased on the full text, not on the abstract, of a given\npaper and nowhere else in the literature. PubMed-\nCentral5 was used to scrape the full articles. Only\n124 out of 200 samples were retained from this\ndataset as some articles were unavailable. We de-\ncided to include the dataset in the analysis despite\nthe small sample size as this is the only high-quality\ndataset that enables evaluations of re-rankers for\nthe biomedical domain.\nChunking approach\nThe chunking is based on\nnewlines, for which each passage is made out by\na new paragraph. Passages can then be matched\n5https://pmc.ncbi.nlm.nih.gov\n6\n\nQuestion\nPassages\nGold labels\nwhen did hyderabad\nbecame a part of india?\n“<H1> Hyderabad state (1948–56) </H1>”\n0\n“Jump to: navigation, search This article is about a State of the Indian Union\n1948–1956 . For other uses, see Hyderabad (disambiguation).”\n0\n“<Table> <Tr> <Td colspan=3> Hyderabad State (1948 - 1956) </Td> </Tr>\n<Tr> <Td colspan=3> State of India </Td> </Tr> <Tr> <Td colspan=3> <Ta-\nble> <Tr> <Td> \\u2190 </Td> <Td> 1948–1956 </Td> <Td> \\u2192 </Td>\n</Tr> </Table> </Td> </Tr> <Tr> <Td colspan=3> 1956 map of Southern India\nshowing Hyderabad state in yellowish green . After the States reorganisation in\n1956, regions west of the red and blue lines merged with Bombay and Mysore\nstates respectively and the remaining part (Telangana) was merged with Andhra\nstate to form Andhra Pradesh . </Td> </Tr> <Tr> <Td colspan=2> History\n</Td> <Td> </Td> </Tr> <Tr> <Td> </Td> <Td> Hyderabad State formed from\nHyderabad Princely State </Td> <Td> 1948 </Td> </Tr> <Tr> <Td> </Td>\n<Td> Reorganized and renamed Andhra Pradesh </Td> <Td> 1956 </Td> </Tr>\n<Tr> <Td colspan=3> States of India since 1947 </Td> </Tr> </Table>”\n0\n“Hyderabad state until 1956”\n0\n“<P> Hyderabad State was a state in Independent India, formed after the accession\nof the princely state of Hyderabad into the Indian Union on 24 November 1949 .\nIt existed from 1948 to 1956 . </P>”\n1\nTable 3: Data sample from NQ.\nto the manually extracted gold passage via fuzzy\nmatching, to get the gold labels for each passage.\nC.3\nDRUID\nDRUID (Dataset of Retrieved Unreliable, Insuf-\nficient and Difficult-to-understand contexts) by\nHagström et al. (2024) contains fact-checked\nclaims and corresponding potential evidence pieces\nretrieved from the Internet. Each evidence piece\nhas been annotated for whether it contains suffi-\ncient information to conclude whether the corre-\nsponding claim is true or false. The claims from\nthe dataset are used as questions to the re-rankers\nand the collected DRUID passages corresponding\nto the given claim make out the passages for the\nquery. Passages with sufficient information to reach\na fact-check verdict, i.e. marked as ‘refuting’ or\n‘supporting’, are considered gold and each sam-\nple corresponds to at least two potential passages\nfrom different webpages, of which at least one has\nto be gold and at least one not gold. The Cohere\nre-ranker was used for the automated retrieval of\nevidence pieces so the samples in DRUID can be\nexpected to be more adversarial in the sense that\nthey already have been pre-selected by a LM re-\nranker (and then manually annotated for quality).\nChunking approach\nThe passages have already\nbeen chunked in a previous automated retrieval\npipeline by the DRUID authors. Each passage is\nbased on text snippets from a webpage, for which\nmultiple snippets may have been extracted across\nthe same webpage.\n7\n\nQuestion\nPassages\nGold labels\nNeonatal male mice\ninjected with NIF, a\nglycoprotein produced\nby a canine hookworm,\nshow a significant\nreduction in microglial\nphagocytic capacity and\nengulfment of which\nneurotransmitter\ntransporter? (A)\nVGlut2, (B) VGlut1,\n(C) VGlut3, (D) GAT1,\n(E) GAT2, (F) GAT3, or\n(G) not enough info?\n“The incidences of neurodevelopmental disorders (NDDs) have been increasing\nin recent decades, suggesting a role for non-genetic environmental factors. Fur-\nthermore, sex is a significant risk factor for these disorders, with a strong male\nbias.”\n0\n“Air pollutant exposure during pregnancy or the first year of life is one of the\nmost consistent environmental risk factors for NDDs. However, the associations\nof single environmental agents with NDDs have been relatively weak, and\nthus causality has been difficult to determine. Non-chemical stressors such as\nlimited resources or social support of the mother can increase the vulnerability\nof the fetus to toxic exposures, which could explain why certain populations\nare disproportionately affected. In fact, neighborhood quality is a significant\nmodifier of air pollution risk, suggesting that environmental and social stressors\nsynergize to increase vulnerability to pollutant exposure, but how these exposures\nalter fetal brain development and affect offspring behavior is largely unknown.”\n0\n“Inflammatory events during pregnancy, such as maternal infection with bacte-\nria or viruses, lead to maternal immune activation (MIA), which is linked to\nNDDs in offspring. Recent transcriptome-wide studies in postmortem brains of\nindividuals diagnosed with an NDD have identified expression modules with\nenrichment of genes involved in neuroinflammatory function, with a particular\ndysregulation of microglial genes. Microglia are the primary immunocompetent\ncells of the brain and are exquisitely sensitive to perturbations of homeostasis\nand thus may be poised to act as immediate responders to environmental insults.\nMicroglia are also essential regulators of activity-dependent synaptic remodeling\nduring development, in which they prune inappropriate/weak synapses while\nsparing appropriate/strong connections. Importantly, transcriptome studies have\nfound that immune changes co-occur with gene enrichment modules affecting\nsynaptic function, suggesting the possibility that neuroimmune changes during\ndevelopment could lead to aberrant synapse development by altering microglial\nfunction.”\n0\n“A recent analysis found that MIA was more common in male children with ASD\nthan female children, suggesting that a sex difference in response to maternal\ninflammation may be one mechanism that underlies increased male vulnerability.\nFurthermore, we and others have found sex differences in microglial develop-\nment, maturation, and function, including an increased relative expression of\nmicroglial genes in male brains, compared with females. Interestingly, the mi-\ncroglial genes enriched in male brains are also implicated in ASD. Together these\ndata point to a mechanism by which sexually dimorphic microglial responses to\nprenatal stressors could lead to aberrant brain development, primarily in males.”\n0\n[...]\n“In this experiment, WT neonatal male mice received bilateral microinjections of\nPBS or NIF (200 ng) into the ACC at P7, and brain tissue was collected 24 h later\n(Figure 7A). To confirm the effects of NIF on microglial phagocytic capacity,\nwe quantified changes in the microglial lysosomal volume of CD68 (Figure 7B).\nAs expected, microglia from animals microinjected with NIF had a significant\nreduction in the phagocytic index ( 50%) and a significant decrease in the total\nlysosomal content within each microglia (Figures 7C and 7D). To determine\nwhether this reduction in CD68 impaired microglial interactions with VGlut2\nsynapses, we once again performed Imaris reconstructions and quantified the\nvolume of VGlut2 within microglia (Figure 7E). Microglia from NIF-treated\nanimals are significantly smaller ( 25%) than PBS control animals (Figure 7F);\nfurthermore, this size reduction is accompanied by a significant decrease in\nthe volume of internalized VGlut2 in microglia cells (Figure 7G). Last, we\nquantified the co-localization of VGlut2 and PSD95 and found that NIF-injected\nanimals had about a 20% increase in VGlut2+ synapses (Figure 7H). Thus, NIF\ninjections at P7 effectively reduce microglial phagocytic capacity and engulfment\nof VGlut2, which induces an abnormal increase in VGlut2 synapse density.”\n1\n[...]\nTable 4: Data sample from LitQA2. “[...]” indicates that we are skipping across passages in the sample to save\nspace.\n8\n\nQuestion\nPassages\nSource\nGold labels\nDRUID labels\nWikiLeaks has\npublished\nthe\n1st list of black\nmoney holders\nin Swiss banks.\n“WikiLeaks has never published the\nlist of Indians who have stashed their\nmoney in Swiss banks. Hence, the\nclaim stands FALSE.”\nhttps://factly.in/wikile\naks-list-of-black-money\n-holders-in-swiss-bank-i\ns-a-fake-one/\n1\nrefutes\n“Various posts on social media claim\nthat WikiLeaks has released the\n\"first list\" of black money holders\nin Swiss Bank.\nThe post is go-\ning viral on all social media plat-\nforms. DigitEye Team also received\nthe message on its Whatsapp fact-\nchecking number. The list contains\n24 names \\u2014 Sonia Gandhi, A\nRaja, Rahul Gandhi, Sharad Pawar,\nP Chidambaram to name a few. All\nthe money listed next to the names\nare figures in dollars. The first name\non the alleged list is Congress leader\nSonia Gandhi who it claimed to be\nholding more than $56 billion. The\nnumbers are not in chronological or-\nder and neither the names are in any\nset order. According to the alleged\nlist, the lowest amount is held by P\nChidambaram. [...] WikiLeaks has\nnot published any report on the same\non its website. The latest report was\npublished in October 2019. Wik-\niLeaks took to Twitter and tweeted\nabout a similar list of Indian black\nmoney holders. In the 2011 tweet,\nit clarified that such list \"never ap-\npeared on WikiLeaks\".”\nhttps://digiteye.in/vira\nl-list-of-black-money-h\nolding-accounts-in-swiss\n-bank-is-fake/\n1\nrefutes\n“INDIA/SWIZERLAND–\nBlack\nmoney trail:\n2nd list of Indian\nSwiss accounts to be shared [...]\nTNN | Sep 14, 2011, 11.11AM IST\nhttp://timesofindia.indiatimes.com/\nindia/Black-money-trail-2nd-list-\nof-Indian-Swiss-accounts-to-be-\nshared/articleshow/9977871.cms\nNEW DELHI: A second list con-\ntaining names of Indians, who have\nstashed black money in Swiss banks,\nwill be shared by the Germans,\nTimes Now reported.”\nhttps://wikileaks.org/gi\nfiles/docs/70/703306_ind\nia-swizerland-black-money\n-trail-2nd-list-of-india\nn-swiss.html\n0\ninsufficient\n“(See attached file: List of Black\nMoney Holders from Wiki”\nhttps://groups.google.co\nm/g/yeida/c/V2gxTIXY-sQ\n0\ninsufficient\nTable 5: Data sample from DRUID. “[...]” inside a passage does not indicate additional information included to the\nre-ranker, it simply indicates that the passage was retrieved as snippets from a webpage, for which there is additional\npage content between the snippets.\n9\n\nD\nRuntime comparison\nTo exemplify the difference in efficiency between\ndifferent re-rankers, we compare runtimes of the\ninvestigated re-rankers in Table 6. Unfortunately,\nthe models could not be run on the same devices\ndue to space and other practical reasons.\nRe-ranker\nRuntime [mins]\nDevice\nCohere\n15\nCohere API\nBGE\n42\nA100:1\nJina turbo\n3\nV100:1\nJina base\n80\nT4:1\nGPT-4o m\n145\nOpenAI API\nGPT-4o\n135\nAzure API\nBM25\n0.5\nMacBook Pro\nTable 6: Runtimes of the different re-rankers for get-\nting scores corresponding to all samples from NQ (no\nprepended titles or context) on their corresponding de-\nvices. The MacBook Pro device is using a 2.3 GHz\nQuad-Core Intel Core i7.\nE\nImplementation details of RankGPT\nLLMs demonstrate strong capabilities in under-\nstanding long texts and handling complex tasks,\nmaking them suitable for use as re-rankers in pas-\nsage re-ranking tasks. Building on the prompt-\ning strategies proposed by Sun et al. (2023), we\nexplore the use of LLM-based re-rankers, specif-\nically leveraging two advanced OpenAI models:\nGPT-4o (gpt-4o-2024-08-06) and GPT-4o mini\n(gpt-4o-mini-2024-07-18). As illustrated in Fig-\nure 3, the re-ranking process with LLMs is facil-\nitated via prompting. Specifically, a set of text\nchunks, each assigned a unique identifier (e.g.,\n[1],[2]) is provided as input to the LLM. The\nmodel is then instructed to reorder the chunks in\ndescending order of relevance to a given query.\nThe output is a ranked list of identifiers in a for-\nmat such as [3] > [4] > [1] > [2]. Notably,\nthis approach directly generates a ranking without\ncalculating intermediate relevance scores.\nFor datasets such as NQ and DRUID, we apply\nthis direct permutation generation strategy without\nmodification. However, for the LitQA2 dataset,\nthe samples of which contain a significantly larger\nnumber of candidate chunks (an average of 145\nper query), the token limitations of LLMs pose a\nchallenge. To address this, we employ the sliding\nwindow strategy, following Sun et al. (2023). This\nmethod processes the chunks iteratively, using a\nsliding window size w and a step size s, to re-\nrank the chunks in a back-to-first order. In our\nexperiments on LitQA2, we set the window size to\n20 and the step size to 2. However, we note that the\nGPT-4o re-ranker performance suffers on LitQA2\nin spite of these adaptations.\nF\nAdjusted prompt for DRUID\nThe prompts used for the prompt adjustment inves-\ntigations for DRUID are as follows:\n• Default prompt: “<claim>”\n• Adjusted prompt: “Is the following claim\naccurate?\\nClaimant:\n<claimant>\\nClaim:\n<claim>”\nHere, “<claim>” and “<claimant>” are replaced by\nthe corresponding values in DRUID. The results for\nthese prompts can be found in Table 1 and Figure 4.\nG\nAdditional re-ranker results\nAdditional results corresponding to Table 1 can be\nfound in Figures 5 and 6. We also report additional\nDBM25 results in Tables 7 and 8.\nDataset\nPartition\n% of data\nP@1\nNQ\nDBM25 < −0.5\n32\n0.31\n−0.5 ≤DBM25\n68\n0.85\nLitQA2\nDBM25 < −0.5\n31\n0.47\n−0.5 ≤DBM25\n69\n0.92\nDRUID\nDBM25 < −0.5\n20\n0.24\n−0.5 ≤DBM25\n80\n0.85\nTable 7: Re-ranker accuracy on the different datasets\npartitioned by DBM25 values.\nP@1 is reported for\nbge-reranker-v2-gemma.\nDRUID\nRe-ranker\nDBM25 < 0.5\n0.5 ≤DBM25\nCohere\n0.10 (−0.78)\n0.83 (−0.07)\nBGE\n0.24 (−0.56)\n0.85 (−0.05)\nJina turbo\n0.13 (−0.72)\n0.83 (−0.07)\nJina base\n0.18 (−0.64)\n0.77 (−0.09)\nGPT-4o m\n0.34 (−0.41)\n0.82 (−0.02)\nGPT-4o\n0.32 (−0.40)\n0.83 (−0.02)\nBM25\n0.00\n0.83\nTable 8: Re-ranker zero-shot alignment with gold mea-\nsured using P@1 on DRUID partitioned by DBM25\nvalues. Values in parenthesis indicate ∆P@1 (Equa-\ntion (1)).\nAdditional separation results for the similarity\nmeasures Jaccard similarity (DJS) and BERT score\n(DBERT) can be found in Figures 7 and 8. DBM25\n10\n\nsystem:\nYou are RankGPT, an intelligent assistant\nthat can rank passages based on their\nrelevancy to the query.\nuser:\nI will provide you with {{num}} passages,\neach indicated by number identifier [].\nRank them based on their relevance to\nquery: {{query}}.\nassistant:\nOkay, please provide the passages.\nuser:\n[1] {{passage_1}}\nassistant:\nReceived passage [1]\nuser:\n[2] {{passage_2}}\nassistant:\nReceived passage [2]\n(more passages) ...\nuser\nSearch Query: {{query}}.\nRank the {{num}} passages above based on\ntheir relevance to the search query. The pas-\nsages should be listed in descending order\nusing identifiers, and the most relevant pas-\nsages should be listed first, and the output\nformat should be [] > [], e.g., [1] > [2]. Only\nresponse the ranking results, do not say any\nword or explain.\nFigure\n3:\nPrompt\ntemplate\nfor\nGPT-4o\nand\nGPT-4o-mini as re-rankers (Sun et al., 2023).\nDefault prompt\nAdjusted prompt\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nP@1\nreranker\nCohere\nBGE\nJina turbo\nJina base\nGPT-4o m\nGPT-4o\nBM25\nFigure 4: Re-ranker zero-shot alignment with gold la-\nbels on DRUID for different prompts.\nNQ\nLitQA2\nDRUID\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nP@1\nCohere\nBGE\nJina turbo\nJina base\nGPT-4o m\nGPT-4o\nBM25\nFigure 5: Re-ranker zero-shot alignment with gold la-\nbels for different datasets. The error bars indicate 95%\nconfidence intervals.\nscores with correctness evaluated based on GPT-\n4o and Jina base scores can be found in Figures 9\nand 10.\nH\nSamples with different separation\nvalues\nTables 9 to 11 contain samples from NQ, LitQA2\nand DRUID with corresponding DBM25 values.\n11\n\nNQ\nLitQA2\nDRUID\n0.0\n0.2\n0.4\n0.6\n0.8\nNDCG@2\nreranker\nCohere\nBGE\nJina turbo\nJina base\nGPT-4o m\nGPT-4o\nBM25\nFigure 6: Re-ranker zero-shot alignment with gold la-\nbels for different datasets. The error bars indicate 95%\nconfidence intervals.\n0\n0 2\n0 0\n0 2\n0\n0 6\n0\n10\nDensity\nCorrect\nIncorrect\n(a) NQ\n0\n0 2\n0 0\n0 2\n0\n0 6\n0\n10\nDensity\n(b) LitQA2\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\nDJS\n0\n10\nDensity\n(c) DRUID\nFigure 7:\nDJS (Equation (2)) on NQ, LitQA2\nand DRUID. Correctness is based on P@1 for\nbge-reranker-v2-gemma.\n0\n20\nDensity\nCorrect\nIncorrect\n(a) NQ\n0\n20\nDensity\n(b) LitQA2\n0.15\n0.10\n0.05\n0.00\n0.05\n0.10\n0.15\nDBERT\n0\n20\nDensity\n(c) DRUID\nFigure 8:\nDBERT (Equation (2)) on NQ, LitQA2\nand DRUID. Correctness is based on P@1 for\nbge-reranker-v2-gemma.\n0.00\n0.25\nDensity\nCorrect\nIncorrect\n(a) NQ\n0.00\n0.25\nDensity\n(b) LitQA2\n0\n5\n10\n15\n20\n25\n30\nDBM25\n0.00\n0.25\nDensity\n(c) DRUID\nFigure 9: DBM25 (Equation (2)) on NQ, LitQA2 and\nDRUID. Correctness is based on P@1 for GPT-4o.\n12\n\n0.00\n0.25\nDensity\nCorrect\nIncorrect\n(a) NQ\n0.00\n0.25\nDensity\n(b) LitQA2\n0\n5\n10\n15\n20\n25\n30\nDBM25\n0.00\n0.25\nDensity\n(c) DRUID\nFigure 10: DBM25 (Equation (2)) on NQ, LitQA2\nand DRUID. Correctness is based on P@1 for\njina-reranker-v2-base-multilingual.\n13\n\nDBM25\nQuestion\nGold passage\nMost similar passage\n-4.92\nwho\nwon\nthe\nacademy award for\nbest original musical\nscore?\n<Table> <Tr> <Th> Year </Th> <Th> Film </Th> <Th>\nNominees </Th> </Tr> <Tr> <Td> (83rd) </Td> </Tr>\n<Tr> <Td> The Social Network </Td> <Td> Trent\nReznor & Atticus Ross </Td> </Tr> <Tr> <Td> How\nto Train Your Dragon </Td> <Td> John Powell </Td>\n</Tr> <Tr> <Td> Inception </Td> <Td> Hans Zimmer\n</Td> </Tr> <Tr> <Td> The King’s Speech </Td> <Td>\nAlexandre Desplat </Td> </Tr> <Tr> <Td> 127 Hours\n</Td> <Td> A.R. Rahman </Td> </Tr> <Tr> <Td> 2011\n(84th) </Td> </Tr> <Tr> <Td> The Artist </Td> <Td>\nLudovic Bource </Td> </Tr> <Tr> <Td> The Adven-\ntures of Tintin </Td> <Td> John Williams </Td> </Tr>\n<Tr> <Td> Hugo </Td> <Td> Howard Shore </Td>\n</Tr> <Tr> <Td> Tinker Tailor Soldier Spy </Td> <Td>\nAlberto Iglesias </Td> </Tr> <Tr> <Td> War Horse\n</Td> <Td> John Williams </Td> </Tr> <Tr> <Td>\n2012 (85th) </Td> </Tr> <Tr> <Td> Life of Pi </Td>\n<Td> Mychael Danna </Td> </Tr> <Tr> <Td> Anna\nKarenina </Td> <Td> Dario Marianelli </Td> </Tr>\n<Tr> <Td> Argo </Td> <Td> Alexandre Desplat </Td>\n</Tr> ... </Td> </Tr> </Table>\n<P> The Academy began awarding movies for their\nscores in 1935 . The category was originally called Best\nScoring . At the time, winners and nominees were a mix\nof original scores and adaptations of pre-existing material\n. Following the controversial win of Charles Previn for\nOne Hundred Men and a Girl in 1938, a film without a\ncredited composer that featured pre-existing classical mu-\nsic, the Academy added a Best Original Score category in\n1939 . In 1942, the distinction between the two Scoring\ncategories changed slightly as they were renamed to Best\nMusic Score of a Dramatic Picture and Best Scoring of\na Musical Picture . This marked the first time the cat-\negory was split into separate genres, a distinction that\ntechnically still lasts today, although there haven’t been\nenough submissions for the musical category to be acti-\nvated since 1985 . From 1942 to 1985, musical scores\nhad their own category, with the exception of 1958, 1981\nand 1982 . During that time, both categories had many\nname changes: </P>\n-3.68\ntumhi\nho\nbandhu\nsakha tumhi cast real\nname?\n<Ul> <Li> Chandni Bhagwanani as Sanjana Ajay Pethe-\nwala </Li> <Li> Sreejita De as Shreya Bhushan Pethe-\nwala </Li> <Li> Kabeer K as Ajay Pethewala </Li> <Li>\nNeil Bhatt as Bhushan Trilokchand Pethewala </Li> <Li>\nDimple Jhangiani as Avni Pethawala </Li> <Li> Lavina\nTandon as Shaina </Li> ... </Ul>\n<P> The show began with the working title Pethawala\nbefore being named Tum Hi Ho Bandhu Sakha Tumhi .\nThe show ended due to low trp ratings . </P>\n-0.33\nwhen did the movie\nkarate kid come out?\n<P> Jaden Christopher Syre Smith (born July 8, 1998)\nis an American actor, rapper, singer and songwriter . He\nis the son of Jada Pinkett Smith and Will Smith . Jaden\nSmith’s first movie role was with his father in the 2006\nfilm The Pursuit of Happyness . He also acted in the\n2008 remake of The Day the Earth Stood Still and the\n2010 remake of The Karate Kid, and was in the 2013 film\nAfter Earth with his father . </P>\n[same as gold]\n5.84\nwho said i think\nthere is a world mar-\nket for maybe five\ncomputers?\n<P> Although Watson is well known for his alleged 1943\nstatement, \"I think there is a world market for maybe five\ncomputers,\" there is scant evidence he said it . Author\nKevin Maney tried to find the origin of the quote, but\nhas been unable to locate any speeches or documents of\nWatson’s that contain this, nor are the words present in\nany contemporary articles about IBM . One of the very\nfirst attributions may be found in The Experts Speak, a\nbook written by Christopher Cerf and Victor S. Navasky\nin 1984, however Cerf and Navasky just quote from a\nbook written by Morgan and Langford, Facts and Falla-\ncies . Another early article source (May 15, 1985) is a\ncolumn by Neil Morgan, a San Diego Evening Tribune\nwriter who wrote: \"Forrest Shumway, chairman of The\nSignal Cos., doesn’t make predictions . His role model is\nTom Watson, then IBM chairman, who said in 1958:’ I\nthink there is a world market for about five computers .\"’\nThe earliest known citation on the Internet is from 1986\non Usenet in the signature of a poster from Convex Com-\nputer Corporation as \"’ I think there is a world market for\nabout five computers’–Remark attributed to Thomas J.\nWatson (Chairman of the Board of International Business\nMachines), 1943\". All these early quotes are questioned\nby Eric Weiss, an editor of the Annals of the History of\nComputing in ACS letters in 1985 . </P>\n[same as gold]\nTable 9: Examples of samples from NQ with relatively high and low DBM25 values. Passages lacking document\ncontext are marked in purple. Passages containing distractors are marked in green with the distracting terms in bold.\n14\n\nDBM25\nQuestion\nGold passage\nMost similar passage\n-5.97\nHow long do mouse neu-\nrons\nsurvive\nfollowing\nCRISPR\ninactivation\nof\nHSPA5? (A) 14 days, (B)\n3 days, (C) 5 days, (D) 10\ndays, (E) 28 days, or (F) not\nenough info?\nWe selected Hspa5, a top hit that was not previously\nidentified as a hit in iPSC-derived neurons, for indi-\nvidual validation. In mouse embryonic fibroblasts\nexpressing CRISPRi machinery, we confirmed that\nan sgRNA targeting Hspa5 (sgHspa5) suppresses ex-\npression of the endogenous Hspa5 transcript (Fig.\n5a). In primary neurons cultured from conditional\nCRISPRi mice, AAVs delivering sgHspa5 led to\nmarked Cre-dependent neuronal death within 2 weeks\nof expression (Fig. 5b,c). Furthermore, injection of\nthis sgRNA into neonatal mice led to a severe motor\nphenotype after approximately 2 weeks in mice co-\nexpressing hSyn1-Cre, but not the sgRNA alone (Sup-\nplementary Videos 1 and 2), and the brains from mice\nwith sgHspa5 + hSyn1-Cre were markedly smaller\nin size relative to sgHspa5-only littermates (Fig. 5d).\nThis confirms the capability of our platform to un-\ncover neuron-essential genes.\nFor mouse primary neurons transduced with AAV,\nlive imaging was performed every other day using\nan ImageXpress Micro Confocal HT.ai High-Content\nImaging System (Molecular Devices). The imaging\nchamber was warmed to 37°C and equilibrated with\n5% CO2. The system used an Andor Zyla 4.5 cam-\nera with a Plan Apo ×10/0.45NA objective lens, an\n89 North LDI laser illumination unit, 10-500 ms ex-\nposure time, 1×1 binning, and 10% laser intensity\nusing 405-nm, 475-nm, and 555-nm lasers, running\nMetaXpress (version 6.7.1.157). Resulting images\nwere imported into Cell Profiler (version 4.2.1)28 and\nanalyzed using a custom pipeline. hSyn1-Cre-GFP+\nnuclei were segmented using the ‘IdentifyPrimaryOb-\njects’ module, with expected diameter 8-40 pixels, us-\ning an Adaptive threshold (size 50) and the Minimum\nCross-Entropy method, with a 1.5 smoothing scale,\n1.0 correction factor, and lower- and upper-bound\nthreshold at 0.435 and 1, respectively. Segmented\nobjects were exported, and counted in each field, then\nsummed across all fields within a well to calculate the\nnumber of objects per well (n=29 fields per well, n=4\nwells per condition), using a custom R script. This\nwas repeated for each timepoint. Data was normal-\nized to fluorescent intensity at day 8 (as before that\nday, fluorescence intensity increased linearly with\ntime in all channels as cells manufactured fluores-\ncent proteins) and percentage change was calculated\nfor each well from day 8, for subsequent timepoints\nthrough day 16.\n11.62\nBased on whole genome\nbisulfite sequencing data\n(WGBS)\nfrom\npublicly\navailable\ndatasets\n(the\nROADMAP\nepigenome\nproject and the ENCODE\ndata portal), what is the\nrelationship between DNA\nmethylation\npatterns\nbe-\ntween introns and exons\n(after excluding considera-\ntion of the first intron and\nfirst exon)? (A) There are\nno significant differences,\n(B) Introns have more DNA\nmethylation,\n(C)\nExons\nhave more DNA methyla-\ntion, (D) Neither introns nor\nexons can be methylated,\n(E) only areas very close to\nthe transcription start site,\nor (F) not enough info?\nFurther, we considered a possible association between\nthese gradients in DNA methylation, and the muta-\ntion risk in comparing exonic versus intronic DNA,\nin light of reports of subtly different mutation rates\nand subtly different DNA methylation in exons ver-\nsus introns . We checked the DNA methylation level\nof exons and introns, separately for each exon/intron\nin sequence, for a representative gene set (middle\ntertile of genes by length, and middle tertile in expres-\nsion level). While methylation in the first exon was\nsubstantially lower compared to the first intron, con-\nsistent with the exon’s more 5’ positioning, the DNA\nmethylation levels across the subsequent introns and\nexons were highly similar (Supplementary Figure S7).\nThus, in human WGBS data, after accounting for 5’\ngene end hypomethylation, we see no notably differ-\nent DNA methylation in the exonic versus intronic\nloci, and if there are any differences between introns\nand exons in mutation rates, these do not stem from\ndifferent DNA methylation.\n[same as gold]\nTable 10: Examples of samples from LitQA2 with relatively high and low DBM25 values. Passages lacking\ndocument context are marked in purple. Passages containing distractors are marked in green with the distracting\nterms in bold.\n15\n\nDBM25\nQuestion\nGold passage\nMost similar passage\n-4.71\n\"Before\nthe\npan-\ndemic,\njust\nover\n40,000 were on con-\ntinuing UI claims.\nNow, there are well\nover\n100,000\non\nstate or federal UI\nbenefits.\"\nDepartment of Workforce Development datashows that\nin the week ending on March 7, 2020, there were 41,015\nunemployment claims statewide. For the week of May\n22, 202, there were 127,745 claims.\nLisa Subeck stated on February 16, 2024 in X, formerly\nTwitter: \"The United States is an outlier, one of only\nabout half a dozen countries, without any guarantee\nof paid leave for new parents and/or other health care\nneeds.\" Tim Kaine stated on March 15, 2022 in a tweet.:\n\"Virginia women are paid 80 cents for every dollar paid\nto Virginia men.\" Mandela Barnes stated on May 23,\n2021 in Twitter: \"It’s been over 50 years since minimum\n(wage) and inflation parted ways, then over a decade\nsince the federal minimum went up at all.\" Glenn Groth-\nman stated on June 8, 2021 in Twitter: \"We have a record\n9.3 million job openings in the U.S.\" Mark Born stated\non June 2, 2021 in Twitter: \"Before the pandemic, just\nover 40,000 were on continuing UI claims. Mandela\nBarnes stated on May 23, 2021 in Twitter: \"Since 1978,\nCEO compensation rose over 1,000% and only 11.9%\nfor average workers.\" Joe Biden stated on April 15, 2020\nin comments at a virtual town hall meeting: \"Until this\nweek, they [OSHA] weren’t even enforcing these guide-\nlines [for coronavirus]. [...] Mark Born stated on June\n2, 2021 in Twitter: \"Before the pandemic, just over\n40,000 were on continuing UI claims.\n-3.59\nClaims\nthat\nPres-\nident\nGeorge\nWashington\nonce\nsaid, \"Government\nis not reason; it is\nnot eloquence;\nit\nis force.\nLike fire,\nit is a dangerous\nservant and a fearful\nmaster.\"\nThere is no record of Washington ever making this state-\nment.\nFACT CHECK: Did George Washington Call Govern-\nment ‘A Dangerous Servant And A Fearful Master’?\nAn image shared on Facebook claims that President\nGeorge Washington once said, \"Government is not rea-\nson; it is not eloquence; it is force. [...] According to\nthe website Quote Investigator, the depiction of govern-\nment as \"a dangerous servant and a fearful master\"\nis reminiscent of a centuries-old saying about water and\nfire. \"Water is a very good seruaunt, but it is a cruell\nmayster,\" reads an excerpt from 1562.\n3.88\nThe Police Service\nof Northern Ireland\n(PSNI) are to pilot a\nSnapchat social me-\ndia platform initia-\ntive to monitor so-\ncial mitigation com-\npliance in Northern\nIreland.\nThe PSNI have no plans to introduce any monitoring\nscheme on any social media platform. Complaints about\nsocial mitigation compliance can be registered on the\nPSNI website. A claim was published on social media,\nthat the Police Service of Northern Ireland (PSNI) is \"to\nroll out a new ‘Snap-fish’ pilot scheme\" on the Snapchat\nsocial media platform \"to help catch individuals not ad-\nhering to social distancing, social bubbles and gathering\nmore than six people\" (often referred to as \"social mitiga-\ntion compliance\"). [...] \"The Police Service of Northern\nIreland has no plans to introduce a ‘snap-fish’ scheme . . .\nnor indeed any new social media platforms around the\nenforcement of COVID-19 restrictions.\"\n[same as gold]\n5.88\nFamilies of the de-\nceased persons to be\ngiven an assistance\nof 4 lakh rupees, up\nfrom 2.5 lakh ru-\npees.\nIs assistance of Rs. 4 lakh being provided for families\nof deceased persons? The third claim is that ‘families\nof the deceased persons to be given an assistance of 4\nlakh rupees, up from 2.5 lakh rupees’. It is true that the\nrevised norms of assistance from the SDRF increase the\nassistance per deceased persons to Rs. 4 lakh from the\nexisting Rs. 1.5 lakh per person. It has to be noted that is\nnot for farmers alone, but for any deceased person during\na notified natural disaster.\n[same as gold]\nTable 11: Examples of samples from DRUID with relatively high and low DBM25 values. Passages lacking\ndocument context are marked in purple. Passages containing distractors are marked in green with the distracting\nterms marked in bold.\n16\n",
  "metadata": {
    "source_path": "papers/arxiv/Language_Model_Re-rankers_are_Steered_by_Lexical_Similarities_a46f88882104796a.pdf",
    "content_hash": "a46f88882104796a585ccb8e98f1f0439ce90014ce6d10e20ec785a65a6fa93a",
    "arxiv_id": null,
    "title": "Language_Model_Re-rankers_are_Steered_by_Lexical_Similarities_a46f88882104796a",
    "author": "",
    "creation_date": "D:20250225023936Z",
    "published": "2025-02-25T02:39:36",
    "pages": 16,
    "size": 746767,
    "file_mtime": 1740470191.3235319
  }
}