{
  "text": "Large Language Models are Powerful EHR\nEncoders\nStefan Hegselmann1,2, Georg von Arnim1, Tillmann Rheude1,\nNoel Kronenberg1, David Sontag3,4, Gerhard Hindricks2,\nRoland Eils1, Benjamin Wild1\n1Center for Digital Health, Berlin Institute of Health (BIH), Charit´e -\nUniversity Medicine Berlin, Berlin, Germany.\n2German Heart Center of the Charit´e, Berlin, Germany.\n3Computer Science and Artificial Intelligence Laboratory (CSAIL),\nMassachusetts Institute of Technology (MIT), Cambridge, MA, USA.\n4Layer Health, Inc., MA, USA.\nAbstract\nElectronic Health Records (EHRs) offer rich potential for clinical prediction, yet\ntheir inherent complexity and heterogeneity pose significant challenges for tra-\nditional machine learning approaches. Domain-specific EHR foundation models\ntrained on large collections of unlabeled EHR data have demonstrated promising\nimprovements in predictive accuracy and generalization; however, their training is\nconstrained by limited access to diverse, high-quality datasets and inconsistencies\nin coding standards and healthcare practices. In this study, we explore the possi-\nbility of using general-purpose Large Language Models (LLMs) based embedding\nmethods as EHR encoders. By serializing patient records into structured Mark-\ndown text, transforming codes into human-readable descriptors, we leverage the\nextensive generalization capabilities of LLMs pretrained on vast public corpora,\nthereby bypassing the need for proprietary medical datasets. We systematically\nevaluate two state-of-the-art LLM-embedding models, GTE-Qwen2-7B-Instruct\nand LLM2Vec-Llama3.1-8B-Instruct, across 15 diverse clinical prediction tasks\nfrom the EHRSHOT benchmark, comparing their performance to an EHR-\nspecific foundation model, CLIMBR-T-Base, and traditional machine learning\nbaselines. Our results demonstrate that LLM-based embeddings frequently match\nor exceed the performance of specialized models, even in few-shot settings, and\nthat their effectiveness scales with the size of the underlying LLM and the avail-\nable context window. Overall, our findings demonstrate that repurposing LLMs\nfor EHR encoding offers a scalable and effective approach for clinical predic-\ntion, capable of overcoming the limitations of traditional EHR modeling and\nfacilitating more interoperable and generalizable healthcare applications.\n1\narXiv:2502.17403v1  [cs.LG]  24 Feb 2025\n\n1 Introduction\nEHRs have become a widespread technology in modern healthcare, providing a com-\nprehensive, longitudinal view of a patient’s health status [1]. Machine learning methods\ncan leverage this rich data to perform risk stratification and support clinical deci-\nsion making [2–4]. In recent years, researchers have explored a variety of prediction\ntasks based on EHRs, including hospital readmission [5, 6], length of hospital stay [6],\nsepsis onset detection [7, 8], mortality prediction [6, 9], discharge diagnoses [6], and\nheart failure outcomes [10]. The overarching objective is to harness existing EHR data\nthrough machine learning to enhance clinical outcomes and reduce healthcare costs.\nHowever, machine learning on EHR data poses significant challenges due to its\ninherent complexity. EHR data is characterized by variable-length sequences of patient\nvisits, irregular sampling intervals, missing entries, heterogeneous and noisy infor-\nmation, and a wide range of hierarchical medical concepts [11]. As a result, deep\nlearning models often achieve only modest improvements over traditional methods\nsuch as logistic regression or tree-based methods [6, 12, 13]. To mitigate these issues,\nrecent approaches have employed large-scale foundation models that are pre-trained\non unlabeled EHR data using unsupervised learning [14]. Many of these models adopt\nstrategies from natural language processing, such as masked-word prediction as in\nBERT [15] or autoregressive next-word prediction as in GPT [16]. Treating EHR data\nas sequences of medical codes, enables analogous methods such as masked code pre-\ndiction [12, 17–19] or next code prediction [13]. However, these techniques also face\nsignificant limitations: coding standards and healthcare practices differ strongly across\nsites, and interoperable EHR foundation models would likely need to be trained on\na wide variety of EHR datasets, which is difficult to achieve due to the sensitivity\nof healthcare data. Therefore, the development of EHR-specific foundation models\nremains constrained by the limited size and restricted availability of EHR data.\nIn contrast, LLMs benefit from training on vast general-purpose text corpora and\na broad range of natural-language tasks [20]. This extensive pre-training enables their\nlanguage comprehension and allows them to capture domain-agnostic patterns that\ncan be adapted for healthcare applications. Consequently, LLMs have demonstrated\nstrong performance in extracting medical concepts [21], summarizing medical texts\n[22], and predicting medical outcomes [23] even in low-resource settings. However,\nmost modern LLMs, such as GPT [24] or Llama [25], utilize a decoder-only trans-\nformer architecture, which complicates the generation of robust text representations.\nTo overcome this limitation, recent work has introduced methods to convert decoder-\nonly LLMs into effective embedding models for downstream prediction tasks [26–29].\nAdditionally, these state-of-the-art models offer an increased context window, making\nthem well-suited for handling long inputs such as serialized EHR data.\nIn this study, we systematically evaluate whether general-purpose LLM-embedding\nmodels can effectively encode EHR records for 15 distinct clinical prediction tasks\n[30] (see Fig. 1). To this end, we first transform the EHR data into a concise text\nrepresentation capturing the most relevant patient information at prediction time.\nWe then use two LLM-embedding models, GTE-Qwen2-7B-Instruct [29, 31] and\nLLM2Vec-Llama3.1-8B-Instruct [26, 32], to generate EHR representations. The result-\ning embeddings serve as inputs to a logistic regression classifier, which is trained\n2\n\nOperational \nOutcomes\nNext / Masked Code \nPrediction\nEvaluation on 15 clinical tasks\nEHR Foundation Model\nLLM-Embedding Model\nLogistic\nRegression\n1. Training\n2. Embedding Creation\n1. LLM Training\nLLM-Embedding Model\nEHR Foundation Model\nNext Word\nPrediction\n2. LLM to LLM-Embedding Model\nContrastive Learning\nSerialize EHR\nEHRSHOT Database\nEmbedding Models\n6,739 patient records\n921,499 visits\n41,661,637 events\nNew \nDiagnosis\nChest X-Ray \nFindings \nLab Test \nResults\na)\nb)\nc)\nA fox \nUse hidden \nstates\n3. Embedding Creation\nUse hidden \nstates\n# EHR Record \n \n## Hospital Visit \n \n- Lung disease \n- Chest x-ray \n[…] \nFig. 1 Study Overview. (a) We use the EHRSHOT database for our experiments. Medical events\nof each patient are converted into numerical embeddings using an EHR foundation model or an LLM-\nembedding model. A logistic regression classification head is trained, validated, and tested on 15\nclinical tasks from 4 task groups as proposed by [30]. (b) EHR foundation models are pre-trained on\nunlabeled EHR data. Common unsupervised learning tasks are masked code or next code prediction.\nTo obtain a representation for an EHR, the hidden states of the pre-trained models are used. (c)\nLLMs are pre-trained on vast amount of text data. To obtain an LLM-embedding model architectural\nchanges are applied, and contrastive learning is used to improve the representational performance.\nTo obtain an EHR embedding, the data is first serialized as text and then processed by the LLM-\nembedding model. Again, hidden states are used for the embedding. (Icons from flaticon.com)\nseparately for each prediction task. We focus on evaluating the performance in few-shot\nsettings to evaluate the generalization ability of this approach and conduct extensive\nablation studies to identify the specific components that drive the LLM’s effectiveness.\n2 Results\n2.1 EHR Database and Prediction Tasks\nWe used the EHRSHOT database containing adult patients from the Stanford Health\nCare and Lucile Packard Children’s Hospital from 1990 to February 8th, 2023 [30]. This\ndataset includes comprehensive EHR timelines for 6.739 patients, covering 921.499\nvisits, and 41.661.637 clinical events (Table 1). The database is part of a rigorous EHR\nbenchmark containing 15 clinical prediction tasks grouped into four task groups. It also\nprovides canonical dataset splits and provides publicly available code. Table 2 presents\na detailed breakdown of the task groups, individual tasks, and the corresponding\nnumber of labels used in our experiments. Notably, a single patient may contribute\nmultiple labels for a given prediction task, as relevant clinical events can recur over\ntime.\n3\n\nTable 1 Cohort Overview. The EHRSHOT database contains 6,739 patients with 921,499 visits,\nand 41,661,637 clinical events. There are canonical dataset splits into train, validation, and test sets\nfor reproducible experiments [30]. Percentages of the total numbers in parentheses.\nAttribute\nTrain\nValidation\nTest\nTotal\nNum Events\n15,511,472\n(37.2)\n13,005,205\n(31.2)\n13,144,960\n(31.6)\n41,661,637\n(100.0)\nNum Visits\n339,504 (36.8)\n300,325 (32.6)\n281,670 (30.6)\n921,499 (100.0)\nNum Patients\n2,295 (34.1)\n2,232 (33.1)\n2,212 (32.8)\n6,739 (100.0)\nNum Female\n1,173 (34.1)\n1,142 (33.2)\n1,126 (32.7)\n3,441 (100.0)\nNum Male\n1,122 (34.0)\n1,090 (33.1)\n1,086 (32.9)\n3,298 (100.0)\nAge, mean ± SD\n59.6 ± 17.8\n58.8 ± 18.0\n59.5 ± 18.0\n59.3 ± 17.9\nAmerican Indian\n14 (56.0)\n7 (28.0)\n4 (16.0)\n25 (100.0)\nAsian\n356 (34.1)\n347 (33.3)\n340 (32.6)\n1,043 (100.0)\nBlack\n98 (32.9)\n105 (35.2)\n95 (31.9)\n298 (100.0)\nPacific Islander\n23 (31.1)\n21 (28.4)\n30 (40.5)\n74 (100.0)\nUnknown\n518 (33.1)\n530 (33.9)\n515 (32.9)\n1,563 (100.0)\nWhite\n1,286 (34.4)\n1,222 (32.7)\n1,228 (32.9)\n3,736 (100.0)\nHispanic\n374 (36.0)\n342 (32.9)\n322 (31.0)\n1,038 (100.0)\nNon-Hispanic\n1,921 (33.7)\n1,890 (33.2)\n1,890 (33.2)\n5,701 (100.0)\nTable 2 Prediction Tasks Overview. The EHRSHOT benchmark defines 15 clinical prediction\ntasks spanning four different task groups. The number of examples per task differs based on the\nprevalence and frequency of clinical events. Again, canonical splits for training, validation, and\ntesting are defined to ensure reproducible experiments [30].\nAttribute\nTrain Labels\n(Positive)\nValid Labels\n(Positive)\nTest Labels\n(Positive)\nTotal Labels\n(Positive)\nOperation Outcomes\nLong Length of Stay\n2,569 (681)\n2,231 (534)\n2,195 (552)\n6,995 (1,767)\n30-day Readmission\n2,609 (370)\n2,207 (281)\n2,189 (260)\n7,005 (911)\nICU Transfer\n2,402 (113)\n2,052 (92)\n2,037 (85)\n6,491 (290)\nAnticipating Lab Test Results\nThrombocytopenia\n68,776 (9,774)\n54,504 (6,962)\n56,338 (7,960)\n179,618 (24,696)\nHyperkalemia\n76,349 (1,215)\n60,168 (886)\n63,653 (948)\n200,170 (3,049)\nHypoglycemia\n122,108 (1,065)\n95,488 (858)\n100,568 (783)\n318,164 (2,706)\nHyponatremia\n81,336 (20,181)\n64,473 (14,674)\n67,028 (16,003)\n212,837 (50,858)\nAnemia\n70,501 (9,544)\n56,224 (7,445)\n58,155 (7,636)\n184,880 (24,625)\nAssignment of New Diagnoses\nHypertension\n1,260 (184)\n1,250 (177)\n1,261 (160)\n3,771 (521)\nHyperlipidemia\n1,684 (205)\n1,441 (189)\n1,317 (172)\n4,442 (566)\nPancreatic Cancer\n2,576 (155)\n2,215 (53)\n2,220 (56)\n7,011 (264)\nCeliac\n2,623 (62)\n2,284 (11)\n2,222 (21)\n7,129 (94)\nLupus\n2,570 (104)\n2,226 (33)\n2,243 (20)\n7,039 (157)\nAcute MI\n2,534 (175)\n2,177 (146)\n2,127 (144)\n6,838 (465)\nAnticipating Chest X-ray Findings\nChest X-Ray Findings\n7,481 (4,771)\n9,366 (6,032)\n9,428 (6,400)\n26,275 (17,203)\n4\n\n2.2 EHR Text Serialization\nTo utilize an LLM-embedding model for EHR representation, the patient records were\nencoded into a structured text based on the widely used Markdown format. Due to\nruntime constraints, the maximum serialization length was limited to 4.096 tokens\n(approximately 16.000 characters). This constraint informed the serialization strategy,\nprioritizing the inclusion of recent data to ensure critical medical information was\npreserved. The serialized record is divided into clearly labeled sections (see example\nin Fig. 2). All dates were normalized relative to a reference prediction date of January\n1st, 2024, explicitly stated at the beginning. This is followed by the patient’s basic\ndemographic information. Approximately 65% of the recorded values were time-series\ndata of Logical Observation Identifiers Names and Codes (LOINC) concepts. To reduce\nthe volume of this data, the serialization focuses on 24 frequently recorded concepts,\ngrouped into three primary categories: Body Metrics, Vital Signs, and Lab Results.\nFor each selected concept, the last three recorded values were included, along with the\ncorresponding units and a classification as low, normal, or high, where applicable. The\nserialization then summarizes all patient visits, including visit type, time and duration,\nfollowed by any event not associated with specific visits. Detailed visit records are\nthen provided in descending chronological order, beginning with the most recent visit.\nEach visit entry is further categorized into conditions, medications, and procedures\nfor improved clarity and utility. As a result, the text serialization ensures a structured\nand concise representation of EHR data, facilitating efficient processing by LLM-based\nmodels.\n2.3 LLM-Embedding Models and Baselines\nWe focus on two LLM-embedding models as part of our experiments to explore their\neffectiveness in representing EHR data. The first model, GTE-Qwen2-7B-instruct\n(GTE-Qwen2-7B), is based on the Qwen2-7B LLM [31] and incorporates bidirectional\nattention and contrastive learning to enhance embedding tasks [29]. The second model,\nLLM2Vec-Llama-3.1-8B-Instruct (LLM2Vec-Llama-3.1-8B), is built on the Llama-3.1-\n8B Instruct architecture [32] and employs similar optimization techniques to improve\nembeddings [26]. Both models were trained using instructions for the embeddings task.\nHence, we added a simple prompt for each task, e.g., “Given a patient’s electronic\nhealthcare record (EHR) in Markdown format, retrieve relevant passages that answer\nthe query: has the patient anemia?” (see Table 8). Our primary goal was to assess\nhow these LLM-based models, trained on publicly available text data, perform in rep-\nresenting EHRs compared to an EHR-specific foundation model. For this, we included\nCLMBR-T-Base, a 141-million-parameter autoregressive foundation model trained on\n2.57 million de-identified EHRs from Stanford Medicine [13, 30]. For each model, we\nused a logistic regression classification head trained on the training split, with hyper-\nparameters tuned on the validation split. Additionally, we included a counts-based\nbaseline using a Gradient Boosted Machine (GBM) model, which has demonstrated\nsuperior performance over logistic regression for EHR tasks [30].\n5\n\n# Electronic Healthcare Record\nCurrent time: [2024-01-01](2024-01-01)\n## Patient Demographics\n- Patient age: 78\n- Black\n- FEMALE\n- Hispanic or Latino\n## Recent Body Metrics\n- Body weight (oz): 1801\n- Body height (inch): 62.0, 61.0\n- Body mass index / BMI (kg/m2): 18.7 (normal)\n- Body surface area (m2): 1.47\n## Recent Vital Signs\n- Heart rate (bpm): 121 (high), 85 (normal)\n- Respiratory rate (breaths/min): 16 (normal)\n- Systolic blood pressure (mmHg): 148 (high), 117 (normal)\n- Diastolic blood pressure (mmHg): 81 (normal), 57 (low)\n- Body temperature (°F): 97.4 (normal), 98.8 (normal)\n- Oxygen saturation (%): 97 (normal), 97 (normal), 99 (normal)\n## Recent Lab Results\n- Hemoglobin (g/dL): 8.2 (low), 8.6 (low), 8.8 (low)\n- Hematocrit (%): 24 (low), 26 (low), 26 (low)\n- Erythrocytes: No recent data\n- Leukocytes (10^3/uL): 2.7 (low), 8.8 (normal), 6.2 (normal)\n- Platelets (10^3/uL): 215 (normal), 199 (normal)\n- Sodium (mmol/L): 132 (low)\n- Potassium (mmol/L): 4.2 (normal), 4.4 (normal)\n- Chloride (mmol/L): 95 (low), 102 (normal)\n- Carbon dioxide, total (mmol/L): No recent data\n- Calcium (mg/dL): 8.9 (low), 8.5 (low)\n- Glucose (mg/dL): 92 (normal), 112 (high)\n- Urea nitrogen (mg/dL): 11 (normal), 8 (normal)\n- Creatinine (mg/dL): 0.4 (low), 0.7 (normal)\n- Anion gap: No recent data\n## Past Medical Visits\n- Inpatient Visit on [2023-12-17](2023-12-17) (14 days before prediction time, \nduration: 4 days)\n- Office Visit on [2023-10-27](2023-10-27) (65 days before prediction time)\n## General Medical Events\n- Cigarette consumption: N, N, N\n- Mitral valve disorder\n## Detailed Past Medical Visits (most recent first)\n### Inpatient Visit on [2023-12-17](2023-12-17) (14 days before prediction \ntime, duration: 4 days)\n#### Conditions\n- Acute posthemorrhagic anemia\n- Partial thromboplastin time, activated\n- pH measurement, venous: 7.25, 7.31, 7.31\n#### Medications\n- furosemide 20 MG Oral Tablet\n- pantoprazole 20 MG Delayed Release Oral Tablet\n#### Procedures\n- Chest x-ray\n- Electrocardiogram report\n### Office Visit on [2023-10-27](2023-10-27) (65 days before prediction time)\n[…]\nFig. 2 Example EHR Text Serialization. The EHR data is serialized into text to apply LLM-\nembedding models. We use Markdown formatting and prioritize relevant medical information. All\ndates were normalized relative to a reference date of January 1, 2024. Next, the patient’s demographics\nare listed. Time-series data coded via Logical Observation Identifiers Names and Codes (LOINC) was\naggregated into 24 key concepts listed with the last three values, units, and classifications into low,\nnormal, and high. Then, a list of all visits and all concepts not associated with a visit are given. Lastly,\ndetailed visit entries beginning with the most recent are listed. Unique concepts are categorized into\nconditions, medications, and procedures. The last three values of a concept are given when present.\n6\n\nTable 3 Performance on All Examples. Macro averaged area under receiver operating\ncharacteristic curve (AUROC) performance and bootstrapped 95% confidence intervals of included\nmodels for four prediction task groups. The macro averaged performance across all task groups is\ngiven in the right-most column. The LLM-embedding model GTE-Qwen2-7B with a logistic\nregression (LR) classification head outperforms the EHR foundation model CLMBR-T-Base and the\nCounts-based baseline using a gradient boosted machine (GBM) head. LLM2Vec-Llama-3.1-8B only\noutperforms CLMBR-T-Base on the task group for assignment of new diagnosis. Combining the\nembeddings of the LLM-embedding models and CLMBR-T-Base by concatenation leads to a further\nincrease in performance. Additional model variants with fewer parameters or using an encoder-only\narchitecture show a lower overall performance.\nModel\nOperational\nOutcomes\nAnticipating\nLab Test\nResults\nAssignment\nof New\nDiagnosis\nAnticipating\nChest X-ray\nFindings\nMacro Avg.\nAcross Task\nGroups\nLLM-Embedding Models\nGTE-Qwen2-7B\n0.844 .821-.867\n0.867 .860-.874\n0.715 .674-.755\n0.670 .656-.683\n0.774 .749-.798\nGTE-Qwen2-1.5B\n0.800 .776-.824\n0.865 .859-.872\n0.699 .656-.741\n0.665 .652-.678\n0.757 .732-.783\nLLM2Vec-Llama-3.1-8B\n0.787 .762-.812\n0.777 .766-.787\n0.727 .680-.773\n0.676 .663-.690\n0.742 .714-.769\nLLM2Vec-Llama 2 1.3B\n0.705 .679-.731\n0.652 .641-.663\n0.633 .595-.670\n0.617 .605-.629\n0.652 .627-.676\nLLM-Embedding Model + EHR Foundation Model\nGTE-Qwen2-7B\n+ CLMBR-T-Base\n0.882 .863-.900\n0.887 .881-.893\n0.725 .682-.768\n0.711 .699-.723\n0.801 .777-.826\nLLM2Vec-Llama-3.1-8B\n+ CLMBR-T-Base\n0.830 .809-.851\n0.841 .832-.849\n0.731 .686-.777\n0.714 .702-.725\n0.779 .753-.805\nBaselines [30]\nCLMBR-T-Base\n0.824 .803-.845\n0.832 .824-.840\n0.707 .667-.746\n0.713 .702-.724\n0.769 .746-.792\nCounts-based + GBM\n0.774 .752-.797\n0.728 .716-.741\n0.719 .669-.768\n0.656 .641-.671\n0.719 .691-.748\nEncoder Language Models\nDeBERTaV3 large\n0.725 .699-.752\n0.712 .700-.724\n0.671 .625-.716\n0.625 .612-.639\n0.683 .656-.711\nDeBERTa V3 base\n0.754 .732-.777\n0.707 .694-.720\n0.661 .611-.712\n0.623 .609-.638\n0.686 .657-.716\nBERT large\n0.735 .711-.759\n0.720 .708-.732\n0.666 .619-.714\n0.640 .626-.653\n0.690 .662-.718\nBERT base\n0.741 .716-.766\n0.718 .706-.730\n0.677 .629-.724\n0.638 .625-.651\n0.694 .665-.722\nClinicalBERT\n0.745 .721-.770\n0.737 .726-.748\n0.702 .657-.746\n0.650 .637-.664\n0.709 .682-.736\n2.4 Performance Results on 15 Prediction Tasks\nThe performance results measured by the Area Under the Receiver Operating Char-\nacteristic Curve (AUROC) for all training and validation examples are presented\nin Table 3. The GTE-Qwen2-7B model demonstrated superior performance over\nCLIMBR-T-Base in three of the four task categories: operation outcomes, lab test\nresult prediction, and assignment of new diagnoses. However, in predicting chest X-ray\nfindings, the EHR foundation model (CLIMBR-T-Base) outperformed GTE-Qwen2-\n7B. The LLM2Vec-Llama-3.1-8B embeddings showed slightly lower performance\ncompared to GTE-Qwen2-7B, surpassing CLIMBR-T-Base only in the assignment of\nnew diagnoses task. Both LLM-based embedding models clearly outperformed the\ncounts-based baseline with a gradient-boosted machine head, which is traditionally\nregarded as a strong baseline for EHR tasks [12, 13]. The only exception was in\nthe assignment of new diagnoses, where the counts baseline marginally outperformed\nGTE-Qwen2-7B. We also combined the CLMBR-T-Base embedding with the LLM\nrepresentations by simple concatenation to test whether they encode orthogonal infor-\nmation. The combined embeddings led to a considerable performance boost for both\n7\n\n102\n103\n104\nModel Parameters (Millions, Log Scale)\n0.600\n0.625\n0.650\n0.675\n0.700\n0.725\n0.750\n0.775\n0.800\nMacro AUROC (95% CI)\nClinical Prediction Performance vs Model Size\nModel Types\nCLIMBR\nLLM Models\nBERT Models\nModel Types\nCLIMBR\nLLM Models\nBERT Models\nModel Names\nCLIMBR-T-Base\nGTE-Qwen2-7B\nLLM2Vec-Llama-3.1-8B\nGTE-Qwen2-1.5B\nLLM2Vec-Llama-2-1.3B\nDeBERTaV3-large\nDeBERTaV3-base\nBio_ClinicalBERT\nFig. 3 Scaling Behavior of Models. Number of model parameters (x-axis) and macro averaged\narea under receiver operating characteristic curve (AUROC) performance and 95% confidence inter-\nvals across all four task groups (y-axis). LLMs with more parameters show an increased performance.\nThe specialized EHR foundation model, CLMBR-T-Base, is the most efficient prediction model.\nmodels with an average AUROC performance of 0.801 (0.777 - 0.826) instead of 0.774\n(0.749 - 0.798) for GTE-Qwen2-7B and 0.779 (0.753 - 0.805) instead of 0.742 (0.714\n- 0.769) for LLM2Vec-Llama-3.1-8B. The smaller GTE-Qwen2-1.5B model only per-\nformed slightly worse than GTE-Qwen2-7B with an average performance of 0.757\n(0.732 - 0.783) compared to 0.774 (0.749 - 0.798) making it a performant model\nchoice for EHR embeddings. LLM2Vec-Llama-2-1.3B on the other hand performed\nsignificantly worse than the larger model which could be due to the older Llama 2\narchitecture. Fig. 3 illustrates the relationship between model size and average per-\nformance across all tasks, suggesting the trend that larger models tend to perform\nbetter. Among the evaluated models, CLIMBR-T-Base proved to be the most effi-\ncient in terms of performance relative to model size. It is noteworthy, however, that\nthe LLM-based embedding models were not specifically optimized for this EHR data\nmodality, which may partially explain their performance gap.\n2.5 Performance Results in Few-Shot Setting\nTo evaluate model performance with limited training data, we conducted experiments\nin a few-shot setting using small numbers of training examples. The experiments fol-\nlowed the EHRSHOT task definitions [30]. Fig. 4 illustrates the aggregated AUROC\nacross all subtasks within the four task categories for varying numbers of training\nexamples (see Fig. 6 for AUPRC results). Both LLM-embedding models demonstrated\nstrong results in the few-shot setting, indicating that their pretraining on general\ntext can be successfully transferred to serialized EHR data. Notably, GTE-Qwen2-\n7B consistently outperformed CLIMBR-T-Base across all training example sizes for\npredicting lab test results and assigning new diagnoses. For operational outcomes, a\nminimum of 32 training examples was required for GTE-Qwen2-7B to surpass the per-\nformance of the EHR-specific foundation model. In contrast, LLM2Vec-Llama-3.1-8B\n8\n\n1\n2\n4\n8\n16\n32\n64\n128\nAll\n# of Train Examples per Class\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAUROC\nOperational Outcomes\n1\n2\n4\n8\n16\n32\n64\n128\nAll\n# of Train Examples per Class\n0.5\n0.6\n0.7\n0.8\n0.9\nAUROC\nAnticipating Lab Test Results\n1\n2\n4\n8\n16\n32\n64\n128\nAll\n# of Train Examples per Class\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nAUROC\nAssignment of New Diagnoses\n1\n2\n4\n8\n16\n32\n64\n128\nAll\n# of Train Examples per Class\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nAUROC\nAnticipating Chest X-ray Findings\nAUROC by Task Group\nCLMBR+LR\nGTE Qwen2 7B+LR\nLLM2Vec Llama 3.1 7B+LR\nCount-based+GBM\nFig. 4 Performance in Few-Shot Settings. Macro averaged area under receiver operating char-\nacteristic curve (AUROC) performance across subtasks for four task groups across (bold). Blurred\nlines are averaged AUROC values across five bootstrapped runs using different seeds [30]. Similar\nto the EHR foundation model, CLMBR-T-Base, the LLM-embedding models show the largest per-\nformance gains over the counts-based model at intermediate numbers of training example. For an\nincreased amount of training examples, the advantage of pretrained LLM-based models decreases.\nshowed weaker performance in the few-shot setting. It only outperformed CLIMBR-T-\nBase in a limited number of scenarios, primarily for the assignment of new diagnoses.\nBoth LLM-embedding models consistently achieved improvements over the counts-\nbased baseline across most configurations, underscoring the impact of their pretraining\non general text. Consistent with previous findings [30], the largest performance gains\nover the counts-based baseline were observed at intermediate training example sizes.\nAs the number of training examples increased, the advantage of pretrained LLM-based\nmodels decreased, highlighting the diminishing returns of foundation models as more\nlabeled data becomes available.\n2.6 Effect of Different Contexts Sizes\nWe evaluated the impact of varying context sizes on the performance of the LLM-\nembedding models by testing GTE-Qwen2-7B and LLM2Vec-Llama-3.1-8B with input\nlengths of 512, 1.024, 2.048, 4.096 (the default), and 8.192 tokens. The objective was\n9\n\nTable 4 Performance of LLM-Embedding Models Across Context Sizes. Macro averaged\narea under receiver operating characteristic curve (AUROC) performance and 95% confidence\nintervals for task groups and macro averaged performance across all task groups for different context\nsizes of the LLM-embedding models. GTE-Qwen2-7B shows the best performance for 4,096-token\ncontext. LLM2Vec-Llama-3.1-8B shows the best performance for 2,048 tokens. Using a context size\nof 8,192 tokens does not show an improvement.\nModel\nOperational\nOutcomes\nAnticipating\nLab Test\nResults\nAssignment\nof New\nDiagnosis\nAnticipating\nChest X-ray\nFindings\nMacro Avg.\nAcross Task\nGroups\nGTE-Qwen2-7B\n0.844 .821-.867\n0.867 .860-.874\n0.715 .674-.755\n0.670 .656-.683\n0.774 .749-.798\n8.192 context size\n0.826 .804-.847\n0.783 .773-.793\n0.684 .637-.730\n0.678 .666-.691\n0.743 .716-.770\n2.048 context size\n0.818 .794-.842\n0.877 .870-.883\n0.723 .681-.766\n0.652 .638-.666\n0.767 .742-.793\n1.024 context size\n0.827 .805-.850\n0.885 .879-.891\n0.679 .641-.716\n0.645 .631-.658\n0.759 .736-.782\n512 context size\n0.692 .666-.718\n0.741 .730-.752\n0.641 .605-.677\n0.613 .597-.628\n0.672 .648-.696\nLLM2Vec-Llama-3.1-8B\n0.787 .762-.812\n0.777 .766-.787\n0.727 .680-.773\n0.676 .663-.690\n0.742 .714-.769\n8.192 context size\n0.793 .769-.816\n0.759 .748-.770\n0.713 .659-.767\n0.689 .676-.701\n0.738 .708-.769\n2.048 context size\n0.812 .787-.837\n0.879 .873-.885\n0.721 .688-.754\n0.652 .636-.667\n0.766 .744-.788\n1.024 context size\n0.797 .773-.820\n0.889 .884-.895\n0.662 .626-.699\n0.630 .615-.645\n0.744 .721-.767\n512 context size\n0.663 .634-.691\n0.820 .810-.830\n0.631 .588-.673\n0.609 .593-.625\n0.680 .653-.707\n512\n1024\n2048\n4096\n8192\nInput Context Size\n0.65\n0.70\n0.75\n0.80\nMacro AUROC (95% CI)\nPerformance Across Context Sizes\nModel Names\nGTE-Qwen2-7B\nLLM2Vec-Llama-3.1-8B\nFig. 5 Performance of LLM-Embedding Models Across Context Sizes. Different context\nsizes (x-axis) and the respective macro averaged area under receiver operating characteristic curve\n(AUROC) performance across all task groups (y-axis) for the LLM-embedding models. Full results\nare given in Table 4.\nto determine whether longer input sequences enhance performance and to assess if the\nmodels effectively leverage additional historical data. As summarized in Table 4 and\nFig. 5, the two models exhibited distinct behaviors.\nFor GTE-Qwen2-7B, the best overall performance was achieved with a context of\n4.096 tokens, with only a slight decline observed when using 2.048 or 1.024 tokens. This\nsuggests that the most relevant information is contained within the first 1.024 tokens\nand that the model can accurately extract these details even when presented within\n10\n\nTable 5 Performance of LLM-Embedding Models for Chunked Context. Macro averaged\narea under receiver operating characteristic curve (AUROC) performance and 95% confidence\nintervals for task groups and macro averaged performance across all task groups for chunked inputs\nand averaged embeddings of the LLM-embedding models. The performance trends are similar to the\ncontext size experiments with less decrease in performance as all 4.096 input tokens are still\nincorporated.\nModel\nOperational\nOutcomes\nAnticipating\nLab Test\nResults\nAssignment\nof New\nDiagnosis\nAnticipating\nChest X-ray\nFindings\nMacro Avg.\nAcross Task\nGroups\nGTE-Qwen2-7B\n0.844 .821-.867\n0.867 .860-.874\n0.715 .674-.755\n0.670 .656-.683\n0.774 .749-.798\n2 x 2.048 tokens chunks\n0.827 .803-.852\n0.860 .853-.868\n0.713 .673-.753\n0.670 .658-.682\n0.768 .743-.792\n4 x 1.024 tokens chunks\n0.836 .814-.859\n0.854 .846-.861\n0.709 .665-.753\n0.671 .659-.683\n0.768 .742-.793\n8 x 512 tokens chunks\n0.768 .743-.793\n0.784 .774-.794\n0.722 .682-.762\n0.667 .655-.680\n0.735 .710-.760\nLLM2Vec-Llama-3.1-8B\n0.787 .762-.812\n0.777 .766-.787\n0.727 .680-.773\n0.676 .663-.690\n0.742 .714-.769\n2 x 2.048 tokens chunks\n0.812 .787-.837\n0.879 .873-.885\n0.721 .688-.754\n0.652 .636-.667\n0.766 .744-.788\n4 x 1.024 tokens chunks\n0.797 .773-.820\n0.889 .884-.895\n0.662 .626-.699\n0.630 .615-.645\n0.744 .721-.767\n8 x 512 tokens chunks\n0.774 .750-.797\n0.774 .750-.797\n0.774 .750-.797\n0.774 .750-.797\n0.774 .750-.797\na longer context. In contrast, LLM2Vec-Llama-3.1-8B showed a marked improvement\nwith a context size of 2.048 tokens and a moderate gain at 1.024 tokens, indicating that\nthe model struggles to effectively handle longer sequences up to 4.096 tokens. Both\nmodels experienced a significant drop in performance when limited to only 512 tokens,\nhighlighting that such a short context omits crucial information; a noteworthy point\ngiven that many existing language models are constrained to 512 input tokens [33].\nFurthermore, extending the context to 8.192 tokens resulted in decreased performance\nfor both models, implying limitations in extracting relevant information from very\nlong inputs. This decline may be partially attributed to the mean pooling of the\nlast hidden layers used for generating embeddings, which can dilute the impact of\nimportant token-level information; an alternative strategy, such as incorporating an\nadditional attention mechanism, might help mitigate this issue.\n2.7 Effect of Chunked Contexts\nTo investigate whether the models can process the full 4.096-token context cohesively,\nwe conducted an experiment in which the serialized EHR input was divided into chunks\nof 512, 1.024, and 2.048 tokens. For each chunk, separate embeddings were generated\nand then averaged to create a final representation (see Table 5). For GTE-Qwen2-7B,\nthe performance decrease with smaller chunks was relatively modest, indicating that\nthe information contained within the full 4.096-token input remains effectively used\neven when segmented. Notably, using 512-token chunks yielded an overall AUROC per-\nformance of 0.735, compared to 0.672 when processing a contiguous 512-token context,\nwhich suggests that chunking can mitigate input constraints. In contrast, LLM2Vec-\nLlama-3.1-8B demonstrated improved performance with chunked inputs, consistent\nwith its behavior on shorter context sizes. This enhancement was primarily driven\nby better lab value prediction, implying that LLM2Vec-Llama-3.1-8B is particularly\neffective when processing inputs of up to 2.048 tokens.\n11\n\nTable 6 EHR Serialization Ablation Experiments for LLM-embedding Models. Macro\naveraged area under receiver operating characteristic curve (AUROC) performance and 95%\nconfidence intervals for task groups and macro averaged performance across all task groups for\ndifferent EHR serialization ablations studies. Removing the task-specific instructions and aggregated\ninformation lead to the largest drop in performance for both LLM-embedding models. For\nLLM2Vec-Llama-3.1-8B, some ablations even show an increased overall performance.\nModel\nOperational\nOutcomes\nAnticipating\nLab Test\nResults\nAssignment\nof New\nDiagnosis\nAnticipating\nChest X-ray\nFindings\nMacro Avg.\nAcross Task\nGroups\nGTE-Qwen2-7B\n0.844 .821-.867\n0.867 .860-.874\n0.715 .674-.755\n0.670 .656-.683\n0.774 .749-.798\nno instructions\n0.762 .737-.788\n0.768 .757-.780\n0.703 .663-.743\n0.666 .653-.680\n0.725 .700-.750\nno demographics\n0.844 .822-.866\n0.864 .857-.871\n0.700 .654-.746\n0.673 .661-.685\n0.770 .744-.797\nno aggregated\n0.855 .833-.877\n0.713 .701-.726\n0.740 .697-.783\n0.668 .654-.682\n0.744 .718-.770\nno visits (both)\n0.752 .728-.776\n0.878 .872-.884\n0.735 .690-.779\n0.663 .649-.677\n0.757 .731-.783\nno conditions\n0.839 .816-.862\n0.866 .859-.873\n0.731 .694-.767\n0.649 .635-.663\n0.771 .748-.794\nno medications\n0.840 .817-.862\n0.866 .859-.872\n0.706 .664-.748\n0.672 .659-.685\n0.771 .746-.796\nno procedures\n0.842 .819-.865\n0.866 .859-.873\n0.718 .677-.758\n0.668 .654-.682\n0.773 .749-.798\nLLM2Vec-Llama-3.1-8B\n0.787 .762-.812\n0.777 .766-.787\n0.727 .680-.773\n0.676 .663-.690\n0.742 .714-.769\nno instructions\n0.759 .734-.784\n0.751 .740-.763\n0.723 .677-.770\n0.676 .663-.690\n0.727 .700-.755\nno demographics\n0.793 .769-.817\n0.780 .769-.790\n0.719 .672-.765\n0.679 .667-.692\n0.743 .715-.770\nno aggregated\n0.812 .789-.836\n0.705 .692-.718\n0.715 .665-.766\n0.676 .662-.690\n0.727 .698-.757\nno visits (both)\n0.734 .709-.759\n0.862 .855-.869\n0.721 .667-.775\n0.677 .663-.690\n0.749 .718-.779\nno conditions\n0.795 .771-.819\n0.784 .774-.795\n0.704 .654-.754\n0.662 .649-.675\n0.736 .708-.765\nno medications\n0.786 .762-.810\n0.786 .776-.797\n0.714 .663-.765\n0.677 .664-.691\n0.741 .711-.770\nno procedures\n0.791 .767-.816\n0.779 .768-.789\n0.733 .690-.776\n0.675 .661-.688\n0.744 .718-.770\n2.8 Ablations of EHR Serialization\nTo assess the impact of different components of the EHR serialization, we conducted a\nseries of ablation studies for both LLM-embedding models, as summarized in Table 6.\nThe removal of instructions had a notable effect on performance for both GTE-Qwen2-\n7B and LLM2Vec-Llama-3.1-8B, leading to a decrease from 0.774 (0.749 - 0.798) to\n0.725 (0.700 - 0.750) and from 0.742 (0.714 - 0.769) to 0.727 (0.700 - 0.755) in averaged\nAUROC, respectively. The most significant drop was observed in tasks related to oper-\national outcomes and lab result prediction, suggesting that the instructions played a\ncritical role in guiding the model to focus on relevant clinical information. The inclu-\nsion of aggregated information, comprising four body metrics, six vital signs, and 14 lab\nvalues, also had a major impact on predictive performance. Removing this information\nsubstantially reduced the accuracy of lab test predictions, with performance dropping\nfrom 0.867 (0.860 - 0.874) to 0.713 (0.701 - 0.726) for GTE-Qwen2-7B and from 0.777\n(0.766 - 0.787) to 0.705 (0.692 - 0.718) for LLM2Vec-Llama-3.1-8B. This highlights\nthe importance of the most recent lab values in predicting future lab results. The\neffect was particularly pronounced for GTE-Qwen2-7B in predicting conditions such\nas thrombocytopenia, hyperkalemia, and hyponatremia. However, for hypoglycemia\nand anemia, GTE-Qwen2-7B performed on par with CLIMBR-T-Base, even though\nthe lab values were still part of the serialized representation (see Fig. 7 and Fig. 8\nfor task specific performance). Interestingly, the removal of aggregated information\nled to slight performance improvements in predicting operational outcomes and the\nassignment of new diagnoses, suggesting that in some cases, a more focused represen-\ntation may be beneficial. Visit-related information proved to be particularly critical\n12\n\nfor operational outcome prediction, as removing it led to a performance drop from\n0.844 (0.821 - 0.867) to 0.752 (0.728 - 0.776) for GTE-Qwen2-7B and from 0.787 (0.762\n- 0.812) to 0.734 (0.709 - 0.759) for LLM2Vec-Llama-3.1-8B. Notably, the removal of\nvisit information resulted in a substantial performance increase for LLM2Vec-Llama-\n3.1-8B predicting lab values, from 0.777 (0.766 - 0.787) to 0.862 (0.855 - 0.869). This\nsuggests that the model struggles to extract relevant information when presented with\na large volume of contextual data, a challenge that GTE-Qwen2-7B handles more\neffectively. On the other hand, the removal of demographic information, conditions,\nmedications, and procedures had only a minor effect on performance, indicating that\nthese features were either redundant or less relevant for the prediction tasks. While\nthe influence of aggregated information and visit details was clear for predicting oper-\national outcomes and lab results, the ablation studies showed less impact on tasks\nrelated to new diagnoses and chest X-ray prediction. This suggests that these latter\ntask groups rely more on a broader set of contextual factors rather than any single\ncomponent of the serialization.\n3 Discussion\nOur study demonstrates that general-purpose LLM-embedding models, originally\npre-trained on extensive natural language corpora, can be repurposed as robust foun-\ndation models for EHR prediction tasks. Specifically, models like GTE-Qwen2-7B and\nLLM2Vec-Llama-3.1-8B not only surpassed a strong counts-based baseline but, in sev-\neral clinical domains, matched or even exceeded the performance of a dedicated EHR\nfoundation model (CLIMBR-T-Base) [13, 30]. Notably, this is despite CLIMBR-T-\nBase being trained on data from the same hospital system as the EHRSHOT database,\nunderscoring the strong generalization capabilities of LLM-based embeddings. This\nis especially significant given the inherent challenges of limited and heterogeneous\nEHR data. Across 15 diverse clinical tasks, including few-shot scenarios, our findings\nindicate that the transferable knowledge acquired during large-scale text pretraining\nenables these models to effectively capture and encode complex clinical information.\nFurthermore, our analysis shows that performance scales with both model size and\narchitectural improvements, suggesting that future advancements in general-purpose\nLLMs may further enhance their applicability in healthcare settings. In contrast, tra-\nditional EHR-specific models face scaling limitations due to restricted data availability\nand the necessity for specialized architectures [34]. Collectively, these results high-\nlight that repurposing LLMs as embedding generators offers a powerful, flexible, and\nscalable alternative for clinical prediction tasks.\nOur experiments suggest that several key factors contribute to the success of\nLLM-embedding models for encoding EHR data. First, providing clear, task-specific\ninstructions within the serialized EHR text was crucial, as it guided the models to focus\non the most clinically relevant sections, particularly benefiting tasks that require atten-\ntion to specific input segments. This design leverages the general-purpose pretraining\nand further instruction-based fine-tuning of LLM-embedding models [26, 29], enabling\nthem to identify and extract meaningful patterns from heterogeneous and complex\nEHR inputs. Second, the inclusion of aggregated clinical data, especially semantic\n13\n\nLOINC-coded events and detailed visit-related information, was strongly associated\nwith improved performance on operational outcomes and lab test predictions. In con-\ntrast, other components of the EHR did not show as clear a relationship with predictive\naccuracy. The effective handling of context size also plays a significant role. While\nmodels like GTE-Qwen2-7B can fully utilize contexts up to 4,096 tokens, our findings\nshow that performance degrades with overly short or excessively long inputs. Short\ncontexts risk omitting recent, crucial clinical encounters, whereas overly long contexts\ncan dilute important token-level details. In cases where models face challenges with\nlonger inputs such as LLM2Vec-Llama-3.1-8B our chunking experiments indicate that\nsegmenting the input and averaging the resulting embeddings can mitigate this issue.\nFinally, combining LLM embeddings with those from an EHR-specific foundation\nmodel (CLMBR-T-Base) led to a considerable boost in performance. This improve-\nment suggests that LLMs capture orthogonal and complementary clinical information.\nOne explanation for this effect could be the limited context size of CLMBR, restricted\nto 496 medical codes, which prevents it from fully leveraging all information present\nin our EHR serialization [13]. Alternatively, the boost may stem from the general-\npurpose pretraining on vast text corpora, which provides LLMs with domain-agnostic\nknowledge that remains untapped in models trained exclusively on EHR data.\nUnlike count-based models and specialized EHR foundation models, LLM-based\nmodels are agnostic towards specific coding systems and data formats. Traditional\nEHR models depend on predefined vocabularies such as SNOMED, LOINC, and ICD,\nwhich restricts their applicability across diverse healthcare settings. In contrast, LLMs\noperate on raw textual representations, enabling them to interpret any code that\ncan be mapped to a human-readable form. This flexibility is particularly valuable\ngiven the persistent challenges in EHR data interoperability, stemming from variable\ncoding practices, privacy constraints, and regulatory limitations, which often hinder\nthe aggregation of large, standardized datasets for model pretraining [35]. Moreover,\nsince LLMs are pre-trained on broad and diverse text corpora, including medical\nliterature and case reports, they are well-equipped to capture the semantics of rare\nor underrepresented clinical concepts [36]. This capacity mitigates the limitations of\ncount-based models that typically filter out infrequent events, ensuring that even rare\nbut clinically significant phenomena are effectively encoded [37]. Additionally, the\nability of LLMs to process both structured and unstructured data paves the way for\nintegrated, multimodal representations that could further enhance clinical decision\nsupport systems [38].\nOur findings highlight that LLM-embedding methods merge the strengths of count-\nbased models, specialized EHR models, and the expansive generalization capabilities\nof large language models. While count-based and specialized EHR foundation mod-\nels can be tuned to output well-calibrated probabilities, thereby enhancing trust in\nclinical applications, they are inherently limited by their reliance on predefined vocab-\nularies and constrained training data. In contrast, LLMs can process any textual\nrepresentation, enabling them to effectively handle rare or unseen codes, although they\ntypically produce unstructured outputs that challenge clinical grounding and calibra-\ntion [23]. LLM-embedding methods bridge this gap by leveraging the representational\npower of LLMs while maintaining compatibility with traditional predictive modeling\n14\n\nframeworks. By transforming EHR data into structured text and embedding it using\ngeneral-purpose LLMs, we enable the application of logistic regression or other con-\nventional machine learning classifiers to make clinically meaningful predictions. This\napproach preserves the calibration advantages of traditional models while benefiting\nfrom the rich contextual understanding of LLMs.\n3.1 Limitations\nThis study has several limitations. First, our approach relies on a subjectively designed\nEHR serialization that we deemed to capture the most medically relevant informa-\ntion. This design choice may introduce bias when comparing LLM-based embedding\nmodels with dedicated EHR foundation models that operate on raw data. In addition,\nthe performance of the LLM-embedding models is sensitive to the specific content\nand instructions provided in the serialized text, which may limit reproducibility and\ngeneralization ability. Although these models achieve competitive predictive perfor-\nmance across diverse clinical tasks, including few-shot scenarios, their substantially\nlarger parameter counts lead to longer computation times and higher resource usage.\nMoreover, by relying on LLM embedding methods, we must train a downstream clas-\nsifier from scratch, thereby forgoing the inherent zero-shot or few-shot prompting\ncapabilities of LLMs. Finally, our serialization was limited to 4,096 tokens to manage\nruntime constraints, potentially omitting valuable long-range historical information,\nand our evaluation on a single institutional dataset may limit broader applicability\nacross diverse healthcare systems.\n3.2 Future work\nFuture research should explore serialization-free approaches that allow LLMs to pro-\ncess raw EHR data directly, thereby reducing potential biases introduced by manual\ntext transformation. Integrating zero-shot and few-shot prompting into the LLM-based\nembedding framework could further enhance model flexibility and reduce dependency\non downstream training. It will also be important to develop strategies to extend the\neffective context window beyond 4.096 tokens to capture more comprehensive patient\nhistories. Moreover, investigating techniques for distilling large LLMs into smaller,\nmore efficient models may enhance their practical applicability in clinical settings.\nFinally, expanding evaluations to multi-institutional datasets and examining how\ncomplementary insights from both domain-specific EHR models and general-purpose\nLLMs can be synergistically combined will be critical for advancing the development\nof robust, scalable EHR foundation models.\n4 Methods\n4.1 EHR Database and Prediction Task\nThe EHR data utilized in our experiments is from the EHRSHOT benchmark for few-\nshot evaluation of EHR foundation models [30]. We obtained version 2.1 of the dataset,\nwhich is accessible via gated access under a research data use agreement. This dataset\ncomprises longitudinal records for 6,739 patients, 921,499 visits, and 41,661,637 clinical\n15\n\nevents collected between 1990 and February 8th, 2023. Each clinical event is linked to a\nspecific patient and includes information such as start time, end time, a semantic code,\na value, unit, visit ID, and the corresponding OMOP source table. We used the official\nehrshot-benchmark repository1 as a starting point to design our experiments enabling\nto us to build on existing functionalities and to facilitate comparisons with prior meth-\nods. The benchmark uses the Framework for Electronic Medical Records (FEMR)2,\nwhich provides Python classes for efficient loading and processing of EHR data. All\nextensions and experiments conducted for this paper have been made publicly avail-\nable via our GitHub repository: https://github.com/stefanhgm/ehrshot-benchmark.\nThe EHRSHOT benchmark defines a rigorous evaluation including 15 clinical predic-\ntions tasks categorized into four groups: operational outcomes, anticipating lab test\nresults, assignment of new diagnoses, and anticipating chest X-ray findings [30]. Task\nlabels are derived from clinical events, resulting in significant variations in task-specific\nsample sizes. For instance, frequent events like lab tests provide considerably more\nexamples compared to rarer events such as new diagnoses. The benchmark focuses\non analyzing model performance in a few-shot setting, which is particularly relevant\nfor large pre-trained foundation models [14] due to their ability to generalize from\nlimited training data. To this end, the benchmark defines evaluation settings with a\nconstrained number of training and validation examples. Specifically, for k in 1, 2, 4,\n8, 12, 16, 24, 32, 48, 64, 128, the benchmark uses k positive and k negative training\nexamples, along with k positive and k negative validation examples, to train and tune\nsupervised classifiers. Testing is always performed on the full set of examples. The\nclassifiers evaluated within the EHRSHOT framework include logistic regression, ran-\ndom forests, and gradient boosting machines [39]. Performance is reported using the\narea under the receiver operating characteristic curve (AUROC) and the area under\nthe precision-recall curve (AUPRC). For few-shot settings, we average the results over\nfive runs with different seeds and compute bootstrapped 95% confidence intervals [30].\nMacro averages are reported for each task group, and an overall macro average is\nprovided across all groups.\n4.2 EHR Text Serialization\nTo leverage LLM-embedding models for representing EHR records, we serialized the\nrecords into textual formats. We had to limit the length of the serializations to 4.096\ntokens (approximately 16.000 characters) to carry out all experiments on the available\ncomputing infrastructure (see Section 4.6). Two experimental runs were conducted\nwith serializations extending up to 8.192 tokens. The primary goal was to create a\ndetailed and informative serialization requiring minimal preprocessing while ensur-\ning that medically relevant information appeared early in the text. This approach\nmitigated truncation risks in lengthy records, preserving critical details even when\nolder entries were omitted. To convert the visits and clinical events in the EHRSHOT\ndataset into text, we leveraged the semantic information embedded in the dataset.\nEach clinical event was labeled using the format “ontology/code”. EHRSHOT pro-\nvided a set of prepared ontologies for resolving concept codes into their descriptions,\n1Github repository: https://github.com/som-shahlab/ehrshot-benchmark\n2Github repository: https://github.com/som-shahlab/femr\n16\n\nwhich we incorporated. An analysis was performed to evaluate the utilization of these\nontologies for all events of a subset of 200 patients across the task groups for opera-\ntional outcomes and new diagnoses, covering 2.968 labels. We identified the following\nontologies: Logical Observation Identifiers Names and Codes, SNOMED, RxNorm,\nCPT4, Domain, CARE SITE, RxNorm Extension, Medicare Specialty, ICD10PCS,\nCMS Place of Service, Cancer Modifier, ICD9Proc, CVX, ICDO3, HCPCS, OMOP\nExtension, Condition Type. We excluded ontologies containing only a single value\n(Domain, Medicare Specialty, CMS Place of Service, OMOP Extension, and Condi-\ntion Type). Codes of the ontologies CPT4, CARE SITE, ICD10PCS, Cancer Modifier,\nCVX, and ICDO3 were not resolved with the provided ontology. Cancer Modifier\ncode contained UICC cancer stages that we parsed manually. For CPT4, ICD10PCS,\nand CVX we used custom mapping files that we manually added.3 We excluded\nCARE SITE and ICDO3 since we found no way to resolve these to useful description.\nVarious approaches exist for serializing structured data, including row-by-row serializa-\ntion [40], template-based methods [41], or structured data formats like JSON, HTML,\nand Markdown [42]. We used Markdown due to its minimal overhead and overall ben-\nefits of using a structured data input format for LLMs [43]. To harmonize dates, all\ntimestamps were normalized relative to January 1, 2024, designated as the prediction\nreference time. Serialization began with patient demographics, typically the first event\nfor each patient, where birthdates were converted into ages (in years) for simplicity.\nSince 65% of the dataset comprised time-series data encoded via LOINC, which we\nfound imbalanced, we aggregated LOINC-coded events. Using the same patient subset\nas the ontology analysis, we identified the most frequent codes and categorized them\ninto vital signs, body metrics, and lab values, selecting 24 key medical concepts. To\navoid duplicates, we merged synonymous codes (see Table 7). The last three values of\neach concept are given, and we filtered implausible values. To further enrich the text\nrepresentation, we manually added default units and assessments (low, normal, high)\nbased on standard ranges (see Table 7). Following the aggregated data, a summary\nof all visits was included to address the potential truncation of older visits. Events\nnot associated with visits were then presented, using the same aggregation logic to\ndisplay the last three values where applicable. Finally, a detailed chronological presen-\ntation of all visits was included, with events categorized into conditions (SNOMED,\nVisit, Cancer Modifier, CVX, HCPCS), medications (RxNorm, RxNorm Extension),\nand procedures (CPT4, ICD10PCS, ICD9Proc).\n4.3 Potential Bias of Manually Defining an EHR Text\nSerialization\nDefining an EHR serialization involved subjective decisions, which may have intro-\nduced bias. For instance, awareness of the prediction tasks could influence the\nprioritization of certain data elements, potentially favoring task-relevant information.\nTo minimize this risk, we aimed to create an objectively defined serialization that\nencapsulates key aspects of the EHR records. Also, due to computational constraints,\n3We downloaded CPT4\nfrom https://gist.github.com/lieldulev/439793dc3c5a6613b661c33d71fdd185,\nICD10PCS from https://hcup-us.ahrq.gov/toolssoftware/procedureicd10/procedure icd10 archive.jsp, and\nCVX from https://www2a.cdc.gov/vaccines/iis/iisstandards/vaccines.asp?rpt=cvx.\n17\n\nwe evaluated only three variants of our final serialization for 4,096 tokens: (1) append-\ning a list of all unique conditions at the beginning, (2) omitting the three values for the\nlisted comments (e.g., for “Cigarette consumption” and “pH measurement, venous”\nin Fig. 2), and (3) combining both approaches. For GTE-Qwen2-7B, the chosen seri-\nalization performed best, but all variants outperformed CLMBR-T-Base. However,\nfor LLM2Vec-Llama-3.1-8B, only the serialization omitting all values (variant 2) per-\nformed slightly better. We selected the current serialization for its simplicity and\ncompleteness of medical information. This approach avoided introducing additional\nentries of all unique conditions and preserved the most recent values for visit-level\nconcepts.\n4.4 LLM-Embedding Models and Baselines\nIn this study, we evaluated two LLM-embedding models, GTE-Qwen2-7B and\nLLM2Vec-Llama-3.1-8B, based on state-of-the-art decoder-only LLMs. These models\nwere selected for their ability to handle the 4.096-token EHR serializations used in\nour experiments. For comparison, we also tested a smaller variant of both models. As\nadditional baselines we included commonly used encoder-only embedding models with\nsmaller input sizes (512 tokens). To use them with 4,096 token inputs, the EHR seri-\nalizations were split into up to eight 512-token chunks, and the resulting embeddings\nwere averaged to generate a single representation. For all LLM-embedding models and\nsmaller language models used in this study, we used the mean pooling of the last layer\nas the final embedding [26, 44]. The LLM2Vec models used a slight variation that only\nincorporates the tokens that do not belong to the instruction. Below is an overview of\nall models:\nGTE-Qwen2-7B\nThis LLM-embedding model is based on the Qwen2-7B-Instruct LLM [31] using a\ndecoder-only Transformer architecture with 28 layers, 28 attention heads, and a hidden\nsize of 3.5844. It was trained with autoregressive next token prediction and converted\ninto an embedding model using the General Text Embedding (GTE) method [29]. This\nconversion replaces causal attention with bidirectional attention, enabling the model\nto attend to both left and right contexts for token embedding, akin to BERT. Con-\ntrastive learning was applied using a mixture of private datasets to enhance embedding\nperformance. The model also incorporates instructions tailored for embedding tasks,\nsupporting a context size of up to 32.000 tokens.\nGTE-Qwen2-1.5B\nA smaller variant of GTE-Qwen2-7B, this model is based on Qwen2-1.5B-Instruct,\nwith 28 layers, 12 attention heads, and a hidden size of 1,536. It was also trained using\nthe GTE method [29] and supports a context size of up to 32.000 tokens.\n4Hugging Face identifier: Alibaba-NLP/gte-Qwen2-7B-instruct\n18\n\nLLM2Vec-Llama-3.1-8B\nThis model is built upon the Llama-3.1-8B-Instruct LLM [32] with a decoder-only\nTransformer architecture with 32 layers, 32 attention heads, and a hidden size of\n4.0965. Initially trained for next-token prediction, it was converted to an embedding\nmodel using the LLM2Vec method [26]. This method adds bidirectional attention\nand fine-tunes the model with supervised contrastive learning on embedding tasks.\nThe finetuning used curated data from the public E5 dataset [45, 46], containing\napproximately 1.5 million entries. The model supports task-specific instructions and\naccommodates a context size of up to 128.000 tokens.\nLLM2Vec-Llama-2-1.3B\nA smaller LLM2Vec variant, this model is derived from Sheared-LLaMA-1.3B [47], a\npruned version of the Llama-2-7B-hf model [25]. It includes 24 layers, 16 attention\nheads, and a hidden size of 2.048. The model follows the same LLM2Vec training\nmethodology as the larger LLM2Vec-Llama-3.1-8B [26].\nDeBERTa v3 base/large\nDeBERTa v3 is an encoder-only Transformer model designed for token embeddings\n[48]. It improves upon its predecessor by replacing the masked language modeling\nobjective with replaced token detection and utilizing Gradient-Disentangled Embed-\nding Sharing. We evaluated the base variant (12 layers, 12 attention heads, 768 hidden\nsize) and the large variant (24 layers, 12 attention heads, 1,024 hidden size), with\nparameter counts of 183M and 434M, respectively6.\nBERT base/large\nBERT is a well-established text embedding model using an encoder-only transformer\ntrained with the masked language modelling objective [15]. We included both the base\n(12 layers, 12 attention heads, 768 hidden size, 110M parameters) and large (24 layers,\n16 attention heads, 1,024 hidden size, 340M parameters) variants as benchmarks7.\nWhile not state-of-the-art, BERT models remain widely used in embedding tasks.\nBio ClinicalBERT\nThis model builds on BERT-Base, further fine-tuned on biomedical [49] and clinical\ndata [50]. It is a widely adopted embedding model for medical text and was included\nas a baseline for comparison8.\nCLIMBR-T-Base\nCLMBR-T-Base is a specialized EHR foundation model trained on 2.57 million deiden-\ntified EHRs from Stanford Medicine with autoregressive next code prediction [13, 30].\nIt uses gated recurrent units and has 12 layers and a hidden dimension of 7689. The\n5Hugging Face identifier: McGill-NLP/LLM2Vec-Meta-Llama-31-8B-Instruct-mntp-supervised\n6Hugging Face identifiers: microsoft/deberta-v3-base,large\n7Hugging Face identifiers: google-bert/bert-base,large-uncased\n8Hugging Face identifier: emilyalsentzer/Bio ClinicalBERT\n9Hugging Face identifier: StanfordShahLab/clmbr-t-base\n19\n\nmodel has 141M parameters and allows for a context window of 496 codes. CLIMBR-\nT-Base has demonstrated consistent improvements over count-based baselines for a\nvariety of clinical prediction tasks [30]. It serves as a main baseline for our experiments\nto test specialized EHR models against general purpose text embeddings models for\nrepresenting EHR records.\nLLM-Embedding Model and CLIMBR-T-Base\nTo test whether the LLM-embedding models and the EHR foundation model learn\northogonal information, we combined both models for the prediction. To this end we\nsimply appended both embeddings. The resulting embeddings have dimensions 4.352\n(GTE-Qwen2-7B) and 4.864 (LLM2Vec-Llama-3.1-8B).\nCounts-based Model\nCounts models have proven to be strong baselines for EHR prediction tasks [6, 12, 13].\nThe basic idea is to encode all EHR events of a patient in a single vector where\neach entry represents the number of occurrences of a medical concept. We used the\ncounts baseline introduced in [30] that further extends this approach with ontology\nexpansion, enriching the vectors with parent and child concepts.\nBased on the embeddings or the counts vectors given by the methods described\nabove a classification head was trained and validated for each prediction task. For the\nembedding models we used a logistic regression head. For the counts-based model,\nwe used a gradient boosting machine [39], which proved superior [30]. We adopted\nthe parameter tuning of the classification heads from the EHRSHOT benchmark to\nensure comparability of results.\n4.5 Instructions for LLM-Embedding Models\nThe GTE and LLM2Vec models used instruction-tuned embeddings, requiring task-\nspecific prompts. Hence, we added simple instructions for each prediction task based\non their respective instruction templates. For instance, for prediction of anemia we\nadded “Given a patient’s electronic healthcare record (EHR) in Markdown format,\nretrieve relevant passages that answer the query: has the patient anemia”. The exist-\ning EHRSHOT benchmark encoded the EHRs of the same patient and the identical\nprediction times only once for efficiency reasons. However, to support task-specific\ninstructions, we had to change this default behavior leading to 1.161.412 instead of\n406.379 EHRs to encode resulting in longer processing times. The difference between\n1.161.412 labels used in our experiments and the total number of labels of 1.178.665\n(Table 2) is since some labels even have the same tasks and prediction time and are\nmerged. We list all instructions in Table 8 and perform ablations testing the effect of\nthe instructions.\n4.6 Computational Setup and Running Times\nAll experiments were conducted on the Charit´e High-Performance Cluster using Nvidia\nA100 GPUs with 80 GB memory, configured with one, four, or eight GPUs (DGX\n20\n\nsystems). Running the GTE-Qwen2-7B model with our serialization truncated at\n4,096-tokens on an 8-GPU DGX system required approximately 20 hours. For the\nLLM2Vec-Llama-3.1-8B model, runtime errors occurred during multi-GPU experi-\nments with the full dataset. These issues were resolved by splitting the data into\nsmaller batches, which introduced additional overhead. Additionally, we optimized the\nLLM2Vec code by removing an initial text pruning step that took approximately eight\nhours when using the full data. Using this modified setup, the LLM2Vec-Llama-3.1-8B\nmodel took approximately 30 hours to complete on eight A100 GPUs.\n4.7 Performance Results on 15 Prediction Tasks and Few-Shot\nSetting\nFollowing the EHRSHOT benchmark, we evaluated all models across 15 prediction\ntasks under various few-shot settings. The benchmark includes a modular pipeline\ndesigned to execute key tasks, with the flexibility to optionally utilize a Slurm cluster\nfor distributed execution. Running all steps within this pipeline ensures full repro-\nducibility of results. Step four of the pipeline, which generates EHR representations\nwith CLIMBR-T-Base and the counts-based model, was extended to incorporate our\nmethod for creating language model-based EHR representations. This adaptation\nallowed us to reuse significant portions of the existing code, including the task evalua-\ntion framework. Additionally, we implemented new functionality for EHR serialization\nand slightly modified other steps of the benchmark to accommodate our experimen-\ntal setup. For instance, the label creation process was adjusted (step three) to enable\ntask-specific instructions for the LLM-embedding models. All modifications have been\ndocumented and can be tracked in our public GitHub repository.\n4.8 Effect of Different Contexts Sizes\nWe investigated the impact of varying context sizes in the LLM-embedding models.\nWe wanted to determine whether encoding information from older visits enhances\nprediction performance and whether longer inputs might dilute critical details, such\nas laboratory values, in the final embeddings. Specifically, we evaluated GTE-Qwen2-\n7B and LLM2Vec-Llama-3.1-8B models with input token limits of 512, 1.024, 2.048,\nand 8.192 tokens. Input tokens exceeding these thresholds were discarded. Due to the\ndesign of our EHR serialization process, additional input tokens primarily consisted\nof medical concepts from past visits. By testing these varying context sizes, we aimed\nto assess the balance between capturing historical medical data and preserving the\nclarity of high-priority information within the embeddings.\n4.9 Effect of Chunked Contexts\nTo further explore whether LLM-embedding models effectively interpret the entirety of\ntheir input, we compared the performance of models processing complete inputs versus\nsegmented (chunked) inputs. For this, the 4.096-token inputs were divided into smaller\nchunks of sizes 512, 1.024, and 2.048 tokens10. Separate embeddings were generated\n10In our implementation we used these chunks sizes decreased by eight tokens to cater for special tokens\nthat might be added by the implementations of the language models.\n21\n\nfor each chunk, and a final embedding was obtained by averaging the embeddings of\nall chunks. This approach aligns with the mean pooling applied to the last layer that\nwe used to compute embeddings. In this setup, task-specific instructions, placed at the\nbeginning of the inputs, were included only in the first chunk. This design allowed us to\nevaluate the models’ ability to contextualize task instructions and whether chunking\naffects the overall performance.\n4.10 Ablations of EHR Serialization\nTo better understand the contribution of various components in the EHR serial-\nization process to the performance of the LLM-embedding models, we conducted a\nseries of ablation experiments. First, we assessed the impact of task-specific instruc-\ntions by removing them entirely to determine their influence on the final embeddings.\nSubsequently, we performed additional ablations by systematically excluding specific\ncomponents of the serialization. These included demographic data, aggregated LOINC\ncodes, and both visit summaries and detailed entries. Furthermore, we examined the\neffect of removing specific fields from the detailed visit entries, such as conditions,\nmedications, and procedures. Throughout these experiments, the rest of the pipeline\nwas kept consistent to isolate the effects of the removed components. This approach\nallowed us to identify which parts of the serialization process were most critical for\ngenerating effective EHR representations, providing insights into how these models\nleverage structured medical data for prediction tasks.\nReferences\n[1] Dash, S., Shakyawar, S.K., Sharma, M., Kaushik, S.: Big data in healthcare:\nmanagement, analysis and future prospects. Journal of Big Data 6(1), 54 (2019)\nhttps://doi.org/10.1186/s40537-019-0217-0 . Accessed 2025-02-15\n[2] Ahsan, H., McInerney, D.J., Kim, J., Potter, C., Young, G., Amir, S., Wallace,\nB.C.: Retrieving Evidence from EHRs with LLMs: Possibilities and Challenges.\nProceedings of machine learning research 248, 489–505 (2024). Accessed 2025-\n02-15\n[3] Rajkomar, A., Dean, J., Kohane, I.: Machine Learning in Medicine. New Eng-\nland Journal of Medicine 380(14), 1347–1358 (2019) https://doi.org/10.1056/\nNEJMra1814259 . Accessed 2020-06-05\n[4] Xu, Y., Lv, T., Cui, L., Wang, G., Lu, Y., Florencio, D., Zhang, C., Wei, F.:\nLayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document\nUnderstanding. arXiv. arXiv:2104.08836 [cs] (2021). https://doi.org/10.48550/\narXiv.2104.08836 . http://arxiv.org/abs/2104.08836 Accessed 2024-07-23\n[5] Golas, S.B., Shibahara, T., Agboola, S., Otaki, H., Sato, J., Nakae, T., Hisamitsu,\nT., Kojima, G., Felsted, J., Kakarmath, S., Kvedar, J., Jethwani, K.: A machine\nlearning model to predict the risk of 30-day readmissions in patients with heart\n22\n\nfailure: a retrospective analysis of electronic medical records data. BMC Medi-\ncal Informatics and Decision Making 18(1), 44 (2018) https://doi.org/10.1186/\ns12911-018-0620-z . Accessed 2024-12-14\n[6] Rajkomar, A., Oren, E., Chen, K., Dai, A.M., Hajaj, N., Hardt, M., Liu, P.J., Liu,\nX., Marcus, J., Sun, M., Sundberg, P., Yee, H., Zhang, K., Zhang, Y., Flores, G.,\nDuggan, G.E., Irvine, J., Le, Q., Litsch, K., Mossin, A., Tansuwan, J., Wang, D.,\nWexler, J., Wilson, J., Ludwig, D., Volchenboum, S.L., Chou, K., Pearson, M.,\nMadabushi, S., Shah, N.H., Butte, A.J., Howell, M.D., Cui, C., Corrado, G.S.,\nDean, J.: Scalable and accurate deep learning with electronic health records. npj\nDigital Medicine 1(1), 1–10 (2018) https://doi.org/10.1038/s41746-018-0029-1 .\nPublisher: Nature Publishing Group. Accessed 2024-12-15\n[7] Lauritsen, S.M., Kalør, M.E., Kongsgaard, E.L., Lauritsen, K.M., Jørgensen,\nM.J., Lange, J., Thiesson, B.: Early detection of sepsis utilizing deep learning\non electronic health record event sequences. Artificial Intelligence in Medicine\n104, 101820 (2020) https://doi.org/10.1016/j.artmed.2020.101820 . Accessed\n2024-12-15\n[8] Moor, M., Bennett, N., Pleˇcko, D., Horn, M., Rieck, B., Meinshausen, N.,\nB¨uhlmann, P., Borgwardt, K.: Predicting sepsis using deep learning across\ninternational sites: a retrospective development and validation study. eClini-\ncalMedicine 62 (2023) https://doi.org/10.1016/j.eclinm.2023.102124 . Publisher:\nElsevier. Accessed 2024-12-15\n[9] Thorsen-Meyer, H.-C., Nielsen, A.B., Nielsen, A.P., Kaas-Hansen, B.S., Toft, P.,\nSchierbeck, J., Strøm, T., Chmura, P.J., Heimann, M., Dybdahl, L., Spangsege,\nL., Hulsen, P., Belling, K., Brunak, S., Perner, A.: Dynamic and explain-\nable machine learning prediction of mortality in patients in the intensive\ncare unit: a retrospective study of high-frequency data in electronic patient\nrecords. The Lancet. Digital Health 2(4), 179–191 (2020) https://doi.org/10.\n1016/S2589-7500(20)30018-2\n[10] Desai, R.J., Wang, S.V., Vaduganathan, M., Evers, T., Schneeweiss, S.: Com-\nparison of Machine Learning Methods With Traditional Models for Use of\nAdministrative Claims With Electronic Medical Records to Predict Heart Failure\nOutcomes. JAMA Network Open 3(1), 1918962 (2020) https://doi.org/10.1001/\njamanetworkopen.2019.18962 . Accessed 2024-12-15\n[11] Kim, E., Rubinstein, S.M., Nead, K.T., Wojcieszynski, A.P., Gabriel, P.E.,\nWarner, J.L.: The Evolving Use of Electronic Health Records (EHR) for Research.\nSeminars in Radiation Oncology 29(4), 354–361 (2019) https://doi.org/10.1016/\nj.semradonc.2019.05.010 . Accessed 2025-02-15\n[12] Rasmy, L., Xiang, Y., Xie, Z., Tao, C., Zhi, D.: Med-BERT: pretrained contextu-\nalized embeddings on large-scale structured electronic health records for disease\nprediction. npj Digital Medicine 4(1), 1–13 (2021) https://doi.org/10.1038/\n23\n\ns41746-021-00455-y . Publisher: Nature Publishing Group. Accessed 2024-12-15\n[13] Steinberg, E., Jung, K., Fries, J.A., Corbin, C.K., Pfohl, S.R., Shah, N.H.:\nLanguage models are an effective representation learning technique for elec-\ntronic health record data. Journal of Biomedical Informatics 113, 103637 (2021)\nhttps://doi.org/10.1016/j.jbi.2020.103637 . Accessed 2024-06-12\n[14] Bommasani, R., Hudson, D.A., Adeli, E., Altman, R., Arora, S., Arx, S.v., Bern-\nstein, M.S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card,\nD., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis, J.Q., Demszky, D.,\nDonahue, C., Doumbouya, M., Durmus, E., Ermon, S., Etchemendy, J., Etha-\nyarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K., Goodman, N.,\nGrossman, S., Guha, N., Hashimoto, T., Henderson, P., Hewitt, J., Ho, D.E.,\nHong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P., Karam-\ncheti, S., Keeling, G., Khani, F., Khattab, O., Koh, P.W., Krass, M., Krishna, R.,\nKuditipudi, R., Kumar, A., Ladhak, F., Lee, M., Lee, T., Leskovec, J., Levent,\nI., Li, X.L., Li, X., Ma, T., Malik, A., Manning, C.D., Mirchandani, S., Mitchell,\nE., Munyikwa, Z., Nair, S., Narayan, A., Narayanan, D., Newman, B., Nie, A.,\nNiebles, J.C., Nilforoshan, H., Nyarko, J., Ogut, G., Orr, L., Papadimitriou, I.,\nPark, J.S., Piech, C., Portelance, E., Potts, C., Raghunathan, A., Reich, R., Ren,\nH., Rong, F., Roohani, Y., Ruiz, C., Ryan, J., R´e, C., Sadigh, D., Sagawa, S.,\nSanthanam, K., Shih, A., Srinivasan, K., Tamkin, A., Taori, R., Thomas, A.W.,\nTram`er, F., Wang, R.E., Wang, W., Wu, B., Wu, J., Wu, Y., Xie, S.M., Yasunaga,\nM., You, J., Zaharia, M., Zhang, M., Zhang, T., Zhang, X., Zhang, Y., Zheng,\nL., Zhou, K., Liang, P.: On the Opportunities and Risks of Foundation Models.\narXiv. arXiv:2108.07258 [cs] (2022). https://doi.org/10.48550/arXiv.2108.07258\n. http://arxiv.org/abs/2108.07258 Accessed 2024-12-16\n[15] Devlin, J., Chang, M.-W., Lee, K., Toutanova, K.: BERT: Pre-training of Deep\nBidirectional Transformers for Language Understanding. Proceedings of the 2019\nConference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers),\n4171–4186 (2019) https://doi.org/10.18653/v1/N19-1423 . Accessed 2022-04-22\n[16] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I.: Lan-\nguage Models are Unsupervised Multitask Learners. Technical report, OpenAi,\n24 (2019)\n[17] Odgaard, M., Klein, K.V., Thysen, S.M., Jimenez-Solem, E., Sillesen, M., Nielsen,\nM.: CORE-BEHRT: A Carefully Optimized and Rigorously Evaluated BEHRT.\narXiv. arXiv:2404.15201 [cs] (2024). https://doi.org/10.48550/arXiv.2404.15201\n. http://arxiv.org/abs/2404.15201 Accessed 2024-12-16\n[18] Pang, C., Jiang, X., Kalluri, K.S., Spotnitz, M., Chen, R., Perotte, A., Natarajan,\nK.: CEHR-BERT: Incorporating temporal information from structured EHR data\nto improve prediction tasks. arXiv. arXiv:2111.08585 [cs] (2021). https://doi.org/\n24\n\n10.48550/arXiv.2111.08585 . http://arxiv.org/abs/2111.08585 Accessed 2024-12-\n16\n[19] Li, Y., Rao, S., Solares, J.R.A., Hassaine, A., Ramakrishnan, R., Canoy, D.,\nZhu, Y., Rahimi, K., Salimi-Khorshidi, G.: BEHRT: Transformer for Electronic\nHealth Records. Scientific Reports 10(1), 7155 (2020) https://doi.org/10.1038/\ns41598-020-62922-y . Publisher: Nature Publishing Group. Accessed 2024-12-16\n[20] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li,\nW., Liu, P.J.: Exploring the limits of transfer learning with a unified text-to-text\ntransformer. J. Mach. Learn. Res. 21(1), 140–54851405551 (2020)\n[21] Agrawal, M., Hegselmann, S., Lang, H., Kim, Y., Sontag, D.: Large Language\nModels are Few-Shot Clinical Information Extractors. Proceedings of the 2022\nConference on Empirical Methods in Natural Language Processing, 1998–2022\n(2022) https://doi.org/10.18653/v1/2022.emnlp-main.130 . Accessed 2024-04-12\n[22] Van Veen, D., Van Uden, C., Blankemeier, L., Delbrouck, J.-B., Aali, A., Blueth-\ngen, C., Pareek, A., Polacin, M., Reis, E.P., Seehofnerov´a, A., Rohatgi, N.,\nHosamani, P., Collins, W., Ahuja, N., Langlotz, C.P., Hom, J., Gatidis, S.,\nPauly, J., Chaudhari, A.S.: Adapted large language models can outperform med-\nical experts in clinical text summarization. Nature Medicine, 1–9 (2024) https:\n//doi.org/10.1038/s41591-024-02855-5 . Publisher: Nature Publishing Group.\nAccessed 2024-04-11\n[23] Hegselmann, S., Buendia, A., Lang, H., Agrawal, M., Jiang, X., Sontag, D.:\nTabLLM: Few-shot Classification of Tabular Data with Large Language Models.\nProceedings of The 26th International Conference on Artificial Intelligence and\nStatistics, 5549–5581 (2023). Accessed 2024-04-12\n[24] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee-\nlakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A.,\nKrueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C.,\nHesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner,\nC., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language Models\nare Few-Shot Learners. Advances in Neural Information Processing Systems 33,\n1877–1901 (2020). Accessed 2024-05-22\n[25] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bash-\nlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer,\nC.C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller,\nB., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan,\nH., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura,\nP.S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Mar-\ntinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein,\nJ., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E.M., Subramanian,\nR., Tan, X.E., Tang, B., Taylor, R., Williams, A., Kuan, J.X., Xu, P., Yan, Z.,\n25\n\nZarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic,\nR., Edunov, S., Scialom, T.: Llama 2: Open Foundation and Fine-Tuned Chat\nModels. arXiv preprint arXiv:2307.09288 (2023). Accessed 2024-01-25\n[26] BehnamGhader, P., Adlakha, V., Mosbach, M., Bahdanau, D., Chapados, N.,\nReddy, S.: LLM2Vec: Large Language Models Are Secretly Powerful Text\nEncoders. arXiv. arXiv:2404.05961 [cs] (2024). http://arxiv.org/abs/2404.05961\nAccessed 2024-07-03\n[27] Lee, C., Roy, R., Xu, M., Raiman, J., Shoeybi, M., Catanzaro, B., Ping, W.:\nNV-Embed: Improved Techniques for Training LLMs as Generalist Embedding\nModels. arXiv. arXiv:2405.17428 [cs] (2024). http://arxiv.org/abs/2405.17428\nAccessed 2024-08-02\n[28] Muennighoff, N., Su, H., Wang, L., Yang, N., Wei, F., Yu, T., Singh, A., Kiela,\nD.: Generative Representational Instruction Tuning. arXiv. arXiv:2402.09906 [cs]\n(2024). http://arxiv.org/abs/2402.09906 Accessed 2024-08-01\n[29] Li, Z., Zhang, X., Zhang, Y., Long, D., Xie, P., Zhang, M.: Towards General Text\nEmbeddings with Multi-stage Contrastive Learning. arXiv. arXiv:2308.03281 [cs]\n(2023). https://doi.org/10.48550/arXiv.2308.03281 . http://arxiv.org/abs/2308.\n03281 Accessed 2024-08-02\n[30] Wornow, M., Thapa, R., Steinberg, E., Fries, J.A., Shah, N.: EHRSHOT:\nAn EHR Benchmark for Few-Shot Evaluation of Foundation Models. (2023).\nhttps://openreview.net/forum?id=CsXC6IcdwI Accessed 2024-06-12\n[31] Yang, A., Yang, B., Hui, B., Zheng, B., Yu, B., Zhou, C., Li, C., Li, C., Liu, D.,\nHuang, F., Dong, G., Wei, H., Lin, H., Tang, J., Wang, J., Yang, J., Tu, J., Zhang,\nJ., Ma, J., Yang, J., Xu, J., Zhou, J., Bai, J., He, J., Lin, J., Dang, K., Lu, K.,\nChen, K., Yang, K., Li, M., Xue, M., Ni, N., Zhang, P., Wang, P., Peng, R., Men,\nR., Gao, R., Lin, R., Wang, S., Bai, S., Tan, S., Zhu, T., Li, T., Liu, T., Ge, W.,\nDeng, X., Zhou, X., Ren, X., Zhang, X., Wei, X., Ren, X., Liu, X., Fan, Y., Yao,\nY., Zhang, Y., Wan, Y., Chu, Y., Liu, Y., Cui, Z., Zhang, Z., Guo, Z., Fan, Z.:\nQwen2 Technical Report. arXiv. arXiv:2407.10671 [cs] (2024). https://doi.org/10.\n48550/arXiv.2407.10671 . http://arxiv.org/abs/2407.10671 Accessed 2025-01-08\n[32] Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Let-\nman, A., Mathur, A., Schelten, A., Vaughan, A., Yang, A., Fan, A., Goyal, A.,\nHartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark,\nA., Rao, A., Zhang, A., Rodriguez, A., Gregerson, A., Spataru, A., Roziere, B.,\nBiron, B., Tang, B., Chern, B., Caucheteux, C., Nayak, C., Bi, C., Marra, C.,\nMcConnell, C., Keller, C., Touret, C., Wu, C., Wong, C., Ferrer, C.C., Nikolaidis,\nC., Allonsius, D., Song, D., Pintz, D., Livshits, D., Wyatt, D., Esiobu, D., Choud-\nhary, D., Mahajan, D., Garcia-Olano, D., Perino, D., Hupkes, D., Lakomkin, E.,\nAlBadawy, E., Lobanova, E., Dinan, E., Smith, E.M., Radenovic, F., Guzm´an, F.,\nZhang, F., Synnaeve, G., Lee, G., Anderson, G.L., Thattai, G., Nail, G., Mialon,\n26\n\nG., Pang, G., Cucurell, G., Nguyen, H., Korevaar, H., Xu, H., Touvron, H., Zarov,\nI., Ibarra, I.A., Kloumann, I., Misra, I., Evtimov, I., Zhang, J., Copet, J., Lee, J.,\nGeffert, J., Vranes, J., Park, J., Mahadeokar, J., Shah, J., Linde, J.v.d., Billock,\nJ., Hong, J., Lee, J., Fu, J., Chi, J., Huang, J., Liu, J., Wang, J., Yu, J., Bitton,\nJ., Spisak, J., Park, J., Rocca, J., Johnstun, J., Saxe, J., Jia, J., Alwala, K.V.,\nPrasad, K., Upasani, K., Plawiak, K., Li, K., Heafield, K., Stone, K., El-Arini,\nK., Iyer, K., Malik, K., Chiu, K., Bhalla, K., Lakhotia, K., Rantala-Yeary, L.,\nMaaten, L.v.d., Chen, L., Tan, L., Jenkins, L., Martin, L., Madaan, L., Malo, L.,\nBlecher, L., Landzaat, L., Oliveira, L.d., Muzzi, M., Pasupuleti, M., Singh, M.,\nPaluri, M., Kardas, M., Tsimpoukelli, M., Oldham, M., Rita, M., Pavlova, M.,\nKambadur, M., Lewis, M., Si, M., Singh, M.K., Hassan, M., Goyal, N., Torabi,\nN., Bashlykov, N., Bogoychev, N., Chatterji, N., Zhang, N., Duchenne, O., C¸elebi,\nO., Alrassy, P., Zhang, P., Li, P., Vasic, P., Weng, P., Bhargava, P., Dubal, P.,\nKrishnan, P., Koura, P.S., Xu, P., He, Q., Dong, Q., Srinivasan, R., Ganapathy,\nR., Calderer, R., Cabral, R.S., Stojnic, R., Raileanu, R., Maheswari, R., Gird-\nhar, R., Patel, R., Sauvestre, R., Polidoro, R., Sumbaly, R., Taylor, R., Silva,\nR., Hou, R., Wang, R., Hosseini, S., Chennabasappa, S., Singh, S., Bell, S., Kim,\nS.S., Edunov, S., Nie, S., Narang, S., Raparthy, S., Shen, S., Wan, S., Bhosale,\nS., Zhang, S., Vandenhende, S., Batra, S., Whitman, S., Sootla, S., Collot, S.,\nGururangan, S., Borodinsky, S., Herman, T., Fowler, T., Sheasha, T., Georgiou,\nT., Scialom, T., Speckbacher, T., Mihaylov, T., Xiao, T., Karn, U., Goswami,\nV., Gupta, V., Ramanathan, V., Kerkez, V., Gonguet, V., Do, V., Vogeti, V.,\nAlbiero, V., Petrovic, V., Chu, W., Xiong, W., Fu, W., Meers, W., Martinet, X.,\nWang, X., Wang, X., Tan, X.E., Xia, X., Xie, X., Jia, X., Wang, X., Goldschlag,\nY., Gaur, Y., Babaei, Y., Wen, Y., Song, Y., Zhang, Y., Li, Y., Mao, Y., Coud-\nert, Z.D., Yan, Z., Chen, Z., Papakipos, Z., Singh, A., Srivastava, A., Jain, A.,\nKelsey, A., Shajnfeld, A., Gangidi, A., Victoria, A., Goldstand, A., Menon, A.,\nSharma, A., Boesenberg, A., Baevski, A., Feinstein, A., Kallet, A., Sangani, A.,\nTeo, A., Yunus, A., Lupu, A., Alvarado, A., Caples, A., Gu, A., Ho, A., Poul-\nton, A., Ryan, A., Ramchandani, A., Dong, A., Franco, A., Goyal, A., Saraf, A.,\nChowdhury, A., Gabriel, A., Bharambe, A., Eisenman, A., Yazdan, A., James, B.,\nMaurer, B., Leonhardi, B., Huang, B., Loyd, B., Paola, B.D., Paranjape, B., Liu,\nB., Wu, B., Ni, B., Hancock, B., Wasti, B., Spence, B., Stojkovic, B., Gamido,\nB., Montalvo, B., Parker, C., Burton, C., Mejia, C., Liu, C., Wang, C., Kim, C.,\nZhou, C., Hu, C., Chu, C.-H., Cai, C., Tindal, C., Feichtenhofer, C., Gao, C.,\nCivin, D., Beaty, D., Kreymer, D., Li, D., Adkins, D., Xu, D., Testuggine, D.,\nDavid, D., Parikh, D., Liskovich, D., Foss, D., Wang, D., Le, D., Holland, D.,\nDowling, E., Jamil, E., Montgomery, E., Presani, E., Hahn, E., Wood, E., Le,\nE.-T., Brinkman, E., Arcaute, E., Dunbar, E., Smothers, E., Sun, F., Kreuk, F.,\nTian, F., Kokkinos, F., Ozgenel, F., Caggioni, F., Kanayet, F., Seide, F., Flo-\nrez, G.M., Schwarz, G., Badeer, G., Swee, G., Halpern, G., Herman, G., Sizov,\nG., Guangyi, Zhang, Lakshminarayanan, G., Inan, H., Shojanazeri, H., Zou, H.,\nWang, H., Zha, H., Habeeb, H., Rudolph, H., Suk, H., Aspegren, H., Goldman,\nH., Zhan, H., Damlaj, I., Molybog, I., Tufanov, I., Leontiadis, I., Veliche, I.-E.,\nGat, I., Weissman, J., Geboski, J., Kohli, J., Lam, J., Asher, J., Gaya, J.-B.,\n27\n\nMarcus, J., Tang, J., Chan, J., Zhen, J., Reizenstein, J., Teboul, J., Zhong, J.,\nJin, J., Yang, J., Cummings, J., Carvill, J., Shepard, J., McPhie, J., Torres, J.,\nGinsburg, J., Wang, J., Wu, K., U, K.H., Saxena, K., Khandelwal, K., Zand, K.,\nMatosich, K., Veeraraghavan, K., Michelena, K., Li, K., Jagadeesh, K., Huang,\nK., Chawla, K., Huang, K., Chen, L., Garg, L., A, L., Silva, L., Bell, L., Zhang,\nL., Guo, L., Yu, L., Moshkovich, L., Wehrstedt, L., Khabsa, M., Avalani, M.,\nBhatt, M., Mankus, M., Hasson, M., Lennie, M., Reso, M., Groshev, M., Nau-\nmov, M., Lathi, M., Keneally, M., Liu, M., Seltzer, M.L., Valko, M., Restrepo,\nM., Patel, M., Vyatskov, M., Samvelyan, M., Clark, M., Macey, M., Wang, M.,\nHermoso, M.J., Metanat, M., Rastegari, M., Bansal, M., Santhanam, N., Parks,\nN., White, N., Bawa, N., Singhal, N., Egebo, N., Usunier, N., Mehta, N., Laptev,\nN.P., Dong, N., Cheng, N., Chernoguz, O., Hart, O., Salpekar, O., Kalinli, O.,\nKent, P., Parekh, P., Saab, P., Balaji, P., Rittner, P., Bontrager, P., Roux, P.,\nDollar, P., Zvyagina, P., Ratanchandani, P., Yuvraj, P., Liang, Q., Alao, R.,\nRodriguez, R., Ayub, R., Murthy, R., Nayani, R., Mitra, R., Parthasarathy, R.,\nLi, R., Hogan, R., Battey, R., Wang, R., Howes, R., Rinott, R., Mehta, S., Siby,\nS., Bondu, S.J., Datta, S., Chugh, S., Hunt, S., Dhillon, S., Sidorov, S., Pan, S.,\nMahajan, S., Verma, S., Yamamoto, S., Ramaswamy, S., Lindsay, S., Lindsay, S.,\nFeng, S., Lin, S., Zha, S.C., Patil, S., Shankar, S., Zhang, S., Zhang, S., Wang, S.,\nAgarwal, S., Sajuyigbe, S., Chintala, S., Max, S., Chen, S., Kehoe, S., Satterfield,\nS., Govindaprasad, S., Gupta, S., Deng, S., Cho, S., Virk, S., Subramanian, S.,\nChoudhury, S., Goldman, S., Remez, T., Glaser, T., Best, T., Koehler, T., Robin-\nson, T., Li, T., Zhang, T., Matthews, T., Chou, T., Shaked, T., Vontimitta, V.,\nAjayi, V., Montanez, V., Mohan, V., Kumar, V.S., Mangla, V., Ionescu, V., Poe-\nnaru, V., Mihailescu, V.T., Ivanov, V., Li, W., Wang, W., Jiang, W., Bouaziz,\nW., Constable, W., Tang, X., Wu, X., Wang, X., Wu, X., Gao, X., Kleinman, Y.,\nChen, Y., Hu, Y., Jia, Y., Qi, Y., Li, Y., Zhang, Y., Zhang, Y., Adi, Y., Nam,\nY., Yu, Wang, Zhao, Y., Hao, Y., Qian, Y., Li, Y., He, Y., Rait, Z., DeVito, Z.,\nRosnbrick, Z., Wen, Z., Yang, Z., Zhao, Z., Ma, Z.: The Llama 3 Herd of Models.\narXiv. arXiv:2407.21783 [cs] (2024). https://doi.org/10.48550/arXiv.2407.21783\n. http://arxiv.org/abs/2407.21783 Accessed 2025-01-08\n[33] Jiang, L.Y., Liu, X.C., Nejatian, N.P., Nasir-Moin, M., Wang, D., Abidin,\nA., Eaton, K., Riina, H.A., Laufer, I., Punjabi, P., Miceli, M., Kim, N.C.,\nOrillac, C., Schnurman, Z., Livia, C., Weiss, H., Kurland, D., Neifert, S., Dasta-\ngirzada, Y., Kondziolka, D., Cheung, A.T.M., Yang, G., Cao, M., Flores, M.,\nCosta, A.B., Aphinyanaphongs, Y., Cho, K., Oermann, E.K.: Health system-\nscale language models are all-purpose prediction engines. Nature 619(7969),\n357–362 (2023) https://doi.org/10.1038/s41586-023-06160-y . Publisher: Nature\nPublishing Group. Accessed 2024-07-31\n[34] Si, Y., Du, J., Li, Z., Jiang, X., Miller, T., Wang, F., Jim Zheng, W., Roberts,\nK.: Deep representation learning of patient data from Electronic Health Records\n(EHR): A systematic review. Journal of Biomedical Informatics 115, 103671\n(2021) https://doi.org/10.1016/j.jbi.2020.103671 . Accessed 2025-02-15\n28\n\n[35] Lehne, M., Sass, J., Essenwanger, A., Schepers, J., Thun, S.: Why digital medicine\ndepends on interoperability. npj Digital Medicine 2(1), 1–5 (2019) https://doi.\norg/10.1038/s41746-019-0158-1 . Publisher: Nature Publishing Group. Accessed\n2025-02-15\n[36] Steinfeldt, J., Wild, B., Buergel, T., Pietzner, M., Belzen, J., Vauvelle, A.,\nHegselmann, S., Denaxas, S., Hemingway, H., Langenberg, C., Landmesser, U.,\nDeanfield, J., Eils, R.: Medical history predicts phenome-wide disease onset\nand enables the rapid response to emerging health threats. Nature Communica-\ntions 16(1), 585 (2025) https://doi.org/10.1038/s41467-025-55879-x . Publisher:\nNature Publishing Group. Accessed 2025-02-15\n[37] Kirchler, M., Ferro, M., Lorenzini, V., FinnGen, Lippert, C., Ganna, A.: Large\nlanguage models improve transferability of electronic health record-based predic-\ntions across countries and coding systems. medRxiv. Pages: 2025.02.03.25321597\n(2025). https://doi.org/10.1101/2025.02.03.25321597 . https://www.medrxiv.\norg/content/10.1101/2025.02.03.25321597v1 Accessed 2025-02-15\n[38] Moor, M., Banerjee, O., Abad, Z.S.H., Krumholz, H.M., Leskovec, J., Topol, E.J.,\nRajpurkar, P.: Foundation models for generalist medical artificial intelligence.\nNature 616(7956), 259–265 (2023) https://doi.org/10.1038/s41586-023-05881-4 .\nPublisher: Nature Publishing Group. Accessed 2025-02-15\n[39] Ke, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., Ye, Q., Liu, T.-\nY.: LightGBM: A Highly Efficient Gradient Boosting Decision Tree. Advances in\nNeural Information Processing Systems 30 (2017). Accessed 2022-08-01\n[40] Liu, Q., Chen, B., Guo, J., Ziyadi, M., Lin, Z., Chen, W., Lou, J.-G.:\nTAPEX: Table Pre-training via Learning a Neural SQL Executor. (2021).\nhttps://openreview.net/forum?id=O50443AsCP Accessed 2025-01-18\n[41] Li, P., He, Y., Yashar, D., Cui, W., Ge, S., Zhang, H., Rifinski Fainman, D.,\nZhang, D., Chaudhuri, S.: Table-GPT: Table Fine-tuned GPT for Diverse Table\nTasks. Proceedings of the ACM on Management of Data 2(3), 1–28 (2024) https:\n//doi.org/10.1145/3654979 . Accessed 2025-01-18\n[42] Dong, H., Zhao, J., Tian, Y., Xiong, J., Zhou, M., Lin, Y., Cambronero, J., He, Y.,\nHan, S., Zhang, D.: Encoding Spreadsheets for Large Language Models. In: Al-\nOnaizan, Y., Bansal, M., Chen, Y.-N. (eds.) Proceedings of the 2024 Conference\non Empirical Methods in Natural Language Processing, pp. 20728–20748. Asso-\nciation for Computational Linguistics, Miami, Florida, USA (2024). https://doi.\norg/10.18653/v1/2024.emnlp-main.1154\n.\nhttps://aclanthology.org/2024.emnlp-\nmain.1154/ Accessed 2025-01-18\n[43] Sui, Y., Zhou, M., Zhou, M., Han, S., Zhang, D.: Table Meets LLM: Can Large\nLanguage Models Understand Structured Table Data? A Benchmark and Empir-\nical Study. In: Proceedings of the 17th ACM International Conference on Web\n29\n\nSearch and Data Mining, pp. 645–654. ACM, Merida Mexico (2024). https://doi.\norg/10.1145/3616855.3635752 . https://dl.acm.org/doi/10.1145/3616855.3635752\nAccessed 2025-01-18\n[44] Reimers, N., Gurevych, I.: Sentence-BERT: Sentence Embeddings using Siamese\nBERT-Networks. In: Inui, K., Jiang, J., Ng, V., Wan, X. (eds.) Proceed-\nings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pp. 3982–3992. Association for Computational\nLinguistics, Hong Kong, China (2019). https://doi.org/10.18653/v1/D19-1410 .\nhttps://aclanthology.org/D19-1410 Accessed 2024-06-12\n[45] Springer, J.M., Kotha, S., Fried, D., Neubig, G., Raghunathan, A.: Rep-\netition Improves Language Model Embeddings. arXiv. arXiv:2402.15449 [cs]\n(2024). https://doi.org/10.48550/arXiv.2402.15449 . http://arxiv.org/abs/2402.\n15449 Accessed 2024-08-01\n[46] Wang, A., Liu, C., Yang, J., Weng, C.: Fine-tuning Large Language Models for\nRare Disease Concept Normalization. bioRxiv. Pages: 2023.12.28.573586 Section:\nNew Results (2024). https://doi.org/10.1101/2023.12.28.573586 . https://www.\nbiorxiv.org/content/10.1101/2023.12.28.573586v3 Accessed 2024-07-26\n[47] Xia, M., Gao, T., Zeng, Z., Chen, D.: Sheared LLaMA: Accelerating Lan-\nguage Model Pre-training via Structured Pruning. arXiv. arXiv:2310.06694 [cs]\n(2024). https://doi.org/10.48550/arXiv.2310.06694 . http://arxiv.org/abs/2310.\n06694 Accessed 2025-01-19\n[48] He, P., Gao, J., Chen, W.: DeBERTaV3: Improving DeBERTa using ELECTRA-\nStyle Pre-Training with Gradient-Disentangled Embedding Sharing. (2022).\nhttps://openreview.net/forum?id=sE7-XhLxHA Accessed 2025-01-19\n[49] Lee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C.H., Kang, J.: BioBERT: a\npre-trained biomedical language representation model for biomedical text mining.\nBioinformatics 36(4), 1234–1240 (2020) https://doi.org/10.1093/bioinformatics/\nbtz682 . Accessed 2022-04-26\n[50] Alsentzer, E., Murphy, J., Boag, W., Weng, W.-H., Jindi, D., Naumann, T.,\nMcDermott, M.: Publicly Available Clinical BERT Embeddings. In: Rumshisky,\nA., Roberts, K., Bethard, S., Naumann, T. (eds.) Proceedings of the 2nd Clinical\nNatural Language Processing Workshop, pp. 72–78. Association for Compu-\ntational Linguistics, Minneapolis, Minnesota, USA (2019). https://doi.org/10.\n18653/v1/W19-1909 . https://aclanthology.org/W19-1909/ Accessed 2025-01-19\n30\n\nA Appendix\n1\n2\n4\n8\n16\n32\n64\n128\nAll\n# of Train Examples per Class\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAUPRC\nOperational Outcomes\n1\n2\n4\n8\n16\n32\n64\n128\nAll\n# of Train Examples per Class\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAUPRC\nAnticipating Lab Test Results\n1\n2\n4\n8\n16\n32\n64\n128\nAll\n# of Train Examples per Class\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nAUPRC\nAssignment of New Diagnoses\n1\n2\n4\n8\n16\n32\n64\n128\nAll\n# of Train Examples per Class\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nAUPRC\nAnticipating Chest X-ray Findings\nAUPRC by Task Group\nCLMBR+LR\nGTE Qwen2 7B+LR\nLLM2Vec Llama 3.1 7B+LR\nCount-based+GBM\nFig. 6 AUPRC Performance in Few-Shot Settings. Macro averaged area under the precision-\nrecall curve (AUPRC) performance across subtasks for four task groups across (bold). Blurred lines\nare averaged AUPRC values across five bootstrapped runs using different seeds [30].\n31\n\nTable 7 Semantic Codes for Aggregated Concepts in EHR Serialization. We aggregated\ntime-series data encoded via LOINC concepts and identified the most frequent concepts from which\nwe selected 24 key medical concepts. To reduce duplicate information, we merged synonymous\nsemantic codes. The primary LOINC codes is presented first in the column Semantic Codes followed\nby identified duplicates. We also defined a unit, minimum and maximum allowed values for filtering,\na normal range to classify values in low, normal, and high, and a formatting strategy to create our\nEHR serialization.\nMedical Concept\nSemantic Codes\nUnit\nMin-Max\nRange\nNormal\nRange\nFormatting\nRecent Body Metrics\nBody weight\nLOINC/29463-7\noz\n350-10000\nOne decimal\nBody height\nLOINC/8302-2\ninch\n5-100\nOne decimal\nBody mass index / BMI\nLOINC/39156-5\nkg/m2\n10-100\n18.5-24.9\nOne decimal\nBody surface area\nLOINC/8277-6,\nSNOMED/301898006\nm2\n0.1-10\nTwo decimals\nRecent Vital Signs\nHeart rate\nLOINC/8867-4,\nSNOMED/364075005,\nSNOMED/78564009\nbpm\n5-300\n60-100\nInteger\nSystolic blood pressure\nLOINC/8480-6,\nSNOMED/271649006\nmmHg\n20-300\n90-140\nInteger\nDiastolic blood pressure\nLOINC/8462-4,\nSNOMED/271650006\nmmHg\n20-300\n60-90\nInteger\nBody temperature\nLOINC/8310-5\n°F\n80-120\n95-100.4\nOne decimal\nRespiratory rate\nLOINC/9279-1\nbreaths/min\n1-100\n12-18\nInteger\nOxygen saturation\nLOINC/LP21258-6\n%\n1-100\n95-100\nInteger\nRecent Lab Results\nHemoglobin\nLOINC/718-7,\nSNOMED/271026005,\nSNOMED/441689006\ng/dL\n1-20\n12-17\nOne decimal\nHematocrit\nLOINC/4544-3,\nLOINC/20570-8,\nLOINC/48703-3,\nSNOMED/28317006\n%\n10-100\n36-51\nInteger\nErythrocytes\nLOINC/789-8,\nLOINC/26453-1\n106/uL\n1-10\n4.2-5.9\nTwo decimals\nLeukocytes\nLOINC/20584-9,\nLOINC/6690-2\n103/uL\n1-100\n4-10\nOne decimal\nPlatelets\nLOINC/777-3,\nSNOMED/61928009\n103/uL\n10-1000\n150-350\nInteger\nSodium\nLOINC/2951-2,\nLOINC/2947-0,\nSNOMED/25197003\nmmol/L\n100-200\n136-145\nInteger\nPotassium\nLOINC/2823-3,\nSNOMED/312468003,\nLOINC/6298-4,\nSNOMED/59573005\nmmol/L\n0.1-10\n3.5-5.0\nOne decimal\nChloride\nLOINC/2075-0,\nSNOMED/104589004,\nLOINC/2069-3\nmmol/L\n50-200\n98-106\nInteger\nCarbon dioxide, total\nLOINC/2028-9\nmmol/L\n10-100\n23-28\nInteger\nCalcium\nLOINC/17861-6,\nSNOMED/271240001\nmg/dL\n1-20\n9-10.5\nOne decimal\nGlucose\nLOINC/2345-7,\nSNOMED/166900001,\nLOINC/2339-0,\nSNOMED/33747003,\nLOINC/14749-6\nmg/dL\n10-1000\n70-100\nInteger\nUrea nitrogen\nLOINC/3094-0,\nSNOMED/105011006\nmg/dL\n1-200\n8-20\nInteger\nCreatinine\nLOINC/2160-0,\nSNOMED/113075003\nmg/dL\n0.1-10\n0.7-1.3\nOne decimal\nAnion gap\nLOINC/33037-3,\nLOINC/41276-7,\nSNOMED/25469001\nmmol/L\n-20-50\n3-11\nInteger\n32\n\nTable 8 Instructions for LLM-Embedding Models. The LLM-embedding models were\ntrained using instructions; hence, we also defined simple task-specific prompts for each of the 15\nclinical prediction tasks. Each prompt is prepended by the prefix given below, containing a\ngeneral task description.\nTask\nPrompt\nPrefix\nGiven a patient’s EHR in Markdown format, retrieve relevant\npassages that answer the query:\nLong Length of Stay\nWill the patient stay in the hospital for more than 7 days\n30-day Readmission\nWill the patient be readmitted to the hospital within 30 days\nICU Transfer\nWill the patient be transferred to the intensive care unit\nThrombocytopenia\nHas the patient thrombocytopenia\nHyperkalemia\nHas the patient hyperkalemia\nHypoglycemia\nHas the patient hypoglycemia\nHyponatremia\nHas the patient hyponatremia\nAnemia\nHas the patient anemia\nHypertension\nHas the patient hypertension\nHyperlipidemia\nHas the patient hyperlipidemia\nPancreatic Cancer\nHas the patient pancreatic cancer\nCeliac\nHas the patient celiac disease\nLupus\nHas the patient lupus\nAcute MI\nHas the patient had an acute myocardial infarction\nChest X-Ray Findings\nWhat are the chest x-ray findings of the patient\n33\n\n1\n2\n4\n8\n16\n32\n64 128 All\n# of Train Examples per Class\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAUROC\nLong LOS\n1\n2\n4\n8\n16\n32\n64 128 All\n# of Train Examples per Class\n0.5\n0.6\n0.7\n0.8\nAUROC\n30-day Readmission\n1\n2\n4\n8\n16\n32\n64 128 All\n# of Train Examples per Class\n0.5\n0.6\n0.7\n0.8\nAUROC\nICU Admission\n1\n2\n4\n8\n16\n32\n64 128 All\n# of Train Examples per Class\n0.5\n0.6\n0.7\n0.8\n0.9\nAUROC\nPancreatic Cancer\n1\n2\n4\n8\n16\n32\n64\nAll\n# of Train Examples per Class\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nAUROC\nCeliac\n1\n2\n4\n8\n16\n32\n64 128 All\n# of Train Examples per Class\n0.4\n0.5\n0.6\n0.7\nAUROC\nLupus\n1\n2\n4\n8\n16\n32\n64 128 All\n# of Train Examples per Class\n0.4\n0.5\n0.6\n0.7\nAUROC\nAcute MI\n1\n2\n4\n8\n16\n32\n64 128 All\n# of Train Examples per Class\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\nAUROC\nHypertension\n1\n2\n4\n8\n16\n32\n64 128 All\n# of Train Examples per Class\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\nAUROC\nHyperlipidemia\n1\n2\n4\n8\n16\n32\n64 128 All\n# of Train Examples per Class\n0.5\n0.6\n0.7\n0.8\n0.9\nAUROC\nThrombocytopenia\n1\n2\n4\n8\n16\n32\n64 128 All\n# of Train Examples per Class\n0.4\n0.5\n0.6\n0.7\n0.8\nAUROC\nHyperkalemia\n1\n2\n4\n8\n16\n32\n64 128 All\n# of Train Examples per Class\n0.5\n0.6\n0.7\n0.8\nAUROC\nHypoglycemia\n1\n2\n4\n8\n16\n32\n64 128 All\n# of Train Examples per Class\n0.5\n0.6\n0.7\n0.8\nAUROC\nHyponatremia\n1\n2\n4\n8\n16\n32\n64 128 All\n# of Train Examples per Class\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nAUROC\nAnemia\n1\n2\n4\n8\n16\n32\n64 128 All\n# of Train Examples per Class\n0.5\n0.6\n0.7\n0.8\nAUROC\nChest X-ray Findings\nAUROC by Task\nCLMBR+LR\nGTE Qwen2 7B+LR\nLLM2Vec Llama 3.1 7B+LR\nCount-based+GBM\nFig. 7 Task-specific AUROC performance. Area under receiver operating characteristic curve\n(AUROC) performance with 95% confidence intervals across all 15 prediction tasks [30].\n34\n\n1\n2\n4\n8\n16\n32\n64 128 All\n# of Train Examples per Class\n0.2\n0.4\n0.6\n0.8\nAUPRC\nLong LOS\n1\n2\n4\n8\n16\n32\n64 128 All\n# of Train Examples per Class\n0.1\n0.2\n0.3\n0.4\nAUPRC\n30-day Readmission\n1\n2\n4\n8\n16\n32\n64 128 All\n# of Train Examples per Class\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nAUPRC\nICU Admission\n1\n2\n4\n8\n16\n32\n64 128 All\n# of Train Examples per Class\n0.0\n0.1\n0.2\n0.3\n0.4\nAUPRC\nPancreatic Cancer\n1\n2\n4\n8\n16\n32\n64\nAll\n# of Train Examples per Class\n0.05\n0.00\n0.05\n0.10\n0.15\n0.20\nAUPRC\nCeliac\n1\n2\n4\n8\n16\n32\n64 128 All\n# of Train Examples per Class\n0.00\n0.05\n0.10\n0.15\nAUPRC\nLupus\n1\n2\n4\n8\n16\n32\n64 128 All\n# of Train Examples per Class\n0.05\n0.10\n0.15\n0.20\nAUPRC\nAcute MI\n1\n2\n4\n8\n16\n32\n64 128 All\n# of Train Examples per Class\n0.125\n0.150\n0.175\n0.200\n0.225\n0.250\nAUPRC\nHypertension\n1\n2\n4\n8\n16\n32\n64 128 All\n# of Train Examples per Class\n0.10\n0.15\n0.20\n0.25\nAUPRC\nHyperlipidemia\n1\n2\n4\n8\n16\n32\n64 128 All\n# of Train Examples per Class\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nAUPRC\nThrombocytopenia\n1\n2\n4\n8\n16\n32\n64 128 All\n# of Train Examples per Class\n0.05\n0.10\n0.15\n0.20\nAUPRC\nHyperkalemia\n1\n2\n4\n8\n16\n32\n64 128 All\n# of Train Examples per Class\n0.02\n0.04\n0.06\n0.08\nAUPRC\nHypoglycemia\n1\n2\n4\n8\n16\n32\n64 128 All\n# of Train Examples per Class\n0.3\n0.4\n0.5\n0.6\n0.7\nAUPRC\nHyponatremia\n1\n2\n4\n8\n16\n32\n64 128 All\n# of Train Examples per Class\n0.7\n0.8\n0.9\nAUPRC\nAnemia\n1\n2\n4\n8\n16\n32\n64 128 All\n# of Train Examples per Class\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nAUPRC\nChest X-ray Findings\nAUPRC by Task\nCLMBR+LR\nGTE Qwen2 7B+LR\nLLM2Vec Llama 3.1 7B+LR\nCount-based+GBM\nFig. 8 Task-specific AUPRC performance. Area under the precision-recall curve (AUPRC)\nperformance with 95% confidence intervals across all 15 prediction tasks [30].\n35\n",
  "metadata": {
    "source_path": "papers/arxiv/Large_Language_Models_are_Powerful_EHR_Encoders_452f482883750a17.pdf",
    "content_hash": "452f482883750a17b45900a2648dfbb500dc94f4a8edbbbac0db7c145cf9043f",
    "arxiv_id": null,
    "title": "Large_Language_Models_are_Powerful_EHR_Encoders_452f482883750a17",
    "author": "",
    "creation_date": "D:20250225030720Z",
    "published": "2025-02-25T03:07:20",
    "pages": 35,
    "size": 1985344,
    "file_mtime": 1740470087.3806136
  }
}