{
  "text": "Bridging Gaps in Natural Language Processing for\nYor`ub´a: A Systematic Review of a Decade of Progress\nand Prospects\nToheeb A. Jimoh∗1, Tabea De Wille1, and Nikola S. Nikolov1\n1Department of Computer Science and Information Systems, University of\nLimerick, Castletroy, V94 T9PX, Limerick, Ireland\nAbstract\nNatural Language Processing (NLP) is becoming a dominant subset of artificial intel-\nligence as the need to help machines understand human language looks indispensable.\nSeveral NLP applications are ubiquitous, partly due to the myriads of datasets being\nchurned out daily through mediums like social networking sites. However, the growing\ndevelopment has not been evident in most African languages due to the persisting re-\nsource limitation, among other issues. Yor`ub´a language, a tonal and morphologically rich\nAfrican language, suffers a similar fate, resulting in limited NLP usage. To encourage\nfurther research towards improving this situation, this systematic literature review aims\nto comprehensively analyse studies addressing NLP development for Yor`ub´a, identify-\ning challenges, resources, techniques, and applications. A well-defined search string from\na structured protocol was employed to search, select, and analyse 105 primary studies\nbetween 2014 and 2024 from reputable databases.\nThe review highlights the scarcity\nof annotated corpora, limited availability of pre-trained language models, and linguis-\ntic challenges like tonal complexity and diacritic dependency as significant obstacles. It\nalso revealed the prominent techniques, including rule-based methods, statistical meth-\nods, deep learning, and transfer learning, which were implemented alongside datasets of\nYor`ub´a speech corpora, among others. The findings reveal a growing body of multilingual\nand monolingual resources, even though the field is constrained by socio-cultural factors\nsuch as code-switching and desertion of language for digital usage. This review synthesises\nexisting research, providing a foundation for advancing NLP for Yor`ub´a and in African\nlanguages generally. It aims to guide future research by identifying gaps and opportuni-\nties, thereby contributing to the broader inclusion of Yor`ub´a and other under-resourced\nAfrican languages in global NLP advancements.\nKeywords: Natural Language Processing (NLP); Yor`ub´a language; Systematic review;\nLow-resource language\n∗Corresponding author\n1\narXiv:2502.17364v1  [cs.CL]  24 Feb 2025\n\n1\nIntroduction\nNatural Language Processing (NLP) is one of the key components of artificial intelligence. It\nhas rapidly gained prominence in recent years as the need to help machines understand human\nlanguages grows. NLP involves developing intelligent systems that can converse with humans\nthrough natural language [1]. These intelligent systems are products of the two main classes of\nNLP: natural language understanding and natural language generation. These classes coalesce\ninto the generation of language and understanding its intricacies, respectively [2].\nNatural language typically denotes languages spoken and used by humans via various commu-\nnication channels for day-to-day interactions. In contrast, artificial languages, like computer\nlanguages, are those created with certain rules and restrictions [3]. Moreover, unlike artificial\nlanguage, natural language possesses ambiguity, making its processing often a hard nut to crack.\nFor instance, the statement, “Erik Ten Hag wins his 50th game with Manchester United,” poses\na situation where there could be two distinct meanings, depending on if one is saying Erik\nwon his 50th game in charge of the club, or that Erik won his 50th career game with Manch-\nester United. NLP applications have been seen in various exciting domains such as sentiment\nanalysis [4, 5, 6], machine translation [7, 8, 9, 10], named entity recognition [11, 12, 13, 14],\nparts-of-speech (POS) tagging [15, 16, 17], question answering [18], amongst others, aiming to\nbridge the communication gap between humans and machines.\nNLP techniques have witnessed fascinating evolution with diverse languages worldwide since\nits inception in the 1950s [19], initially with the rule-based approach. This involves defining\nlinguistic rules for core language concepts, including semantics, pragmatics, morphology and\nphonology. These techniques have metamorphosed from the rule-based approach to statistical\napproaches, subsequently incorporating machine learning techniques, which utilise large-scale\nlinguistic resources. Undoubtedly, the explosion of large datasets, mainly through social net-\nworking sites, necessitated the implementation of more advanced techniques. This births the\nintroduction of deep learning methods [20], built on neural networks. Interestingly, the field\nof NLP witnessed revolutionisation through the advancement in deep learning, with notable\nmodels like the recurrent neural networks (RNNs), long short-term memory (LSTM), and more\nrecently, the transformer architectures [21]. Moreover, this development has improved NLP\nperformance in machine translation and question-answering tasks. More importantly, it gives\nway to a better understanding of sequential dependencies in natural language [22].\nFurthermore, self-attention mechanisms and bidirectional training in models such as bidirec-\ntional encoder representations from transformers (BERT), alongside the generative pre-trained\ntransformer (GPT-3) model developed by OpenAI1, resulted in remarkable improvement in nat-\nural language understanding [23]. This innovation continues to set the pace for the emergence\nof many improved large language models (LLMs) like the Large Language Model Architecture\n(LLaMA) [24], Mistral [25], and others. LLMs, usually trained on huge datasets, have been\nrecorded to have achieved state-of-the-art result performance across various tasks, which tran-\nsitions from task-specific to task-independent architectures [26]. However, the dataset need of\nthese models translates into a boon and bane for NLP in the context of many under-resourced\nlanguages in the world. These languages, because they are less digitised, sparingly taught, har-\nbour resource scarcity & low density, are less privileged, among other identifiers, are referred\n1https://openai.com/\n2\n\nto as low-resource languages (LRLs) [27, 28].\nDespite these significant advancements in NLP for major global languages like English and\nChinese, underrepresented languages like Yor`ub´a—a language spoken by about 50 million peo-\nple [29, 30] primarily in Nigeria and its diaspora—remain under-explored in computational\nlinguistics research. Yor`ub´a’s linguistic richness, characterized by tone marking, complex mor-\nphology, and extensive oral traditions, presents unique challenges and opportunities for NLP\ndevelopment. Recent studies have underscored the importance of developing NLP resources\nand techniques for low-resource languages to ensure equitable access to technology, thereby\nconserving linguistic diversity. For Yor`ub´a, these early efforts include tasks such as diacritic\nrestoration [31], machine translation [9], sentiment analysis [32], and parts-of-speech tagging\n[33]. However, the dearth of annotated datasets, tools, and computational models tailored to\nYoruba significantly hampers progress in the field.\nTo promote NLP research involving Yor`ub´a language, it is essential to access the current status\nof such research involvement. To the best of our knowledge, no publication has addressed this\nneed. Consequently, this research seeks to provide a systematic literature review (SLR) by\nsynthesising existing research efforts in NLP for Yor`ub´a language. The study seeks to iden-\ntify trends, highlight gaps, and propose directions for future research by answering specific\nresearch questions on the tasks addressed, methods employed, available resources, and prevail-\ning challenges in NLP research involving Yor`ub´a language, either as a monolingual or bilingual\ncomponent or as part of a multilingual study, where it is greatly emphasised. These questions\nare explicitly outlined in Section 3.2.\nUltimately, the motivation for this study is twofold. Firstly, it aspires to advocate for NLP\nsolutions tailored to African languages, which remain underrepresented in global research. Ul-\ntimately, it is intended to bridge the gap between linguistic features unique to Yor`ub´a and\ntheir NLP representations, thereby contributing to linguistic preservation and technological\ninclusion as part of the Sustainable Development Goals (SDGs) of the United Nations (UN).\nThe outcomes of this review are expected to serve as a comprehensive resource for researchers\nand practitioners working on Yor`ub´a language. Moreover, the findings would inform efforts to\ndevelop robust NLP systems accounting for the linguistic and cultural needs of Yor`ub´a while\nimproving cross-lingual applications among other African languages.\n2\nBackground of study\nThis section briefly introduces the Yor`ub´a language by discussing its constituents, such as\nletters and types, and detailing its root regarding its language family.\n2.1\nYor`ub´a language overview\nYor`ub´a language is one of the largest low-resource African languages with over 47 million\nspeakers, encompassing several dialects with considerable similarities [34, 35]. It is adopted\nas a native and social language in Western African countries, including Nigeria, Togo, Benin\nRepublic, and other countries like Cuba, Brazil, etc. [36]. Yor`ub´a uses 25 out of the 26 Latin-\nscript letters, excluding q, z, v, x and c [37]. Thus, an additional 4 letters—e., o. , gb. and s.—with\n3\n\nthe existing 21 makes up the entirety of the Yor`ub´a alphabets. In addition, it is a tonal lan-\nguage with three primary lexical tones: high, medium, and low [37]. The tones are usually\nrepresented by acute (as in ´u), grave (as in `u), and an optional macron (as in ¯u), denoting\nthe high, low and mid-tone, respectively [12]. The three tonal signs and the underdots cater\nfor diacritics, which determine the linguistic meaning of words in the language. Generally,\nthe language is composed of 7 vowels (a, e, e., i, o, o. , and u), about 5 nasal vowels, (an, e.n,\nin, o. n, and un), and 18 consonants (b, d, f, g, gb, h, j, k, l, m, n, p, r, s, s., t, w, and y) [38, 31].\nAs derived by the Expanded Graded Intergenerational Disruption Scale (EGIDS) categorisation\n[39], the language currently has the institutional level status. This implies that it has achieved\nsizable development and is still utilised beyond the community and individual homes. This\nis evident in its usage and availability of resources through written books, mass media, and\nvarious undocumented oral traditions. However, it is still classified as a low-resource language\nalongside other Nigerian languages due to the dearth of basic computing resources [40]. This\nindicates that available resources remain untapped for creating natural language corpora and\ndeveloping technological and NLP tools [41].\n2.2\nYor`ub´a Language Family\nThe Yor`ub´a language belongs to the Niger-Congo family [41]. This language family is the most\nprominent and largest of the four major African linguistic groups: Niger-Congo, Nilo-Saharan,\nAfro-Asiatic and Khoisan [42]. This language family possesses distinctive noun class systems;\nnonetheless, they exhibit substantial variations in types, especially in morphological complexity\n[43]. A substantial part of languages of sub-Saharan Africa—containing Western Africa, South-\nern Africa, Eastern Africa, and Central Africa—belong to this family, making up about 85% of\nthe entire African language population [44]. Among these are Cape Town, South Africa, in the\nsouthern part; Dakar, Senegal, in the western part; and Mombasa, Kenya, in the east of Africa.\nFigure 1 presents a language tree visualisation showing the connection of these language groups\nor families. It starts with the Niger-Congo family and shows the groups and subgroups up to\nthe last node, displaying the position of the Yor`ub´a language, including others in the same class.\nAt the time of writing this paper, the Niger-Congo language family had 1552 language sub-\ngroups, which are classified into three main groups: Atlantic-Congo, Mande, and Kordofanian2.\nIn addition to three groups, a language group, Mbre, is left unclassified within the Niger-Congo\nlanguage family. The Kordofanian branch is seen to be primarily spoken by the Nuba people\nof southern Sudan. Unlike the former, the Mande languages are commonly spoken in Western\nAfrican countries, mostly in The Gambia, Burkina Faso, Senegal, and Mali, to mention a few.\nSimilarly, the Atlantic-Congo languages are mostly used in a similar demographic as the Mande\nlanguages. The languages are used mainly in Western African countries like Liberia, Guinea,\nGuinea-Bissau, Senegal, The Gambia, and Sierra Leone. Moreover, they are documented as\nthe most prominent, diverse, and primary component of the Nigeria-Congo language family [45].\nGoing down the language tree, it is seen that the Yor`ub´a language stems from the Atlantic-\nCongo language group, recognised as the most used in the Niger-Congo language family. More-\nover, the Ijoid, Atlantic and the Volta-Congo language groups emerge from this Atlantic-Congo\n2https://www.ethnologue.com/subgroup/47/\n4\n\nFigure 1: Yor`ub´a Language Family Tree\ngroup. Among these three branches, the Yoruba follows the path of the Volta-Congo subgroup,\nwhich was recorded to contain 5 branches: Kwa, Kru, Dogon, North and the Benue-Congo\nsubgroups. From these, about 12 language family subgroups are documented to be contained\nin the Benue-Congo subgroups, which house the Yor`ub´a language. However, one language,\nFali of Baissa, is left unclassified. Furthermore, the Defoid subgroup, from the Benue-Congo\nlanguages begets the Yoruboid group, which in turn contains the Edekiri and Igala language\nsubgroups. Consequently, the Yor`ub´a language stems from this Edekiri branch. Other language\nin the same class as the Yor`ub´a language include: Itsekiri, Ede Idaca, Ede Cabe, If`e, Nago,\nSouthern, Ede Ije, Ede Nago, Kura, Nago, Northern, Mokole, Ulukwumi (Ol`uk`umi), Ede Ica,\nand Lucum´ı. In summary, the Yor`ub´a language belongs to the Niger-Congo language family,\nsequentially from Atlantic-Congo, Volta-Congo, Benue-Congo, Defoid, Yoruboid, with Edekiri\nas its last branch.\n3\nMaterials and Methods\nThis section details the review processes, from planning to data synthesis.\nThe guidelines\ncomprehensively detailed in [46, 47] were followed to ensure adequacy in the review processes,\nfrom the selection of studies to the reporting stage. These guidelines help identify, analyse, and\ninterpret all information from the primary studies considered without bias or prejudice.\n3.1\nReview Planning\nA systematic literature review requires formal planning and conscientiousness. Consequently,\na defined protocol was followed to ensure the success of the entire process. The data, in the\nform of research articles, the review objectives, and defined selection criteria, were managed\n5\n\nusing the Covidence3 platform, a software tool for effectively organising systematic reviews. It\nallows for a sequential flow, following a standard review protocol. Moreover, Zotero4 is the\nreference manager, which ensures optimality and efficiency in the selection and writing process.\nCombining these tools with Microsoft Excel, the study benefits from tools that ensure flexibility\nand convenience, minimizing the required rigour of a typical systematic review.\nFigure 2: The systematic literature review protocol\nFollowing the review protocol shown in Figure 2, the first task was to define the research\nobjectives and develop the research questions based on the objectives. Thereafter, the search\nstrategy was developed. This includes defining the search string and the scope of the search.\nSubsequently, potential papers are searched through identified databases and screened based\non the selection criteria. A quality assessment is performed on the included paper to ensure\nstandard review and that the relevant data are extracted thereafter. Furthermore, the available\ndata are analysed to obtain meaningful results reported comprehensively afterward. The next\nsections explained the processes in more detail.\n3.2\nResearch Questions\nResearch questions were developed to define a precise template for the broad study objectives.\nFour questions have been carefully defined to cater to all possible domains intertwined with the\nresearch objectives by forming a structured set of interrogative statements whose answers are\nmeant to provide insight into the research goal. The questions are motivated by the need to\nknow the current status of Yor`ub´a language involvement in NLP by investigating the specific\n3https://www.covidence.org/\n4https://www.zotero.org/\n6\n\ntasks, techniques, resources, and challenges. The research questions are presented sequentially\nin Table 1 to cater to structure flow and a quick overview.\nTable 1: Research questions\nS/N\nResearch Questions\nRQ1\nWhat NLP tasks have been addressed for Yor`ub´a language?\nRQ2\nWhat techniques have been employed for Yor`ub´a NLP?\nRQ3\nWhat language resources are available for Yor`ub´a language?\nRQ4\nWhat challenges are associated with NLP development involving Yor`ub´a\nlanguage?\n3.3\nSearch Strategy\nThis section describes the gathering of the vital primary studies and the “systematic” steps\ntoward achieving the goal. It is a crucial phase planned to eliminate potential bias and in-\ncorporate randomisation in the studies’ selection and sample size determination. Reputable\ndatabases related to the subject matter were painstakingly explored to obtain all relevant stud-\nies for the systematic review using a well-defined search strategy. Generally, the central goal\nof the strategy involves attracting studies that have applied computational NLP approaches\ninvolving Yor`ub´a language, be it as a monolingual, bilingual, or multilingual NLP set-up.\n3.3.1\nMethod and Scope of Search\nThe strategy initially employed an automated search method to obtain studies relevant to the\nsystematic review. This method involved combing each electronic database with the defined\nkeywords, boolean operators, and wild cards—as and when due. Furthermore, to ensure a\nrepresentative sample and a higher recall, 9 databases were targeted in total viz: Web of Sci-\nence5, ScienceDirect6, Google Scholar7, Association for Computing Machinery (ACM) Digital\nLibrary8, Institute of Electrical and Electronics Engineers (IEEE) Xplore9, Semantic Scholar10,\nScopus11, ScienceDirect12, and Association for Computational Linguistics (ACL) Anthology13.\nMoreover, with the motive of exploring a decade of the progress of NLP research in the Yor`ub´a\nlanguage paradigm, studies between 2014 and 2024 were chosen. This decade range was also\nestablished by considering the time of writing this paper, and it is plausible since the studies of\n2014 were published towards the year’s end. In addition, to ensure potent quality, only peer-\nreviewed journal articles and conference papers were included in the search results, with the\nlanguage of writing exclusively in English. Also, due to the computational requirement of NLP\nresearch, studies mainly in Computer Science, Engineering, and Computational Linguistics\n5https://www.webofscience.com/\n6https://www.sciencedirect.com/\n7https://scholar.google.com\n8https://dl.acm.org/\n9https://ieeexplore.ieee.org/\n10https://www.semanticscholar.org/\n11https://www.scopus.com/\n12https://www.sciencedirect.com/\n13https://aclanthology.org/\n7\n\nwere considered, further focusing strictly on empirical studies. Figure 3 is a Pareto plot used to\nshow the statistics of the initial search results across the databases, alongside the cumulative\npercentage. It shows that most papers were from Google Scholar, accounting for about 60%\nof the search results, while the least was from IEEE Xplore. The curve shows the cumulative\npercentage of studies from each database.\nIt indicates the percentage contribution of each\ndatabase, with the least being IEEE Xplore.\nFigure 3: Initial Search Results across Databases\n3.3.2\nSearch Strings\nThe search strings denote the combination of keywords used in exploring the databases to ex-\ntract potential primary studies. Potential keywords in the systematic review objectives were\nidentified and combined to build this. However, before the combination, a few related research\npublications were scanned through to see the usage of the relevant phrases and words. Thus,\nsince the study seeks to synthesize information vis-a-vis NLP research and solutions associated\nwith Yor`ub´a language, the general search string is formulated based on the three segments and\npresented as follows:\n(“Natural Language Processing” OR NLP OR “Computational linguistics” OR “neural net-\nwork*” OR statistic* OR “machine learning” OR “artificial intelligence” OR corpus OR dataset*\nOR “data set*”) AND Yoruba AND (solution OR task OR application)\nHowever, it is pertinent to mention that the search string was modified slightly for different\ndatabases based on their unique requirements and search functionalities.\n3.3.3\nStudy Selection Criteria\nHaving obtained a substantial number of studies considered relevant to the study, the crite-\nria to painstakingly select the most essential and directly aligned studies were defined. This\n8\n\ninvolves a set of defined statements based on the research objectives and scope, making up\nthe inclusion and exclusion criteria. It accounts for the desired year range, publication type,\naccess to full-text documents, etc. Table 2 shows the inclusion and exclusion criteria, which are\ndefined following the PICO criteria: population, intervention, context, and outcome [46]. In\naddition, Study Range and Publication Type and Publication Language were used to represent\nthe publication years considered for the primary, the type of publication, and the language of\npublication, respectively. Furthermore, the two classes of criteria—inclusion and exclusion—\ndefined statements to determine if the particular study is to be part of the review or irrelevant\nto it, respectively. Relevant articles were searched and gathered via the listed databases, bear-\ning in mind the defined criteria.\nInitially, 3144 studies were obtained across the 9 databases. The RIS files containing these\nstudies from their respective sources were uploaded on the Covidence software tool. Due to the\ninevitable intersection across databases, the software detected a total duplicate count of 816,\nwhile 50 duplicates were manually detected. Again, following the selection criteria, abstract\nscreening was initially carried out, leaving out a total of 1865 from the unique 2278 studies.\nMoreover, full-text screening was carried out on the remaining 413 studies, and a total of 308\nstudies were discarded based on defined reasons. Eventually, 105 primary studies were obtained\nfor inclusion in the review.\nSubsequently, backward and forward snowballing techniques were also used to capture studies\nthat might have been missed in database searches due to conflicting terminology, following [48].\nFor backward snowballing, reference lists of a quarter of the 105 articles were surveyed to check\nfor additional relevant studies. It was observed that the identified publications were part of\nthe initial search results across databases. Additionally, forward snowballing, which involves\nchecking for publications that have cited the potential primary study, was carried out. Here,\n5 pre-prints and 1 review publications were discovered; however, they were not included due\nto the defined exclusion criteria. The snowballing loops were ended after no new studies were\nobtained. Ultimately, 105 primary studies were included in the SLR after conducting a quality\nassessment check, presented in Section 3.4. The whole selection process ensures a detailed and\ncomprehensive overview by incorporating the Preferred Reporting Items for Systematic Reviews\nand Meta-Analysis (PRISMA) framework, which is also presented in Figure 4, following [47].\n3.4\nQuality Assessment\nThis section shows further efforts to ensure the credibility of primary studies used in the system-\natic literature review. Specific quality questions were used for verification to further ascertain\nthe reliability of the primary studies to be eventually included in the SLR. Thus, a quality\nchecklist was created, following the guidelines in [46]. The response associated with each ques-\ntion is defined through a multiple-choice response: Yes, Partially, and No. A “Yes” response\nshows higher certainty, thus getting a score of 2. Similarly, a “Partially” response is assigned\n1, while a “No” response is assigned 0. For this research purpose, a total quality score below\nthe median score is to be excluded. The median is calculated as the value in the\n\u0010\nn+1\n2\n\u0011th\nposition, where n is the maximum quality score, which equals 16. Thus, for quality scores\nSi = {1, 2, . . . , 16}, the median position, Mp, is derived as follows:\n9\n\nFigure 4: PRISMA diagram of selection process\n10\n\nTable 2: Inclusion and exclusion criteria\nS/N\nInclusion\nExclusion\nPopulation\nStudies specifically addressing NLP so-\nlutions, tasks, or methods for Yor`ub´a\nlanguage\nStudies not involving Yor`ub´a language\nComparison\nComparative studies of NLP solutions\nfor low-resource languages, including\nYor`ub´a\nComparative\nstudies\nnot\nincluding\nYor`ub´a\nOutcome\nStudies reporting the performance of\nNLP solutions involving Yor`ub´a\nTheoretical studies without practical\nevaluations or results\nIntervention\nStudy\ninvolving\nNLP\nsolutions\nfor\nYor`ub´a language\nStudy involving NLP solutions for un-\nrelated languages\nStudy Range\nPublications between 2014 & 2024 in-\nclusive\nPublications before 2014\nPublication type\nPeer-review journal articles and confer-\nence papers\nNon-peer review studies, including the-\nsis, reviews, surveys, etc.\nPublication language\nStudies published in English language\nStudies published in languages other\nthan English\nMp =\n\u0012n + 1\n2\n\u0013th\nposition\n(1)\nMp =\n\u001216 + 1\n2\n\u0013th\nposition\n∴Mp = 81\n2\nThus, the median, M, is\nM = 8 + 9\n2\n= 8.5\nConsequently, a paper with a score less than 8.5 out of the 16 total score will be excluded.\nHowever, no study falls in this range. Hence, the total 105 primary studies are included in the\nsystematic review. Table 3 shows the checklist containing the quality domain and the questions\nasked for each.\nAfter assessing each potential primary study based on the quality questions, scores were as-\nsigned—a maximum of 16 for each. The results are plotted through the histogram in Figure 5.\nThe plot shows that most of the primary studies have a score of 15, followed by 14. Moreover,\nonly a minute quantity of the primary studies has a total score between 9 and 12, with 9 being\nthe minimum total score being 9, while the maximum is 16. This shows high quality in the\nincluded primary studies, potentially making this review’s results highly valuable.\n3.5\nData Extraction\nData extraction involves collecting information relevant to the SLR from the primary studies.\nIt was carried out by defining an extraction template. This template is structured based on\n11\n\nTable 3: Quality Assessment Checklist\nDomain\nQuestion\nResearch objective\nAre the objectives of the study clearly stated?\nMethodology and study design\nAre the study’s methods and experimental design clearly defined?\nResearch documentation\nAre the study’s processes comprehensively documented?\nResearch Question Alignment\nAre the research questions answered through the findings?\nStudy conclusion\nDo the conclusions of the study relate to its objectives?\nResult evaluation\nDoes the study validate the results using standard evaluation met-\nrics?\nLimitations and bias\nAre the limitations of the study acknowledged?\nNovelty and contribution\nDoes the study contribute new tools, resources, insights, or inves-\ntigate new questions?\nFigure 5: Histogram of total quality score for each study\n12\n\nthe following sections: Bibliographic information, research context, NLP focus, language scope,\nresults and evaluation, and contributions. The sections, in turn, contain individual elements\nthat constitute all the extraction items. Pilot extractions were carried out with fewer samples\nto verify the results and update the form based on the need or irrelevance of variables to be\nextracted. For this study, the “research country” and “research continent” are taken as the\naffiliation of the lead author at the time of their research publication. The final extraction\ntemplate is given in Table 4.\nTable 4: Data Extraction Template\nVariable\nExtraction Item\nV1\nStudy ID\nV2\nArticle Title\nV3\nPublication Year\nV4\nResearch Country\nV5\nResearch Continent\nV6\nPublication Type\nV7\nPublication Source\nV8\nNLP Task\nV9\nNLP Technique\nV10\nLanguage Scope\nV11\nDataset\nV12\nCitation Count\n3.6\nData Synthesis\nThis section involves integrating and interpreting information extracted from the selected stud-\nies. Initially, data were obtained from individual databases and combined to form the primary\nstudies after excluding required studies based on the selection criteria. Findings were obtained\nfrom the primary studies across diverse domains, including the NLP tasks, techniques, re-\nsources, and challenges. Moreover, the process was carried out using Microsoft Excel and the\nCovidence platform, mentioned explicitly in Section 3.1. The ultimate goal at this stage is to\nidentify patterns, trends, and knowledge gaps to inform future research directions in Yor`ub´a\nNLP.\n4\nResults and Interpretations\nThis section discusses the results obtained from the primary studies included in the systematic\nreview through the data synthesis. It helps answer the research questions and provides visual\nillustrations of every component of the primary studies.\n4.1\nGeneral Statistics\nThe primary studies included in the research, totalling 105, were synthesized for relevant data.\nThey consist only of journal articles and conference papers based on the selection criteria spec-\nified in the systematic review protocol. Figure 6 reveals the frequency of the sample—included\n13\n\nprimary studies—over the years. It attests that the Yor`ub´a NLP research has grown over the\nyears, with an apparent upward trend after the break in year 2017. In addition, 2023 and 2024\nhave the highest number of primary studies, even though the latter year is yet to end at the\ntime of this review. Subsequently, the publications are assessed based on the type. Figure 7\nshows the distribution of article type, in which there are more conference papers (62.9%) than\njournal articles (37.1%). This observation is plausible as most researchers in this domain are\nprimarily involved in conferences.\nFigure 6: Distribution of publication per year\nFurthermore, accessing the authors contributing to this field shows an informative trend, as\ndepicted in Figure 8. Figure 8b shows the top 20 authors in this field based on the primary\nstudies. The top author, David Adelani, has about 18 included publications in total. Moreover,\nthere have been observable collaborations among the top 7 authors, including Dossou, BFP;\nand Osei, S. Similarly, significant collaborations exist among Lin, J; Oladipo, A; and Ogundepo,\nOJ, among other notable collaborations. In addition, even though the top 20 authors in the\nfield were represented, the minimum number of publications on the chart in Figure 8b being 5,\nleft out authors with publications between 2 and 4. Hence, Figure 8a ensures a representative\nvisualization by revealing famous authors in the research domain.\nFinally, Figure 9 shows the first authors’ geographical locations at the publication’s time. It\nreveals an interesting insight, making it known that most publishers were based in Nigeria at the\ntime of publishing, while a considerable number were also in the United States, Canada, South\n14\n\nFigure 7: Distribution of publication types\n(a) Common authors\n(b) Top 20 authors\nFigure 8: Popular authors by publication number\n15\n\nAfrica, Germany, and the Benin Republic. This tells that research in this domain extends\nbeyond the dominant speaking country of the Language—Nigeria. It also shows promising\noutcomes due to visible collaborations taking place among researchers across the world.\nFigure 9: Publications by geographical location\n4.2\nResearch Question Analysis\nThis section contains pertinent responses to specific research questions outlined by the system-\natic literature review objectives. The analysis presented herein represents a key contribution to\nthe review. It summarises the inquiry findings through structured responses to each question,\nmaking it an essential tool for answering similar questions for future researchers.\n4.2.1\nRQ1: What NLP tasks have been addressed for the Yor`ub´a language?\nInvestigating the various NLP tasks involved with applying Yor`ub´a language has revealed im-\nproved and promising results over the years. These NLP tasks or solutions synthesis considered\nthe application of the language either as a monolingual NLP research or as part of a bilingual\nor multilingual task—only considered when there is a vivid emphasis on the language.\nEfforts have been made to democratize the numbering system of Yor`ub´a language through NLP\nwith an earlier study [49], which tried to develop a computational system for converting cardi-\nnal numbers into their Yor`ub´a number names in a number-to-text conversion task. Similarly,\n[50] develops a machine translation system for translating English number text into Yor`ub´a\ntext, considering the context. While the former focused on the text normalization task, a core\n16\n\nfoundational NLP task required in this research domain, the latter focused on machine trans-\nlation. Moreover, diacritic restoration, an important component of text normalization tasks,\nhas been given considerable research efforts, as seen from [31, 51, 52, 53] studies. Ultimately,\nthree studies [54, 55, 56] have made efforts toward improving spell-checking and correction in\nthe Yor`ub´a texts. These tasks are crucial for accurately representing texts in the language,\nthereby improving the availability of quality data.\nThe need to understand sentence structure due to its importance for the syntactic parsing of\ntext has also made parts-of-speech tagging essential research towards improving Yor`ub´a NLP.\nThis involves assigning grammatical categories, such as nouns, adjectives, adverbs, etc., to\nwords based on the meaning or context. Four studies [17, 15, 33, 16] worked on this task,\nwhereas [17] involved 20 typologically diverse African languages, in which Yor`ub´a was a key\nsubset of. Similarly, NLP tasks involving morphological induction [57] and analysis [58, 59]\nwere explored as these are essential for breaking words into their roots and affixes. In addition,\ntwo studies [60, 61] investigated syllabification tasks, which involve breaking words into their\nsyllables. These tasks are essential for ensuring precise semantic and syntactic parsing in texts.\nFurthermore, identifying entities, such as names of people, organizations and places, in texts is\ncrucial to advancing Yor`ub´a NLP. This involves the named entity recognition aspect of NLP\nand has been investigated in a significant number of studies, including [11, 62, 12, 14, 13].\nThese studies are essential for identifying specific entities, such as cultural components and\npersonal names in Yor`ub´a, and are, in turn, vital for downstream tasks. A study [63] also\ndeveloped a dependency parsing multi-task model in a multilingual approach by considering\nWolof, Bambara, and Yor`ub´a language as core components. The study is crucial for analyz-\ning grammatical relationships among words in a sentence, and it involves syntactic knowledge\ntransfer from high-resource to the extremely low-resource languages referenced.\nMany studies also explored the crucial task involving automatic translation of text data between\nYor`ub´a and other languages. About 18 studies explored the machine translation (MT) tasks,\ncutting across rule-based machine translation (RBMT) [30, 64, 65, 66, 67, 50, 9], statistical\nmachine translation (SMT) [8, 68], neural machine translation (NMT) [28, 10, 69, 70, 71, 72],\nand a few hybrid MT methods involving SMT and RBMT [29]; RBMT and example-based\nmachine translation (EBMT) [73]; SMT and NMT [7]. Moreover, a similar task on sentence\nalignment, which is crucial to NMT, was explored in [74], considering a bilingual approach\ninvolving English-Yor`ub´a pair.\nLanguage modelling is also an indispensable task, as it is often necessary to predict the likeli-\nhood of word sequences to capture linguistic patterns in languages. Only a handful of studies\nhave been seen to work in this domain; the related studies of [75, 76] specifically involve multilin-\ngual language model pre-training, which were developed for African languages, with significant\nemphasis on Yor`ub´a language. Moreover, to verify the effectiveness of a modest amount of\ndata for multilingual language modelling, [77] introduced the AfriBERTa model, while [78]\nalso introduced AfriTeVa, to further extend language modelling with limited training data to\nsequence-to-sequence modelling. Furthermore, the semantic modelling task was explored in\n[38]. It involves the construction of a Yor`ub´a language ontology meant to characterise the\nsemantic relationships between the words, among which are antonyms, synonyms, hyponyms,\nhypernyms, holonyms and meronyms.\n17\n\nUnderstanding sentiment-bearing phrases or words in text and speech is equally essential to\nYor`ub´a NLP for determining the language users’ emotional undertone. Research in sentiment\nanalysis has also been explored across a few studies, with most using a multilingual approach.\n[79, 80] strictly investigate sentiment analysis while [5, 6, 81, 4] includes language resource de-\nvelopment alongside the sentiment analysis tasks addressed. In addition, research which solely\ninvolves text [65, 82, 77] or topic classification [83, 84] was also explored in five studies alto-\ngether. Moreover, under these broader classification categories, the language identification or\ndetection task was also carried out in two studies [85, 86].\nEven though most studies tend to involve text processing, quite a substantial number of studies\nalso investigated speech processing in Yor`ub´a NLP. This ranges from text-to-speech synthesis\n[87, 88, 89, 90], tone recognition for continuous speech [91], speech recognition [92, 93, 94],\ntext-to-speech analysis [95], speech-based gender recognition [96] and multi-tasks involving\nboth text-to-speech and speech-to-text [97], text-to-speech and language speech resource de-\nvelopment [98, 99, 36, 100, 101], speech synthesis and language pitch modelling [102], and two\nstudies exclusively focusing on speech corpus development [103, 104]. Whereas, [35] differs\nas it is based on corpus development for visually grounded speech. Furthermore, to optimize\nmodelling in speech recognition, acoustic unit discovery tasks [105, 106, 107] are essential in\nlearning speech sounds embedding to retain the linguistically relevant acoustic information and\ndiscard the irrelevant ones.\nUltimately, increased usage of social networking sites and websites, among others, has undoubt-\nedly exploded the extent of information available in recent years. Thus, developing effective\nmethods of accessing and retrieving information is indispensable. Few studies also explored\nthis, including tasks on dense information retrieval [108], monolingual retrieval tasks [109], and\ncross-lingual information retrieval tasks [18, 110, 111].\nThese insights show considerable progress with the involvement of Yor`ub´a language in various\nNLP tasks, culminating in invaluable efforts at solution development in the research domain.\nTable 5 is used to summarize the various NLP tasks involving Yor`ub´a as presented by the\nprimary studies.\nTable 5: Summary of NLP Tasks and Corresponding Studies\nNLP Task\nStudies\nAddressing\nTask\nPopular Techniques\nDataset used\nAcoustic unit discov-\nery\n[106], [107], [105]\nStatistical modelling\nYor`ub´a Speech Cor-\npus; Custom data\nAutomatic\nspeech\nrecognition\n[101], [103], [94], [92]\nStatistical modelling\n`Ir`oy`ınSpeech; Custom\ndata\nDiacritic restoration\n[51], [53], [52]\nNeural networks\nLagos-NWU\nYor`ub´a\nSpeech Corpus; Cus-\ntom data\nContinued on next page...\n18\n\nNLP Task\nStudies\nAddressing\nthe Task\nPopular Techniques\nDataset used\nLanguage modelling\n[38], [75], [77], [76]\nMultilingual\npre-\ntraining\nW´UR`A;\nCommon\nCrawl Corpus;\nCus-\ntom data\nInformation retrieval\n[110],\n[109],\n[108],\n[111], [18]\nMultilingual language\nmodelling\nAFRIQA;\nMIRACL;\nCIRAL; Custom data\nMachine translation\n[8], [29], [30], [28], [71],\n[9], [64], [70], [69], [65],\n[7], [73], [66], [67], [74],\n[50], [68], [72], [10],\n[34], [112], [37]\nRule-based;\nstatisti-\ncal modelling; multi-\nlingual pre-training\nWebCrawl\nAfrican;\nYOR`ULECT;\nIkiniYor`ub´a;\nMENYO-20k; Custom\ndata\nNamed Entity Recog-\nnition\n[11], [62], [12], [14],\n[113], [13], [77], [84]\nTransfer\nlearning;\nPre-training multilin-\ngual embeddings\nMasakhaNER;\nCom-\nmon\nCrawl\nCorpus;\nCustom data\nPart-of-speech tagging\n[17], [16], [33], [15]\nStatistical modelling;\ncross-lingual transfer\nMasakhaPOS; Lagos-\nNWU Yor`ub´a Speech\nCorpus; Custom data\nSentiment analysis\n[4], [5], [6], [81], [80],\n[79]\nDeep learning; trans-\nfer learning\nAfriSenti; NaijaSenti;\nNollySenti;\nCustom\ndata\nSpeech-based\ngender\nrecognition\n[114], [96]\nNeural networks\nLagos-NWU\nYor`ub´a\nSpeech Corpus\nSpell checking & cor-\nrection\n[54], [55], [56]\nStatistical modelling\nCustom data\nSyllabification\n[61], [60]\nRule-based\nCustom data\nSpeech-to-Text\n[97], [106]\nStatistical modelling\nYor`ub´a Speech Cor-\npus; Custom data\nText and topic classi-\nfication\n[115], [83], [82], [77],\n[84], [85], [86]\nMachine\nlearning;\nmultilingual language\npre-training\nMasakhaNEWS;\nCommon\nCrawl\nCorpus; Custom data\nText-to-Speech\n[87], [88], [97], [101],\n[98], [99], [36], [100],\n[104], [89], [90]\nRule-based\n`Ir`oy`ınSpeech;\nProsodic Read Speech\nCorpus;\nBibleTTS;\nAntenatal\nOrienta-\ntion Speech Dataset;\nYor`ub´a Speech Cor-\npus; Custom data\nTone\nidentification\nand recognition\n[91], [93], [95]\nMachine\nlearning;\nneural networks\nYor`ub´a Speech Cor-\npus; Custom data\n4.2.2\nRQ2: What techniques have been employed for Yor`ub´a NLP?\nThe metamorphosis of NLP techniques involving Yor`ub´a language is observed to not necessarily\nlag behind the general trend of accomplishment in the NLP community. Most of the earlier\nresearch works are seen to be limited to implementing rule-based techniques [87, 30, 64, 55] be-\n19\n\nfore the advancement, leading to incorporating statistical modelling [103, 86, 8, 116], and some-\ntimes, a combination of both techniques [81]. Furthermore, recent studies have incorporated\nsupervised machine learning [11, 51, 82, 93] and unsupervised learning [117, 107, 57, 105, 59]\ntechniques in this research domain, and a few times, the hybrid of statistics and machine\nlearning techniques [33, 116]. Lately, deep learning architectures [80, 79, 52, 31, 28], as well as\na hybrid of machine and deep learning methods [115] are being utilised for various tasks as well.\nAlso, due to the low-resource nature of Yor`ub´a language, NLP research works in its domain have\nbenefitted from transfer learning by pre-training on a high-resource language and fine-tuning on\nspecific tasks in this language. Transfer learning of this nature over the years has been mainly\ncomposed of leveraging multilingual pre-training [71, 118, 119, 120, 63, 77] and cross-lingual\ntransfer [121, 18, 17, 111]. Figure 10 shows word clouds of both the employed techniques 10b\nand the NLP tasks 10a carried out in the primary studies involved in the review.\n(a) Common NLP tasks\n(b) Common NLP techniques\nFigure 10: Word clouds of NLP tasks and techniques\nTable 6 shows a cross-tabulation of the specific NLP tasks involving Yor`ub´a language together\nwith the NLP techniques utilised in each task. It reveals that only studies involving senti-\nment analysis and TTS tasks utilised all the techniques as classified. The MT task closely\nfollowed the previous two tasks vis-a-vis the varieties of techniques involved in their studies; all\nthe techniques classes were utilised in the MT studies, except for the machine learning tech-\nnique. Moreover, other tasks have studies utilising most of the classes of techniques; however,\nacoustic unit discovery, ASR, spell checking & correction, speech-to-text, and tone identifica-\ntion & recognition tasks include studies using only two different techniques. In addition, only\nthe speech-based gender recognition task has studies involving only a single technique—deep\nlearning technique.\n20\n\nTable 6: Cross Tabulation of NLP Tasks and Techniques\nNLP Task\nTechniques\nRule-\nBased\nStatistical\nModelling\nMachine\nLearning\nDeep\nLearning\nTransfer\nLearning\nMultilingual\nPre-training &\nCross-Lingual\nTransfer\nAcoustic unit discov-\nery\n-\n✓\n✓\n-\n-\n-\nAutomatic\nspeech\nrecognition\n-\n✓\n-\n-\n✓\n-\nDiacritic restoration\n-\n✓\n✓\n✓\n-\n-\nLanguage modelling\n-\n-\n-\n✓\n✓\n✓\nInformation retrieval\n-\n-\n-\n-\n✓\n✓\nMachine translation\n✓\n✓\n-\n✓\n✓\n✓\nNamed Entity Recog-\nnition\n-\n-\n✓\n-\n✓\n✓\nPart-of-speech tagging\n-\n✓\n✓\n-\n✓\n✓\nSentiment analysis\n✓\n✓\n✓\n✓\n✓\n✓\nSpeech-based\ngender\nrecognition\n-\n-\n-\n✓\n-\n-\nSpell checking & cor-\nrection\n✓\n✓\n-\n-\n-\n-\nSyllabification\n✓\n-\n✓\n✓\n-\n-\nSpeech-to-Text\n-\n✓\n-\n-\n✓\n-\nText and topic classi-\nfication\n-\n-\n✓\n✓\n✓\n✓\nText-to-Speech\n✓\n✓\n✓\n✓\n✓\n✓\nTone\nidentification\nand recognition\n-\n-\n✓\n✓\n-\n-\n4.2.3\nRQ3: What language resources are available for Yor`ub´a language?\nMany low-resource languages experience stunted growth in their language development due\nto the flaw associated with their lack of substantial datasets required for various NLP tasks\namidst recent data-hungry models. This factor has undoubtedly prompted significant efforts at\ndeveloping various language resources for Yor`ub´a language to further improve the performance\nof various NLP tasks in the language. The developed resources have been seen to cut across\ndifferent domains; while some studies solely focused on resource development, others utilised\nthe datasets or corpora on specific tasks. The summary of the corpora, which contains Yor`ub´a\neither as a monolingual, bilingual, or as an important part of a multilingual dataset, is pre-\nsented in Table 7. Furthermore, for detail purposes, specific datasets or corpora with assigned\nnames are briefly outlined individually as follows, while others are listed as part of the general\ndescriptions:\nAFRIQA: AFRIQA [18] pioneers among cross-lingual question-answering (QA) corpus devel-\n21\n\nopment for African languages. It is the first cross-lingual open retrieval question-answering\n(XOR QA) dataset focused on African languages, bridging the gap in resource availability\nand paving the way for more equitable and inclusive question-answering technologies. More\nimportantly, Yor`ub´a language is a core component among the languages in the dataset, as\nit comprises about 12, 239 XOR QA examples for 10 African languages across the southern,\nwestern, eastern and central African regions, viz: Bemba, Fon, Hausa, Igbo, Kinyarwanda,\nSwahili, Twi, Wolof, Yor`ub´a and Zulu. Specifically, for Yor`ub´a language, the training, dev,\nand testing examples are 360, 261, and 332, respectively. Furthermore, experiments to evaluate\nthe performance of state-of-the-art models on the dataset were carried out, and this mainly\ninvolved automatic translation, which entails translating questions or retrieved documents in\npreparation for processing, and multilingual retrieval, involving carrying out direct retrieval\nthrough the aid of multilingual embeddings.\nAfriSenti: AfriSenti [4] focuses on providing a high-quality, large-scale dataset to address the\nlack of resources for sentiment analysis in underrepresented African languages. This benchmark\ncontains over 110, 000 posts involving 14 African languages across four language families—\nNiger-Congo, Afro-Asiatic, English Creole and Indo-European—in which Yor`ub´a is a core part.\nThe languages featured in the dataset are namely: Amharic, Algerian Arabic, Hausa, Igbo,\nKinyarwanda, Moroccan Arabic, Mozambican Portuguese, Nigerian Pidgin, Oromo, Swahili,\nTigrinya, Twi, Xitsonga, and Yor`ub´a. The sentiment categories captured in the corpus include\npositive, negative and neutral. Furthermore, baseline traditional machine learning and pre-\ntrained multilingual language models were trained to facilitate empirical analysis. Moreover,\nevaluation metrics include accuracy, F1 score, and precision/recall. Overall, it established a\nbenchmark for sentiment analysis in Yor`ub´a language, also enabling comparisons of models and\ntechniques for African languages.\nAfriWOZ: AfriWOZ [122] also pioneers in resource development involving dialogue generation\nfor low-resource African languages. It presents a high-quality dialogue generation dataset for 6\nAfrican languages, emphasising the Yor`ub´a language as a subset. Other five languages include\nHausa, Kinyarwanda, Nigerian Pidgin English, Swahili, and Wolof; all languages spread across\nthree language families—Afro-Asiatic, Niger-Congo, and English Creole. The dialogue dataset\nfor the included languages was developed from the MultiWoz dataset [123], and experiments\nwere performed to avail empirical analysis for related tasks using transfer learning approaches.\nAntenatal Orientation Speech Dataset: This dataset contains multilingual speech datasets\nin English, Nigerian Pidgin English, and Yor`ub´a languages, basically for antenatal orientation\nin Nigeria, being the main geographical source of the languages. The dataset [100] is composed\nof a word count of 2, 639 for English, 3, 202 for Yor`ub´a, and 2, 521 for Pidgin. Moreover, the\ncorresponding speech datasets had 59, 880 seconds for English, 70, 380 seconds for Yor`ub´a, and\n69, 840 seconds for Pidgin, both showing a dominant percentage for Yor`ub´a language. The size\nof the speech datasets was 15.6 KB for English, 17.9 KB for Yor`ub´a, and 18.3 KB for Pidgin.\nThis dataset will be valuable for antenatal orientation in Nigeria and contribute to the currently\nlow-resource status of Yor`ub´a language by improving its availability for NLP tasks.\nBibleTTS: BibleTTS [98] is an open speech multilingual dataset containing ten languages of\nsub-Saharan Africa, viz: Akuapem Twi, Asante Twi, Chichewa, Ewe, Hausa, Kikuyu, Lin-\ngala, Luganda, Luo, and Yor`ub´a. The corpus emanates from Bible recordings provided by\n22\n\nthe Open-Bible project of Biblica and contains high-quality 48kHz studio recordings for single\nspeakers, with up to 86 hours of data per language. Furthermore, TTS models were developed\nby leveraging the dataset, which ascertains its usability and quality for speech synthesis. The\ndataset is openly available for commercial and non-commercial use as it’s licensed CC-BY-SA\nLicense, promoting accessibility and further research in text-to-speech for Yor`ub´a, among other\nlanguages captured.\nCIRAL: CIRAL[111] presents a cross-lingual information retrieval (CLIR) in four languages—\nHausa, Somali, Swahili, and Yor`ub´a—with great emphasis on Yor`ub´a language. The corpus\ncomprises over 2.5 million in passages curated from indigenous African websites. The data\nannotation involved native speakers annotating over 1, 600 queries and 30, 000 binary relevance\njudgments, ensuring high-quality data for evaluation. Furthermore, reproducible baselines in-\nvolving translation-based and embedding-based CLIR were further developed to support em-\npirical analysis for related tasks. The dataset is relevant for retrieving relevant documents or\npassages in one language, e.g., Hausa, Swahili, Yor`ub´a, or Somali, based on queries expressed\nin another language.\nIkiniYor`ub´a: IkiniYor`ub´a[37] is a dataset focusing on the cultural aspect of Yor`ub´a, intending\nto investigate how well MT models can translate greetings in the language into English. This\ndataset comprises 960 parallel sentences of Yor`ub´a greetings and their English translations.\nMoreover, it also incorporates the movie transcript subset of the MENYO-20k dataset[72],\nwhich contains conversational Yor`ub´a-English data. The performance of MT models was eval-\nuated on the datasets to enable empirical analysis for related studies in future. The introduction\nof the IkiniYor`ub´a dataset is a valuable contribution to the research community working on\nYor`ub´a NLP.\n`Ir`oy`ınSpeech: `Ir`oy`ınSpeech [101] presents high-quality speech corpus exclusively in Yor`ub´a\nlanguage, which allows for multipurpose usage involving Automatic Speech Recognition(ASR)\nand Text-to-Speech (TTS) tasks. The data collection involves curating 23,000 sentences from\nnews and creative writing domains under an open license (CC-BY-4.0). Also, it included 5,000\nsentences on the Mozilla Common Voice platform to crowdsource recordings and validations.\nThe data contributions include 42 hours of in-house recorded speech data by 80 volunteers and\nan additional 6 hours of validated recordings on Mozilla Common Voice. Ultimately, a high-\nfidelity single-speaker Yor`ub´a TTS system was evaluated on the curated speech data, including\na baseline word error rate (WER) of 23.8 for ASR purposes.\nLagos-NWU Yor`ub´a Speech Corpus: To promote research voice recognition, a speech\ncorpus including 16 female and 17 male speakers was recorded in Lagos, Nigeria [16]. A total\nof 130 utterances read from brief texts chosen for phonetic coverage were recorded by each\nspeaker. Moreover, recordings were made using a microphone attached to a laptop computer in\na peaceful office setting to ensure quality in the corpus. Ultimately, this corpus will be relevant\nfor various NLP tasks in Yor`ub´a, such as TTS, ASR, etc.\nMasakhaNER: MaskhaNER [12] boasts as the first largest high-quality dataset for named\nentity recognition tasks focused on African languages, in which Yor`ub´a was greatly empha-\nsised. The dataset scope extends to 10 African languages and across four language families,\nand they include Yor`ub´a, Amharic, Hausa, Igbo, Kinyarwanda, Wolof, Nigerian-Pidgin, Luo,\n23\n\nand Swahili.\nMasakhaPOS: MasakhaPOS [17] presents a huge POS dataset available for 20 diverse African\nlanguages, of which Yor`ub´a language was greatly emphasised. Other languages featured in the\ndataset include Bambara, Fon, Hausa, Igbo, Luo, Luganda, Akan/Twi, Wolof, isiZulu, isiXhosa,\nSetswana, Kiswahili, Nigerian Pidgin English, Mossi, Kinyarwanda, Ghom´al´a, ´Ew´e, chiShona,\nand Chichewa. Moreover, Yor`ub´a data was jointly obtained from Voice of Nigeria and Ase-\njere online newspapers, containing a 43, 601 token with an average sentence length of 24.4.\nMoreover, baseline models using conditional random fields (CRF) and multilingual pre-trained\nlanguage models such as mBERT and XLM-R were developed to support empirical analysis for\nrelated tasks.\nMENYO-20k: MENYO-20k [72] presents a multi-domain parallel corpus of Yor`ub´a-English\nwith clean orthography and standardized train-test splits, with focus on improving the evalua-\ntions of MT models on low-resource language. The dataset’s domain includes texts from news\narticles, radio and movie transcripts, TED talks, etc. In addition, special attention was given\nto the diacritization of Yor`ub´a texts in the corpus, as it plays an essential role in intelligibility\nand translation quality in MT NLP tasks. Neural MT models are benchmarked to enable future\nempirical analysis in similar studies. The dataset is available through a CC BY-NC 4.0 licence.\nNaijaSenti: NaijaSenti [6] is a sentiment classification corpus for four prominent languages\nused as a medium of communication in Nigeria: Yor`ub´a, Hausa, Nigerian Pidgin English and\nIgbo. The dataset was obtained from X—formerly Twitter— and comprises roughly 30, 000\ntweets for each language, including code-mixed tweets. Benchmarks were also developed on\nmonolingual sentiment analysis tasks for each language, and the dataset and model codes are\npublicly available online.\nNollySenti: NollySenti [5] is a parallel multilingual corpus for sentiment classification in five\nlanguages spoken in Nigeria, including Yor`ub´a, Nigerian Pidgin English, Hausa, Igbo and En-\nglish. The dataset source reviews from Nollywood movies—movies primarily made in Nigeria—\ninitially in English, with translations into the four Nigerian languages involved. Moreover, the\ninitial collection for the dataset comprises 882 negative reviews and 1, 018 positive reviews, with\nan average word length of 20.7 and 35.0, respectively, making up two sentiment classification\ncategories. Benchmarks were also developed through traditional machine-learning techniques\nand pre-trained language modelling to enable empirical analysis for future research in the do-\nmain.\nProsodic Read Speech Corpus: This corpus contains a high-quality speech corpus [104] of\n7, 376 phrases and sentences in Standard Yor`ub´a language. A TTS system was developed on\nthe corpus and evaluated using the mean open score and semantically unpredictable sentence\n(SUS) test score to support empirical analysis for future related studies. The developed corpus\ncan be used as valuable research material for future studies on Yor`ub´a speech processing, syn-\nthesis, and recognition\nTATA: TATA [119] dataset is a large and high-quality multilingual dataset focused on African\nlanguages, created from Demographic and Health Surveys (DHS) Program bilingual reports. It\nconsists of 8, 700 examples in nine languages, with Russian as a zero-shot test language. Yor`ub´a\n24\n\nis a core component of these languages; other languages in the dataset include Arabic, English,\nFrench, Hausa, Igbo, Portuguese, and Swahili. A transformer-based multilingual pre-trained\nlanguage model was evaluated on the dataset, supporting empirical analysis in future related\nstudies.\nTTS Yor`ub´a Speech Corpus: A comprehensive Yor`ub´a speech corpus was designed pri-\nmarily for TTS synthesis research and development in the language [99]. The Yor`ub´a speech\ncorpus contains 2, 415 sentences with 46, 117 words and 148, 823 phonemes. The corpus has\na good balance of sentence types—affirmative, interrogative, and exclamatory—and phoneme\ndistribution. Furthermore, the Yor`ub´a speech corpus was also integrated into the MaryTTS14\nopen-source multilingual text-to-speech (TTS) synthesis platform, which achieved a Mean Opin-\nion Score (MOS) of 2.9 out of 5 for the quality of the synthesized speech.\nWebCrawl African: WebCrawl African [112] contains multilingual parallel corpora for di-\nverse African languages.\nIt was gathered to build resources for machine translation tasks\nin low-resource and extremely low-resource African languages. The parallel corpus spans 74\nlanguage pairs, which includes 15 African languages, of which Yor`ub´a language was greatly\nemphasised. For empirical analysis purposes, two multilingual models were trained on behalf\nof 24 African languages, including Yor`ub´a language, using the dataset and evaluated using\nFLORES-200 [124] benchmarks. The dataset will be useful for multilingual and cross-lingual\nmachine translation tasks involving Yor`ub´a language.\nW ´UR`A: W´UR`A [76] presents a new high-quality multilingual pre-training corpus for African\nlanguages, with Yor`ub´a greatly emphasised. The dataset is ≈30GB for all languages and\n≈19GB for African languages. Downstream tasks ranging from MT, summarization, cross-\nlingual question answering, and text classification were built on the corpora to enable empirical\nanalysis for related future studies.\nYFACC: YFACC [35] connected speech with visual representations in the dataset compris-\ning spoken captions in Yor`ub´a language for 6, 000 Flickr15 images. The audio captions also\nutilised single-speaker recording to ensure consistency in the audio data. Moreover, it includes\ncross-lingual pairing, whereby images are tagged with English visual labels and paired with\nYor`ub´a speech, permitting cross-lingual applications. The empirical analysis is also catered\nfor by developing a baseline cross-lingual model. Ultimately, this dataset addressed the dearth\nof visually-grounded speech datasets in low-resource languages, essentially focusing on Yor`ub´a\nlanguage. Furthermore, it provides a new benchmark for visually-grounded speech models in\nlow-resource language settings.\nYor`ub´a Speech Corpus: An open-source speech dataset exclusively for Yor`ub´a developed\nin [36]. This corpus comprises over 4 hours of 48kHz recordings from 36 male and female\nvolunteers. It also includes transcriptions with disfluency annotations and full diacritization,\nessential for pronunciation and lexical disambiguation. Moreover, the dataset was tested in\na statistical parametric speech synthesis (SPSS) for evaluation purposes and compared with\nrelated corpora in the same domain. The corpus will support TTS, ASR, and speech-to-speech\ntranslation, contributing to West African corpus linguistics.\n14https://marytts.github.io/\n15https://www.flickr.com\n25\n\nYor`ub´a Treebank (YTB): YTB [41] boasts of the first-tree bank in Yor`ub´a language. It\ncontains manually annotated Yor`ub´a Bible sections and is relevant for investigating dependency\nanalysis in the language.\nYOR `ULECT: YOR`ULECT [34] introduces a high-quality parallel text and speech corpus\nspecifically in Yor`ub´a language, across three domains—machine translation, automatic speech\nrecognition, and speech-to-text translation—and four Yor`ub´a dialects— Standard Yor`ub´a, If`e.,\n`Ij`e.b´u, and `Il`aje.. The If`e. dialect is spoken primarily among the people of `O. s.un state, `Ij`e.b´u\namong people of `O. g`un state, and `Il`aje., among the people of O`nd´o state—all states located\nin the South West geopolitical zone of Nigeria. Text data from various sources were obtained\nand localised into the three Yor`ub´a dialects for the corpus development. Similarly, high-quality\nspeech data in standard Yor`ub´a, If`e., and `Il`aje. were recorded. The text portion contains about\n1506 parallel sentences for each dialect, totalling 6024, and the speech part contains ≈3 hours\nof audio for the trio of standard Yor`ub´a, If`e., and `Il`aje.. Furthermore, to aid empirical analysis,\nexperiments involving the domain of text MT, ASR and speech-to Yor`ub´a were carried out, and\nthe dataset and models were published under an open licence, making it relevant for Yor`ub´a\nNLP tasks.\nTable 7 is used to summarize the available language resources relevant for NLP development\ninvolving Yor`ub´a language.\nTable 7: Summary of Resources Available for Yor`ub´a NLP Development\nResource\nType\nUse Case\nLanguage\nFrequency\nStudy\nUsed\nAFRIQA\nAnnotated corpus\nCross-lingual\nopen-\nretrieval\nquestion\nanswering\nMultilingual [18]\nAfriSenti\nAnnotated corpus\nSentiment analysis\nMultilingual [4]\nAfriWOZ\nDialogue dataset\nDialogue generation\nMultilingual [122]\nAntenatal orien-\ntation\nSpeech\ncorpus,\ntext\ncorpus\nSpeech\nrecognition,\nmachine translation\nMultilingual [100]\nBibleTTS\nSpeech corpus\nText-to-speech\nMultilingual [98]\nCIRAL\nAnnotated corpus\nCross-lingual informa-\ntion retrieval\nMultilingual [111]\nIkiniYor`ub´a\nParallel corpus\nMachine translation\nBilingual\n[37]\n`Ir`oy`ınSpeech\nSpeech corpus\nAutomatic\nspeech\nrecognition,\ntext-to-\nspeech\nMonolingual [101]\nLagos\nNWU\nYor`ub´a\nSpeech\nCorpus\nSpeech corpus\nAutomatic\nspeech\nrecognition\nMonolingual [16],\n[53],\n[52], [31]\nMasakhaNER\nAnnotated corpus\nNamed entity recogni-\ntion\nMultilingual [12]\nMasakhaPOS\nPOS dataset\nPart-of-speech tagging\nMultilingual [17]\nContinued on next page...\n26\n\nResource\nType\nUse Case\nLanguage\nFrequency\nStudy\nUsed\nMIRACL\nAnnotated corpus\nMonolingual informa-\ntion retrieval\nMultilingual [109], [108]\nMENYO-20k\nParallel corpus\nMachine translation\nBilingual\n[72],\n[28],\n[74]\nNaijaSenti\nAnnotated corpus\nSentiment analysis\nMultilingual [6]\nNollySenti\nParallel corpus\nSentiment\nclassi-\nfication,\nmachine\ntranslation\nMultilingual [5]\nProsodic\nRead\nSpeech\nSpeech corpus\nText-to-speech\nMonolingual [104]\nTATA\nParallel corpus\nData-to-text\ngener-\nation,\nmultilingual\ngeneration\nMultilingual [119]\nTTS\nYor`ub´a\nSpeech Corpus\nSpeech corpus\nText-to-speech\nMonolingual [99]\nWebcrawl\nAfrican\nParallel corpora\nMachine translation\nMultilingual [112]\nW´UR`A\nParallel corpus\nCross-lingual question\nanswering; text clas-\nsification; summariza-\ntion; & machine trans-\nlation\nMultilingual [76]\nYFACC\nSpeech-image dataset\nCross-lingual keyword\nlocalisation\nMonolingual [35]\nYor`ub´a\nSpeech\nCorpus\nSpeech corpus\nText-to-speech; auto-\nmatic speech recogni-\ntion; speech-to-speech\ntranslation\nMonolingual [36,\n91],\n[105], [107]\nYor`ub´a\nTree-\nbank\nDependency treebank\nDependency\nparsing\nand analysis\nMonolingual [41]\nYOR`ULECT\nParallel\ntext\nand\nspeech corpus\nMachine\ntranslation;\nautomatic\nspeech\nrecognition; & speech-\nto-text translation\nMonolingual [34]\n4.2.4\nRQ4:\nWhat are the major challenges in developing NLP solutions for\nYor`ub´a?\nNatural language solution development for Yor`ub´a faces multiple challenges limiting its growth\nas a low-resource language. Data synthesis in this study has helped identify five primary chal-\nlenges, including linguistic, technical, resource, cultural and societal factors, and evaluation\nand benchmarking challenges.\nLinguistic challenges: Yor`ub´a language exhibits several linguistic properties that make NLP\n27\n\ndevelopment in it an exigent task. These linguistic features of the language often complicate\nits NLP development, and they mainly account for its tone dependency, morphology, diacritic\ndemand, and code-switching demerits, which are summarised briefly below:\n• Tonal complexity: Yor`ub´a language implicit pitch contours, denoting tonal compo-\nnents, in communicating emphasis and other para-linguistic expressions.\nThus, they\nusually determine the semantic property of the message conveyed in the language. For\ninstance, `Og´un (a deity associated with iron) is different from either of ogun (war) and\nog´un (twenty). This distinction in linguistic meaning complicates various NLP tasks even\nthough the words belong to the same phonetic sequence. For instance, one of the main\nchallenges for speech recognition in Yor`ub´a is determining the tone associated with a sylla-\nble [91]. Similarly, utilising context-dependent phone units to capture long-term spectrum\ndependencies of tone in Yor`ub´a is typically less successful and oftentimes requires a dif-\nferent means of acoustic modelling [93]. Furthermore, in bilingual English-Yor`ub´a MT\ntasks, tone-changing verbs [66] also present challenges as they frequently alter the seman-\ntic properties of the sentence they are used in by shifting from a low-tone to a mid-tone.\nConsequently, accurate representation and processing of tones are essential for NLP tasks\nlike diacritic restoration, TTS synthesis, and MT.\n• Diacritic dependency: Similar to the tonal feature of Yor`ub´a language, it also requires\nappropriate assignment of accent property to characters in a segment. This task is usu-\nally referred to as diacritic restoration, and it’s essential to fully decipher the linguistic\nmeaning of words. Diacritic marks are usually placed above, below, or between characters\nto indicate pronunciation and may change the meaning of the composed words [53]. Like\nmost languages involving diacritic, Yor`ub´a language users often omit them in electronic\ntexts, increasing lexical ambiguity and also posing challenges to NLP systems as a result\nof mislaying the accompanying syntactic, grammatical, or semantic information, partially\nor totally [51]. This omission is sometimes due to the unavailability of supporting appli-\ncations and devices [31] or lack of knowledge by most users [90], resulting in a drawback\ntowards improving NLP representation in the language.\n• Morphological Complexity: Carrying out analysis of word formation and structure\nin Yor`ub´a language tends to be oftentimes complex due to its plenitude in terms of rules\ninvolving noun and verb inflection patterns [58]. Handling agglutinative morphology and\nword compounding has also been stated to require advanced morphological analysis tools\n[59]. The deficiency in this regard frequently creates ambiguity in unsophisticated NLP\nsystems due to issues like affixes of words and the required rules to correctly programme\nthe system for effective analysis. Efforts towards addressing this have incorporated au-\ntomatic morphological induction [57] to ensure compatibility, producing a more accurate\nrepresentation in the NLP domain.\n• Code-Switching: This is common for most Nigerian languages, and Yor`ub´a is not spared\nof the growing ‘civilization’. Despite initiatives towards sustaining native languages, it\nis evident that the younger generation finds it difficult to carry on lengthy conversations\nor even to form lengthy sentences without interspersing the conversation with terms fre-\nquently borrowed from the English language [30]. This phenomenon introduces additional\ncomplexity in NLP tasks like language identification, sentiment analysis, and machine\ntranslation, as they reduce data availability purely in the language. This is evident in\nthe dataset obtained for NaijaSenti [6], which aimed to conduct monolingual sentiment\n28\n\nanalysis for four languages through a curated corpus from X, as a certain proportion of\nthe corpus had to cater for the phenomenon.\nResource challenges: The challenges in obtaining substantial or quality resources for training\nNLP models involving Yor`ub´a language is the ultimate pitfall for their NLP development, just\nlike in many other low-resource languages [76, 72]. The existence of a comprehensive lexicon for\nmost high-resource languages makes their pre-processing task less taxing, unlike a low-resource\nlanguage like Yor`ub´a. The unavailability often leads to applying manual data cleaning, hence\nthe task of token validation for corpus development [125].\nIn addition, limited corpora pose significant challenges in various NLP tasks. For instance,\na comparatively short corpus of speech recordings and minimal language-specific development\ncan automatically build acoustic models for an elementary speech synthesiser in a new lan-\nguage; however, tonal languages like Yor`ub´a require additional resources [102] which is usually\nnot readily available. Also, word-level and character-level models are especially common for\nmost diacritic restoration tasks [53]. However, they require significant training data to prevent\nsparsity in the models. Similarly, out-of-vocabulary word translation is a significant issue for\nlow-resource languages that lack parallel training data [70]. This phenomenon is considered a\nstumbling block to NLP development in these different domains, albeit recent efforts have been\ndirected towards implementing other viable methods requiring lower resources for a handful of\ntasks supporting such.\nGenerally, limited available corpora for developing NLP tools have been a consistent challenge\nin NLP development involving Yor`ub´a and under low-resource languages. Studies have also\nshown the presence of quality issues for existing corpora and models [76], which limits the\nreliability of findings from using such corpus. Consequently, efforts towards creating readily\navailable corpora for specific NLP tasks are sacrosanct since better results mostly require high-\nquality and large data [51].\nTechnological challenges: Limited availability of pre-processing tools is also a phenomenon\nconstituting a major setback for most low-resource African languages, as the tool suitable for a\nlanguage tends not to suit another, such as Yor`ub´a, owing to its morphological complexity and\ndiacritic dependency. For instance, English word punctuation segmentation will not necessar-\nily work for Arabizi, the Arabic chat language, since these marks define its own orthography\n[126]. Furthermore, research on pre-processing tools for different languages within the same\nfamily collectively suggests that while some can work across related languages, optimal results\noften require language-specific adjustments and careful consideration of the target domain and\nlinguistic features [127].\nOther language tools, including the syntactic parser and POS tagger, have also been reported\nto mostly fail in accounting for Yor`ub´a language linguistic nuances [15], thereby constituting a\nchallenge requiring the development of language-specific tools for these specific tasks. Similarly,\nan essential NLP task such as effective spell-checking development in Yor`ub´a is still at the early\ndevelopment [56] due to non-consideration of diacritic necessity in earlier studies [55], resulting\nin their limitation. A limited number of studies [17, 54, 56] involving computational NLP have\nbeen seen to include Yor`ub´a language in these domains. Furthermore, the majority of Yor`ub´a\ntexts are also seen to be typed using the plain ASCII Character Set, without diacritics [31],\nperhaps due to limited tools supporting their full implementation. This situation also induces\n29\n\nlower quality translation between Yor`ub´a language and most European languages due to lack\nof adequate elision resolution tool [30]. Thus, there is a growing need for flexible, multilingual\npre-processing solutions.\nCultural and Societal Factors: An African language, such as Yor`ub´a, is not only a mirror\ninto the mind of its users but also a mirror into their culture and history [38]. This emphasises\nthe richness of the culture and language. However, younger generations of Yor`ub´a speakers\nincreasingly favour English for formal and digital communication, while the parents also decide\nnot to teach their infants many times [90]. This shift reduces the volume of contemporary\nYor`ub´a texts and contributes to the decline of the language in digital contexts. Moreover,\ndespite the evident progress in MT tasks for low-resource languages, NMT models still lag\nin accurately carrying out automatic translation involving cultures [72] for Yor`ub´a language.\nConsequently, cultural and societal challenges of Yor`ub´a in NLP development require consid-\nerable attention, as they could potentially endanger the language in the face of westernization,\nglobalization, and inter-ethnic communication [38].\nTable 8 summarizes the challenges of NLP development involving Yor`ub´a language and some\nof the highlighted solutions.\nTable 8: Summary of Challenges in Yor`ub´a NLP Development\nChallenge category\nSpecific challenge\nPrimary studies\nProposed solutions\nLinguistic\nTonal\ncomplexity,\ndiacritic\ndepen-\ndency,\nmorphological\ncomplexity,\n& code-\nswitching\n[66], [91], [102], [93],\n[31], [51], [52], [53]\nDiacritic-aware\nmod-\nels;\nAutomatic mor-\nphological induction\nTechnological\nLimited\npre-trained\nmodels\nand\nlack\nof\nlanguage-specific tools\n[77], [29], [30]\nFine-tuning\nmultilin-\ngual models; Develop-\ning elision resolution\ntool\nResource scarcity\nLimited\nannotated\ncorpora\nfor\nspecific\ntasks\n[11],[76], [122], [30]\nCollaborative\ncorpus\ndevelopment\nacross\nvarious\nNLP\ntasks;\nMultilingual\ncorpus\ndevelopment\nSocio-cultural\nAdopting foreign lan-\nguage as first language\n[90], [38]\nImplementing\nitera-\ntive learning systems\nfor Yor`ub´a language\n30\n\n5\nDiscussion\nThis systematic review was intended to capture the growth and current stage of NLP participa-\ntion for Yor`ub´a language over a decade. Through the information synthesised from the primary\nstudies, it is obvious that significant progress has been made in notable areas of NLP involving\nYor`ub´a language. MT has particularly received more attention through the inclusion of the\nlanguage primarily in bilingual MT research [64, 68, 74, 69, 28, 67, 50] involving translation\nbetween two languages. Related studies investigating the inherent prerequisite towards enhanc-\ning translation qualities in MT model qualities, the development of MT tools, such as vowel\nelision resolution [30], and the application of rough set theory [29] in translation have been\nexplored. Moreover, the need to create better quality data [76] has also led towards improved\nresearch in this domain through the development of additional parallel corpora and leveraging\nmultilingual language pre-training to improve the model training capability in the presence of\nlarge high-quality datasets.\nSimilar to MT research, significant efforts have been made in TTS synthesis, with considerable\nefforts towards corpus development, as evidenced by publicly available datasets, most of which\nare speech corpora. Even though it is the primary focus in most research investigating the task,\nsome of the studies have included ASR and speech-to-text [101, 97, 34] in their experiments\ncontributing to the entire research. Furthermore, sentiment analysis and information retrieval\ntasks involving Yor`ub´a have also benefited significantly from multilingual corpus development\nthrough the need to develop language resources for low-resource African languages. Most have\nbeen through the works in [4] and [111].\nGenerally, NLP tasks such as sentiment analysis, machine translation, text-to-speech synthe-\nsis, automatic speech recognition, named entity recognition, information retrieval, parts-of-\nspeech tagging, and language modelling have received considerable research efforts, with at\nleast four studies each addressing them. In addition, innovative approaches, transfer learning,\nand diacritic-aware systems have demonstrated promising results, showcasing the adaptability\nof state-of-the-art techniques to Yor`ub´a NLP, albeit earlier studies have depended mostly on\nthe rule-based methods. Resources like Yor`ub´a speech corpus [36], MENYO-20k [72], Lagos\nNWU Yor`ub´a Speech Corpus [16] among others, have also been pivotal in advancing the field\nby providing foundational datasets, as they have been used in more than one study.\nFurthermore, recent studies have showcased the importance of multilingual and cross-lingual\ntechniques, as they help to promote language availability for many African languages simulta-\nneously. This is evident from named entity recognition [12, 14], sentiment analysis [6, 4, 5], and\ninformation retrieval tasks [18, 111, 109]. Also, models pre-trained on multilingual datasets\nhave been seen to be beneficial for a low-resource language like Yor`ub´a. Ultimately, it is also\nevident that collaborative, open-source and community initiatives have greatly improved re-\nsearch and availability of resources in the bid to overcome resource scarcity whilst fostering\nknowledge-sharing among these resource-scarce African languages.\nHowever, in the context of constraints stunting the rapid growth of NLP in Yor`ub´a language,\nstudies have highlighted mainly linguistic complexity such as diacritic dependency [31] and tonal\nvariation [66]. Other challenges involving cultural and societal factors were also highlighted,\nincluding the primary hindrance, which has always been a limitation in available corpora or\n31\n\nthe quality of such available corpora.\n5.1\nLimitation of study\nWhile this systematic literature review investigates the progress and status of NLP involving\nYor`ub´a, it is noteworthy to mention that “Yor`ub´a” in this case is not specific to a certain\ndialect of the language, such as Yor`ub´a of If`e., `Ij`e.b´u, or `Il`aje.. The study recognised Yor`ub´a\nlanguage as one encompassing several dialects across different countries. Even though certain\nsections of it specifically highlighted NLP research and resources involving Yor`ub´a dialects [34],\nthis study might not be sufficient when dialects of the language are the sole focus in the NLP\nresearch.\nFurthermore, the last date for retrieving information for the study was October 2024. Hence,\npublications emerging afterwards would not have been included in the synthesis. Similarly,\nonly peer-reviewed studies were included to ensure high quality and reliability in findings. This\nmight limit a recently published relevant study undergoing a peer-review process during the\nperiod the databases were searched. Nevertheless, the systematic review ensures a substantial\nrepresentation of information from various primary studies by considering a decade of publica-\ntion years.\n6\nConclusion and Future Directions\nThis section summarises the study and process involved and aims to inform readers of possible\nresearch areas towards improving Yor`ub´a language involvement in NLP research.\n6.1\nConclusion\nThis SLR explores NLP progress involving Yor`ub´a language.\nIt involves surveying studies\nbetween 2014 and 2024, which have used Yor`ub´a language in their NLP research, and those\nwith great emphasis on the language, in case of a multilingual setting. Moreover, data were\nsynthesized from these primary studies to deduce findings, thereby procuring answers to the\nestablished four research questions, which form the core objectives of the research.\nThe research questions explore the tasks, techniques, language resources, and challenges in-\nvolved in Yor`ub´a NLP over a decade.\nEstablished protocols and guidelines were followed\nsystematically to ensure maximum formal inundation, eliminating possible bias. Moreover, the\ninformation synthesised from the 105 primary studies has been carefully reported, encapsulat-\ning the relevant findings from the systematic review.\nUltimately, with this study, language researchers will be abreast of the current progress in\nYor`ub´a NLP, thereby equipping them with the necessary ideas to preserve the language through\nNLP tool representation—this is crucial as it is a widely spoken language with abundant cultural\nrichness and linguistic features. Similarly, it is required to guide future researchers plying this\nroute to eliminate or limit possible odyssey in their research journey.\n32\n\n6.2\nFuture Directions\nEven though the findings show promising results for NLP research involving Yor`ub´a language,\nit is pertinent to outline the current absence of significant efforts in some domains, which\nare equally important towards improving NLP in the language. For instance, research efforts\ninvolving the identification of abusive, offensive, hate speech or cyberbullying, which are all\nregarded as harmful language usage, have not been explored. This could be due to a lack of or\nlimited annotated corpus in this domain. Such research tasks are essential for ameliorating the\nusage rate of sensitive words, phrases, and sentences on social media, as they could potentially\nendanger other users. Consequently, future research efforts can be directed toward building rel-\nevant corpora to address these NLP challenges and creating relevant benchmarks to facilitate\nempirical analysis and continuous research.\nSarcasm detection is another fascinating realm that remains untapped yet for Yor`ub´a and most\nunder-resource languages. Sarcasm is used to mask the true emotion in a state, mostly by\nexuding positivity or a seeming positive demeanour. This phenomenon of saying what is not\nmeant or meaning what is not said by natural language users poses challenges for NLP. How-\never, it is essential to accurately detect users’ real emotions for different purposes, including\nproduct reviews, feedback analysis, politics & governance, among others. Consequently, devel-\noping relevant sarcasm corpora and benchmarks that will potentially birth advanced sarcasm\ndetection models is crucial. Future research can be focused on this domain to promote certainty\nin natural language usage.\nMoreover, general resource development tasks are essential to limit the paucity of Yor`ub´a lan-\nguage resources. This could be achieved through continuous collaboration, open-source, and\ncommunity initiatives, such as the one carried out through Maskhane16. Moreover, more pri-\nority can be given to developing Yor`ub´a-specific pre-trained models and fine-tuning existing\nmultilingual models for better performance in under-resourced settings.\nFurthermore, more research should be directed towards solving linguistic challenges such as\ntonal variations, morphology complexities, and diacritic restoration, as these are essential for\ndecoding the nuances in the language. Developing pre-processing tools and models that cater\nto and integrate linguistic knowledge or incorporate phonological features in the language could\nsignificantly improve performance.\nFinally, addressing the drawbacks associated with cultural and social challenges will require\ndeveloping context-aware models that can adapt to real-world changes. Continuous language\nusage would also benefit from exploring new cases, such as LLM conversational agents, and\ndeveloping healthcare and educational tools incorporating NLP. These innovations can aid in\nbridging the digital gaps among the Yor`ub´a-speaking communities.\nDeclaration of Conflict of Interest\nThe authors attest that no conflict of interest is involved in the publication.\n16https://www.masakhane.io/\n33\n\nAcknowledgements\nThis publication emanated from research conducted with the financial support of Science Foun-\ndation Ireland under Grant number 18/CRT/6049. For Open Access, the author has applied a\nCC BY public copyright licence to any Author Accepted Manuscript version arising from this\nsubmission. The authors also show gratitude to the reviewers for their painstaking efforts.\nData Availability\nNo data was used for this research task. However, datasheets that record the different stages\nin the SLR can be found in the GitHub repository17 for reference.\nReferences\n[1] Jalaj Thanaki. Python natural language processing. Packt Publishing Ltd, 2017.\n[2] Hima Yeldo. Natural language processing: Components, advances, tools and industrial\napplications.\nInternational Journal for Research in Applied Science and Engineering\nTechnology, 2021.\n[3] Dipanjan Sarkar. Text Analytics with Python: A Practitioner’s Guide to Natural Lan-\nguage Processing. APress, 2nd edition, 2019.\n[4] Shamsuddeen Muhammad, Idris Abdulmumin, Abinew Ayele, Nedjma Ousidhoum,\nDavid Adelani, Seid Yimam, Ibrahim Ahmad, Meriem Beloucif, Saif Mohammad, Se-\nbastian Ruder, et al. Afrisenti: A twitter sentiment analysis benchmark for african lan-\nguages. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language\nProcessing, pages 13968–13981, 2023.\n[5] Iyanuoluwa Shode, David Ifeoluwa Adelani, Jing Peng, and Anna Feldman. Nollysenti:\nLeveraging transfer learning and machine translation for nigerian movie sentiment classi-\nfication. In Proceedings of the 61st Annual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 986–998, 2023.\n[6] Shamsuddeen\nHassan\nMuhammad,\nDavid\nIfeoluwa\nAdelani,\nSebastian\nRuder,\nIbrahim Said Ahmad, Idris Abdulmumin, Bello Shehu-Bello, Monojit Choudhury,\nChris Chinenye Emezue, Saheed Salahudeen Abdullahi, Anuoluwapo Aremu, et al. Nai-\njasenti: A nigerian twitter sentiment corpus for multilingual sentiment analysis. In Pro-\nceedings of the Thirteenth Language Resources and Evaluation Conference, pages 590–602,\n2022.\n[7] Ife Adebara, Muhammad Abdul-Mageed, and Miikka Silfverberg.\nLinguistically-\nmotivated yor`ub´a-english machine translation. In Proceedings of the 29th International\nConference on Computational Linguistics, pages 5066–5075, 2022.\n[8] Ignatius Ikechukwu Ayogu, Adebay Olusla Adetunmbi, and Bolanle Adefowoke Ojokoh.\nDeveloping statistical machine translation system for english and nigerian languages.\nAsian Journal of Research in Computer Science, 2018.\n17https://github.com/toheebadura/SLR\n34\n\n[9] Safiriyu I Eludiora and Odetunji A Odejobi. Development of an english to yor`ub´a machine\ntranslator. International Journal of Modern Education and Computer Science, 8(11):8,\n2016.\n[10] Orevaoghene Ahia, Julia Kreutzer, and Sara Hooker.\nThe low-resource double bind:\nAn empirical study of pruning for low-resource machine translation. In Findings of the\nAssociation for Computational Linguistics: EMNLP 2021, pages 3316–3333, 2021.\n[11] Bosede A. Ayogu Ikechukwu I. Ayogu, Adebayo O. Adetunmbi. A first step towards\nthe development of yoruba named entity recognition system. International Journal of\nComputer Applications, 182(41):1–4, Feb 2019.\n[12] David Ifeoluwa Adelani, Jade Abbott, Graham Neubig, Daniel Dsouza, Julia Kreutzer,\nConstantine Lignos, Chester Palen-Michel, Happy Buzaaba, Shruti Rijhwani, Sebastian\nRuder, et al. Masakhaner: Named entity recognition for african languages. Transactions\nof the Association for Computational Linguistics, 9:1116–1131, 2021.\n[13] Hailemariam Mehari Yohannes, Steven Lynden, Toshiyuki Amagasa, and Akiyoshi\nMatono. Semi-supervised named entity recognition for low-resource languages using dual\nplms. In International Conference on Applications of Natural Language to Information\nSystems, pages 166–180. Springer, 2024.\n[14] David Adelani, Graham Neubig, Sebastian Ruder, Shruti Rijhwani, Michael Beukman,\nChester Palen-Michel, Constantine Lignos, Jesujoba Alabi, Shamsuddeen Muhammad,\nPeter Nabende, et al. Masakhaner 2.0: Africa-centric transfer learning for named entity\nrecognition.\nIn Proceedings of the 2022 Conference on Empirical Methods in Natural\nLanguage Processing, pages 4488–4508, 2022.\n[15] Abiola O. B, Bankole O. H, Adeyemo O. A, Adewole L. B, Ogundipe A. T, Saka-\nBalogun O. Y, Obamiyi S. E, Okebule Toyin, and Christianah O. Akinduyite. A hidden\nmarkov model-based parts-of-speech tagger for yoruba language. In 2024 International\nConference on Science, Engineering and Business for Driving Sustainable Development\nGoals (SEB4SDG), pages 1–6, 2024.\n[16] Ayokunle Oluwatoyin Enikuomehin and Ahmad Adib Tijani. Implementation of hidden\nmarkov model on lagos nigeria women union (nwu) yoruba speech corpus. The Pacific\nJournal of Science and Technology, 2020.\n[17] Cheikh M Bamba Dione, David Ifeoluwa Adelani, Peter Nabende, Jesujoba Alabi,\nThapelo Sindane, Happy Buzaaba, Shamsuddeen Hassan Muhammad, Chris Chinenye\nEmezue, Perez Ogayo, Anuoluwapo Aremu, et al. Masakhapos: Part-of-speech tagging\nfor typologically diverse african languages. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers), pages 10883–\n10900, 2023.\n[18] Odunayo Ogundepo, Tajuddeen Gwadabe, Clara Rivera, Jonathan Clark, Sebastian\nRuder, David Adelani, Bonaventure Dossou, Abdou Diop, Claytone Sikasote, Gilles\nHacheme, Happy Buzaaba, Ignatius Ezeani, Rooweither Mabuya, Salomey Osei, Chris\nEmezue, Albert Kahira, Shamsuddeen Muhammad, Akintunde Oladipo, Abraham\nOwodunni, Atnafu Tonja, Iyanuoluwa Shode, Akari Asai, Anuoluwapo Aremu, Ayodele\n35\n\nAwokoya, Bernard Opoku, Chiamaka Chukwuneke, Christine Mwase, Clemencia Siro,\nStephen Arthur, Tunde Ajayi, Verrah Otiende, Andre Rubungo, Boyd Sinkala, Daniel\nAjisafe, Emeka Onwuegbuzia, Falalu Lawan, Ibrahim Ahmad, Jesujoba Alabi, Chinedu\nMbonu, Mofetoluwa Adeyemi, Mofya Phiri, Orevaoghene Ahia, Ruqayya Iro, and So-\nnia Adhiambo. Cross-lingual open-retrieval question answering for African languages.\nIn Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association\nfor Computational Linguistics: EMNLP 2023, pages 14957–14972, Singapore, December\n2023. Association for Computational Linguistics.\n[19] Prashant Johri, Mukul Kathait, Munish Sabharwal, Ahmad T. Al-Taani, and Shakhzod\nSuvanov. Natural language processing: History, evolution, application, and future work.\nLecture Notes in Networks and Systems, 2020.\n[20] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–\n444, 2015.\n[21] Sharavana K, Kedarnath Bhakta, Jayanth Sai Chethan S, Jayant Chand, and Meet Joshi\nK. Evolution of natural language processing: A review. Journal of Knowledge in Data\nScience and Information Management, 2024.\n[22] Subham Tripathi, Shaina Anjum, and Manmohan. Natural language processing in the\nera of deep learning: A survey of applications and advances. International Journal of\nApplied Research, 2018.\n[23] Chinmay Shripad Kulkarni. The evolution of large language models in natural language\nunderstanding. Journal of Artificial Intelligence, Machine Learning and Data Science,\n2023.\n[24] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,\nTimoth´ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien\nRodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and\nefficient foundation language models. ArXiv, abs/2302.13971, 2023.\n[25] Albert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Deven-\ndra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume\nLample, Lucile Saulnier, L’elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock,\nTeven Le Scao, Thibaut Lavril, Thomas Wang, Timoth´ee Lacroix, and William El Sayed.\nMistral 7b. ArXiv, abs/2310.06825, 2023.\n[26] Rajvardhan Patil and Venkat Rao Gudivada. A review of current trends, techniques, and\nchallenges in large language models (llms). Applied Sciences, 2024.\n[27] Anil Kumar Singh. Natural language processing for less privileged languages: Where do\nwe come from? where are we going? In Proceedings of the IJCNLP-08 Workshop on NLP\nfor Less Privileged Languages, 2008.\n[28] Olawale Adeboje, Adetunmbi Adebayo, Arome Gabriel, and Akinyede Olufemi. Bilingual\nneural machine translation from english to yoruba using a transformer model. Interna-\ntional Journal of Innovative Science and Research Technology (IJISRT), pages 826–833,\n07 2024.\n36\n\n[29] Olutola\nOlaide\nFagbolu,\nBabatunde\nSunday\nObalalu,\nSamuel\nS\nUdoh,\nand\nIbadan Abeokuta Uyo. Applying rough set theory to yorub´a language translation. In\nInternational Conference on Advanced Trends in ICT and Management (ICAITM) 28th,\n2016.\n[30] Lawrence B Adewole, Adebayo O Adetunmbi, Boniface K Alese, Samuel A Oluwadare,\nOluwatoyin B Abiola, and Olaiya Folorunsho.\nAutomatic vowel elision resolution in\nyor`ub´a language. In Conference of the South African Institute of Computer Scientists\nand Information Technologists 2020, pages 126–133, 2020.\n[31] Iroro Orife. Attentive sequence-to-sequence learning for diacritic restoration of yor`ub´a\nlanguage text. In Interspeech, pages 2848–2852, 09 2018.\n[32] Safiriyu I Eludiora, Abayomi O Agbeyangi, and DI Ojediran. Word sense disambiguation\nin english to yor`ub´a machine translation system. Journal of Multidisciplinary Engineering\nScience and Technology, Berlin, Germany, 2(7):1814–1819, 2015.\n[33] Ikechukwu I. Ayogu, Adebayo O. Adetunmbi, Bolanle A. Ojokoh, and Samuel A.\nOluwadare. A comparative study of hidden markov model and conditional random fields\non a yorb part-of-speech tagging task. In 2017 International Conference on Computing\nNetworking and Informatics (ICCNI), pages 1–6. IEEE, 2017.\n[34] Orevaoghene Ahia, Anuoluwapo Aremu, Diana Abagyan, Hila Gonen, David Adelani,\nDaud Abolade, Noah A Smith, and Yulia Tsvetkov. Voices unheard: Nlp resources and\nmodels for yor`ub´a regional dialects. In Proceedings of the 2024 Conference on Empirical\nMethods in Natural Language Processing, pages 4392–4409, 2024.\n[35] Kayode Olaleye, Dan Oneat¸˘a, and Herman Kamper.\nYfacc: A yor`ub´a speech–image\ndataset for cross-lingual keyword localisation through visual grounding. In 2022 IEEE\nSpoken Language Technology Workshop (SLT), pages 731–738. IEEE, 2023.\n[36] Alexander Gutkin, Isin Demirsahin, Oddur Kjartansson, Clara E Rivera, and K´ol´a\nT´ub`os´un. Developing an open-source corpus of yoruba speech. In Interspeech, 2020.\n[37] Idris Akinade, Jesujoba Alabi, David Adelani, Clement Odoje, and Dietrich Klakow.\nVarepsilon k´u mask: Integrating yor`ub´a cultural greetings into machine translation. In\nProceedings of the First Workshop on Cross-Cultural Considerations in NLP (C3NLP),\npages 1–7, 2023.\n[38] Theresa Okediya, Ibukun Afolabi, Olamma Iheanetu, and Sunday Ojo. Building ontology\nfor yor`ub´a language. In Proceedings of the First International Workshop on NLP Solu-\ntions for Under Resourced Languages (NSURL 2019) co-located with ICNLSP 2019-Short\nPapers, pages 124–130, 2019.\n[39] Gary Simons and M Paul Lewis. Making egids assessments for the ethnologue, 2010.\n[40] Toluwase Victor Asubiaro and Ebelechukwu Gloria Igwe. A state-of-the-art review of\nnigerian languages natural language processing research. In Developing Countries and\nTechnology Inclusion in the 21st Century Information Society, 2021.\n[41] Ol´aj´ıd´e Ishola and Daniel Zeman. Yorub´a dependency treebank (ytb). In Proceedings of\nthe Twelfth Language Resources and Evaluation Conference, pages 5178–5186, 2020.\n37\n\n[42] John T. Bendor-Samuel. Niger-congo languages. Encyclopedia Britannica, March 2024.\nAccessed 24 September 2024.\n[43] Jeff Good. Niger-congo languages. Oxford Bibliographies Online Datasets, 2018.\n[44] Joaquim Mussandi and Andreas Wichert. Nlp tools for african languages: Overview.\nIn Proceedings of the 16th International Conference on Computational Processing of\nPortuguese-Vol. 2, pages 73–82, 2024.\n[45] John Merrill. Atlantic groups as primary branches of niger-congo. In A paper presented\nat Westermann Workshop in Humboldt Univer sity, Berlin, 2021.\n[46] Barbara Kitchenham and Stuart Charters. Guidelines for performing systematic literature\nreviews in software engineering, 2007.\n[47] Matthew J Page, Joanne E McKenzie, Patrick M Bossuyt, Isabelle Boutron, Tammy C\nHoffmann, Cynthia D Mulrow, Larissa Shamseer, Jennifer M Tetzlaff, Elie A Akl, Sue E\nBrennan, et al. The prisma 2020 statement: an updated guideline for reporting systematic\nreviews. bmj, 372, 2021.\n[48] Claes Wohlin. Guidelines for snowballing in systematic literature studies and a replication\nin software engineering. In Proceedings of the 18th international conference on evaluation\nand assessment in software engineering, pages 1–10, 2014.\n[49] Ol´ugb´enga O Akinad´e and dt´unj´ı A djb´ı. Computational modelling of yor`ub´a numerals in\na number-to-text conversion system. Journal of Language Modelling, 2(1):167–211, 2014.\n[50] Isaac Elesemoyo and Odtnj Odjob. Machine translation system for numeral in english text\nto yorb language. Ife Journal of Information and Communication Technology, 6:26–37,\n12 2022.\n[51] Ikechukwu Ignatius Ayogu and Onoja Abu.\nAutomatic diacritic recovery with focus\non the quality of the training corpus for resource-scarce languages. In 2020 IEEE 2nd\nInternational Conference on Cyberspac (CYBER NIGERIA), pages 98–103. IEEE, 2021.\n[52] Sawsan Alqahtani, Ajay Mishra, and Mona Diab. Efficient convolutional neural networks\nfor diacritic restoration. In Proceedings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP), pages 1442–1448, 2019.\n[53] Sawsan Alqahtani and Mona Diab.\nInvestigating input and output units in diacritic\nrestoration. In 2019 18th IEEE International Conference on Machine Learning and Ap-\nplications (ICMLA), pages 811–817. IEEE, 2019.\n[54] Erinfolami Oluwaseyi, Oguntimilehin Abiodun, Bukola Badeji-Ajisafe, et al. Automatic\nspelling corrector for yor`ub´a language using edit distance and n-gram language mod-\nels. In 2024 International Conference on Science, Engineering and Business for Driving\nSustainable Development Goals (SEB4SDG), pages 1–6. IEEE, 2024.\n[55] Asahiah Franklin Oladiipo, Onifade Mary Taiwo, and Adegunlehin Abayomi Emmanuel.\nSpelling error patterns in typed yor`ub´a text documents. International Journal of Infor-\nmation Engineering and Electronic Business, 14(6):28, 2020.\n38\n\n[56] Franklin Ol´adi´ıp`o Asahiah, Mary Taiwo On´ıf´ad´e, Adekemisola Olufunmilayo Asahiah,\nAbayomi Emmanuel Adegunlehin, and Adekemi Olawunmi Amoo. Diacritic-aware yor`ub´a\nspell checker. Computer Science, 24, 2023.\n[57] Tunde Adegbola. Automatic detection of morphological processes in the yor`ub´a language.\nIn Proceedings of the 1st Annual Meeting of the ELRA/ISCA Special Interest Group on\nUnder-Resourced Languages, pages 146–154, 2022.\n[58] AO Agbeyangi, JO Odiete, TA Lawal, and AB Olorunlomerue. Morphological analysis\nof standard yoruba nouns. American Journal of Engineering Research, 5(6):8–12, 2016.\n[59] Tunde Adegbola. Pattern-based unsupervised induction of yoruba morphology. In Pro-\nceedings of the 25th International conference companion on World wide web, pages 599–\n604, 2016.\n[60] Franklin l´adi´ıp Asahiah. Comparison of rule-based and data-driven approaches for syl-\nlabification of simple syllable languages and the effect of orthography. Computer Speech\n& Language, 70:101233, 2021.\n[61] AE Akinwonmi. Rule-induced misanalysis of nasal syllables in yoruba declarative syllab-\nification algorithm. Journal of Sustainable Technology, 13(1), 2024.\n[62] Michael Beukman and Manuel Fokam. Analysing cross-lingual transfer in low-resourced\nafrican named entity recognition. In Proceedings of the 13th International Joint Confer-\nence on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter\nof the Association for Computational Linguistics (Volume 1: Long Papers), pages 199–\n224, 2023.\n[63] Cheikh M Bamba Dione. Multilingual dependency parsing for low-resource african lan-\nguages: Case studies on bambara, wolof, and yoruba. In Proceedings of the 17th Interna-\ntional Conference on Parsing Technologies and the IWPT 2021 Shared Task on Parsing\ninto Enhanced Universal Dependencies (IWPT 2021), pages 84–92, 2021.\n[64] I Eludiora Safiriyu, O Awoniyi Akindeji, and O Azeez Isau. Computational modelling\nof personal pronouns for english to yor`ub`a machine translation system. In 2015 SAI\nIntelligent Systems Conference (IntelliSys), pages 733–741. IEEE, 2015.\n[65] A Adegoke-Elijah, K. Jimoh, and A. Alabi. Development of a xlm-encoded machine-\nreadable dictionary for yoruba word sense disambiguation. UNIOSUN Journal of Engi-\nneering and Environmental Sciences, 5(2), September 2023.\n[66] Safiriyu I., Abayomi O., and O.I. Fatusin. Development of english to yoruba machine\ntranslation system for yoruba verbs tone changing. International Journal of Computer\nApplications, 129(10):1217, November 2015.\n[67] Akinbowale Nathaniel Babatunde, Christiana Oluwakemi Abikoye, Abdulkarim Ayopo\nOloyede, Roseline Oluwaseun Ogundokun, Afeez Adeshina Oke, and Hafsat Omolola\nOlawuyi. English to yoruba short message service speech and text translator for android\nphones. International Journal of Speech Technology, 24(4):979–991, 2021.\n39\n\n[68] Akinbowale Nathaniel Babatunde, Ronke Seyi Babatunde, Bukola Fatimah Balogun, Em-\nmanuel Umar, Shuaib Babatunde Mohammed, Afeez Adeshina Oke, and Kolawole Yusuf\nObiwusi. Speech-to-text hybrid english to yoruba sms translator. Innovative Computing\nReview, 4(1):15–36, 2024.\n[69] Adebimpe Esan, John Oladosu, Christopher Oyeleye, Ibrahim Adeyanju, Olatayo\nOlaniyan, Nnamdi Okomba, Bolaji Omodunbi, and Opeyemi Adanigbo. Development\nof a recurrent neural network model for english to yorb machine translation. Interna-\ntional Journal of Advanced Computer Science and Applications, 11(5), 2020.\n[70] Angli Liu and Katrin Kirchhoff. Context models for oov word translation in low-resource\nlanguages. In Proceedings of the 13th Conference of the Association for Machine Trans-\nlation in the Americas (Volume 1: Research Track), pages 54–67, 2018.\n[71] Ife Adebara, AbdelRahim Elmadany, and Muhammad Abdul-Mageed. Cheetah: Nat-\nural language generation for 517 African languages. In Lun-Wei Ku, Andre Martins,\nand Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pages 12798–12823, Bangkok,\nThailand, August 2024. Association for Computational Linguistics.\n[72] David Adelani, Dana Ruiter, Jesujoba Alabi, Damilola Adebonojo, Adesina Ayeni, Mofe\nAdeyemi, Ayodele Esther Awokoya, and Cristina Espa˜na-Bonet. The effect of domain\nand diacritics in yoruba–english neural machine translation. In Proceedings of Machine\nTranslation Summit XVIII: Research Track, pages 61–75, 2021.\n[73] Olumide Adewale, Oluwatoyin Agbonifo, and Julius Olaniyan.\nDevelopment of bi-\ndirectional english to yoruba translator for real-time mobile chatting. International Jour-\nnal of Corpus Linguistics, 11:2020–2038, 09 2023.\n[74] Edoardo Signoroni and Pavel Rychl`y. Evaluating sentence alignment methods in a low-\nresource setting: An english-yor`ub´a study case. In Proceedings of the Sixth Workshop\non Technologies for Machine Translation of Low-Resource Languages (LoResMT 2023),\npages 123–129, 2023.\n[75] Bonaventure F. P. Dossou, Atnafu Lambebo Tonja, Oreen Yousuf, Salomey Osei, Abi-\ngail Oppong, Iyanuoluwa Shode, Oluwabusayo Olufunke Awoyomi, and Chris Emezue.\nAfroLM: A self-active learning-based multilingual pretrained language model for 23\nAfrican languages.\nIn Angela Fan, Iryna Gurevych, Yufang Hou, Zornitsa Kozareva,\nSasha Luccioni, Nafise Sadat Moosavi, Sujith Ravi, Gyuwan Kim, Roy Schwartz, and\nAndreas R¨uckl´e, editors, Proceedings of The Third Workshop on Simple and Efficient\nNatural Language Processing (SustaiNLP), pages 52–64, Abu Dhabi, United Arab Emi-\nrates (Hybrid), December 2022. Association for Computational Linguistics.\n[76] Akintunde Oladipo, Mofetoluwa Adeyemi, Orevaoghene Ahia, Abraham Owodunni,\nOdunayo Ogundepo, David Adelani, and Jimmy Lin. Better quality pre-training data\nand t5 models for african languages. In Proceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing, pages 158–168, 2023.\n40\n\n[77] Kelechi Ogueji, Yuxin Zhu, and Jimmy Lin. Small data? no problem! exploring the\nviability of pretrained multilingual language models for low-resourced languages. In Pro-\nceedings of the 1st Workshop on Multilingual Representation Learning, pages 116–126,\n2021.\n[78] Odunayo Jude Ogundepo, Akintunde Oladipo, Mofetoluwa Adeyemi, Kelechi Ogueji, and\nJimmy Lin. AfriTeVA: Extending ?small data? pretraining approaches to sequence-to-\nsequence models. In Colin Cherry, Angela Fan, George Foster, Gholamreza (Reza) Haf-\nfari, Shahram Khadivi, Nanyun (Violet) Peng, Xiang Ren, Ehsan Shareghi, and Swabha\nSwayamdipta, editors, Proceedings of the Third Workshop on Deep Learning for Low-\nResource Natural Language Processing, pages 126–135, Hybrid, July 2022. Association\nfor Computational Linguistics.\n[79] Nilanjana Raychawdhary, Amit Das, Sutanu Bhattacharya, Gerry Dozier, and Cheryl D\nSeals. Optimizing multilingual sentiment analysis in low-resource languages with adaptive\npretraining and strategic language selection. In 2024 IEEE 3rd International Conference\non Computing and Machine Intelligence (ICMI), pages 1–5. IEEE, 2024.\n[80] Nilanjana Raychawdhary, Nathaniel Hughes, Sutanu Bhattacharya, Gerry Dozier, and\nCheryl D Seals. A transformer-based language model for sentiment classification and\ncross-linguistic generalization: Empowering low-resource african languages. In 2023 IEEE\nInternational Conference on Artificial Intelligence, Blockchain, and Internet of Things\n(AIBThings), pages 1–5. IEEE, 2023.\n[81] Adedapo Bolaji Adeniji, Taiwo Kolajo, and Joshua Babatunde Agbogun. A framework for\nyoruba sentiment lexicon disambiguation. In 2024 International Conference on Science,\nEngineering and Business for Driving Sustainable Development Goals (SEB4SDG), pages\n1–6, 2024.\n[82] II Ayogu. Exploring multinomial na¨ıve bayes for yor`ub´a text document classification.\nNigerian Journal of Technology, 39(2):528–535, 2020.\n[83] David Ifeoluwa Adelani, Marek Masiak, Israel Abebe Azime, Jesujoba Alabi, Atnafu Lam-\nbebo Tonja, Christine Mwase, Odunayo Ogundepo, Bonaventure FP Dossou, Akintunde\nOladipo, Doreen Nixdorf, et al. Masakhanews: News topic classification for african lan-\nguages. In Proceedings of the 13th International Joint Conference on Natural Language\nProcessing and the 3rd Conference of the Asia-Pacific Chapter of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages 144–159, 2023.\n[84] Michael A Hedderich, David I Adelani, Dawei Zhu, Jesujoba Alabi, Udia Markus, and\nDietrich Klakow. Transfer learning and distant supervision for multilingual transformer\nmodels: A study on african languages. In 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), 2020.\n[85] Ife Adebara, AbdelRahim Elmadany, Muhammad Abdul-Mageed, and Alcides Inciarte.\nAfroLID: A neural language identification tool for African languages. In Yoav Goldberg,\nZornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Em-\npirical Methods in Natural Language Processing, pages 1958–1981, Abu Dhabi, United\nArab Emirates, December 2022. Association for Computational Linguistics.\n41\n\n[86] Toluwase Asubiaro, Tunde Adegbola, Robert Mercer, and Isola Ajiferuke. A word-level\nlanguage identification strategy for resource-scarce languages. Proceedings of the Associ-\nation for Information Science and Technology, 55(1):19–28, 2018.\n[87] Itunuoluwa Isewon, Adejumoke Famade, and Jelili Oyelade. A graphene-based text to\nspeech system for yoruba language. Journal of Applied Information Science, 04 2022.\n[88] Charbel Arnaud Cedrique Y Boco and Th´eophile K Dagba. An end to end bilingual tts\nsystem for fongbe and yoruba. In International Conference on Computational Collective\nIntelligence, pages 294–304. Springer, 2022.\n[89] John OR Aoga, Theophile K Dagba, and Codjo C Fanou. Integration of yoruba language\ninto marytts. International Journal of Speech Technology, 19:151–158, 2016.\n[90] Zainab Abdulkareem and Emmanuel E Effiong. Yorcall: Improving and sustaining yoruba\nlanguage through a practical iterative learning approach. In OcRI, pages 1–5, 2016.\n[91] Saint Germes B Bengono Obiang, Norbert Tsopze, Paulin Melatagia Yonta, Jean-Francois\nBonastre, and Tania Jim´enez. Improving tone recognition performance using wav2vec 2.0-\nbased learned representation in yoruba, a low-resourced language. ACM Transactions on\nAsian and Low-Resource Language Information Processing, 2024.\n[92] Samson Isaac, Khalid Haruna, Muhammad Aminu Ahmad, and Rabi Mustapha. Deep\nreinforcement learning with hidden markov model for speech recognition.\nJournal of\nTechnology and Innovation, pages 01–05, 2023.\n[93] AA Sosimi, T Adegbola, and OA Fakinlede. Standard yor`ub´a context dependent tone\nidentification using multi-class support vector machine (msvm). Journal of Applied Sci-\nences and Environmental Management, 23(5):895–901, 2019.\n[94] Habeeb A Rahmon, Tope G Jimoh, and Fatimoh O Madaiyese. Speech recognition model\nin yoruba language. Smartify: Journal of Smart Education and Pedagogy, 1(1):28–46,\n2024.\n[95] AR Iyanda. Statistical text analysis for yor`ub´a speech generation using zipfs law. Ife\nJournal of Technology, 23(2):40–44, 2015.\n[96] Tshephisho Joseph Sefara and Abiodun Modupe. Yor`ub´a gender recognition from speech\nusing neural networks. In 2019 6th International conference on soft computing & machine\nintelligence (ISCMI), pages 50–55. IEEE, 2019.\n[97] Abimbola Ganiyat Akintola, Tunji Samuel Ibiyemi, and Kayode Sakariyah Adewole. An\nhci speech-based architecture for man-to-machine and machine-to-man communication in\nyor`ub´a language. Computer Engineering and Intelligent Systems, 6:19–27, 2015.\n[98] Josh Meyer, David Adelani, Edresson Casanova, Alp Oktem, Daniel Whitenack, Julian\nWeber, Salomon Kabongo Kabenamualu, Elizabeth Salesky, Iroro Orife, Colin Leong,\nPerez Ogayo, Chris Emezue, Jonathan Mukiibi, Salomey Osei, Apelete AGBOLO, Vic-\ntor Akinode, Bernard Opoku, Jesujoba Alabi, and Shamsuddeen Hassan Muhammad.\nBibletts: a large, high-fidelity, multilingual, and uniquely african speech corpus. In In-\nterspeech 2022, pages 2383–2387, 09 2022.\n42\n\n[99] Th´eophile K Dagba, John OR Aoga, and Codjo C Fanou. Design of a yoruba language\nspeech corpus for the purposes of text-to-speech (tts) synthesis. In Intelligent Information\nand Database Systems: 8th Asian Conference, ACIIDS 2016, Da Nang, Vietnam, March\n14–16, 2016, Proceedings, Part I 8, pages 161–169. Springer, 2016.\n[100] Sunday Adeola Ajagbe. Developing nigeria multilingual languages speech datasets for an-\ntenatal orientation. In Hector Florez and Hern´an Astudillo, editors, Applied Informatics,\npages 157–170, Cham, 2024. Springer Nature Switzerland.\n[101] Tolulope Ogunremi, Kola Tubosun, Anuoluwapo Aremu, Iroro Orife, and David Ifeoluwa\nAdelani. `Ir`oy`ınspeech: A multi-purpose yor`ub´a speech corpus. In Proceedings of the 2024\nJoint International Conference on Computational Linguistics, Language Resources and\nEvaluation (LREC-COLING 2024), pages 9296–9303, 2024.\n[102] Daniel R Van Niekerk and Etienne Barnard. Predicting utterance pitch targets in yor`ub´a\nfor tone realisation in speech synthesis. Speech Communication, 56:229–242, 2014.\n[103] Adeyanju Sosimi, Tunde Adegbola, and Omotayo Fakinlede. A supervised phrase selec-\ntion strategy for phonetically balanced standard yor`ub´a corpus. In Computational Lin-\nguistics and Intelligent Text Processing: 16th International Conference, CICLing 2015,\nCairo, Egypt, April 14-20, 2015, Proceedings, Part II 16, pages 565–582. Springer, 2015.\n[104] Akintoba Emmanuel Akinwonm. Development of a prosodic read speech syllabic corpus\nof the yoruba language. Communications on Applied Electronics, 2021.\n[105] Lucas Ondel, Bolaji Yusuf, Luk´aˇs Burget, and Murat Sara¸clar. Non-parametric bayesian\nsubspace models for acoustic unit discovery. IEEE/ACM Transactions on Audio, Speech,\nand Language Processing, 30:1902–1917, 2022.\n[106] Lydia Kehinde Ajayi, Ambrose Azeta, Isaac Odun-Ayo, and Enem Theophilus Aniemeka.\nAcoustic nudging-based model for vocabulary reformulation in continuous yor`ub´a speech\nrecognition. In International Conference on Computational Science and Its Applications,\npages 494–508. Springer, 2022.\n[107] Bolaji Yusuf, Lucas Ondel, Luk Burget, Jan ernock, and Murat Saralar. A hierarchical\nsubspace model for language-attuned acoustic unit discovery. In ICASSP 2021 - 2021\nIEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),\npages 3710–3714, 2021.\n[108] Akintunde Oladipo, Mofetoluwa Adeyemi, and Jimmy Lin. On backbones and training\nregimes for dense retrieval in african languages. In Proceedings of the 47th International\nACM SIGIR Conference on Research and Development in Information Retrieval, pages\n2564–2568, 2024.\n[109] Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-\nHermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin. Miracl: A\nmultilingual retrieval dataset covering 18 diverse languages. Transactions of the Associ-\nation for Computational Linguistics, 11:1114–1131, 2023.\n[110] Olufade FW Onifade, Ayodeji OJ Ibitoye, and Pabitra Mitra. Embedded fuzzy bilingual\ndictionary model for cross language information retrieval systems. International Journal\nof Information Technology, 10:457–463, 2018.\n43\n\n[111] Mofetoluwa Adeyemi, Akintunde Oladipo, Xinyu Zhang, David Alfonso-Hermelo, Mehdi\nRezagholizadeh, Boxing Chen, Abdul-Hakeem Omotayo, Idris Abdulmumin, Naome A\nEtori, Toyib Babatunde Musa, et al. Ciral: A test collection for clir evaluations in african\nlanguages. In Proceedings of the 47th International ACM SIGIR Conference on Research\nand Development in Information Retrieval, pages 293–302, 2024.\n[112] Pavanpankaj Vegi, J Sivabhavani, Biswajit Paul, Abhinav Mishra, Prashant Banjare,\nKR Prasanna, and Chitra Viswanathan. Webcrawl african: A multilingual parallel cor-\npora for african languages. In Proceedings of the Seventh Conference on Machine Trans-\nlation (WMT), pages 1076–1089, 2022.\n[113] Jesujoba Alabi, Kwabena Amponsah-Kaakyire, David Adelani, and Cristina Espana-\nBonet. Massive vs. curated embeddings for low-resourced languages: the case of yor`ub´a\nand twi. In Proceedings of the Twelfth Language Resources and Evaluation Conference,\npages 2754–2762, 2020.\n[114] Ibukunola Abosede Modupe, Tshephisho Joseph Sefara, and Ojo Sunday. Yorub´a gender\nrecognition from speech using attention-based bilstm. In Proceedings of The First In-\nternational Workshop on NLP Solutions for Under Resourced Languages (NSURL 2019)\nco-located with ICNLSP 2019-Short Papers, pages 16–22, 2019.\n[115] Awoniran Olalekan, Ogundiran Daniel, Ozichi N Emuoyibofarhe, et al. Machine learn-\ning algorithms for multilingual text classification of nigerian local languages. Computer\nScience & Telecommunications, 62(2), 2022.\n[116] Enikuomehin A Oluwatoyin and Adewumi O Opeyemi. A stochastic collocation algorithm\nmethod for processing the yoruba language using the data context approach based on text,\nlexicon, and grammar. The Pacific Journal of Science and Technology, 2018.\n[117] Abayomi O Agbeyangi, Omolayo Abegunde, and Safiriyu I Eludiora. Authorship verifi-\ncation of yoruba blog posts using character n-grams. In 2020 International Conference\nin Mathematics, Computer Engineering and Computer Science (ICMCECS), pages 1–6.\nIEEE, 2020.\n[118] Ife Adebara, Abdelrahim Elmadany, Muhammad Abdul-Mageed, and Alcides Alcoba\nInciarte. Serengeti: Massively multilingual language models for africa. In Findings of the\nAssociation for Computational Linguistics: ACL 2023, pages 1498–1537, 2023.\n[119] Sebastian Gehrmann, Sebastian Ruder, Vitaly Nikolaev, Jan Botha, Michael Chavinda,\nAnkur Parikh, and Clara Rivera. Tata: A multilingual table-to-text dataset for african\nlanguages. In Findings of the Association for Computational Linguistics: EMNLP 2023,\npages 1719–1740, 2023.\n[120] Sebastian Ruder, Jonathan H Clark, Alexander Gutkin, Mihir Kale, Min Ma, Massimo\nNicosia, Shruti Rijhwani, Parker Riley, Jean-Michel Sarr, Xinyi Wang, et al. Xtreme-up:\nA user-centric scarce-data benchmark for under-represented languages. In Findings of\nthe Association for Computational Linguistics: EMNLP 2023, pages 1856–1884, 2023.\n[121] Tunde Oluwaseyi Ajayi, Mihael Arcan, and Paul Buitelaar. Cross-lingual transfer and\nmultilingual learning for detecting harmful behaviour in african under-resourced language\n44\n\ndialogue. In Proceedings of the 25th Annual Meeting of the Special Interest Group on\nDiscourse and Dialogue, pages 579–589, 2024.\n[122] Tosin Adewumi, Mofetoluwa Adeyemi, Aremu Anuoluwapo, Bukola Peters, Happy Buza-\naba, Oyerinde Samuel, Amina Mardiyyah Rufai, Benjamin Ajibade, Tajudeen Gwadabe,\nMory Moussou Koulibaly Traore, et al. Afriwoz: Corpus for exploiting cross-lingual trans-\nfer for dialogue generation in low-resource, african languages. In 2023 International Joint\nConference on Neural Networks (IJCNN), pages 1–8. IEEE, 2023.\n[123] Pawe l Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, I˜nigo Casanueva, Stefan Ultes,\nOsman Ramadan, and Milica Gaˇsi´c. MultiWOZ - a large-scale multi-domain Wizard-of-\nOz dataset for task-oriented dialogue modelling. In Ellen Riloff, David Chiang, Julia\nHockenmaier, and Jun’ichi Tsujii, editors, Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing, pages 5016–5026, Brussels, Belgium,\nOctober-November 2018. Association for Computational Linguistics.\n[124] NLLB Team, Marta R. Costa-juss, James Cross, Onur elebi, Maha Elbayad, Kenneth\nHeafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard,\nAnna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Bar-\nrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett,\nKaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews,\nNecip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj\nGoswami, Francisco Guzmn, Philipp Koehn, Alexandre Mourachko, Christophe Ropers,\nSafiyyah Saleem, Holger Schwenk, and Jeff Wang.\nNo language left behind: Scaling\nhuman-centered machine translation, 2022.\n[125] Lawrence B Adewole, Adebayo O Adetunmbi, Boniface K Alese, and Samuel A\nOluwadare. Token validation in automatic corpus gathering for yoruba language. FUOYE\nJournal of Engineering and Technology, 2(1):4, 2017.\n[126] Mohamed Al-Badrashiny, Arfath Pasha, Mona T. Diab, Nizar Habash, Owen Rambow,\nWael Salloum, and Ramy Eskander. Split: Smart preprocessing (quasi) language inde-\npendent tool. In International Conference on Language Resources and Evaluation, 2016.\n[127] Alper Kursat Uysal and Serkan Gunal. The impact of preprocessing on text classification.\nInformation processing & management, 50(1):104–112, 2014.\n45\n",
  "metadata": {
    "source_path": "papers/arxiv/Bridging_Gaps_in_Natural_Language_Processing_for_Yorùbá_A\n__Systematic_Review_of_a_Decade_of_Progress_and_Prospects_a7143dc7732e9b56.pdf",
    "content_hash": "a7143dc7732e9b564f07e2972157e80fa733513613208aeaecc5e301c99a04d7",
    "arxiv_id": null,
    "title": "Bridging_Gaps_in_Natural_Language_Processing_for_Yorùbá_A\n__Systematic_Review_of_a_Decade_of_Progress_and_Prospects_a7143dc7732e9b56",
    "author": "",
    "creation_date": "D:20250225030413Z",
    "published": "2025-02-25T03:04:13",
    "pages": 45,
    "size": 2464621,
    "file_mtime": 1740470155.1971235
  }
}