{
  "text": "LONGSPEC:\nLong-Context Speculative Decoding with Efficient Drafting and Verification\nPenghui Yang * 2 Cunxiao Du * 1 Fengzhuo Zhang 3 Haonan Wang 3 Tianyu Pang 1 Chao Du 1 Bo An 2\nAbstract\nSpeculative decoding has become a promising\ntechnique to mitigate the high inference latency\nof autoregressive decoding in Large Language\nModels (LLMs). Despite its promise, the effec-\ntive application of speculative decoding in LLMs\nstill confronts three key challenges: the increasing\nmemory demands of the draft model, the distribu-\ntion shift between the short-training corpora and\nlong-context inference, and inefficiencies in atten-\ntion implementation. In this work, we enhance\nthe performance of speculative decoding in long-\ncontext settings by addressing these challenges.\nFirst, we propose a memory-efficient draft model\nwith a constant-sized Key-Value (KV) cache. Sec-\nond, we introduce novel position indices for short-\ntraining data, enabling seamless adaptation from\nshort-context training to long-context inference.\nFinally, we present an innovative attention ag-\ngregation method that combines fast implemen-\ntations for prefix computation with standard at-\ntention for tree mask handling, effectively re-\nsolving the latency and memory inefficiencies\nof tree decoding. Our approach achieves strong\nresults on various long-context tasks, including\nrepository-level code completion, long-context\nsummarization, and o1-like long reasoning tasks,\ndemonstrating significant improvements in la-\ntency reduction. The code is available at https:\n//github.com/sail-sg/LongSpec.\n1. Introduction\nLarge Language Models (LLMs) have achieved remark-\nable success across various natural language processing\n*Equal contribution. Work done during Penghui Yang’s as-\nsociate membership at Sea AI Lab.\n1Sea AI Lab, Singa-\npore.\n2Nanyang Technological University.\n3National Uni-\nversity of Singapore.\nCorrespondence to:\nPenghui Yang\n<phyang.cs@gmail.com>, Cunxiao Du <ducx@sea.com>,\nFengzhuo Zhang <fzzhang@u.nus.edu>.\ntasks (Achiam et al., 2023), but their autoregressive de-\ncoding mechanism often results in high latency. To ad-\ndress this limitation, speculative decoding (Leviathan et al.,\n2023) has emerged as a promising solution. By employing\na lightweight draft model to generate multiple candidate\ntokens, the target model can verify these tokens in parallel,\nthereby accelerating the inference process without compro-\nmising output quality.\nDespite the great advancements in speculative decoding, ex-\nisting research has primarily concentrated on short-context\nscenarios. However, as highlighted by Chen et al. (2025),\nthe core potential of speculative decoding lies in long-\ncontext settings, where the maximum inference batch size\nis relatively low. The limited batch size restricts autoregres-\nsive decoding from fully utilizing the GPU computation\nresource, making speculative decoding an ideal approach to\naddress this constraint. Yet, despite its advantages, the devel-\nopment of draft models specially designed for long-context\nscenarios remains largely unexplored.\nWhile the need for a long-context draft model is clear from\nboth application demands and theoretical considerations,\nwe find that existing methodologies developed for short-\ncontext scenarios are inadequate when applied to longer\nsequences. This inadequacy stems from three emergent\nchallenges unique to long-context speculative decoding: 1)\nArchitecture: the extra memory overhead of the draft model,\n2) Training: the distribution shift of position indices, and 3)\nInference: the inefficiencies in tree attention implementation.\nFirst, as decoding length increases, previous State-of-The-\nArt (SoTA) autoregressive draft models like EAGLE (Li\net al., 2024) and GliDe (Du et al., 2024) require linearly\nincreasing KV caches, resulting in substantial storage over-\nhead. This issue becomes critical in long-context settings,\nwhere memory usage is of vital importance. Second, the\ntraining data of the draft model mainly consists of short con-\ntext data, rendering it undertrained over the large position\nindices (An et al., 2025), while the inference is for long-\ncontext data. The discrepancy will cause the draft model\nunable to perform speculation when facing large position\nindices of long-context input. Moreover, SoTA approaches\noften rely on tree attention, which is incompatible with the\n1\narXiv:2502.17421v1  [cs.CL]  24 Feb 2025\n\nLongSpec: Long-Context Speculative Decoding with Efficient Drafting and Verification\ncurrent advanced attention kernels due to the tree mask.\nThis incompatibility further constrains the usage of tree\nspeculative decoding in long-context scenarios.\nTo address these challenges, we introduce LONGSPEC, a\nframework for long-context speculative decoding, featur-\ning architectural innovation (Sec. 3.1), novel training meth-\nods (Sec. 3.2), and optimized inference implementation\n(Sec. 3.3). The three key innovations significantly enhance\nthe efficiency and scalability of speculative decoding in\nlong-context scenarios.\nFirst, to alleviate the memory overhead problem, we develop\na draft model architecture that circumvents the linear ex-\npansion of KV caches which maintains a constant memory\nfootprint as the context grows. Concretely, our approach em-\nploys a sliding window self-attention component to capture\nlocal dependencies, complemented by a cache-free cross-\nattention module for effectively modeling long-context rep-\nresentations. This approach effectively mitigates memory\noverhead which is particularly critical in long-context infer-\nence without compromising performance.\nNext, to handle the training discrepancy problem, we\npropose the Anchor-Offset Indices to reconcile the short-\ncontext training for long-context inference. To fully train all\nthe indices, we randomly add an offset to position indices.\nThis ensures that some larger indices are also sufficiently\ntrained. However, since RoPE (Su et al., 2024) is based\non relative positions, directly adding an offset to all in-\ndices does not have a direct effect. Inspired by Streaming\nLLM (Xiao et al., 2024), we set the first four attention sink\ntokens as the anchor indices and only add the random offset\nto the remaining token index as shown in Figure 2. This\nindexing strategy ensures that all the indices can be suffi-\nciently trained for the draft model. In contrast, the vanilla\nindexing strategy repeatedly trains only the smaller indices\nand is unable to train those exceeding the training set length.\nMeanwhile, anchoring the sink tokens ensures that the target\nmodel can approximate the attention sink patterns found in\nlong texts, even with short texts.\nFinally, to implement highly efficient tree attention, we\npropose a new computation method called Hybrid Tree At-\ntention. Our insight comes from the discovery that the tree\nmask in tree-based speculative decoding can be decomposed\ninto two parts, the previously cached part with a chain struc-\nture and the speculation part with a tree structure. Specif-\nically, the tree mask is only required between the current\nqueries and the speculation tokens (i.e., current input tokens)\nto ensure correctness. So we use Flash Decoding (Dao,\n2024) to compute the first part efficiently and use a custom\nTriton kernel fused mask attn to compute the second\npart. We then combine these components using a log-sum-\nexp trick, enabling our approach to accelerate tree attention\ncomputations up to 4.1 × compared to previous implemen-\ntations in Medusa (Cai et al., 2024).\nExtensive experiments are conducted to evaluate the effec-\ntiveness of LONGSPEC. Experiments on five long-context\nunderstanding datasets using five LLMs as target models\nshow that our LONGSPEC can effectively reduce the long-\ncontext inference latency, leading to a maximum speedup\nof 3.26× compared with the strong baseline model with\nFlash Decoding. Additional experiments on the long\nreasoning task AIME24 with the o1-like model QwQ (Qwen,\n2024) further validate the effectiveness of LONGSPEC,\nachieving a 2.25× speedup in wall-clock time.\n2. Related Work\nSpeculative decoding offers a promising approach to accel-\nerating LLMs while maintaining the quality of their out-\nputs. Early efforts, such as Speculative Decoding (Xia et al.,\n2023), SpS (Leviathan et al., 2023), BiLD (Kim et al., 2024),\nand OSD (Liu et al., 2024c), rely on existing smaller LLMs\nto generate draft sequences. Some other methods aim to\nimprove upon those early efforts (Sun et al., 2023; Miao\net al., 2024; Chen et al., 2024). There are also some works\nusing part of the target model as the draft model (Liu et al.,\n2024a; Zhang et al., 2024; Elhoushi et al., 2024). Retrieval-\nbased speculative decoding methods offer an alternative by\nutilizing N-gram matching rather than relying on smaller\nmodels. Examples include Lookahead Decoding (Fu et al.,\n2024), REST (He et al., 2024), and Ouroboros (Zhao et al.,\n2024). These approaches bypass the need for additional\nmodel training, leveraging pre-existing data patterns to con-\nstruct draft sequences efficiently.\nMore recent advancements, including Medusa (Cai et al.,\n2024), EAGLE (Li et al., 2024), and GliDe (Du et al., 2024),\nhave expanded on these foundations by designing special-\nized draft models and introducing tree-based speculative\ntechniques. These methods leverage customized draft mod-\nels tailored for speculative decoding, achieving higher ef-\nficiency and performance. Additionally, the tree-based ap-\nproaches employed in these methods allow for more adap-\ntive and parallelizable decoding processes, paving the way\nfor broader applications in real-world systems.\nAlthough speculative decoding has progressed significantly\nfor conventional context lengths, only two existing papers\nfocus on speculative decoding in long-context scenarios.\nTriForce (Sun et al., 2024) introduces a three-layer specu-\nlative decoding system that is scalable for long sequence\ngeneration. MagicDec (Chen et al., 2025) uses speculative\ndecoding to improve both the throughput and latency of\nLLM inference. However, these methods mainly utilize the\ntarget model with the sparse KV cache as the draft model.\nThe computation-intensive draft models restrict the practical\nusage of these methods when facing various batch sizes. In\ncontrast, our work focuses on efficiently building a draft\n2\n\nLongSpec: Long-Context Speculative Decoding with Efficient Drafting and Verification\nmodel with only one transformer block, achieving more\neffective performance across different scenarios.\n3. Methodology\nIn this section, we present our framework LONGSPEC for\nLong-Context Speculative Decoding, which addresses three\nkey challenges: (1) designing a lightweight draft model ar-\nchitecture with minimal additional memory overhead, (2)\ndevising the training strategy with anchor-offset indices to\nhandle long contexts effectively, and (3) implementing a\nfast tree attention mechanism that leverages tree-based spec-\nulation for practical usage. We detail each core component\nin the following subsections: Section 3.1 introduces our\nMemory-Efficient Architecture, Section 3.2 explains the Ef-\nfective Training Regimes, and Section 3.3 describes the Fast\nTree Attention implementation for inference.\n3.1. Memory-Efficient Architecture\nIn previous work, the success of the SoTA model EAGLE\ndepends on two factors: (1) the hidden states provided by the\ntarget model, and (2) an autoregressive structure. However,\nan autoregressive draft model inevitably needs to store its\nown KV cache, which introduces additional overhead in\nlong-context inference requiring large GPU memory.\nTo avoid this extra memory overhead, we propose a draft\nmodel with constant memory usage regardless of the length\nof the context. As illustrated in Figure 1, our model com-\nprises two components: the self-attention module and the\nfollowing cross-attention module. The self-attention module\nfocuses on modeling local context, while the cross-attention\nmodule captures long-context information. Because the self-\nattention module only processes local information, we adopt\na sliding-window attention mechanism. Hence, during infer-\nence, the self-attention memory footprint does not exceed\nthe window size, which we set to 512. We also provide\nthe theoretical upper bound on the performance degradation\ncaused by the slicing window in Section 3.4.\nFor the cross-attention component, inspired by GliDe (Du\net al., 2024), we leverage the KV cache of the target model.\nThis design not only enables better modeling of previous\ninformation but also completely removes additional stor-\nage overhead for long contexts, since the large model’s KV\ncache must be stored regardless of whether or not specula-\ntive decoding is employed. Different from GliDe, we also\nshare the weights of the Embedding Layer and LM Head\nbetween the target model and the draft model, which sig-\nnificantly reduces the memory usage for large-vocabulary\nLLMs such as LLaMA.\nEmbedding Layer\nFixed Length Window\nPrefix & Cached Tokens\ndeep\nwrinkles\nQ K V Projection Layer\nSelf Attention Layer\nQ Projection Layer\nCross Attention & FFN Layer\nLM Head\nHe was an old man who fished alone\n...... \nThe old man was thin and gaunt with\nNew Input Token\n...\nTarget LLM KV Cache\nTarget Model\nDraft Model\nKgaunt\nQdeep\nself\nQdeep\ncross\nKcache\nlast\nVcache\nlast\nVgaunt\nKwith\nVwith\nKdeep\nVdeep\nKHe\nVHe\nFigure 1. Illustration of the memory-efficient draft model. We use\nsliding window self-attention to capture the local context informa-\ntion and cross attention layer to gather long-context information.\n3.2. Effective Training Regimes\nAnchor-Offset Indices.\nWith vanilla position indices,\nwhich consist of successive integers starting from 0, those\nindices appearing earlier in sequences occur more frequently\nthan larger position indices (An et al., 2025), as shown in\nthe Figure 2 upper left part. Consequently, larger position\nindices receive insufficient training updates, which leads to a\ntraining inference discrepancy. A common method to solve\nthis problem is RoPE-based extrapolation, which trains the\nmodel with a smaller RoPE base and extends the RoPE base\nwith interpolation for longer contexts (Gao et al., 2024; Liu\net al., 2024d; Peng et al., 2024). However, directly using\nthese methods will cause an inconsistent problem in draft\nmodel training. To leverage the target model’s KV cache,\nour draft model must keep the RoPE base the same as the\ntarget model. Based on our exploratory experiments, the in-\nconsistent RoPE base between the target model and the draft\nmodel will cause a significant collapse of the cross-attention\nlayer, which makes the draft model’s long-text capability\ndegrade. The consistency requirement of RoPE base in the\ndraft model limits the usage of methods like RoPE-based\n3\n\nLongSpec: Long-Context Speculative Decoding with Efficient Drafting and Verification\n(a) Anchor-Offset Indexing\n(b) Hybrid Tree Attention\nSpeculation Tokens’ KV\nCached Tokens’ KV\nthin and gaunt\nwas\nman\nold\nThe\nHe\nwas\n...\nweak\nwrinkles\nan\nwith\ndeep\nvoice\narms\nhands\nweak\nwrinkles\nwith\ndeep\nvoice\narms\nhands\nTarget Model’s Max Length\nTarget Model’s Max Length\nTarget Model’s Max Length\nShort-Context Training Stage\n              with Vanilla Indicies\nShort-Context Training Stage\n   with Anchor-Offset Indices\nLong-Context Training Stage\n0 1 2               803\n0 1 2                           1021\n0 1 2    55\n0 1 2                  996\n0 1 2 3\n0 1 2 3\n0 1 2 3\n0 1 2 3\n20004     20803\n10204       11221\n20304    21296\n30004      30055\n0 1 2 3\n0 1 2 3\n0 1 2 3\n0 1 2 3\n21119\n30043\n30187\n28187\nAnchor\nIndices\noffset2=10200\noffset1=20000\noffset3=30000\noffset4=20300\nFlash Decoding\nFast\nTriton Attention\nFlexible\nHybrid Tree Attention\nFast & Flexible\nFigure 2. Illustration of the Anchor-Offset Indices and the Hybrid Tree Attention. (a) The left section illustrates the differences between\nthe vanilla indexing and the Anchor-Offset Indices. By introducing a randomly selected offset and some anchor indices, the Anchor-Offset\nIndices enable the short-context training stage to seamlessly integrate with the long-context training stage. (b) The right section displays\nthe Hybrid Tree Attention, which combines the advantages of Flash Decoding and our Triton-implemented Attention.\nextrapolation, which requires flexible adjustments to the\nRoPE base.\nInstead, we tackle this challenge by leveraging carefully\ndesigned indices. These indices must ensure that (1) the\nposition indices in the draft model can be sufficiently trained\nusing short-context data and (2) the indices would not cause\nthe target model to exhibit out-of-distribution behavior be-\ncause the target model shares the same indices as the draft\nmodel during training.\nTo satisfy these constraints, we propose the Anchor-\nOffset Indices strategy. Specifically, we reserve the first\nfour positions [0, 1, 2, 3] as attention sink tokens (Xiao\net al., 2024), then assign all subsequent tokens to large\nconsecutive indices starting at a random offset (e.g.,\n[0, 1, 2, 3, 8192, 8193, 8194, . . . ]). The anchor indices and\nrandom offset ensure that every position index can be suffi-\nciently trained, addressing the limitation of the vanilla one\nthat repeatedly trains only smaller indices.\nAdditionally, according to Xiao et al. (2024), LLM exhibits\nan attention sink phenomenon when dealing with long texts,\nwhich means the attention weights primarily concentrate on\nthe first four tokens and the recent tokens. Therefore, we be-\nlieve that utilizing Anchor-Offset Indices can naturally lead\nthe target model to exhibit in-distribution behavior. In our\nexperiments, adopting these indices in the target model only\nincreases the loss by approximately 0.001, indicating that\nthe target model is indeed well-suited to such changes. We\nalso provide the theoretical upper bound of the distribution\nshift error in Section 3.4.\nFlash Noisy Training. During training our draft model\nleverages the KV caches from a large model, while these\nKV caches are not always visible during inference. This is\nbecause the large model only updates its KV cache upon\nverification completion. Concretely, for the cross-attention\nquery Qt in the draft model, we can only guarantee access\nto the corresponding key-value states K<t′, V<t′ satisfying\n1 ≤|t′ −t| < γ, where γ is the number of speculative steps.\nTo ensure consistency between training and inference, a\nstraightforward solution would be to add an attention\nmask (Du et al., 2024). However, this method is incom-\npatible with Flash Attention (Dao et al., 2023), which would\nsignificantly degrade training speed and cause prohibitive\nmemory overhead, particularly in long-context training sce-\nnarios. Therefore, we propose a technique called flash noisy\ntraining. During training, we randomly shift the indices of\nqueries and key-value states with 1 ≤j < γ. Suppose the\nsequence length is l, then we compute\nO≥j = flash attn\n\u0000Q≥j, K<l−j, V<l−j\n\u0001\n.\nIn this way, we effectively simulate the same visibility con-\nstraints as in the inference phase, i.e., 1 ≤|t′ −t| < γ,\nthereby aligning the behavior at training time with the infer-\nence behavior.\n3.3. Fast Tree Attention\nTree Speculative Decoding (Miao et al., 2024) leverages\nprefix trees and the causal structure of LLMs so that a draft\nmodel can propose multiple candidate sequences, while the\ntarget model only needs to verify them once, without alter-\ning the final results. In this process, Tree Attention plays a\nkey role in ensuring both correctness and efficiency. Early\n4\n\nLongSpec: Long-Context Speculative Decoding with Efficient Drafting and Verification\nworks (Cai et al., 2024; Li et al., 2024) apply attention masks\nderived from prefix trees to the QKT attention matrix, thus\ndisabling wrong combinations between speculation tokens.\nHowever, these methods only run on PyTorch’s eager exe-\ncution mode, precluding more advanced attention kernels\n(e.g., Flash Decoding). As a result, the inference speed\ndecreases significantly when the sequence length increases.\nTo address these performance bottlenecks, we propose a Hy-\nbrid Tree Attention mechanism, as illustrated in Figure 2.\nOur method is based on two key insights: 1) When perform-\ning Tree Attention, the queries and the cached key-value\npairs {Kcache, Vcache} do not require additional masks; 2)\nOnly the queries and the key-value pairs {Kspecs, Vspecs}\nfrom the current speculative tokens need masking, and the\nnumber of such speculative tokens is typically no more than\n128. Based on these observations, we adopt a divide and\naggregate approach that splits the attention computation into\ntwo parts and merges them afterward.\nSplitting Key-Value Pairs.\nWe partition all key-value\npairs into two groups: {Kcache, Vcache}: the cached part of\nthe main sequence, which requires no attention mask; and\n{Kspecs, Vspecs}: the speculative-stage part, which needs\nattention masks. For {Kcache, Vcache}, we invoke the effi-\ncient Flash Decoding kernel. For {Kspecs, Vspecs}, we\nuse our custom Triton kernel fused mask attn, which\napplies blockwise loading and masking in the KV dimen-\nsion, enabling fast computation of attention. This step yields\ntwo sets of attention outputs {Ocache, Ospecs} along with\ntheir corresponding denominators (i.e., log-sum-exp of all\nattention scores) {LSEcache, LSEspecs}.\nAggregation. We then combine these two parts into the\nfinal attention output Omerge via a log-sum-exp trick. First,\nwe compute\nLSEmerge = log\n\u0010\nexp\n\u0000LSEcache\n\u0001\n+ exp\n\u0000LSEspecs\n\u0001\u0011\n,\nand then apply a weighted summation to the two outputs:\nOmerge = Ocache · exp\n\u0000LSEcache −LSEmerge\n\u0001\n+ Ospecs · exp\n\u0000LSEspecs −LSEmerge\n\u0001\n.\nThe theoretical guarantee is provided in Appendix A. As\noutlined above, this hybrid approach employs the highly\nefficient Flash Decoding kernel for most of the compu-\ntations in long-sequence inference and only uses a custom\nmasking attention fused mask attn for the small num-\nber of speculative tokens. The kernel fused mask attn\nfollows the design philosophy of Flash Attention 2 (Dao\net al., 2023) by splitting Q, Kspecs, and Vspecs into small\nblocks. This strategy reduces global memory I/O and fully\nleverages GPU streaming multiprocessors. Furthermore, for\neach block in the computation of QK⊤\nspecs, the mask matrix\nis loaded and used to apply the masking operation. The\nHybrid Tree Attention effectively balances the parallel veri-\nfication of multiple branches with improved inference speed,\nall without compromising the correctness.\n3.4. Theoretical Analysis\nHere we would like to provide the theoretical analysis of our\nmethod. Before the statement of our results, we would like\nto define some quantities. First, for the memory-efficient ar-\nchitecture, we denote the sum of the attention score outside\nthe window as ε, which should be small due to the locality\nof the language. Second, for our index offset technique, we\ndenote the distribution of the offset as ˜Pt. In addition, we\ndenote the true distribution of the index of the first token\nin the window as Pt. Finally, we assume that the GliDe\nfunction is trained on N i.i.d. samples, and the Frobenius\nnorms of all the parameters are upper bounded by B.\nTheorem 3.1 (Informal). Under regularity assumptions,\nthe inference error between the Maximum Likelihood Esti-\nmate (MLE) trained by our method and the optimal GliDe\nparameter that take all tokens as inputs is upper bounded by\nAE + DE + GE\nwith probability at least 1−δ. Here the approximation error\nis AE = (1 + dV exp(B))HB4(1 + B2\nXB2)ε, the distribu-\ntion shift error is DE = log(1 + dV exp(B)) · TV( ˜Pt, Pt),\nand the generalization error is GE\n=\nN −1/2(d(d +\ndV ) log(1 + NHB6) + log δ−1).\nThe formal statement and the proof are provided in Ap-\npendix D. The approximation error results from that we\nadopt a sliding window instead of using all tokens. This\nerror is proportional to the attention score outside the win-\ndow. The distribution shift error results from the positional\nembedding distributional shift. In our experiments, we set\n˜Pt as the uniform distribution, which will have a smaller\ndistribution shift than not adopting this position offset tech-\nnique. The generalization error results from that we use N\nsamples to train the model.\n4. Experiments\n4.1. Settings\nTarget and draft models. We select four widely-used long-\ncontext LLMs, Vicuna (including 7B and 13B) (Chiang\net al., 2023), LongChat (including 7B and 13B) (Li et al.,\n2023), LLaMA-3.1-8B-Instruct (Dubey et al., 2024), and\nQwQ-32B (Qwen, 2024), as target models. In order to\nmake the draft model and target model more compatible,\nour draft model is consistent with the target model in various\nparameters such as the number of KV heads.\nTraining Process.\nWe first train our draft model with\nAchor-Offest Indices on the SlimPajama-6B pretraining\n5\n\nLongSpec: Long-Context Speculative Decoding with Efficient Drafting and Verification\nTable 1. Mean accepted length (τ), decoding speed (tokens/s), and speedups across different models and settings. Specifically, “Vanilla\nHF” refers to HuggingFace’s PyTorch-based attention implementation, while “Vanilla FA” employs Flash Decoding. The speedup\nstatistic calculates the acceleration ratio relative to the Vanilla HF method. All results are computed at T = 0.\nSetting\nGovReport\nQMSum\nMulti-News\nLCC\nRepoBench-P\nτ\nTokens/s\nSpeedup\nτ\nTokens/s\nSpeedup\nτ\nTokens/s\nSpeedup\nτ\nTokens/s\nSpeedup\nτ\nTokens/s\nSpeedup\nV-7B\nVanilla HF\n1.00\n25.25\n-\n1.00\n18.12\n-\n1.00\n27.29\n-\n1.00\n25.25\n-\n1.00\n19.18\n-\nVanilla FA\n1.00\n45.76\n1.00×\n1.00\n43.68\n1.00×\n1.00\n55.99\n1.00×\n1.00\n54.07\n1.00×\n1.00\n46.61\n1.00×\nMagicDec\n2.23\n41.68\n0.91×\n2.29\n42.91\n0.98×\n2.31\n44.82\n0.80×\n2.52\n46.96\n0.87×\n2.57\n48.75\n1.05×\nLongSpec\n3.57\n102.23\n2.23×\n3.14\n88.87\n2.04×\n3.51\n100.55\n1.80×\n3.73\n107.30\n1.99×\n3.86\n110.76\n2.38×\nV-13B\nVanilla HF\n1.00\n17.25\n-\n1.00\n11.86\n-\n1.00\n18.81\n-\n1.00\n17.25\n-\n1.00\n13.44\n-\nVanilla FA\n1.00\n28.52\n1.00×\n1.00\n27.43\n1.00×\n1.00\n35.01\n1.00×\n1.00\n33.87\n1.00×\n1.00\n29.14\n1.00×\nMagicDec\n2.95\n38.24\n1.34×\n2.87\n37.15\n1.35×\n2.97\n39.47\n1.13×\n2.96\n38.40\n1.13×\n2.94\n36.66\n1.26×\nLongSpec\n3.31\n71.08\n2.49×\n2.76\n57.15\n2.08×\n3.44\n78.20\n2.23×\n3.57\n81.00\n2.39×\n3.59\n77.22\n2.65×\nLC-7B\nVanilla HF\n1.00\n25.27\n-\n1.00\n14.11\n-\n1.00\n27.66\n-\n1.00\n25.27\n-\n1.00\n17.02\n-\nVanilla FA\n1.00\n42.14\n1.00×\n1.00\n36.87\n1.00×\n1.00\n50.19\n1.00×\n1.00\n54.17\n1.00×\n1.00\n42.69\n1.00×\nMagicDec\n2.26\n41.90\n0.99×\n2.20\n40.82\n1.11×\n2.32\n43.94\n0.88×\n2.77\n51.73\n0.96×\n2.57\n44.13\n1.03×\nLongSpec\n3.59\n101.43\n2.41×\n3.06\n85.23\n2.31×\n3.41\n97.93\n1.95×\n4.21\n122.30\n2.26×\n4.03\n115.27\n2.70×\nLC-13B\nVanilla HF\n1.00\n17.72\n-\n1.00\n12.08\n-\n1.00\n18.74\n-\n1.00\n17.72\n-\n1.00\n13.85\n-\nVanilla FA\n1.00\n28.56\n1.00×\n1.00\n27.18\n1.00×\n1.00\n35.37\n1.00×\n1.00\n34.58\n1.00×\n1.00\n29.74\n1.00×\nMagicDec\n2.40\n31.37\n1.10×\n2.38\n30.84\n1.13×\n2.43\n32.58\n0.92×\n2.68\n35.77\n1.03×\n2.85\n35.67\n1.20×\nLongSpec\n3.58\n76.26\n2.67×\n3.15\n64.41\n2.37×\n3.50\n80.48\n2.28×\n4.01\n90.92\n2.63×\n4.46\n96.96\n3.26×\nL-8B\nVanilla HF\n1.00\n21.59\n-\n1.00\n18.67\n-\n1.00\n29.91\n-\n1.00\n29.48\n-\n1.00\n22.77\n-\nVanilla FA\n1.00\n53.14\n1.00×\n1.00\n51.22\n1.00×\n1.00\n56.94\n1.00×\n1.00\n56.73\n1.00×\n1.00\n54.08\n1.00×\nMagicDec\n2.04\n36.14\n0.68×\n2.00\n35.78\n0.70×\n2.33\n39.57\n0.70×\n2.65\n46.95\n0.83×\n2.61\n44.39\n0.82×\nLongSpec\n3.25\n84.57\n1.59×\n2.99\n75.68\n1.48×\n3.36\n91.11\n1.60×\n3.28\n89.33\n1.57×\n3.39\n91.28\n1.69×\nG\nQ\nM\nL\nR\n0\n20\n40\n60\n80\n100\n120\nTokens/s\n46\n46\n48\n50\n50\n108\n105\n116\n119\n114\nVicuna-7B\nG\nQ\nM\nL\nR\n38\n37\n39\n38\n36\n78\n66\n84\n80\n76\nVicuna-13B\nG\nQ\nM\nL\nR\n44\n43\n47\n51\n48\n108\n103\n110\n124\n119\nLongChat-7B\nG\nQ\nM\nL\nR\n33\n33\n34\n37\n36\n75\n76\n81\n93\n88\nLongChat-13B\nG\nQ\nM\nL\nR\n40\n38\n45\n48\n45\n97\n98\n101\n108\n104\nLLaMA-3.1-8B\nMagicDec\nLongSpec\nFigure 3. Decoding speed (tokens/s) across different models and settings. All results are computed at T = 1. The letters G, Q, M, L, and\nR on the horizontal axis represent the dataset GovReport, QMSum, Multi-News, LCC, and RepoBench-P respectively.\ndataset (Soboleva et al., 2023). The random offset is set\nas a random integer from 0 to 15k for Vicuna models and\nLongChat-7B, and 0 to 30k for the other three models be-\ncause they have longer maximum context length. Then we\ntrain our model on a small subset of the Prolong-64k long-\ncontext dataset (Gao et al., 2024) in order to gain the ability\nto handle long texts. Finally, we finetune our model on a self-\nbuilt long-context supervised-finetuning (SFT) dataset to\nfurther improve the model performance. The position index\nof the last two stages is the vanilla indexing policy because\nthe training data is sufficiently long. We apply flash noisy\ntraining during all three stages to mitigate the training and\ninference inconsistency, the extra overhead of flash noisy\ntraining is negligible. Standard cross-entropy is used to\noptimize the draft model while the parameters of the target\nmodel are kept frozen. To mitigate the VRAM peak caused\nby the computation of the logits, we use a fused-linear-and-\ncross-entropy loss implemented by the Liger Kernel (Hsu\net al., 2024), which computes the LM head and the soft-\nmax function together and can greatly alleviate this problem.\nMore details on model training can be found in Appendix B.\nTest Benchmarks.\nWe select tasks from the Long-\nBench benchmark (Bai et al., 2024) that involve gener-\nating longer outputs, because tasks with shorter outputs,\nsuch as document-QA, make it challenging to measure the\nspeedup ratio fairly with speculative decoding. Specifi-\ncally, we focus on long-document summarization and code\ncompletion tasks and conduct tests on five datasets: Gov-\nReport (Huang et al., 2021), QMSum (Zhong et al., 2021),\nMulti-News (Fabbri et al., 2019), LCC (Guo et al., 2023),\nand RepoBench-P (Liu et al., 2024b). We test QwQ-32B on\nthe famous reasoning dataset AIME24 (Numina, 2024).\nWe compare our method with the original target model and\nMagicDec, a simple prototype of TriForce. To highlight the\nsignificance of Flash Decoding in long-context scenar-\nios, we also present the performance of the original target\nmodel using both eager attention implemented by Hugging-\nface and Flash Decoding for comparison. To make a\nfair comparison, we also use Flash Decoding for base-\n6\n\nLongSpec: Long-Context Speculative Decoding with Efficient Drafting and Verification\nline MagicDec. The most important metric for speculative\ndecoding is the walltime speedup ratio, which is the actual\ntest speedup ratio relative to vanilla autoregressive decod-\ning. We also test the average acceptance length τ, i.e., the\naverage number of tokens accepted per forward pass of the\ntarget LLM.\n4.2. Main Results\nTable 1 and Figure 3 show the decoding speeds and mean\naccept lengths across the five evaluated datasets at T = 0\nand T = 1 respectively. Our proposed method significantly\noutperforms all other approaches on both summarization\ntasks and code completion tasks. When T = 0, on sum-\nmarization tasks, our method can achieve a mean accepted\nlength of around 3.5 and a speedup of up to 2.67×; and\non code completion tasks, our method can achieve a mean\naccepted length of around 4 and a speedup of up to 3.26×.\nThis highlights the robustness and generalizability of our\nspeculative decoding approach, particularly in long-text gen-\neration tasks. At T = 1, our method’s performance achieves\naround 2.5× speedup, maintaining a substantial lead over\nMagicDec. This indicates that our approach is robust across\ndifferent temperature settings, further validating its sound-\nness and efficiency.\nWhile MagicDec demonstrates competitive acceptance rates\nwith LongSpec, its speedup is noticeably lower in our exper-\niments. This is because MagicDec is primarily designed for\nscenarios with large batch sizes and tensor parallelism. In\nlow-batch-size settings, its draft model leverages all param-\neters of the target model with sparse KV Cache becomes\nexcessively heavy. This design choice leads to inefficiencies,\nas the draft model’s computational overhead outweighs its\nspeculative benefits. Our results reveal that MagicDec only\nachieves acceleration ratios > 1 on partial datasets when\nusing a guess length γ =2 and consistently exhibits negative\nacceleration around 0.7× when γ ≥3, further underscoring\nthe limitations of this method in such configurations.\nLastly, we can find attention implementation plays a criti-\ncal role in long-context speculative decoding performance.\nIn our experiments, “Vanilla HF” refers to HuggingFace’s\nattention implementation, while “Vanilla FA” employs\nFlash Decoding. The latter demonstrates nearly a 2×\nspeedup over the former, even as a standalone component,\nand our method can achieve up to 6× speedup over HF\nAttention on code completion datasets.\nThis result un-\nderscores the necessity for speculative decoding methods\nto be compatible with optimized attention mechanisms\nlike Flash Decoding, especially in long-text settings.\nOur hybrid tree attention approach achieves this compat-\nibility, allowing us to fully leverage the advantages of\nFlash Decoding and further speedup.\nTable 2. Performance comparison with and without Anchor-Offset\nIndices on the Multi-News and RepoBench-P datasets. Models\nwith Anchor-Offset Indices achieve higher output speed and larger\naccept length, highlighting its efficiency and effectiveness.\nMulti-News\nRepoBench-P\nτ\nTokens/s\nτ\nTokens/s\nw/o Anchor-Offset\n3.20\n85.98\n3.26\n85.21\nw/ Anchor-Offset\n3.36\n91.11\n3.39\n91.28\n3.93×\nNo Anchor-Offset\nAnchor-Offset\nSteps\nLoss\n1200\n600\n300\n900\n150\n450\n750\n1050\n0\n6\n5\n4\n3\nFigure 4. Training loss curves on long-context data. Pretrained\nmodels with Anchor-Offset Indices exhibit lower initial and final\nloss, and reach the same loss level 3.93× faster compared to mod-\nels without Anchor-Offset Indices.\n4.3. Ablation Studies\nAnchor-Offset Indices. The experimental results demon-\nstrate the significant benefits of incorporating the Anchor-\nOffset Indices. Figure 4 shows that pretrained with Anchor-\nOffset Indices achieve a lower initial loss and final loss com-\npared to those trained without it when training over the real\nlong-context dataset. Notably, the initalization with Anchor-\nOffset Indices reaches the same loss level 3.93× faster than\nits counterpart. Table 2 further highlights the performance\nimprovements across two datasets, a summary dataset Multi-\nNews, and a code completion dataset RepoBench-P. Models\nwith Anchor-Offset Indices exhibit faster output speed and\nlarger average acceptance length τ. These results underscore\nthe effectiveness of Anchor-Offset Indices in enhancing both\ntraining efficiency and model performance.\nHybrid Tree Attention. The results presented in Figure\n5 highlight the effectiveness of the proposed Hybrid Tree\nAttention, which combines Flash Decoding with the\nTriton kernel fused mask attn. While the time spent\non the draft model forward pass and the target model FFN\ncomputations remain comparable across the two methods,\nthe hybrid approach exhibits a significant reduction in la-\ntency for the target model’s attention layer (the yellow part).\nSpecifically, the attention computation latency decreases\nfrom 49.92 ms in the HF implementation to 12.54 ms in\nthe hybrid approach, resulting in an approximately 75% im-\n7\n\nLongSpec: Long-Context Speculative Decoding with Efficient Drafting and Verification\n0\n10\n20\n30\n40\n50\n60\n70\n80\nlatency (ms)\nEAGLE\nHybrid\ndraft model forward\ntarget model attention\ntarget model FFN\nverification\nFigure 5. Latency breakdown for a single speculative decoding\nloop comparing the EAGLE implementation and the proposed\nHybrid Tree Attention. Significant latency reduction is observed\nin the target model’s attention layer (the yellow part) using our\napproach.\n2.25×\nTokens/s\n18.92\n42.63\n3.82×\n1.00\n3.82\nMean Accepted Tokens\nVanilla\nVanilla\nLongSpec\nLongSpec\nFigure 6. Performance of our method on the QwQ-32B model with\nthe AIME24 dataset, using a maximum output length of 32k tokens.\nThe left plot shows the tokens generated per second, where our\napproach achieves 2.25× higher speed compared to the baseline.\nThe right plot shows the mean number of accepted tokens, where\nour method achieves an average of 3.82 mean accepted tokens.\nprovement. The verification step time difference is minimal,\nfurther solidifying the conclusion that the primary perfor-\nmance gains stem from optimizing the attention mechanism.\n4.4. Long CoT Acceleration\nLong Chain-of-Thought (LongCoT) tasks have gained sig-\nnificant attention recently due to their ability to enable mod-\nels to perform complex reasoning and problem-solving over\nextended outputs (Qwen, 2024; OpenAI, 2024). In these\ntasks, while the prefix input is often relatively short, the\ngenerated output can be extremely long, posing unique chal-\nlenges in terms of efficiency and token acceptance. Our\nmethod is particularly well-suited for addressing these chal-\nlenges, effectively handling scenarios with long outputs. It\nis worth mentioning that MagicDec is not suitable for such\nlong-output scenarios because the initial inference stage of\nthe LongCoT task is not the same as the traditional long-\ncontext task. In LongCoT tasks, where the prefix is rela-\ntively short, the draft model in MagicDec will completely de-\ngrade into the target model, failing to achieve acceleration.\nWe evaluate our method on the QwQ-32B model using the\nwidely-used benchmark AIME24 dataset, with a maximum\noutput length set to 32k tokens. The results, illustrated in\n1\n2\n4\n8\nBatch Size\n100\n200\n300\n400\n500\nThroughput\nVanilla\nMagicDec\nLongSpec\nFigure 7. Throughput comparison of Vanilla, MagicDec, and\nLONGSPEC on RepoBench-P using Vicuna-7B across different\nbatch sizes. LONGSPEC shows superior throughput and scalability,\noutperforming both Vanilla and MagicDec in all batch sizes.\nFigure 6, demonstrate a significant improvement in both\ngeneration speed and mean accepted tokens. Specifically,\nour method achieved a generation rate of 42.63 tokens/s,\n2.25× higher than the baseline’s 18.92 tokens/s, and an\naverage of 3.82 mean accepted tokens. Notably, QwQ-\n32B with LONGSPEC achieves even lower latency than the\nvanilla 7B model with Flash Decoding, demonstrating\nthat our method effectively accelerates the LongCoT model.\nThese findings not only highlight the effectiveness of our\nmethod in the LongCoT task but also provide new insights\ninto lossless inference acceleration for the o1-like model.\nWe believe that speculative decoding will play a crucial role\nin accelerating this type of model in the future.\n4.5. Throughput\nThe throughput results of Vicuna-7B on the RepoBench-P\ndataset show that LONGSPEC consistently outperforms both\nVanilla and MagicDec across all batch sizes. At a batch\nsize of 8, LONGSPEC achieves a throughput of 561.32 to-\nkens/s, approximately 1.8× higher than MagicDec (310.58\ntokens/s) and nearly 2× higher than Vanilla (286.96 to-\nkens/s). MagicDec, designed with throughput optimization\nin mind, surpasses Vanilla as the batch size increases, re-\nflecting its targeted improvements. However, LONGSPEC\nstill sustains its advantage, maintaining superior throughput\nacross all tested batch sizes.\n5. Conclusion\nIn this paper, we propose LONGSPEC, a novel framework\ndesigned to enhance speculative decoding for long-context\nscenarios. Unlike previous speculative decoding methods\nthat primarily focus on short-context settings, LONGSPEC\ndirectly addresses three key challenges: excessive memory\noverhead, inadequate training for large position indices, and\ninefficient tree attention computation. To mitigate memory\nconstraints, we introduce an efficient draft model architec-\nture that maintains a constant memory footprint by lever-\n8\n\nLongSpec: Long-Context Speculative Decoding with Efficient Drafting and Verification\naging a combination of sliding window self-attention and\ncache-free cross-attention. To resolve the training limita-\ntions associated with short context data, we propose the\nAnchor-Offset Indices, ensuring that large positional in-\ndices are sufficiently trained even within short-sequence\ndatasets. Finally, we introduce Hybrid Tree Attention, which\nefficiently integrates tree-based speculative decoding with\nFlash Decoding. Extensive experiments demonstrate\nthe effectiveness of LONGSPEC in long-context understand-\ning tasks and real-world long reasoning tasks. Our findings\nhighlight the importance of designing speculative decod-\ning methods specifically tailored for long-context settings\nand pave the way for future research in efficient large-scale\nlanguage model inference.\nImpact Statements\nThis paper presents work whose goal is to advance the field\nof Machine Learning. There are many potential societal\nconsequences of our work, none which we feel must be\nspecifically highlighted here.\nReferences\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I.,\nAleman, F. L., Almeida, D., Altenschmidt, J., Altman, S.,\nAnadkat, S., et al. GPT-4 technical report. arXiv preprint\narXiv:2303.08774, 2023.\nAn, C., Zhang, J., Zhong, M., Li, L., Gong, S., Luo, Y., Xu,\nJ., and Kong, L. Why does the effective context length\nof llms fall short? In Proceedings of the International\nConference on Learning Representations, 2025.\nBai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du,\nZ., Liu, X., Zeng, A., Hou, L., Dong, Y., Tang, J., and\nLi, J. LongBench: A bilingual, multitask benchmark for\nlong context understanding. In Proceedings of the 62nd\nAnnual Meeting of the Association for Computational\nLinguistics, 2024.\nCai, T., Li, Y., Geng, Z., Peng, H., Lee, J. D., Chen, D.,\nand Dao, T. Medusa: Simple LLM inference acceleration\nframework with multiple decoding heads. In Proceedings\nof the International Conference on Machine Learning,\n2024.\nChen, J., Tiwari, V., Sadhukhan, R., Chen, Z., Shi, J., Yen,\nI. E.-H., and Chen, B. MagicDec: Breaking the latency-\nthroughput tradeoff for long context generation with spec-\nulative decoding. In Proceedings of the International\nConference on Learning Representations, 2025.\nChen, Z., Yang, X., Lin, J., Sun, C., Chang, K., and Huang,\nJ.\nCascade speculative drafting for even faster LLM\ninference. In Advances in Neural Information Processing\nSystems, 2024.\nChiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H.,\nZheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E., et al.\nVicuna: An open-source chatbot impressing GPT-4 with\n90% ChatGPT quality, 2023. URL https://vicuna.\nlmsys.org.\nDao, T. FlashAttention-2: Faster attention with better par-\nallelism and work partitioning. In Proceedings of the\nInternational Conference on Learning Representations,\n2024.\nDao,\nT.,\nHaziza,\nD.,\nMassa,\nF.,\nand\nSizov,\nG.\nFlash-Decoding\nfor\nlong-context\ninference,\n2023.\nURL https://crfm.stanford.edu/2023/10/\n12/flashdecoding.html.\nDeepseek. Deepseek’s api context caching on disk technol-\nogy, 2024. URL https://api-docs.deepseek.\ncom/guides/kv_cache.\nDu, C., Jiang, J., Yuanchen, X., Wu, J., Yu, S., Li, Y., Li, S.,\nXu, K., Nie, L., Tu, Z., and You, Y. Glide with a cape: A\nlow-hassle method to accelerate speculative decoding. In\nProceedings of the International Conference on Machine\nLearning, 2024.\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle,\nA., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan,\nA., et al. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783, 2024.\nElhoushi, M., Shrivastava, A., Liskovich, D., Hosmer, B.,\nWasti, B., Lai, L., Mahmoud, A., Acun, B., Agarwal, S.,\nRoman, A., Aly, A., Chen, B., and Wu, C.-J. LayerSkip:\nEnabling early exit inference and self-speculative decod-\ning. In Proceedings of the 62nd Annual Meeting of the\nAssociation for Computational Linguistics, 2024.\nFabbri, A. R., Li, I., She, T., Li, S., and Radev, D. Multi-\nNews: A large-scale multi-document summarization\ndataset and abstractive hierarchical model. In Proceed-\nings of the 57th Annual Meeting of the Association for\nComputational Linguistics, 2019.\nFu, Y., Bailis, P., Stoica, I., and Zhang, H. Break the se-\nquential dependency of LLM inference using lookahead\ndecoding. In Proceedings of the International Conference\non Machine Learning, 2024.\nGao, T., Wettig, A., Yen, H., and Chen, D. How to train long-\ncontext language models (effectively). arXiv preprint\narXiv:2410.02660, 2024.\nGoogle.\nGemini API context caching feature, 2024.\nURL https://ai.google.dev/gemini-api/\ndocs/caching.\n9\n\nLongSpec: Long-Context Speculative Decoding with Efficient Drafting and Verification\nGuo, D., Xu, C., Duan, N., Yin, J., and McAuley, J. Long-\ncoder: A long-range pre-trained language model for code\ncompletion. In Proceedings of the International Confer-\nence on Machine Learning, 2023.\nHe, Z., Zhong, Z., Cai, T., Lee, J., and He, D. REST:\nRetrieval-based speculative decoding. In Proceedings\nof the Conference of the North American Chapter of the\nAssociation for Computational Linguistics, 2024.\nHsu, P.-L., Dai, Y., Kothapalli, V., Song, Q., Tang, S., Zhu,\nS., Shimizu, S., Sahni, S., Ning, H., and Chen, Y. Liger\nkernel: Efficient triton kernels for LLM training. arXiv\npreprint arXiv:2410.10989, 2024.\nHuang, L., Cao, S., Parulian, N., Ji, H., and Wang, L. Ef-\nficient attentions for long document summarization. In\nProceedings of the Conference of the North American\nChapter of the Association for Computational Linguistics,\n2021.\nKim, S., Mangalam, K., Moon, S., Malik, J., Mahoney,\nM. W., Gholami, A., and Keutzer, K. Speculative de-\ncoding with big little decoder. In Advances in Neural\nInformation Processing Systems, 2024.\nKingma, D. P. and Ba, J. L. Adam: A method for stochastic\ngradient descent. In Proceedings of the International\nConference on Learning Representations, 2015.\nLeviathan, Y., Kalman, M., and Matias, Y. Fast inference\nfrom transformers via speculative decoding. In Proceed-\nings of the International Conference on Machine Learn-\ning, 2023.\nLi, D., Shao, R., Xie, A., Sheng, Y., Zheng, L., Gon-\nzalez, J. E., Stoica, I., Ma, X., and Zhang, H.\nHow\nlong can open-source llms truly promise on context\nlength?, 2023. URL https://lmsys.org/blog/\n2023-06-29-longchat.\nLi, Y., Wei, F., Zhang, C., and Zhang, H. Eagle: Specula-\ntive sampling requires rethinking feature uncertainty. In\nProceedings of the International Conference on Machine\nLearning, 2024.\nLiu, F., Tang, Y., Liu, Z., Ni, Y., Tang, D., Han, K., and\nWang, Y. Kangaroo: Lossless self-speculative decoding\nfor accelerating LLMs via double early exiting. In Ad-\nvances in Neural Information Processing Systems, 2024a.\nLiu, T., Xu, C., and McAuley, J. RepoBench: Benchmark-\ning repository-level code auto-completion systems. In\nProceedings of the International Conference on Learning\nRepresentations, 2024b.\nLiu, X., Hu, L., Bailis, P., Cheung, A., Deng, Z., Stoica,\nI., and Zhang, H. Online speculative decoding. In Pro-\nceedings of the International Conference on Machine\nLearning, 2024c.\nLiu, X., Yan, H., An, C., Qiu, X., and Lin, D. Scaling\nlaws of roPE-based extrapolation. In Proceedings of the\nInternational Conference on Learning Representations,\n2024d.\nLoshchilov, I. and Hutter, F. SGDR: Stochastic gradient\ndescent with warm restarts. In Proceedings of the Inter-\nnational Conference on Learning Representations, 2017.\nMiao, X., Oliaro, G., Zhang, Z., Cheng, X., Wang, Z.,\nZhang, Z., Wong, R. Y. Y., Zhu, A., Yang, L., Shi, X.,\nShi, C., Chen, Z., Arfeen, D., Abhyankar, R., and Jia,\nZ. Specinfer: Accelerating large language model serving\nwith tree-based speculative inference and verification. In\nProceedings of the 29th ACM International Conference\non Architectural Support for Programming Languages\nand Operating Systems, 2024.\nNumina, P.\nAIMO validation AIME, 2024.\nURL\nhttps://huggingface.co/datasets/\nAI-MO/aimo-validation-aime.\nOpenAI.\nLearning to reason with llms,\nSeptem-\nber 2024.\nURL https://openai.com/index/\nlearning-to-reason-with-llms/.\nPeng, B., Quesnelle, J., Fan, H., and Shippole, E. YaRN:\nEfficient context window extension of large language\nmodels. In Proceedings of the International Conference\non Learning Representations, 2024.\nQwen.\nQwQ: Reflect deeply on the boundaries of the\nunknown, November 2024. URL https://qwenlm.\ngithub.io/blog/qwq-32b-preview/.\nRasley, J., Rajbhandari, S., Ruwase, O., and He, Y. Deep-\nspeed: System optimizations enable training deep learn-\ning models with over 100 billion parameters. In Proceed-\nings of the 26th ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining, 2020.\nSoboleva, D., Al-Khateeb, F., Myers, R., Steeves, J. R.,\nHestness, J., and Dey, N.\nSlimPajama:\nA 627b\ntoken cleaned and deduplicated version of redpa-\njama, 2023.\nURL https://huggingface.co/\ndatasets/cerebras/SlimPajama-627B.\nSu, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y.\nRoformer: Enhanced transformer with rotary position\nembedding. Neurocomputing, 2024.\nSun, H., Chen, Z., Yang, X., Tian, Y., and Chen, B. TriForce:\nLossless acceleration of long sequence generation with\n10\n\nLongSpec: Long-Context Speculative Decoding with Efficient Drafting and Verification\nhierarchical speculative decoding. In Proceedings of the\nFirst Conference on Language Modeling, 2024.\nSun, Z., Suresh, A. T., Ro, J. H., Beirami, A., Jain, H., and\nYu, F. X. Spectr: Fast speculative decoding via optimal\ntransport. In Advances in Neural Information Processing\nSystems, 2023.\nXia, H., Ge, T., Wang, P., Chen, S.-Q., Wei, F., and Sui,\nZ. Speculative decoding: Exploiting speculative execu-\ntion for accelerating seq2seq generation. In Findings of\nthe Association for Computational Linguistics: EMNLP\n2023, 2023.\nXiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Effi-\ncient streaming language models with attention sinks. In\nProceedings of the International Conference on Learning\nRepresentations, 2024.\nZhang, F., Liu, B., Wang, K., Tan, V., Yang, Z., and Wang,\nZ. Relational reasoning via set transformers: Provable\nefficiency and applications to marl. Advances in Neural\nInformation Processing Systems, 35:35825–35838, 2022.\nZhang, J., Wang, J., Li, H., Shou, L., Chen, K., Chen, G.,\nand Mehrotra, S. Draft& verify: Lossless large language\nmodel acceleration via self-speculative decoding. In Pro-\nceedings of the 62nd Annual Meeting of the Association\nfor Computational Linguistics, 2024.\nZhang, Y., Zhang, F., Yang, Z., and Wang, Z. What and\nhow does in-context learning learn? Bayesian model\naveraging, parameterization, and generalization. arXiv\npreprint arXiv:2305.19420, 2023.\nZhao, W., Huang, Y., Han, X., Xu, W., Xiao, C., Zhang, X.,\nFang, Y., Zhang, K., Liu, Z., and Sun, M. Ouroboros:\nGenerating longer drafts phrase by phrase for faster specu-\nlative decoding. In Proceedings of the Conference on Em-\npirical Methods in Natural Language Processing, 2024.\nZhong, M., Yin, D., Yu, T., Zaidi, A., Mutuma, M., Jha, R.,\nHassan, A., Celikyilmaz, A., Liu, Y., Qiu, X., et al. QM-\nSum: A new benchmark for query-based multi-domain\nmeeting summarization. In Proceedings of the Confer-\nence of the North American Chapter of the Association\nfor Computational Linguistics, 2021.\n11\n\nLongSpec: Long-Context Speculative Decoding with Efficient Drafting and Verification\nA. Correctness for Attention Aggregation\nBecause the query matrix Q can be decomposed into several rows, each representing a separate query q, we can only\nconsider the output of each row’s q after calculating attention with KV. In this way, we can assume that the KV involved in\nthe calculation has undergone the tree mask, which can simplify our proof. We only need to prove that the output o obtained\nfrom each individual q meets the requirements, which can indicate that the overall output O of the entire matrix Q also\nmeets the requirements.\nProposition A.1. Denote the log-sum-exp of the merged attention as follows:\nLSEmerge = log\n\u0010\nexp\n\u0000LSEcache\n\u0001\n+ exp\n\u0000LSEspecs\n\u0001\u0011\n,\nThen we can write the merged attention output in the following way:\nomerge = ocache · exp\n\u0000LSEcache −LSEmerge\n\u0001\n+ ospecs · exp\n\u0000LSEspecs −LSEmerge\n\u0001\n.\nProof. A standard scaled dot-product attention for q (of size dqk) attending to Kmerge and Vmerge (together of size\n(M + N) × dqk and (M + N) × dv respectively) can be written as:\nomerge = mha (q, Kmerge, Vmerge) = softmax\n\u0010\nqK⊤\nmerge/\np\ndqk\n\u0011\nVmerge.\nBecause K and V are formed by stacking (Kspecs, Kcache) and (Vspecs, Vcache), we split the logit matrix accordingly:\nqK⊤\nmerge/\np\ndqk = concat\n\u0010\nq K⊤\ncache/\np\ndqk\n|\n{z\n}\nsub−logitsforhistory\n, q K⊤\nspecs/\np\ndqk\n|\n{z\n}\nsub−logitsfornew\n\u0011\n.\nDenote these sub-logit matrices as:\nZcache = q K⊤\ncache/\np\ndqk, Zspecs = q K⊤\nspecs/\np\ndqk.\nEach row i of Zspecs corresponds to the dot products between the i-th query in q and all rows in Kspecs, while rows of\nZcache correspond to the same query but with Kcache.\nIn order to combine partial attentions, we keep track of the log of the sum of exponentials of each sub-logit set. Concretely,\ndefine:\nLSEcache = log\n\u0012XN\nj=1 exp\n\u0010\nZ(j)\ncache\n\u0011\u0013\n, LSEspecs = log\n\u0012XM\nj=1 exp\n\u0010\nZ(j)\nspecs\n\u0011\u0013\n,\n(1)\nwhere Z(j)\nspecs denotes the logit for the j-th element, and similarly for Z(j)\ncache.\nThen ocache and ospecs can be written as:\nocache =\nPN\nj=1 exp\n\u0010\nZ(j)\ncache\n\u0011\nV (j)\ncache\nexp (LSEcache)\n, ospecs =\nPM\nj=1 exp\n\u0010\nZ(j)\nspecs\n\u0011\nV (j)\nspecs\nexp (LSEspecs)\n.\n(2)\nAnd the whole attention score can be written as:\nomerge =\nPN\nj=1 exp\n\u0010\nZ(j)\ncache\n\u0011\nV (j)\ncache + PM\nj=1 exp\n\u0010\nZ(j)\nspecs\n\u0011\nV (j)\nspecs\nexp (LSEcache) + exp (LSEspecs)\n.\n(3)\nBy aggregating Equation 2 into Equation 3, we can get the following equation:\nomerge = ocache · exp\n\u0000LSEcache −LSEmerge\n\u0001\n+ ospecs · exp\n\u0000LSEspecs −LSEmerge\n\u0001\n.\n(4)\n12\n\nLongSpec: Long-Context Speculative Decoding with Efficient Drafting and Verification\nB. Experiments Details\nAll models are trained using eight A100 80GB GPUs. For the 7B, 8B, and 13B target models trained on short-context data,\nwe employ LONGSPEC with ZeRO-1 (Rasley et al., 2020). For the 7B, 8B, and 13B models trained on long-context data, as\nwell as for all settings of the 33B target models, we utilize ZeRO-3.\nFor the SlimPajama-6B dataset, we configure the batch size (including accumulation) to 2048, set the maximum learning\nrate to 5e-4 with a cosine learning rate schedule (Loshchilov & Hutter, 2017), and optimize the draft model using\nAdamW (Kingma & Ba, 2015). When training on long-context datasets, we adopt a batch size of 256 and a maximum\nlearning rate of 5e-6. The draft model is trained for only one epoch on all datasets.\nIt is important to note that the primary computational cost arises from forwarding the target model to obtain the KV cache.\nRecently, some companies have introduced a service known as context caching (Deepseek, 2024; Google, 2024), which\ninvolves storing large volumes of KV cache. Consequently, in real-world deployment, these pre-stored KV caches can be\ndirectly utilized as training data, significantly accelerating the training process.\nFor the tree decoding of LONGSPEC, we employ dynamic beam search to construct the tree. Previous studies have shown that\nbeam search, while achieving high acceptance rates, suffers from slow processing speed in speculative decoding (Du et al.,\n2024). Our research identifies that this slowdown is primarily caused by KV cache movement. In traditional beam search,\nnodes that do not fall within the top-k likelihood are discarded, a step that necessitates KV cache movement. However,\nin speculative decoding, discarding these nodes is unnecessary, as draft sequences are not required to maintain uniform\nlengths. Instead, we can simply halt the computation of descendant nodes for low-likelihood branches without removing\nthem entirely. By adopting this approach, beam search attains strong performance without excessive computational overhead.\nIn our experiments, the beam width is set to [4, 16, 16, 16, 16] for each speculation step. All inference experiments in this\nstudy are conducted using float16 precision on a single A100 80GB GPU.\nC. Notation\nFor a positive integer N ∈N, we define the set [N] = {1, · · · , N}. For a vector x ∈Rd, we adopt ∥· ∥p to denote the ℓp\nnorm of vectors. For a matrix X = [x⊤\n1 , · · · , x⊤\nd1]⊤∈Rd1×d2, where xi ∈Rd2 for i = 1, · · · , d1, we define the ℓp,q-norm\nof X as ∥X∥p,q = ∥[∥x1∥p, · · · , ∥xd1∥p]∥q The Frobenius norm ∥· ∥2,2 is denoted as ∥· ∥F. We write x ≲y to mean\nx ≤Cy for an absolute constant C > 0.\nD. Theoretical Analysis\nIn this section, we provide the theoretical analysis of our methods. We begin with the definition of our Glide network. It\nconsists of three modules: the self-attention module, the cross-attention module, and the Feed-Forward (FF) module. Here\nthe self-attention and the cross-attention are both based on the attention module. For a query q ∈R1×d and N KV pairs\nK, V ∈RN×d, the attention module calculate the output as\nattn(q, K, V, {WQ, WK, WV }) = softmax\n\u0010\nRoPE\n\u0000LN(q)WQ\n\u0001\nRoPE\n\u0000LN(K)WK\n\u0001⊤\u0011\nV WV ,\n(5)\nwhere softmax is the softmax operator, WQ, WK, WV ∈Rd×d are the weight matrices, RoPE is the rotary positional\nembedding function that appiles RoPE on inputs, and LN is the row-wise normalization of the input, which is defined as\nLN(x) =\n\u001a\nx\nif ∥x∥2 ≤1\nx/∥x∥2\notherwise.\nCompared to the implementation of attention in PyTorch, we merge the weight matrix WO into WV here for ease of notation.\nOur results can be easily generalized to the formulation that explicitly parameterizes WO. The Multi-Head Attention (MHA)\nwith H heads is the combination of H attention, i.e.,\nmha(q, K, V, {W h\nQ, W h\nK, W h\nV }H\nh=1) =\nH\nX\nh=1\nsoftmax\n\u0000RoPE(LN(q)W h\nQ)RoPE(LN(K)W h\nK)⊤\u0001\nV W h\nV ,\n(6)\n13\n\nLongSpec: Long-Context Speculative Decoding with Efficient Drafting and Verification\nwhere {W h\nQ, W h\nK, W h\nV }H\nh=1 are the weights of all the attention heads. The self attention module generates the output for a\nquery x according to the KV pairs that include x and other tokens, i.e., K, V are defined as\nK = V = [x⊤\n1 , · · · , x⊤\nT , x⊤]⊤= [X⊤\nT , x⊤]⊤,\nwhere xi ∈R1×d for i ∈[N] are the input vectors prior to x. In contrast, the cross-attention generates outputs for x\naccording to the KV pairs that excludes x, i.e., K = V = X′\nN. The FF module process an input vector x ∈Rd as\nffn(x, WA,1, WA,2) = σ\n\u0000LN(x)WA,1\n\u0001\nWA,2,\n(7)\nwhere WA,1, WA,2 ∈Rd×d are weights of FF module, and σ(·) is an element-wise activation function. For example, the\nactivation function σ can be ReLU and sigmoid. In the following, we will omit the parameters of each module for ease of\nnotation. The Glide function, denoted as Gθ, is defined as\nGθ(q, X, X′) = ffn\n\u0010\nmha\n\u0000mha(q, [X⊤, q⊤]⊤, [X⊤, q⊤]⊤), X′, X′\u0001\u0011\nWunemb,\n(8)\nwhere X is the embeddings of all the tokens prior to q, X′ are the hidden states of large models, Wunemb ∈Rd×dV is the\nunembedding matrix (dV is the alphabet size of the tokens), and θ denotes the parameters of all these three modules and\nWunemb, i.e.,\nθ =\n\u0000{W h,(1)\nQ\n, W h,(1)\nK\n, W h,(1)\nV\n}H\nh=1, {W h,(2)\nQ\n, W h,,(2)\nK\n, W h,,(2)\nV\n}H\nh=1, WA,1, WA,2, Wunemb\n\u0001\n.\nHere the superscripts (1) and (2) denote the index of the layer. We denote all the plausible parameter configurations as Θ as\nΘ =\nn\nθ\n\f\f\f max\nh,i\n\b\n∥W h,(i)\nQ\n∥F, ∥W h,(i)\nK\n∥F, ∥W h,(i)\nV\n∥F\n\t\n≤B, ∥WA,1∥F ≤B, ∥WA,2∥F ≤B, ∥Wunemb∥1,2 ≤B\no\n.\nIn the following, we would like to study the error analysis of our method. In fact, we need to define a variant of this function,\nwhich contains two modifications. The first modification is the positional embedding. Instead of using the positional\nembeddings corresponding to the continuous positions, we offset the positions of the tokens with position index larger than 4\njointly to t ∈N. We denote such positional embedding with RoPEs,t. The corresponding mha is denoted as mhas,t. Here\nwe note that mha = mhas,0, i.e., there is no position offset in the original attention module. The second modification is that\nwe truncate X to a sliding window, which is detailed discussed in Section 3.1. We denote the truncated version of X as Xw.\nThen our proposed training method is to train the following function\nGs,t\nθ (q, Xw, X′) = ffn\n\u0010\nmhas,t\u0000mhas,t(q, [Xw,⊤, q⊤]⊤, [Xw,⊤, q⊤]⊤), X′, X′\u0001\u0011\nWunemb.\n(9)\nWe assume that the glide function is trained on a dataset with N i.i.d. samples of a distribution P, i.e., DN =\n{qi, Xi, X′\ni, Id∗\ni }N\ni=1 ∼P, where Id∗\ni is the vocabulary index of true next token for i-th sample. During the training\nprocess, the position offsets Dt\nN = {ti}N\ni=1 are i.i.d. samples of a distribution ˜Pt. Then we define the Maximum Likelihood\nEstimate (MLE) ˆθ as\nˆθ = argminθ∈Θ −EDN,Dt\nN\nh\nlog softmax\n\u0000Gs,t\nθ (q, Xw, X′)\n\u0001\nId∗\ni\n= argminθ∈Θ −1\nN\nN\nX\ni=1\nlog softmax\n\u0000Gs,ti\nθ\n(qi, Xw\ni , X′\ni)\n\u0001\nId∗\ni ,\nwhere ED denotes the expectation with respect to the empirical distribution induced by DN, and ˜Pt is the distribution of the\nposition offset. After the training process, we will inference according to the following function\nGˆθ(q, Xw, X′) = ffn\n\u0010\nmha\n\u0000mha(q, [Xw,⊤, q⊤]⊤, [Xw,⊤, q⊤]⊤), X′, X′\u0001\u0011\nWunemb,\ni.e., we do not delibrately shfit the token positions in the inference. To analyze the performance loss due to our method, we\nthen define the optimal parameters of the original Glide function. Here optimality means that we have infinite number of\ntraining data points, i.e., the expectation is taken with respect to the true distribution P instead of the empirical distribution\ninduced by the dataset.\nθ∗= argminθ∈Θ −EP\nh\nlog softmax\n\u0000Gθ(q, X, X′)\n\u0001\nId∗\ni\n.\n14\n\nLongSpec: Long-Context Speculative Decoding with Efficient Drafting and Verification\nAssumption D.1 (Concentration of Window Attention Score). For any (q, X) on the support of P and the optimal parameter\nθ∗, the sum of the attention scores of the tokens in X that are not included in Xw is upper bounded by ε at the first layer.\nIntuitively, we note that this assumption largely holds when the answer for the query is included in the window we use. It\nmeans that the glide function does not need to gather information outside the window.\nAssumption D.2 (Boundness of Inputs). For any (q, X, X′) on the support of P, we have that ∥q∥2 ≤BX, ∥X∥2,∞≤BX,\nand ∥X′∥2,∞≤BX.\nThis assumption always holds in the realistic setting, since all the inputs are stored in the computer, which can only represent\nfinite numbers. For the prompt distribution P, we denote the distribution of the position index tW of the starting token in the\nwindow as Pt.\nAssumption D.3. The content in the window Xw is independent of its starting index tw for distribution P.\nThis assumption states that the contents and its absolute position in the context are independent. This generally holds in the\nlong context, since the position of a sentence can hardly imply the content of this sentence in the long context.\nTheorem D.4. When the glide function is trained on a dataset with N i.i.d. samples, under Assumptions D.1, D.2, and D,\nthe gap between the population inference loss of the MLE from our training method and that of the optimal parameter can\nbe upper bounded as\nEP\nh\nlog softmax\n\u0000Gθ∗(q, X, X′)\n\u0001\nId∗\ni\n−EP\nh\nlog softmax\n\u0000Gˆθ(q, Xw, X′)\n\u0001\nId∗\ni\n≲(1 + dV exp(B))HB4(1 + B2\nXB2)ε\n|\n{z\n}\nErr. From Using Windows\n+ log(1 + dV exp(B)) · TV( ˜Pt, Pt)\n|\n{z\n}\nPositional Embedding Distribution Shift\n+\n1\n√\nN\nh\nd(d + dV ) log(1 + NHB6) + log 1\nδ\ni\n|\n{z\n}\nGeneralization Err.\nwith probability at least 1 −δ.\nHere we can see that the error consists of three components. The first one results from that we adopt a sliding window\ninstead of using all tokens. This error is proportional to the attention score outside the window. The second one results from\nthe positinoal embedding distributional shift. In our experiments, we set ˜Pt as the uniform distribution, which will have\nsmaller distribution shift than not adopting this position offse technique. The last term results from that we use N samples\nto train the model.\nProof of Theorem D.4. The proof takes three steps.\n• Decompose the performance error.\n• Bound each term in the decomposition.\n• Conclude the proof.\nStep 1: Decompose the performance error.\nBefore the detailed decomposition, we would like to define optimal parameters of the modified Glide function as follows.\n˜θs,∗= argminθ∈Θ −EP, ˜\nPt\nh\nlog softmax\n\u0000Gs,t\nθ (q, Xw, X′)\n\u0001\nId∗\ni\n˜θ∗= argminθ∈Θ −EP\nh\nlog softmax\n\u0000Gθ(q, Xw, X′)\n\u0001\nId∗\ni\n,\nwhere ˜θs,∗is the optimal parameter that considers both the position offse and inputs truncation, and ˜θ∗is the optimal\n15\n\nLongSpec: Long-Context Speculative Decoding with Efficient Drafting and Verification\nparameter that only consider the input truncation.\nEP\nh\nlog softmax\n\u0000Gθ∗(q, X, X′)\n\u0001\nId∗\ni\n−EP\nh\nlog softmax\n\u0000Gˆθ(q, Xw, X′)\n\u0001\nId∗\ni\n= EP\nh\nlog softmax\n\u0000Gθ∗(q, X, X′)\n\u0001\nId∗\ni\n−EP\nh\nlog softmax\n\u0000G˜θ∗(q, Xw, X′)\n\u0001\nId∗\ni\n|\n{z\n}\napproximation error\n+ EP\nh\nlog softmax\n\u0000G˜θ∗(q, Xw, X′)\n\u0001\nId∗\ni\n−EP, ˜\nPt\nh\nlog softmax\n\u0000Gs,t\n˜θ∗(q, Xw, X′)\n\u0001\nId∗\ni\n|\n{z\n}\npositional embedding distribution shift\n+ EP, ˜\nPt\nh\nlog softmax\n\u0000Gs,t\n˜θ∗(q, Xw, X′)\n\u0001\nId∗\ni\n−EP, ˜\nPt\nh\nlog softmax\n\u0000Gs,t\n˜θs,∗(q, Xw, X′)\n\u0001\nId∗\ni\n|\n{z\n}\noptimization error\n+ EP, ˜\nPt\nh\nlog softmax\n\u0000Gs,t\n˜θs,∗(q, Xw, X′)\n\u0001\nId∗\ni\n−EDN,Dt\nN\nh\nlog softmax\n\u0000Gs,t\n˜θs,∗(q, Xw, X′)\n\u0001\nId∗\ni\n|\n{z\n}\ngeneralization error\n+ EDN,Dt\nN\nh\nlog softmax\n\u0000Gs,t\n˜θs,∗(q, Xw, X′)\n\u0001\nId∗\ni\n−EDN,Dt\nN\nh\nlog softmax\n\u0000Gs,t\nˆθ (q, Xw, X′)\n\u0001\nId∗\ni\n|\n{z\n}\noptimization error\n+ EDN,Dt\nN\nh\nlog softmax\n\u0000Gs,t\nˆθ (q, Xw, X′)\n\u0001\nId∗\ni\n−EP, ˜\nPt\nh\nlog softmax\n\u0000Gs,t\nˆθ (q, Xw, X′)\n\u0001\nId∗\ni\n|\n{z\n}\ngeneration error\n+ EP, ˜\nPt\nh\nlog softmax\n\u0000Gs,t\nˆθ (q, Xw, X′)\n\u0001\nId∗\ni\n−EP\nh\nlog softmax\n\u0000Gˆθ(q, Xw, X′)\n\u0001\nId∗\ni\n|\n{z\n}\npositional embedding distribution shift\n≤EP\nh\nlog softmax\n\u0000Gθ∗(q, X, X′)\n\u0001\nId∗\ni\n−EP\nh\nlog softmax\n\u0000G˜θ∗(q, Xw, X′)\n\u0001\nId∗\ni\n|\n{z\n}\napproximation error\n+ 2 max\nθ∈Θ\n\f\f\f\fEP\nh\nlog softmax\n\u0000Gθ(q, Xw, X′)\n\u0001\nId∗\ni\n−EP, ˜\nPt\nh\nlog softmax\n\u0000Gs,t\nθ (q, Xw, X′)\n\u0001\nId∗\ni\f\f\f\f\n|\n{z\n}\npositional embedding distribution shift\n+ 2 max\nθ∈Θ\n\f\f\f\fEDN,Dt\nN\nh\nlog softmax\n\u0000Gs,t\nθ (q, Xw, X′)\n\u0001\nId∗\ni\n−EP, ˜\nPt\nh\nlog softmax\n\u0000Gs,t\nθ (q, Xw, X′)\n\u0001\nId∗\ni\f\f\f\f\n|\n{z\n}\ngeneration error\n,\n(10)\nwhere the inequality follows from the fact that all the optimization error is less and equal to 0 according to the definitions.\nStep 2: Bound each term in the decomposition.\nThen we would like to separately upper bound the three kinds of error derived in the first step. Before calculating the\nupper bounds, we note that softmax\n\u0000Gθ(q, X, X′)\n\u0001\nId∗≥(1 + dV exp(B))−1 for any θ ∈Θ due to Lemma E.3. For the\napproximation error, we have that\nEP\nh\nlog softmax\n\u0000Gθ∗(q, X, X′)\n\u0001\nId∗\ni\n−EP\nh\nlog softmax\n\u0000G˜θ∗(q, Xw, X′)\n\u0001\nId∗\ni\n≤EP\nh\nlog softmax\n\u0000Gθ∗(q, X, X′)\n\u0001\nId∗\ni\n−EP\nh\nlog softmax\n\u0000Gθ∗(q, Xw, X′)\n\u0001\nId∗\ni\n≲(1 + dV exp(B))HB4(1 + B2\nXB2)ε,\nwhere the first inequality results from the definition of ˜θ∗, and the second inequality results from Lemmas E.2 and E.1. For\nthe positional embedding distribution shift, we have that\nmax\nθ∈Θ\n\f\f\f\fEP\nh\nlog softmax\n\u0000Gθ(q, Xw, X′)\n\u0001\nId∗\ni\n−EP, ˜\nPt\nh\nlog softmax\n\u0000Gs,t\nθ (q, Xw, X′)\n\u0001\nId∗\ni\f\f\f\f\n≤log(1 + dV exp(B)) TV( ˜Pt, Pt),\n16\n\nLongSpec: Long-Context Speculative Decoding with Efficient Drafting and Verification\nwhere the inequality results from the definition of the total variation. For the generalization error, we would like to apply\nTheorem 4.3 of (Zhang et al., 2023). In fact, we have that with probability at least 1 −δ, the following inequality holds.\nmax\nθ∈Θ\n\f\f\f\fEDN,Dt\nN\nh\nlog softmax\n\u0000Gs,t\nθ (q, Xw, X′)\n\u0001\nId∗\ni\n−EP, ˜\nPt\nh\nlog softmax\n\u0000Gs,t\nθ (q, Xw, X′)\n\u0001\nId∗\ni\f\f\f\f\n≲\n1\n√\nN\nh\nd(d + dV ) log(1 + NHB6) + log 1\nδ\ni\n.\nStep 3: Conclude the proof.\nCombining all the results in steps 1 and 2, we have that\nEP\nh\nlog softmax\n\u0000Gθ∗(q, X, X′)\n\u0001\nId∗\ni\n−EP\nh\nlog softmax\n\u0000Gˆθ(q, Xw, X′)\n\u0001\nId∗\ni\n≲(1 + dV exp(B))HB4(1 + B2\nXB2)ε + log(1 + dV exp(B)) · TV( ˜Pt, Pt)\n+\n1\n√\nN\nh\nd(d + dV ) log(1 + NHB6) + log 1\nδ\ni\n.\nE. Supporting Lemmas\nProposition E.1 (Proposition 11.1 in (Zhang et al., 2023)). For any x, ˜x ∈Rd, A1, ˜A1 ∈Rd×dF , and A2, ˜A2 ∈RdF ×d, we\nhave that\n\r\rffn(x, A) −ffn(˜x, ˜A)\n\r\r\n2\n≤∥A1∥F · ∥A2∥F · ∥x −˜x∥2 + ∥A1 −˜A1∥F · ∥A2∥F · ∥˜x∥2 + ∥˜A1∥F · ∥A2 −˜A2∥F · ∥˜x∥2.\nLemma E.2 (Lemma I.8 in (Zhang et al., 2023)). For any X, ˜X ∈RN×d, and any WQ,h, WK,h ∈Rd×dh, WV,h ∈Rd×d\nfor h ∈[H] , if ∥X∥2,∞, ∥˜X∥2,∞≤BX, ∥WQ,h∥F ≤BQ, ∥WK,h∥F, ≤BK, ∥WV,h∥F ≤BV for h ∈[H], then we have\n\r\r\rmha\n\u0000X, {WQ,h, WK,h, WV,h}H\nh=1) −mha( ˜X, {WQ,h, WK,h, WV,h}H\nh=1\n\u0001\r\r\r\n2,∞\n≤H · BV\n\u00001 + 4B2\nX · BQBK\n\u0001\n∥X −˜X∥2,∞.\nLemma E.3 (Lemma 17 in (Zhang et al., 2022) ). Given any two conjugate numbers u, v ∈[1, ∞], i.e., 1\nu + 1\nv = 1, and\n1 ≤p ≤∞, for any A ∈Rr×c and x ∈Rc, we have\n∥Ax∥p ≤∥A⊤∥p,u∥x∥v\nand\n∥Ax∥p ≤∥A∥u,p∥x∥v.\nLemma E.4. For a query vector q ∈Rd, and two sets of key-value pairs K1 ∈RN1×d, K2 ∈RN2×d, V1 ∈RN1×d, and\nV2 ∈RN2×d, We define attention scores softmax(q⊤[K1, K2]⊤) and softmax(q⊤K⊤\n1 ) as\nsoftmax(q⊤[K1, K2]⊤) = [s⊤\n1 , s⊤\n2 ], and softmax(q⊤K⊤\n1 ) = ˜s⊤\n1 .\nThen we have that\n\r\rsoftmax(q⊤K⊤\n1 )V1 −softmax(q⊤[K1, K2]⊤)[V ⊤\n1 , V ⊤\n2 ]⊤\r\r\n2 ≤2∥s2∥1 · max{∥V1∥2,∞, ∥V2∥2,∞}.\nProof of Lemma E.4. In fact, we have that\nsoftmax(q⊤[K1, K2]⊤)[V ⊤\n1 , V ⊤\n2 ]⊤= s⊤\n1 V1 + s⊤\n2 V2, and softmax(q⊤K⊤\n1 )V1 = ˜s⊤\n1 V1.\nFurther, the difference between s1 and ˜s1 can be upper bounded as\n∥s1 −˜s1∥1\n=\nN1\nX\ni=1\nexp(q⊤[K1]i,:) PN2\nl=1 exp(q⊤[K2]l,:)\n\u0000 PN1\nj=1 exp(q⊤[K1]j,:) + PN2\nl=1 exp(q⊤[K2]l,:)\n\u0001 PN1\nj=1 exp(q⊤[K1]j,:)\n= ∥s2∥1,\n17\n\nLongSpec: Long-Context Speculative Decoding with Efficient Drafting and Verification\nwhere the equalities result from the definitions of softmax(·) and s2. Then we have that\n\r\rsoftmax(q⊤K⊤\n1 )V1 −softmax(q⊤[K1, K2]⊤)[V ⊤\n1 , V ⊤\n2 ]⊤\r\r\n2\n=\n\r\rs⊤\n1 V1 + s⊤\n2 V2 −˜s⊤\n1 V1\n\r\r\n2\n≤∥s1 −˜s1∥1 · ∥V1∥2,∞+ ∥s2∥1 · ∥V2∥2,∞\n≤2∥s2∥1 · max{∥V1∥2,∞, ∥V2∥2,∞}.\nThus, we conclude the proof of Lemma E.4.\nF. Case Study\nHere we display some illustrative cases from GovReport, where tokens marked in blue indicate draft tokens accepted by the\ntarget model.\nThe Congressional Gold Medal is a prestigious award given by the United States Congress to individuals and groups\nin recognition of their distinguished contributions, achievements, and services to the country. The tradition of\nawarding gold medals dates back to the late 18th century, and it has been used to honor a wide range of individuals,\nincluding military leaders, scientists, artists, and humanitarians.\nThe first Congressional Gold Medals were issued by the Continental Congress in the late 1700s, and since then,\nCongress has awarded over 2,000 medals to various individuals and groups. The awarding of the Congressional\nGold Medal is not a permanent statutory provision, and it is typically done through special legislation.\nThe process of awarding the Congressional Gold Medal involves several steps, including the introduction of\nlegislation, the consideration of the legislation by the relevant committees, and the approval of the legislation by\nboth the House of Representatives and the Senate. Once the legislation is approved, the Secretary of the Treasury is\nresponsible for striking the medal, which is then presented\nThe design of the Congressional Gold Medal is typically determined by the Secretary of the Treasury, in consultation\nwith the Citizens Coinage Advisory Committee and the Commission of Fine Arts. The medal typically features a\nportrait of the recipient, as well as inscriptions and symbols that reflect the recipient’s achievements and contributions.\nThe Congressional Gold Medal is considered one of the highest civilian honors in the United States, and it is often\ngiven to individuals who have made significant contributions to their field or to the country as a whole. The award\nhas been given to a wide range of individuals, including military heroes, civil rights leaders, and artists.\nIn recent years, the number of Congressional Gold Medals awarded has increased, with over 50 bills introduced in\nthe 113th Congress alone. The award has also been given to a growing number of groups, including military units,\norganizations, and even entire cities.\nOverall, the Congressional Gold Medal is a prestigious award that recognizes the achievements and contributions of\nindividuals and groups to the United States. The award is given through special legislation and involves several steps,\nincluding the introduction of legislation, the consideration of the legislation by the relevant committees, and the\napproval of the legislation by both the House of Representatives and the Senate. The medal is typically designed by\nthe Secretary of the Treasury, in consultation with the Citizens Coinage Advisory Committee and the Commission\nof Fine Arts, and it features a portrait of the recipient, as well as inscriptions and symbols that reflect the recipient’s\nachievements and contributions.\n18\n\nLongSpec: Long-Context Speculative Decoding with Efficient Drafting and Verification\nThe report discusses the use of opioid medications in healthcare and the potential risks associated with their misuse.\nOpioid medications are used to treat pain and can also be used to treat other health problems, such as severe coughing.\nThere are three types of opioid medications that are approved for use in the treatment of opioid use disorders:\nmethadone, buprenorphine, and naltrexone. Methadone is a full opioid agonist, meaning it binds to and activates\nopioid receptors in the body. Buprenorphine is a partial opioid agonist, meaning it also binds to and activates opioid\nreceptors, but to a lesser extent than methadone. Naltrexone is an opioid antagonist, meaning it binds to and blocks\nthe effects of opioid receptors.\nThe report also discusses the potential risks associated with the use of opioid medications, including the risk of\naddiction and the risk of overdose. The use of opioid medications can lead to physical dependence and tolerance,\nwhich can make it difficult to stop using the medication. Additionally, the misuse of opioid medications can lead to\naddiction, which can have serious consequences for the individual and their loved ones.\nThe report also discusses the potential risks associated with the diversion of opioid medications, which is the illegal\nuse of prescription opioids for non-medical purposes. Diversion can lead to increased rates of addiction, overdose,\nand death.\nThe report concludes by discussing the importance of proper use and monitoring of opioid medications, as well as\nthe need for continued research and development of new treatments for opioid use disorders.\nThe Railroad Retirement Board (RRB) is an independent federal agency that administers retirement, survivor,\ndisability, unemployment, and sickness insurance for railroad workers and their families. The RRB covers workers\nwho are employed by railroads engaged in interstate commerce and related subsidiaries, railroad associations, and\nrailroad labor organizations.\nThe RRB has two main programs: the Railroad Retirement Act (RRA) and the Railroad Unemployment Insurance\nAct (RUIA). The RRA authorizes retirement, survivor, and disability benefits for railroad workers and their families.\nThe RUIA provides unemployment and sickness benefits for railroad workers.\nThe number of railroad workers has been declining since the 1950s, although the rate of decline has been irregular.\nIn recent years, railroad employment has increased after reaching an all-time low of 215,000 workers in January\n2010. In April 2015, railroad employment peaked at 253,000 workers, the highest level since November 1999, and\nthen declined through FY2017, falling to 221,000 workers.\nThe RRB’s programs are designed to provide comprehensive benefits to railroad workers and their families. The\nRRA and RUIA are important components of the railroad industry’s retirement and benefits system. The RRB’s\nefforts to maintain and improve these programs are crucial for the well-being of railroad workers and their families.\nThe report provides an overview of the annual appropriations for the Department of Homeland Security (DHS) for\nFY2019. It compares the enacted FY2018 appropriations for DHS, the Trump Administration’s FY2019 budget\nrequest, and the appropriations measures developed and considered by Congress in response to the request. The\nreport identifies additional informational resources, reports, and policy experts that can provide further information\non DHS appropriations.\nThe report explains several specialized budgetary concepts, including budget authority, obligations, outlays,\ndiscretionary and mandatory spending, offsetting collections, allocations, and adjustments to the discretionary\nspending caps under the Budget Control Act (BCA). It also provides a detailed analysis of the appropriations process\nfor DHS, including the various committees and subcommittees involved, and the role of the Congressional Budget\nOffice (CBO) and the Government Accountability Office (GAO).\nThe report highlights the key issues and debates surrounding DHS appropriations, including funding for border\nsecurity, immigration enforcement, cybersecurity, and disaster response. It also discusses the impact of the BCA on\nDHS appropriations and the potential for future changes to the spending caps.\nOverall, the report provides a comprehensive analysis of the annual appropriations for DHS and the factors that\ninfluence the allocation of funding. It is a valuable resource for policymakers, analysts, and stakeholders interested\nin understanding the complexities of DHS appropriations and the challenges facing the department in the coming\nyears.\n19\n",
  "metadata": {
    "source_path": "papers/arxiv/LongSpec_Long-Context_Speculative_Decoding_with_Efficient_Drafting_and\n__Verification_a9460fb04e88bec1.pdf",
    "content_hash": "a9460fb04e88bec13d0fbfd1239b93ba765114c82a2296f0565f04f322f3591d",
    "arxiv_id": null,
    "title": "  LongSpec: Long-Context Speculative Decoding with Efficient Drafting and Verification  ",
    "author": "Penghui Yang, Cunxiao Du, Fengzhuo Zhang, Haonan Wang, Tianyu Pang, Chao Du, Bo An",
    "creation_date": "D:20250225030853Z",
    "published": "2025-02-25T03:08:53",
    "pages": 19,
    "size": 5096140,
    "file_mtime": 1740470084.8844478
  }
}