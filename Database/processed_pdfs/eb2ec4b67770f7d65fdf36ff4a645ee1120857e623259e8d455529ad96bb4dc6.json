{
  "text": "Published as a conference paper at ICLR 2025\nIMPROVED DIFFUSION-BASED GENERATIVE MODEL\nWITH BETTER ADVERSARIAL ROBUSTNESS\nZekun Wang∗1, Mingyang Yi∗†2, Shuchen Xue3,4, Zhenguo Li5, Ming Liu†1, Bing Qin1,\nZhi-Ming Ma3,4\n1Harbin Institute of Technology\n2Renmin University of China\n3Academy of Mathematics and Systems Science, Chinese Academy of Sciences\n4University of Chinese Academy of Sciences\n5Huawei Noah’s Ark Lab\nzkwang@ir.hit.edu.cn\nyimingyang@ruc.edu.cn\nxueshuchen17@mails.ucas.ac.cn\nABSTRACT\nDiffusion Probabilistic Models (DPMs) have achieved significant success in gener-\native tasks. However, their training and sampling processes suffer from the issue of\ndistribution mismatch. During the denoising process, the input data distributions\ndiffer between the training and inference stages, potentially leading to inaccurate\ndata generation. To obviate this, we analyze the training objective of DPMs and the-\noretically demonstrate that this mismatch can be alleviated through Distributionally\nRobust Optimization (DRO), which is equivalent to performing robustness-driven\nAdversarial Training (AT) on DPMs. Furthermore, for the recently proposed Con-\nsistency Model (CM), which distills the inference process of the DPM, we prove\nthat its training objective also encounters the mismatch issue. Fortunately, this\nissue can be mitigated by AT as well. Based on these insights, we propose to\nconduct efficient AT on both DPM and CM. Finally, extensive empirical studies\nvalidate the effectiveness of AT in diffusion-based models. The code is available at\nhttps://github.com/kugwzk/AT_Diff.\n1\nINTRODUCTION\nDiffusion Probabilistic Models (DPMs) (Ho et al., 2020; Song et al., 2020; Yi et al., 2024) have\nachieved remarkable success across a wide range of generative tasks such as image synthesis (Dhari-\nwal & Nichol, 2021; Rombach et al., 2022; Ho et al., 2022a), video generation (Ho et al., 2022b;\nBlattmann et al., 2023), text-to-image generation (Nichol et al.; Ramesh et al., 2022; Saharia et al.,\n2022), etc. The core mechanism of DPMs involves a forward diffusion process that progressively\ninjects noise into the data, followed by a reverse process that learns to generate data by denoising the\nnoise. Unlike traditional generative models such as GANs(Goodfellow et al., 2014) or VAEs (Kingma\n& Welling, 2013), which directly map an easily sampled latent variable (e.g., Gaussian noise) to the\ntarget data through a single network function evaluation (NFE), DPMs adopt a gradual denoising\napproach that requires multiple NFEs (Song et al., 2022; Salimans & Ho, 2022; Lu et al., 2022b;\nMa et al., 2024). However, this noising-then-denoising process introduces a distribution mismatch\nbetween the training and sampling stages, potentially leading to inaccuracies in the generated outputs.\nConcretely, during the training stage, the model is learned to predict the noise in ground-truth noisy\ndata derived from the training set. In contrast, during the inference stage, the input distribution is\nobtained from the output generated by the DPM in the previous step, which differs from the training\nphase, caused by the inaccurate estimation of the score function due to training (Song et al., 2021;\nYi et al., 2023a) and the discretization error (Chen et al., 2022; Li et al., 2023; Xue et al., 2024b;a)\nbrought by sampling. Such distribution mismatches are referred to as Exposure Bias, which has been\ndiscussed in auto-regressive language models (Bengio et al., 2015; Ranzato et al., 2016).\nRecently, the aforementioned distribution mismatch problem in diffusion has been also recognized\nby (Ning et al., 2023; Li & van der Schaar, 2024; Ren et al., 2024; Ning et al., 2024; Li et al.,\n∗Equal contribution\n†Corresponding author\n1\narXiv:2502.17099v1  [cs.LG]  24 Feb 2025\n\nPublished as a conference paper at ICLR 2025\n2024; Lou & Ermon, 2023). However, these studies are either rely on strong mismatch distributional\nassumptions (e.g., Gaussian) (Ning et al., 2023; 2024; Ren et al., 2024) or incur significant additional\ncomputational costs (Li & van der Schaar, 2024). This indicates that a more practical solution to\nthis problem has been overlooked until now. To bridge this gap, we begin with the discrete DPM\nintroduced in (Ho et al., 2020). Intuitively, although there is a mismatch between training and\ninference, the distributions of intermediate noise generated during the inference stage are close to\nthe ground-truth distributions observed during training. Therefore, improving the distributional\nrobustness (Yi et al., 2021; Namkoong, 2019; Shapiro, 2017) (which measures the robustness of the\nmodel to distributional perturbations in training data) of DPM mitigates the distribution mismatch\nproblem. To achieve this, we refer to Distribution Robust Optimization (DRO) (Shapiro, 2017;\nNamkoong, 2019), which aims to improve the distributional robustness of models. We then prove that\napplying DRO to DPM is mathematically equivalent to implementing robustness-driven Adversarial\nTraining (AT) (Madry et al., 2018; Shafahi et al., 2019; Yi et al., 2021) on DPM. 1 Following the DRO\nframework, we also analyze the recently proposed diffusion-based Consistency Model (CM) (Song\net al., 2023; Luo et al., 2023) which distills the trajectory of DPM into a model with one NFE\ngeneration. We first prove that the training objective of CM similarly suffers from the mismatch issue\nas in multi-step DPM. Moreover, the issue can also be mitigated by implementing AT. Therefore,\nfor both DPM and CM, we propose to apply efficient AT (e.g., “Free-AT” (Shafahi et al., 2019))\nduring their training stages to mitigate the distribution mismatch problem.2 Finally, we summarize\nour contributions as follows.\n• We conduct an in-depth analysis of the diffusion-based models (DPM and CM) from a\ntheoretical perspective and systematically characterize its distribution mismatch problem.\n• For both DPM and CM, we theoretically show that their mismatch problem is mitigated by\nDRO, which is equivalent to implementing AT with proved error bounds during training.\n• We propose to conduct efficient AT on both DPM and CM in various tasks, including\nimage generation on CIFAR10 32×32(Krizhevsky & Hinton, 2009) and ImageNet\n64×64 (Deng et al., 2009), and zero-shot Text-to-Image (T2I) generation on MS-COCO\n512×512 (Lin et al., 2014b). Extensive experimental results illustrate the effectiveness of\nthe proposed AT training method in alleviating the distribution mismatch of DPM and CM.\n2\nRELATED WORK\nDistribution Mismatch in DPM.\nThe problem is analogous to the exposure bias in auto-regressive\nlanguage models (Bengio et al., 2015; Ranzato et al., 2016; Shen et al., 2016; Rennie et al., 2017;\nZhang et al., 2019c), whereas the next word prediction (Radford et al., 2019) relies on tokens predicted\nby the model in the inference stage, which may be mismatched with the ground-truth one taken in the\ntraining stage. The similarity to DPMs becomes evident due to their gradual denoising generation\nprocess. Ning et al. (2023) and Ning et al. (2024) propose adding extra Gaussian perturbation\nduring the training stage or data-dependent perturbation during the inference stage, to mitigate this\nissue. Following this line of work, several methods are further proposed. For instance, to reduce\nthe accumulated discrepancy between the intermediate noisy data in the training and inference\nstages, Li et al. (2024) search for a suboptimal mismatched input time step of the model to conduct\ninference. Similarly, Li & van der Schaar (2024) and Ren et al. (2024) directly minimize the difference\nbetween the generated intermediate noisy data and the ground-truth data. However, these methods\neither rely on strong assumptions (Ning et al., 2023; 2024; Li et al., 2024; Ren et al., 2024) or are\ncomputationally expensive (Li & van der Schaar, 2024). In contrast, we are the first to explore the\ndistribution mismatch problem from the perspective of DRO. Meanwhile, our proposed AT with\nstrong theoretical foundations is both simple and efficient, compared with the existing methods.\nAdversarial Training and DRO.\nIn this paper, we leverage the Distributionally Robust Opti-\nmization (DRO) (Shapiro, 2017; Namkoong, 2019; Yi et al., 2021; Sinha et al., 2018; Wang et al.,\n2022; Yi et al., 2023b) to improve the distributional robustness of DPM and CM, thereby mitigating\n1Note that the “adversarial” here refers to perturbation to input training data, instead of the adversarial of\ngenerator-discriminator in GAN (Goodfellow et al., 2014).\n2Notably, the standard AT (Madry et al., 2018) solves a minimax problem that slows the training process.\nThe efficient AT has no extra computational cost compared to the standard training ones (Shafahi et al., 2019).\n2\n\nPublished as a conference paper at ICLR 2025\nthe distribution mismatch problem. As demonstrated in (Sinha et al., 2018; Yi et al., 2021; Lee &\nRaginsky, 2018), we link the DRO with AT (Madry et al., 2018; Goodfellow et al., 2015), which\nis designed to improve the input (instead of distributional) robustness of the model. In supervised\nlearning, the adversarial examples generated by efficient AT methods (Shafahi et al., 2019; Zhang\net al., 2019a;b; Zhu et al., 2020; Jiang et al., 2020) have been proven to be efficient augmented data\nto improve the robustness and generalization performance of models (Rebuffi et al., 2021; Wu et al.,\n2020; Yi et al., 2021). In this paper, we further verify that the AT generated adversarial augmented\nexamples are also beneficial for generative models DPM and CM.\nIn addition, recent studies (Nie et al., 2022; Wang et al., 2023; Zhang et al., 2023) utilize DPM to\ngenerate examples in adversarial training to improve the robustness of the classification model. This is\nquite different from the method in this paper, as we focus on employing AT during training of diffusion-\nbased model to improve its distributional robustness to alleviate the distribution mismatching.\n3\nPRELIMINARY\nDiffusion Probabilistic Models.\nDPM (Sohl-Dickstein et al., 2015; Ho et al., 2020) constructs the\nMarkov chain xt by transition kernel q(xt+1 | xt) = N(√αt+1xt, (1−αt+1)I), where α1, · · · , αT\nare in [0, 1]. Let ¯αt := Πt\ns=1αs, and x0 ∼q be ground-truth data. Then, for xt, it holds\nxt = √¯αtx0 +\n√\n1 −¯αtϵt\nt = 1, · · · , T,\n(1)\nwith ϵt ∼N(0, I). The reverse process pθ(xt | xt+1) is parameterized as\npθ(xt | xt+1) = N(µθ(xt+1, t + 1), σ2\nt+1I),\n(2)\nwhere σ2\nt+1 = 1 −αt+1. To learn pθ(xt | xt+1), a standard method is to minimize the following\nevidence lower bound of negative log-likelihood (NLL) (Ho et al., 2020),\n−Eq [log pθ(x0)] ≤Eq\n\u0014\n−log\npθ(x0:T )\nq(x1:T | x0)\n\u0015\n.\n(3)\nHere, minimizing the ELBO in the r.h.s. of above inequality links to pθ(xt | xt+1) since it is\nequivalent to minimizing the following rewritten objective\nmin\nθ\n\n\nDKL(q(xT ) ∥pθ(xT )) +\nT −1\nX\nt=0\nDKL(q(xt | xt+1) ∥pθ(xt | xt+1))\n|\n{z\n}\nLt\n\n\n,\n(4)\nas in (Ho et al., 2020; Bao et al., 2022; Yi et al., 2023a). Here, the conditional Kullback–Leibler (KL)\ndivergence DKL(q(xt | xt+1) ∥p(xt | xt+1)) =\nR\nq(xt | xt+1) log q(xt|xt+1)\np(xt|xt+1)dxtdxt+1 (Duchi,\n2016), and minimizing Lt is equivalent to solve the following noise prediction problem\nmin\nθ\nE\nh\r\rϵθ(√¯αtx0 +\n√\n1 −¯αtϵt, t) −ϵt\n\r\r2i\n.\n(5)\nWe use ∥· ∥p to denote ℓp-norm. Unless specified, the norm ∥· ∥refers to the ℓ2-norm ∥· ∥2. Since\n¯αt →0 for t →T, x0 is obtained by conducting the reverse diffusion process pθ(xt | xt+1) starting\nfrom xT ∼N(0, I) and ϵ ∼N(0, I), under the learned model ϵθ with\nxt =\n1\n√αt+1\n\u0012\nxt+1 −1 −αt+1\n√1 −¯αt+1\nϵθ(xt+1, t + 1)\n\u0013\n+\np\n1 −αt+1ϵ.\n(6)\nWasserstein Distance.\nFor integer p > 0, Γ(µ, ν) as the set of union distributions with marginal\nµ and ν, the Wasserstein p-distance (Villani et al., 2009) between distributions µ and ν with finite\np-moments is\nWp\np(µ, ν) =\ninf\nγ∈Γ(µ,ν) E(x,y)∼γ∥x −y∥p\np.\n(7)\n4\nROBUSTNESS-DRIVEN ADVERSARIAL TRAINING OF DIFFUSION MODELS\nIn this section, we formally show that the success of DPM relies on specific conditions, i.e., xt is\nclose to xt+1. Next, to mitigate the drawbacks brought by the restriction, we propose to consider\nthe distribution mismatch problem as discussed in Section 1, and connect the problem to a rewritten\nELBO. Finally, we apply DRO for this ELBO to mitigate the distribution mismatch problem and\nfinally link it to AT to be implemented in practice.\n3\n\nPublished as a conference paper at ICLR 2025\n: 𝑞(𝑥! ∣𝑥!\"#)\n: 𝑝$(𝑥! ∣𝑥!\"#)\n: 𝑞'(𝑥! ∣𝑥!\"#)\n: Matching\n𝑥! ∼𝑞 (𝑥!)\n𝑥! ∼𝑝$ (𝑥!)\n𝑥! ∼𝑞 *(𝑥!)\n: 𝐵&!\"(𝑞𝑥! , 𝜂')\n𝑝$ 𝑥! , 𝑞 * 𝑥! ∈\nTraining\nInference\n𝑥(\n…\n𝑥!\"#\nmin 𝐷!\"(𝑞(𝑥# ∣𝑥#$%)||𝑝&(𝑥# ∣𝑥#$%))\n…\n𝑥'\n…\n𝑥!\"#\n𝑥!\n𝑥!\n…\n𝑥'\nStandard \nTraining\nAdv  Training\nInference\n𝑥(\n……\n𝑥!\"#\n𝑥!\"#\nmin 𝐷!\"(𝑞(𝑥# ∣𝑥#$%)||𝑝&(𝑥# ∣𝑥#$%))\n𝑥!\n𝑥!\n……\n𝑥'\n𝑥'𝑥'\nAdversarial \nTraining\nFigure 1: A comparison between standard training and the proposed distributional robust optimization\nin (12). When minimizing DKL(˜qt(xt | xt+1) ∥pθ(xt | xt+1)), the xt+1 is sampled from ˜qt(xt+1),\nsuch that both ˜qt(xt+1) in training stage and pθ(xt+1) in inference stage are in BDKL(q(xt+1), η0),\nso that pθ(xt) tends to locates in BDKL(q(xt), η0) as well as ˜qt(xt). Then, the distributional\nrobustness captured by (12) guarantees the generated pθ(xt) always locates around q(xt) for all t.\n4.1\nHOW DOES DPM WORKS IN PRACTICE?\nNotably, minimizing (4) potentially obtains a sharp NLL under target distribution q(x0). However,\nin the following proposition, we show that (4) also implicitly minimizes the NLL of each xt.\nProposition 1. The minimization problem (4) is equivalent to minimizing an upper bound of\nEq[−log pθ(xt)] for any 0 ≤t ≤T.\nThe proof is provided in Appendix A. It shows that though (4) is proposed to generate x0 ∼q(x0),\nit also guides the model to generate xt such that pθ(xt) approximates the ground-truth distribution\nq(xt). The conclusion is nontrivial as minimizing the ELBO of NLL Eq [−log pθ(x0)] does not\nnecessarily impose any restrictions on xt for t ≥1.\nNext, we will further explain why (4) leads to a small NLL of xt. In Lt of (4), pθ(xt | xt+1)\napproximates q(xt | xt+1) with xt+1 ∼q(xt+1) representing ground-truth data. Consequently,\npθ(xt) approximates q(xt) by recursively applying such a relationship as in the following proposition.\nProposition 2. Suppose pθ(xt | xt+1) matches q(xt | xt+1) well such that\nLt = DKL(q(xt | xt+1) ∥pθ(xt | xt+1)) ≤γ\nT ,\n(8)\nand the discrepancy satisfies DKL(q(xT ) ∥pθ(xT )) ≤γ0, then for any 0 ≤t ≤T, we have\nDKL(q(xt) ∥pθ(xt)) ≤DKL(q(xT ) ∥pθ(xT )) + Lt ≤γ0 + (T −t)γ\nT\n.\n(9)\nThe results is similarly obtained in (Chen et al., 2023), while their result is applied for DKL(q(x0) ∥\npθ0), which is narrowed compared with Proposition 2. The proof is provided in Appendix A, which\nformally explains why (4) results in pθ(xt) approximating q(xt). However, this proposition is built\nupon small Lt, and notably, the error introduced by Lt will be accumulated on the r.h.s. of (9), as\nit increases w.r.t. t. This phenomenon is caused by the distribution mismatch problem discussed in\nSection 1. Concretely, in (4), minimizing Lt learns the transition probability pθ(xt | xt+1) based on\nxt+1 ∼q(xt+1), while in practice, xt in (6) is generated from xt+1 ∼pθ(xt+1). The error between\npθ(xt+1) and q(xt+1) will propagates into the error between pθ(xt) and q(xt) as in (9).\nTherefore, owing to the existence of distribution mismatch, only if Lt is minimized, the gap between\npθ(xt) and q(xt) can be guaranteed. However, the following proposition proved in Appendix A\nindicates that Lt is theoretically minimized with restrictions.\nProposition 3. Lt in (4) is well minimized, only if q(xt+1) is Gaussian or ∥xt+1 −xt∥→0.\nIn practice, the q(xt+1) is usually non-Gaussian. Besides, the gap ∥xt+1 −xt∥is not necessarily\nsmall, especially for samplers with few sampling steps, e.g., DDIM (Song et al., 2022), DPM-Solver\n(Lu et al., 2022a). Therefore, in practice, the accumulated error in (9) caused by the distribution\nmismatch problem may become large, and degenerate the quality of x0.\n4\n\nPublished as a conference paper at ICLR 2025\n4.2\nDISTRIBUTIONAL ROBUSTNESS IN DPM\nInspired by the discussion above, we propose a new training objective as the sum of NLLs under xt,\nmin\nθ\nL(θ) =\nT\nX\nt=0\nEq [−log pθ(xt)] .\n(10)\nThen the following proposition constructs ELBOs for each of Eq[−log pθ(xt)].\nProposition 4. For any distribution ˜q satisfies ˜q(xt) = q(xt) for specific t, we have\nEq [−log pθ(xt)] ≤DKL(˜q(xt | xt+1) ∥pθ(xt | xt+1))\n|\n{z\n}\nL˜q\nt\n+C,\n(11)\nfor a constant C independent of θ.\nThe proof is in Appendix A.2. This proposition generalizes the results in Proposition 1 since ˜q can be\ntaken as q in Proposition 1. During minimizing L˜q\nt, the transition probability pθ(xt | xt+1) matches\n˜q(xt | xt+1), while xt+1 ∼˜q(xt+1) in the training stage has no restriction. Thus, one may take\n˜q(xt+1) ≈pθ(xt+1), then in L˜q\nt, pθ(xt | xt+1) matches ˜q(xt | xt+1) leads pθ(xt) ≈˜q(xt) =\nq(xt), which mitigates the distribution mismatch problem, when minimizing such L˜q\nt.\nUnfortunately, for each t, obtaining such specific ˜qt(xt+1) = pθ(xt+1) is computationally expensive\n(Li & van der Schaar, 2024), which prevents us using desired ˜qt(xt+1). However, we know pθ(xt+1)\nis around q(xt+1). Therefore, by borrowing the idea from DRO (Shapiro, 2017), for each t, we\npropose to minimize the maximal value of L˜qt\nt over all possible ˜qt(xt+1) around q(xt+1). This leads\nto a small Lpθ\nt , as pθ(xt+1) locates around q(xt+1), so that is included in the “maximal range”.\nTechnically, the DRO-based EBLO of (11) is formulated as follows. Here pθ(xt+1) is supposed in\nBDKL(q(xt+1), η0), and it capatures the distributional robustness of pθ(xt | xt+1) w.r.t. input xt+1.\nmin\nθ\nT −1\nX\nt=0\nLDRO\nt\n(θ) = min\nθ\nT −1\nX\nt=0\nsup\n˜qt(xt+1)∈BDKL (q(xt+1),η0)\nDKL(˜qt(xt | xt+1) ∥pθ(xt | xt+1));\ns.t.\n˜qt(xt) = q(xt).\n(12)\nHere ˜qt(xt+1) ∈BDKL(q(xt+1), η0) means DKL(q(xt+1) ∥˜qt(xt+1)) ≤η0. By solving problem\n(12), if the desired ˜qt(xt+1) = pθ(xt+1) is in BDKL(q(xt+1), η0), then the conditional probability\nin (12) transfers xt+1 ∼pθ(xt+1) to target xt ∼q(xt) is learned, which mitigates the distribution\nmismatch problem. The theoretical clarification is in the following Proposition proved in Appendix\nA.2, which indicates that small DRO loss (12) guarantees the quality of generated x0.\nProposition 5. If LDRO\nt\n(θ) ≤η0 in (12) for all t, and DKL(q(xT ) ∥pθ(xT )) ≤η0, then\nDKL(q(x0) ∥pθ(x0)) ≤η0.\nUp to now, we do not know how to compute the DRO-based training objective (12) we derived.\nFortunately, the following theorem corresponds (12) to a “perturbed” noise prediction problem similar\nto (5). The theorem is proved in Appendix A.2.\nTheorem 1. There exists δt depends on x0 and ϵt makes (13) equivalent to problem (12).\nmin\nθ\nT −1\nX\nt=0\nEq(x0),ϵt\n\"\r\r\r\rϵθ(√¯αtx0 +\n√\n1 −¯αtϵt + δt, t) −ϵt −\nδt\n√1 −¯αt\n\r\r\r\r\n2#\n.\n(13)\nThis theorem connects the proposed DRO problem (12) with noise prediction problem (13). Naturally,\nwe can solve (13), if we know the exact δt. Fortunately, we have the following proposition to\ncharacterize the range of δt, and it is proved in Appendix A.2.\nProposition 6. For η > 0 and δt in (13), ∥δt∥1 ≤η holds with probability at least 1−\np\n2(1 −¯αt)/η.\nThe proposition indicates that for any δt depends on x0, ϵt in (13), it is likely in a small range\n(measured under any ℓp-norm, since they can bound each other in Euclidean space). Thus, to resolve\n(13) (so that (12)), we propose to directly consider the following adversarial training (Madry et al.,\n5\n\nPublished as a conference paper at ICLR 2025\n2018) objective with the perturbation δ is taken over its possible range as proved in Proposition 6,\nwhich captures the input (instead of distribution) robustness of model ϵθ.\nmin\nθ\nT −1\nX\nt=0\nEq(x0)\n\"\nEq(xt|x0)\n\"\nsup\nδ:∥δ∥≤η\n\r\r\r\rϵθ(√¯αtx0 +\n√\n1 −¯αtϵt + δ) −ϵt −\nδ\n√1 −¯αt\n\r\r\r\r\n2##\n.\n(14)\nWe present a fine-grained connection between (14) and classical AT in Appendix C. Notably, our\nobjective (14) is different from the ones in (Ning et al., 2023), whereas δ in it is a Gaussian, and ϵθ\npredicts ϵt instead of ϵt + δ/√1 −¯αt as ours.\nTo make it clear, we summarize the rationale from DRO objective (12) to AT our objective (14).\nSince Theorem 1 shows solving (12) is equivalent to (13), which conducts noise prediction (5) with a\nperturbation δt in a small range added (Proposition 6). Thus, we propose to minimize the maximal\nloss over the possible δt, which is indeed our AT objective (14).\n5\nADVERSARIAL TRAINING UNDER CONSISTENCY MODEL\nAlthough the DPM generates high-quality target data x0, the multi-step denoising process (6) requires\nnumerous model evaluations, which can be computationally expensive. To resolve this, the diffusion-\nbased consistency model (CM) is proposed in (Song et al., 2023). Consistency model fθ(xt, t)\ntransfers xt ∼q(xt) into a distribution that approximates the target q(x0). fθ is optimized by the\nfollowing consistency distillation (CD) loss 3\nmin\nθ\nLCD(θ) =\nT −1\nX\nt=0\nExt+1∼q(xt+1) [d (fθ(Φt(xt+1), t), fθ(xt+1, t + 1))] ,\n(15)\nwhere Φt(xt+1) is a solution of a specific ordinary differential equation (ODE) ((37) in Appendix\nB) which is a deterministic function transfers xt+1 to xt, i.e., Φt(xt+1) ∼q(xt), and d(x, y) is a\ndistance between x and y e.g., ℓ1, ℓ2 distance.\nRemark 1. In (Song et al., 2023; Luo et al., 2023), the noisy data xt in (15) is described by an ODE\n(37) in Appendix B. However, we use the discrete xt (1) here to unify the notations with Section 4.\nThe two frameworks are mathematically equivalent as all xt in (1) located in the trajectory of ODE\nin (Song et al., 2023). More details of this claim refer to Appendix B.\nNext, we use the following theorem to illustrate that solving problem (15) indeed creates fθ(xt, t)\nwith distribution close target q(x0). The theorem is proved in Appendix B.\nTheorem 2. For LCD(θ) in (15) with d(·, ·) is ℓ2 distance, then W1(fθ(xt, t), x0) ≤\np\ntLCD(θ) 4.\nThough solving problem (15) creates the desired CM fθ, computing the exact Φt(xt+1) involves\nsolving an ODE as pointed out in Appendix B. Thus, in practice (Song et al., 2023; Luo et al., 2023),\nthe Φt(xt+1) is approximated by a computable numerical estimation ˆΦt(xt+1, ϵϕ) of it, e.g., Euler\n((42) in Appendix B.1) or DDIM (Song et al., 2023), where ϵϕ is a pretrained noise prediction model\nas in (5). Therefore, the practical training objective of (15) becomes\nmin\nθ\nT −1\nX\nt=0\nˆLCD(θ) = Ext+1∼q(zt)\nh\nd\n\u0010\nfθ(ˆΦt(xt+1, ϵϕ), t), fθ(xt+1, t + 1)\n\u0011i\n.\n(16)\nIn (16), ˆΦt(xt+1, ϵϕ) is an estimation to Φt(xt+1), which causes an inaccurate training objective\nˆLCD in (16), compared with target LCD (15). Thus, this results in the distribution mismatch problem\nin CM, as in DPM of Section 4. However, similar to Section 4.2, if we train fθ with robustness to the\ngap between ˆΦt(xt+1, ϵϕ) and Φt(xt+1), the distribution mismatch problem in CM is mitigated.\nTechnically, suppose Φt(xt+1) = ˆΦt(xt+1, ϵϕ) + δt(xt+1), we can consider minimizing the follow-\ning adversarial training objective of CM, if ∥δt(xt+1)∥≤η uniformly over t, for some constant η,\n3In practice, (15) is updated under target model fθ−(Φt(xt+1), t) with exponential moving average (EMA)\nθ−under a stop gradient operation. (Song et al., 2023) find that it greatly stabilizes the training process. In this\nsection, we focus on the theory of consistency model and still use θ in formulas.\n4Here W1(fθ(xt, t), x0) is the Wasserstein 1-distance between distributions of fθ(xt, t) and x0.\n6\n\nPublished as a conference paper at ICLR 2025\nAlgorithm 1 Adversarial Training for Diffusion Model\n1: Input: dataset D, model parameter θ, learning rate κ, loss weighting λ(·), adversarial steps K,\nadversarial learning rate α\n2: while do not converge do\n3:\nSample x ∼D and t ∼U[1, T]\n4:\nSample ϵ ∼N(0, I)\n5:\nδ ←0\n6:\nfor i = 1, 2, . . . , K do\n7:\nL ←\n\r\r\rϵθ(√¯αtx0 + √1 −¯αtϵ + δ) −ϵ −\nδ\n√1−¯αt\n\r\r\r\n2\nin (14)\n8:\nδ ←δ + α ·\n∇δL\n∥∇δL∥\n▷maximize perturbation\n9:\nθ ←θ −κ · ∇θL\n▷update model\n10:\nend for\n11: end while\nAlgorithm 2 Adversarial Training for Consistency Distillation\n1: Input: dataset D, initial model parameter θ, learning rate κ, pretrained noise prediction model\nϵϕ, ODE solver ˆΦ·(·, ϵϕ, metric d(·, ·), loss weighting λ(·), target model EMA µ, adversarial\nsteps K, adversarial learning rate α\n2: θ−←θ\n3: while do not converge do\n4:\nSample x ∼D and t ∼U[0, T −1]\n5:\nSample xt+1 from (1)\n6:\nδ ←0\n7:\nfor i = 1, 2, . . . , K do\n8:\nL ←λ(t)d(fθ(xt+1, t + 1), fθ−(ˆΦt(xt+1, ϵϕ) + δ, t)) in (17)\n9:\nδ ←δ + α ·\n∇δL\n∥∇δL∥\n▷maximize perturbation\n10:\nθ ←θ −κ · ∇θL\n▷update model\n11:\nθ−←stopgrad(µθ−+ (1 −µ)θ)\n12:\nend for\n13: end while\nso that the target Φt(xt+1) is included in the maximal range as well.\nˆLAdv\nCD (θ) =\nT −1\nX\nt=0\nExt+1\n\"\nsup\n∥δ∥≤η\nd\n\u0010\nfθ(ˆΦt(xt+1, ϵϕ) + δ, t), fθ(xt+1, t + 1)\n\u0011#\n.\n(17)\nBy doing so, the learned model fθ can be robust to the perturbation brought by δt(xt+1), so that\nresults in a small LCD(θ), as well as the small W1(fθ(xT , T), x0) as proved in Theorem 2. Next,\nwe use the following theorem to show that ∥δt(xt+1)∥is indeed small, and minimizing ˆLAdv\nCD (θ)\nresults in fθ(xT , T) with distribution approximates x0.\nTheorem 3. Under proper regularity conditions, for 0 ≤t < T, we have Ext+1[∥δt(xt+1)∥] ≤o(1).\nOn the other hand, it holds\nW1(fθ(xT , T), x0) ≤\nq\nT ˆLAdv\nCD (θ) + o(1).\n(18)\nThe theorem is proved in Appendix B.1, and it indicates that using the proposed adversarial training\nobjective (17) of CM indeed guarantees the learned CM transfers xT into data from q(x0).\n6\nEXPERIMENTS\n6.1\nALGORITHMS\nIn the standard adversarial training method like Projected Gradient Descent (PGD) (Madry et al.,\n2018), the perturbation δ is constructed by implementing numbers (3-8) of gradient ascents to δ\n7\n\nPublished as a conference paper at ICLR 2025\nbefore updating the model, which slows down the training process. To resolve this, we adopt an\nefficient implementation (Shafahi et al., 2019) in Algorithms 1, 2 to solve AT (14) and (17) of DPM\nand CM, which has similar computational cost compared to standard training, and significantly\naccelerate standard AT. Notably, unlike PGD, in Algorithms 1 and 2, every maximization step of\nperturbation δ follows an update step of the model θ. Thus, the efficient AT do not require further\nback propagations to construct adversarial samples as in PGD. We provide a comparison between our\nefficient AT and standard AT (PGD) with the same update iterations of model θ in Appendix G.1.\nMoreover, we observe that efficient AT can yield comparable and even better performance than PGD\nwhile accelerating the training (2.6× speed-up), further verifying the benefits of our efficient AT. 5\n6.2\nPERFORMANCE ON DPM\nSettings.\nThe experiments are conducted on the unconditional generation on CIFAR-10 32×32\n(Krizhevsky & Hinton, 2009) and the class-conditional generation on ImageNet 64 × 64 (Deng\net al., 2009). Our model and training pipelines in adopted from ADM (Dhariwal & Nichol, 2021)\npaper, where ADM is a UNet-type network (Ronneberger et al., 2015), with strong performance in\nimage generation under diffusion model.\nTo save training costs, our methods and baselines are fine-tuned from pretrained models, rather than\ntraining from scratch. By doing so, we can efficiently assess the performance of methods, which\nis more practical for general scenarios. We also explore training from scratch in Appendix G.2,\nwhich also verifies the effectiveness of our method in this regime. During training, we fine-tune\nthe pretrained models (details are in Appendix E.1) with batch size 128 for 150K iterations under\nlearning rate 1e-4 on CIFAR-10, and batch size 1024 for 50K iterations under learning rate of\n3e-4 on ImageNet. For the hyperparameters of AT, we select the adversarial learning rate α from\n{0.05, 0.1, 0.5} and the adversarial step K from {3, 5}. More details are in Appendix E.1.\nWe use the Frechet Inception Distance (FID) (Heusel et al., 2017) to evaluate image quality. Unless\notherwise specified, 50K images are sampled for evaluation. Other results of metric Classification\nAccuracy Score (CAS) (Ravuri & Vinyals, 2019), sFID, Inception Score, Precision, and Recall are in\nAppendix F.1 and F.4 for comprehensive evaluation.\nBaselines.\nFor experiments on diffusion models, we consider the following baselines. 1): the\noriginal pretrained model. Compared with it, we verify whether the models are overfitting during\nfine-tuning. 2): continue fine-tuning the pretrained model, which is fine-tuned with the standard\ndiffusion objective (5). Compared to it, we validate whether performance improvements come only\nfrom more training costs. We also compare with the existing typical method to alleviate the DPM\ndistribution mismatch, 3): ADM-IP (Ning et al., 2023), which adds a Gaussian perturbation to the\ninput data to simulate mismatch errors during the training process. The last two fine-tuning baselines\nare based on the same pretrained model and hyperparameters as in the original literature.\nResults.\nTo verify the effectiveness of our AT method, we conduct experiments with four diffusion\nsamplers: IDDPM (Dhariwal & Nichol, 2021), DDIM (Song et al., 2022), DPM-Solver (Lu et al.,\n2022b), and ES (Ning et al., 2024) under various NFEs. The sampler choices contain the three\nmost popular samplers: IDDPM, DDIM, DPM-Solver, and ES, a sampler that scales down the norm\nof predicted noise to mitigate the distribution mismatch from the perspective of sampling. The\nexperimental results of CIFAR-10 and ImageNet are shown in Table 1 and Table 2, respectively.\nResults of more than hundreds of NFEs are shown in Appendix F.3\nAs can be seen, the proposed AT for DPM significantly improves the performance of the original\npretrained model and outperforms the other baselines (continue fine-tuning and ADM-IP) overall for\nall diffusion samplers and NFEs we take. Moreover, we have the following observarions.\n1): Fewer (practically used) sampling steps (5,10) will result in larger mismatching errors, while\nour AT method demonstrates significant improvements in this regime across various samplers, e.g.,\nAT improves FID 27.72 to 17.36 under 5 NFEs DPM-Solver on ImageNet. This suggests that\nour method is indeed effective in alleviating the distribution mismatch of DPM. The results also\nindicate that our method consistently beats the baseline methods, regardless of stochastic (IDDPM)\n5For the experts in AT, they would recognize that the AT in Algorithms 1, 2 actually constructs the adversarial\naugmented data to improve the performance of the model (Zhu et al., 2020; Jiang et al., 2020; Yi et al., 2021).\n8\n\nPublished as a conference paper at ICLR 2025\nTable 1: Sample quality measured by FID ↓of different sampling methods of DPM under different\nNFEs on CIFAR10 32x32. All models are trained with same iterations (computational costs).\n(a) IDDPM\nMethods \\ NFEs\n5\n8\n10\n20\n50\nADM (original)\n37.99\n26.75\n22.62\n10.52\n4.55\nADM (finetune)\n36.91\n26.06\n21.94\n10.58\n4.34\nADM-IP\n47.57\n26.91\n20.09\n7.81\n3.42\nADM-AT (Ours)\n37.15\n23.59\n15.88\n6.60\n3.34\n(b) DDIM\nMethods \\ NFEs\n5\n8\n10\n20\n50\nADM (original)\n34.28\n14.34\n11.66\n7.00\n4.68\nADM (finetune)\n29.30\n15.08\n12.06\n6.80\n4.15\nADM-IP\n43.15\n15.72\n10.47\n4.58\n4.89\nADM-AT (Ours)\n26.38\n12.98\n9.30\n4.40\n3.07\n(c) ES\nMethods \\ NFEs\n5\n8\n10\n20\n50\nADM (original)\n82.18\n29.28\n17.73\n5.11\n2.70\nADM (finetune)\n63.46\n24.80\n17.03\n5.19\n2.52\nADM-IP\n91.10\n31.44\n18.72\n5.19\n2.89\nADM-AT (Ours)\n41.07\n21.62\n14.68\n4.36\n2.48\n(d) DPM-Solver\nMethods \\ NFEs\n5\n8\n10\n20\n50\nADM (original)\n23.95\n8.00\n5.46\n3.46\n3.14\nADM (finetune)\n22.98\n7.61\n5.29\n3.41\n3.12\nADM-IP\n43.83\n6.70\n6.80\n9.78\n10.91\nADM-AT (Ours)\n18.40\n5.84\n4.81\n3.28\n3.01\nTable 2: Sample quality measured by FID ↓of different sampling methods of DPM under different\nNFEs on ImageNet 64x64. All models are trained with the same iterations (computational costs).\n(a) IDDPM\nMethods \\ NFEs\n5\n8\n10\n20\n50\nADM (original)\n76.92\n33.74\n27.63\n12.85\n5.30\nADM (finetune)\n78.87\n33.99\n27.82\n12.80\n5.26\nADM-IP\n67.12\n29.96\n22.60\n8.66\n3.83\nADM-AT (Ours)\n45.65\n23.79\n19.18\n8.28\n4.01\n(b) DDIM\nMethods \\ NFEs\n5\n8\n10\n20\n50\nADM (original)\n60.07\n20.10\n14.97\n8.41\n5.65\nADM (finetune)\n60.32\n20.26\n15.04\n8.32\n5.48\nADM-IP\n76.51\n26.25\n18.05\n8.40\n6.94\nADM-AT (Ours)\n43.04\n16.08\n12.15\n6.20\n4.67\n(c) ES\nMethods \\ NFEs\n5\n8\n10\n20\n50\nADM (original)\n71.31\n28.97\n21.10\n8.23\n3.76\nADM (finetune)\n72.30\n29.24\n21.58\n8.25\n3.64\nADM-IP\n88.37\n33.91\n23.32\n7.80\n3.54\nADM-AT (Ours)\n43.95\n19.57\n14.12\n6.16\n3.45\n(d) DPM-Solver\nMethods \\ NFEs\n5\n8\n10\n20\n50\nADM (original)\n27.72\n10.06\n7.21\n4.69\n4.24\nADM (finetune)\n27.82\n9.97\n7.22\n4.64\n4.15\nADM-IP\n32.43\n9.94\n8.87\n9.16\n9.68\nADM-AT (Ours)\n17.36\n6.55\n5.78\n4.56\n4.34\nor deterministic samplers (DDIM, DPM-Solver). 2): The ES sampler results show that our AT is\northogonal to the sampling-based method to mitigate the distribution mismatch problem and can be\ncombined to further alleviate the issue. Notably, we further verify in Appendix G.2 that our methods\nwill not slow the convergence unlike AT in classification (Madry et al., 2018). We also perform\nablation analysis of hyperparameters in our AT framework in Appendix G.3.\n6.3\nPERFORMANCE ON LATENT CONSISTENCY MODELS\nSettings.\nWe further evaluate the proposed AT for consistency models on text-to-image generation\ntasks with Latent Consistency Models (Luo et al., 2023) Stable Diffusion (SD) v1.5 (Rombach et al.,\n2022) backbone, which generates 512×512 images. Both our AT and the original LCM training\n(baseline) are trained from scratch with the same hyperparameters (the IP method (Ning et al., 2023)\nis not applied straightforwardly). The training set is LAION-Aesthetics-6.5+ (Schuhmann et al.,\n2022) with hyperparameters following Song et al. (2023); Luo et al. (2023). We select the adversarial\nlearning rate α from {0.02, 0.05} and adversarial step K from {2, 3}. The models are trained with a\nbatch size of 64 for 100K iterations. More details are shown in Appendix E.2.\nFollowing Luo et al. (2023) and Chen et al. (2024), we evaluate models on MS-COCO 2014 (Lin et al.,\n2014a) at a resolution of 512×512 by randomly drawing 30K prompts from its validation set. Then,\nwe report the FID between the generated samples under these prompts and the reference samples\nfrom the full validation set following Saharia et al. (2022). We also report CLIP scores (Hessel et al.,\n2021) to evaluate the text-image alignment by CLIP-ViT-B/16.\n9\n\nPublished as a conference paper at ICLR 2025\nTable 3: Results of LCM on MS-COCO 2014 validation set at 512×512 resolution in terms of FID ↓\nand CLIP score ↑. All models are trained with the same setting (computational costs).\nMethods\nFID ↓\nCLIP Score ↑\n1 step\n2 step\n4 step\n8 step\n1 step\n2 step\n4 step\n8 step\nLCM\n25.43\n12.61\n11.61\n12.62\n29.25\n30.24\n30.40\n30.47\nLCM-AT (Ours)\n23.34\n11.28\n10.31\n10.68\n29.63\n30.43\n30.49\n30.53\nResults.\nThe methods are evaluated under various sampling steps in Table 3, which shows that the\nLCM with AT consistently improves FID under various sampling steps. Besides, though the AT is\nnot specified to improve text-image alignment, we observe that it has comparable or even better CLIP\nscores across various sampling steps, which shows that AT will not degenerate text-image alignment.\n7\nCONCLUSION\nIn this paper, we novelly introduce efficient Adversarial Training (AT) in the training of DPM and\nCM to mitigate the issue of distribution mismatch between training and sampling. We conduct\nan in-depth analysis of the DPM training objective and systematically characterize the distribution\nmismatch problem. Furthermore, we prove that the training objective of CM similarly faces the\ndistribution mismatch issue. We theoretically prove that DRO can mitigate the mismatch for both\nDPM and CM, which is equivalent to conducting AT. Experiments on image generation and text-to-\nimage generation benchmarks verify the effectiveness of the proposed AT method in alleviating the\ndistribution mismatch of DPM and CM.\nACKNOWLEDGMENTS\nWe thank anonymous reviewers for insightful feedback that helped improve the paper. Zekun\nWang, Ming Liu, Bing Qin are supported by the National Science Foundation of China (U22B2059,\n62276083), the Human-Machine Integrated Consultation System for Cardiovascular Diseases\n(2023A003). They also appreciate the support from China Mobile Group Heilongjiang Co., Ltd.\nREFERENCES\nFan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analytic-dpm: an analytic estimate of the optimal\nreverse variance in diffusion probabilistic models. In International Conference on Learning\nRepresentations, 2022.\nSamy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence\nprediction with recurrent neural networks. In Advances in Neural Information Processing Systems,\n2015.\nAndreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and\nKarsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In\nConference on Computer Vision and Pattern Recognition, 2023.\nHongrui Chen, Holden Lee, and Jianfeng Lu. Improved analysis of score-based generative modeling:\nUser-friendly bounds under minimal smoothness assumptions. In International Conference on\nMachine Learning, 2023.\nJunsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Zhongdao Wang, James T. Kwok,\nPing Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for\nphotorealistic text-to-image synthesis. In International Conference on Learning Representations,\n2024.\nSitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru Zhang. Sampling is as easy as\nlearning the score: theory for diffusion models with minimal data assumptions. In International\nConference on Learning Representations, 2022.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In Conference on Computer Vision and Pattern Recognition, 2009.\n10\n\nPublished as a conference paper at ICLR 2025\nPrafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat GANs on image synthesis. In\nAdvances in Neural Information Processing Systems, volume 34, pp. 8780–8794, 2021.\nJohn Duchi. Lecture notes for statistics 311/electrical engineering 377. URL: https://stanford.\nedu/class/stats311/Lectures/full notes. pdf. Last visited on, 2:23, 2016.\nIan J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron C Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural\nInformation Processing Systems, 2014.\nIan J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial\nexamples. In International Conference on Learning Representations, 2015.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In Conference on Computer Vision and Pattern Recognition, 2016.\nJack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-\nfree evaluation metric for image captioning. In Proceedings of the Conference on Empirical\nMethods in Natural Language Processing,, 2021.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium. 2017.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in\nNeural Information Processing Systems, 2020.\nJonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans.\nCascaded diffusion models for high fidelity image generation. Journal of Machine Learning\nResearch, 23(47):1–33, 2022a.\nJonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J\nFleet. Video diffusion models. In Advances in Neural Information Processing Systems, 2022b.\nHaoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Tuo Zhao. SMART:\nrobust and efficient fine-tuning for pre-trained natural language models through principled regular-\nized optimization. In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, 2020.\nDiederik P Kingma and Max Welling. Auto-encoding variational {Bayes}. In International Confer-\nence on Learning Representations, 2013.\nAlex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.\nJaeho Lee and Maxim Raginsky. Minimax statistical learning with wasserstein distances. In Advances\nin Neural Information Processing Systems, 2018.\nGen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi. Towards faster non-asymptotic convergence for\ndiffusion-based generative models. Preprint arXiv:2306.09251, 2023.\nMingxiao Li, Tingyu Qu, Wei Sun, and Marie-Francine Moens. Alleviating exposure bias in diffusion\nmodels through sampling with shifted time steps. In International Conference on Learning\nRepresentations, 2024.\nYangming Li and Mihaela van der Schaar. On error propagation of diffusion models. In The Twelfth\nInternational Conference on Learning Representations, 2024.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–\nECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, 2014a.\nTsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll´ar, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. In ECCV, 2014b.\n11\n\nPublished as a conference paper at ICLR 2025\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-\nence on Learning Representations, 2019.\nAaron Lou and Stefano Ermon. Reflected diffusion models. In International Conference on Machine\nLearning, 2023.\nCheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast\node solver for diffusion probabilistic model sampling in around 10 steps. In Advances in Neural\nInformation Processing Systems, 2022a.\nCheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan LI, and Jun Zhu. Dpm-solver: A fast\node solver for diffusion probabilistic model sampling in around 10 steps. In Advances in Neural\nInformation Processing Systems, 2022b.\nSimian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models:\nSynthesizing high-resolution images with few-step inference, 2023.\nJiajun Ma, Shuchen Xue, Tianyang Hu, Wenjia Wang, Zhaoqiang Liu, Zhenguo Li, Zhi-Ming Ma,\nand Kenji Kawaguchi. The surprising effectiveness of skip-tuning in diffusion sampling. arXiv\npreprint arXiv:2402.15170, 2024.\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.\nTowards deep learning models resistant to adversarial attacks. In International Conference on\nLearning Representations, 2018.\nHongseok Namkoong. Reliable machine learning via distributional robustness. PhD thesis, Stanford\nUniversity, 2019.\nAlexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob\nMcGrew, Ilya Sutskever, and Mark Chen. GLIDE: towards photorealistic image generation and\nediting with text-guided diffusion models. In International Conference on Machine Learning.\nWeili Nie, Brandon Guo, Yujia Huang, Chaowei Xiao, Arash Vahdat, and Animashree Anandkumar.\nDiffusion models for adversarial purification. In International Conference on Machine Learning,\n2022.\nMang Ning, Enver Sangineto, Angelo Porrello, Simone Calderara, and Rita Cucchiara. Input\nperturbation reduces exposure bias in diffusion models. In International Conference on Machine\nLearning, 2023.\nMang Ning, Mingxiao Li, Jianlin Su, Albert Ali Salah, and Itir ¨Onal Ertugrul. Elucidating the\nexposure bias in diffusion models. In International Conference on Learning Representations, 2024.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\nconditional image generation with clip latents, 2022.\nMarc’Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training\nwith recurrent neural networks. In International Conference on Learning Representations, 2016.\nSuman V. Ravuri and Oriol Vinyals. Classification accuracy score for conditional generative models.\nIn Advances in Neural Information Processing Systems, 2019.\nSylvestre-Alvise Rebuffi, Sven Gowal, Dan A. Calian, Florian Stimberg, Olivia Wiles, and Timothy A.\nMann. Fixing data augmentation to improve adversarial robustness. Preprint arXiv:2103.01946,\n2021.\nZhiyao Ren, Yibing Zhan, Liang Ding, Gaoang Wang, Chaoyue Wang, Zhongyi Fan, and Dacheng\nTao. Multi-step denoising scheduled sampling: Towards alleviating exposure bias for diffusion\nmodels. In AAAI Conference on Artificial Intelligence, 2024.\n12\n\nPublished as a conference paper at ICLR 2025\nSteven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. Self-critical\nsequence training for image captioning. In IEEE Conference on Computer Vision and Pattern\nRecognition, 2017.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨orn Ommer. High-\nresolution image synthesis with latent diffusion models. In Conference on Computer Vision and\nPattern Recognition, 2022.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical\nimage segmentation. In Medical Image Computing and Computer-Assisted Intervention–MICCAI\n2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III\n18, pp. 234–241. Springer, 2015.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J\nFleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language\nunderstanding. In Advances in Neural Information Processing Systems, 2022.\nTim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In\nInternational Conference on Learning Representations, 2022.\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi\nCherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski,\nSrivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev.\nLAION-5B: an open large-scale dataset for training next generation image-text models. In Advances\nin Neural Information Processing Systems, 2022.\nAli Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John P. Dickerson, Christoph Studer, Larry S.\nDavis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! In Advances in Neural\nInformation Processing Systems 32: Annual Conference on Neural Information Processing Systems\n2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, 2019.\nAlexander Shapiro. Distributionally robust stochastic programming. SIAM Journal on Optimization,\n27(4):2258–2275, 2017.\nShiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. Minimum\nrisk training for neural machine translation. In Proceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics, 2016.\nAlbert N Shiryaev. Probability-1, volume 95. Springer, 2016.\nAman Sinha, Hongseok Namkoong, and John Duchi. Certifying some distributional robustness with\nprincipled adversarial training. In International Conference on Learning Representations, 2018.\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. In International Conference on Machine Learning,\n2015.\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In Interna-\ntional Conference on Learning Representations, 2022.\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. Score-based generative modeling through stochastic differential equations. In International\nConference on Learning Representations, 2020.\nYang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of\nscore-based diffusion models. 2021.\nYang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In International\nConference on Machine Learning, 2023.\nC´edric Villani et al. Optimal transport: old and new, volume 338. Springer, 2009.\n13\n\nPublished as a conference paper at ICLR 2025\nMartin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cam-\nbridge university press, 2019.\nRuoyu Wang, Mingyang Yi, Zhitang Chen, and Shengyu Zhu. Out-of-distribution generalization\nwith causal invariant transformations. In Conference on Computer Vision and Pattern Recognition,\n2022.\nZekai Wang, Tianyu Pang, Chao Du, Min Lin, Weiwei Liu, and Shuicheng Yan. Better diffusion\nmodels further improve adversarial training. In International Conference on Machine Learning,\n2023.\nDongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust general-\nization. In Advances in Neural Information Processing Systems, 2020.\nShuchen Xue, Zhaoqiang Liu, Fei Chen, Shifeng Zhang, Tianyang Hu, Enze Xie, and Zhenguo Li.\nAccelerating diffusion sampling with optimized time steps. arXiv preprint arXiv:2402.17376,\n2024a.\nShuchen Xue, Mingyang Yi, Weijian Luo, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhi-Ming\nMa. Sa-solver: Stochastic adams solver for fast sampling of diffusion models. Advances in Neural\nInformation Processing Systems, 36, 2024b.\nMingyang Yi, Lu Hou, Jiacheng Sun, Lifeng Shang, Xin Jiang, Qun Liu, and Zhiming Ma. Improved\nood generalization via adversarial training and pretraing. In International Conference on Machine\nLearning, 2021.\nMingyang Yi, Jiacheng Sun, and Zhenguo Li. On the generalization of diffusion model. Preprint\narXiv:2305.14712, 2023a.\nMingyang Yi, Ruoyu Wang, Jiacheng Sun, Zhenguo Li, and Zhi-Ming Ma. Breaking correlation shift\nvia conditional invariant regularizer. In The International Conference on Learning Representations,\n2023b.\nMingyang Yi, Aoxue Li, Yi Xin, and Zhenguo Li. Towards understanding the working mechanism of\ntext-to-image diffusion model. Preprint arXiv:2405.15330, 2024.\nTianwei Yin, Micha¨el Gharbi, Richard Zhang, Eli Shechtman, Fr´edo Durand, William T Freeman,\nand Taesung Park. One-step diffusion with distribution matching distillation. In CVPR, 2024.\nBoya Zhang, Weijian Luo, and Zhihua Zhang. Enhancing adversarial robustness via score-based\noptimization. In Advances in Neural Information Processing Systems, 2023.\nDinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, and Bin Dong. You only propagate\nonce: Accelerating adversarial training via maximal principle. In Advances in Neural Information\nProcessing Systems, 2019a.\nHongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and Michael I. Jordan.\nTheoretically principled trade-off between robustness and accuracy. In International Conference\non Machine Learning, 2019b.\nWen Zhang, Yang Feng, Fandong Meng, Di You, and Qun Liu. Bridging the gap between training and\ninference for neural machine translation. In Proceedings of the 57th Conference of the Association\nfor Computational Linguistics, 2019c.\nChen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, and Jingjing Liu. Freelb: Enhanced\nadversarial training for natural language understanding. In International Conference on Learning\nRepresentations, 2020.\n14\n\nPublished as a conference paper at ICLR 2025\nA\nPROOFS IN SECTION 4\nIn this section, we present the proofs of the results in Section 4.\nA.1\nPROOFS IN SECTION 4.2\nProposition 1. The minimization problem (4) is equivalent to minimizing an upper bound of\nEq[−log pθ(xt)] for any 0 ≤t ≤T.\nProof. We prove the first equivalence, by Jensen’s inequality. For any 0 ≤t < T, we have\n−Eq [log pθ(xt)]\n≤Eq\n\u0014\n−log\npθ(xt:T )\nq(xt+1:T | xt)\n\u0015\n=Eq\n\n−log pθ(xT ) −\nX\nt≤s<T\nlog pθ(xs | xs+1)\nq(xs+1 | xs)\n\n\n=Eq\n\n−log pθ(xT ) −\nX\nt≤s<T\nlog pθ(xs | xs+1)\nq(xs | xs+1) ·\nq(xs)\nq(xs+1)\n\n\n=Eq\n\n−log pθ(xT )\nq(xT ) −\nX\nt≤s<T\nlog pθ(xs | xs+1)\nq(xs | xs+1) −log q(xt)\n\n\n=DKL(q(xT ) ∥pθ(xT )) + Eq\n\n\nT −1\nX\ns=t\nDKL(q(xs | xs+1) ∥pθ(xs | xs+1))\n|\n{z\n}\nLt\n\n+ H(xt)\n(19)\nTaking t = 0, we prove the first equivalence. Besides that, the entropy H(xt) of xt is a constant for\nθ given data distribution x0 for any 0 ≤t < T. The second conclusion holds due to the non-negative\nproperty of KL-divergence.\nProposition 2. Suppose pθ(xt | xt+1) matches q(xt | xt+1) well such that\nLt = DKL(q(xt | xt+1) ∥pθ(xt | xt+1)) ≤γ\nT ,\n(8)\nand the discrepancy satisfies DKL(q(xT ) ∥pθ(xT )) ≤γ0, then for any 0 ≤t ≤T, we have\nDKL(q(xt) ∥pθ(xt)) ≤DKL(q(xT ) ∥pθ(xT )) + Lt ≤γ0 + (T −t)γ\nT\n.\n(9)\nProof. We have the following decomposition due to the chain rule of KL-divergence\nDKL(q(xt, xt+1) ∥pθ(xt, xt+1)) = DKL(q(xt | xt+1) ∥pθ(xt | xt+1)) + DKL(q(xt+1) ∥pθ(xt+1))\n= DKL(q(xt+1 | xt) ∥pθ(xt+1 | xt)) + DKL(q(xt) ∥pθ(xt)),\n(20)\nThe transition probability pθ(xt | xt+1) matches q(xt | xt+1), so that the above equality implies\nDKL(q(xt) ∥pθ(xt))\n=DKL(q(xt+1) ∥pθ(xt+1)) + DKL(q(xt | xt+1) ∥pθ(xt | xt+1)) −DKL(q(xt+1 | xt) ∥pθ(xt+1 | xt))\n≤DKL(q(xt+1) ∥pθ(xt+1)) + γ\nT .\n(21)\nThe proposition holds due to initial condition DKL(q(xT ) ∥pθ(xT )) ≤γ0 and simple induction.\nProposition 3. Lt in (4) is well minimized, only if q(xt+1) is Gaussian or ∥xt+1 −xt∥→0.\n15\n\nPublished as a conference paper at ICLR 2025\nProof. Due to Bayes’ rule, we have\nq(xt | xt+1) = q(xt+1 | xt)q(xt)\nq(xt+1)\n∝exp\n \n−\n\r\rxt+1 −√αt+1xt\n\r\r2\n2(1 −αt+1)\n+ log q(xt) −log q(xt+1)\n!\n∝exp\n \n−\n\r\rxt+1 −√αt+1xt\n\r\r2\n2(1 −αt+1)\n+ ⟨∇x log q(xt+1), xt −xt+1⟩\n!\n·\nexp\n\u00121\n2(xt −xt+1)⊤∇2\nx log q(xt+1)(xt −xt+1) + O(∥xt+1 −xt∥3)\n\u0013\n.\n(22)\nAs can be seen, the conditional probability can be approximated by Gaussian only if ∇3\nx log q(xt+1)\nis zero or ∥xt+1 −xt∥3 is extremely small with high probability. The two conditions can be\nrespectively satisfied when q(xt) is a Gaussian or xt close to xt+1.\nA.2\nPROOFS IN SECTION 4.2\nProposition 4. For any distribution ˜q satisfies ˜q(xt) = q(xt) for specific t, we have\nEq [−log pθ(xt)] ≤DKL(˜q(xt | xt+1) ∥pθ(xt | xt+1))\n|\n{z\n}\nL˜q\nt\n+C,\n(11)\nfor a constant C independent of θ.\nProof. W.o.l.g., suppose pθ(xt, xt+1) = pθ(xt | xt+1)q(xt+1) and ˜q(xt, xt+1) = ˜q(xt+1 |\nxt)q(xt). By Jensen’s inequality, we have\nEq [−log pθ(xt)]\n= −\nZ\nq(xt)\n\u0012\nlog\nZ\npθ(xt, xt+1)dxt+1\n\u0013\ndxt\n= −\nZ\nq(xt)\n\u0012\nlog\nZ pθ(xt, xt+1)\n˜q(xt+1 | xt) ˜q(xt+1 | xt)dxt+1\n\u0013\ndxt\n≤−\nZ\nq(xt)\n\u0012Z\nlog pθ(xt, xt+1)\n˜q(xt+1 | xt) ˜q(xt+1 | xt)dxt+1\n\u0013\ndxt\n= −\nZ\nq(xt)\n\u0012Z\n˜q(xt+1 | xt) log pθ(xt | xt+1)\n˜q(xt+1 | xt) dxt+1\n\u0013\ndxt\n−\nZ\nq(xt)\n\u0012Z\n˜q(xt+1 | xt) log\nq(xt+1)\n˜q(xt+1 | xt)dxt+1\n\u0013\ndxt\n= −\nZ\n˜q(xt, xt+1) log pθ(xt | xt+1)\n˜q(xt+1 | xt) dxtdxt+1 + C1\n= −\nZ\n˜q(xt, xt+1) log pθ(xt | xt+1)\n˜q(xt | xt+1) ·\nq(xt)\n˜q(xt+1)dxtdxt+1 + C1\n= −\nZ\n˜q(xt, xt+1) log pθ(xt | xt+1)\n˜q(xt | xt+1) dxtdxt+1 + C1 + C2\n=DKL(˜q(xt | xt+1) ∥pθ(xt | xt+1)) + C\n=L˜q\nvlb(θ, t) + C,\n(23)\nwhere C, C1, C2 are all constants independent of θ.\nA.2.1\nPROOF OF THEOREM 1\nIn this section, we prove the Theorem 1.\nTo simplify the notation, let pθ(xt | xt+1) ∼\nN(µθ(xt+1, t + 1), σt+1 6 in (6), then the optimal solution (Lemma 9 in (Bao et al., 2022)) of\nminimizing L˜qt\nt+1 is\nµθ(xt+1, t + 1) = E˜qt[xt | xt+1].\n(24)\n6Here σt+1 can be also optimized as in (Bao et al., 2022), but we find optimizing it in practice does not\nimprove the empirical results.\n16\n\nPublished as a conference paper at ICLR 2025\nFor every specific t, we consider the following ˜qt in (12) 7, such that\n˜qt(xt+1 | xt) ̸= q(xt+1 | xt);\n˜qt(xt+1) ̸= q(xt+1);\n˜qt(x0:t) = q(x0:t).\n˜qt(xt | x0, xt+1) = q(xt | x0, xt+1) = N(µt+1(x0, xt+1), σt).\n(25)\nwhere µt+1(x0, xt+1) =\n√¯αt(1−αt+1)\n1−¯αt+1\nx0 +\n√αt+1(1−¯αt)\n1−¯αt+1\nxt+1. The ˜qt can be taken due to the\nBayesian rule. Next, we analyze the optimal formulation in (24). Due to the property of conditional\nexpectation, we have\nµθ(xt+1, t + 1) = E˜qt [E˜qt [xt | x0, xt+1] | xt+1] = µt+1 (E˜qt[x0 | xt+1], xt+1) .\n(26)\nAs can be seen, the optimal transition rule is decided by the conditional expectation E˜qt[x0 | xt+1]\nfor some ˜qt(xt+1) ∈BDKL(˜q(xt+1), η0) in (12). Then, we have the following lemma to get the\ndesired conditional expectation.\nLemma 1. There exists some η ≥η0 in (27) which makes (27) equivalent to problem (12).\nmin\nθ\nT −1\nX\nt=0\nE˜qt(x0)\nsup\n˜qt(xt+1|x0)∈BDKL (qt(xt+1|x0),η)\nE˜qt(xt+1|x0)\n\u0002\n∥xθ(xt+1, t + 1) −x0∥2\u0003\n,\n(27)\nwhere Epθ[x0 | xt+1] = xθ(xt+1, t + 1).\nProof. Let us check the training objective minθ sup˜qt∈BDKL(qt+1,η) DKL(˜qt(xt | xt+1) ∥pθ(xt |\nxt+1)). During this proof, we abbreviate BDKL(qt+1(xt+1), η) as B. Since pθ(xt | xt+1) ∼\nN(µθ(xt+1, t + 1), σt+1), then\nsup\n˜qt(xt+1)∈B\nDKL(˜qt(xt | xt+1) ∥pθ(xt | xt+1))\n∝−d\n2 log 2πσ2\nt+1 −\n1\n2σ2\nt+1\nsup\n˜qt(xt+1)∈B\nE˜q(xt,xt+1)\n\u0002\n∥xt −µθ(xt+1, t + 1)∥2\u0003\n.\n(28)\nAs we consider σt+1 as constant, an analysis of the expectation term is enough. Due to\nE˜qt(xt,xt+1)\n\u0002\n∥xt −µθ(xt+1, t + 1)∥2\u0003\n≥inf\nf E˜qt(x0,xt,xt+1)\n\u0002\n∥xt −f(x0, xt+1)∥2\u0003\n= E˜qt(x0,xt,xt+1)\n\u0002\n∥xt −E˜q[xt | x0, xt+1]∥2\u0003\n,\n(29)\nwhere the last term is invariant over ˜qt ∈B so that it is a uniform lower bound over all possible ˜qt\nand pθ(xt | xt+1). The above inequality indicates that the optimal µθ(xt+1, t + 1) is achieved when\nthe left in (29) becomes the right in (29).\nOn the other hand, for any ˜qt ∈B, let us compute the gap such that\nE˜qt(xt,xt+1)\n\u0002\n∥xt −µθ(xt+1, t + 1)∥2\u0003\n= E˜qt\n\u0002\n∥xt −E˜qt[xt | x0, xt+1] + E˜qt[xt | x0, xt+1] −µθ(xt+1, t + 1)∥2\u0003\n= E˜qt\n\u0002\n∥xt −E˜qt[xt | x0, xt+1]∥2\u0003\n+ E˜qt\n\u0002\n∥µθ(xt+1, t + 1) −E˜qt[xt | x0, xt+1]∥2\u0003\n−2E˜qt [⟨xt −E˜qt[xt | x0, xt+1], µθ(xt+1, t + 1) −E˜qt[xt | x0, xt+1]⟩]\n= E˜qt\n\u0002\n∥xt −E˜qt[xt | x0, xt+1]∥2\u0003\n+\n\u0012√¯αt −\nq\n1 −¯αt −σ2\nt+1\nr\n¯αt+1\n1 −¯αt+1\n\u0013\nE˜qt(x0,xt+1)\n\u0002\n∥x0 −xθ(xt+1, t + 1)∥2\u0003\n,\n(30)\nwhere the equality is due to the property of conditional expectation leads to E˜qt[⟨xt −E˜qt[xt |\nx0, xt+1], µθ(xt+1, t+1)−E˜qt[xt | x0, xt+1]⟩] = 0, and rewriting E˜qt[∥µθ(xt+1, t+1)−E˜qt[xt |\nx0, xt+1]∥2] as in equations (5)-(10) in (Ho et al., 2020). Due to this, we know that minimizing the\n7We can do this since (12) only relates to ˜qt(xt+1)\n17\n\nPublished as a conference paper at ICLR 2025\nsquare error is equivalent to minimizing the E˜qt(xt,xt+1)[∥x0 −xθ(xt+1, t + 1)∥2]. On the other\nhand, since ˜q∗\nt ∈B, then we have\nDKL(q(xt+1 | x0) ∥˜q∗\nt (xt+1 | x0))\n=DKL(q(x0 | xt+1) ∥˜q∗\nt (x0 | xt+1)) + DKL(q(xt+1) ∥˜q∗\nt (xt+1))\n≥η0.\n(31)\nThus, we prove our conclusion.\nTheorem 1. There exists δt depends on x0 and ϵt makes (13) equivalent to problem (12).\nmin\nθ\nT −1\nX\nt=0\nEq(x0),ϵt\n\"\r\r\r\rϵθ(√¯αtx0 +\n√\n1 −¯αtϵt + δt, t) −ϵt −\nδt\n√1 −¯αt\n\r\r\r\r\n2#\n.\n(13)\nProof. By combining Lemma 1, suppose the supreme is attained under ˜qt−1 such that xt ∼˜qt−1(xt)\nwith\nxt = √¯αtx0 +\n√\n1 −¯αtϵt + δt,\n(32)\nwith δt depends on x0 and xt. Then we prove the conclusion.\nA.2.2\nPROOF OF PROPOSITION 5\nProposition 5. If LDRO\nt\n(θ) ≤η0 in (12) for all t, and DKL(q(xT ) ∥pθ(xT )) ≤η0, then\nDKL(q(x0) ∥pθ(x0)) ≤η0.\nProof. This theorem can proved by induction. Since DKL(q(xT ) ∥pθ(xT )) ≤η0, then, let\n˜q∗\nT −1(xT ) = pθ(xT ) and satisfies ˜q∗\nT −1(xT ) = q(xT −1). The existence of such distribution is due\nto Kolmogorov existence theorem (Shiryaev, 2016). Then, we have\nDKL(˜q∗\nT −1(xT −1) ∥pθ(xT −1)) ≤DKL(˜q∗\nT −1(xT ) ∥pθ(xT ))\n+ DKL(˜q∗\nT −1(xT −1 | xT ) ∥pθ(xT −1 | xT ))\n≤LDRO\nt\n(θ)\n≤η0,\n(33)\nwhere the first inequality is due to the definition of LDRO\nt\n(θ) and ˜q∗\nT −1(xT ) = pθ(xT ). Then, we\nprove our conclusion by induction over t.\nA.2.3\nPROOF OF PROPOSITION 6\nProposition 6. For η > 0 and δt in (13), ∥δt∥1 ≤η holds with probability at least 1−\np\n2(1 −¯αt)/η.\nProof. Due to the definition of the first order Wasserstein distance W1(·, ·) (Villani et al., 2009) for\nany specific x0, suppose\nπ∗∈\narg min\nπ(xt,˜xt)∈qt(xt|x0)×˜qt(˜xt|x0)\nE [∥˜xt −xt∥1] ,\n(34)\nso that\nEπ∗[∥˜xt −xt∥1] = W1(qt(xt | x0), ˜qt(xt | x0)).\n(35)\nLet δt be the one of (13) under π∗derived by Lemma 1, then\nP (∥δt∥1 ≥η | x0) ≤Eπ∗[∥δt∥1]\nη\n= W1(qt(xt | x0), ˜qt(xt | x0))\nη\n≤a\np\n2(1 −¯αt)DKL(qt(xt | x0) ∥˜qt(xt | x0))\nη\n≤\ns\n2(1 −¯αt)\nη\n,\n(36)\nwhere inequality a is due to the Talagrand’s inequality (Wainwright, 2019). Then we prove our\nconclusion.\n18\n\nPublished as a conference paper at ICLR 2025\nB\nPROOFS IN SECTION 5\nNext, we give the proof of results in Section 5. Firstly, let us check the definition of the Φt(xt+1).\nFor the variance-preserving stochastic differential equation in Song et al. (2022)\ndzs = −βs\n2 zsdt +\np\nβsdWs.\n(37)\nDue to the solution of zs in Song et al. (2023), we know zst has the same distribution with xt in (1)\nfor {st}T\nt=1 satisfies\nexp\n\u0012\n−\nZ st\n0\nβ(u)du\n\u0013\n= ¯αt\n(s0 = 0).\n(38)\nIn the rest of this section, we use d(x, y) in (15) as ℓ2 distance ∥x −y∥2, whereas the conclusions\nunder other distance can be similarly derived. Owing the the discussion in above, similar to (Song\net al., 2023), when xt+1 = zst+1, let Φt(xt+1) = Ψst(zst+1), we can rewrite the objective (15) as\nfollows.\nmin\nθ\nLCD(θ) = min\nθ\nT −1\nX\nt=0\nEzst\nh\r\rfθ(Ψst(zst+1), t) −fθ(zst+1, t + 1)\n\r\r2i\n.\n(39)\nHere zs follows the following reverse time ODE of (37) with z0 ∼q(x0),\ndzs = −βs\n2\n\u0012\nzs + 1\n2∇z log qs(zs)\n\u0013\n|\n{z\n}\nϕs\nds,\n(40)\nand such zs has the same distribution with the ones in (37) (Song et al., 2022), where qs is the\ndensity of zs. Ψst(zst+1) = zst+1 −\nR st+1\nst\nϕs(zs)ds, which is a deterministic function of zst+1,\nand fθ(zs0, 0) = zs0 = z0.\nNow, we are ready to prove the Theorem 2 as follows.\nTheorem 2. For LCD(θ) in (15) with d(·, ·) is ℓ2 distance, then W1(fθ(xt, t), x0) ≤\np\ntLCD(θ) 8.\nProof. Owing to the definition of W1-distance, and the discussion in above, we have\nW1(fθ(xT , T), x0) = W1(fθ(zsT , T), zs0)\n= W1\n\u0000fθ(zsT , T), Ψs0\n\u0000Ψs1\n\u0000· · · ΨsT −1 (zsT )\n\u0001\u0001\u0001\n≤E\n\u0002\n∥fθ(zsT , T) −Ψs0\n\u0000Ψs1\n\u0000· · · ΨsT −1 (zsT )\n\u0001\u0001\n∥\n\u0003\n≤\nT −1\nX\nt=0\nE\n\u0002\r\rfθ(zst+1, t + 1) −fθ(Ψst(zst+1), t)\n\r\r\u0003\n≤\np\nTLCD(θ),\n(41)\nwhere the first inequality is due to the definition of Wasserstein distance, the second and last\ninequalities respectively use the triangle inequality and Schwarz’s inequality.\nB.1\nPROOF OF THEOREM 3\nAs pointed out in the above, the used ˆΦt(xt+1, ϵϕ) is a numerical estimator of Φt(xt+1). In the\nsequel, let us consider ˆΦ is an Euler estimator as follows, whereas our analysis can be similarly\ngeneralized to the other estimators.\nˆΦt(xt+1, ϵϕ) = ˆΨst(zst+1, ϵϕ) = zst+1 + (st+1 −st) βst+1\n2\n\u0010\nzst+1 + ϵϕ(zst+1, t + 1)/\np\n1 −¯αt+1\n\u0011\n|\n{z\n}\nˆϕst+1\n,\n(42)\n8Here W1(fθ(xt, t), x0) is the Wasserstein 1-distance between distributions of fθ(xt, t) and x0.\n19\n\nPublished as a conference paper at ICLR 2025\nwhere √1 −¯αt+1ϵϕ(zst+1, t+1) estimates ∇z log qst+1(zst+1) as pointed out in (Song et al., 2020),\nand the condition xt+1 = zst+1 is hold.\nNext, we illustrate the used regularity conditions to derive Theorem 3.\nAssumption 1. The discretion error of ˆΨst(zst+1, ϵϕ) is smaller than C(st+1 −st)2 for constant C,\nthat says\n\r\r\r\rˆΨst(zst+1, ϵϕ) −zst+1 −\nZ st+1\nst\nˆϕs(zs)ds\n\r\r\r\r ≤C(st+1 −st)2\n(43)\nAssumption 2. The estimated score ∇z log ˆqs(z) has bounded expected error, i.e.,\nEz∼qst (z)\n\u0014\r\r\rˆϕst(z) −ϕst(z)\n\r\r\r\n2\u0015\n≤ϵ.\n(44)\nfor all 0 ≤t < T.\nAssumption 3. For the learned model fθ, it holds ∥fθ∥≤D.\nThe Assumption 1 describes the discretion error of the Euler method under ODE with drift term\nˆϕs, which can be satisfied under proper continuity conditions of model ϵϕ. On the other hand,\nAssumption 2 describes the estimation error of ˆϕst(z), which terms out to be the training objective of\nobtaining it, see (Song et al., 2020) for more details. The Assumption 3 is natural, since fθ predicts\nx0, which is usually an image data with bounded norm. Now, we are ready to prove the Theorem 3,\nwhich is presented by proving the following formal version.\nTheorem 4. Under Assumptions 1, 2, and 3, for all δst, we have Ezst[∥δst(zst)∥] ≤O(∆2\nst +\nϵ\np\n∆st) for ∆st = st+1 −st. Besides that, we have\nW1(fθ(zT , T), z0) ≤\ns\nT ˆLAdv\nCD (θ) + 4D2\nη\nh\nC∆2st + ϵO(\np\n∆st)\ni\n.\n(45)\nProof. Noting that Φt(xt+1) = Ψst(zst+1) and ˆΦt(xt+1, ϵϕ) = ˆΨst(zst+1, ϵϕ), the key problem is\nto upper bound the difference between ˆΨst(z, ϵϕ) and Ψst(z) for all t and z. To do so, we note that\n\r\r\rˆΨst(z, ϵϕ) −Ψst(z)\n\r\r\r ≤\n\r\r\r\rˆΨst(z, ϵϕ) −z −\nZ st+1\nst\nˆϕs(zs)ds\n\r\r\r\r +\n\r\r\r\rz −\nZ st+1\nst\nˆϕs(zs)ds −Ψst(z)\n\r\r\r\r ,\n(46)\nwhere the first one in r.h.s can be upper bounded by C(st+1 −st)2 according to Assumption 1. On\nthe other hand, define dˆzs\nds = ˆϕs(ˆzs), then when ˆzst+1 = zst+1 = z and s ∈[st, st+1].\nd\nds∥ˆzs −zs∥2 =\nD\nˆzs −zs, ˆϕs(ˆzs) −ϕs(zs)\nE\n=\nD\nˆzs −zs, ˆϕs(ˆzs) −ˆϕs(zs) + ˆϕs(zs) −ϕs(zs)\nE\n≤L∥ˆzs −zs∥2 +\nD\nˆzs −zs, ˆϕs(zs) −ϕs(zs)\nE\n≤\n\u00121\n2 + L\n\u0013\n∥ˆzs −zs∥2 + 1\n2\n\r\r\rˆϕs(zs) −ϕs(zs)\n\r\r\r\n2\n.\n(47)\nTaking expectation over z, by Gronwall’s inequality, Assumption 2 and ˆzst+1 = zst+1, we have\nE\n\u0002\n∥ˆzst −zst∥2\u0003\n≤\nZ st+1\nst\ne(1/2+L)(s−st)\n2\nE\nh\n∥ˆϕs(zs) −ϕs(zs)∥2i\nds ≤ϵ\n4\nZ st+1\nst\nβse(1/2+L)(s−st)ds.\n(48)\nPlugging this into (46), we know\nE\nh\r\r\rˆΨst(zst, ϵϕ) −Ψst(zst)\n\r\r\r\ni\n≤C(st+1 −st)2 + ϵO(√st+1 −st).\n(49)\nBy Markov’s inequality, we have\nP\n\u0010\r\r\rˆΨst(zst, ϵϕ) −Ψst(zst)\n\r\r\r ≥η\n\u0011\n≤\nE\nh\r\r\rˆΨst(zst, ϵϕ) −Ψst(zst)\n\r\r\r\ni\nη\n≤1\nη\n\u0002\nC(st+1 −st)2 + ϵO(√st+1 −st)\n\u0003\n.\n(50)\n20\n\nPublished as a conference paper at ICLR 2025\nThus,\nE\n\u0002\n∥fθ(xt+1, t + 1) −fθ(Φt(xt+1), t)∥2\u0003\n= E\nh\r\rfθ(zst+1, t + 1) −fθ(Ψst(zst+1), t)\n\r\r2i\n= E\nh\r\r\rfθ(zst+1, t + 1) −fθ(ˆΨst(zst+1 + δst, ϵϕ), t)\n\r\r\r\ni\n= E\n\u0014\u0010\n1∥δst ∥>η + 1∥δst ∥≤η\n\u0011 \r\r\rfθ(zst+1, t + 1) −fθ(ˆΨst(zst+1 + δst, ϵϕ), t)\n\r\r\r\n2\u0015\n≤E\n\"\nsup\n∥δ∥≤η\n\r\r\rfθ(zst+1, t + 1) −fθ(ˆΨst(zst+1 + δst, ϵϕ), t)\n\r\r\r\n#\n+ 4D2P\n\u0000∥δst∥2 ≥η\n\u0001\n≤E\n\"\nsup\n∥δ∥≤η\n\r\r\rfθ(zst+1, t + 1) −fθ(ˆΨst(zst+1 + δ, ϵδ), t)\n\r\r\r\n2\n#\n+ 4D2\nη\n\u0002\nC(st+1 −st)2 + ϵO(√st+1 −st)\n\u0003\n.\n(51)\nTaking sum over t and combining Theorem 2, we prove our conclusion.\nTherefore, in this theorem, by taking ∆st = st+1 −st close to zero, we get the results in Theorem 3.\nC\nTHE CONNECTION TO STANDARD ADVERSARIAL TRAINING\nIn this section, we clarify why the proposed AT objective (14) is a general version of the standard AT\nobjective proposed in (Madry et al., 2018) used for classification problems.\nFor classification problem, given model fθ(x), data x, and label y, it aims to minimize the adversarial\ntraining objective\nmin\nθ\nE(x,y)\n\"\nsup\nδ:∥δ∥≤η0\nℓ(fθ(x + δ), y)\n#\n,\n(52)\nfor some loss function ℓ(e.g. cross entropy) and adversarial radius η0. However, the objective is not\ndirectly generalized to the diffusion model, as its training objective is a regression problem instead\nof classification (52). Thus, we should refer to the general version of adversarial training as in (Yi\net al., 2021; Sinha et al., 2018), where the training objective is minθ Ex[ℓθ(x)], and the adversarial\ntraining objective becomes\nmin\nθ\nEx\n\"\nsup\nδ:∥δ∥≤η0\nℓθ(x + δ))\n#\n,\n(53)\nwhere ℓθ is the parameterized loss function, and x is data. Then, we can conclude our objective (14)\nfollows the above formulation, such that the goal is represented as\nmin\nθ\nT −1\nX\nt=0\nEx0\n\"\nExt|x0\n\"\nsup\nδ:∥δ∥≤η0\nℓx0\nθ (xt + δ)\n##\n,\n(54)\ncompared with the original noise prediction objective minθ\nPT −1\nt=0 Ex0\n\u0002\nExt|x0 [ℓx0\nθ (xt)]\n\u0003\n(5), such\nthat the loss function\nℓx0\nθ (xt) =\n\r\r\r\rϵθ(t, xt) −xt −√¯αtx0\n√1 −¯αt\n\r\r\r\r\n2\n.\n(55)\nThis clarifies the equivalence of our objective (14) to general adversarial training.\nD\nADVERSARIAL TRAINING ON CONSISTENCY TRAINING MODEL\nIn (Song et al., 2023), the consistency model can be even trained without estimator ˆϕs. They prove\nthat the empirical consistency distillation loss ˆLCD(θ) can be approximated by the following LCT (θ)\nLCT (θ) =\nT −1\nX\nt=0\nExt+1∼q(xt+1)\n\u0002\n∥fθ(xt, t) −fθ(xt+1, t + 1)∥2\u0003\n.\n(56)\n21\n\nPublished as a conference paper at ICLR 2025\nIn our adversarial regime, we can also prove that the desired ˆLAdv\nCD (θ) can be approximated by the\nfollowing LAdv\nCT (θ) with adversarial perturbation\nLAdv\nCT (θ) =\nT −1\nX\nt=0\nExt+1∼q(xt+1)\n\"\nsup\n∥δ∥≤η\n∥fθ(xt + δ, t) −fθ(xt+1, t + 1)∥2\n#\n.\n(57)\nThe results can be checked by the following theorem.\nTheorem 5. Suppose fθ(xt, t) is twice continuously differentiable with a bounded second derivative.\nThen\nˆLAdv\nCD (θ) ≲LAdv\nCT (θ) + O\n \nT −\nT\nX\nt=1\n√αt + Tη2\n!\n,\n(58)\nwhere “≲” means approximately less than or equal.\nProof. Due to the continuity of fθ(x, t), for any δ with ∥δ∥≤η, by Taylor’s expansion on xt+1\nfrom xt + δ, we have\nE\n\u0002\n∥fθ(xt + δ, t) −fθ(xt+1, t + 1)∥2\u0003\n= E\n\u0002\n∥fθ(xt+1, t) −fθ(xt+1, t + 1)∥2\u0003\n+ E\nh\n(fθ(xt+1, t) −fθ(xt+1, t + 1))⊤∇fθ(xt+1, t)(xt + δ −xt+1)\ni\n+ O\n\u0000E\n\u0002\n∥xt+1 −xt −δ∥2\u0003\u0001\n.\n(59)\nDue to the Taylor’s expansion fθ(xt + δ, t) = fθ(xt+1, t) + ∇fθ(xt+1, t)(xt + δ −xt+1) +\nO(∥xt+1 −xt −δ∥2). Then, from the formulation of xt, we know E\n\u0002\n∥xt+1 −xt −δ∥2\u0003\n=\nO(1 −√αt + η2). Noting that due to definition of st, we have\nE[xt | xt+1 = zst+1] = E[zst | zst+1]\n=\n1\n√αt+1\n\u0000zst+1 −(1 −αt+1)∇x log qst+1(zst+1)\n\u0001\n= exp\n\u00121\n2\nZ st+1\nst\nβsds\n\u0013 \u0010\nzst+1 −\n\u0010\n1 −e\nR st+1\nst\nβsds\u0011\n∇z log qst+1(zst+1)\n\u0011\n≈\n\u0012\n1 + 1\n2\nZ st+1\nst\nβsds\n\u0013\nzst+1 + 1\n2\nZ st+1\nst\nβsds∇z log qst+1(zst+1)\n≈ˆΨst(zst+1,\np\n1 −¯αt+1∇z log qst+1),\n(60)\nwhere the first equality is due to Tweedie’s formula i.e., Lemma 11 in (Bao et al., 2022), the “≈” is\ndue to ea ≈1 + a when a →0, and the last ≈is due to Euler-Mayaruma discretion. Due to this, we\nnotice that\nE\nh\n(fθ(xt+1, t) −fθ(xt+1, t + 1))⊤∇fθ(xt+1, t)(xt + δ −xt+1) | xt+1 = zst+1\ni\n= E\nh\n(fθ(xt+1, t) −fθ(xt+1, t + 1))⊤∇fθ(xt+1, t)\n\u0000E\n\u0002\nxt + δ | xt+1 = zst+1\n\u0003\n−xt+1\n\u0001\n| xt+1 = zst+1\ni\n≈E\nh\n(fθ(zst+1, t) −fθ(zst+1, t + 1))⊤∇fθ(zst+1, t)\n\u0010\nˆΨst(zst+1, ∇z log qst+1) + E[δ | zst+1] −zt+1\n\u0011i\n,\n(61)\nwhere the first equality is due to the property of conditional expectation, and the second “≈” is due to\n(60). Combining this with (59), we have\nE\n\u0002\n∥fθ(xt + δ, t) −fθ(xt+1, t + 1)∥2 | xt+1 = zst+1\n\u0003\n= E\nh\r\rfθ(zst + δ, t) −fθ(zst+1, t + 1)\n\r\r2 | zst+1\ni\n= E\nh\r\rfθ(zst+1, t) −fθ(zst+1, t + 1)\n\r\r2i\n+ E\nh\n(fθ(zst+1, t) −fθ(zst+1, t + 1))⊤∇fθ(zst+1, t)\n\u0010\nˆΨst(zst+1, ∇z log qst+1) + E[δ | zst+1] −zst+1\n\u0011i\n+ O(1 −√αt + η2)\n= E\n\u0014\r\r\rfθ(ˆΨst(zst+1, ∇z log qst+1) + δ, t) −fθ(zst+1, t + 1)\n\r\r\r\n2\u0015\n+ O(1 −√αt + η2),\n(62)\nwhere the last equality is due to Taylor’s expansion from fθ(ˆΨst(zst+1, ∇z log qst+1) + δ, t) to\nfθ(zst+1, t). Due to the arbitrariness of δ, we prove our conclusion.\n22\n\nPublished as a conference paper at ICLR 2025\nE\nIMPLEMENTATION DETAILS\nE.1\nHYPERPARAMETERS OF DIFFUSION MODELS\nFor the diffusion models, all methods adopt the ADM model (Dhariwal & Nichol, 2021) as the\nbackbone and follow the same training pipeline. Following existing work (Dhariwal & Nichol, 2021;\nNing et al., 2023), we train models using the AdamW optimizer (Loshchilov & Hutter, 2019) with\nmixed precision training and the EMA rate is set to 0.9999. For CIFAR-10, the pretrained ADM is\ntrained using a batch size of 128 for 250K iterations with a learning rate set to 1e-4. For ImageNet,\nthe pretrained model is trained with a batch size of 1024 for 400K iterations, employing a learning\nrate of 3e-4. The models are trained in a cluster of NVIDIA Tesla V100s. More hyperparameters are\nreported in Table 4.\nTable 4: Hyperparameters of diffusion model on each datasets.\nHyperparameters\nCIFAR10 32 × 32\nImageNet 64 × 64\nChannels\n128\n192\nBatch size\n128\n1024\nLearning rate\n1e-4\n3e-4\nFine-tuning iterations\n200K\n200K\nDropout\n0.3\n0.1\nNoise schedule\nCosine\nCosine\nE.2\nHYPERPARAMETERS OF LATENT CONSISTENCY MODELS\nFor experiments on Latent Consistency Models (LCM) (Luo et al., 2023), we train models on\nLAIOIN-Aesthetic-6.5+ (Schuhmann et al., 2022) at the resolution of 512×512, comprising 650K\ntext-image pairs with predicted aesthetic scores higher than 6.5. Stable Diffusion v1.5 (Rombach\net al., 2022) is adopted as the teacher model and initialized the student and target models in the latent\nconsistency distillation framework. We set the range of the guidance scale [wmin, wmax] = [3, 5]\nduring training and use w = 4 in sampling because it performs better in our preliminary experiments,\nwhich is similar to DMD (Yin et al., 2024). The models are trained in a cluster of NVIDIA Tesla\nV100s. Both models of our AT and the original LCM training are trained from scratch with the\nsame hyperparameters. We select the adversarial learning rate α from {0.02, 0.05} and adversarial\nstep K from {2, 3}. More details of hyperparameters are shown in Table 5 and other details of\nimplementations can be found in the original LCM paper (Luo et al., 2023).\nTable 5: Hyperparameters of latent consistency model.\nHyperparameters\nLAIOIN-Aesthetic-6.5+\nBatch size\n64\nLearning rate\n8e-6\nTraining iterations\n100K\nEMA rate of target model\n0.95\nConditional guidance scale [wmin, wmax]\n[3, 5]\nF\nADDITIONAL RESULTS\nF.1\nRESULTS OF CLASSIFICATION ACCURACY SCORE\nClassification Accuracy Score (CAS) (Ravuri & Vinyals, 2019) is proposed to evaluate the utility of\nthe images produced by the generative model for downstream classification tasks. The underlying\nmotivation for this metric is that if the generative model captures the real data distribution, the\nreal data distribution can be replaced by the model-generated data and achieve similar results on\ndownstream tasks like image classification.\n23\n\nPublished as a conference paper at ICLR 2025\nTable 6: Comparasion of CAS of different methods on CIFAR-10 32×32 dataset.\nMethods\nCAS\nReal\n92.5\nonly using the synthetic data.\nADM\n91.0\nADM-IP\n89.2\nADM-AT (Ours)\n91.6\nusing the synthetic data with real data.\nADM\n95.0\nADM-IP\n94.9\nADM-AT (Ours)\n95.4\nFollowing the evaluation pipeline in Ravuri & Vinyals (2019), we train the image classifier in two\nsettings: only on synthetic data or real data augmented with synthetic data, and use the classifier to\npredict labels on the test set of real data. Synthetic images are generated with a DDIM sampler under\n20 NFEs. We use ResNet-18 (He et al., 2016) as the image classifier and train it for 200 epochs with a\nlearning rate of 0.1 and a batch size of 128. We report CAS in the CIFAR-10 dataset at a resolution\nof 32×32 in Table 6. The results indicate that our method consistently performs better than other\nbaseline methods on CAS metric in both settings. Although CAS with synthetic data cannot surpass\nreal data, it demonstrates significant potential for enhancing classifier accuracy when employed as an\naugmentation technique alongside real data.\nTable 7: Comparasion of AT with TS-DDIM on CIFAR10 32×32. Both models are based on the\nADM backbone. The results of TS are taken directly from the original paper.\nMethods \\ NFEs\n50\n20\n10\n5\nADM-TS-DDIM\n3.52\n5.35\n10.73\n26.94\nADM-AT (Ours)\n3.07\n4.40\n9.30\n26.38\nF.2\nCOMPARISON TO TS-DDIM\nLi et al. (2024) introduces another approach named Time-Shift (TS) to alleviate the DPM distribution\nmismatch by searching for coupled time steps in sampling. Table 7 shows the comparison between\nour AT method with TS on CIFAR-10 with the DDIM Sampler. Both methods are based on the\nADM pretrained model (Dhariwal & Nichol, 2021) as a backbone, which is the same as Section 6.2.\nWe observe our method consistently better than the TS method across various sampling steps.\nF.3\nRESULTS OF MORE NFES\nWe present results obtained with various samplers under 100 or 200 NFEs on CIFAR10 32x32 and\nImageNet 64x64 in Table 8 and Table 9, respectively. The results show that our method is still\neffective for samplers under hundreds of NFEs.\nF.4\nRESULTS OF MORE METRICS\nWe present the results of more generation quality metrics, including sFID, Inception Score (IS),\nPrecision, and Recall, on CIFAR10 32x32 (Table 10 and Table 11) and ImageNet 64x64 (Table 12\nand Table 13). The evaluation is performed following Dhariwal & Nichol (2021). We observe that\nour method shows effectiveness across these metrics.\n24\n\nPublished as a conference paper at ICLR 2025\nTable 8: Sample quality measured by FID ↓of various sampling methods of DPM under 100 or 200\nNFEs on CIFAR10 32x32.\nMethods\nIDDPM\nDDIM\nES\nDPM-Solver\n100\n200\n100\n200\n100\n200\n100\n200\nADM-FT\n3.34\n3.02\n4.02\n4.22\n2.38\n2.45\n2.97\n2.97\nADM-IP\n2.83\n2.73\n6.69\n8.44\n2.97\n3.12\n10.10\n10.11\nADM-AT (Ours)\n2.52\n2.46\n3.19\n3.23\n2.18\n2.35\n2.83\n3.00\nTable 9: Sample quality measured by FID ↓of various sampling methods of DPM under 100 or 200\nNFEs on ImageNet 64x64.\nMethods\nIDDPM\nDDIM\nES\nDPM-Solver\n100\n200\n100\n200\n100\n200\n100\n200\nADM-FT\n3.88\n3.48\n4.71\n4.38\n3.07\n2.98\n4.20\n4.13\nADM-IP\n3.55\n3.08\n8.53\n10.43\n3.36\n3.31\n9.75\n9.77\nADM-AT (Ours)\n3.35\n3.16\n4.58\n4.34\n3.05\n3.10\n4.31\n4.10\nTable 10: Comparison of sFID ↓and IS ↑on CIFAR10 32x32.\n(a) IDDPM\n5\n8\n10\n20\n50\nsFID\nIS\nsFID\nIS\nsFID\nIS\nsFID\nIS\nsFID\nIS\nADM\n20.95\n8.25\n25.03\n8.51\n23.56\n8.50\n16.01\n9.14\n6.81\n9.49\nADM-IP\n25.81\n7.02\n24.51\n8.04\n19.02\n8.50\n8.99\n9.28\n5.32\n9.66\nADM-AT\n19.78\n8.71\n25.67\n8.66\n23.09\n8.77\n6.01\n9.30\n5.04\n9.65\n(b) DDIM\n5\n8\n10\n20\n50\nsFID\nIS\nsFID\nIS\nsFID\nIS\nsFID\nIS\nsFID\nIS\nADM\n12.75\n7.76\n8.53\n8.62\n8.39\n8.70\n6.19\n9.08\n4.99\n9.19\nADM-IP\n15.53\n7.55\n8.00\n8.98\n7.12\n9.15\n5.30\n9.41\n5.64\n9.49\nADM-AT\n12.56\n7.97\n7.93\n8.90\n7.08\n8.90\n5.37\n9.17\n4.66\n9.51\n(c) ES\n5\n8\n10\n20\n50\nsFID\nIS\nsFID\nIS\nsFID\nIS\nsFID\nIS\nsFID\nIS\nADM\n27.39\n6.14\n14.91\n8.33\n10.04\n8.79\n5.45\n9.55\n4.12\n9.62\nADM-IP\n34.70\n5.73\n16.84\n8.23\n10.89\n8.88\n4.94\n9.59\n4.08\n9.70\nADM-AT\n16.84\n6.97\n10.33\n8.60\n8.00\n8.95\n4.78\n9.65\n4.04\n9.77\n(d) DPM-Solver\n5\n8\n10\n20\n50\nsFID\nIS\nsFID\nIS\nsFID\nIS\nsFID\nIS\nsFID\nIS\nADM\n11.82\n8.00\n5.79\n9.12\n5.05\n9.41\n4.43\n9.78\n4.32\n9.82\nADM-IP\n26.46\n7.09\n5.93\n9.19\n5.49\n9.45\n7.53\n9.66\n8.37\n9.75\nADM-AT\n11.19\n8.43\n5.10\n9.35\n5.29\n9.65\n4.75\n10.03\n4.59\n9.93\nG\nMORE ANALYSIS\nG.1\nEFFICIENT AT VS STANDARD AT\nIn this section, we conduct an ablation of the AT method in diffusion model training. We compare\nthe performance of our used efficient AT and a standard AT method PGD on CIFAR-10 dataset at\nthe resolution of 32×32. The adversarial step K is set to be 3 for both methods. We fine-tune both\n25\n\nPublished as a conference paper at ICLR 2025\nTable 11: Comparison of Precision (P) ↑and Recall (R) ↑on CIFAR10 32x32.\n(a) IDDPM\n5\n8\n10\n20\n50\nP\nR\nP\nR\nP\nR\nP\nR\nP\nR\nADM\n0.54\n0.47\n0.59\n0.45\n0.61\n0.46\n0.64\n0.52\n0.68\n0.58\nADM-IP\n0.54\n0.39\n0.59\n0.43\n0.61\n0.46\n0.66\n0.54\n0.68\n0.59\nADM-AT\n0.52\n0.47\n0.57\n0.45\n0.62\n0.46\n0.68\n0.55\n0.69\n0.59\n(b) DDIM\n5\n8\n10\n20\n50\nP\nR\nP\nR\nP\nR\nP\nR\nP\nR\nADM\n0.57\n0.47\n0.59\n0.52\n0.61\n0.52\n0.64\n0.52\n0.63\n0.60\nADM-IP\n0.57\n0.44\n0.62\n0.53\n0.63\n0.56\n0.65\n0.60\n0.65\n0.61\nADM-AT\n0.59\n0.46\n0.62\n0.52\n0.63\n0.54\n0.65\n0.58\n0.66\n0.61\n(c) ES\n5\n8\n10\n20\n50\nP\nR\nP\nR\nP\nR\nP\nR\nP\nR\nADM\n0.54\n0.37\n0.60\n0.48\n0.61\n0.52\n0.64\n0.52\n0.63\n0.60\nADM-IP\n0.46\n0.32\n0.58\n0.45\n0.62\n0.51\n0.67\n0.58\n0.68\n0.60\nADM-AT\n0.61\n0.45\n0.64\n0.51\n0.65\n0.54\n0.65\n0.58\n0.66\n0.61\n(d) DPM-Solver\n5\n8\n10\n20\n50\nP\nR\nP\nR\nP\nR\nP\nR\nP\nR\nADM\n0.61\n0.47\n0.65\n0.58\n0.65\n0.59\n0.66\n0.61\n0.63\n0.62\nADM-IP\n0.49\n0.32\n0.65\n0.58\n0.65\n0.59\n0.62\n0.58\n0.61\n0.56\nADM-AT\n0.62\n0.49\n0.65\n0.59\n0.65\n0.61\n0.67\n0.62\n0.65\n0.61\nmodels from the same pretrained ADM model with 100K update iterations of the model. The results\nare shown in Table 14. We report the results of 4 sampler settings (method-NFEs): IDDPM-50,\nDDIM-50, ES-20, and DPM-Solver-10.\nWe observe that efficient AT achieves performance comparable to or even better than PGD with the\nsame model update iterations while accelerating the training (2.6× speed-up). Thus, we propose\napplying the efficient AT method for our adversarial training framework.\nG.2\nCONVERGENCE OF AT ON DIFFUSION MODELS\n100K\n150K\n200K\nIterations\n5\n6\n7\n8\n9\nFID\nDDIM-50\nMethod\nADM\nADM-IP\nADM-AT (ours)\nFigure 2: The convergence of methods trained from scratch on CIFAR-10 32 × 32. We use the\nDDIM sampler with 50 NFEs for sampling.\n26\n\nPublished as a conference paper at ICLR 2025\nTable 12: Comparison of sFID ↓and IS ↑on ImageNet 64x64.\n(a) IDDPM\n5\n8\n10\n20\n50\nsFID\nIS\nsFID\nIS\nsFID\nIS\nsFID\nIS\nsFID\nIS\nADM\n26.17\n12.55\n36.34\n22.61\n40.52\n26.55\n26.08\n39.10\n11.35\n45.68\nADM-IP\n40.90\n12.19\n47.98\n23.47\n37.72\n27.86\n25.06\n39.40\n6.75\n44.87\nADM-AT\n24.82\n14.50\n37.04\n23.84\n36.50\n30.03\n22.83\n39.12\n5.69\n46.25\n(b) DDIM\n5\n8\n10\n20\n50\nsFID\nIS\nsFID\nIS\nsFID\nIS\nsFID\nIS\nsFID\nIS\nADM\n27.74\n14.30\n14.27\n25.88\n12.78\n28.29\n8.84\n33.54\n6.31\n38.08\nADM-IP\n52.08\n10.21\n16.40\n22.03\n11.70\n25.94\n9.09\n32.04\n15.14\n31.62\nADM-AT\n25.49\n14.82\n10.68\n26.62\n9.22\n29.29\n6.41\n34.33\n4.66\n39.36\n(c) ES\n5\n8\n10\n20\n50\nsFID\nIS\nsFID\nIS\nsFID\nIS\nsFID\nIS\nsFID\nIS\nADM\n34.55\n13.29\n42.32\n24.98\n34.44\n29.36\n14.44\n40.45\n6.41\n45.36\nADM-IP\n44.81\n10.07\n41.01\n22.44\n30.12\n27.66\n10.13\n39.50\n4.67\n44.69\nADM-AT\n29.72\n16.49\n33.58\n27.85\n27.64\n31.94\n10.22\n42.18\n5.10\n45.59\n(d) DPM-Solver\n5\n8\n10\n20\n50\nsFID\nIS\nsFID\nIS\nsFID\nIS\nsFID\nIS\nsFID\nIS\nADM\n25.70\n24.34\n11.08\n34.77\n8.05\n37.45\n5.35\n40.54\n4.69\n41.31\nADM-IP\n42.68\n16.93\n7.47\n33.85\n7.22\n33.57\n14.74\n31.29\n18.99\n30.32\nADM-AT\n20.79\n26.32\n7.60\n34.89\n6.36\n36.51\n4.51\n38.79\n4.22\n39.10\n0\n50K\n150K\n250K\nIterations\n7\n8\n9\n10\nFID\nIDDPM-20\n0\n50K\n150K\n250K\nIterations\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\nFID\nDDIM-50\n0\n50K\n150K\n250K\nIterations\n4.4\n4.6\n4.8\n5.0\n5.2\n5.4\n5.6\nFID\nES-20\n0\n50K\n150K\n250K\nIterations\n4.5\n5.0\n5.5\n6.0\n6.5\n7.0\n7.5\n8.0\n8.5\nFID\nDPM-Solver-10\nMethod\nADM\nADM-IP\nADM-AT (ours)\nFigure 3: The convergence of methods fine-tuned from a same pretrained model on CIFAR-10\n32 × 32. We compare the performance of methods on various samplers.\nIn classification tasks, adding adversarial perturbations usually slows the convergence of model\ntraining (Zhu et al., 2020). We are interested to see whether AT also affects the convergence of the\ndiffusion training process.\nFirstly, we explore the convergence of models trained from scratch. We utilize DDIM as the sampler\nwith 50 NFEs and the results are shown in Figure 2. We observe that our AT method and ADM-IP\nexhibit slower convergence compared to ADM at the beginning (before 100K iterations), while as\ntraining more iterations (200K), our AT method shows a notable advantage.\nMoreover, we explore the convergence of models under fine-tuning setting and the results are shown\nin Figure 3. We observe under this setting, when given a pretrained diffusion model like ADM,\nfine-tuning it with our proposed AT improves performance faster than other baselines. Overall, we\nobserve that incorporating AT with a diffusion framework does not affect the convergence of the\nmodel much, especially in the fine-tuning setting.\n27\n\nPublished as a conference paper at ICLR 2025\nTable 13: Comparison of Precision (P) ↑and Recall (R) ↑on ImageNet 64x64.\n(a) IDDPM\n5\n8\n10\n20\n50\nP\nR\nP\nR\nP\nR\nP\nR\nP\nR\nADM\n0.34\n0.48\n0.46\n0.50\n0.51\n0.48\n0.65\n0.52\n0.73\n0.57\nADM-IP\n0.39\n0.39\n0.50\n0.45\n0.56\n0.48\n0.68\n0.55\n0.73\n0.60\nADM-AT\n0.40\n0.50\n0.50\n0.50\n0.55\n0.49\n0.69\n0.52\n0.77\n0.59\n(b) DDIM\n5\n8\n10\n20\n50\nP\nR\nP\nR\nP\nR\nP\nR\nP\nR\nADM\n0.42\n0.47\n0.54\n0.56\n0.58\n0.58\n0.65\n0.60\n0.69\n0.61\nADM-IP\n0.38\n0.40\n0.51\n0.53\n0.55\n0.57\n0.63\n0.61\n0.62\n0.61\nADM-AT\n0.44\n0.43\n0.58\n0.55\n0.62\n0.56\n0.69\n0.59\n0.72\n0.61\n(c) ES\n5\n8\n10\n20\n50\nP\nR\nP\nR\nP\nR\nP\nR\nP\nR\nADM\n0.40\n0.44\n0.52\n0.47\n0.58\n0.48\n0.69\n0.55\n0.73\n0.59\nADM-IP\n0.37\n0.35\n0.49\n0.44\n0.56\n0.49\n0.68\n0.57\n0.72\n0.60\nADM-AT\n0.44\n0.46\n0.58\n0.48\n0.63\n0.49\n0.73\n0.55\n0.76\n0.59\n(d) DPM-Solver\n5\n8\n10\n20\n50\nP\nR\nP\nR\nP\nR\nP\nR\nP\nR\nADM\n0.51\n0.49\n0.65\n0.58\n0.67\n0.60\n0.69\n0.62\n0.69\n0.62\nADM-IP\n0.39\n0.44\n0.64\n0.60\n0.64\n0.60\n0.59\n0.60\n0.57\n0.59\nADM-AT\n0.56\n0.50\n0.68\n0.57\n0.69\n0.59\n0.72\n0.60\n0.71\n0.61\nTable 14: Comparison of different AT methods used in our AT framework. All models are trained\nwith the same model-updating iterations while the efficient AT has less training time.\nMethods\nFID\nTraining Time\nIDDPM-50\nDDIM-50\nES-20\nDPM-Solver-10\nSpeedup\nStandard AT PGD-3\n4.02\n3.37\n6.42\n7.60\n1.0×\nEfficient AT (Ours)\n3.97\n3.42\n5.98\n6.05\n2.6×\nTable 15: Comparison of different adversarial learning rate α of our AT framework on CIFAR10\n32x32. IDDPM is adopted as the inference sampler.\nα \\ NFEs\n5\n8\n10\n20\n50\nα = 0.05\n51.72\n32.09\n25.48\n10.38\n4.36\nα = 0.1\n37.15\n23.59\n15.88\n6.60\n3.34\nα = 0.5\n63.73\n40.08\n27.57\n7.23\n3.42\nTable 16: Comparison of different adversarial learning rate α of our AT framrwork on ImageNet\n64x64. IDDPM is adopted as the inference sampler.\nα \\ NFEs\n5\n8\n10\n20\n50\nα = 0.1\n56.92\n27.39\n24.06\n10.17\n5.82\nα = 0.5\n45.65\n23.79\n19.18\n8.28\n4.01\nα = 0.8\n46.92\n28.46\n22.47\n9.70\n4.25\n28\n\nPublished as a conference paper at ICLR 2025\nTable 17: Comparison of different perturbation norms (l1, l2 l∞) of our AT framework on CIFAR10\n32x32.\nPerturbation Norm\nIDDPM-50\nDDIM-50\nES-20\nDPM-Solver-10\nl1\n4.45\n4.91\n4.72\n5.05\nl2\n3.34\n3.07\n4.36\n4.81\nl∞\n3.87\n3.63\n4.48\n5.32\nG.3\nMORE ABLATION STUDY\nAblation on α\nWe investigate the impact of adversarial learning rate α in our framework. The\nresults of various α on CIFAR10 32x32 and ImageNet 64x64 are shown in Table 15 and Table 16,\nrespectively. We observe that α set to 0.1 is better on CIFAR10 32x32 and α = 0.5 is better for\nImageNet 64x64. That says, the image in larger size corresponds to larger optimal perturbation\nlevel α. We speculate this is because we use the perturbation measured under ℓ2-norm, where the\nℓ2-norm of vector will increase with its dimension.\nAblation on perturbation norm\nDuring our experiments, we adopt ℓ2-adversarial perturbation.\nActually, perturbations in Euclidean space under different ℓp norm are equivalent with each other, e.g.,\nfor vector δ ∈Rd, it holds ∥δ∥∞≤∥δ∥2 ≤\n√\nd∥δ∥∞. Therefore, we select ∥· ∥2 as representation\nin our paper. Next, we explore the proposed ADM-AT under different adversarial perturbations.\nThe results are in Table 17. We found that our method under ℓ2-perturbation is more stable and indeed\nhas better performance, thus we suggest to use ℓ2-perturbation as in the main body of this paper.\nG.4\nQUALITATIVE COMPARISONS\nFigure 4: The qualitative comparsions of ADM-AT (top, FID 6.60), ADM-IP (middle, FID 7.81), and\nADM (bottom, FID 10.58) on CIFAR10 32 × 32. We use the IDDPM sampler with 20 NFEs for\nsampling.\nFigure 4, 5, 6, 7 show the qualitative comparisons between our proposed AT method and baselines.\nOur proposed AT method generates more realistic and higher-fidelity samples. We attribute this to\nour AT algorithm mitigates the distribution mismatch problem.\n29\n\nPublished as a conference paper at ICLR 2025\nFigure 5: The qualitative comparsions of ADM-AT (top, FID 6.20), ADM-IP (middle, FID 8.40)\nand ADM (bottom, FID 8.32) on ImageNet 64 × 64. We use the DDIM sampler with 20 NFEs for\nsampling.\nFigure 6: The qualitative comparsions of LCM (left) and LCM-AT (right) with one-step generation.\nThe text prompt is A photo of beautiful mountain with realistic sunset and blue lake, highly detailed,\nmasterpiece.\nFigure 7: The qualitative comparsions of LCM (left) and LCM-AT (right) with one-step generation.\nThe text prompt is Astronaut in a jungle, cold color palette, muted colors, detailed, 8k.\n30\n",
  "metadata": {
    "source_path": "papers/arxiv/Improved_Diffusion-based_Generative_Model_with_Better_Adversarial\n__Robustness_eb2ec4b67770f7d6.pdf",
    "content_hash": "eb2ec4b67770f7d65fdf36ff4a645ee1120857e623259e8d455529ad96bb4dc6",
    "arxiv_id": null,
    "title": "Improved_Diffusion-based_Generative_Model_with_Better_Adversarial\n__Robustness_eb2ec4b67770f7d6",
    "author": "",
    "creation_date": "D:20250225024435Z",
    "published": "2025-02-25T02:44:35",
    "pages": 30,
    "size": 1343888,
    "file_mtime": 1740470187.363268
  }
}