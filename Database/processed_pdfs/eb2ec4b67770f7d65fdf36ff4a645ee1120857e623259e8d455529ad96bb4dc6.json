{
  "text": "Published as a conference paper at ICLR 2025\nIMPROVED DIFFUSION-BASED GENERATIVE MODEL\nWITH BETTER ADVERSARIAL ROBUSTNESS\nZekun Wangâˆ—1, Mingyang Yiâˆ—â€ 2, Shuchen Xue3,4, Zhenguo Li5, Ming Liuâ€ 1, Bing Qin1,\nZhi-Ming Ma3,4\n1Harbin Institute of Technology\n2Renmin University of China\n3Academy of Mathematics and Systems Science, Chinese Academy of Sciences\n4University of Chinese Academy of Sciences\n5Huawei Noahâ€™s Ark Lab\nzkwang@ir.hit.edu.cn\nyimingyang@ruc.edu.cn\nxueshuchen17@mails.ucas.ac.cn\nABSTRACT\nDiffusion Probabilistic Models (DPMs) have achieved significant success in gener-\native tasks. However, their training and sampling processes suffer from the issue of\ndistribution mismatch. During the denoising process, the input data distributions\ndiffer between the training and inference stages, potentially leading to inaccurate\ndata generation. To obviate this, we analyze the training objective of DPMs and the-\noretically demonstrate that this mismatch can be alleviated through Distributionally\nRobust Optimization (DRO), which is equivalent to performing robustness-driven\nAdversarial Training (AT) on DPMs. Furthermore, for the recently proposed Con-\nsistency Model (CM), which distills the inference process of the DPM, we prove\nthat its training objective also encounters the mismatch issue. Fortunately, this\nissue can be mitigated by AT as well. Based on these insights, we propose to\nconduct efficient AT on both DPM and CM. Finally, extensive empirical studies\nvalidate the effectiveness of AT in diffusion-based models. The code is available at\nhttps://github.com/kugwzk/AT_Diff.\n1\nINTRODUCTION\nDiffusion Probabilistic Models (DPMs) (Ho et al., 2020; Song et al., 2020; Yi et al., 2024) have\nachieved remarkable success across a wide range of generative tasks such as image synthesis (Dhari-\nwal & Nichol, 2021; Rombach et al., 2022; Ho et al., 2022a), video generation (Ho et al., 2022b;\nBlattmann et al., 2023), text-to-image generation (Nichol et al.; Ramesh et al., 2022; Saharia et al.,\n2022), etc. The core mechanism of DPMs involves a forward diffusion process that progressively\ninjects noise into the data, followed by a reverse process that learns to generate data by denoising the\nnoise. Unlike traditional generative models such as GANs(Goodfellow et al., 2014) or VAEs (Kingma\n& Welling, 2013), which directly map an easily sampled latent variable (e.g., Gaussian noise) to the\ntarget data through a single network function evaluation (NFE), DPMs adopt a gradual denoising\napproach that requires multiple NFEs (Song et al., 2022; Salimans & Ho, 2022; Lu et al., 2022b;\nMa et al., 2024). However, this noising-then-denoising process introduces a distribution mismatch\nbetween the training and sampling stages, potentially leading to inaccuracies in the generated outputs.\nConcretely, during the training stage, the model is learned to predict the noise in ground-truth noisy\ndata derived from the training set. In contrast, during the inference stage, the input distribution is\nobtained from the output generated by the DPM in the previous step, which differs from the training\nphase, caused by the inaccurate estimation of the score function due to training (Song et al., 2021;\nYi et al., 2023a) and the discretization error (Chen et al., 2022; Li et al., 2023; Xue et al., 2024b;a)\nbrought by sampling. Such distribution mismatches are referred to as Exposure Bias, which has been\ndiscussed in auto-regressive language models (Bengio et al., 2015; Ranzato et al., 2016).\nRecently, the aforementioned distribution mismatch problem in diffusion has been also recognized\nby (Ning et al., 2023; Li & van der Schaar, 2024; Ren et al., 2024; Ning et al., 2024; Li et al.,\nâˆ—Equal contribution\nâ€ Corresponding author\n1\narXiv:2502.17099v1  [cs.LG]  24 Feb 2025\n\nPublished as a conference paper at ICLR 2025\n2024; Lou & Ermon, 2023). However, these studies are either rely on strong mismatch distributional\nassumptions (e.g., Gaussian) (Ning et al., 2023; 2024; Ren et al., 2024) or incur significant additional\ncomputational costs (Li & van der Schaar, 2024). This indicates that a more practical solution to\nthis problem has been overlooked until now. To bridge this gap, we begin with the discrete DPM\nintroduced in (Ho et al., 2020). Intuitively, although there is a mismatch between training and\ninference, the distributions of intermediate noise generated during the inference stage are close to\nthe ground-truth distributions observed during training. Therefore, improving the distributional\nrobustness (Yi et al., 2021; Namkoong, 2019; Shapiro, 2017) (which measures the robustness of the\nmodel to distributional perturbations in training data) of DPM mitigates the distribution mismatch\nproblem. To achieve this, we refer to Distribution Robust Optimization (DRO) (Shapiro, 2017;\nNamkoong, 2019), which aims to improve the distributional robustness of models. We then prove that\napplying DRO to DPM is mathematically equivalent to implementing robustness-driven Adversarial\nTraining (AT) (Madry et al., 2018; Shafahi et al., 2019; Yi et al., 2021) on DPM. 1 Following the DRO\nframework, we also analyze the recently proposed diffusion-based Consistency Model (CM) (Song\net al., 2023; Luo et al., 2023) which distills the trajectory of DPM into a model with one NFE\ngeneration. We first prove that the training objective of CM similarly suffers from the mismatch issue\nas in multi-step DPM. Moreover, the issue can also be mitigated by implementing AT. Therefore,\nfor both DPM and CM, we propose to apply efficient AT (e.g., â€œFree-ATâ€ (Shafahi et al., 2019))\nduring their training stages to mitigate the distribution mismatch problem.2 Finally, we summarize\nour contributions as follows.\nâ€¢ We conduct an in-depth analysis of the diffusion-based models (DPM and CM) from a\ntheoretical perspective and systematically characterize its distribution mismatch problem.\nâ€¢ For both DPM and CM, we theoretically show that their mismatch problem is mitigated by\nDRO, which is equivalent to implementing AT with proved error bounds during training.\nâ€¢ We propose to conduct efficient AT on both DPM and CM in various tasks, including\nimage generation on CIFAR10 32Ã—32(Krizhevsky & Hinton, 2009) and ImageNet\n64Ã—64 (Deng et al., 2009), and zero-shot Text-to-Image (T2I) generation on MS-COCO\n512Ã—512 (Lin et al., 2014b). Extensive experimental results illustrate the effectiveness of\nthe proposed AT training method in alleviating the distribution mismatch of DPM and CM.\n2\nRELATED WORK\nDistribution Mismatch in DPM.\nThe problem is analogous to the exposure bias in auto-regressive\nlanguage models (Bengio et al., 2015; Ranzato et al., 2016; Shen et al., 2016; Rennie et al., 2017;\nZhang et al., 2019c), whereas the next word prediction (Radford et al., 2019) relies on tokens predicted\nby the model in the inference stage, which may be mismatched with the ground-truth one taken in the\ntraining stage. The similarity to DPMs becomes evident due to their gradual denoising generation\nprocess. Ning et al. (2023) and Ning et al. (2024) propose adding extra Gaussian perturbation\nduring the training stage or data-dependent perturbation during the inference stage, to mitigate this\nissue. Following this line of work, several methods are further proposed. For instance, to reduce\nthe accumulated discrepancy between the intermediate noisy data in the training and inference\nstages, Li et al. (2024) search for a suboptimal mismatched input time step of the model to conduct\ninference. Similarly, Li & van der Schaar (2024) and Ren et al. (2024) directly minimize the difference\nbetween the generated intermediate noisy data and the ground-truth data. However, these methods\neither rely on strong assumptions (Ning et al., 2023; 2024; Li et al., 2024; Ren et al., 2024) or are\ncomputationally expensive (Li & van der Schaar, 2024). In contrast, we are the first to explore the\ndistribution mismatch problem from the perspective of DRO. Meanwhile, our proposed AT with\nstrong theoretical foundations is both simple and efficient, compared with the existing methods.\nAdversarial Training and DRO.\nIn this paper, we leverage the Distributionally Robust Opti-\nmization (DRO) (Shapiro, 2017; Namkoong, 2019; Yi et al., 2021; Sinha et al., 2018; Wang et al.,\n2022; Yi et al., 2023b) to improve the distributional robustness of DPM and CM, thereby mitigating\n1Note that the â€œadversarialâ€ here refers to perturbation to input training data, instead of the adversarial of\ngenerator-discriminator in GAN (Goodfellow et al., 2014).\n2Notably, the standard AT (Madry et al., 2018) solves a minimax problem that slows the training process.\nThe efficient AT has no extra computational cost compared to the standard training ones (Shafahi et al., 2019).\n2\n\nPublished as a conference paper at ICLR 2025\nthe distribution mismatch problem. As demonstrated in (Sinha et al., 2018; Yi et al., 2021; Lee &\nRaginsky, 2018), we link the DRO with AT (Madry et al., 2018; Goodfellow et al., 2015), which\nis designed to improve the input (instead of distributional) robustness of the model. In supervised\nlearning, the adversarial examples generated by efficient AT methods (Shafahi et al., 2019; Zhang\net al., 2019a;b; Zhu et al., 2020; Jiang et al., 2020) have been proven to be efficient augmented data\nto improve the robustness and generalization performance of models (Rebuffi et al., 2021; Wu et al.,\n2020; Yi et al., 2021). In this paper, we further verify that the AT generated adversarial augmented\nexamples are also beneficial for generative models DPM and CM.\nIn addition, recent studies (Nie et al., 2022; Wang et al., 2023; Zhang et al., 2023) utilize DPM to\ngenerate examples in adversarial training to improve the robustness of the classification model. This is\nquite different from the method in this paper, as we focus on employing AT during training of diffusion-\nbased model to improve its distributional robustness to alleviate the distribution mismatching.\n3\nPRELIMINARY\nDiffusion Probabilistic Models.\nDPM (Sohl-Dickstein et al., 2015; Ho et al., 2020) constructs the\nMarkov chain xt by transition kernel q(xt+1 | xt) = N(âˆšÎ±t+1xt, (1âˆ’Î±t+1)I), where Î±1, Â· Â· Â· , Î±T\nare in [0, 1]. Let Â¯Î±t := Î t\ns=1Î±s, and x0 âˆ¼q be ground-truth data. Then, for xt, it holds\nxt = âˆšÂ¯Î±tx0 +\nâˆš\n1 âˆ’Â¯Î±tÏµt\nt = 1, Â· Â· Â· , T,\n(1)\nwith Ïµt âˆ¼N(0, I). The reverse process pÎ¸(xt | xt+1) is parameterized as\npÎ¸(xt | xt+1) = N(ÂµÎ¸(xt+1, t + 1), Ïƒ2\nt+1I),\n(2)\nwhere Ïƒ2\nt+1 = 1 âˆ’Î±t+1. To learn pÎ¸(xt | xt+1), a standard method is to minimize the following\nevidence lower bound of negative log-likelihood (NLL) (Ho et al., 2020),\nâˆ’Eq [log pÎ¸(x0)] â‰¤Eq\n\u0014\nâˆ’log\npÎ¸(x0:T )\nq(x1:T | x0)\n\u0015\n.\n(3)\nHere, minimizing the ELBO in the r.h.s. of above inequality links to pÎ¸(xt | xt+1) since it is\nequivalent to minimizing the following rewritten objective\nmin\nÎ¸\nï£±\nï£²\nï£³DKL(q(xT ) âˆ¥pÎ¸(xT )) +\nT âˆ’1\nX\nt=0\nDKL(q(xt | xt+1) âˆ¥pÎ¸(xt | xt+1))\n|\n{z\n}\nLt\nï£¼\nï£½\nï£¾,\n(4)\nas in (Ho et al., 2020; Bao et al., 2022; Yi et al., 2023a). Here, the conditional Kullbackâ€“Leibler (KL)\ndivergence DKL(q(xt | xt+1) âˆ¥p(xt | xt+1)) =\nR\nq(xt | xt+1) log q(xt|xt+1)\np(xt|xt+1)dxtdxt+1 (Duchi,\n2016), and minimizing Lt is equivalent to solve the following noise prediction problem\nmin\nÎ¸\nE\nh\r\rÏµÎ¸(âˆšÂ¯Î±tx0 +\nâˆš\n1 âˆ’Â¯Î±tÏµt, t) âˆ’Ïµt\n\r\r2i\n.\n(5)\nWe use âˆ¥Â· âˆ¥p to denote â„“p-norm. Unless specified, the norm âˆ¥Â· âˆ¥refers to the â„“2-norm âˆ¥Â· âˆ¥2. Since\nÂ¯Î±t â†’0 for t â†’T, x0 is obtained by conducting the reverse diffusion process pÎ¸(xt | xt+1) starting\nfrom xT âˆ¼N(0, I) and Ïµ âˆ¼N(0, I), under the learned model ÏµÎ¸ with\nxt =\n1\nâˆšÎ±t+1\n\u0012\nxt+1 âˆ’1 âˆ’Î±t+1\nâˆš1 âˆ’Â¯Î±t+1\nÏµÎ¸(xt+1, t + 1)\n\u0013\n+\np\n1 âˆ’Î±t+1Ïµ.\n(6)\nWasserstein Distance.\nFor integer p > 0, Î“(Âµ, Î½) as the set of union distributions with marginal\nÂµ and Î½, the Wasserstein p-distance (Villani et al., 2009) between distributions Âµ and Î½ with finite\np-moments is\nWp\np(Âµ, Î½) =\ninf\nÎ³âˆˆÎ“(Âµ,Î½) E(x,y)âˆ¼Î³âˆ¥x âˆ’yâˆ¥p\np.\n(7)\n4\nROBUSTNESS-DRIVEN ADVERSARIAL TRAINING OF DIFFUSION MODELS\nIn this section, we formally show that the success of DPM relies on specific conditions, i.e., xt is\nclose to xt+1. Next, to mitigate the drawbacks brought by the restriction, we propose to consider\nthe distribution mismatch problem as discussed in Section 1, and connect the problem to a rewritten\nELBO. Finally, we apply DRO for this ELBO to mitigate the distribution mismatch problem and\nfinally link it to AT to be implemented in practice.\n3\n\nPublished as a conference paper at ICLR 2025\n: ğ‘(ğ‘¥! âˆ£ğ‘¥!\"#)\n: ğ‘$(ğ‘¥! âˆ£ğ‘¥!\"#)\n: ğ‘'(ğ‘¥! âˆ£ğ‘¥!\"#)\n: Matching\nğ‘¥! âˆ¼ğ‘ (ğ‘¥!)\nğ‘¥! âˆ¼ğ‘$ (ğ‘¥!)\nğ‘¥! âˆ¼ğ‘ *(ğ‘¥!)\n: ğµ&!\"(ğ‘ğ‘¥! , ğœ‚')\nğ‘$ ğ‘¥! , ğ‘ * ğ‘¥! âˆˆ\nTraining\nInference\nğ‘¥(\nâ€¦\nğ‘¥!\"#\nmin ğ·!\"(ğ‘(ğ‘¥# âˆ£ğ‘¥#$%)||ğ‘&(ğ‘¥# âˆ£ğ‘¥#$%))\nâ€¦\nğ‘¥'\nâ€¦\nğ‘¥!\"#\nğ‘¥!\nğ‘¥!\nâ€¦\nğ‘¥'\nStandard \nTraining\nAdv  Training\nInference\nğ‘¥(\nâ€¦â€¦\nğ‘¥!\"#\nğ‘¥!\"#\nmin ğ·!\"(ğ‘(ğ‘¥# âˆ£ğ‘¥#$%)||ğ‘&(ğ‘¥# âˆ£ğ‘¥#$%))\nğ‘¥!\nğ‘¥!\nâ€¦â€¦\nğ‘¥'\nğ‘¥'ğ‘¥'\nAdversarial \nTraining\nFigure 1: A comparison between standard training and the proposed distributional robust optimization\nin (12). When minimizing DKL(Ëœqt(xt | xt+1) âˆ¥pÎ¸(xt | xt+1)), the xt+1 is sampled from Ëœqt(xt+1),\nsuch that both Ëœqt(xt+1) in training stage and pÎ¸(xt+1) in inference stage are in BDKL(q(xt+1), Î·0),\nso that pÎ¸(xt) tends to locates in BDKL(q(xt), Î·0) as well as Ëœqt(xt). Then, the distributional\nrobustness captured by (12) guarantees the generated pÎ¸(xt) always locates around q(xt) for all t.\n4.1\nHOW DOES DPM WORKS IN PRACTICE?\nNotably, minimizing (4) potentially obtains a sharp NLL under target distribution q(x0). However,\nin the following proposition, we show that (4) also implicitly minimizes the NLL of each xt.\nProposition 1. The minimization problem (4) is equivalent to minimizing an upper bound of\nEq[âˆ’log pÎ¸(xt)] for any 0 â‰¤t â‰¤T.\nThe proof is provided in Appendix A. It shows that though (4) is proposed to generate x0 âˆ¼q(x0),\nit also guides the model to generate xt such that pÎ¸(xt) approximates the ground-truth distribution\nq(xt). The conclusion is nontrivial as minimizing the ELBO of NLL Eq [âˆ’log pÎ¸(x0)] does not\nnecessarily impose any restrictions on xt for t â‰¥1.\nNext, we will further explain why (4) leads to a small NLL of xt. In Lt of (4), pÎ¸(xt | xt+1)\napproximates q(xt | xt+1) with xt+1 âˆ¼q(xt+1) representing ground-truth data. Consequently,\npÎ¸(xt) approximates q(xt) by recursively applying such a relationship as in the following proposition.\nProposition 2. Suppose pÎ¸(xt | xt+1) matches q(xt | xt+1) well such that\nLt = DKL(q(xt | xt+1) âˆ¥pÎ¸(xt | xt+1)) â‰¤Î³\nT ,\n(8)\nand the discrepancy satisfies DKL(q(xT ) âˆ¥pÎ¸(xT )) â‰¤Î³0, then for any 0 â‰¤t â‰¤T, we have\nDKL(q(xt) âˆ¥pÎ¸(xt)) â‰¤DKL(q(xT ) âˆ¥pÎ¸(xT )) + Lt â‰¤Î³0 + (T âˆ’t)Î³\nT\n.\n(9)\nThe results is similarly obtained in (Chen et al., 2023), while their result is applied for DKL(q(x0) âˆ¥\npÎ¸0), which is narrowed compared with Proposition 2. The proof is provided in Appendix A, which\nformally explains why (4) results in pÎ¸(xt) approximating q(xt). However, this proposition is built\nupon small Lt, and notably, the error introduced by Lt will be accumulated on the r.h.s. of (9), as\nit increases w.r.t. t. This phenomenon is caused by the distribution mismatch problem discussed in\nSection 1. Concretely, in (4), minimizing Lt learns the transition probability pÎ¸(xt | xt+1) based on\nxt+1 âˆ¼q(xt+1), while in practice, xt in (6) is generated from xt+1 âˆ¼pÎ¸(xt+1). The error between\npÎ¸(xt+1) and q(xt+1) will propagates into the error between pÎ¸(xt) and q(xt) as in (9).\nTherefore, owing to the existence of distribution mismatch, only if Lt is minimized, the gap between\npÎ¸(xt) and q(xt) can be guaranteed. However, the following proposition proved in Appendix A\nindicates that Lt is theoretically minimized with restrictions.\nProposition 3. Lt in (4) is well minimized, only if q(xt+1) is Gaussian or âˆ¥xt+1 âˆ’xtâˆ¥â†’0.\nIn practice, the q(xt+1) is usually non-Gaussian. Besides, the gap âˆ¥xt+1 âˆ’xtâˆ¥is not necessarily\nsmall, especially for samplers with few sampling steps, e.g., DDIM (Song et al., 2022), DPM-Solver\n(Lu et al., 2022a). Therefore, in practice, the accumulated error in (9) caused by the distribution\nmismatch problem may become large, and degenerate the quality of x0.\n4\n\nPublished as a conference paper at ICLR 2025\n4.2\nDISTRIBUTIONAL ROBUSTNESS IN DPM\nInspired by the discussion above, we propose a new training objective as the sum of NLLs under xt,\nmin\nÎ¸\nL(Î¸) =\nT\nX\nt=0\nEq [âˆ’log pÎ¸(xt)] .\n(10)\nThen the following proposition constructs ELBOs for each of Eq[âˆ’log pÎ¸(xt)].\nProposition 4. For any distribution Ëœq satisfies Ëœq(xt) = q(xt) for specific t, we have\nEq [âˆ’log pÎ¸(xt)] â‰¤DKL(Ëœq(xt | xt+1) âˆ¥pÎ¸(xt | xt+1))\n|\n{z\n}\nLËœq\nt\n+C,\n(11)\nfor a constant C independent of Î¸.\nThe proof is in Appendix A.2. This proposition generalizes the results in Proposition 1 since Ëœq can be\ntaken as q in Proposition 1. During minimizing LËœq\nt, the transition probability pÎ¸(xt | xt+1) matches\nËœq(xt | xt+1), while xt+1 âˆ¼Ëœq(xt+1) in the training stage has no restriction. Thus, one may take\nËœq(xt+1) â‰ˆpÎ¸(xt+1), then in LËœq\nt, pÎ¸(xt | xt+1) matches Ëœq(xt | xt+1) leads pÎ¸(xt) â‰ˆËœq(xt) =\nq(xt), which mitigates the distribution mismatch problem, when minimizing such LËœq\nt.\nUnfortunately, for each t, obtaining such specific Ëœqt(xt+1) = pÎ¸(xt+1) is computationally expensive\n(Li & van der Schaar, 2024), which prevents us using desired Ëœqt(xt+1). However, we know pÎ¸(xt+1)\nis around q(xt+1). Therefore, by borrowing the idea from DRO (Shapiro, 2017), for each t, we\npropose to minimize the maximal value of LËœqt\nt over all possible Ëœqt(xt+1) around q(xt+1). This leads\nto a small LpÎ¸\nt , as pÎ¸(xt+1) locates around q(xt+1), so that is included in the â€œmaximal rangeâ€.\nTechnically, the DRO-based EBLO of (11) is formulated as follows. Here pÎ¸(xt+1) is supposed in\nBDKL(q(xt+1), Î·0), and it capatures the distributional robustness of pÎ¸(xt | xt+1) w.r.t. input xt+1.\nmin\nÎ¸\nT âˆ’1\nX\nt=0\nLDRO\nt\n(Î¸) = min\nÎ¸\nT âˆ’1\nX\nt=0\nsup\nËœqt(xt+1)âˆˆBDKL (q(xt+1),Î·0)\nDKL(Ëœqt(xt | xt+1) âˆ¥pÎ¸(xt | xt+1));\ns.t.\nËœqt(xt) = q(xt).\n(12)\nHere Ëœqt(xt+1) âˆˆBDKL(q(xt+1), Î·0) means DKL(q(xt+1) âˆ¥Ëœqt(xt+1)) â‰¤Î·0. By solving problem\n(12), if the desired Ëœqt(xt+1) = pÎ¸(xt+1) is in BDKL(q(xt+1), Î·0), then the conditional probability\nin (12) transfers xt+1 âˆ¼pÎ¸(xt+1) to target xt âˆ¼q(xt) is learned, which mitigates the distribution\nmismatch problem. The theoretical clarification is in the following Proposition proved in Appendix\nA.2, which indicates that small DRO loss (12) guarantees the quality of generated x0.\nProposition 5. If LDRO\nt\n(Î¸) â‰¤Î·0 in (12) for all t, and DKL(q(xT ) âˆ¥pÎ¸(xT )) â‰¤Î·0, then\nDKL(q(x0) âˆ¥pÎ¸(x0)) â‰¤Î·0.\nUp to now, we do not know how to compute the DRO-based training objective (12) we derived.\nFortunately, the following theorem corresponds (12) to a â€œperturbedâ€ noise prediction problem similar\nto (5). The theorem is proved in Appendix A.2.\nTheorem 1. There exists Î´t depends on x0 and Ïµt makes (13) equivalent to problem (12).\nmin\nÎ¸\nT âˆ’1\nX\nt=0\nEq(x0),Ïµt\n\"\r\r\r\rÏµÎ¸(âˆšÂ¯Î±tx0 +\nâˆš\n1 âˆ’Â¯Î±tÏµt + Î´t, t) âˆ’Ïµt âˆ’\nÎ´t\nâˆš1 âˆ’Â¯Î±t\n\r\r\r\r\n2#\n.\n(13)\nThis theorem connects the proposed DRO problem (12) with noise prediction problem (13). Naturally,\nwe can solve (13), if we know the exact Î´t. Fortunately, we have the following proposition to\ncharacterize the range of Î´t, and it is proved in Appendix A.2.\nProposition 6. For Î· > 0 and Î´t in (13), âˆ¥Î´tâˆ¥1 â‰¤Î· holds with probability at least 1âˆ’\np\n2(1 âˆ’Â¯Î±t)/Î·.\nThe proposition indicates that for any Î´t depends on x0, Ïµt in (13), it is likely in a small range\n(measured under any â„“p-norm, since they can bound each other in Euclidean space). Thus, to resolve\n(13) (so that (12)), we propose to directly consider the following adversarial training (Madry et al.,\n5\n\nPublished as a conference paper at ICLR 2025\n2018) objective with the perturbation Î´ is taken over its possible range as proved in Proposition 6,\nwhich captures the input (instead of distribution) robustness of model ÏµÎ¸.\nmin\nÎ¸\nT âˆ’1\nX\nt=0\nEq(x0)\n\"\nEq(xt|x0)\n\"\nsup\nÎ´:âˆ¥Î´âˆ¥â‰¤Î·\n\r\r\r\rÏµÎ¸(âˆšÂ¯Î±tx0 +\nâˆš\n1 âˆ’Â¯Î±tÏµt + Î´) âˆ’Ïµt âˆ’\nÎ´\nâˆš1 âˆ’Â¯Î±t\n\r\r\r\r\n2##\n.\n(14)\nWe present a fine-grained connection between (14) and classical AT in Appendix C. Notably, our\nobjective (14) is different from the ones in (Ning et al., 2023), whereas Î´ in it is a Gaussian, and ÏµÎ¸\npredicts Ïµt instead of Ïµt + Î´/âˆš1 âˆ’Â¯Î±t as ours.\nTo make it clear, we summarize the rationale from DRO objective (12) to AT our objective (14).\nSince Theorem 1 shows solving (12) is equivalent to (13), which conducts noise prediction (5) with a\nperturbation Î´t in a small range added (Proposition 6). Thus, we propose to minimize the maximal\nloss over the possible Î´t, which is indeed our AT objective (14).\n5\nADVERSARIAL TRAINING UNDER CONSISTENCY MODEL\nAlthough the DPM generates high-quality target data x0, the multi-step denoising process (6) requires\nnumerous model evaluations, which can be computationally expensive. To resolve this, the diffusion-\nbased consistency model (CM) is proposed in (Song et al., 2023). Consistency model fÎ¸(xt, t)\ntransfers xt âˆ¼q(xt) into a distribution that approximates the target q(x0). fÎ¸ is optimized by the\nfollowing consistency distillation (CD) loss 3\nmin\nÎ¸\nLCD(Î¸) =\nT âˆ’1\nX\nt=0\nExt+1âˆ¼q(xt+1) [d (fÎ¸(Î¦t(xt+1), t), fÎ¸(xt+1, t + 1))] ,\n(15)\nwhere Î¦t(xt+1) is a solution of a specific ordinary differential equation (ODE) ((37) in Appendix\nB) which is a deterministic function transfers xt+1 to xt, i.e., Î¦t(xt+1) âˆ¼q(xt), and d(x, y) is a\ndistance between x and y e.g., â„“1, â„“2 distance.\nRemark 1. In (Song et al., 2023; Luo et al., 2023), the noisy data xt in (15) is described by an ODE\n(37) in Appendix B. However, we use the discrete xt (1) here to unify the notations with Section 4.\nThe two frameworks are mathematically equivalent as all xt in (1) located in the trajectory of ODE\nin (Song et al., 2023). More details of this claim refer to Appendix B.\nNext, we use the following theorem to illustrate that solving problem (15) indeed creates fÎ¸(xt, t)\nwith distribution close target q(x0). The theorem is proved in Appendix B.\nTheorem 2. For LCD(Î¸) in (15) with d(Â·, Â·) is â„“2 distance, then W1(fÎ¸(xt, t), x0) â‰¤\np\ntLCD(Î¸) 4.\nThough solving problem (15) creates the desired CM fÎ¸, computing the exact Î¦t(xt+1) involves\nsolving an ODE as pointed out in Appendix B. Thus, in practice (Song et al., 2023; Luo et al., 2023),\nthe Î¦t(xt+1) is approximated by a computable numerical estimation Ë†Î¦t(xt+1, ÏµÏ•) of it, e.g., Euler\n((42) in Appendix B.1) or DDIM (Song et al., 2023), where ÏµÏ• is a pretrained noise prediction model\nas in (5). Therefore, the practical training objective of (15) becomes\nmin\nÎ¸\nT âˆ’1\nX\nt=0\nË†LCD(Î¸) = Ext+1âˆ¼q(zt)\nh\nd\n\u0010\nfÎ¸(Ë†Î¦t(xt+1, ÏµÏ•), t), fÎ¸(xt+1, t + 1)\n\u0011i\n.\n(16)\nIn (16), Ë†Î¦t(xt+1, ÏµÏ•) is an estimation to Î¦t(xt+1), which causes an inaccurate training objective\nË†LCD in (16), compared with target LCD (15). Thus, this results in the distribution mismatch problem\nin CM, as in DPM of Section 4. However, similar to Section 4.2, if we train fÎ¸ with robustness to the\ngap between Ë†Î¦t(xt+1, ÏµÏ•) and Î¦t(xt+1), the distribution mismatch problem in CM is mitigated.\nTechnically, suppose Î¦t(xt+1) = Ë†Î¦t(xt+1, ÏµÏ•) + Î´t(xt+1), we can consider minimizing the follow-\ning adversarial training objective of CM, if âˆ¥Î´t(xt+1)âˆ¥â‰¤Î· uniformly over t, for some constant Î·,\n3In practice, (15) is updated under target model fÎ¸âˆ’(Î¦t(xt+1), t) with exponential moving average (EMA)\nÎ¸âˆ’under a stop gradient operation. (Song et al., 2023) find that it greatly stabilizes the training process. In this\nsection, we focus on the theory of consistency model and still use Î¸ in formulas.\n4Here W1(fÎ¸(xt, t), x0) is the Wasserstein 1-distance between distributions of fÎ¸(xt, t) and x0.\n6\n\nPublished as a conference paper at ICLR 2025\nAlgorithm 1 Adversarial Training for Diffusion Model\n1: Input: dataset D, model parameter Î¸, learning rate Îº, loss weighting Î»(Â·), adversarial steps K,\nadversarial learning rate Î±\n2: while do not converge do\n3:\nSample x âˆ¼D and t âˆ¼U[1, T]\n4:\nSample Ïµ âˆ¼N(0, I)\n5:\nÎ´ â†0\n6:\nfor i = 1, 2, . . . , K do\n7:\nL â†\n\r\r\rÏµÎ¸(âˆšÂ¯Î±tx0 + âˆš1 âˆ’Â¯Î±tÏµ + Î´) âˆ’Ïµ âˆ’\nÎ´\nâˆš1âˆ’Â¯Î±t\n\r\r\r\n2\nin (14)\n8:\nÎ´ â†Î´ + Î± Â·\nâˆ‡Î´L\nâˆ¥âˆ‡Î´Lâˆ¥\nâ–·maximize perturbation\n9:\nÎ¸ â†Î¸ âˆ’Îº Â· âˆ‡Î¸L\nâ–·update model\n10:\nend for\n11: end while\nAlgorithm 2 Adversarial Training for Consistency Distillation\n1: Input: dataset D, initial model parameter Î¸, learning rate Îº, pretrained noise prediction model\nÏµÏ•, ODE solver Ë†Î¦Â·(Â·, ÏµÏ•, metric d(Â·, Â·), loss weighting Î»(Â·), target model EMA Âµ, adversarial\nsteps K, adversarial learning rate Î±\n2: Î¸âˆ’â†Î¸\n3: while do not converge do\n4:\nSample x âˆ¼D and t âˆ¼U[0, T âˆ’1]\n5:\nSample xt+1 from (1)\n6:\nÎ´ â†0\n7:\nfor i = 1, 2, . . . , K do\n8:\nL â†Î»(t)d(fÎ¸(xt+1, t + 1), fÎ¸âˆ’(Ë†Î¦t(xt+1, ÏµÏ•) + Î´, t)) in (17)\n9:\nÎ´ â†Î´ + Î± Â·\nâˆ‡Î´L\nâˆ¥âˆ‡Î´Lâˆ¥\nâ–·maximize perturbation\n10:\nÎ¸ â†Î¸ âˆ’Îº Â· âˆ‡Î¸L\nâ–·update model\n11:\nÎ¸âˆ’â†stopgrad(ÂµÎ¸âˆ’+ (1 âˆ’Âµ)Î¸)\n12:\nend for\n13: end while\nso that the target Î¦t(xt+1) is included in the maximal range as well.\nË†LAdv\nCD (Î¸) =\nT âˆ’1\nX\nt=0\nExt+1\n\"\nsup\nâˆ¥Î´âˆ¥â‰¤Î·\nd\n\u0010\nfÎ¸(Ë†Î¦t(xt+1, ÏµÏ•) + Î´, t), fÎ¸(xt+1, t + 1)\n\u0011#\n.\n(17)\nBy doing so, the learned model fÎ¸ can be robust to the perturbation brought by Î´t(xt+1), so that\nresults in a small LCD(Î¸), as well as the small W1(fÎ¸(xT , T), x0) as proved in Theorem 2. Next,\nwe use the following theorem to show that âˆ¥Î´t(xt+1)âˆ¥is indeed small, and minimizing Ë†LAdv\nCD (Î¸)\nresults in fÎ¸(xT , T) with distribution approximates x0.\nTheorem 3. Under proper regularity conditions, for 0 â‰¤t < T, we have Ext+1[âˆ¥Î´t(xt+1)âˆ¥] â‰¤o(1).\nOn the other hand, it holds\nW1(fÎ¸(xT , T), x0) â‰¤\nq\nT Ë†LAdv\nCD (Î¸) + o(1).\n(18)\nThe theorem is proved in Appendix B.1, and it indicates that using the proposed adversarial training\nobjective (17) of CM indeed guarantees the learned CM transfers xT into data from q(x0).\n6\nEXPERIMENTS\n6.1\nALGORITHMS\nIn the standard adversarial training method like Projected Gradient Descent (PGD) (Madry et al.,\n2018), the perturbation Î´ is constructed by implementing numbers (3-8) of gradient ascents to Î´\n7\n\nPublished as a conference paper at ICLR 2025\nbefore updating the model, which slows down the training process. To resolve this, we adopt an\nefficient implementation (Shafahi et al., 2019) in Algorithms 1, 2 to solve AT (14) and (17) of DPM\nand CM, which has similar computational cost compared to standard training, and significantly\naccelerate standard AT. Notably, unlike PGD, in Algorithms 1 and 2, every maximization step of\nperturbation Î´ follows an update step of the model Î¸. Thus, the efficient AT do not require further\nback propagations to construct adversarial samples as in PGD. We provide a comparison between our\nefficient AT and standard AT (PGD) with the same update iterations of model Î¸ in Appendix G.1.\nMoreover, we observe that efficient AT can yield comparable and even better performance than PGD\nwhile accelerating the training (2.6Ã— speed-up), further verifying the benefits of our efficient AT. 5\n6.2\nPERFORMANCE ON DPM\nSettings.\nThe experiments are conducted on the unconditional generation on CIFAR-10 32Ã—32\n(Krizhevsky & Hinton, 2009) and the class-conditional generation on ImageNet 64 Ã— 64 (Deng\net al., 2009). Our model and training pipelines in adopted from ADM (Dhariwal & Nichol, 2021)\npaper, where ADM is a UNet-type network (Ronneberger et al., 2015), with strong performance in\nimage generation under diffusion model.\nTo save training costs, our methods and baselines are fine-tuned from pretrained models, rather than\ntraining from scratch. By doing so, we can efficiently assess the performance of methods, which\nis more practical for general scenarios. We also explore training from scratch in Appendix G.2,\nwhich also verifies the effectiveness of our method in this regime. During training, we fine-tune\nthe pretrained models (details are in Appendix E.1) with batch size 128 for 150K iterations under\nlearning rate 1e-4 on CIFAR-10, and batch size 1024 for 50K iterations under learning rate of\n3e-4 on ImageNet. For the hyperparameters of AT, we select the adversarial learning rate Î± from\n{0.05, 0.1, 0.5} and the adversarial step K from {3, 5}. More details are in Appendix E.1.\nWe use the Frechet Inception Distance (FID) (Heusel et al., 2017) to evaluate image quality. Unless\notherwise specified, 50K images are sampled for evaluation. Other results of metric Classification\nAccuracy Score (CAS) (Ravuri & Vinyals, 2019), sFID, Inception Score, Precision, and Recall are in\nAppendix F.1 and F.4 for comprehensive evaluation.\nBaselines.\nFor experiments on diffusion models, we consider the following baselines. 1): the\noriginal pretrained model. Compared with it, we verify whether the models are overfitting during\nfine-tuning. 2): continue fine-tuning the pretrained model, which is fine-tuned with the standard\ndiffusion objective (5). Compared to it, we validate whether performance improvements come only\nfrom more training costs. We also compare with the existing typical method to alleviate the DPM\ndistribution mismatch, 3): ADM-IP (Ning et al., 2023), which adds a Gaussian perturbation to the\ninput data to simulate mismatch errors during the training process. The last two fine-tuning baselines\nare based on the same pretrained model and hyperparameters as in the original literature.\nResults.\nTo verify the effectiveness of our AT method, we conduct experiments with four diffusion\nsamplers: IDDPM (Dhariwal & Nichol, 2021), DDIM (Song et al., 2022), DPM-Solver (Lu et al.,\n2022b), and ES (Ning et al., 2024) under various NFEs. The sampler choices contain the three\nmost popular samplers: IDDPM, DDIM, DPM-Solver, and ES, a sampler that scales down the norm\nof predicted noise to mitigate the distribution mismatch from the perspective of sampling. The\nexperimental results of CIFAR-10 and ImageNet are shown in Table 1 and Table 2, respectively.\nResults of more than hundreds of NFEs are shown in Appendix F.3\nAs can be seen, the proposed AT for DPM significantly improves the performance of the original\npretrained model and outperforms the other baselines (continue fine-tuning and ADM-IP) overall for\nall diffusion samplers and NFEs we take. Moreover, we have the following observarions.\n1): Fewer (practically used) sampling steps (5,10) will result in larger mismatching errors, while\nour AT method demonstrates significant improvements in this regime across various samplers, e.g.,\nAT improves FID 27.72 to 17.36 under 5 NFEs DPM-Solver on ImageNet. This suggests that\nour method is indeed effective in alleviating the distribution mismatch of DPM. The results also\nindicate that our method consistently beats the baseline methods, regardless of stochastic (IDDPM)\n5For the experts in AT, they would recognize that the AT in Algorithms 1, 2 actually constructs the adversarial\naugmented data to improve the performance of the model (Zhu et al., 2020; Jiang et al., 2020; Yi et al., 2021).\n8\n\nPublished as a conference paper at ICLR 2025\nTable 1: Sample quality measured by FID â†“of different sampling methods of DPM under different\nNFEs on CIFAR10 32x32. All models are trained with same iterations (computational costs).\n(a) IDDPM\nMethods \\ NFEs\n5\n8\n10\n20\n50\nADM (original)\n37.99\n26.75\n22.62\n10.52\n4.55\nADM (finetune)\n36.91\n26.06\n21.94\n10.58\n4.34\nADM-IP\n47.57\n26.91\n20.09\n7.81\n3.42\nADM-AT (Ours)\n37.15\n23.59\n15.88\n6.60\n3.34\n(b) DDIM\nMethods \\ NFEs\n5\n8\n10\n20\n50\nADM (original)\n34.28\n14.34\n11.66\n7.00\n4.68\nADM (finetune)\n29.30\n15.08\n12.06\n6.80\n4.15\nADM-IP\n43.15\n15.72\n10.47\n4.58\n4.89\nADM-AT (Ours)\n26.38\n12.98\n9.30\n4.40\n3.07\n(c) ES\nMethods \\ NFEs\n5\n8\n10\n20\n50\nADM (original)\n82.18\n29.28\n17.73\n5.11\n2.70\nADM (finetune)\n63.46\n24.80\n17.03\n5.19\n2.52\nADM-IP\n91.10\n31.44\n18.72\n5.19\n2.89\nADM-AT (Ours)\n41.07\n21.62\n14.68\n4.36\n2.48\n(d) DPM-Solver\nMethods \\ NFEs\n5\n8\n10\n20\n50\nADM (original)\n23.95\n8.00\n5.46\n3.46\n3.14\nADM (finetune)\n22.98\n7.61\n5.29\n3.41\n3.12\nADM-IP\n43.83\n6.70\n6.80\n9.78\n10.91\nADM-AT (Ours)\n18.40\n5.84\n4.81\n3.28\n3.01\nTable 2: Sample quality measured by FID â†“of different sampling methods of DPM under different\nNFEs on ImageNet 64x64. All models are trained with the same iterations (computational costs).\n(a) IDDPM\nMethods \\ NFEs\n5\n8\n10\n20\n50\nADM (original)\n76.92\n33.74\n27.63\n12.85\n5.30\nADM (finetune)\n78.87\n33.99\n27.82\n12.80\n5.26\nADM-IP\n67.12\n29.96\n22.60\n8.66\n3.83\nADM-AT (Ours)\n45.65\n23.79\n19.18\n8.28\n4.01\n(b) DDIM\nMethods \\ NFEs\n5\n8\n10\n20\n50\nADM (original)\n60.07\n20.10\n14.97\n8.41\n5.65\nADM (finetune)\n60.32\n20.26\n15.04\n8.32\n5.48\nADM-IP\n76.51\n26.25\n18.05\n8.40\n6.94\nADM-AT (Ours)\n43.04\n16.08\n12.15\n6.20\n4.67\n(c) ES\nMethods \\ NFEs\n5\n8\n10\n20\n50\nADM (original)\n71.31\n28.97\n21.10\n8.23\n3.76\nADM (finetune)\n72.30\n29.24\n21.58\n8.25\n3.64\nADM-IP\n88.37\n33.91\n23.32\n7.80\n3.54\nADM-AT (Ours)\n43.95\n19.57\n14.12\n6.16\n3.45\n(d) DPM-Solver\nMethods \\ NFEs\n5\n8\n10\n20\n50\nADM (original)\n27.72\n10.06\n7.21\n4.69\n4.24\nADM (finetune)\n27.82\n9.97\n7.22\n4.64\n4.15\nADM-IP\n32.43\n9.94\n8.87\n9.16\n9.68\nADM-AT (Ours)\n17.36\n6.55\n5.78\n4.56\n4.34\nor deterministic samplers (DDIM, DPM-Solver). 2): The ES sampler results show that our AT is\northogonal to the sampling-based method to mitigate the distribution mismatch problem and can be\ncombined to further alleviate the issue. Notably, we further verify in Appendix G.2 that our methods\nwill not slow the convergence unlike AT in classification (Madry et al., 2018). We also perform\nablation analysis of hyperparameters in our AT framework in Appendix G.3.\n6.3\nPERFORMANCE ON LATENT CONSISTENCY MODELS\nSettings.\nWe further evaluate the proposed AT for consistency models on text-to-image generation\ntasks with Latent Consistency Models (Luo et al., 2023) Stable Diffusion (SD) v1.5 (Rombach et al.,\n2022) backbone, which generates 512Ã—512 images. Both our AT and the original LCM training\n(baseline) are trained from scratch with the same hyperparameters (the IP method (Ning et al., 2023)\nis not applied straightforwardly). The training set is LAION-Aesthetics-6.5+ (Schuhmann et al.,\n2022) with hyperparameters following Song et al. (2023); Luo et al. (2023). We select the adversarial\nlearning rate Î± from {0.02, 0.05} and adversarial step K from {2, 3}. The models are trained with a\nbatch size of 64 for 100K iterations. More details are shown in Appendix E.2.\nFollowing Luo et al. (2023) and Chen et al. (2024), we evaluate models on MS-COCO 2014 (Lin et al.,\n2014a) at a resolution of 512Ã—512 by randomly drawing 30K prompts from its validation set. Then,\nwe report the FID between the generated samples under these prompts and the reference samples\nfrom the full validation set following Saharia et al. (2022). We also report CLIP scores (Hessel et al.,\n2021) to evaluate the text-image alignment by CLIP-ViT-B/16.\n9\n\nPublished as a conference paper at ICLR 2025\nTable 3: Results of LCM on MS-COCO 2014 validation set at 512Ã—512 resolution in terms of FID â†“\nand CLIP score â†‘. All models are trained with the same setting (computational costs).\nMethods\nFID â†“\nCLIP Score â†‘\n1 step\n2 step\n4 step\n8 step\n1 step\n2 step\n4 step\n8 step\nLCM\n25.43\n12.61\n11.61\n12.62\n29.25\n30.24\n30.40\n30.47\nLCM-AT (Ours)\n23.34\n11.28\n10.31\n10.68\n29.63\n30.43\n30.49\n30.53\nResults.\nThe methods are evaluated under various sampling steps in Table 3, which shows that the\nLCM with AT consistently improves FID under various sampling steps. Besides, though the AT is\nnot specified to improve text-image alignment, we observe that it has comparable or even better CLIP\nscores across various sampling steps, which shows that AT will not degenerate text-image alignment.\n7\nCONCLUSION\nIn this paper, we novelly introduce efficient Adversarial Training (AT) in the training of DPM and\nCM to mitigate the issue of distribution mismatch between training and sampling. We conduct\nan in-depth analysis of the DPM training objective and systematically characterize the distribution\nmismatch problem. Furthermore, we prove that the training objective of CM similarly faces the\ndistribution mismatch issue. We theoretically prove that DRO can mitigate the mismatch for both\nDPM and CM, which is equivalent to conducting AT. Experiments on image generation and text-to-\nimage generation benchmarks verify the effectiveness of the proposed AT method in alleviating the\ndistribution mismatch of DPM and CM.\nACKNOWLEDGMENTS\nWe thank anonymous reviewers for insightful feedback that helped improve the paper. Zekun\nWang, Ming Liu, Bing Qin are supported by the National Science Foundation of China (U22B2059,\n62276083), the Human-Machine Integrated Consultation System for Cardiovascular Diseases\n(2023A003). They also appreciate the support from China Mobile Group Heilongjiang Co., Ltd.\nREFERENCES\nFan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analytic-dpm: an analytic estimate of the optimal\nreverse variance in diffusion probabilistic models. In International Conference on Learning\nRepresentations, 2022.\nSamy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence\nprediction with recurrent neural networks. In Advances in Neural Information Processing Systems,\n2015.\nAndreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and\nKarsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In\nConference on Computer Vision and Pattern Recognition, 2023.\nHongrui Chen, Holden Lee, and Jianfeng Lu. Improved analysis of score-based generative modeling:\nUser-friendly bounds under minimal smoothness assumptions. In International Conference on\nMachine Learning, 2023.\nJunsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Zhongdao Wang, James T. Kwok,\nPing Luo, Huchuan Lu, and Zhenguo Li. Pixart-Î±: Fast training of diffusion transformer for\nphotorealistic text-to-image synthesis. In International Conference on Learning Representations,\n2024.\nSitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru Zhang. Sampling is as easy as\nlearning the score: theory for diffusion models with minimal data assumptions. In International\nConference on Learning Representations, 2022.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In Conference on Computer Vision and Pattern Recognition, 2009.\n10\n\nPublished as a conference paper at ICLR 2025\nPrafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat GANs on image synthesis. In\nAdvances in Neural Information Processing Systems, volume 34, pp. 8780â€“8794, 2021.\nJohn Duchi. Lecture notes for statistics 311/electrical engineering 377. URL: https://stanford.\nedu/class/stats311/Lectures/full notes. pdf. Last visited on, 2:23, 2016.\nIan J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron C Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural\nInformation Processing Systems, 2014.\nIan J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial\nexamples. In International Conference on Learning Representations, 2015.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In Conference on Computer Vision and Pattern Recognition, 2016.\nJack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-\nfree evaluation metric for image captioning. In Proceedings of the Conference on Empirical\nMethods in Natural Language Processing,, 2021.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium. 2017.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in\nNeural Information Processing Systems, 2020.\nJonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans.\nCascaded diffusion models for high fidelity image generation. Journal of Machine Learning\nResearch, 23(47):1â€“33, 2022a.\nJonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J\nFleet. Video diffusion models. In Advances in Neural Information Processing Systems, 2022b.\nHaoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Tuo Zhao. SMART:\nrobust and efficient fine-tuning for pre-trained natural language models through principled regular-\nized optimization. In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, 2020.\nDiederik P Kingma and Max Welling. Auto-encoding variational {Bayes}. In International Confer-\nence on Learning Representations, 2013.\nAlex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.\nJaeho Lee and Maxim Raginsky. Minimax statistical learning with wasserstein distances. In Advances\nin Neural Information Processing Systems, 2018.\nGen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi. Towards faster non-asymptotic convergence for\ndiffusion-based generative models. Preprint arXiv:2306.09251, 2023.\nMingxiao Li, Tingyu Qu, Wei Sun, and Marie-Francine Moens. Alleviating exposure bias in diffusion\nmodels through sampling with shifted time steps. In International Conference on Learning\nRepresentations, 2024.\nYangming Li and Mihaela van der Schaar. On error propagation of diffusion models. In The Twelfth\nInternational Conference on Learning Representations, 2024.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDollÂ´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Visionâ€“\nECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, 2014a.\nTsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDollÂ´ar, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. In ECCV, 2014b.\n11\n\nPublished as a conference paper at ICLR 2025\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-\nence on Learning Representations, 2019.\nAaron Lou and Stefano Ermon. Reflected diffusion models. In International Conference on Machine\nLearning, 2023.\nCheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast\node solver for diffusion probabilistic model sampling in around 10 steps. In Advances in Neural\nInformation Processing Systems, 2022a.\nCheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan LI, and Jun Zhu. Dpm-solver: A fast\node solver for diffusion probabilistic model sampling in around 10 steps. In Advances in Neural\nInformation Processing Systems, 2022b.\nSimian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models:\nSynthesizing high-resolution images with few-step inference, 2023.\nJiajun Ma, Shuchen Xue, Tianyang Hu, Wenjia Wang, Zhaoqiang Liu, Zhenguo Li, Zhi-Ming Ma,\nand Kenji Kawaguchi. The surprising effectiveness of skip-tuning in diffusion sampling. arXiv\npreprint arXiv:2402.15170, 2024.\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.\nTowards deep learning models resistant to adversarial attacks. In International Conference on\nLearning Representations, 2018.\nHongseok Namkoong. Reliable machine learning via distributional robustness. PhD thesis, Stanford\nUniversity, 2019.\nAlexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob\nMcGrew, Ilya Sutskever, and Mark Chen. GLIDE: towards photorealistic image generation and\nediting with text-guided diffusion models. In International Conference on Machine Learning.\nWeili Nie, Brandon Guo, Yujia Huang, Chaowei Xiao, Arash Vahdat, and Animashree Anandkumar.\nDiffusion models for adversarial purification. In International Conference on Machine Learning,\n2022.\nMang Ning, Enver Sangineto, Angelo Porrello, Simone Calderara, and Rita Cucchiara. Input\nperturbation reduces exposure bias in diffusion models. In International Conference on Machine\nLearning, 2023.\nMang Ning, Mingxiao Li, Jianlin Su, Albert Ali Salah, and Itir Â¨Onal Ertugrul. Elucidating the\nexposure bias in diffusion models. In International Conference on Learning Representations, 2024.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\nconditional image generation with clip latents, 2022.\nMarcâ€™Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training\nwith recurrent neural networks. In International Conference on Learning Representations, 2016.\nSuman V. Ravuri and Oriol Vinyals. Classification accuracy score for conditional generative models.\nIn Advances in Neural Information Processing Systems, 2019.\nSylvestre-Alvise Rebuffi, Sven Gowal, Dan A. Calian, Florian Stimberg, Olivia Wiles, and Timothy A.\nMann. Fixing data augmentation to improve adversarial robustness. Preprint arXiv:2103.01946,\n2021.\nZhiyao Ren, Yibing Zhan, Liang Ding, Gaoang Wang, Chaoyue Wang, Zhongyi Fan, and Dacheng\nTao. Multi-step denoising scheduled sampling: Towards alleviating exposure bias for diffusion\nmodels. In AAAI Conference on Artificial Intelligence, 2024.\n12\n\nPublished as a conference paper at ICLR 2025\nSteven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. Self-critical\nsequence training for image captioning. In IEEE Conference on Computer Vision and Pattern\nRecognition, 2017.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÂ¨orn Ommer. High-\nresolution image synthesis with latent diffusion models. In Conference on Computer Vision and\nPattern Recognition, 2022.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical\nimage segmentation. In Medical Image Computing and Computer-Assisted Interventionâ€“MICCAI\n2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III\n18, pp. 234â€“241. Springer, 2015.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J\nFleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language\nunderstanding. In Advances in Neural Information Processing Systems, 2022.\nTim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In\nInternational Conference on Learning Representations, 2022.\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi\nCherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski,\nSrivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev.\nLAION-5B: an open large-scale dataset for training next generation image-text models. In Advances\nin Neural Information Processing Systems, 2022.\nAli Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John P. Dickerson, Christoph Studer, Larry S.\nDavis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! In Advances in Neural\nInformation Processing Systems 32: Annual Conference on Neural Information Processing Systems\n2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, 2019.\nAlexander Shapiro. Distributionally robust stochastic programming. SIAM Journal on Optimization,\n27(4):2258â€“2275, 2017.\nShiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. Minimum\nrisk training for neural machine translation. In Proceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics, 2016.\nAlbert N Shiryaev. Probability-1, volume 95. Springer, 2016.\nAman Sinha, Hongseok Namkoong, and John Duchi. Certifying some distributional robustness with\nprincipled adversarial training. In International Conference on Learning Representations, 2018.\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. In International Conference on Machine Learning,\n2015.\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In Interna-\ntional Conference on Learning Representations, 2022.\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. Score-based generative modeling through stochastic differential equations. In International\nConference on Learning Representations, 2020.\nYang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of\nscore-based diffusion models. 2021.\nYang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In International\nConference on Machine Learning, 2023.\nCÂ´edric Villani et al. Optimal transport: old and new, volume 338. Springer, 2009.\n13\n\nPublished as a conference paper at ICLR 2025\nMartin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cam-\nbridge university press, 2019.\nRuoyu Wang, Mingyang Yi, Zhitang Chen, and Shengyu Zhu. Out-of-distribution generalization\nwith causal invariant transformations. In Conference on Computer Vision and Pattern Recognition,\n2022.\nZekai Wang, Tianyu Pang, Chao Du, Min Lin, Weiwei Liu, and Shuicheng Yan. Better diffusion\nmodels further improve adversarial training. In International Conference on Machine Learning,\n2023.\nDongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust general-\nization. In Advances in Neural Information Processing Systems, 2020.\nShuchen Xue, Zhaoqiang Liu, Fei Chen, Shifeng Zhang, Tianyang Hu, Enze Xie, and Zhenguo Li.\nAccelerating diffusion sampling with optimized time steps. arXiv preprint arXiv:2402.17376,\n2024a.\nShuchen Xue, Mingyang Yi, Weijian Luo, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhi-Ming\nMa. Sa-solver: Stochastic adams solver for fast sampling of diffusion models. Advances in Neural\nInformation Processing Systems, 36, 2024b.\nMingyang Yi, Lu Hou, Jiacheng Sun, Lifeng Shang, Xin Jiang, Qun Liu, and Zhiming Ma. Improved\nood generalization via adversarial training and pretraing. In International Conference on Machine\nLearning, 2021.\nMingyang Yi, Jiacheng Sun, and Zhenguo Li. On the generalization of diffusion model. Preprint\narXiv:2305.14712, 2023a.\nMingyang Yi, Ruoyu Wang, Jiacheng Sun, Zhenguo Li, and Zhi-Ming Ma. Breaking correlation shift\nvia conditional invariant regularizer. In The International Conference on Learning Representations,\n2023b.\nMingyang Yi, Aoxue Li, Yi Xin, and Zhenguo Li. Towards understanding the working mechanism of\ntext-to-image diffusion model. Preprint arXiv:2405.15330, 2024.\nTianwei Yin, MichaÂ¨el Gharbi, Richard Zhang, Eli Shechtman, FrÂ´edo Durand, William T Freeman,\nand Taesung Park. One-step diffusion with distribution matching distillation. In CVPR, 2024.\nBoya Zhang, Weijian Luo, and Zhihua Zhang. Enhancing adversarial robustness via score-based\noptimization. In Advances in Neural Information Processing Systems, 2023.\nDinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, and Bin Dong. You only propagate\nonce: Accelerating adversarial training via maximal principle. In Advances in Neural Information\nProcessing Systems, 2019a.\nHongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and Michael I. Jordan.\nTheoretically principled trade-off between robustness and accuracy. In International Conference\non Machine Learning, 2019b.\nWen Zhang, Yang Feng, Fandong Meng, Di You, and Qun Liu. Bridging the gap between training and\ninference for neural machine translation. In Proceedings of the 57th Conference of the Association\nfor Computational Linguistics, 2019c.\nChen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, and Jingjing Liu. Freelb: Enhanced\nadversarial training for natural language understanding. In International Conference on Learning\nRepresentations, 2020.\n14\n\nPublished as a conference paper at ICLR 2025\nA\nPROOFS IN SECTION 4\nIn this section, we present the proofs of the results in Section 4.\nA.1\nPROOFS IN SECTION 4.2\nProposition 1. The minimization problem (4) is equivalent to minimizing an upper bound of\nEq[âˆ’log pÎ¸(xt)] for any 0 â‰¤t â‰¤T.\nProof. We prove the first equivalence, by Jensenâ€™s inequality. For any 0 â‰¤t < T, we have\nâˆ’Eq [log pÎ¸(xt)]\nâ‰¤Eq\n\u0014\nâˆ’log\npÎ¸(xt:T )\nq(xt+1:T | xt)\n\u0015\n=Eq\nï£®\nï£°âˆ’log pÎ¸(xT ) âˆ’\nX\ntâ‰¤s<T\nlog pÎ¸(xs | xs+1)\nq(xs+1 | xs)\nï£¹\nï£»\n=Eq\nï£®\nï£°âˆ’log pÎ¸(xT ) âˆ’\nX\ntâ‰¤s<T\nlog pÎ¸(xs | xs+1)\nq(xs | xs+1) Â·\nq(xs)\nq(xs+1)\nï£¹\nï£»\n=Eq\nï£®\nï£°âˆ’log pÎ¸(xT )\nq(xT ) âˆ’\nX\ntâ‰¤s<T\nlog pÎ¸(xs | xs+1)\nq(xs | xs+1) âˆ’log q(xt)\nï£¹\nï£»\n=DKL(q(xT ) âˆ¥pÎ¸(xT )) + Eq\nï£®\nï£°\nT âˆ’1\nX\ns=t\nDKL(q(xs | xs+1) âˆ¥pÎ¸(xs | xs+1))\n|\n{z\n}\nLt\nï£¹\nï£»+ H(xt)\n(19)\nTaking t = 0, we prove the first equivalence. Besides that, the entropy H(xt) of xt is a constant for\nÎ¸ given data distribution x0 for any 0 â‰¤t < T. The second conclusion holds due to the non-negative\nproperty of KL-divergence.\nProposition 2. Suppose pÎ¸(xt | xt+1) matches q(xt | xt+1) well such that\nLt = DKL(q(xt | xt+1) âˆ¥pÎ¸(xt | xt+1)) â‰¤Î³\nT ,\n(8)\nand the discrepancy satisfies DKL(q(xT ) âˆ¥pÎ¸(xT )) â‰¤Î³0, then for any 0 â‰¤t â‰¤T, we have\nDKL(q(xt) âˆ¥pÎ¸(xt)) â‰¤DKL(q(xT ) âˆ¥pÎ¸(xT )) + Lt â‰¤Î³0 + (T âˆ’t)Î³\nT\n.\n(9)\nProof. We have the following decomposition due to the chain rule of KL-divergence\nDKL(q(xt, xt+1) âˆ¥pÎ¸(xt, xt+1)) = DKL(q(xt | xt+1) âˆ¥pÎ¸(xt | xt+1)) + DKL(q(xt+1) âˆ¥pÎ¸(xt+1))\n= DKL(q(xt+1 | xt) âˆ¥pÎ¸(xt+1 | xt)) + DKL(q(xt) âˆ¥pÎ¸(xt)),\n(20)\nThe transition probability pÎ¸(xt | xt+1) matches q(xt | xt+1), so that the above equality implies\nDKL(q(xt) âˆ¥pÎ¸(xt))\n=DKL(q(xt+1) âˆ¥pÎ¸(xt+1)) + DKL(q(xt | xt+1) âˆ¥pÎ¸(xt | xt+1)) âˆ’DKL(q(xt+1 | xt) âˆ¥pÎ¸(xt+1 | xt))\nâ‰¤DKL(q(xt+1) âˆ¥pÎ¸(xt+1)) + Î³\nT .\n(21)\nThe proposition holds due to initial condition DKL(q(xT ) âˆ¥pÎ¸(xT )) â‰¤Î³0 and simple induction.\nProposition 3. Lt in (4) is well minimized, only if q(xt+1) is Gaussian or âˆ¥xt+1 âˆ’xtâˆ¥â†’0.\n15\n\nPublished as a conference paper at ICLR 2025\nProof. Due to Bayesâ€™ rule, we have\nq(xt | xt+1) = q(xt+1 | xt)q(xt)\nq(xt+1)\nâˆexp\n \nâˆ’\n\r\rxt+1 âˆ’âˆšÎ±t+1xt\n\r\r2\n2(1 âˆ’Î±t+1)\n+ log q(xt) âˆ’log q(xt+1)\n!\nâˆexp\n \nâˆ’\n\r\rxt+1 âˆ’âˆšÎ±t+1xt\n\r\r2\n2(1 âˆ’Î±t+1)\n+ âŸ¨âˆ‡x log q(xt+1), xt âˆ’xt+1âŸ©\n!\nÂ·\nexp\n\u00121\n2(xt âˆ’xt+1)âŠ¤âˆ‡2\nx log q(xt+1)(xt âˆ’xt+1) + O(âˆ¥xt+1 âˆ’xtâˆ¥3)\n\u0013\n.\n(22)\nAs can be seen, the conditional probability can be approximated by Gaussian only if âˆ‡3\nx log q(xt+1)\nis zero or âˆ¥xt+1 âˆ’xtâˆ¥3 is extremely small with high probability. The two conditions can be\nrespectively satisfied when q(xt) is a Gaussian or xt close to xt+1.\nA.2\nPROOFS IN SECTION 4.2\nProposition 4. For any distribution Ëœq satisfies Ëœq(xt) = q(xt) for specific t, we have\nEq [âˆ’log pÎ¸(xt)] â‰¤DKL(Ëœq(xt | xt+1) âˆ¥pÎ¸(xt | xt+1))\n|\n{z\n}\nLËœq\nt\n+C,\n(11)\nfor a constant C independent of Î¸.\nProof. W.o.l.g., suppose pÎ¸(xt, xt+1) = pÎ¸(xt | xt+1)q(xt+1) and Ëœq(xt, xt+1) = Ëœq(xt+1 |\nxt)q(xt). By Jensenâ€™s inequality, we have\nEq [âˆ’log pÎ¸(xt)]\n= âˆ’\nZ\nq(xt)\n\u0012\nlog\nZ\npÎ¸(xt, xt+1)dxt+1\n\u0013\ndxt\n= âˆ’\nZ\nq(xt)\n\u0012\nlog\nZ pÎ¸(xt, xt+1)\nËœq(xt+1 | xt) Ëœq(xt+1 | xt)dxt+1\n\u0013\ndxt\nâ‰¤âˆ’\nZ\nq(xt)\n\u0012Z\nlog pÎ¸(xt, xt+1)\nËœq(xt+1 | xt) Ëœq(xt+1 | xt)dxt+1\n\u0013\ndxt\n= âˆ’\nZ\nq(xt)\n\u0012Z\nËœq(xt+1 | xt) log pÎ¸(xt | xt+1)\nËœq(xt+1 | xt) dxt+1\n\u0013\ndxt\nâˆ’\nZ\nq(xt)\n\u0012Z\nËœq(xt+1 | xt) log\nq(xt+1)\nËœq(xt+1 | xt)dxt+1\n\u0013\ndxt\n= âˆ’\nZ\nËœq(xt, xt+1) log pÎ¸(xt | xt+1)\nËœq(xt+1 | xt) dxtdxt+1 + C1\n= âˆ’\nZ\nËœq(xt, xt+1) log pÎ¸(xt | xt+1)\nËœq(xt | xt+1) Â·\nq(xt)\nËœq(xt+1)dxtdxt+1 + C1\n= âˆ’\nZ\nËœq(xt, xt+1) log pÎ¸(xt | xt+1)\nËœq(xt | xt+1) dxtdxt+1 + C1 + C2\n=DKL(Ëœq(xt | xt+1) âˆ¥pÎ¸(xt | xt+1)) + C\n=LËœq\nvlb(Î¸, t) + C,\n(23)\nwhere C, C1, C2 are all constants independent of Î¸.\nA.2.1\nPROOF OF THEOREM 1\nIn this section, we prove the Theorem 1.\nTo simplify the notation, let pÎ¸(xt | xt+1) âˆ¼\nN(ÂµÎ¸(xt+1, t + 1), Ïƒt+1 6 in (6), then the optimal solution (Lemma 9 in (Bao et al., 2022)) of\nminimizing LËœqt\nt+1 is\nÂµÎ¸(xt+1, t + 1) = EËœqt[xt | xt+1].\n(24)\n6Here Ïƒt+1 can be also optimized as in (Bao et al., 2022), but we find optimizing it in practice does not\nimprove the empirical results.\n16\n\nPublished as a conference paper at ICLR 2025\nFor every specific t, we consider the following Ëœqt in (12) 7, such that\nËœqt(xt+1 | xt) Ì¸= q(xt+1 | xt);\nËœqt(xt+1) Ì¸= q(xt+1);\nËœqt(x0:t) = q(x0:t).\nËœqt(xt | x0, xt+1) = q(xt | x0, xt+1) = N(Âµt+1(x0, xt+1), Ïƒt).\n(25)\nwhere Âµt+1(x0, xt+1) =\nâˆšÂ¯Î±t(1âˆ’Î±t+1)\n1âˆ’Â¯Î±t+1\nx0 +\nâˆšÎ±t+1(1âˆ’Â¯Î±t)\n1âˆ’Â¯Î±t+1\nxt+1. The Ëœqt can be taken due to the\nBayesian rule. Next, we analyze the optimal formulation in (24). Due to the property of conditional\nexpectation, we have\nÂµÎ¸(xt+1, t + 1) = EËœqt [EËœqt [xt | x0, xt+1] | xt+1] = Âµt+1 (EËœqt[x0 | xt+1], xt+1) .\n(26)\nAs can be seen, the optimal transition rule is decided by the conditional expectation EËœqt[x0 | xt+1]\nfor some Ëœqt(xt+1) âˆˆBDKL(Ëœq(xt+1), Î·0) in (12). Then, we have the following lemma to get the\ndesired conditional expectation.\nLemma 1. There exists some Î· â‰¥Î·0 in (27) which makes (27) equivalent to problem (12).\nmin\nÎ¸\nT âˆ’1\nX\nt=0\nEËœqt(x0)\nsup\nËœqt(xt+1|x0)âˆˆBDKL (qt(xt+1|x0),Î·)\nEËœqt(xt+1|x0)\n\u0002\nâˆ¥xÎ¸(xt+1, t + 1) âˆ’x0âˆ¥2\u0003\n,\n(27)\nwhere EpÎ¸[x0 | xt+1] = xÎ¸(xt+1, t + 1).\nProof. Let us check the training objective minÎ¸ supËœqtâˆˆBDKL(qt+1,Î·) DKL(Ëœqt(xt | xt+1) âˆ¥pÎ¸(xt |\nxt+1)). During this proof, we abbreviate BDKL(qt+1(xt+1), Î·) as B. Since pÎ¸(xt | xt+1) âˆ¼\nN(ÂµÎ¸(xt+1, t + 1), Ïƒt+1), then\nsup\nËœqt(xt+1)âˆˆB\nDKL(Ëœqt(xt | xt+1) âˆ¥pÎ¸(xt | xt+1))\nâˆâˆ’d\n2 log 2Ï€Ïƒ2\nt+1 âˆ’\n1\n2Ïƒ2\nt+1\nsup\nËœqt(xt+1)âˆˆB\nEËœq(xt,xt+1)\n\u0002\nâˆ¥xt âˆ’ÂµÎ¸(xt+1, t + 1)âˆ¥2\u0003\n.\n(28)\nAs we consider Ïƒt+1 as constant, an analysis of the expectation term is enough. Due to\nEËœqt(xt,xt+1)\n\u0002\nâˆ¥xt âˆ’ÂµÎ¸(xt+1, t + 1)âˆ¥2\u0003\nâ‰¥inf\nf EËœqt(x0,xt,xt+1)\n\u0002\nâˆ¥xt âˆ’f(x0, xt+1)âˆ¥2\u0003\n= EËœqt(x0,xt,xt+1)\n\u0002\nâˆ¥xt âˆ’EËœq[xt | x0, xt+1]âˆ¥2\u0003\n,\n(29)\nwhere the last term is invariant over Ëœqt âˆˆB so that it is a uniform lower bound over all possible Ëœqt\nand pÎ¸(xt | xt+1). The above inequality indicates that the optimal ÂµÎ¸(xt+1, t + 1) is achieved when\nthe left in (29) becomes the right in (29).\nOn the other hand, for any Ëœqt âˆˆB, let us compute the gap such that\nEËœqt(xt,xt+1)\n\u0002\nâˆ¥xt âˆ’ÂµÎ¸(xt+1, t + 1)âˆ¥2\u0003\n= EËœqt\n\u0002\nâˆ¥xt âˆ’EËœqt[xt | x0, xt+1] + EËœqt[xt | x0, xt+1] âˆ’ÂµÎ¸(xt+1, t + 1)âˆ¥2\u0003\n= EËœqt\n\u0002\nâˆ¥xt âˆ’EËœqt[xt | x0, xt+1]âˆ¥2\u0003\n+ EËœqt\n\u0002\nâˆ¥ÂµÎ¸(xt+1, t + 1) âˆ’EËœqt[xt | x0, xt+1]âˆ¥2\u0003\nâˆ’2EËœqt [âŸ¨xt âˆ’EËœqt[xt | x0, xt+1], ÂµÎ¸(xt+1, t + 1) âˆ’EËœqt[xt | x0, xt+1]âŸ©]\n= EËœqt\n\u0002\nâˆ¥xt âˆ’EËœqt[xt | x0, xt+1]âˆ¥2\u0003\n+\n\u0012âˆšÂ¯Î±t âˆ’\nq\n1 âˆ’Â¯Î±t âˆ’Ïƒ2\nt+1\nr\nÂ¯Î±t+1\n1 âˆ’Â¯Î±t+1\n\u0013\nEËœqt(x0,xt+1)\n\u0002\nâˆ¥x0 âˆ’xÎ¸(xt+1, t + 1)âˆ¥2\u0003\n,\n(30)\nwhere the equality is due to the property of conditional expectation leads to EËœqt[âŸ¨xt âˆ’EËœqt[xt |\nx0, xt+1], ÂµÎ¸(xt+1, t+1)âˆ’EËœqt[xt | x0, xt+1]âŸ©] = 0, and rewriting EËœqt[âˆ¥ÂµÎ¸(xt+1, t+1)âˆ’EËœqt[xt |\nx0, xt+1]âˆ¥2] as in equations (5)-(10) in (Ho et al., 2020). Due to this, we know that minimizing the\n7We can do this since (12) only relates to Ëœqt(xt+1)\n17\n\nPublished as a conference paper at ICLR 2025\nsquare error is equivalent to minimizing the EËœqt(xt,xt+1)[âˆ¥x0 âˆ’xÎ¸(xt+1, t + 1)âˆ¥2]. On the other\nhand, since Ëœqâˆ—\nt âˆˆB, then we have\nDKL(q(xt+1 | x0) âˆ¥Ëœqâˆ—\nt (xt+1 | x0))\n=DKL(q(x0 | xt+1) âˆ¥Ëœqâˆ—\nt (x0 | xt+1)) + DKL(q(xt+1) âˆ¥Ëœqâˆ—\nt (xt+1))\nâ‰¥Î·0.\n(31)\nThus, we prove our conclusion.\nTheorem 1. There exists Î´t depends on x0 and Ïµt makes (13) equivalent to problem (12).\nmin\nÎ¸\nT âˆ’1\nX\nt=0\nEq(x0),Ïµt\n\"\r\r\r\rÏµÎ¸(âˆšÂ¯Î±tx0 +\nâˆš\n1 âˆ’Â¯Î±tÏµt + Î´t, t) âˆ’Ïµt âˆ’\nÎ´t\nâˆš1 âˆ’Â¯Î±t\n\r\r\r\r\n2#\n.\n(13)\nProof. By combining Lemma 1, suppose the supreme is attained under Ëœqtâˆ’1 such that xt âˆ¼Ëœqtâˆ’1(xt)\nwith\nxt = âˆšÂ¯Î±tx0 +\nâˆš\n1 âˆ’Â¯Î±tÏµt + Î´t,\n(32)\nwith Î´t depends on x0 and xt. Then we prove the conclusion.\nA.2.2\nPROOF OF PROPOSITION 5\nProposition 5. If LDRO\nt\n(Î¸) â‰¤Î·0 in (12) for all t, and DKL(q(xT ) âˆ¥pÎ¸(xT )) â‰¤Î·0, then\nDKL(q(x0) âˆ¥pÎ¸(x0)) â‰¤Î·0.\nProof. This theorem can proved by induction. Since DKL(q(xT ) âˆ¥pÎ¸(xT )) â‰¤Î·0, then, let\nËœqâˆ—\nT âˆ’1(xT ) = pÎ¸(xT ) and satisfies Ëœqâˆ—\nT âˆ’1(xT ) = q(xT âˆ’1). The existence of such distribution is due\nto Kolmogorov existence theorem (Shiryaev, 2016). Then, we have\nDKL(Ëœqâˆ—\nT âˆ’1(xT âˆ’1) âˆ¥pÎ¸(xT âˆ’1)) â‰¤DKL(Ëœqâˆ—\nT âˆ’1(xT ) âˆ¥pÎ¸(xT ))\n+ DKL(Ëœqâˆ—\nT âˆ’1(xT âˆ’1 | xT ) âˆ¥pÎ¸(xT âˆ’1 | xT ))\nâ‰¤LDRO\nt\n(Î¸)\nâ‰¤Î·0,\n(33)\nwhere the first inequality is due to the definition of LDRO\nt\n(Î¸) and Ëœqâˆ—\nT âˆ’1(xT ) = pÎ¸(xT ). Then, we\nprove our conclusion by induction over t.\nA.2.3\nPROOF OF PROPOSITION 6\nProposition 6. For Î· > 0 and Î´t in (13), âˆ¥Î´tâˆ¥1 â‰¤Î· holds with probability at least 1âˆ’\np\n2(1 âˆ’Â¯Î±t)/Î·.\nProof. Due to the definition of the first order Wasserstein distance W1(Â·, Â·) (Villani et al., 2009) for\nany specific x0, suppose\nÏ€âˆ—âˆˆ\narg min\nÏ€(xt,Ëœxt)âˆˆqt(xt|x0)Ã—Ëœqt(Ëœxt|x0)\nE [âˆ¥Ëœxt âˆ’xtâˆ¥1] ,\n(34)\nso that\nEÏ€âˆ—[âˆ¥Ëœxt âˆ’xtâˆ¥1] = W1(qt(xt | x0), Ëœqt(xt | x0)).\n(35)\nLet Î´t be the one of (13) under Ï€âˆ—derived by Lemma 1, then\nP (âˆ¥Î´tâˆ¥1 â‰¥Î· | x0) â‰¤EÏ€âˆ—[âˆ¥Î´tâˆ¥1]\nÎ·\n= W1(qt(xt | x0), Ëœqt(xt | x0))\nÎ·\nâ‰¤a\np\n2(1 âˆ’Â¯Î±t)DKL(qt(xt | x0) âˆ¥Ëœqt(xt | x0))\nÎ·\nâ‰¤\ns\n2(1 âˆ’Â¯Î±t)\nÎ·\n,\n(36)\nwhere inequality a is due to the Talagrandâ€™s inequality (Wainwright, 2019). Then we prove our\nconclusion.\n18\n\nPublished as a conference paper at ICLR 2025\nB\nPROOFS IN SECTION 5\nNext, we give the proof of results in Section 5. Firstly, let us check the definition of the Î¦t(xt+1).\nFor the variance-preserving stochastic differential equation in Song et al. (2022)\ndzs = âˆ’Î²s\n2 zsdt +\np\nÎ²sdWs.\n(37)\nDue to the solution of zs in Song et al. (2023), we know zst has the same distribution with xt in (1)\nfor {st}T\nt=1 satisfies\nexp\n\u0012\nâˆ’\nZ st\n0\nÎ²(u)du\n\u0013\n= Â¯Î±t\n(s0 = 0).\n(38)\nIn the rest of this section, we use d(x, y) in (15) as â„“2 distance âˆ¥x âˆ’yâˆ¥2, whereas the conclusions\nunder other distance can be similarly derived. Owing the the discussion in above, similar to (Song\net al., 2023), when xt+1 = zst+1, let Î¦t(xt+1) = Î¨st(zst+1), we can rewrite the objective (15) as\nfollows.\nmin\nÎ¸\nLCD(Î¸) = min\nÎ¸\nT âˆ’1\nX\nt=0\nEzst\nh\r\rfÎ¸(Î¨st(zst+1), t) âˆ’fÎ¸(zst+1, t + 1)\n\r\r2i\n.\n(39)\nHere zs follows the following reverse time ODE of (37) with z0 âˆ¼q(x0),\ndzs = âˆ’Î²s\n2\n\u0012\nzs + 1\n2âˆ‡z log qs(zs)\n\u0013\n|\n{z\n}\nÏ•s\nds,\n(40)\nand such zs has the same distribution with the ones in (37) (Song et al., 2022), where qs is the\ndensity of zs. Î¨st(zst+1) = zst+1 âˆ’\nR st+1\nst\nÏ•s(zs)ds, which is a deterministic function of zst+1,\nand fÎ¸(zs0, 0) = zs0 = z0.\nNow, we are ready to prove the Theorem 2 as follows.\nTheorem 2. For LCD(Î¸) in (15) with d(Â·, Â·) is â„“2 distance, then W1(fÎ¸(xt, t), x0) â‰¤\np\ntLCD(Î¸) 8.\nProof. Owing to the definition of W1-distance, and the discussion in above, we have\nW1(fÎ¸(xT , T), x0) = W1(fÎ¸(zsT , T), zs0)\n= W1\n\u0000fÎ¸(zsT , T), Î¨s0\n\u0000Î¨s1\n\u0000Â· Â· Â· Î¨sT âˆ’1 (zsT )\n\u0001\u0001\u0001\nâ‰¤E\n\u0002\nâˆ¥fÎ¸(zsT , T) âˆ’Î¨s0\n\u0000Î¨s1\n\u0000Â· Â· Â· Î¨sT âˆ’1 (zsT )\n\u0001\u0001\nâˆ¥\n\u0003\nâ‰¤\nT âˆ’1\nX\nt=0\nE\n\u0002\r\rfÎ¸(zst+1, t + 1) âˆ’fÎ¸(Î¨st(zst+1), t)\n\r\r\u0003\nâ‰¤\np\nTLCD(Î¸),\n(41)\nwhere the first inequality is due to the definition of Wasserstein distance, the second and last\ninequalities respectively use the triangle inequality and Schwarzâ€™s inequality.\nB.1\nPROOF OF THEOREM 3\nAs pointed out in the above, the used Ë†Î¦t(xt+1, ÏµÏ•) is a numerical estimator of Î¦t(xt+1). In the\nsequel, let us consider Ë†Î¦ is an Euler estimator as follows, whereas our analysis can be similarly\ngeneralized to the other estimators.\nË†Î¦t(xt+1, ÏµÏ•) = Ë†Î¨st(zst+1, ÏµÏ•) = zst+1 + (st+1 âˆ’st) Î²st+1\n2\n\u0010\nzst+1 + ÏµÏ•(zst+1, t + 1)/\np\n1 âˆ’Â¯Î±t+1\n\u0011\n|\n{z\n}\nË†Ï•st+1\n,\n(42)\n8Here W1(fÎ¸(xt, t), x0) is the Wasserstein 1-distance between distributions of fÎ¸(xt, t) and x0.\n19\n\nPublished as a conference paper at ICLR 2025\nwhere âˆš1 âˆ’Â¯Î±t+1ÏµÏ•(zst+1, t+1) estimates âˆ‡z log qst+1(zst+1) as pointed out in (Song et al., 2020),\nand the condition xt+1 = zst+1 is hold.\nNext, we illustrate the used regularity conditions to derive Theorem 3.\nAssumption 1. The discretion error of Ë†Î¨st(zst+1, ÏµÏ•) is smaller than C(st+1 âˆ’st)2 for constant C,\nthat says\n\r\r\r\rË†Î¨st(zst+1, ÏµÏ•) âˆ’zst+1 âˆ’\nZ st+1\nst\nË†Ï•s(zs)ds\n\r\r\r\r â‰¤C(st+1 âˆ’st)2\n(43)\nAssumption 2. The estimated score âˆ‡z log Ë†qs(z) has bounded expected error, i.e.,\nEzâˆ¼qst (z)\n\u0014\r\r\rË†Ï•st(z) âˆ’Ï•st(z)\n\r\r\r\n2\u0015\nâ‰¤Ïµ.\n(44)\nfor all 0 â‰¤t < T.\nAssumption 3. For the learned model fÎ¸, it holds âˆ¥fÎ¸âˆ¥â‰¤D.\nThe Assumption 1 describes the discretion error of the Euler method under ODE with drift term\nË†Ï•s, which can be satisfied under proper continuity conditions of model ÏµÏ•. On the other hand,\nAssumption 2 describes the estimation error of Ë†Ï•st(z), which terms out to be the training objective of\nobtaining it, see (Song et al., 2020) for more details. The Assumption 3 is natural, since fÎ¸ predicts\nx0, which is usually an image data with bounded norm. Now, we are ready to prove the Theorem 3,\nwhich is presented by proving the following formal version.\nTheorem 4. Under Assumptions 1, 2, and 3, for all Î´st, we have Ezst[âˆ¥Î´st(zst)âˆ¥] â‰¤O(âˆ†2\nst +\nÏµ\np\nâˆ†st) for âˆ†st = st+1 âˆ’st. Besides that, we have\nW1(fÎ¸(zT , T), z0) â‰¤\ns\nT Ë†LAdv\nCD (Î¸) + 4D2\nÎ·\nh\nCâˆ†2st + ÏµO(\np\nâˆ†st)\ni\n.\n(45)\nProof. Noting that Î¦t(xt+1) = Î¨st(zst+1) and Ë†Î¦t(xt+1, ÏµÏ•) = Ë†Î¨st(zst+1, ÏµÏ•), the key problem is\nto upper bound the difference between Ë†Î¨st(z, ÏµÏ•) and Î¨st(z) for all t and z. To do so, we note that\n\r\r\rË†Î¨st(z, ÏµÏ•) âˆ’Î¨st(z)\n\r\r\r â‰¤\n\r\r\r\rË†Î¨st(z, ÏµÏ•) âˆ’z âˆ’\nZ st+1\nst\nË†Ï•s(zs)ds\n\r\r\r\r +\n\r\r\r\rz âˆ’\nZ st+1\nst\nË†Ï•s(zs)ds âˆ’Î¨st(z)\n\r\r\r\r ,\n(46)\nwhere the first one in r.h.s can be upper bounded by C(st+1 âˆ’st)2 according to Assumption 1. On\nthe other hand, define dË†zs\nds = Ë†Ï•s(Ë†zs), then when Ë†zst+1 = zst+1 = z and s âˆˆ[st, st+1].\nd\ndsâˆ¥Ë†zs âˆ’zsâˆ¥2 =\nD\nË†zs âˆ’zs, Ë†Ï•s(Ë†zs) âˆ’Ï•s(zs)\nE\n=\nD\nË†zs âˆ’zs, Ë†Ï•s(Ë†zs) âˆ’Ë†Ï•s(zs) + Ë†Ï•s(zs) âˆ’Ï•s(zs)\nE\nâ‰¤Lâˆ¥Ë†zs âˆ’zsâˆ¥2 +\nD\nË†zs âˆ’zs, Ë†Ï•s(zs) âˆ’Ï•s(zs)\nE\nâ‰¤\n\u00121\n2 + L\n\u0013\nâˆ¥Ë†zs âˆ’zsâˆ¥2 + 1\n2\n\r\r\rË†Ï•s(zs) âˆ’Ï•s(zs)\n\r\r\r\n2\n.\n(47)\nTaking expectation over z, by Gronwallâ€™s inequality, Assumption 2 and Ë†zst+1 = zst+1, we have\nE\n\u0002\nâˆ¥Ë†zst âˆ’zstâˆ¥2\u0003\nâ‰¤\nZ st+1\nst\ne(1/2+L)(sâˆ’st)\n2\nE\nh\nâˆ¥Ë†Ï•s(zs) âˆ’Ï•s(zs)âˆ¥2i\nds â‰¤Ïµ\n4\nZ st+1\nst\nÎ²se(1/2+L)(sâˆ’st)ds.\n(48)\nPlugging this into (46), we know\nE\nh\r\r\rË†Î¨st(zst, ÏµÏ•) âˆ’Î¨st(zst)\n\r\r\r\ni\nâ‰¤C(st+1 âˆ’st)2 + ÏµO(âˆšst+1 âˆ’st).\n(49)\nBy Markovâ€™s inequality, we have\nP\n\u0010\r\r\rË†Î¨st(zst, ÏµÏ•) âˆ’Î¨st(zst)\n\r\r\r â‰¥Î·\n\u0011\nâ‰¤\nE\nh\r\r\rË†Î¨st(zst, ÏµÏ•) âˆ’Î¨st(zst)\n\r\r\r\ni\nÎ·\nâ‰¤1\nÎ·\n\u0002\nC(st+1 âˆ’st)2 + ÏµO(âˆšst+1 âˆ’st)\n\u0003\n.\n(50)\n20\n\nPublished as a conference paper at ICLR 2025\nThus,\nE\n\u0002\nâˆ¥fÎ¸(xt+1, t + 1) âˆ’fÎ¸(Î¦t(xt+1), t)âˆ¥2\u0003\n= E\nh\r\rfÎ¸(zst+1, t + 1) âˆ’fÎ¸(Î¨st(zst+1), t)\n\r\r2i\n= E\nh\r\r\rfÎ¸(zst+1, t + 1) âˆ’fÎ¸(Ë†Î¨st(zst+1 + Î´st, ÏµÏ•), t)\n\r\r\r\ni\n= E\n\u0014\u0010\n1âˆ¥Î´st âˆ¥>Î· + 1âˆ¥Î´st âˆ¥â‰¤Î·\n\u0011 \r\r\rfÎ¸(zst+1, t + 1) âˆ’fÎ¸(Ë†Î¨st(zst+1 + Î´st, ÏµÏ•), t)\n\r\r\r\n2\u0015\nâ‰¤E\n\"\nsup\nâˆ¥Î´âˆ¥â‰¤Î·\n\r\r\rfÎ¸(zst+1, t + 1) âˆ’fÎ¸(Ë†Î¨st(zst+1 + Î´st, ÏµÏ•), t)\n\r\r\r\n#\n+ 4D2P\n\u0000âˆ¥Î´stâˆ¥2 â‰¥Î·\n\u0001\nâ‰¤E\n\"\nsup\nâˆ¥Î´âˆ¥â‰¤Î·\n\r\r\rfÎ¸(zst+1, t + 1) âˆ’fÎ¸(Ë†Î¨st(zst+1 + Î´, ÏµÎ´), t)\n\r\r\r\n2\n#\n+ 4D2\nÎ·\n\u0002\nC(st+1 âˆ’st)2 + ÏµO(âˆšst+1 âˆ’st)\n\u0003\n.\n(51)\nTaking sum over t and combining Theorem 2, we prove our conclusion.\nTherefore, in this theorem, by taking âˆ†st = st+1 âˆ’st close to zero, we get the results in Theorem 3.\nC\nTHE CONNECTION TO STANDARD ADVERSARIAL TRAINING\nIn this section, we clarify why the proposed AT objective (14) is a general version of the standard AT\nobjective proposed in (Madry et al., 2018) used for classification problems.\nFor classification problem, given model fÎ¸(x), data x, and label y, it aims to minimize the adversarial\ntraining objective\nmin\nÎ¸\nE(x,y)\n\"\nsup\nÎ´:âˆ¥Î´âˆ¥â‰¤Î·0\nâ„“(fÎ¸(x + Î´), y)\n#\n,\n(52)\nfor some loss function â„“(e.g. cross entropy) and adversarial radius Î·0. However, the objective is not\ndirectly generalized to the diffusion model, as its training objective is a regression problem instead\nof classification (52). Thus, we should refer to the general version of adversarial training as in (Yi\net al., 2021; Sinha et al., 2018), where the training objective is minÎ¸ Ex[â„“Î¸(x)], and the adversarial\ntraining objective becomes\nmin\nÎ¸\nEx\n\"\nsup\nÎ´:âˆ¥Î´âˆ¥â‰¤Î·0\nâ„“Î¸(x + Î´))\n#\n,\n(53)\nwhere â„“Î¸ is the parameterized loss function, and x is data. Then, we can conclude our objective (14)\nfollows the above formulation, such that the goal is represented as\nmin\nÎ¸\nT âˆ’1\nX\nt=0\nEx0\n\"\nExt|x0\n\"\nsup\nÎ´:âˆ¥Î´âˆ¥â‰¤Î·0\nâ„“x0\nÎ¸ (xt + Î´)\n##\n,\n(54)\ncompared with the original noise prediction objective minÎ¸\nPT âˆ’1\nt=0 Ex0\n\u0002\nExt|x0 [â„“x0\nÎ¸ (xt)]\n\u0003\n(5), such\nthat the loss function\nâ„“x0\nÎ¸ (xt) =\n\r\r\r\rÏµÎ¸(t, xt) âˆ’xt âˆ’âˆšÂ¯Î±tx0\nâˆš1 âˆ’Â¯Î±t\n\r\r\r\r\n2\n.\n(55)\nThis clarifies the equivalence of our objective (14) to general adversarial training.\nD\nADVERSARIAL TRAINING ON CONSISTENCY TRAINING MODEL\nIn (Song et al., 2023), the consistency model can be even trained without estimator Ë†Ï•s. They prove\nthat the empirical consistency distillation loss Ë†LCD(Î¸) can be approximated by the following LCT (Î¸)\nLCT (Î¸) =\nT âˆ’1\nX\nt=0\nExt+1âˆ¼q(xt+1)\n\u0002\nâˆ¥fÎ¸(xt, t) âˆ’fÎ¸(xt+1, t + 1)âˆ¥2\u0003\n.\n(56)\n21\n\nPublished as a conference paper at ICLR 2025\nIn our adversarial regime, we can also prove that the desired Ë†LAdv\nCD (Î¸) can be approximated by the\nfollowing LAdv\nCT (Î¸) with adversarial perturbation\nLAdv\nCT (Î¸) =\nT âˆ’1\nX\nt=0\nExt+1âˆ¼q(xt+1)\n\"\nsup\nâˆ¥Î´âˆ¥â‰¤Î·\nâˆ¥fÎ¸(xt + Î´, t) âˆ’fÎ¸(xt+1, t + 1)âˆ¥2\n#\n.\n(57)\nThe results can be checked by the following theorem.\nTheorem 5. Suppose fÎ¸(xt, t) is twice continuously differentiable with a bounded second derivative.\nThen\nË†LAdv\nCD (Î¸) â‰²LAdv\nCT (Î¸) + O\n \nT âˆ’\nT\nX\nt=1\nâˆšÎ±t + TÎ·2\n!\n,\n(58)\nwhere â€œâ‰²â€ means approximately less than or equal.\nProof. Due to the continuity of fÎ¸(x, t), for any Î´ with âˆ¥Î´âˆ¥â‰¤Î·, by Taylorâ€™s expansion on xt+1\nfrom xt + Î´, we have\nE\n\u0002\nâˆ¥fÎ¸(xt + Î´, t) âˆ’fÎ¸(xt+1, t + 1)âˆ¥2\u0003\n= E\n\u0002\nâˆ¥fÎ¸(xt+1, t) âˆ’fÎ¸(xt+1, t + 1)âˆ¥2\u0003\n+ E\nh\n(fÎ¸(xt+1, t) âˆ’fÎ¸(xt+1, t + 1))âŠ¤âˆ‡fÎ¸(xt+1, t)(xt + Î´ âˆ’xt+1)\ni\n+ O\n\u0000E\n\u0002\nâˆ¥xt+1 âˆ’xt âˆ’Î´âˆ¥2\u0003\u0001\n.\n(59)\nDue to the Taylorâ€™s expansion fÎ¸(xt + Î´, t) = fÎ¸(xt+1, t) + âˆ‡fÎ¸(xt+1, t)(xt + Î´ âˆ’xt+1) +\nO(âˆ¥xt+1 âˆ’xt âˆ’Î´âˆ¥2). Then, from the formulation of xt, we know E\n\u0002\nâˆ¥xt+1 âˆ’xt âˆ’Î´âˆ¥2\u0003\n=\nO(1 âˆ’âˆšÎ±t + Î·2). Noting that due to definition of st, we have\nE[xt | xt+1 = zst+1] = E[zst | zst+1]\n=\n1\nâˆšÎ±t+1\n\u0000zst+1 âˆ’(1 âˆ’Î±t+1)âˆ‡x log qst+1(zst+1)\n\u0001\n= exp\n\u00121\n2\nZ st+1\nst\nÎ²sds\n\u0013 \u0010\nzst+1 âˆ’\n\u0010\n1 âˆ’e\nR st+1\nst\nÎ²sds\u0011\nâˆ‡z log qst+1(zst+1)\n\u0011\nâ‰ˆ\n\u0012\n1 + 1\n2\nZ st+1\nst\nÎ²sds\n\u0013\nzst+1 + 1\n2\nZ st+1\nst\nÎ²sdsâˆ‡z log qst+1(zst+1)\nâ‰ˆË†Î¨st(zst+1,\np\n1 âˆ’Â¯Î±t+1âˆ‡z log qst+1),\n(60)\nwhere the first equality is due to Tweedieâ€™s formula i.e., Lemma 11 in (Bao et al., 2022), the â€œâ‰ˆâ€ is\ndue to ea â‰ˆ1 + a when a â†’0, and the last â‰ˆis due to Euler-Mayaruma discretion. Due to this, we\nnotice that\nE\nh\n(fÎ¸(xt+1, t) âˆ’fÎ¸(xt+1, t + 1))âŠ¤âˆ‡fÎ¸(xt+1, t)(xt + Î´ âˆ’xt+1) | xt+1 = zst+1\ni\n= E\nh\n(fÎ¸(xt+1, t) âˆ’fÎ¸(xt+1, t + 1))âŠ¤âˆ‡fÎ¸(xt+1, t)\n\u0000E\n\u0002\nxt + Î´ | xt+1 = zst+1\n\u0003\nâˆ’xt+1\n\u0001\n| xt+1 = zst+1\ni\nâ‰ˆE\nh\n(fÎ¸(zst+1, t) âˆ’fÎ¸(zst+1, t + 1))âŠ¤âˆ‡fÎ¸(zst+1, t)\n\u0010\nË†Î¨st(zst+1, âˆ‡z log qst+1) + E[Î´ | zst+1] âˆ’zt+1\n\u0011i\n,\n(61)\nwhere the first equality is due to the property of conditional expectation, and the second â€œâ‰ˆâ€ is due to\n(60). Combining this with (59), we have\nE\n\u0002\nâˆ¥fÎ¸(xt + Î´, t) âˆ’fÎ¸(xt+1, t + 1)âˆ¥2 | xt+1 = zst+1\n\u0003\n= E\nh\r\rfÎ¸(zst + Î´, t) âˆ’fÎ¸(zst+1, t + 1)\n\r\r2 | zst+1\ni\n= E\nh\r\rfÎ¸(zst+1, t) âˆ’fÎ¸(zst+1, t + 1)\n\r\r2i\n+ E\nh\n(fÎ¸(zst+1, t) âˆ’fÎ¸(zst+1, t + 1))âŠ¤âˆ‡fÎ¸(zst+1, t)\n\u0010\nË†Î¨st(zst+1, âˆ‡z log qst+1) + E[Î´ | zst+1] âˆ’zst+1\n\u0011i\n+ O(1 âˆ’âˆšÎ±t + Î·2)\n= E\n\u0014\r\r\rfÎ¸(Ë†Î¨st(zst+1, âˆ‡z log qst+1) + Î´, t) âˆ’fÎ¸(zst+1, t + 1)\n\r\r\r\n2\u0015\n+ O(1 âˆ’âˆšÎ±t + Î·2),\n(62)\nwhere the last equality is due to Taylorâ€™s expansion from fÎ¸(Ë†Î¨st(zst+1, âˆ‡z log qst+1) + Î´, t) to\nfÎ¸(zst+1, t). Due to the arbitrariness of Î´, we prove our conclusion.\n22\n\nPublished as a conference paper at ICLR 2025\nE\nIMPLEMENTATION DETAILS\nE.1\nHYPERPARAMETERS OF DIFFUSION MODELS\nFor the diffusion models, all methods adopt the ADM model (Dhariwal & Nichol, 2021) as the\nbackbone and follow the same training pipeline. Following existing work (Dhariwal & Nichol, 2021;\nNing et al., 2023), we train models using the AdamW optimizer (Loshchilov & Hutter, 2019) with\nmixed precision training and the EMA rate is set to 0.9999. For CIFAR-10, the pretrained ADM is\ntrained using a batch size of 128 for 250K iterations with a learning rate set to 1e-4. For ImageNet,\nthe pretrained model is trained with a batch size of 1024 for 400K iterations, employing a learning\nrate of 3e-4. The models are trained in a cluster of NVIDIA Tesla V100s. More hyperparameters are\nreported in Table 4.\nTable 4: Hyperparameters of diffusion model on each datasets.\nHyperparameters\nCIFAR10 32 Ã— 32\nImageNet 64 Ã— 64\nChannels\n128\n192\nBatch size\n128\n1024\nLearning rate\n1e-4\n3e-4\nFine-tuning iterations\n200K\n200K\nDropout\n0.3\n0.1\nNoise schedule\nCosine\nCosine\nE.2\nHYPERPARAMETERS OF LATENT CONSISTENCY MODELS\nFor experiments on Latent Consistency Models (LCM) (Luo et al., 2023), we train models on\nLAIOIN-Aesthetic-6.5+ (Schuhmann et al., 2022) at the resolution of 512Ã—512, comprising 650K\ntext-image pairs with predicted aesthetic scores higher than 6.5. Stable Diffusion v1.5 (Rombach\net al., 2022) is adopted as the teacher model and initialized the student and target models in the latent\nconsistency distillation framework. We set the range of the guidance scale [wmin, wmax] = [3, 5]\nduring training and use w = 4 in sampling because it performs better in our preliminary experiments,\nwhich is similar to DMD (Yin et al., 2024). The models are trained in a cluster of NVIDIA Tesla\nV100s. Both models of our AT and the original LCM training are trained from scratch with the\nsame hyperparameters. We select the adversarial learning rate Î± from {0.02, 0.05} and adversarial\nstep K from {2, 3}. More details of hyperparameters are shown in Table 5 and other details of\nimplementations can be found in the original LCM paper (Luo et al., 2023).\nTable 5: Hyperparameters of latent consistency model.\nHyperparameters\nLAIOIN-Aesthetic-6.5+\nBatch size\n64\nLearning rate\n8e-6\nTraining iterations\n100K\nEMA rate of target model\n0.95\nConditional guidance scale [wmin, wmax]\n[3, 5]\nF\nADDITIONAL RESULTS\nF.1\nRESULTS OF CLASSIFICATION ACCURACY SCORE\nClassification Accuracy Score (CAS) (Ravuri & Vinyals, 2019) is proposed to evaluate the utility of\nthe images produced by the generative model for downstream classification tasks. The underlying\nmotivation for this metric is that if the generative model captures the real data distribution, the\nreal data distribution can be replaced by the model-generated data and achieve similar results on\ndownstream tasks like image classification.\n23\n\nPublished as a conference paper at ICLR 2025\nTable 6: Comparasion of CAS of different methods on CIFAR-10 32Ã—32 dataset.\nMethods\nCAS\nReal\n92.5\nonly using the synthetic data.\nADM\n91.0\nADM-IP\n89.2\nADM-AT (Ours)\n91.6\nusing the synthetic data with real data.\nADM\n95.0\nADM-IP\n94.9\nADM-AT (Ours)\n95.4\nFollowing the evaluation pipeline in Ravuri & Vinyals (2019), we train the image classifier in two\nsettings: only on synthetic data or real data augmented with synthetic data, and use the classifier to\npredict labels on the test set of real data. Synthetic images are generated with a DDIM sampler under\n20 NFEs. We use ResNet-18 (He et al., 2016) as the image classifier and train it for 200 epochs with a\nlearning rate of 0.1 and a batch size of 128. We report CAS in the CIFAR-10 dataset at a resolution\nof 32Ã—32 in Table 6. The results indicate that our method consistently performs better than other\nbaseline methods on CAS metric in both settings. Although CAS with synthetic data cannot surpass\nreal data, it demonstrates significant potential for enhancing classifier accuracy when employed as an\naugmentation technique alongside real data.\nTable 7: Comparasion of AT with TS-DDIM on CIFAR10 32Ã—32. Both models are based on the\nADM backbone. The results of TS are taken directly from the original paper.\nMethods \\ NFEs\n50\n20\n10\n5\nADM-TS-DDIM\n3.52\n5.35\n10.73\n26.94\nADM-AT (Ours)\n3.07\n4.40\n9.30\n26.38\nF.2\nCOMPARISON TO TS-DDIM\nLi et al. (2024) introduces another approach named Time-Shift (TS) to alleviate the DPM distribution\nmismatch by searching for coupled time steps in sampling. Table 7 shows the comparison between\nour AT method with TS on CIFAR-10 with the DDIM Sampler. Both methods are based on the\nADM pretrained model (Dhariwal & Nichol, 2021) as a backbone, which is the same as Section 6.2.\nWe observe our method consistently better than the TS method across various sampling steps.\nF.3\nRESULTS OF MORE NFES\nWe present results obtained with various samplers under 100 or 200 NFEs on CIFAR10 32x32 and\nImageNet 64x64 in Table 8 and Table 9, respectively. The results show that our method is still\neffective for samplers under hundreds of NFEs.\nF.4\nRESULTS OF MORE METRICS\nWe present the results of more generation quality metrics, including sFID, Inception Score (IS),\nPrecision, and Recall, on CIFAR10 32x32 (Table 10 and Table 11) and ImageNet 64x64 (Table 12\nand Table 13). The evaluation is performed following Dhariwal & Nichol (2021). We observe that\nour method shows effectiveness across these metrics.\n24\n\nPublished as a conference paper at ICLR 2025\nTable 8: Sample quality measured by FID â†“of various sampling methods of DPM under 100 or 200\nNFEs on CIFAR10 32x32.\nMethods\nIDDPM\nDDIM\nES\nDPM-Solver\n100\n200\n100\n200\n100\n200\n100\n200\nADM-FT\n3.34\n3.02\n4.02\n4.22\n2.38\n2.45\n2.97\n2.97\nADM-IP\n2.83\n2.73\n6.69\n8.44\n2.97\n3.12\n10.10\n10.11\nADM-AT (Ours)\n2.52\n2.46\n3.19\n3.23\n2.18\n2.35\n2.83\n3.00\nTable 9: Sample quality measured by FID â†“of various sampling methods of DPM under 100 or 200\nNFEs on ImageNet 64x64.\nMethods\nIDDPM\nDDIM\nES\nDPM-Solver\n100\n200\n100\n200\n100\n200\n100\n200\nADM-FT\n3.88\n3.48\n4.71\n4.38\n3.07\n2.98\n4.20\n4.13\nADM-IP\n3.55\n3.08\n8.53\n10.43\n3.36\n3.31\n9.75\n9.77\nADM-AT (Ours)\n3.35\n3.16\n4.58\n4.34\n3.05\n3.10\n4.31\n4.10\nTable 10: Comparison of sFID â†“and IS â†‘on CIFAR10 32x32.\n(a) IDDPM\n5\n8\n10\n20\n50\nsFID\nIS\nsFID\nIS\nsFID\nIS\nsFID\nIS\nsFID\nIS\nADM\n20.95\n8.25\n25.03\n8.51\n23.56\n8.50\n16.01\n9.14\n6.81\n9.49\nADM-IP\n25.81\n7.02\n24.51\n8.04\n19.02\n8.50\n8.99\n9.28\n5.32\n9.66\nADM-AT\n19.78\n8.71\n25.67\n8.66\n23.09\n8.77\n6.01\n9.30\n5.04\n9.65\n(b) DDIM\n5\n8\n10\n20\n50\nsFID\nIS\nsFID\nIS\nsFID\nIS\nsFID\nIS\nsFID\nIS\nADM\n12.75\n7.76\n8.53\n8.62\n8.39\n8.70\n6.19\n9.08\n4.99\n9.19\nADM-IP\n15.53\n7.55\n8.00\n8.98\n7.12\n9.15\n5.30\n9.41\n5.64\n9.49\nADM-AT\n12.56\n7.97\n7.93\n8.90\n7.08\n8.90\n5.37\n9.17\n4.66\n9.51\n(c) ES\n5\n8\n10\n20\n50\nsFID\nIS\nsFID\nIS\nsFID\nIS\nsFID\nIS\nsFID\nIS\nADM\n27.39\n6.14\n14.91\n8.33\n10.04\n8.79\n5.45\n9.55\n4.12\n9.62\nADM-IP\n34.70\n5.73\n16.84\n8.23\n10.89\n8.88\n4.94\n9.59\n4.08\n9.70\nADM-AT\n16.84\n6.97\n10.33\n8.60\n8.00\n8.95\n4.78\n9.65\n4.04\n9.77\n(d) DPM-Solver\n5\n8\n10\n20\n50\nsFID\nIS\nsFID\nIS\nsFID\nIS\nsFID\nIS\nsFID\nIS\nADM\n11.82\n8.00\n5.79\n9.12\n5.05\n9.41\n4.43\n9.78\n4.32\n9.82\nADM-IP\n26.46\n7.09\n5.93\n9.19\n5.49\n9.45\n7.53\n9.66\n8.37\n9.75\nADM-AT\n11.19\n8.43\n5.10\n9.35\n5.29\n9.65\n4.75\n10.03\n4.59\n9.93\nG\nMORE ANALYSIS\nG.1\nEFFICIENT AT VS STANDARD AT\nIn this section, we conduct an ablation of the AT method in diffusion model training. We compare\nthe performance of our used efficient AT and a standard AT method PGD on CIFAR-10 dataset at\nthe resolution of 32Ã—32. The adversarial step K is set to be 3 for both methods. We fine-tune both\n25\n\nPublished as a conference paper at ICLR 2025\nTable 11: Comparison of Precision (P) â†‘and Recall (R) â†‘on CIFAR10 32x32.\n(a) IDDPM\n5\n8\n10\n20\n50\nP\nR\nP\nR\nP\nR\nP\nR\nP\nR\nADM\n0.54\n0.47\n0.59\n0.45\n0.61\n0.46\n0.64\n0.52\n0.68\n0.58\nADM-IP\n0.54\n0.39\n0.59\n0.43\n0.61\n0.46\n0.66\n0.54\n0.68\n0.59\nADM-AT\n0.52\n0.47\n0.57\n0.45\n0.62\n0.46\n0.68\n0.55\n0.69\n0.59\n(b) DDIM\n5\n8\n10\n20\n50\nP\nR\nP\nR\nP\nR\nP\nR\nP\nR\nADM\n0.57\n0.47\n0.59\n0.52\n0.61\n0.52\n0.64\n0.52\n0.63\n0.60\nADM-IP\n0.57\n0.44\n0.62\n0.53\n0.63\n0.56\n0.65\n0.60\n0.65\n0.61\nADM-AT\n0.59\n0.46\n0.62\n0.52\n0.63\n0.54\n0.65\n0.58\n0.66\n0.61\n(c) ES\n5\n8\n10\n20\n50\nP\nR\nP\nR\nP\nR\nP\nR\nP\nR\nADM\n0.54\n0.37\n0.60\n0.48\n0.61\n0.52\n0.64\n0.52\n0.63\n0.60\nADM-IP\n0.46\n0.32\n0.58\n0.45\n0.62\n0.51\n0.67\n0.58\n0.68\n0.60\nADM-AT\n0.61\n0.45\n0.64\n0.51\n0.65\n0.54\n0.65\n0.58\n0.66\n0.61\n(d) DPM-Solver\n5\n8\n10\n20\n50\nP\nR\nP\nR\nP\nR\nP\nR\nP\nR\nADM\n0.61\n0.47\n0.65\n0.58\n0.65\n0.59\n0.66\n0.61\n0.63\n0.62\nADM-IP\n0.49\n0.32\n0.65\n0.58\n0.65\n0.59\n0.62\n0.58\n0.61\n0.56\nADM-AT\n0.62\n0.49\n0.65\n0.59\n0.65\n0.61\n0.67\n0.62\n0.65\n0.61\nmodels from the same pretrained ADM model with 100K update iterations of the model. The results\nare shown in Table 14. We report the results of 4 sampler settings (method-NFEs): IDDPM-50,\nDDIM-50, ES-20, and DPM-Solver-10.\nWe observe that efficient AT achieves performance comparable to or even better than PGD with the\nsame model update iterations while accelerating the training (2.6Ã— speed-up). Thus, we propose\napplying the efficient AT method for our adversarial training framework.\nG.2\nCONVERGENCE OF AT ON DIFFUSION MODELS\n100K\n150K\n200K\nIterations\n5\n6\n7\n8\n9\nFID\nDDIM-50\nMethod\nADM\nADM-IP\nADM-AT (ours)\nFigure 2: The convergence of methods trained from scratch on CIFAR-10 32 Ã— 32. We use the\nDDIM sampler with 50 NFEs for sampling.\n26\n\nPublished as a conference paper at ICLR 2025\nTable 12: Comparison of sFID â†“and IS â†‘on ImageNet 64x64.\n(a) IDDPM\n5\n8\n10\n20\n50\nsFID\nIS\nsFID\nIS\nsFID\nIS\nsFID\nIS\nsFID\nIS\nADM\n26.17\n12.55\n36.34\n22.61\n40.52\n26.55\n26.08\n39.10\n11.35\n45.68\nADM-IP\n40.90\n12.19\n47.98\n23.47\n37.72\n27.86\n25.06\n39.40\n6.75\n44.87\nADM-AT\n24.82\n14.50\n37.04\n23.84\n36.50\n30.03\n22.83\n39.12\n5.69\n46.25\n(b) DDIM\n5\n8\n10\n20\n50\nsFID\nIS\nsFID\nIS\nsFID\nIS\nsFID\nIS\nsFID\nIS\nADM\n27.74\n14.30\n14.27\n25.88\n12.78\n28.29\n8.84\n33.54\n6.31\n38.08\nADM-IP\n52.08\n10.21\n16.40\n22.03\n11.70\n25.94\n9.09\n32.04\n15.14\n31.62\nADM-AT\n25.49\n14.82\n10.68\n26.62\n9.22\n29.29\n6.41\n34.33\n4.66\n39.36\n(c) ES\n5\n8\n10\n20\n50\nsFID\nIS\nsFID\nIS\nsFID\nIS\nsFID\nIS\nsFID\nIS\nADM\n34.55\n13.29\n42.32\n24.98\n34.44\n29.36\n14.44\n40.45\n6.41\n45.36\nADM-IP\n44.81\n10.07\n41.01\n22.44\n30.12\n27.66\n10.13\n39.50\n4.67\n44.69\nADM-AT\n29.72\n16.49\n33.58\n27.85\n27.64\n31.94\n10.22\n42.18\n5.10\n45.59\n(d) DPM-Solver\n5\n8\n10\n20\n50\nsFID\nIS\nsFID\nIS\nsFID\nIS\nsFID\nIS\nsFID\nIS\nADM\n25.70\n24.34\n11.08\n34.77\n8.05\n37.45\n5.35\n40.54\n4.69\n41.31\nADM-IP\n42.68\n16.93\n7.47\n33.85\n7.22\n33.57\n14.74\n31.29\n18.99\n30.32\nADM-AT\n20.79\n26.32\n7.60\n34.89\n6.36\n36.51\n4.51\n38.79\n4.22\n39.10\n0\n50K\n150K\n250K\nIterations\n7\n8\n9\n10\nFID\nIDDPM-20\n0\n50K\n150K\n250K\nIterations\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\nFID\nDDIM-50\n0\n50K\n150K\n250K\nIterations\n4.4\n4.6\n4.8\n5.0\n5.2\n5.4\n5.6\nFID\nES-20\n0\n50K\n150K\n250K\nIterations\n4.5\n5.0\n5.5\n6.0\n6.5\n7.0\n7.5\n8.0\n8.5\nFID\nDPM-Solver-10\nMethod\nADM\nADM-IP\nADM-AT (ours)\nFigure 3: The convergence of methods fine-tuned from a same pretrained model on CIFAR-10\n32 Ã— 32. We compare the performance of methods on various samplers.\nIn classification tasks, adding adversarial perturbations usually slows the convergence of model\ntraining (Zhu et al., 2020). We are interested to see whether AT also affects the convergence of the\ndiffusion training process.\nFirstly, we explore the convergence of models trained from scratch. We utilize DDIM as the sampler\nwith 50 NFEs and the results are shown in Figure 2. We observe that our AT method and ADM-IP\nexhibit slower convergence compared to ADM at the beginning (before 100K iterations), while as\ntraining more iterations (200K), our AT method shows a notable advantage.\nMoreover, we explore the convergence of models under fine-tuning setting and the results are shown\nin Figure 3. We observe under this setting, when given a pretrained diffusion model like ADM,\nfine-tuning it with our proposed AT improves performance faster than other baselines. Overall, we\nobserve that incorporating AT with a diffusion framework does not affect the convergence of the\nmodel much, especially in the fine-tuning setting.\n27\n\nPublished as a conference paper at ICLR 2025\nTable 13: Comparison of Precision (P) â†‘and Recall (R) â†‘on ImageNet 64x64.\n(a) IDDPM\n5\n8\n10\n20\n50\nP\nR\nP\nR\nP\nR\nP\nR\nP\nR\nADM\n0.34\n0.48\n0.46\n0.50\n0.51\n0.48\n0.65\n0.52\n0.73\n0.57\nADM-IP\n0.39\n0.39\n0.50\n0.45\n0.56\n0.48\n0.68\n0.55\n0.73\n0.60\nADM-AT\n0.40\n0.50\n0.50\n0.50\n0.55\n0.49\n0.69\n0.52\n0.77\n0.59\n(b) DDIM\n5\n8\n10\n20\n50\nP\nR\nP\nR\nP\nR\nP\nR\nP\nR\nADM\n0.42\n0.47\n0.54\n0.56\n0.58\n0.58\n0.65\n0.60\n0.69\n0.61\nADM-IP\n0.38\n0.40\n0.51\n0.53\n0.55\n0.57\n0.63\n0.61\n0.62\n0.61\nADM-AT\n0.44\n0.43\n0.58\n0.55\n0.62\n0.56\n0.69\n0.59\n0.72\n0.61\n(c) ES\n5\n8\n10\n20\n50\nP\nR\nP\nR\nP\nR\nP\nR\nP\nR\nADM\n0.40\n0.44\n0.52\n0.47\n0.58\n0.48\n0.69\n0.55\n0.73\n0.59\nADM-IP\n0.37\n0.35\n0.49\n0.44\n0.56\n0.49\n0.68\n0.57\n0.72\n0.60\nADM-AT\n0.44\n0.46\n0.58\n0.48\n0.63\n0.49\n0.73\n0.55\n0.76\n0.59\n(d) DPM-Solver\n5\n8\n10\n20\n50\nP\nR\nP\nR\nP\nR\nP\nR\nP\nR\nADM\n0.51\n0.49\n0.65\n0.58\n0.67\n0.60\n0.69\n0.62\n0.69\n0.62\nADM-IP\n0.39\n0.44\n0.64\n0.60\n0.64\n0.60\n0.59\n0.60\n0.57\n0.59\nADM-AT\n0.56\n0.50\n0.68\n0.57\n0.69\n0.59\n0.72\n0.60\n0.71\n0.61\nTable 14: Comparison of different AT methods used in our AT framework. All models are trained\nwith the same model-updating iterations while the efficient AT has less training time.\nMethods\nFID\nTraining Time\nIDDPM-50\nDDIM-50\nES-20\nDPM-Solver-10\nSpeedup\nStandard AT PGD-3\n4.02\n3.37\n6.42\n7.60\n1.0Ã—\nEfficient AT (Ours)\n3.97\n3.42\n5.98\n6.05\n2.6Ã—\nTable 15: Comparison of different adversarial learning rate Î± of our AT framework on CIFAR10\n32x32. IDDPM is adopted as the inference sampler.\nÎ± \\ NFEs\n5\n8\n10\n20\n50\nÎ± = 0.05\n51.72\n32.09\n25.48\n10.38\n4.36\nÎ± = 0.1\n37.15\n23.59\n15.88\n6.60\n3.34\nÎ± = 0.5\n63.73\n40.08\n27.57\n7.23\n3.42\nTable 16: Comparison of different adversarial learning rate Î± of our AT framrwork on ImageNet\n64x64. IDDPM is adopted as the inference sampler.\nÎ± \\ NFEs\n5\n8\n10\n20\n50\nÎ± = 0.1\n56.92\n27.39\n24.06\n10.17\n5.82\nÎ± = 0.5\n45.65\n23.79\n19.18\n8.28\n4.01\nÎ± = 0.8\n46.92\n28.46\n22.47\n9.70\n4.25\n28\n\nPublished as a conference paper at ICLR 2025\nTable 17: Comparison of different perturbation norms (l1, l2 lâˆ) of our AT framework on CIFAR10\n32x32.\nPerturbation Norm\nIDDPM-50\nDDIM-50\nES-20\nDPM-Solver-10\nl1\n4.45\n4.91\n4.72\n5.05\nl2\n3.34\n3.07\n4.36\n4.81\nlâˆ\n3.87\n3.63\n4.48\n5.32\nG.3\nMORE ABLATION STUDY\nAblation on Î±\nWe investigate the impact of adversarial learning rate Î± in our framework. The\nresults of various Î± on CIFAR10 32x32 and ImageNet 64x64 are shown in Table 15 and Table 16,\nrespectively. We observe that Î± set to 0.1 is better on CIFAR10 32x32 and Î± = 0.5 is better for\nImageNet 64x64. That says, the image in larger size corresponds to larger optimal perturbation\nlevel Î±. We speculate this is because we use the perturbation measured under â„“2-norm, where the\nâ„“2-norm of vector will increase with its dimension.\nAblation on perturbation norm\nDuring our experiments, we adopt â„“2-adversarial perturbation.\nActually, perturbations in Euclidean space under different â„“p norm are equivalent with each other, e.g.,\nfor vector Î´ âˆˆRd, it holds âˆ¥Î´âˆ¥âˆâ‰¤âˆ¥Î´âˆ¥2 â‰¤\nâˆš\ndâˆ¥Î´âˆ¥âˆ. Therefore, we select âˆ¥Â· âˆ¥2 as representation\nin our paper. Next, we explore the proposed ADM-AT under different adversarial perturbations.\nThe results are in Table 17. We found that our method under â„“2-perturbation is more stable and indeed\nhas better performance, thus we suggest to use â„“2-perturbation as in the main body of this paper.\nG.4\nQUALITATIVE COMPARISONS\nFigure 4: The qualitative comparsions of ADM-AT (top, FID 6.60), ADM-IP (middle, FID 7.81), and\nADM (bottom, FID 10.58) on CIFAR10 32 Ã— 32. We use the IDDPM sampler with 20 NFEs for\nsampling.\nFigure 4, 5, 6, 7 show the qualitative comparisons between our proposed AT method and baselines.\nOur proposed AT method generates more realistic and higher-fidelity samples. We attribute this to\nour AT algorithm mitigates the distribution mismatch problem.\n29\n\nPublished as a conference paper at ICLR 2025\nFigure 5: The qualitative comparsions of ADM-AT (top, FID 6.20), ADM-IP (middle, FID 8.40)\nand ADM (bottom, FID 8.32) on ImageNet 64 Ã— 64. We use the DDIM sampler with 20 NFEs for\nsampling.\nFigure 6: The qualitative comparsions of LCM (left) and LCM-AT (right) with one-step generation.\nThe text prompt is A photo of beautiful mountain with realistic sunset and blue lake, highly detailed,\nmasterpiece.\nFigure 7: The qualitative comparsions of LCM (left) and LCM-AT (right) with one-step generation.\nThe text prompt is Astronaut in a jungle, cold color palette, muted colors, detailed, 8k.\n30\n",
  "metadata": {
    "source_path": "papers/arxiv/Improved_Diffusion-based_Generative_Model_with_Better_Adversarial\n__Robustness_eb2ec4b67770f7d6.pdf",
    "content_hash": "eb2ec4b67770f7d65fdf36ff4a645ee1120857e623259e8d455529ad96bb4dc6",
    "arxiv_id": null,
    "title": "Improved_Diffusion-based_Generative_Model_with_Better_Adversarial\n__Robustness_eb2ec4b67770f7d6",
    "author": "",
    "creation_date": "D:20250225024435Z",
    "published": "2025-02-25T02:44:35",
    "pages": 30,
    "size": 1343888,
    "file_mtime": 1740470187.363268
  }
}