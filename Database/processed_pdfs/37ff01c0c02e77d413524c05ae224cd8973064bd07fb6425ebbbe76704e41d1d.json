{
  "text": "Benchmarking Retrieval-Augmented Generation in Multi-Modal Contexts\nZhenghao Liu1, Xingsheng Zhu1, Tianshuo Zhou1, Xinyi Zhang1, Xiaoyuan Yi2,\nYukun Yan3, Yu Gu1, Ge Yu1, Maosong Sun3*\n1Department of Computer Science and Technology, Northeastern University, China\n2Microsoft Research Asia, Beijing, China\n3Department of Computer Science and Technology, Institute for AI, Tsinghua University, China\nBeijing National Research Center for Information Science and Technology, China\nAbstract\nThis paper introduces Multi-Modal Retrieval-\nAugmented Generation (M2RAG), a bench-\nmark designed to evaluate the effective-\nness of Multi-modal Large Language Models\n(MLLMs) in leveraging knowledge from multi-\nmodal retrieval documents. The benchmark\ncomprises four tasks: image captioning, multi-\nmodal question answering, multi-modal fact\nverification, and image reranking. All tasks\nare set in an open-domain setting, requiring\nRAG models to retrieve query-relevant informa-\ntion from a multi-modal document collection\nand use it as input context for RAG modeling.\nTo enhance the context utilization capabilities\nof MLLMs, we also introduce Multi-Modal\nRetrieval-Augmented Instruction Tuning (MM-\nRAIT), an instruction tuning method that opti-\nmizes MLLMs within multi-modal contexts.\nOur experiments show that MM-RAIT im-\nproves the performance of RAG systems by\nenabling them to effectively learn from multi-\nmodal contexts. All data and code are available\nat https://github.com/NEUIR/M2RAG.\n1\nIntroduction\nWith the rapid development of Large Language\nModels (LLMs), such as GPT-4 (OpenAI, 2023)\nand LLaMA (Touvron et al., 2023), they have\ndemonstrated strong emergent abilities in many\nNLP tasks (Wei et al., 2022; Zhao et al., 2023).\nHowever, LLMs often face the issue of hallu-\ncinations, making them produce unreliable re-\nsponses (Ji et al., 2023; Huang et al., 2023; Shus-\nter et al., 2021). Retrieval-Augmented Generation\n(RAG) (Lewis et al., 2020; Asai et al., 2024b; Shi\net al., 2024; Yao et al., 2023) has proven effective\nin mitigating this hallucination problem by integrat-\ning external knowledge with LLMs.\nTo enhance LLMs with retrieved knowledge,\nexisting approaches typically feed retrieved doc-\numents as input contexts, prompting LLMs to\n* indicates corresponding author.\nDescribe the Image：\nTwo flagpoles are located at the base of the dome \non the East and West sides. These flagpoles …\nMāui dolphin, Maui's \ndolphin, or Popoto is …\nRetrieved Documents\nQuestion:How many U.S. flags in front of the U.S. capital dome?\nDescription:\nImage showcases two Māui \ndolphins, also known as Maui's \ndolphin or Popoto, gracefully \nswimming in the turquoise …\nAnswer：There is one U.S. flag in front of the dome.\nRetrieved Documents\nTexts\nIn 2002, Māui dolphins \nwere classified as a …\nTexts\nImages\nImages\n…\n…\nText Docs\nImage Docs\nImage Captioning Task\nMulti-Modal QA Task\nFigure 1: Illustration of Multi-Modal RAG Tasks. We\nincorporate multi-modal retrieval documents as the in-\nput context for MLLMs.\ngenerate responses based on this in-context in-\nformation (Ram et al., 2023). Existing RAG ap-\nproaches (Petroni et al., 2021; Lin et al., 2024)\nusually focus on retrieving textual knowledge from\ncorpora to aid LLMs in answering queries. Re-\ncent studies (Hu et al., 2024; Sharifymoghaddam\net al., 2024) have extended RAG to Multi-modal\nLarge Language Models (MLLMs), enabling them\nto address knowledge-intensive and information-\nseeking tasks that involve visual queries. However,\nthese approaches largely rely on text or images\nas the sole sources of external knowledge, often\noverlooking the critical role of multi-modal data\nin providing richer and more comprehensive in-\nformation that leads to producing more accurate\nanswers (Hu et al., 2024; Liu et al., 2023b).\narXiv:2502.17297v1  [cs.AI]  24 Feb 2025\n\nTo advance RAG modeling in multi-modal\nscenarios, we introduce the Multi-Modal RAG\n(M2RAG) benchmark, designed to explore the ef-\nfectiveness of MLLMs by feeding multi-modal re-\ntrieved documents as the input contexts to answer\nthe question. As shown in Figure 1, we can use im-\nages or text as queries to retrieve multi-modal doc-\numents via multi-modal dense retrievers (Liu et al.,\n2023b; Zhou et al., 2024b,a). These multi-modal\ndocuments are then used as the input contexts to\nassist MLLMs during generation. Specifically, our\nM2RAG benchmark includes four distinct tasks:\nimage captioning, multi-modal question answering,\nmulti-modal fact verification, and image reranking.\nDifferent from existing works (Aghajanyan et al.,\n2022; Sharifymoghaddam et al., 2024), M2RAG\nis built upon high-quality datasets (Chang et al.,\n2022; Mishra et al., 2022) and designs four evalu-\nation tasks in the open-domain setting, aiming to\nassess the effectiveness of MLLMs in leveraging\nthe knowledge from multi-modal contexts.\nIn this paper, we also propose the Multi-Modal\nRetrieval Augmented Instruction Tuning (MM-\nRAIT) method to adapt MLLMs to the multi-modal\nin-context learning scenario, enhancing the effec-\ntiveness of MLLMs in utilizing the knowledge from\nthese multi-modal retrieval documents. Specifi-\ncally, we design task-specific prompt templates\nand fine-tune MLLMs on the M2RAG benchmark,\nmaking MLLMs maintain contextual awareness\nduring generation. Our experimental results demon-\nstrate that using retrieved knowledge significantly\nenhances MLLMs’ performance, achieving signifi-\ncant improvements in both zero-shot and few-shot\nsettings. After training with MM-RAIT, MiniCPM-\nV and Qwen2-VL show an average improvement of\n27% and 34% over vanilla RAG modeling methods,\nshowing the effectiveness of MM-RAIT.\n2\nRelated Work\nExisting RAG models (Shi et al., 2024; Asai et al.,\n2024a; Yu et al., 2023b; Yan et al., 2024) typically\nrely on dense retrievers (Karpukhin et al., 2020;\nXiong et al., 2021a; Ren et al., 2021; Xiong et al.,\n2021b; Gao and Callan, 2022) or sparse retriev-\ners like BM25 (Robertson et al., 2009) for text\ndocument retrieval. More recent efforts have in-\ntegrated multi-modal retrieval methods, allowing\nthe inclusion of rich external knowledge from dif-\nferent modalities within RAG frameworks. For\nexample, some works (Liu et al., 2023b; Zhou\net al., 2024b) have introduced unified multi-modal\nretrieval systems that map images and texts into\na shared semantic space. This approach allows\nfor single-modal matching, cross-modal matching,\nand modality routing within the embedding space.\nVISTA (Zhou et al., 2024a) further enhances multi-\nmodal retrieval by optimizing synthetic training\ndata and refining training strategies. These advance-\nments enable the retrieval of multi-modal knowl-\nedge, providing a way for evaluating the effective-\nness of MLLMs in multi-modal contexts.\nMulti-modal\nLarge\nLanguage\nModels\n(MLLMs) (Achiam et al., 2023; Team et al.,\n2023; Sun et al., 2024b,a; Aghajanyan et al., 2022;\nLu et al., 2024) have proven their effectiveness\nin understanding, integrating, and utilizing both\nvisual and textual knowledge in generation\ntasks. Models like BLIP (Li et al., 2022, 2023),\nLLaVA (Liu et al., 2023a), and Flamingo (Alayrac\net al., 2022) build the MLLMs by combining\npre-trained vision encoders with Large Language\nModels (LLMs) to process multi-modal inputs\nfor generation.\nThriving on the advancements\nin MLLMs, researchers pay more attention to\nextending the advantages of Retrieval-Augmented\nGeneration (RAG) to these MLLMs, enhancing\ntheir generation capability.\nMulti-modal RAG has demonstrated its potential\nto enhance knowledge-intensive and information-\nseeking tasks, such as question answering (Chang\net al., 2022; Marino et al., 2019) and fact verifi-\ncation (Mishra et al., 2022). These models utilize\nretrieval-based multi-modal documents to provide\nricher and contextually relevant information. Ad-\nditionally, other works have applied multi-modal\nRAG to improve the performance of MLLMs on\nthe tasks like image captioning (Lin et al., 2014;\nYoung et al., 2014) and generation (Yasunaga et al.,\n2023; Yu et al., 2023a; Sharifymoghaddam et al.,\n2024).\nHowever, existing multi-modal bench-\nmarks (Johnson et al., 2017; Schuhmann et al.,\n2021; Lin et al., 2014; Young et al., 2014; Marino\net al., 2019) are typically tailored to specific tasks\nand lack a comprehensive framework for evaluating\nmulti-modal RAG systems.\n3\nM2RAG Benchmark for Multi-Modal\nRetrieval-Augmented Generation\nIn this section, we describe our Multi-Modal\nRetrieval-Augmented Generation (M2RAG) bench-\nmark. We first introduce the RAG tasks in M2RAG,\n\nMulti-Modal Fact Verification\nA  Speaker who \nwas scheduled \nto deliver \nremarks at the Republican …\nMendoza was \nscheduled to \nappear Tuesday \nnight in a video message … \nClaim:\nRelated Doc:\nCategory:  Support\nMore than 100 \nChinese soldiers \nwere killed in the \ndreadful …\nClaim:\nRelated Doc:\n… There are \nseveral red flags \nthat suggest the \nclaim is false. …\nCategory:  Refute\nMulti-Modal Question Answering\nHow many people are holding \nwhips in the painting “The Paris \nDiligence” ?\nQuestion: \nRelated Doc: \nAnswer: There are 2 people \nholding whips.\nQuestion: \nWere the British more successful \nin the Battle of Atbara or the \nBattle of Sangshak?\nRelated Doc: \nThe Battle of Atbara took \nplace during … . The battle \nproved to be the turning point \nin the conquest of Sudan by a \nBritish and Egyptian coalition.\nAnswer: The Battle of Atbara.\nImage Reranking\nRetrieved Order: \nQuery: Dole container is placed onto a truck.\n1st: \n2nd: \n3rd: \n4th: \nReranked\nTop 1: \n1st: \nImage Captioning\nCaption: \nFood truck in Stockholm, 2014.\nDescribe Image:\nCaption:\nBeyazit Mosque Istanbul.\nDescribe Image:\nRelated Doc: \nFood Truck \nat FOSDEM \n2013.\nRelated Doc: \nMinaret \nSultan Ahmet \nMosque.\nFigure 2: Examples of Different Tasks Defined in the M2RAG Benchmark. All tasks are designed for the open-\ndomain setting. Thus, we present the input, ground truth answers, and retrieved documents for each task.\nfollowed by a detailed explanation of the construc-\ntion process. Finally, we present a comparison of\nexisting multi-modal benchmarks with M2RAG.\nTask Definition. As shown in Figure 2, M2RAG\ndefines four tasks to evaluate the capabilities of\nMLLMs in open-domain RAG scenarios: image\ncaptioning, multi-modal question answering, multi-\nmodal fact verification, and image reranking. For\neach task, MLLMs are required to retrieve knowl-\nedge from the multi-modal document collection D\nand generate responses to answer the question q.\nDetails of the prompt templates of different tasks\nare shown in Appendix A.3.\nImage Captioning Task. Image Captioning is a\nwidely used task for evaluating the performance of\nmulti-modal RAG models (Aghajanyan et al., 2022;\nSharifymoghaddam et al., 2024). In this task, an\nimage is provided as the query q, and the document\ncollection D is constructed using image documents\nthat contain captions. The goal of image captioning\nis to generate concise and semantically coherent\ncaptions that accurately describe the image content.\nUnlike previous works (Aghajanyan et al., 2022;\nSharifymoghaddam et al., 2024), we source image\ncaptions from WebQA (Chang et al., 2022), where\nall image documents are collected from Wikimedia\nCommons. These captions often include important\ndetails, such as named entities, which make the\ntask more challenging and provide crucial informa-\ntion for query matching (Liu et al., 2023b). More\ncomparison details of the image captioning tasks in\ndifferent benchmarks are shown in Appendix A.4.\nMulti-Modal Question Answering Task. Follow-\ning the WebQA benchmark (Chang et al., 2022),\nthe Multi-Modal QA task involves answering text-\nbased queries q by leveraging both text and image\ndocuments. The document collection D consists\nof both text and image documents containing cap-\ntions. Additionally, we extend WebQA to an open-\ndomain setting by instructing the retriever to return\nquery-relevant documents from the collection D,\nas demonstrated by Liu et al. (2023b).\nMulti-Modal Fact Verification Task. The Multi-\nModal Fact Verification task challenges MLLMs\nto verify the accuracy of claims using multi-modal\nevidence. In this task, the query q can be a multi-\nmodal claim, and the document collection D con-\nsists of both text and image documents, where the\nimage documents do not contain captions. The rela-\ntionship between the claim and the evidence is cat-\negorized into three possible outcomes: “Support”,\n“Refute”, or “Insufficient”, indicating whether the\nevidence supports, refutes, or lacks sufficient in-\nformation to verify the claim. We build this task\non the Factify dataset (Mishra et al., 2022), but\nwe focus on open-domain fact verification by re-\n\nBenchmarks\nInput Modality\nKnowledge Source\nRetrieval Modality\nMulti Task\nOpen Domain\nMSCOCO (2014)\nImage\nWeb\nFlickr30K (2014)\nImage\nWeb\nK-VQA (2019)\nMulti\nWikipedia\nWebQA (2022)\nText\nWikipedia\nMulti\nMRAG-Bench (2024)\nMulti\nWeb and Other Sources\nImage\nM2RAG (Ours)\nMulti\nWikipedia, Twitter\nMulti\nTable 1: Comparison of Multi-Modal Benchmarks.\ntrieving evidence from a multi-modal document\ncollection (Thorne et al., 2018).\nImage Reranking Task. In the Image Rerank-\ning task, the objective is to identify the most rel-\nevant images based on a given image description.\nHere, the image description serves as the query q,\nand the document collection D consists of image\ndocuments that do not include captions. For each\ndescription, we first use a multi-modal retriever\nto retrieve image candidates based solely on their\nimage features and then rerank the images using\nMLLMs. To adapt MLLMs for this task, we follow\nprior work (Muennighoff, 2022) and compute the\nPerplexity (PPL) score for reranking image candi-\ndates based on their image features. This approach\nmodels the relevance between queries and images\nin a manner similar to image captioning, where\na lower PPL score indicates greater relevance be-\ntween the candidate image and the given query.\nDetails of Data Construction. To build the\nM2RAG benchmark, we collect data from two\nwidely used datasets, WebQA (Chang et al., 2022)\nand Factify (Mishra et al., 2022), constructing\n3,000 instances for both training and evaluation\nprocesses for each task.\nFirst, we select WebQA to build the tasks for im-\nage captioning, multi-modal QA, and image rerank-\ning. For the multi-modal QA task, we select an\nequal number of text-based and image-based QA\npairs from WebQA to construct the dataset. For\nimage captioning and image reranking tasks, we\nrandomly select image-text pairs with a similarity\nscore greater than 0.65, which are then split into\ntraining and evaluation sets. The retrieval corpus\nfor these tasks consists of all image-text pairs ex-\ncept the selected ones. Then, for the multi-modal\nfact verification task, we use the Factify dataset\nand the entire set of image-text documents in the\ndataset is used as the retrieval corpus. Additionally,\nwe consolidate the modality based categories into\nthree: “Support”, “Refute”, and “Insufficient.”\nBenchmark Comparison. The comparison of\ndifferent benchmarks is presented in Table 1.\nExisting multi-modal benchmarks primarily eval-\nuate the effectiveness of MLLMs on single tasks,\nsuch as image captioning (Lin et al., 2014; Young\net al., 2014) or question answering (Chang et al.,\n2022), limiting their ability to conduct compre-\nhensive evaluations of MLLMs in multi-modal\nRAG scenarios. In contrast, M2RAG offers sev-\neral unique features for a more thorough evalu-\nation: 1) M2RAG defines four tasks that assess\nan MLLM’s ability to effectively understand and\nutilize retrieved knowledge. These tasks require\nMLLMs to perform reasoning and information\nmatching based on both queries and contextual\nknowledge. 2) M2RAG incorporates the retrieval\nresults as the multi-modal contexts for model in-\nputs, avoiding the need for separate processing of\nthe retrieval documents of different modalities. 3)\nM2RAG adapts these tasks to an open-domain set-\nting, requiring MLLMs to retrieve knowledge from\na comprehensive multi-modal document collection,\noffering a more realistic RAG scenario.\n4\nInstruction Tuning for Multi-Modal\nRetrieval-Augmented Generation\nIn this section, we present our Multi-Modal\nRetrieval-Augmented Instruction Tuning (MM-\nRAIT) method. First, we describe the framework\nfor multi-modal Retrieval-Augmented Generation\n(RAG) (Sec. 4.1). Then, we introduce multi-task\ninstruction tuning to enhance the performance of\nMLLMs in multi-modal RAG tasks (Sec. 4.2).\n4.1\nThe Framework of Multi-Modal\nRetrieval-Augmented Generation\nGiven a query q, multi-modal RAG models first em-\nploy a retriever to search for query-relevant multi-\nmodal documents D and then feed these documents\nto MLLMs to assist them in answering the query\nq. Each document d ∈D can be either an image\ndocument or a text document. The multi-modal\nRAG framework consists of two main components:\nthe multi-modal retrieval module and the retrieval-\naugmented generation module.\n\nMulti-Modal Retrieval. To retrieve documents\nfrom the multi-modal document collection D, ex-\nisting methods typically rely on multi-modal dense\nretrieval models (Zhou et al., 2024b,a).\nGiven a query q and a multi-modal document\nd, multi-modal dense retrieval models, such as\nVISTA (Zhou et al., 2024a), encode both as repre-\nsentations hq and hd, respectively, and map them\ninto an embedding space for retrieval:\nhq = Enc(q); hd = Enc(d),\n(1)\nwhere Enc denotes the encoder model. The query\nq can be either text or an image, and the multi-\nmodal document d can be a text document or an\nimage document. For documents containing cap-\ntions, both image features and image captions are\nfed into the encoder model.\nNext, we compute the similarity score S(q, d)\nbetween the representations hq and hd of the query\nand document:\nS(q, d) = Sim(hq, hd),\n(2)\nwhere Sim denotes cosine similarity. We then per-\nform a KNN search (Johnson et al., 2019) to re-\ntrieve the top-k most relevant multi-modal doc-\numents ˜D = {d1, ..., dk} to the query q. Dur-\ning retrieval, the multi-modal retriever needs to\nconduct single-modality matching, cross-modality\nmatching and modality routing in the embedding\nspace (Liu et al., 2023b).\nMulti-Modal RAG Module. After retrieval,\nwe input the retrieved documents ˜D and query q\ninto the MLLM (M), such as MiniCPM-V (Yao\net al., 2024) or Qwen2-VL (Wang et al., 2024), to\ngenerate the output y:\ny = M( ˜D, q).\n(3)\nThese retrieved documents provide external knowl-\nedge, which helps to update the parametric memory\nof the MLLM, enabling it to generate more accu-\nrate responses to the query q.\n4.2\nMM-RAIT: Multi-Task Multi-Modal\nInstruction Tuning for MLLMs\nTo adapt MLLMs to the multi-modal RAG scenario,\nwe propose the Multi-Modal Retrieval-Augmented\nInstruction Tuning (MM-RAIT) method, designed\nto further enhance the performance of MLLMs\nacross various RAG tasks.\nTo improve the MLLM generation process, we\nincorporate external knowledge to assist in answer-\ning the query (Eq.3). Specifically, we follow pre-\nvious work (Ram et al., 2023) and concatenate the\nrepresentations of the retrieved documents ˜D along\nwith the query q as the input for the MLLM (M)\nto generate the output y:\ny = M(Instructp, X( ˜D), q),\n(4)\nwhere Instructp is the instruction for the task p, and\nX( ˜D) denotes the concatenation of the representa-\ntions of the retrieved documents:\nX( ˜D) = X(d1) ⊕· · · ⊕X(dk).\n(5)\nFor the i-th retrieved document di, its representa-\ntion can be the text sequence for a text document,\nthe image features for an image document, or the\nconcatenation of both image features and caption\nfor an image document that contains a caption.\nNext, we gather queries from three tasks to form\nthe query set Q: image captioning, multi-modal\nquestion answering, and multi-modal fact verifica-\ntion. For each query q in these tasks, the training\nobjective for the model is to minimize the negative\nlog-likelihood of generating the target sequence y∗:\nL = −\nX\nq∈Q\nT\nX\nt=1\nlog P(y∗\nt | y∗\n<t, ˜D, q; θ),\n(6)\nwhere T is the length of the ground truth response,\ny∗\nt is the t-th token of the ground truth response,\nand θ represents the parameters of MLLM (M).\n5\nExperimental Methodology\nThis section outlines the datasets, evaluation met-\nrics, baselines, and implementation details used in\nour experiments.\nDataset. We use the M2RAG dataset to eval-\nuate the performance of different MLLMs in the\nmulti-modal RAG scenario. The dataset consists of\nfour tasks: image captioning, multi-modal question\nanswering, multi-modal fact verification, and im-\nage reranking. For multi-modal retrieval, we adopt\nVISTA (Zhou et al., 2024a), a universal embed-\nding model to search for query-related documents.\nVISTA integrates image token embeddings into the\nBGE Text Embedding (Xiao et al., 2024) frame-\nwork, enabling flexible processing of the inputs of\nboth text and image data.\nEvaluation Metrics.\nFor image captioning\nand multi-modal QA tasks, we use BLEU (Pa-\npineni et al., 2002), ROUGE (Lin, 2004), and\nCIDEr (Vedantam et al., 2015) scores to assess\nperformance. In the multi-modal fact verification\ntask, we evaluate the performance of different RAG\n\nModel\nImage Captioning\nMulti-Modal QA\nMM Fact Verification\nImage Reranking\nBLEU-4\nROUGE-L\nCIDEr\nBLEU-4\nROUGE-L\nCIDEr\nACC\nF1\nFID↓\nMiniCPM-V 2.6 (8B)\nVanilla RAG\n1.91\n17.58\n18.39\n13.84\n32.78\n82.65\n43.03\n41.13\n-\nw/ top1\n3.82\n24.28\n43.76\n17.18\n37.89\n119.92\n54.83\n53.49\n12.17\nw/ top3\n3.46\n23.13\n37.89\n17.56\n38.46\n124.16\n56.33\n54.01\n10.77\nw/ top5\n3.29\n22.82\n36.09\n17.15\n37.99\n114.21\n55.33\n52.69\n11.15\nMM-RAIT\n6.25\n32.77\n77.08\n26.37\n53.21\n266.47\n60.17\n60.22\n10.32\nQwen2-VL (7B)\nVanilla RAG\n2.24\n19.48\n26.01\n18.77\n39.05\n153.40\n45.43\n34.23\n-\nw/ top1\n3.79\n25.43\n46.32\n21.21\n42.14\n187.99\n51.60\n41.05\n12.17\nw/ top3\n3.62\n25.70\n45.08\n20.98\n41.90\n178.28\n52.43\n41.94\n9.88\nw/ top5\n3.62\n25.31\n44.45\n21.26\n42.41\n181.56\n52.00\n41.64\n9.71\nMM-RAIT\n10.53\n39.79\n123.97\n32.00\n62.49\n329.05\n65.13\n62.97\n9.15\nTable 2: Overall Performance. We evaluate the performance of different RAG models implemented with MiniCPM-\nV 2.6 and Qwen2-VL on our M2RAG benchmark. MM-RAIT uses the top-5 multi-modal documents for inference.\nmodels using accuracy (ACC) and F1 score. For\nthe image reranking task, we use the Fréchet Incep-\ntion Distance (FID↓) (Heusel et al., 2017)1.\nBaselines. We compare our models with two\nmulti-modal baselines: MiniCPM-V 2.6 (Yao et al.,\n2024) and Qwen2-VL (Wang et al., 2024). These\nmodels are evaluated in a zero-shot setting to as-\nsess their effectiveness in leveraging multi-modal\nknowledge from the input context. We feed the top-\n1, top-3, and top-5 ranked documents into these\nMLLMs to evaluate their RAG performance.\nImplementation Details. We apply Low-Rank\nAdaptation (LoRA) (Hu et al., 2022) to fine-tune\nboth MiniCPM-V 2.6 and Qwen2-VL using the top-\n5 retrieved multi-modal documents for 2 epochs.\nThe batch size is 4, with a maximum token limit\nof 4,096. A cosine learning rate scheduler is used,\nwith the learning rate set to 1e −6 for MiniCPM-V\nand 1e−4 for Qwen2-VL. We fine-tune Qwen2-VL\nusing LLaMA-Factory (Zheng et al., 2024) and set\nmax_pixels=512 × 512 for training and inference.\n6\nEvaluation Result\nIn this section, we first evaluate the performance\nof MLLMs on the M2RAG benchmark. We then\nconduct ablation studies to assess the impact of\nvarying numbers of retrieved documents of differ-\nent modalities. Following that, we analyze the role\nof different retrieval modalities in RAG models.\nFinally, case studies are shown.\n6.1\nOverall Performance\nAs shown in Table 2, we report the performance of\nvarious RAG models on the M2RAG benchmark.\nThe vanilla RAG models directly use retrieved doc-\numents to augment LLMs, while MM-RAIT mod-\nels fine-tune MLLMs within the RAG framework.\n1https://github.com/mseitzer/pytorch-fid\nFor these vanilla RAG models, performance gen-\nerally improves as the number of retrieved doc-\numents increases. However, when retrieving the\ntop-5 ranked documents, the overall performance\nof vanilla RAG models on most tasks is lower com-\npared to using top-1 or top-3 documents. This sug-\ngests that vanilla LLMs still struggle to fully lever-\nage multi-modal knowledge to enhance MLLMs.\nAlthough some related works also use image cap-\ntioning tasks to evaluate RAG performance (Shar-\nifymoghaddam et al., 2024), the performance of\nthese MLLMs on M2RAG is considerably worse,\nindicating that M2RAG offers a more challenging\ndataset for image captioning. In contrast to vanilla\nRAG models, both MiniCPM-V 2.6 and Qwen2-\nVL demonstrate strong performance across all tasks\non the M2RAG benchmark after training with MM-\nRAIT. Specifically, MiniCPM-V 2.6 achieves an av-\nerage improvement of over 27% across all tasks in\nM2RAG, while Qwen2-VL shows an even greater\nimprovement of 34%. These results highlight the\neffectiveness of MM-RAIT, showcasing its ability\nto help MLLMs better utilize multi-modal contexts\nto enhance their performance.\n6.2\nAblation Study\nAs shown in Table 3, we conduct ablation stud-\nies to evaluate RAG effectiveness with retrieved\ndocuments of different modalities and numbers.\nSpecifically, we conduct two evaluation settings to\nevaluate the roles of different modalities: Only Text\nand Only Image. Only Text indicates removing all\nimage features from multi-modal input contexts to\nenhance the MLLM, while Only Image removes\nall texts from top-ranked multi-modal documents.\nCompared with the RAG models using top-3\nranked multi-modal documents for augmentation,\nthe performance of vanilla RAG models usually de-\n\nModel\n#Doc\nImage Captioning\nMulti-Modal QA\nMM Fact Verification\nOnly Text\nOnly Image\nMulti\nOnly Text\nOnly Image\nMulti\nOnly Text\nOnly Image\nMulti\nMiniCPM-V 2.6 (8B)\nMLLM\n0\n17.58\n17.58\n17.58\n32.78\n32.78\n32.78\n41.13\n41.13\n41.13\nVanilla RAG\n3\n24.20\n17.15\n23.13\n38.25\n37.57\n38.46\n53.21\n43.61\n54.01\n5\n24.95\n17.33\n22.82\n37.84\n37.21\n37.99\n53.28\n42.46\n52.69\nMM-RAIT\n3\n32.46\n25.09\n32.88\n50.83\n47.73\n52.69\n60.52\n45.66\n60.41\n5\n33.04\n25.38\n32.77\n50.32\n47.12\n53.21\n60.88\n47.71\n60.22\nQwen2-VL (7B)\nMLLM\n0\n19.48\n19.48\n19.48\n39.05\n39.05\n39.05\n34.23\n34.23\n34.23\nVanilla RAG\n3\n26.87\n19.12\n25.70\n41.02\n43.10\n41.90\n43.16\n33.56\n41.94\n5\n26.89\n17.33\n25.31\n41.10\n43.32\n42.41\n43.24\n33.50\n41.62\nMM-RAIT\n3\n37.67\n35.32\n39.11\n61.35\n53.67\n62.04\n61.82\n57.90\n63.36\n5\n38.39\n35.27\n39.79\n61.95\n53.97\n62.49\n61.40\n57.41\n62.97\nTable 3: Ablation Study. We evaluate the performance of different retrieval modalities for candidate corpora on\nM2RAG benchmark. For Image Captioning and Multi-Modal QA, we use ROUGE-L as the evaluation metric. And\nF1-score is used for the MM Fact Verification task.\nText\nImage\nMulti\n0\n20\n40\n60\n80\n100\nROUGE-L\n40.43\n19.62\n38.05\n20.00\n14.44\n19.79\nVanilla LLM\nMM-RAIT\n(a) MiniCPM Performance on\nText Answerable Queries.\nText\nImage\nMulti\n0\n20\n40\n60\n80\n100\nROUGE-L\n50.46\n31.83\n50.09\n19.83\n17.74\n19.61\n(b) Qwen2 Performance on\nText Answerable Queries.\nText\nImage\nMulti\n0\n20\n40\n60\n80\n100\nROUGE-L\n58.76\n72.19\n68.38\n54.19\n54.82\n56.19\n(c) MiniCPM Performance on\nImage Answerable Queries.\nText\nImage\nMulti\n0\n20\n40\n60\n80\n100\nROUGE-L\n73.51\n76.11\n74.90\n62.24\n70.21\n65.21\n(d) Qwen2 Performance on\nImage Answerable Queries.\nFigure 3: RAG Performance in Multi-Modal QA Task\nUsing Retrieved Documents of Different Modalities.\nText, Image, and Multi denote that retrieved text, image,\nand multi-modal documents are fed to different RAG\nmodels for evaluation.\ncreases with top-5 ranked documents, while MM-\nRAIT alleviates the performance decreases but\nalso shows limited improvements. It illustrates\nthat effectively using the multi-modal contextual\nknowledge is still challenging for existing MLLMs.\nMoreover, we further remove all texts or image\nfeatures to show the roles of different modalities\nin RAG modeling. For all tasks, the RAG perfor-\nmance of the Only Text model slightly decreases,\nshowing that these texts contribute to the primary\nknowledge source for these RAG models. After\nadding the image features, the RAG performance\nusually increases, showing that these image fea-\ntures can improve the performance of RAG models.\nEven though different modalities show the effec-\ntiveness in multi-modal RAG modeling, it is still\nhard to effectively learn more crucial semantics\nfrom these image features to improve the RAG\nperformance within multi-modal contexts.\n6.3\nEffectiveness of MLLMs in Different\nModality-based RAG Scenarios\nIn this experiment, we investigate the impact of\nretrieved documents from different modalities on\nthe effectiveness of RAG models.\nAs shown in Figure 3, we divide the multi-modal\nQA dataset of M2RAG into two groups: image-\nanswerable queries and text-answerable queries.\nThese categories represent queries that can be an-\nswered by image or text documents, respectively.\nWe compare both vanilla RAG and MM-RAIT, im-\nplemented using MiniCPM-V and Qwen2-VL. Top-\n5 ranked documents from texts, images, and both\nmodalities are fed to the different RAG models to\nevaluate the QA performance.\nFigures 3(a) and 3(b) present the RAG perfor-\nmance on text-answerable queries. Overall, the\nRAG models using multi-modal retrieved docu-\nments exhibit comparable performance to those\nusing only text-based documents, indicating that\nMLLMs can effectively learn from text documents\nto answer queries. Notably, vanilla RAG models\nshow minimal differences in performance when\nusing text, image, or both types of documents,\nwhereas MM-RAIT significantly improves perfor-\nmance when leveraging documents from multiple\nmodalities. This highlights the effectiveness of\nMM-RAIT in enabling MLLMs to learn from multi-\nmodal contexts. Interestingly, vanilla MLLMs ap-\npear insensitive to the retrieved contexts, likely\nbecause they rely heavily on internal knowledge\n\nRetrieved top 5 Documents:\nQuestion: What animal is included in the painting of John Campbell, 1st Baron Cawdor?\nGround Truth Answer:  A dog is included in the painting of John Campbell, 1st Baron Cawdor.\nVanilla Model:  The painting of John Campbell, 1st Baron Cawdor includes a lion.\nVanilla RAG:  The painting of John Campbell, 1st Baron Cawdor, includes a dog.\nJohn Campbell …\n2\nJohn Campbell, 1st \nBaron Cawdor, FRS \nFSA (ca. 1753, 1 June \n1821), was a Welsh \nart-collector and \npolitician who sat …\n1\nA portrait of John \nCampbell was made \nby Joshua Reynolds \n(1778; now in Cawdor \nCastle, Nairn); a \nminiature of him by …\n3\nOudry also painted \na second painting of \nMisse with a different, \nnon-sighthound dog. \nThere is a 1758 \npainting by …\n4\n    There is a 1758 \npainting by Pompeo \nBatoni entitled \nPortrait of Charles \nCompton, 7th Earl of \nNorthampton which …\n5\nRetrieved top 5 Documents:\nQuestion: What weapon is the man in Daniel Maclis's  \"A Scene from 'Undine' (detail)\" holding?\nGround Truth Answer:  The man is holding a sword.\nVanilla RAG:  The man in Daniel Maclis's \"A Scene from 'Undine' (detail)\" is holding a dirk, which is a type of dagger.\nMM-RAIT:  The man in Daniel Maclis's \"A Scene from 'Undine' (detail)\" is holding a sword.\nDaniel Maclise - A …\n1\nIn Leonardo‘s absence, \nhe has spent his nights as \nan armored vigilante \nknown as The \nNightwatcher. His time as \nthe Nightwatcher is one …\n2\nThe hilt (rarely \ncalled a haft or \nshaft) of a knife, \ndagger, sword, \nor bayonet is its \nhandle …\n3\nconsisting of a \nguard, grip and \npommel. The \nguard may contain \na crossguard or \nquillons. …\n4\nScottish dirk, blade …\n5\nFigure 4: Case Studies. We highlight the relevant phrases, correct answers, and query-unrelated phrases.\nwhen processing text-answerable queries.\nNext, we evaluate the RAG performance on\nimage-answerable queries, shown in Figures 3(c)\nand 3(d). The results indicate that RAG models us-\ning multi-modal documents generally outperform\nthose using only text documents, confirming that\nincorporating image documents during retrieval en-\nhances the ability of MLLMs to answer questions.\nThe performance gap narrows for Qwen2-VL, sug-\ngesting that different MLLMs exhibit varying lev-\nels of reliance on multi-modal documents.\n6.4\nCase Study\nIn this section, we show two cases from Qwen2-VL\nin the Multi-Modal QA task of M2RAG to evaluate\nthe effectiveness of the MM-RAIT method within\nthe multi-modal retrieval contexts. More cases are\nshown in Appendix A.5.\nAs illustrated in Figure 4, in the first case, the\nquestion asks, “What animal is included in the\npainting of John Campbell, 1st Baron Cawdor?”.\nThis requires the MLLM to match the “1st Baron\nCawdor” and extract information about animals\nin the painting. Due to limited internal knowl-\nedge, the model encounters hallucination issues\nand generates an incorrect answer, “a lion”. When\nthe retrieved multi-modal document of “1st Baron\nCawdor” is fed into the MLLM, the vanilla RAG\nmodel can directly extract “dog” from the painting,\nthus providing the correct response. This highlights\nthe importance of multi-modal information in offer-\ning more intuitive and richer semantic insights to\nanswer the question, underscoring the effectiveness\nof constructing the M2RAG benchmark.\nIn the second case, the question asks that, “What\nweapon is the man in Daniel Maclis’s A Scene from\n‘Undine’ (detail) holding?” Based on retrieved doc-\numents, the vanilla RAG model focuses on the fifth\ndocument, which depicts a “Scottish dirk”. This\nleads the vanilla RAG model to generate an incor-\nrect response, “holding a dirk”. After MM-RAIT\ntraining, the model can accurately identify the rele-\nvant document describing the man holding a sword\nand extract pertinent information from it, thereby\ngenerating the correct response.\n7\nConclusion\nThis paper introduces Multi-Modal Retrieval-\nAugmented Generation (M2RAG), a benchmark\ndesigned to evaluate MLLMs with retrieved multi-\nmodal contexts across four tasks. To further en-\nhance the utilization of retrieved information, we\nalso propose a Multi-Modal Retrieval-Augmented\nInstruction Tuning (MM-RAIT) method, which\noptimizes MLLMs with multi-modal contexts as\ninputs, thereby improving their ability to effectively\nutilize retrieved information.\n\nLimitations\nAlthough our M2RAG benchmark includes four\ncommon multi-modal tasks, incorporating addi-\ntional tasks can provide a more comprehensive\nevaluation of the capabilities of MLLMs. Further-\nmore, while MLLMs perform satisfactorily within\nretrieved multi-modal contexts, they still rely pre-\ndominantly on textual data for some tasks. Find-\ning ways to enable MLLMs to more effectively\nleverage multi-modal contexts remains a critical\nchallenge that requires further exploration. Ad-\nditionally, due to the performance limitations of\nmulti-modal retrieval models, the quality of the\nretrieved multi-modal documents directly impacts\nthe overall performance of MLLMs. Improving the\naccuracy of multi-modal retrieval remains a vital\narea for future research.\nReferences\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, et al. 2023. Gpt-4 technical report.\nArXiv preprint.\nArmen Aghajanyan, Bernie Huang, Candace Ross,\nVladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro\nOkhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis,\net al. 2022.\nCm3: A causal masked multimodal\nmodel of the internet. ArXiv preprint.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,\nAntoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm\nReynolds, Roman Ring, Eliza Rutherford, Serkan\nCabi, Tengda Han, Zhitao Gong, Sina Samangooei,\nMarianne Monteiro, Jacob L. Menick, Sebastian\nBorgeaud, Andy Brock, Aida Nematzadeh, Sahand\nSharifzadeh, Mikolaj Binkowski, Ricardo Barreira,\nOriol Vinyals, Andrew Zisserman, and Karén Si-\nmonyan. 2022. Flamingo: a visual language model\nfor few-shot learning. In Proceedings of NeurIPS.\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and\nHannaneh Hajishirzi. 2024a. Self-rag: Learning to\nretrieve, generate, and critique through self-reflection.\nIn Proceedings of ICLR.\nAkari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh,\nLuke Zettlemoyer, Hannaneh Hajishirzi, and Wen-\ntau Yih. 2024b. Reliable, adaptable, and attributable\nlanguage models with retrieval. ArXiv preprint.\nYingshan Chang, Guihong Cao, Mridu Narang, Jianfeng\nGao, Hisami Suzuki, and Yonatan Bisk. 2022. We-\nbqa: Multihop and multimodal QA. In Proceedings\nof CVPR, pages 16474–16483.\nLuyu Gao and Jamie Callan. 2022. Unsupervised cor-\npus aware language model pre-training for dense pas-\nsage retrieval. In Proceedings of ACL, pages 2843–\n2853.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. 2017. Gans\ntrained by a two time-scale update rule converge to a\nlocal nash equilibrium. In Proceedings of NeurIPS,\npages 6626–6637.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2022. Lora: Low-rank adaptation of\nlarge language models. In Proceedings of ICLR.\nWenbo Hu, Jia-Chen Gu, Zi-Yi Dou, Mohsen Fayyaz,\nPan Lu, Kai-Wei Chang, and Nanyun Peng. 2024.\nMrag-bench: Vision-centric evaluation for retrieval-\naugmented multimodal models. ArXiv preprint.\nLei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,\nZhangyin Feng, Haotian Wang, Qianglong Chen,\nWeihua Peng, Xiaocheng Feng, Bing Qin, et al. 2023.\nA survey on hallucination in large language models:\nPrinciples, taxonomy, challenges, and open questions.\nArXiv preprint.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput-\ning Surveys, (12):1–38.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with gpus.\nIEEE\nTransactions on Big Data, (3):535–547.\nJustin Johnson, Bharath Hariharan, Laurens van der\nMaaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B.\nGirshick. 2017. CLEVR: A diagnostic dataset for\ncompositional language and elementary visual rea-\nsoning. In 2017 IEEE Conference on Computer Vi-\nsion and Pattern Recognition, CVPR 2017, Honolulu,\nHI, USA, July 21-26, 2017, pages 1988–1997.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020.\nDense passage retrieval for\nopen-domain question answering. In Proceedings\nof EMNLP, pages 6769–6781.\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\ntus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,\nTim Rocktäschel, Sebastian Riedel, and Douwe\nKiela. 2020.\nRetrieval-augmented generation for\nknowledge-intensive NLP tasks. In Proceedings of\nNeurIPS.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven C. H.\nHoi. 2023. BLIP-2: bootstrapping language-image\npre-training with frozen image encoders and large\nlanguage models. In Proceedings of ICML, pages\n19730–19742.\n\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven\nC. H. Hoi. 2022.\nBLIP: bootstrapping language-\nimage pre-training for unified vision-language un-\nderstanding and generation. In Proceedings of ICML,\npages 12888–12900.\nChin-Yew Lin. 2004. ROUGE: A package for automatic\nevaluation of summaries.\nIn Text Summarization\nBranches Out, pages 74–81.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C Lawrence Zitnick. 2014.\nMicrosoft coco:\nCommon objects in context.\nIn Proceedings of\nECCV, pages 740–755. Springer.\nXi Victoria Lin, Xilun Chen, Mingda Chen, Weijia\nShi, Maria Lomeli, Richard James, Pedro Rodriguez,\nJacob Kahn, Gergely Szilvasy, Mike Lewis, Luke\nZettlemoyer, and Wen-tau Yih. 2024.\nRA-DIT:\nretrieval-augmented dual instruction tuning. In Pro-\nceedings of ICLR.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023a. Visual instruction tuning. In Proceed-\nings of NeurIPS.\nZhenghao Liu, Chenyan Xiong, Yuanhuiyi Lv, Zhiyuan\nLiu, and Ge Yu. 2023b. Universal vision-language\ndense retrieval: Learning A unified representation\nspace for multi-modal retrieval. In Proceedings of\nICLR.\nHaoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai\nDong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhu-\noshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng,\nHanwei Xu, Zhenda Xie, and Chong Ruan. 2024.\nDeepseek-vl: Towards real-world vision-language\nunderstanding.\nKenneth Marino, Mohammad Rastegari, Ali Farhadi,\nand Roozbeh Mottaghi. 2019. OK-VQA: A visual\nquestion answering benchmark requiring external\nknowledge. In Proceedings of CVPR, pages 3195–\n3204.\nShreyash Mishra, S Suryavardan, Amrit Bhaskar, Parul\nChopra, Aishwarya N Reganti, Parth Patwa, Amitava\nDas, Tanmoy Chakraborty, Amit P Sheth, Asif Ekbal,\net al. 2022. Factify: A multi-modal fact verification\ndataset. In DE-FACTIFY@ AAAI.\nNiklas Muennighoff. 2022. Sgpt: Gpt sentence embed-\ndings for semantic search. ArXiv preprint.\nR OpenAI. 2023. Gpt-4 technical report. arXiv, pages\n2303–08774.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of ACL,\npages 311–318.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\nVassilis Plachouras, Tim Rocktäschel, and Sebastian\nRiedel. 2021. KILT: a benchmark for knowledge\nintensive language tasks. In Proceedings of NAACL-\nHLT, pages 2523–2544.\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\nShoham. 2023. In-context retrieval-augmented lan-\nguage models. Proceedings of TACL, pages 1316–\n1331.\nRuiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao,\nQiaoQiao She, Hua Wu, Haifeng Wang, and Ji-Rong\nWen. 2021. RocketQAv2: A joint training method\nfor dense passage retrieval and passage re-ranking.\nIn Proceedings of EMNLP, pages 2825–2835.\nStephen Robertson, Hugo Zaragoza, et al. 2009. The\nprobabilistic relevance framework: Bm25 and be-\nyond. Foundations and Trends® in Information Re-\ntrieval, (4):333–389.\nChristoph Schuhmann, Richard Vencu, Romain Beau-\nmont, Robert Kaczmarczyk, Clayton Mullis, Aarush\nKatta, Theo Coombes, Jenia Jitsev, and Aran Komat-\nsuzaki. 2021. Laion-400m: Open dataset of clip-\nfiltered 400 million image-text pairs. ArXiv preprint.\nSanket Shah, Anand Mishra, Naganand Yadati, and\nPartha Pratim Talukdar. 2019. KVQA: knowledge-\naware visual question answering. In Proceedings of\nAAAI, pages 8876–8884.\nSahel Sharifymoghaddam, Shivani Upadhyay, Wenhu\nChen, and Jimmy Lin. 2024. Unirag: Universal re-\ntrieval augmentation for multi-modal large language\nmodels. ArXiv preprint.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Min-\njoon Seo, Richard James, Mike Lewis, Luke Zettle-\nmoyer, and Wen-tau Yih. 2024. REPLUG: Retrieval-\naugmented black-box language models. In Proceed-\nings of NAACL-HLT, pages 8371–8384.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation re-\nduces hallucination in conversation. In Proceedings\nof EMNLP Findings, pages 3784–3803.\nQuan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang,\nQiying Yu, Yueze Wang, Yongming Rao, Jingjing\nLiu, Tiejun Huang, and Xinlong Wang. 2024a. Gen-\nerative multimodal models are in-context learners. In\nProceedings of CVPR, pages 14398–14409.\nQuan Sun, Qiying Yu, Yufeng Cui, Fan Zhang,\nXiaosong Zhang, Yueze Wang, Hongcheng Gao,\nJingjing Liu, Tiejun Huang, and Xinlong Wang.\n2024b. Emu: Generative pretraining in multimodal-\nity. In Proceedings of ICLR.\nSahar Tahmasebi, Eric Müller-Budack, and Ralph Ew-\nerth. 2024. Multimodal misinformation detection\nusing large vision-language models. In Proceedings\nof CIKM, pages 2189–2199.\n\nGemini Team, Rohan Anil, Sebastian Borgeaud, Jean-\nBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan\nSchalkwyk, Andrew M Dai, Anja Hauth, Katie Milli-\ncan, et al. 2023. Gemini: a family of highly capable\nmultimodal models. ArXiv preprint.\nJames\nThorne,\nAndreas\nVlachos,\nChristos\nChristodoulopoulos,\nand\nArpit\nMittal.\n2018.\nFEVER: a large-scale dataset for fact extraction and\nVERification. In Proceedings of NAACL-HLT, pages\n809–819.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023. Llama: Open and efficient foun-\ndation language models. ArXiv preprint.\nRamakrishna Vedantam, C. Lawrence Zitnick, and Devi\nParikh. 2015. Cider: Consensus-based image de-\nscription evaluation. In Proceedings of CVPR, pages\n4566–4575.\nPeng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhi-\nhao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin\nWang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei\nDu, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang\nZhou, Jingren Zhou, and Junyang Lin. 2024. Qwen2-\nvl: Enhancing vision-language model’s perception of\nthe world at any resolution. ArXiv preprint.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, et al.\n2022. Emergent abilities of large language models.\nTransactions on Machine Learning Research.\nShitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muen-\nnighoff, Defu Lian, and Jian-Yun Nie. 2024. C-pack:\nPacked resources for general chinese embeddings. In\nProceedings of SIGIR, pages 641–649.\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\nJialin Liu, Paul N. Bennett, Junaid Ahmed, and\nArnold Overwijk. 2021a. Approximate nearest neigh-\nbor negative contrastive learning for dense text re-\ntrieval. In Proceedings of ICLR.\nWenhan Xiong, Xiang Lorraine Li, Srini Iyer, Jingfei\nDu, Patrick S. H. Lewis, William Yang Wang, Yashar\nMehdad, Scott Yih, Sebastian Riedel, Douwe Kiela,\nand Barlas Oguz. 2021b. Answering complex open-\ndomain questions with multi-hop dense retrieval. In\nProceedings of ICLR.\nShi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling.\n2024.\nCorrective retrieval augmented generation.\nArXiv preprint.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik R. Narasimhan, and Yuan Cao. 2023.\nReact: Synergizing reasoning and acting in language\nmodels. In Proceedings of ICLR.\nYuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo\nCui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao,\nZhihui He, et al. 2024. Minicpm-v: A gpt-4v level\nmllm on your phone. ArXiv preprint.\nMichihiro Yasunaga, Armen Aghajanyan, Weijia Shi,\nRichard James, Jure Leskovec, Percy Liang, Mike\nLewis, Luke Zettlemoyer, and Wen-Tau Yih. 2023.\nRetrieval-augmented multimodal language modeling.\nIn Proceedings of ICML, pages 39755–39769.\nPeter Young, Alice Lai, Micah Hodosh, and Julia Hock-\nenmaier. 2014. From image descriptions to visual\ndenotations: New similarity metrics for semantic\ninference over event descriptions. Proceedings of\nTACL, pages 67–78.\nLili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin\nMuller, Olga Golovneva, Tianlu Wang, Arun Babu,\nBinh Tang, Brian Karrer, Shelly Sheynin, et al. 2023a.\nScaling autoregressive multi-modal models: Pretrain-\ning and instruction tuning. ArXiv preprint.\nZichun Yu, Chenyan Xiong, Shi Yu, and Zhiyuan Liu.\n2023b. Augmentation-adapted retriever improves\ngeneralization of language models as generic plug-in.\nIn Proceedings of ACL, pages 2421–2436.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, et al. 2023. A\nsurvey of large language models. ArXiv preprint.\nYaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan\nYe, and Zheyan Luo. 2024. LlamaFactory: Unified\nefficient fine-tuning of 100+ language models. In\nProceedings of ACL, pages 400–410.\nJunjie Zhou, Zheng Liu, Shitao Xiao, Bo Zhao, and\nYongping Xiong. 2024a. VISTA: Visualized text\nembedding for universal multi-modal retrieval. In\nProceedings of ACL, pages 3185–3200.\nTianshuo Zhou, Sen Mei, Xinze Li, Zhenghao Liu,\nChenyan Xiong, Zhiyuan Liu, Yu Gu, and Ge Yu.\n2024b. MARVEL: Unlocking the multi-modal capa-\nbility of dense retrieval via visual module plugin. In\nProceedings of ACL, pages 14608–14624.\n\nTask\nSource\n#Docs\n#Query\nImage\nText\nTrain\nTest\nImg Cap.\nWebQA\n383,750\n-\n3,000\n3,000\nMM QA.\nWebQA\n389,750\n787,697\n3,000\n3,000\nMM FV.\nFactify\n41,000\n41,000\n3,000\n3,000\nImg Rerank.\nWebQA\n389,750\n-\n-\n3,000\nTable 4: Data Statistics of M2RAG.\nA\nAppendix\nA.1\nLicense\nWe show the licenses of the datasets that we use.\nWebQA uses CC0-1.0 license, while Factify uses\nMIT license. All these licenses and agreements\npermit the use of their data for academic purposes.\nA.2\nMore Details of M2RAG Benchmark\nIn this section, we provide additional details regard-\ning the data used in the M2RAG benchmark. The\ndata statistics are shown in Table 4.\nThe M2RAG benchmark incorporates tasks,\nsuch as image captioning, multi-modal question\nanswering, and image reranking, all of which are\nbuilt upon the WebQA (Chang et al., 2022) bench-\nmark. For both the multi-modal question answering\nand image reranking tasks, the same multi-modal\nretrieval corpus is used. For the image captioning\ntask, we filter out any image documents that are\nselected for the construction of both the training\nand evaluation sets. The image-caption pairs used\nin the image reranking test set are the same as those\nin the image captioning test set.\nFor the multi-modal fact verification task, since\nthe test set labels are not publicly available in the\nFactify (Mishra et al., 2022) dataset, we follow the\napproach of Tahmasebi et al. (2024) and sample\ndata from the validation set to create the evalua-\ntion set. All text and image documents from the\ntraining and validation sets of the Factify dataset\nare then collected to construct the retrieval corpus.\nThe original Factify dataset consists of five cat-\negories: “Support_Text”, “Support_Multimodal”,\n“Insufficient_Text”, “Insufficient_Multimodal”, and\n“Refute”.When constructing the training and eval-\nuation datasets for M2RAG, we select an equal\nnumber of samples from each of these five cat-\negories. Since our RAG scenario involves both\ntext and image information, we consolidate these\nmodality-based categories into three: “Support”,\n“Refute”, and “Insufficient”, in order to better eval-\nuate the effectiveness of LLMs in the multi-modal\nfact verification task.\nBenchmark\nBLEU-2\nBLEU-4\nROUGE-L\nCIDEr\nMiniCPM-V 2.6 (8B)\nMSCOCO\n11.53\n4.19\n30.68\n33.48\nM2RAG\n4.62\n1.91\n17.58\n18.39\nQwen2-VL (7B)\nMSCOCO\n16.93\n6.75\n37.51\n70.75\nM2RAG\n5.08\n2.24\n19.48\n26.01\nTable 5: Performance of Different MLLMs in the Image\nCaptioning Tasks of MSCOCO and M2RAG.\nFigure 5:\nLength Distribution of Captions in the\nMSCOCO and M2RAG Benchmarks.\nA.3\nPrompt Templates Used in M2RAG\nIn Figure 6, we present the prompt templates de-\nsigned for various task scenarios in M2RAG. Addi-\ntionally, we describe the input format for the image\nreranking task. In terms of image placement, we\nuse the placeholder {image} for the image, follow-\ning the method proposed by Hu et al. (2024).\nA.4\nComparison of Different Image\nCaptioning Tasks\nIn this section, we compare the performance of\nMiniCPM-V 2.6 and Qwen2-VL on the image cap-\ntioning task using the MSCOCO (Lin et al., 2014)\nand M2RAG datasets. For the MSCOCO dataset,\nwe use the version employed in the image caption-\ning task of UniRAG (Sharifymoghaddam et al.,\n2024) and follow the same processing method de-\nscribed in their paper.\nAs shown in Table 5, both MiniCPM-V 2.6 and\nQwen2-VL exhibit lower performance on M2RAG\ncompared to MSCOCO, with average declines of\nover 9% and 19%, respectively. This suggests\nthat the image captioning task in M2RAG is more\nchallenging for multi-modal large language models\n(MLLMs) than that in MSCOCO. As shown in Fig-\nure 5, the length of image captions in M2RAG is\nnot fixed and varies depending on the scene, with di-\nverse entities and detailed scene descriptions. This\nindicates the complexity of M2RAG that requires\nMLLMs to leverage external knowledge to generate\n\naccurate captions. In contrast, the image captions\nin MSCOCO are more straightforward and regular,\nenabling MLLMs to perform better by relying pri-\nmarily on internal knowledge. These observations\nfurther underscore the need for M2RAG to serve\nas a more challenging benchmark for multi-modal\nRAG tasks.\nA.5\nAdditional Case Studies\nAs shown in Figure 7, we present additional case\nstudies from various tasks to assess the effective-\nness of vanilla RAG MM-RAIT models in utilizing\nmulti-modal context. In the RAG setting, we use\nthe top-5 retrieved multi-modal documents for in-\nference.\nIn the image captioning task, MLLMs initially\nprovide generic descriptions based on internal\nknowledge. However, when multi-modal context\nis incorporated, they are able to extract richer and\nmore specific information, such as identifying land-\nmarks like the “Library of Congress”. After MM-\nRAIT training, both MiniCPM-V 2.6 and Qwen2-\nVL generate more accurate captions. A similar\nimprovement is observed in the image reranking\ntask, where vanilla MLLMs initially struggle to\nalign the semantics of the image and caption. Af-\nter MM-RAIT training, fine-grained alignments\nbetween images and captions are achieved, allow-\ning Qwen2-VL to rank the image of “Bellagio and\nCaesars Palace from the left side of Bellagio’s foun-\ntains” first, even though the reranking task is not\ninvolved during training.\nIn multi-modal question answering, the retrieved\nimage, along with the caption “1929 Cadillac\nSports Phaeton”, aids Qwen2-VL in producing ac-\ncurate answers within the RAG framework. How-\never, MiniCPM-V initially struggles to provide a\ncorrect answer. After MM-RAIT training, both\nmodels consistently provide stable and accurate an-\nswers. Similarly, in multi-modal fact verification,\nvanilla RAG models struggle to extract useful in-\nformation from noisy documents while MM-RAIT\nenables the models to better extract and utilize rel-\nevant evidence, thereby improving their fact verifi-\ncation performance.\n\nPrompts of Different Tasks w/o Retrieval\n[Task]: Multi-Modal QA\n[Instruction]: You are an intelligent assistant capable \nof answering complex questions. Your task is to \ncarefully analyze the queation and provide a detailed, \nwell-structured, and contextually accurate answer in \nthe form of a complete sentence. Write a detailed and \naccurate answer directly, do not include unrelated \ndetails in your response.\nQuestion: {question}\nAnswer:\n[Task]: Multi-Modal Fact Verification\n[Instruction]: You are an intelligent assistant capable \nof verifying the factual accuracy by classifying data \nsamples into one of three categories: Support, \nInsufficient, and Refute. I will provide you with a claim \n(to be verified) consisting of text and an image. Your \ntask is to verify the claim based on your own \nknowledge into one of the three categories. Response \nto the category of the claim directly, do not say any \nother words or explain.\n{image}\nClaim_Image: The first image.\nClaim_Text: {claim_text}\nCategory:\n[Task]: Image Captioning\n[Instruction]:You are an intelligent assistant capable of \ngenerating accurate and detailed captions for images.\nI will provide you with an image. Response the caption \nfor the image directly, do not include any explanations \nor unrelated details.\n{image}\nCaption:\nPrompts of Different Tasks w/ Retrieval\n[Task]: Multi-Modal QA\n[Instruction]: You are an intelligent assistant capable \nof answering complex questions using both visual and \ntextual data. I will provide you with a question and \nseveral retrieved images or texts to assist you.\nYour task is to carefully analyze the queation and \nretrieved information to provide a detailed, well-\nstructured, and contextually accurate answer in the \nform of a complete sentence.  Write a detailed and \naccurate answer directly, do not include additional \ncontext, or unrelated details in your response.\n{image} … {image}\nQuestion: {question}\nRetrieved_Image_Caption: {retrieval_image_caption}\nRetrieved_Text: {retrieval_text}\nAnswer:\n[Task]: Multi-Modal Fact Verification\n[Instruction]: You are an intelligent assistant capable \nof verifying the factual accuracy by classifying data \nsamples into one of three categories: Support, \nInsufficient, and Refute. I will provide you with a claim \n(to be verified) consisting of text and an image, and \nretrieved images or texts. The first image is the \nclaim_image, and others are retrieved examples to \nhelp you. Your task is to classify the relationship \nbetween the claim and the evidence from the \nretrieved documents and your own knowedge into \none of the three categories. Response the category for \nthe claim and document directly, do not say any other \nword or explain.\n{image} … {image}\nClaim_Image: The first image.\nClaim_Text: {claim_text}\nRetrieved_Document_Text: {retrieval_text}\nCategory:\n[Task]: Image Captioning\n[Instruction]: You are an intelligent assistant capable \nof generating accurate and detailed captions for \nimages. You will be given one image and several \nretrieved images and their captions. The first image is \nthe input image, others are retrieved examples to help \nyou. Response the caption for the image directly, do \nnot include any explanations or unrelated details.\n{image} … {image}\nImage: The first image.\nRetrieved_Image_Caption: {retrieval_image_caption}\nCaption:\n[Task]: Image Reranking\n[Input Format]:\n<Rule>: User\n<Content>: {retrieved_image}\n<Role>: Assistant\n<Content>: {caption}\nInput format of Image Reranking Task\nFigure 6: Prompts Used for Different Tasks in Our M2RAG Benchmark.\n\nImage:\nGround Truth Caption:\nLibrary of Congress ceiling \ncolumns Washington DC\nVanilla Model:  Looking up at the grandiose interior of a historical \nbuilding, showcasing ornate columns and intricately designed ceilings\nwith stained glass windows.\nVanilla RAG:  Magnificent interior of the Library of Congress, showcasing \nintricate architectural details and vibrant ceiling artwork.\nMM-RAIT:  Library of Congress - Washington, D.C., 2014 Library of \nCongress interior in 2014\nMM-RAIT w/o RAG:  Looking Up at the Grand Stained Glass Ceiling of a \nLibrary.\nRetrieved Documents:\nLibrary of Congress \n- Washington, D.C…\n…\nLibrary of Congress \nInterior Jan 2006\nLibrary of Congress \nWashington, D.C\nMiniCPM-V 2.6\nQwen2-VL\nVanilla Model:  The interior of a grand building with ornate columns and \na detailed ceiling.\nVanilla RAG:  The Library of Congress in Washington, D.C.\nMM-RAIT:  Library of Congress interior, Washington, D.C., 2012 Library \nof Congress in Washington, D.C. in 2012\nMM-RAIT w/o RAG:  Library of Congress - interior\n(a) Image Captioning Task.\nQuestion:\nDoes the 1929 Cadillac Sports \nPhaeton have an extra wheel on its \ndesign?\nGround Truth Answer:\nYes, the 1929 Cadillac Sports \nPhaeton has an extra wheel on its \nside that does not touch the road.\nVanilla Model: The 1929 Cadillac Phaeton does not have an extra wheel\nin its design; it is a standard four-wheel vehicle.\nVanilla RAG:  The 1929 Cadillac Sports Phaeton does not have an extra \nwheel on its design. The images and …\nMM-RAIT Model:  The 1929 Cadillac Sports Phaeton has an extra wheel \non its design.\nMM-RAIT w/o RAG:  No, the 1929 Cadillac Sports Phaeton does not have \nan extra wheel on its design.\nRetrieved Documents:\nWheels were \navailable in hickory \nartillery style, wire \nwheels or solid \npressed ... \nThe engine \ndisplacement of the \n1930 LaSalle and the \n1928-1929 Cadillac \nSeries 341 were …\n…\nMiniCPM-V 2.6\nQwen2-VL\nVanilla Model: The 1929 Cadillac Sports Phaeton does not have an extra \nwheel on its design; it is a classic car known for its …\nVanilla RAG: Yes, the 1929 Cadillac Sports Phaeton has an extra wheel on \nits design.\nMM-RAIT Model: Yes, the 1929 Cadillac Sports Phaeton has an extra \nwheel on its …\nMM-RAIT w/o RAG: The 1929 Cadillac Sports Phaeton has an extra \nwheel on its design.\n1929 Cadillac Sports …\n(b) Multi-Modal Question Answering Task.\nClaim:\nKerala has decided to ban Jio’s Internet services \nfrom 2021. The government will start its own \nnetwork services and mobile phones at half of \nJio’s prices.\nVanilla Model: Insufficient\nVanilla RAG:  Insufficient\nMM-RAIT:  Refute\nMM-RAIT w/o RAG:  Insufficient\nRetrieved Documents:\nCategory:\nRefute\n… The Kerala \ngovernment has \nneither banned Jio \nnor started a state-\nowned Internet …\n…\nMiniCPM-V 2.6\nQwen2-VL\nVanilla Model: Support\nVanilla RAG:  Insufficient\nMM-RAIT:  Refute\nMM-RAIT w/o RAG:  Insufficient\n(c) Multi-Modal Fact Verification Task.\nCaption:\nBellagio Caesars Palace Night August 2005 \nA view of Bellagio and Caesars Palace from \nthe left side of Bellagio's fountains, at night \n(10 PM local time) inLas Vegas,Nevada,USA.\nGround Truth Image:\nVanilla Model:\nMM-RAIT:\nMM-RAIT:\nRetrieved Order:\nImage 1\nImage 5\nImage 4\nImage 3\nImage 2\nVanilla Model:\nImage 3\nImage 1\nImage 5\nImage 2\nImage 4\n＜\n＜\n＜\n＜\n11.4375\n12.0\n12.75\n12.75\n13.375\nImage 5\nImage 4\nImage 3\nImage 2\nImage 1\n＜\n＜\n＜\n＜\n12.9375\n13.375\n14.9375\n15.375\n16.375\nImage 5\nImage 4\nImage 2\nImage 3\nImage 1\n＜\n＜\n＜\n＜\n9.625\n9.9375\n9.9375\n10.125\n10.25\nImage 1\nImage 4\nImage 3\nImage 2\nImage 5\n＜\n＜\n＜\n＜\n7.875\n8.125\n9.1875\n9.625\n10.4375\nMiniCPM-V 2.6\nQwen2-VL\n(d) Image Reranking Task.\nFigure 7: Cases in Different Tasks. For generation tasks, we present the responses of different models using different\nRAG strategies (w/ or w/o RAG). We use green boxes to mark the documents that can provide information for\nthe question. In the model output part, correct answers are marked in green, and red for incorrect. For Image\nReranking task, we presented the order reranked by different models through corresponding PPL scores.\n",
  "metadata": {
    "source_path": "papers/arxiv/Benchmarking_Retrieval-Augmented_Generation_in_Multi-Modal_Contexts_37ff01c0c02e77d4.pdf",
    "content_hash": "37ff01c0c02e77d413524c05ae224cd8973064bd07fb6425ebbbe76704e41d1d",
    "arxiv_id": null,
    "title": "Benchmarking_Retrieval-Augmented_Generation_in_Multi-Modal_Contexts_37ff01c0c02e77d4",
    "author": "",
    "creation_date": "D:20250225025918Z",
    "published": "2025-02-25T02:59:18",
    "pages": 15,
    "size": 7294679,
    "file_mtime": 1740470164.0457132
  }
}