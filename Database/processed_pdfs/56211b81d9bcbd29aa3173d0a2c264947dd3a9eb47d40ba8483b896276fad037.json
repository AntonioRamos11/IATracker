{
  "text": "Unveiling Downstream Performance Scaling of LLMs:\nA Clustering-Based Perspective\nChengyin Xu∗, Kaiyuan Chen∗, Xiao Li, Ke Shen, Chenggang Li\nSeed-LLM, ByteDance\n{xuchengyin.98, chenkaiyuan.99, lixiao.20, shenke, lichenggang}@bytedance.com\nAbstract\nThe rapid advancements in computing dramatically increase the scale and cost\nof training Large Language Models (LLMs). Accurately predicting downstream\ntask performance prior to model training is crucial for efficient resource allocation,\nyet remains challenging due to two primary constraints: (1) the “emergence phe-\nnomenon”, wherein downstream performance metrics become meaningful only\nafter extensive training, which limits the ability to use smaller models for predic-\ntion; (2) Uneven task difficulty distributions and the absence of consistent scaling\nlaws, resulting in substantial metric variability. Existing performance prediction\nmethods suffer from limited accuracy and reliability, thereby impeding the assess-\nment of potential LLM capabilities. To address these challenges, we propose a\nClustering-On-Difficulty (COD) downstream performance prediction framework.\nCOD first constructs a predictable support subset by clustering tasks based on\ndifficulty features, strategically excluding non-emergent and non-scalable clusters.\nThe scores on the selected subset serve as effective intermediate predictors of\ndownstream performance on the full evaluation set. With theoretical support, we\nderive a mapping function that transforms performance metrics from the predictable\nsubset to the full evaluation set, thereby ensuring accurate extrapolation of LLM\ndownstream performance. The proposed method has been applied to predict perfor-\nmance scaling for a 70B LLM, providing actionable insights for training resource\nallocation and assisting in monitoring the training process. Notably, COD achieves\nremarkable predictive accuracy on the 70B LLM by leveraging an ensemble of\nsmall models, demonstrating an absolute mean deviation of 1.36% across eight\nimportant LLM evaluation benchmarks.\n1\nIntroduction\nLarge Language Models (LLMs) have emerged as transformative technologies in natural language\nunderstanding, generation, and reasoning [1, 14, 5]. Their impressive success heavily relies on\nscaling model parameters and pre-training data, with training loss empirically following a power-law\nrelationship with compute [18, 21]. However, this reduction in training loss primarily reflects an in-\ndomain compression effect and does not necessarily indicate improved out-of-domain generalization\nor downstream performance–the factor of primary concern in practice. Specifically, performance\nscaling of downstream tasks aims to predict the accuracy of the target LLM on downstream tasks\nusing metrics from smaller models. Our objective is to develop a prediction method that works\nreliably across a diverse range of downstream tasks, minimizing the worst-case prediction error.\nDespite extensive efforts, a reliable scaling law for downstream tasks remains elusive. One line\nof work attempts to extrapolate large-model performance by modeling the performance-loss rela-\ntionship [6, 13, 8, 38, 26], but this often fails to capture the emergent behaviors of LLMs and the\n∗Equal contribution.\nPreprint.\narXiv:2502.17262v1  [cs.CL]  24 Feb 2025\n\nmismatch between in-domain loss and downstream metrics [42]. Another line of research focuses on\ndirect extrapolation of performance-compute relationship [1, 19], yet the uneven difficulty distribution\nacross different evaluation samples undermines its accuracy. We observe that different evaluation\nsamples actually follow distinct performance scaling patterns, and thus applying a single extrapolation\nformula to the entire evaluation set is suboptimal. We give the detailed analysis in Section 3.\nTo address these challenges, we propose a new performance scaling law, derived from the existing\nloss scaling law [21], specifically applicable to evaluation subsets that exhibit consistent performance\nscaling patterns. Building on the performance scaling law, we develop a Clustering-On-Difficulty\n(COD) multi-stage framework for predicting downstream performance. Specifically, we first cluster\ntasks by their difficulty features, and then filter out clusters that lack valid extrapolation patterns.\nNext, we fit the performance-compute relationships in the remaining clusters under our performance\nscaling law, extrapolate the performance of large models within these clusters, and finally map the\naggregated predictions to the complete task set.\nWe validate our COD approach on eight evaluation sets, including popular MATH [15], BBH [32],\nand MMLU pro [36]. COD achieves an average prediction error of 1.36% on a 70B-parameter\nLLM. Our results demonstrate that this difficulty-aware framework substantially outperforms existing\nmethods, establishing a promising paradigm for accurate downstream performance scaling of LLMs.\nOur contributions can be summarized as follows:\n• We propose the COD framework to address high variance and emergent phenomena in the\nLLM performance by effectively modeling the difficulty distribution within the evaluation\nsets.\n• We introduce a performance scaling law for cluster-wise performance prediction, with\ntheoretical support and experimental validation.\n• Extensive experiments conducted across eight diverse evaluation sets demonstrate that COD\nachieves a state-of-the-art average prediction error of 1.36% on a 70B-parameter LLM.\n2\nRelated Work\n2.1\nLoss Scaling Laws\nLoss scaling laws provide a systematic framework for understanding the relationship between\ncomputational resources, data, model size, and the final performance of LLMs. Early work by Kaplan\net al. [21] demonstrates that the pre-training loss of LLMs follows a power-law relationship with the\ncompute (the number of floating-point operations) used in training. Subsequent studies extend these\nfindings to other domains, such as computer vision [41], graph learning [24] and vision-language\nmodels [2, 16]. Recent research has also explored scaling laws in specific contexts, such as fine-tuning\n[17, 34], vocabulary size optimization [33], retrieval-augmented models [30], and hyperparameter\ntuning [23, 40]. These studies highlight the broad applicability of scaling laws and their potential to\nguide the efficient allocation of computational resources.\n2.2\nDownstream Task Performance Scaling\nPredicting downstream task performance remains a critical challenge due to emergent abilities in\nLLMs that manifest only after exceeding task-specific thresholds [37, 28]. Recent works, such\nas using loss as a proxy [6] or increasing metric resolution [19], have demonstrated potential but\nencounter challenges in aligning surrogate metrics with original task objectives. Here, we briefly\nreview the two main types of methods for predicting downstream performance:\n1. Loss-intermediate prediction. These methods predict the final training loss (or in-domain\nvalidation loss) of LLMs with loss scaling laws first, and then predict downstream performance\nthrough loss-performance relationships [6, 13, 8]. While these methods leverage established scaling\nlaws for loss predictions, they encounter a fundamental limitation: the inconsistent mapping between\nloss and performance metrics. In addition, Xiao et al. [38] employ the evaluation set answer loss as\nan intermediate variable for estimation. Although answer loss correlates with the final performance\nmetrics, its predictability remains low as predicting answer loss shares the challenges with predicting\nperformance, including emergence phenomenon and high variance in task difficulty.\n2\n\n2.0\n2.5\n3.0\n3.5\nTraining Loss\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nAccuracy\nCEval\n70B\n7B\n1.9B\n1.4B\n973M\n652M\n411M\n238M\n122M\n2.0\n2.2\n2.4\n2.6\n2.8\nTraining Loss\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nAccuracy\nGSM8k\n7B-Const\n7B-Cosine\n1.3B-Const\n1.3B-Cosine\n102\n104\n106\n108\nCompute\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nBBH\nCluster 0\nCluster 1\nCluster 2\nSteps\nLearning Rate\nConst LR\nCosine LR\nFigure 1: Performance-loss relationship across difference model size (left) and learning rate schedule\n(middle). At equivalent loss values, smaller models or those with lower learning rates generally\nachieve higher accuracy than larger models or those with higher learning rates. Performance-compute\nrelationship for different clusters of BBH samples(right). Different clusters demonstrate diverse\nscaling patterns.\n2. End-to-end performance-compute prediction. These methods [19, 26, 1] directly model the\nrelationship between performance and compute (or the number of model parameters). Additionally,\nAchiam et al. [1] estimate and fit this relationship using a subset of the evaluation set. Hu et al.\n[19] address the challenge of non-emergent capabilities in smaller models by employing multiple\nnon-greedy decoding evaluations, thereby enabling accurate extrapolation of performance predictions\nfor models with up to 2.4B parameters.\n3\nPilot Study\nIn this section, we present the pilot experiments to illustrate the shortcomings of existing approaches.\nTraining loss may mismatch downstream tasks performance. Predicting downstream performance\nbased on training loss relies on the assumption that LLMs achieve identical downstream performance\nat the same loss value–an assumption that often does not hold. In practice, training loss primarily\nserves as an indicator of in-domain fitting, whereas downstream tasks typically represent out-of-\ndomain evaluations. Moreover, training configurations, such as model size and learning rate, can\nsignificantly affect not only the final loss but also the model’s generalization capabilities.\nFig. 1(left) illustrates the performance–loss relationships for LLMs of different sizes on the CEval\nbenchmark [29]. At the same training loss level, smaller models can outperform larger ones in terms\nof test accuracy. Because smaller models initially exhibit weaker in-domain fitting capacity, they\ntypically require more training steps to reach the same loss value, which can lead to better in-domain\ngeneralization once they do. Fig. 1(middle) compares the performance of LLMs trained under\ndifferent learning rate schedules on the GSM8k dataset [7]. At the same loss level, the performance\nunder the cosine schedule is always worse than that under the constant schedule, indicating that a\nlower learning rate may prioritize memorization over generalization, thereby diminishing downstream\nperformance.\nDiverse scaling patterns within the evaluation set.\nScaling patterns capture the perfor-\nmance–compute relationship for a single task sample. However, different task samples exhibit\nunique computational thresholds, learning slopes, and upper bounds, making it challenging to find\na single fitting function (or set of fitting functions) that generalizes well across diverse task sam-\nples. Fig. 1(right) illustrates the performance-compute relationships on three random clusters of the\nBBH benchmark [32], with each cluster containing samples with similar difficulty. Even within a\nsingle evaluation set, these scaling curves can vary significantly, indicating that a one-size-fits-all\nperformance-compute curve is insufficient for capturing the full spectrum of a downstream bench-\nmark.\n3\n\nFigure 2: The pipeline of Cluster-On-Difficulty downstream task performance scaling, including\n4 stages: a. Represent task difficulty feature with task-wise passrate vector. Cluster on difficulty\nfeature and filter outliers. b. Fit cluster-wise performance-compute curve. Classify clusters into\nextrapolatable clusters, non-extrapolatable clusters, and non-emergent cluster. c. Predict accuracy on\nextrapolatable clusters. d. Map subset accuracy prediction to full evaluation set performance.\nTaken together, these observations highlight the importance of modeling the heterogeneous scaling\nproperties within an evaluation set and identifying a robust intermediate metric to serve as a reliable\nindicator of the downstream performance of LLMs.\n4\nMethod\nIn this section, we introduce the COD method in four parts, illustrated in Fig. 2: 1) We show the\nadvantages of COD and present an improved mean-shift clustering algorithm (Section 4.1); 2) We\nderive a performance scaling law corresponding to task difficulty variance, which enhances the\nbenefit of extrapolating the performance-compute relationship for task clusters with similar difficulty\nfeatures (Section 4.2). We fit cluster-wise performance-compute curves on small models and filter\nextrapolatable clusters; 3) We extrapolate the performance on extrapolatable clusters and predict the\naccuracy of the target large model on the predictable subset(Section 4.3); 4) We show how to map\naccuracy on the predictable subset to full evaluations (Section 4.4).\n4.1\nClustering on Difficulty\nDespite sharing common themes, tasks within evaluation sets demonstrate substantial difficulty\ndifferences. These differences result in diverse performance scaling patterns across tasks, making it\nchallenging to apply a universal fitting function for predictions. Instead, we propose clustering tasks\nwith comparable performance scaling behaviors to enable more accurate predictions. This approach\nminimizes the heterogeneity of difficulty features within clusters while ensuring that each cluster\ncontains a sufficient number of samples for robust evaluation.\nWe adopt the passrate metric to quantify the capabilities of small-scale models [19]. For each model,\nwe conduct multiple evaluation runs (e.g., 100 trials) and calculate the mean accuracy as the expected\nprobability of correct responses. For each task, we characterize its difficulty through the passrates of\nmodels of increasing size. These passrates are arranged in ascending order of model scale, forming\nfeature vectors that ideally exhibit monotonic growth within the [0, 1] range, as model capability\ntypically increases with size. However, we observe that some tasks deviate from the expected scaling\npattern, showing non-monotonic difficulty features. This phenomenon may be attributed to metric\ninstability or fluctuations in model performance during training.\nImproved clustering methods. We hope to adopt clustering algorithms with the following features:\n1. Minimizing intra-class variance to ensure similar extrapolation properties within each cluster, 2.\nAutomatic determination of cluster numbers, as the optimal number varies across evaluation sets and\nis difficult to pre-specify.\n4\n\nFigure 3: t-SNE visualization of different clustering methods. Each point represents an evaluation\nsample. DBSCAN(left): Continuous diffusion of clustering leads to too many samples within a class\nand large inner-group variance. MeanShift(Middle): Keep the high-density region as a unique group.\nImproved-MeanShift(Right): Constrain inner-group variance with the radius parameter.\nAmong classical clustering algorithms, the K-Means algorithm [25] needs to specify the number of\nclusters in advance. Although there are methods for automatically selecting the optimal number of\nclusters, e.g. Elbow method [35] and Silhouette [27], these methods need to introduce additional\nhyperparameters. DBSCAN [11] is a non-parametric density-based clustering algorithm that marks\npoints in low-density regions as outliers while cluster points in the connected high-density regions.\nIn practice, DBSCAN may lead to a larger final intra-class variance and does not meet the clustering\nrequirements of the current task. MeanShift [12] algorithm adopts an extra clustering radius parameter\nto constrain the intra-class variance, which better fits our demands.\nTo further reduce intro-class variance, we propose an improved MeanShift algorithm to constrain the\ncluster diameter. At the same time, we maintain a minimum number of tasks in each cluster to reduce\nmetric fluctuations. We provide the t-SNE visualization of evaluation tasks on BBH [32]. Each\npoint represents an evaluation sample and its color denotes the cluster type. Fig. 3. The improved\nMeanShift prevails, as it effectively splits dense areas into reasonable clusters. We explain the details\nof clustering algorithms in Appendix A.1, and smoothing techniques in Appendix A.2.\n4.2\nFitting\nFollowing cluster analysis, we compute evaluation metrics of small models within each cluster and\nconduct separate extrapolation curve fitting procedures. Small models are trained with the same\nratio of training tokens to Compute Per Token (CPT). We propose a scaling law for downstream\ntask performance, supported by theoretical analysis, which allows us to derive prediction formulas\nfor performance scaling within clusters of tasks that share similar difficulty features. The fitting\nprocess initially excludes outlier samples, focusing only on the clustered sample set. For each\ncluster identified in the previous step, we compute accuracy metrics across small models, yielding an\nexpected accuracy array for each cluster. By fitting these accuracy values against the computational\ncosts of small models, we derive the expected accuracy-to-compute curve for each cluster.\nWe derive the fitting formula for the downstream task scaling law based on the following three\nassumptions:\n1. The relationship between the answer loss and the compute follows a power law, which\ngeneralizes the power law in loss prediction into (Question, Answer) format data.\n2. For task samples with a finite set of answers, the model gives a random guess choice if it\ncannot accurately solve it.\n3. The task passrate is defined as the product of the predicted probabilities for each token,\nimplying that each task sample has a unique answer, and the model outputs the answer only\nwithout any intermediate reasoning progress.\nNote that these assumptions may not perfectly hold in practice, we provide additional discussions\non Assumption 3 in Section 6. Under the above assumptions we can derive the scaling law for\ndownstream task performance.\n5\n\nProposition 1 (Scaling Law for Downstream Task Performance). Given a language model trained\nwith computational budget C, and a set of downstream tasks P, under the following assumptions:\nThe expected accuracy on tasks P can be modeled as:\nEp[Acc(C)] = g + (1 −g)\n\u0012\ne−aC−b−c + σ2\nµ\n\u0013\n+ o(µ)\nwhere:\n• g represents the random guess performance floor;\n• 1 −g represents the maximum achievable performance improvement;\n• a, b, c are positive constants;\n• µ =\n1\n#P\nP\n(q,ans)∈P lossans;\n• σ2 =\n1\n#P\nP(lossans_t −µ)2.\nWe outline the key proof intuition here, with detailed proofs provided in Appendix B. Like existing\napproaches [19], we aim to establish the relationship between lossans and model passrate, leveraging\nloss power-law scaling to derive a scaling formula for downstream task passrate metrics.\nProof intuition\nThe assumption 3 ensures unique task answers and neglecting the impact of model thinking before\nanswers, the probability of correct task completion equals the product of token probabilities in\nthe model output. This implies a negative logarithmic relationship between lossans and passrate\nfor individual tasks.\nPrevious works overlook that computing the passrate metric for an evaluation set requires\naveraging exp(−lossans) across tasks, whereas applying the loss scaling law necessitates aver-\naging loss before exponentiation. Mathematically, the performance scaling law computes the\narithmetic mean of exp(−lossans), while the loss scaling law after exponentiation yields the\ngeometric mean.\nWe show that the difference between arithmetic and geometric means can be estimated by σ2/2µ,\nwhere σ2 and µ denote the variance and mean of task passrates, respectively. Consequently, the\ndownstream tasks performance scaling law derived from the loss scaling law is valid only for\nevaluation sets with limited difficulty variance. Our proposed clustering-based COD method\nconstrains the variance of difficulty features within clusters, enabling better alignment with the\nperformance scaling law.\nFinally, we constrain the model output space to a finite answer set, random guessing yields an\nexpected score g for unsuccessful attempts.\nProposition 1 demonstrates that a metric of an evaluation set with similar difficulty features can be\neffectively modeled using the following formula:\ny(C) = g + (1 −g) ∗e−aC−b−c,\n(1)\nwhere a and b jointly influence how accuracy varies with C, c controls the upper bound of the fitting\ncurve, and g represents the expected random guess metric for the model on this cluster. Parameters a,\nb, c, and g are to be fitted.\n4.3\nExtrapolation\nWe aim to identify clusters exhibiting robust scaling patterns for reliable performance extrapolation\nsince some clusters have saturated or non-emergent performance on small models and are not expected\nto give reasonable predictions. We will show that performance prediction on these scalable clusters\ncontributes to the prediction on the full evaluation set. We give the following definition to check\nwhether a cluster is extrapolatable.\n6\n\nDefinition: A cluster demonstrates scaling patterns if: (1) its expected accuracy increases mono-\ntonically with model size, and (2) the probability of correct responses converges to at least P as\ncomputational resources approach infinity, where P ≤1 is a predefined threshold accounting for\npractical limitations such as ambiguous questions and finite training coverage.\nWe filter clusters lacking scaling patterns by the following two rules:\n1. Negligible accuracy growth with increased computational resources, manifested as minimal\na or b values in Eq. (1);\n2. Poor extrapolation reliability, indicated by excessive c values in Eq. (1).\nIn practice, we set the parameter ranges a priori as a > 1, b > 0.1, and 0 < c < 1.\nThe predictable subset comprises all samples from clusters that exhibit scaling patterns. The final\nperformance prediction for a target model in the predictable subset is computed as the weighted\naverage of the individual cluster predictions, with weights proportional to the cluster sizes.\n4.4\nMapping from Predictable Subset to Target Evaluation Set\nWe extend our predictions from the predictable subset to the complete evaluation set through a\nprincipled mapping approach. Our method rests on the observation that extrapolatable and non-\nextrapolatable samples share question types but differ primarily in difficulty features, suggesting\na preserved partial order of metrics across these subsets. We formalize this relationship through a\nmapping function f : f(T ′) →T from predictable subset metrics T ′ to total evaluation set metrics\nT. This function exhibits key properties: 1. continuity and smoothness over [0, 1], 2. monotonic\nincrease, and 3. passage through points (0, 0) and (1, 1). Empirical validation reveals that a quartic\nfunction optimally captures this relationship:\nf(x) = α1x4 + α2x3 + α3x2 + (1 −α1 −α2 −α3)x\n(2)\nTo ensure reliable extrapolation, we calibrate the mapping curve using evaluation results of existing\nmodels as anchors. Our results show that the subset-to-full mapping generally maintains robustness\nacross model architectures and training data, enabling the use of external models (e.g., Qwen2-\n72B [39]) as anchors for most tasks. We conduct corresponding experiments in Section 5.5. For data-\nsensitive tasks, models with similar training distributions provide more reliable anchors, indicating\nthat data consistency takes precedence over architectural variation ensuring mapping accuracy.\nThis calibration strategy enables accurate metric predictions for the complete evaluation set while\nmaintaining computational efficiency.\nFinally, combining Eq. (1) and Eq. (2), we get our final metric prediction p = f ◦y(C0), where C0 is\nthe estimated computation of training the target LLM.\n5\nExperiments\n5.1\nExperimental Setups\nIn our experimental setup, we train several smaller versions of the target model architecture for\nprediction. These models vary in size but share similar training procedures, with the training data\nscaled proportionally to their sizes.\nDownstream evaluation sets. We adopt the following widely-used benchmarks as our target\ndownstream tasks: For evaluation, we adopt the following widely-used benchmarks, shown in Table 1.\nEvaluation sets cover popular downstream tasks of the language model, including math, logic, coding,\nreading comprehension, professional knowledge, etc.\nAll models are evaluated in a few-shot in-context learning manner, where they need to generate final\nanswer labels based on given demonstrations and test inputs. We aligned our evaluation setups with\nLLaMa3 [10].\nModel training. To establish performance predictions for large language models, we conduct\nsystematic experiments with a suite of smaller-scale models across different parameter counts and\ntraining data volumes, while controlling for other training configurations such as learning rate, batch\nsize, and additional hyperparameters. All models are trained on a constant learning rate scheduler\nand data with the same distribution. We list model configurations in Table 2.\n7\n\nTable 1: Information of evaluation datasets used in the study.\nDataset\nGSM8K MATH\nBBH\nTriviaQA\nMBPP\nAGIEval\nDROP\nMMLU-pro\n[7]\n[15]\n[32]\n[20]\n[3]\n[43]\n[9]\n[36]\nDomain\nMath\nMath Reasoning Knowledge Coding Comprehensive Reading Comprehensive\n#Questions\n1,319\n5,000\n6,511\n17,944\n500\n8,063\n9,536\n12,032\n#Shots in Prompt\n8\n4\n3\n5\n3\n5\n3\n5\nTable 2: Model architecture specifications across different sizes.\n122M\n238M\n411M\n652M\n973M\n1.9B\n7B\n12B\n70B (Target)\nParam. (M)\n122\n238\n411\n652\n973\n1,901\n6,980\n12,022\n68,452\nCompute Per Token (B)\n1.535\n2.684\n4.275\n6.378\n9.060\n16.436\n54.761\n91.609\n475.131\nTokens (B)\n26\n45\n72\n108\n153\n277\n923\n1,544\n8,012\nLayers\n8\n10\n12\n14\n16\n20\n32\n43\n80\nModel Dimension\n1,024\n1,280\n1,536\n1,792\n2,048\n2,560\n4,096\n4,608\n8,192\nFFN Dimension\n3,584\n4,480\n5,376\n6,272\n7,168\n8,960\n14,336\n16,128\n28,672\nHeads\n8\n10\n12\n14\n16\n20\n32\n36\n64\nKV Heads\n8\n10\n12\n14\n16\n20\n8\n12\n8\n5.2\nPrediction Experiments\nBaselines. We evaluate our proposed COD performance scaling for LLMs against existing approaches.\nThe evaluation is conducted on multiple public benchmarks mentioned above, where we utilize a\nseries of smaller models with identical data distribution and architecture but different configurations\nto estimate the downstream tasks performance of the target 70B large language model.\nWe compare against three existing prediction methods:\n1. End-to-end performance-compute prediction: Extrapolate larger model metrics directly\nfrom smaller model evaluation set metrics using performance scaling laws.\n2. Passrate-compute prediction: Estimate large model passrates from smaller model pass-\nrates [1, 19]. We repeat and evaluate 100 trials for each evaluation set to enhance the\nperformance reliability on smaller models. For a fair comparison, we report the absolution\nprediction error on the passrate metric instead of greedy decoding accuracy.\n3. Loss-intermediate performance prediction: First predict the final training loss of large\nlanguage model, then estimate downstream task metrics based on the relationship between\nsmaller model evaluation metrics and their corresponding losses [6].\nWe design two experimental groups to validate the benefits of clustering and the complete pipeline,\nrespectively:\n1. COD w/o. mapping: Performing difficulty-based clustering using K-Means, extrapolating\nwithin each cluster independently, and then aggregating metrics across clusters without\nrequiring subset-to-full mappings.\n2. COD complete: Complete multi-stage proposed approach consisting of clustering, pre-\ndictable cluster filtering, subset extrapolation, and subset-to-full mapping.\nThe comparative results across different benchmarks and estimation approaches are presented in\nTable 3. We evaluate prediction accuracy with the absolute error between predicted and actual\nperformance. We report the prediction error on each single evaluation set and list the mean and the\nmax prediction error.\nResults. Predictions with an absolute error of less than 2 percentage points (pp) are considered\naccurate estimations, and when the predicted values fall within the training metric fluctuation range,\nthey are marked in green; predictions with an absolute error greater than 5 indicate invalid estimations,\nwhich reduce the overall reliability of the prediction method and are marked in red. These results\nshow our approach significantly outperforms existing methods in both mean and maximum prediction\nerrors, maintaining mean prediction error within 2 pp, thus offering practical guidance for large model\n8\n\nTable 3: Absolute prediction error on several evaluation sets. A prediction error less than 2 pp is\nconsidered an accurate estimate (marked in green), while an error greater than 5 pp is regarded as an\ninvalid estimate (marked in red).\nMethod\nOverall Metrics\nIndividual Task Sets\nMean↓Max↓\nGSM8k MATH BBH TriviaQA MBPP AGIEval DROP MMLU-pro\nEnd-to-end\n3.10\n6.00\n4.00\n3.86\n0.64\n0.68\n1.75\n6.00\n4.11\n3.72\nPassrate\n5.02\n8.80\n6.71\n8.80\n3.51\n4.00\n7.34\n6.78\n0.26\n2.74\nLoss-intermediate\n5.29\n9.39\n9.39\n6.95\n2.33\n5.81\n5.52\n1.41\n5.37\n5.55\nCOD (w/o mapping)\n2.24\n5.26\n4.70\n0.50\n2.91\n1.98\n0.89\n5.26\n1.08\n0.57\nCOD (Complete)\n1.63\n2.38\n2.23\n1.28\n1.77\n1.64\n2.19\n2.38\n0.23\n1.35\ntraining. While existing methods demonstrate good performance on certain evaluation sets, they\nconsistently exhibit substantial estimation errors on a minority of sets, undermining the credibility of\ntheir predictions.\n101\n102\n103\n104\n105\n106\n107\nCompute(BB)\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nAccuracy\nBBH\nSmall Models\nTarget Model\nEnd-to-End\nLoss-Intermediate\nCOD (Ours)\n101\n102\n103\n104\n105\n106\n107\nCompute(BB)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nAccuracy\nMATH\nSmall Models\nTarget Model\nEnd-to-End\nLoss-Intermediate\nClustering->COD (Ours)\n101\n102\n103\n104\n105\n106\n107\nCompute(BB)\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nAccuracy\nMMLU-pro\nSmall Models\nTarget Model\nEnd-to-End\nLoss-Intermediate\nCOD (Ours)\nFigure 4: Performance-compute relationship across difference prediction method\nThrough visualization of the performance-compute relationship, we illustrate the distinctive char-\nacteristics of different prediction methods, shown in Fig. 4. On the BBH evaluation set, while all\nthree methods yield comparable estimates, end-to-end and loss-intermediate methods demonstrate\ninadequate fitting for small model evaluation points. In contrast, the cluster method reveals a more\nsophisticated and well-fitted multi-phase trajectory. For MATH and MMLU pro evaluation sets,\nthe critical challenge involves determining whether large model metrics will experience accelerated\ngrowth with increased computing power or encounter performance plateaus. The loss-intermediate\nmethod exhibits an underestimation of the model capability ceiling, while the end-to-end method\nshows prediction errors exceeding 3 pp.\nThe clustering method’s effectiveness can be attributed to its comprehensive analysis of evaluation\nset difficulty distributions and scaling laws. It successfully predicts growth patterns in mathematical\nevaluation sets where most problems demonstrate expanded improvement potential, while accurately\ncapturing the diminishing scaling properties in evaluation sets with score saturation as computational\nresources increase.\n5.3\nComparison of Clustering Methods\nWe evaluate the impact of clustering methods on the estimation approach. Our goal is to control\nthe average distance between samples and cluster centers within clusters, making difficulty features\nmore similar within clusters. We also ensure that the minimum number of questions in any cluster\nis not less than 10, considering that too small clusters may lead to instability in metric values. We\ncompared our proposed Improved-MeanShift algorithm with clustering methods including DBScan,\nMeanShift, and K-Means. Since standard K-Means lacks the ability to filter outliers and directly\ncontrol intra-cluster distances, we made the following adjustments: (1) Search for the number of\nclusters such that the minimum cluster size is close to but not less than 10 samples; (2) Draw spheres\naround cluster centers with a given threshold radius, and treat samples not covered by any sphere as\noutliers. If a cluster drops to less than 10 samples, we treat its samples as outliers.\n9\n\nTable 4: Clustering performance on popular benchmarks.\nMethod\nMMLU-pro\nGSM8k\nMATH\nBBH\nIAD↓\nOR(%)\nIAD↓\nOR(%)\nIAD↓\nOR(%)\nIAD↓\nOR(%)\nK-Means\n0.3236\n-\n0.2238\n-\n0.2238\n-\n0.6284\n-\nDBScan\n0.4242\n0.56\n0.5131\n0.53\n0.4775\n0.68\n0.7113\n18.92\nMeanShift\n0.2859\n0.39\n0.2852\n0.61\n0.2110\n1.44\n0.2679\n20.72\nImproved-KMeans\n0.1609\n2.85\n0.1321\n2.73\n0.0902\n2.22\n0.1953\n37.23\nImproved-MeanShift\n0.2225\n4.40\n0.1854\n4.93\n0.1463\n2.66\n0.2143\n33.58\nTable 5: Prediction errors across clustering algorithms.\nMethod\nMean\nMax\nMMLU-pro\nGSM8k\nMATH\nBBH\nEE↓\nFE↓\nEE↓\nFE↓\nEE↓\nFE↓\nEE↓\nFE↓\nEE↓\nFE↓\nEE↓\nFE↓\nK-Means\n3.62\n3.76\n8.16\n8.99\n3.69\n3.69\n0.01\n0.00\n2.62\n2.34\n8.16\n8.99\nDBScan\n3.93\n4.08\n4.38\n4.36\n3.72\n3.69\n4.08\n4.12\n4.38\n4.16\n3.53\n4.36\nMeanShift\n2.12\n1.68\n3.15\n3.08\n3.15\n3.08\n0.67\n0.74\n2.55\n2.26\n2.12\n0.65\nImproved-KMeans\n1.33\n1.84\n3.92\n4.08\n0.56\n0.61\n3.92\n4.08\n0.81\n0.51\n0.02\n2.17\nImproved-MeanShift\n1.23\n1.66\n2.20\n2.23\n1.27\n1.35\n2.20\n2.23\n1.14\n1.28\n0.29\n1.77\nWe use Intra-cluster Average Distance (IAD) and Outlier Rate (OR) as direct evaluation metrics. With\nsimilar OR, a smaller IAD indicates better clustering performance, as shown in Table 4. Additionally,\nwe measure the benefits of different clustering methods on the prediction process by comparing the\nExtrapolation Errors(EE) of the predictable subset and Final prediction Errors (FE) after clustering,\nas shown in Table 5.\nTable 4 shows that Improved-KMeans and Improved-MeanShift achieve better clustering performance,\nwhich is attributed to their incorporation of intra-cluster distance constraints during the clustering\nprocess. From Table 5, we can observe that the two methods with better clustering performance\ncorrespond to smaller extrapolation errors of estimable subsets and final metric prediction errors.\nAlthough Improved-KMeans achieves optimal clustering performance, its downstream estimation\nperformance on GSM8k is notably inferior compared to other evaluation sets. We believe this is\nbecause the K-Means algorithm requires pre-specifying a more explicit number of clusters, but\nthe number of clusters in evaluation sets is difficult to know in advance. While the number of\nclusters obtained through our search method is effective for some evaluation sets, it lacks stability\nand ultimately leads to excessive prediction errors in a few evaluation sets. In contrast, our adopted\nImproved-MeanShift algorithm inherently does not require pre-specifying the number of clusters;\ninstead, it automatically determines this based on our intra-cluster distance constraints. This results\nin a more stable clustering performance and yields the smallest maximum estimation error across\nevaluation sets. We present additional clustering experimental results on more evaluation sets in\nAppendix C.1, where the conclusions are consistent with existing evaluation sets.\n5.4\nExtrapolation Formula\nTo evaluate the effectiveness of different fitting formulas, we conducted an ablation study comparing\nvarious formulations of the accuracy-compute relationship. Our baseline formula incorporates random\nguess probability, exponential decay, and a constant offset term:\nf(C) = g + (1 −g) ∗e−aC−b−c\n(3)\nTo understand the contribution of each component, we perform ablation experiments by removing or\nmodifying different terms. 1) Without random guess component: f1(C) = e−aC−b−c; 2) Without\nconstant term c: f2(C) = g + (1 −g) ∗e−aC−b; 3) Direct power law relationship [19]: f3(C) =\ne−aC−b.\nThe comparative results of these formulations are shown in Table 6. Results from three evaluation sets,\nBBH, Math, and MMLU-pro, are presented here, showing the Extrapolation Error of extrapolatable\nclusters (EE), the Task Ratio of predictable subset (TR), and the Final prediction Error (FE). These\n10\n\nTable 6: Ablation study results across different benchmarks.\nMethod\nBBH\nMATH\nMMLU-pro\nEE↓\nTR(%)\nFE↓\nEE↓\nTR(%)\nFE↓\nEE↓\nTR(%)\nFE↓\nDirect Power Law\n8.90\n49.06\n8.88\n3.81\n81.46\n3.35\n4.30\n95.15\n4.27\nw/o Random Guess\n10.27\n45.75\n11.20\n4.04\n81.46\n3.55\n4.40\n95.05\n4.37\nw/o Constant c\n2.14\n57.26\n4.01\n1.40\n81.46\n1.56\n3.85\n95.60\n3.88\nOurs\n0.29\n52.46\n1.77\n1.14\n81.24\n1.28\n1.27\n94.38\n1.35\nresults show that the proposed formula f consistently achieves the smallest extrapolation error and\nfinal prediction error, while the ratio of estimable subsets remains similar across different clustering\nmethods.\nIn the control group, f1 performs poorly on tasks with finite answer sets, where small models achieve\nnon-zero scores that f1 cannot effectively fit. f2 removes c, which determines the maximum value of\nthe prediction curve, assuming that the evaluation set performance would reach perfect scores given\nsufficient computation and parameters. However, this assumption is unreasonable due to the limited\ntraining data distribution and ambiguous answers in evaluation set questions, leading to inaccurate\npredictions. Direct power-law fitting f3 fails to model both the metric range constraint of 0 to 1 and\nthe characteristic that metric improvement is typically more difficult near Random Guess capability\nand capability saturation compared to other regions.\nWe also observe that TR has little influence on prediction error, which also indicate the robustness\nof the proposed method. Some evaluation set with low TR due to the non-emergent subset, and our\nresults show that this subset can largely be predicted with metrics of the extrapolatable clusters. We\nvisualize the difficulty distribution of predictable subset and the full evaluation set in Appendix D.\n5.5\nAnchor Point in Interpolation Mapping\nDuring the mapping phase from estimable subset metrics to full evaluation set metrics, we discovered\nthat models with different training data and architectures exhibit similar mapping relationships. This\nallows us to leverage metrics from pre-trained models to refine the mapping relationship, thereby\nimproving the accuracy of our final metric estimation.\nWe use both Qwen2-72B [39] and an in-house model M with a Mixture-of-Experts(MoE) [22]\nstructure trained on the same data distribution as anchor points in the mapping phase. We first\nobtain the interpolation curve using only small model metrics with points (0,0) and (1,1), then verify\nthe compatibility of anchor points with the existing interpolation curve. When the score of the\nfull set is 0 (1), the score of the subset must also be 0 (1). Results show that despite differences\nin computational requirements, model architectures, and training data, these models share similar\nmapping relationships.\nThis finding also indicates that estimable subset metrics are highly correlated with full-set metrics.\nCompared to loss-intermediate estimation, estimable subset metrics maintain predictability while\nreducing interference from other model parameters. Based on these observations, we incorporated\nthe mapping relationships from pre-trained models into the interpolation process, thereby improving\nboth the accuracy and confidence of our large model metric estimations.\nWe establish three experimental configurations of our method:\n• COD w/o. anchor: The complete estimation process is employed except for not using\nanchor point interpolation in the Mapping phase.\n• COD w. out-of-distribution (OOD) anchor: The complete proposed methodology incor-\nporates both difficulty-based clustering and predictable subset identification. Using the 72B\nQwen2 pretraining model as the anchor model [39].\n• COD w. in-domain(ID) anchor: Using an in-house MoE model with consistent training\ndistribution but different model architecture as the anchor point.\nWe present the results in Table 7, demonstrating that incorporating both out-of-distribution models and\nin-distribution models as anchors consistently enhance prediction accuracy. These findings suggest a\n11\n\nTable 7: Influence of anchor point usage in the mapping stage.\nMethod\nOverall\nIndividual Task Sets\nMean Max\nGSM8k MATH BBH TriviaQA MBPP AGIEval DROP MMLU-pro\nw/o. anchor\n3.96\n8.80\n2.17\n5.46\n5.08\n1.68\n8.80\n2.38\n4.44\n1.68\nw. ood anchor\n1.59\n2.56\n2.22\n0.94\n1.84\n1.04\n2.56\n1.86\n1.18\n1.04\nw. id anchor\n1.63\n2.38\n2.23\n1.28\n1.77\n1.64\n2.19\n2.38\n0.23\n1.35\nrelatively stable correlation between the metrics of predictable subsets and the full dataset, indicating\nthat the relationship between subset and full-set metrics remains consistent across models trained\non different data and with varying architectures. This property enables us to leverage evaluation\nresults from existing models to improve the accuracy of metric predictions for large models with\nnew data and structures. Besides, the division of the evaluation set obtained through clustering is\nan intrinsic property of the evaluation set itself, independent of the model architecture and training\ndata. Therefore, the predictable subset derived from clustering can also be extended to estimate the\nmetrics of new models. Additionally, we conduct the ablation study on the interpolation method in\nAppendix C.2, and results indicate that quartic functions are suitable in our setting.\n6\nConclusion and Discussion\nIn this work, we introduce a novel downstream performance scaling framework including (1) a\ndifficulty-based clustering approach that effectively models the underlying distribution of each\nevaluation set; (2) a theoretically grounded scaling law for downstream task performance that\nprovides a fitting formula for performance-compute extrapolation; and (3) a systematic methodology\nfor identifying and leveraging predictable subset that provides a robust intermediate metric for\naccurate full-set performance predictions.\nOur framework, while effective for dense transformers, has not been fully explored for cost-efficient\nMoE models and does not account for the annealing phase in training, where high-quality data can\nrapidly enhance performance. The COD method requires sufficient test cases and is not suited for\nmultiple-choice tasks, where performance metrics may diverge from true passrates. Additionally, the\nframework’s theoretical foundation is insufficient for chain-of-thought reasoning, necessitating future\nadaptations to address these challenges. We provide detailed discussion in Appendix E.\nLooking forward, our approach can be further expanded across model architectures, training methods,\nand evaluation set types, while extending this framework to address chain-of-thought reasoning\npatterns offer promising avenues for future research.\nReferences\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni\nAleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4\ntechnical report. arXiv preprint arXiv:2303.08774, 2023.\n[2] Ibrahim M Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. Revisiting neural scaling\nlaws in language and vision. Adv. Neural Inform. Process. Syst. (NeurIPS), 35:22300–22312,\n2022.\n[3] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David\nDohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large\nlanguage models. arXiv preprint arXiv:2108.07732, 2021.\n[4] Hritik Bansal, Arian Hosseini, Rishabh Agarwal, Vinh Q Tran, and Mehran Kazemi. Smaller,\nweaker, yet better: Training llm reasoners via compute-optimal sampling. arXiv preprint\narXiv:2408.16737, 2024.\n[5] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece\nKamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general\nintelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.\n12\n\n[6] Yangyi Chen, Binxuan Huang, Yifan Gao, Zhengyang Wang, Jingfeng Yang, and Heng Ji.\nScaling laws for predicting downstream performance in llms. arXiv preprint arXiv:2410.08527,\n2024.\n[7] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to\nsolve math word problems. arXiv preprint arXiv:2110.14168, 2021.\n[8] Zhengxiao Du, Aohan Zeng, Yuxiao Dong, and Jie Tang. Understanding emergent abilities of\nlanguage models from the loss perspective. arXiv preprint arXiv:2403.15796, 2024.\n[9] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt\nGardner. DROP: A reading comprehension benchmark requiring discrete reasoning over\nparagraphs. In NAACL-HLT, pages 2368–2378, 2019.\n[10] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle,\nAiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd\nof models. arXiv preprint arXiv:2407.21783, 2024.\n[11] Martin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu, et al. A density-based algorithm for\ndiscovering clusters in large spatial databases with noise. In KDD, volume 96, pages 226–231,\n1996.\n[12] Keinosuke Fukunaga and Larry Hostetler. The estimation of the gradient of a density function,\nwith applications in pattern recognition. IEEE Transactions on information theory, 21(1):32–40,\n1975.\n[13] Samir Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar, Suchin Gururangan, Mitchell\nWortsman, Rulin Shao, Jean Mercat, Alex Fang, Jeffrey Li, Sedrick Keh, et al. Language models\nscale reliably with over-training and on downstream tasks. arXiv preprint arXiv:2403.08540,\n2024.\n[14] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\nShirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in\nllms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.\n[15] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn\nSong, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset.\nIn Adv. Neural Inform. Process. Syst. (NeurIPS), 2021.\n[16] Tom Henighan, Jared Kaplan, Maxwell Katz, Anselm Levskaya, Sam McCandlish, Andreas\nStuhlmuller, Scott Gray, and Dario Amodei. Scaling laws for autoregressive generative modeling.\narXiv preprint arXiv:2010.14701, 2020.\n[17] Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. Scaling laws for\ntransfer. arXiv preprint arXiv:2102.01293, 2021.\n[18] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.\nTraining compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.\n[19] Shengding Hu, Xin Liu, Xu Han, Xinrong Zhang, Chaoqun He, Weilin Zhao, Yankai Lin, Ning\nDing, Zebin Ou, Guoyang Zeng, et al. Predicting emergent abilities with infinite resolution\nevaluation. In Int. Conf. Learn. Rep. (ICLR), 2024.\n[20] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale\ndistantly supervised challenge dataset for reading comprehension. In Annual Meeting of the\nAssociation for Computational Linguistics, pages 1601–1611, 2017.\n[21] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language\nmodels. arXiv preprint arXiv:2001.08361, 2020.\n13\n\n[22] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with condi-\ntional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.\n[23] Lucas Lingle. A large-scale exploration of µ-transfer. arXiv preprint arXiv:2404.05728, 2024.\n[24] Qian Ma, Haitao Mao, Jingzhe Liu, Zhehua Zhang, Chunlin Feng, Yu Song, Yihan Shao, and\nYao Ma. Do neural scaling laws exist on graph self-supervised learning?\narXiv preprint\narXiv:2408.11243, 2024.\n[25] James MacQueen et al. Some methods for classification and analysis of multivariate observations.\nIn Proceedings of the fifth Berkeley symposium on mathematical statistics and probability,\nvolume 1, pages 281–297, 1967.\n[26] David Owen. How predictable is language model benchmark performance? arXiv preprint\narXiv:2401.04757, 2024.\n[27] Peter J Rousseeuw. Silhouettes: a graphical aid to the interpretation and validation of cluster\nanalysis. Journal of computational and applied mathematics, 20:53–65, 1987.\n[28] Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities of large language\nmodels a mirage? In Adv. Neural Inform. Process. Syst. (NeurIPS), 2023.\n[29] Christin Seifert, Jörg Schlötterer, et al. Ceval: A benchmark for evaluating counterfactual text\ngeneration. In International Natural Language Generation Conference, pages 55–69, 2024.\n[30] Rulin Shao, Jacqueline He, Akari Asai, Weijia Shi, Tim Dettmers, Sewon Min, Luke Zettle-\nmoyer, and Pang Wei Koh. Scaling retrieval-based language models with a trillion-token\ndatastore. arXiv preprint arXiv:2407.12854, 2024.\n[31] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute opti-\nmally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314,\n2024.\n[32] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won\nChung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging big-bench\ntasks and whether chain-of-thought can solve them. In Findings of the Association for Compu-\ntational Linguistics, pages 13003–13051, 2023.\n[33] Chaofan Tao, Qian Liu, Longxu Dou, Niklas Muennighoff, Zhongwei Wan, Ping Luo, Min\nLin, and Ngai Wong. Scaling laws with vocabulary: Larger models deserve larger vocabularies.\narXiv preprint arXiv:2407.13623, 2024.\n[34] Yi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung Won Chung,\nSharan Narang, Dani Yogatama, Ashish Vaswani, and Donald Metzler. Scale efficiently: Insights\nfrom pretraining and finetuning transformers. In Int. Conf. Learn. Rep. (ICLR), 2022.\n[35] Robert L Thorndike. Who belongs in the family? Psychometrika, 18(4):267–276, 1953.\n[36] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo,\nWeiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: A more robust and\nchallenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574,\n2024.\n[37] Jason Wei, Yi Tay, Rishi Bommasani, et al. Emergent abilities of large language models. arXiv\npreprint arXiv:2206.07682, 2022.\n[38] Chaojun Xiao, Jie Cai, Weilin Zhao, Guoyang Zeng, Xu Han, Zhiyuan Liu, and Maosong Sun.\nDensing law of llms. arXiv preprint arXiv:2412.04315, 2024.\n[39] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan\nLi, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv preprint\narXiv:2412.15115, 2024.\n14\n\n[40] Greg Yang, Edward J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick\nRyder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large\nneural networks via zero-shot hyperparameter transfer. arXiv preprint arXiv:2203.03466, 2022.\n[41] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transform-\ners. In IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pages 1204–1213, 2022.\n[42] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding\ndeep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):\n107–115, 2021.\n[43] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied,\nWeizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation\nmodels. In Findings of the Association for Computational Linguistics, pages 2299–2314, 2024.\n15\n\nA\nImprovements of Clustering Algorithm\nA.1\nImproved MeanShift Algorithm\nWe iteratively apply the MeanShift algorithm with a predefined cluster radius R and a minimum\ncluster size K. In each iteration, for the clustered samples, we examine whether the distance between\neach sample and its cluster center exceeds R, and relabel those samples that exceed this threshold as\nunclustered. For clusters containing fewer than K samples, we mark all samples in these clusters as\nunclustered. At the end of each iteration, we incorporate both the outliers from MeanShift and our\nmarked unclustered samples into the next round of clustering, continuing this process until no further\nchanges occur in sample labels. We present the pseudocode in Algorithm 1.\nAlgorithm 1 Iterative MeanShift Clustering Algorithm\n1: Initialize all labels in the evaluation set to −1\n2: repeat\n3:\nPerform MeanShift clustering with radius R on all samples labeled −1\n4:\nAssign new labels to clustered samples\n5:\nfor each newly labeled sample i do\n6:\nCalculate distance disti to its cluster center\n7:\nif disti > R then\n8:\nReset label to −1\n9:\nend if\n10:\nend for\n11:\nfor each cluster do\n12:\nif number of samples in cluster < K then\n13:\nReset all samples in this cluster to −1\n14:\nend if\n15:\nend for\n16:\nRenumber all non-{−1} newly labeled samples to avoid overlap with old labels\n17: until no label changes\nFiltering Zero-performance Samples\nIn the evaluation set, there may exist a few extremely\ndifficult problems that require sufficient model parameters to emerge. All small models may fail to\nsolve these problems even after 100 evaluation attempts, resulting in difficulty feature vectors of all\nzeros. We refer to these as zero-performance samples. Their presence leads to two issues:\n1. Zero performance on small models does not necessarily indicate zero accuracy on large\nmodels. For these samples, we cannot estimate when emergence will occur or predict large\nmodel metrics.\n2. During clustering, they may be confused with other low-performing but non-zero samples.\nIncluding them in the same cluster would lower the expected accuracy of that cluster, leading\nto inaccurate fitting and extrapolation later.\nTherefore, we pre-filter these zero-performance samples before clustering, treating them as outliers\nthat do not participate in the clustering process. This approach eliminates the need to consider their\nmetrics under large models during subsequent extrapolation and prevents disruption to the clustering\nof normal difficult samples.\nA.2\nSmoothing Techniques\nHorizontal smoothing: adjacent checkpoint smoothing. Metric fluctuations of individual samples\nin downstream tasks are not solely due to limited sampling. Another potential factor is noise\nfrom uneven data distribution in recent training batches. Therefore, in addition to performing 100\nevaluations to mitigate sampling variance, we evaluated 100 times on each of the adjacent checkpoints\nbefore and after the selected model. We then averaged these accuracy expectation values across three\ncheckpoints, further reducing sampling variance while offsetting noise from uneven training data\ndistribution. This approach also reduces the number of zero-performance samples, further improving\nclustering and prediction effectiveness.\n16\n\nVertical smoothing. Each sample’s features represent the expected correct response rate across\nmodels of increasing size, forming a partially ordered sequence. However, the Euclidean distance\nused for measurement does not consider this sequential information. For example, if a cluster center\nhas a feature sequence of [0, 0, 0, 0.5], sample A with [0, 0, 0.2, 0.5] and sample B with [0.2, 0, 0, 0.5],\nsample A clearly fits the cluster better than sample B, yet their Euclidean distances are identical.\nNote that this smoothing method may not be effective for all downstream tasks. Our current\nobservations suggest that for freeform tasks with limited solution spaces (such as multiple choice,\nordering, or judgment questions in freeform format), once models learn to answer within the solution\nspace, their random guess metrics on the evaluation set will be non-zero, more significantly affected\nby recent training batch data and few-shot cases in prompts. In such cases, vertical smoothing is\nmore likely to bring positive benefits.\nIn our experiment, we only adopt horizontal smoothing, and leave vertical smoothing as an optional\nselection.\nB\nProof of Proposition\nWe use Proposition B.1 to derive scaling law for downstream task performance (Proposition B.2).\nProposition B.1 (Arithmetic-geometric mean difference). For any sequence of positive real numbers\n{xi}n\ni=1, let:\n• µa = 1\nn\nPn\ni=1 xi be the arithmetic mean;\n• µg = Qn\ni=1 x1/n\ni\nbe the geometric mean;\n• σ2 = 1\nn\nPn\ni=1(xi −µ)2 be the variance.\nThen the difference between the arithmetic mean and geometric mean can be estimated as:\n∆= µa −µg = 1\nn\nn\nX\ni=1\nxi −\n n\nY\ni=1\nxi\n! 1\nn\n= σ2\n2µa\n+ o(µa)\n(4)\nProof. Taking the logarithm of the geometric mean µg:\nlog(µg) = 1\nn\nn\nX\ni=1\nlog xi\n(5)\nUsing Taylor expansion of log x around µ:\nlog x = log µ + x −µ\nµ\n−(x −µ)2\n2µ2\n+ o\n\u0000(x −µ)2\u0001\n(6)\nWe can simplify:\nlog(GM) = 1\nn\nn\nX\ni=1\nlog xi\n= log µ + 1\nn\nn\nX\ni=1\n\u0012(xi −µa)\nµa\n−(xi −µa)2\n2µ2a\n\u0013\n+ o(µa)\n= log µ + 1\nµ\n \n1\nn\nn\nX\ni=1\nxi −µa\n!\n|\n{z\n}\nequal to 0\n+ 1\n2µ2a\n \n1\nn\nn\nX\ni=1\n(xi −µa)2\n!\n|\n{z\n}\nσ2\n+o(µa)\n= log µ −σ2\n2µ2 + o(µa)\n17\n\nTherefore:\nµa −µg = µa\n\u0012\n1 −exp\n\u0012\n−σ2\n2µ2\n\u0013\u0013\n+ o(µa)\n(7)\nWhen\nσ2\n2µ2 is small, this can be approximated as:\n∆≈σ2\n2µa\n(8)\nProposition B.2 (Scaling law for downstream task performance). Given a language model trained\nwith computational budget C, and a set of downstream tasks P, under the following assumptions:\n1. The relationship between the answer loss and compute follows a power law,\n1\nn\nP\n(q,ans) lossans(C) ∼aC−b + c;\n2. For tasks with a finite answer set, the model gives a random guess choice if it cannot truly\nsolve it;\n3. Task passrate equals the product of the predicted probability of each token pans =\nQ\nt∈ans p(t), which means that each task has unique answer and model output answer\nonly without thinking progress.\nThe expected accuracy on tasks P can be modeled as:\nEp[Acc(C)] = g + (1 −g)\n\u0012\ne−aC−b−c + σ2\n2µ\n\u0013\n+ o(µ)\n(9)\nwhere:\n• g represents the random guess performance floor;\n• a, b, c are positive constants;\n• µ =\n1\n#P\nP\n(q,ans)∈P lossans;\n• σ2 =\n1\n#P\nP(lossans_t −µ)2.\nProof. We first use assumption 3 to establish the relationship between model passrate and loss on a\ntask.\n−log(pans) = −log\n Y\nt∈ans\np(t)\n!\n= −\nX\nt∈ans\nlog(pt) = lossans\n(10)\nThen take the exponential of both sides, and then take the expectation with respect to different tasks\nin the evaluation set p = (q, ans) ∈P. We note that both pans and lossans are functions of C.\nEp[pans(C)] = Ep[exp(−lossans(C))]\n(11)\n= 1\nn\nX\n(q,ans)∈P\nexp(−lossans(C)).\n(12)\nWe can adopt Proposition B.1 to switch from arithmetic mean to geometric mean of loss, and apply\nthe power law assumption 1.\n1\nn\nX\n(q,anst)∈P\nexp(−lossans(C)) = exp\n\n−1\nn\nX\n(q,anst)∈P\nlossans(C)\n\n\n|\n{z\n}\nuse loss scaling law\n+ σ2\n2µ + o(µ)\n(13)\n= exp (−aC−b −c) + σ2\n2µ + o(µ)\n(14)\n18\n\nTable C1: Clustering performance on advanced task benchmarks (IAD: Intra-cluster Average Distance,\nOR: Outlier Rate)\nMethod\nTriviaQA\nAGIEval\nDROP\nMBPP\nIAD↓\nOR(%)\nIAD↓\nOR(%)\nIAD↓\nOR(%)\nIAD↓\nOR(%)\nK-Means\n0.4388\n-\n0.4572\n-\n0.5554\n-\n0.3383\n-\nDBScan\n0.7039\n6.38\n0.5591\n3.67\n0.6651\n11.08\n0.5060\n12.80\nMeanShift\n0.2521\n6.77\n0.2886\n2.99\n0.2507\n11.81\n0.2167\n15.60\nImproved-KMeans\n0.1239\n11.97\n0.1536\n7.60\n0.1428\n21.42\n0.1667\n19.40\nImproved-MeanShift\n0.1871\n11.54\n0.2100\n11.50\n0.1974\n19.88\n0.1745\n21.60\nTable C2: Prediction errors on advanced task benchmarks.\nMethod\nMean\nMax\nTriviaQA\nAGIEval\nDROP\nMBPP\nEE\nFE\nEE\nFE\nEE\nFE\nEE\nFE\nEE\nFE\nEE\nFE\nK-Means\n2.44\n3.36\n2.97\n8.99\n2.97\n2.46\n2.61\n2.68\n1.66\n1.64\n2.53\n2.67\nDBScan\n3.04\n2.79\n6.43\n4.36\n1.11\n0.81\n6.43\n6.27\n3.03\n2.66\n1.57\n1.41\nMeanShift\n3.21\n2.34\n4.18\n4.90\n3.64\n4.90\n2.63\n3.23\n4.18\n4.00\n2.40\n1.22\nImproved-KMeans\n3.38\n3.80\n5.96\n5.56\n1.18\n1.12\n5.96\n5.56\n3.99\n5.24\n2.39\n3.25\nImproved-MeanShift\n1.13\n1.61\n1.58\n2.38\n1.58\n1.64\n1.11\n2.38\n0.26\n0.23\n1.56\n2.19\nwhere n = #P, and µ, σ2 follow definitions in the proposition.\nFinally, we use assumption 2 to align the answer passrate and the accuracy metric. We can adopt the\nlaw of total expectation:\nEp[Acc(C)] = Pp(correct)E[Acc|correct] + Pp(incorrect)E[Acc|incorrect]\n(15)\nNote that Pp(correct) = Ep[pans(C)], E[Acc|correct] = 1 and Pp(incorrect) = 1 −Pp(correct).\nWe also define g as the random guess accuracy performance, thus we have E[Acc|incorrect] = g.\nTake these results into Eq. (15), and we have:\nEp[Acc(C)] = Ep[pans(C)] + (1 −Ep[pans(C)]) ∗g\n(16)\n= g + (1 −g)Ep[pans(C)]\n(17)\n= g + (1 −g)\n\u0012\ne−aC−b−c + σ2\n2µ\n\u0013\n+ o(µ)\n(18)\nProposition B.2 demonstrates that a metric of an evaluation set with similar difficulty features can be\neffectively modeled using the following formula:\nf(C) = g + (1 −g) exp (−aC−b −c)\n(19)\nC\nAdditional Ablation Studies\nC.1\nComparison of Clustering Methods on Extra Evaluation Sets.\nWe provide additional clustering evaluation results across more evaluation sets in Table C1 and Ta-\nble C2, which maintain consistency with the conclusions presented in the main text.\nC.2\nInterpolation Method\nTo evaluate different interpolation methods for prediction accuracy, we compared various mathemati-\ncal approaches. Our baseline method uses quartic polynomial interpolation, which we compare against\nseveral alternative approaches, including Cubic spline interpolation, Cubic polynomial interpolation,\nand Quintic polynomial interpolation\n19\n\nTable C3: Comparison of different interpolation methods across benchmarks.\nPrediction Error\nBBH\nMath\nMMLU-pro\nCubic Spline\n0.68\n1.31\n1.37\nCubic Polynomial\n3.38\n1.12\n1.35\nQuintic Polynomial\n0.18\n1.42\n1.36\nQuartic Polynomial\n1.77\n1.28\n1.35\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCluster Accuracy\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTotal Accuracy\nCubic Spline\nprediction: 0.8237\nSmall Models\nTarget Model\nAnchor Model\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCluster Accuracy\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTotal Accuracy\nCubic Polynomial\nPrediction\nSmall Models\nTarget Model\nAnchor Model\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCluster Accuracy\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTotal Accuracy\nQuintic Polynomial\nPrediction\nSmall Models\nTarget Model\nAnchor Model\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCluster Accuracy\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTotal Accuracy\nQuartic Polynomial\nPrediction\nSmall Models\nTarget Model\nAnchor Model\nFigure C1: Performance mapping with different interpolation methods on BBH evaluation set. The\ncubic spline is overfitted, and the cubic polynomial method is underfitted. Quartic polynomials and\nquintic polynomials are comparable while quartic polynomial has fewer parameters.\nThe comparative results across different benchmarks are shown in Table C3. We report the prediction\nerror between the real performance of a large model and the mapping result.\nFurthermore, in Fig. C1, we plot the mapping process using different interpolation formulas, where\nthe x-axis represents the predictable subset indices and the y-axis represents the full set indices. The\nred points are the numerical values to be fitted, green points represent predicted values, purple points\nrepresent anchor points and blue points show the actual performance.\nThe prediction performance shows certain robustness across different interpolation methods. We aim\nto use the simplest possible interpolation function while maintaining low prediction errors. Based\non the above results, we observe that the Cubic Polynomial shows larger prediction errors due to\nunderfitting on BBH, while the Cubic Spline exhibits some overfitting. Both Quartic Polynomial and\nQuintic Polynomial perform well, therefore we chose the Quartic Polynomial method as it requires\nfewer fitting parameters.\nD\nDifficulty Distribution of Predictable Subset\nWe analyze the proportion of predictable subset tasks across different difficulty levels. The difficulty\ndistributions of predictable subset versus complete sets for different evaluation benchmarks are\nillustrated in Fig. D2. We use the scores from the 12B model as the basis for difficulty classification.\nThe results show that MMLU-pro and GSM8k evaluation sets have larger proportions of predictable\nsubset, indicating that most questions in these datasets exhibit good performance scaling properties.\nIn contrast, many difficult questions with near-zero scores in the Math evaluation set fall outside the\npredictable subset, requiring adjustment during the mapping phase. Meanwhile, BBH shows consis-\ntent proportions of predictable subset across difficulty levels, as some of its questions demonstrate\noscillatory patterns with limited improvement despite increased computing.\nThe proportion of predictable subset can serve as a metric for assessing evaluation set quality.\nEvaluation sets with larger predictable subset yield more reliable experimental conclusions from\nsmaller models. When constructing evaluation sets, we recommend screening or supplementing\nunpredictable clusters and ensuring a minimum number of questions for each difficulty feature to\nreduce metric volatility.\nE\nLimitations\nInfluence of model structure and training configurations. Mixture-of-Experts (MoE) models\nexcel in training and inference cost, and are widely used in production. In this work, we reveal the\n20\n\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCluster Accuracy\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\nTotal Accuracy\nBBH\nFull Evalset\nExtrapolatable Subset\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCluster Accuracy\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\nTotal Accuracy\nMATH\nFull Evalset\nExtrapolatable Subset\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCluster Accuracy\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nTotal Accuracy\nMMLU-pro\nFull Evalset\nExtrapolatable Subset\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCluster Accuracy\n0\n50\n100\n150\n200\n250\n300\nTotal Accuracy\nGSM8K\nFull Evalset\nExtrapolatable Subset\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCluster Accuracy\n0\n500\n1000\n1500\n2000\nTotal Accuracy\nDROP\nFull Evalset\nExtrapolatable Subset\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCluster Accuracy\n0\n20\n40\n60\n80\n100\nTotal Accuracy\nMBPP\nFull Evalset\nExtrapolatable Subset\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCluster Accuracy\n0\n1000\n2000\n3000\n4000\n5000\nTotal Accuracy\nTriviaQA\nFull Evalset\nExtrapolatable Subset\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCluster Accuracy\n0\n200\n400\n600\n800\n1000\nTotal Accuracy\nAGIEval\nFull Evalset\nExtrapolatable Subset\nFigure D2: Difficulty distribution comparison on a 12B model between predictable subset and full\nevaluation set.\nperformance scaling on dense transformers, while prediction on MoE models is still underexplored.\nHowever, we believe that the proposed method is not significantly affected by the model architecture.\nIf we apply the complete pipeline to MoE models, we expect to achieve similar results. Besides, we\nonly study the pre-training performance prediction with a constant learning rate and do not cover\nthe impact of the annealing training. In this phase, higher-quality data is usually adopted, which can\nrapidly improve the model’s capabilities. As a result, performance prediction faces greater challenges.\nCategory of evaluation sets. The proposed Clustering-on-Difficulty method requires a sufficient\nnumber of test cases, as too few samples can lead to unstable cluster metrics and ineffective estimation.\nHowever, from an evaluation set design perspective, an evaluation set with good predictive properties\nenables more effective generalization from small-scale to large-scale models, thus providing better\nguidance for model iteration.\nFurthermore, for multiple-choice tasks, the model only needs to assign a higher probability to the\ncorrect option compared to others, creating a discrepancy between this metric and the model’s true\npassrate. Given that more evaluation sets are adopting the Chain-of-Thoughts prompts, we have not\nincluded multiple-choice tasks that only require option selection.\nChain-of-thought performance prediction. Proposition B.2 assumes that evaluation sets directly\nassess models’ ability to provide answers. However, increasingly more evaluations allow models to\nthink before providing answers. Recent works on inference time scaling [31, 4] further demonstrate\nthat for tasks involving mathematics, reasoning, and coding, training models to complete tasks\nthrough longer inference computation can significantly improve downstream task performance. In\ncases where the reasoning process or answers are not unique, the relationship between a model’s\nanswer loss and passrate on a task may not necessarily follow the exponential relationship between\nthe answer loss and the sample passrate. Although our approach maintains its prediction effectiveness\nin such situations, the theoretical explanation for these cases is insufficient. Therefore, we consider\nimproving prediction methods based on chain-of-thought characteristics and expanding theoretical\nfoundations as future work.\n21\n",
  "metadata": {
    "source_path": "papers/arxiv/Unveiling_Downstream_Performance_Scaling_of_LLMs_A_Clustering-Based\n__Perspective_56211b81d9bcbd29.pdf",
    "content_hash": "56211b81d9bcbd29aa3173d0a2c264947dd3a9eb47d40ba8483b896276fad037",
    "arxiv_id": null,
    "title": "Unveiling_Downstream_Performance_Scaling_of_LLMs_A_Clustering-Based\n__Perspective_56211b81d9bcbd29",
    "author": "",
    "creation_date": "D:20250225025604Z",
    "published": "2025-02-25T02:56:04",
    "pages": 21,
    "size": 2661101,
    "file_mtime": 1740470165.7978299
  }
}