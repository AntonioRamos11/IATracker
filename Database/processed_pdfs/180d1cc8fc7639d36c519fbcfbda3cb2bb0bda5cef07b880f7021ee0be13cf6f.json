{
  "text": "FACTR: Force-Attending Curriculum Training for\nContact-Rich Policy Learning\nJason Jingzhou Liu∗, Yulong Li∗, Kenneth Shaw, Tony Tao, Ruslan Salakhutdinov, Deepak Pathak\nCarnegie Mellon University\n∗Equal contribution\nFig. 1: FACTR. We present Force-Attending Curriculum Training (FACTR) – a system that leverages robot external joint torques for both\nteleoperation and improving policy generalization for complex contact-rich tasks. [Left] Our low-cost leader-follower setup employs actuated\nservo motors to enable force feedback in both the leader arm and gripper, improving teleoperation success rate, completion time, and ease of\nuse. [Right] FACTR’s behavior cloning policy utilizes robot force information to enhance performance and generalization across objects with\ndiverse geometries and textures in contact-rich tasks. Videos at https://jasonjzliu.com/factr/\nAbstract—Many contact-rich tasks humans perform, such\nas box pickup or rolling dough, rely on force feedback for\nreliable execution. However, this force information, which is\nreadily available in most robot arms, is not commonly used in\nteleoperation and policy learning. Consequently, robot behavior\nis often limited to quasi-static kinematic tasks that do not\nrequire intricate force-feedback. In this paper, we first present\na low-cost, intuitive, bilateral teleoperation setup that relays\nexternal forces of the follower arm back to the teacher arm,\nfacilitating data collection for complex, contact-rich tasks. We\nthen introduce FACTR, a policy learning method that employs\na curriculum which corrupts the visual input with decreasing\nintensity throughout training. The curriculum prevents our\ntransformer-based policy from over-fitting to the visual input\nand guides the policy to properly attend to the force modality.\nWe demonstrate that by fully utilizing the force information, our\nmethod significantly improves generalization to unseen objects\nby 43% compared to baseline approaches without a curriculum.\nVideo results and instructions at https://jasonjzliu.com/factr/\nI. INTRODUCTION\nContact-rich tasks are an integral part of daily life, from\nlifting a box and rolling dough to cracking an egg or opening a\ndoor. These tasks, while seemingly simple, involve a complex\ninterplay of forces and require precise adjustments based on\nforce feedback. Humans rely heavily on this force feedback\nto generalize across tasks and objects, adapting seamlessly to\nvariations in visual appearances and geometries. However, in\nrobot learning, force information remains underutilized, even\nthough it is readily available on many modern robotic arms,\nsuch as the Franka Panda and the KUKA LBR iiwa. Instead,\nmost data-driven methods, including those using Behavior\nCloning (BC), focus primarily on visual feedback for both data\ncollection and policy learning, overlooking the critical role of\nforce. This limited use of force information hinders the vision-\nonly policies’ ability to generalize to novel objects. For instance,\nin tasks like lifting up a box with two arms, the primary factor\ninfluencing the action is the object’s geometry, while attributes\nsuch as color or texture are irrelevant. In such cases, force\nfeedback provides a clear signal for mode switching, such\nas detecting when contact is established, which can facilitate\nobject generalization compared to relying solely on vision.\nOne of the main reasons for the under-utilization of force\nfeedback in robot learning is the lack of an intuitive and low-\ncost teleoperation system that can capture force feedback during\narXiv:2502.17432v1  [cs.RO]  24 Feb 2025\n\ndata collection itself. Recently, low-cost leader-follower systems\nhave become popular for teleoperation, offering intuitive control\nof robot arms by mirroring the joint movements of the leader\narm controlled by a teleoperator to the follower arm [31, 28].\nHowever, these systems are typically passive (leader arm\njoints are not actuated) and unilateral (the leader arm does\nnot receive information from the follower arm). This makes\nteleoperation difficult for dynamic, contact-rich tasks where\nprecise force adjustments are necessary [22]. To overcome this\nlimitation, we present a bilateral low-cost teleoperation system\nthat provides force feedback by actuating motors in the leader\narm joints based on external joint torques transmitted from the\nfollower arm (Fig. 1 Left). By actuating the motors, we also\nprovide active gravity compensation and resolve the kinematic\nredundancy due to the redundant degrees of freedom of the\narm. These enhancements improve the teleoperation experience,\nleading to a 64.7% increase in the task completion rate, a 37.4%\nreduction in completion time, and an 83.3% improvement in\nsubjective ease of use across four evaluated contact-rich tasks.\nThe second challenge lies in effectively incorporating robot\nforce information into policy learning. Although recent methods\nsuch as diffusion policy [4] and action chunking transformers\n[31] achieve impressive results for fine-grained manipulation,\nthey often fail to generalize to unseen objects with variations\nin object visual appearances and geometries. Humans, on the\nother hand, can disregard irrelevant visual details once contact\nis established and rely solely on force feedback to perform\ntasks such as lifting a box or rolling dough. Therefore, to\nimprove generalization, we seek to incorporate force input\ninto autonomous robot policies. However, making effective\nuse of force information in policy learning is challenging, as\npolicies often overfit to using visual modality [27], effectively\ndisregarding force data. This issue arises because contact force\nsignals are typically less discriminative, often remaining near\nzero for extended periods when the arm is not in contact with\nthe environment during an episode. Hence, without proper care\nduring training, policies tend to ignore force input and rely\nprimarily on visual information. We empirically analyze this\neffect in Sec. V-C and Fig. 9.\nTo mitigate this imbalance, we propose Force-Aware Curricu-\nlum Training (FACTR), a curriculum training strategy designed\nto improve the policy’s ability to effectively leverage force\ninformation. FACTR systematically reduces the reliance on\nvisual information during training by applying operators such\nas Gaussian blurring or downsampling with varying scales to\nvisual inputs. A scheduler gradually decreases the blurring\nscale and increases the fidelity of the visual inputs. Intuitively,\nthis approach encourages the policy to focus more on force\ninput during initial training phases and gradually balances\nforce with visual inputs as training progresses. We ground this\nintuition with a theoretical analysis on a simplified scenario\nthrough the framework of Neural Tangent Kernels [11]. We\nexplore FACTR in both the pixel space and latent space, testing\nvarious operators and scheduling strategies. Our experiments\nshow that FACTR improves the success rate for unseen objects\nby an average of 40.0% in four challenging contact-rich tasks\nFig. 2: Our low-cost bimanual teleoperation system with force-\nfeedback. The system features two actuated leader arms, two follower\narms with external joint torque sensors (such as the Franka Panda\nand the KUKA LBR iiwa), a front camera and two wrist cameras.\n(Fig. 1 Right) – box lifting, prehensile pivoting, fruit pick-\nand-place, and dough rolling – showcasing the efficacy of our\nforce-attending curriculum training.\nIn summary, our contributions are as follows.\n• Low-Cost Teleoperation with Force Feedback: We\ndesign a low-cost bilateral leader-follower teleoperation\nsystem with force feedback, gravity compensation, and\nredundancy resolution, demonstrating a 64.7% improve-\nment in task completion rate and an 83.3% enhancement\nin ease of use for teleoperation through a user study.\n• Force-Attending Curriculum Training: We propose\nFACTR, a curriculum training framework that better learns\nto use force feedback in policy learning and achieves better\ngeneralization capability to object visual appearances and\ngeometries. Evaluated on four challenging contact-rich\ntasks, FACTR improves the performance of autonomous\npolicies by 40.0% compared to policies with force\ninformation as part of input but trained without FACTR.\nII. RELATED WORKS\nA. Imitation Learning with Force\nImitation learning has recently experienced significant ad-\nvancements, driven by the development of more effective\nalgorithms that leverage demonstrations to train robotic poli-\ncies [17, 4]. Although traditional approaches primarily rely on\nvisual and joint position inputs, many real-world tasks require\nexplicit force feedback to improve stability, adaptability, and\nsafety [18, 9]. Recent work has applied learning methods to\ntrain with demonstrations that incorporate gripper force or\ntactile signals, resulting in policies capable of handling small,\nfragile objects and performing contact-intensive tasks such as\nvegetable peeling [15, 30, 14]. However, utilizing force data\nfrom the robot arms, such as joint torques, remains under-\nexplored. One approach involves using an end-effector force\nsensor to estimate compliance parameters or virtual position\ntargets through kinesthetic teaching and force tensors [10, 3].\nAnother method infers a 6D wrench for low-level control by\nintegrating torque sensing into a diffusion policy [29].\n\nHowever, naively incorporating force feedback into policy\nlearning can lead to overfitting to visual information, causing\nthe policy to disregard force input. FoAR [8] explicitly predicts\ncontact and non-contact phases to regulate the fusion of vision\nand force modalities, which requires additional data labeling.\nWe propose FACTR to effectively incorporate force and vision\ninput into policy through a curriculum, enabling policies to\nleverage force for improved object generalization.\nB. Low-Cost Teleoperation Systems with Force Feedback\nParallel to advances in imitation learning, significant efforts\nhave been made to collect low-cost and high-quality data with\nhand-held grippers [26, 5] or leader-follower systems [31, 28,\n24]. Hand-held grippers naturally provide force feedback to\nthe operator, but they do not directly record force data. Recent\nwork has added force sensors to hand-held grippers to address\nthis limitation [15]. However, hand-held grippers are in general\nlimited by the kinematic differences between humans and\nrobots, resulting in commands that might be unachievable for\nthe robots. Although the leader-follower systems are not prone\nto this limitation, they often lack force feedback, impairing\ntheir effectiveness in contact-rich tasks. Recently, Kobayashi et\nal. [13] implemented a bilateral leader-follower teleoperation\nsystem where in addition to the follower following the joint\npositions of the leader, the leader also gets an additional torque\nif there is a difference in its joint position from that of the\nfollower. However, when the follower arm is in motion without\ncontact, this system causes the operator to experience inertial,\nfrictional, and other dynamic forces of the follower, reducing\nthe ease of use and precision of the system [25]. Our approach\nintroduces an alternative bilateral teleoperation method by\nrelaying only external joint torques from the follower arm back\nto the leader arm, providing force feedback without impairing\noperational precision.\nIII. FACTR LOW-COST BILATERAL TELEOPERATION\nLeader-follower\nsystems,\nsuch\nas\nGELLO\n[28]\nor\nALOHA [31], offer a simple and cost-effective solution to\nteleoperation in manipulation tasks. These systems feature\nkinematically equivalent leader and follower arms, allowing\nintuitive control through joint space mapping, where the leader’s\njoint positions are mirrored as targets for the follower. This\nsetup lets users naturally feel the follower arms’ kinematic\nconstraints. However, most implementations lack force feed-\nback, preventing users from sensing the geometric constraints\nof the environment, which is crucial for teleoperating contact-\nrich tasks [22]. Instead, those leader arms are mostly passive,\nlacking active motor torque actuation, despite being equipped\nwith servo motors capable of actuation. Furthermore, the\nlack of active torque means the leader arms require external\nstructural frames and rubber bands or strings to achieve gravity\ncompensation, reducing portability [31].\nIn this paper, we aim to fully leverage the servo motors in the\nleader arm and gripper to achieve force-feedback enabled teleop-\neration with affordable hardware. Similarly to GELLO [28], our\nleader arms use off-the-shelf servos and 3D-printed components,\nforming a scaled-down but kinematically equivalent version of\nthe follower arms, as shown in Fig. 2. By actuating the servo\nmotors, we introduce force feedback, customizable redundancy\nresolution through nullspace projection, gravity and friction\ncompensation, and joint limit avoidance. These functionalities\naugment the teleoperation experience while still using low-cost\nhardware to provide functions that are usually only available\nwith much more expensive teleoperation devices. Please see\nAppendix VIII for a detailed Bill of Materials.\nA. Force Feedback\nForce feedback provides the operator with a tangible sense of\ninteraction with the environment, allowing more intuitive and\ndelicate manipulation, especially in contact-rich tasks or tasks\nwith limited visual feedback [22]. We implement a control law\nthat relays external joint torques sensed by the follower arm\nto the leader arm, allowing the operator to feel the physical\nconstraints experienced by the follower arm:\nτfeedback = µfKf,pτext −Kf,d ˙q\n(1)\nwhere µf is a scalar constant, τext is the external joint torque\nsensed by the follower arm, Kf,p and Kf,d are the PD gains\nfor the force feedback, respectively. Here, Kf,p is calculated as\nthe ratio between the maximum torque of the leader and that of\nthe follower, and Kf,d ˙q helps reduce oscillations in the leader\narm when the follower arm is in contact. We note that τext is\na readily available measurement in various collaborative robot\nmanipulators, such as the Franka Panda and the KUKA LBR\niiwa. In particular, we implement mediated force feedback by\nscaling down τext with µf, which has been shown to improve\nthe accuracy of the operation while reducing the cognitive\nload of the operator [32]. Furthermore, we highlight that our\nimplementation only transmits external forces from the follower\nto the leader; as a result, the operator does not experience the\ninternal friction and inertia of the follower arm during motion,\nproviding a clearer perception of the environment [25].\nIn addition, we implement force feedback for the parallel-\njaw gripper. Since our servo-based gripper does not contain\nan external force sensor, we utilize the present current reading\nof the gripper servo to provide force feedback as follows:\nτh,t = α(−khIg,t) + (1 −α)τh,t−1\n(2)\nwhere τh,t is the force feedback torque sent to the gripper leader\ndevice, Ig,t is the present current reading from the follower\ngripper, and α is the smoothing factor for the EMA filter. Our\nsystem sets α = 0.1 which provides a good user experience.\nB. Customizable Redundancy Resolution\nFor kinematic redundant manipulators, without regulating\nthe joint space, the manipulator tends to drift into unde-\nsirable configurations under the influence of gravity during\nteleoperation. Approaches like Gello [28] rely on mechan-\nical components, such as springs, to regularize the joint\nspace. However, these components introduce non-uniform,\nconfiguration-dependent wrenches at the end-effector, resulting\nin an unintuitive teleoperation experience. In addition, using\n\nmechanical joint regularization effectively prevents the user\nfrom setting custom joint regularization targets for redundancy\nresolution. In confined-space manipulation settings, the inability\nto control the joint regularization target can impair the arm’s\nreachability, as demonstrated in Fig. 3.\nIn contrast, our proposed method leverages the following\nnull-space projection control law to regulate joint positions [12],\nwhich stabilizes the joint-space at any user-defined desirable\nposture without imposing additional end-effector wrenches,\nregardless of the arm’s configuration:\nτnull =\n\u0000I −J†J\n\u0001\n(−Kn,p (q −qrest) −Kn,d ˙q)\n(3)\nwhere J is the manipulator Jacobian matrix, qrest is a user-\ndefined resting posture configuration, Kn,p and Kn,d are the\nPD gains for the null space projection. Note that\n\u0000I −J†J\n\u0001\nis\nthe null-space projector.\nC. Gravity Compensation\nTo ensure the leader arms remain stationary, allowing the\nuser to easily pause teleoperation, we implement gravity\ncompensation. This is achieved by modeling the dynamics\nof the leader arm and computing the joint torques required to\ncounteract dynamic forces using the recursive Newton-Euler\nalgorithm (RNEA) for real-time inverse dynamics [16].\nτgrav = M(q)¨q + C(q, ˙q)˙q + g(q) = RNEA(q, ˙q, ¨q)\n(4)\nwhere M(q) is the mass (or inertia) matrix, C(q, ˙q) is the\nCoriolis and centrifugal matrix, and g(q) is the gravity vector.\nD. Additional Compensation and Controls\nTo reduce the perceived friction in the leader arm during tele-\noperation, our system provides friction compensation τfriction.\nFurthermore, since the leader arm joints lack mechanical joint\nlimits, we implement an artificial potential based control law to\nprevent users from exceeding joint limits of the follower arm in\norder to respect the workspace of the follower arm. Finally, for\nbi-manual follower arms, the system uses Riemannian Motion\nPolicies [23] for dynamic obstacle avoidance between the two\nfollower arms. Please refer to Appendix IX for more details.\nE. Overall Control Law for the Leader Arm\nIn summary, the control torques are defined as follows:\n• τfeedback relays external forces from the follower arm\nback to the leader arm, allowing the operator to sense the\ngeometric constraints of the environment.\n• τnull resolves kinematic redundancy by regulating the\njoints at a user-defined rest posture in the null-space.\n• τgrav provides gravity compensation for the leader arm.\n• τfriction compensates for the leader arm joint frictions to\nenable smoother teleoperation.\n• τlimit prevents the joints of the leader arm from violating\nthe joint position limits of the follower arm.\nThe resulting combined torque applied to the servo motors\nof the leader arm is defined as follows:\nτ = τfeedback + τnull + τgrav + τfriction + τlimit\n(5)\nFig. 3: Customizable Joint Regularization [Left] Without the\nflexibility to define the resting joint configuration qrest, the arm’s\nreachability is restricted, leading to collisions in confined spaces.\n[Right] Our leader arm allows the user to define custom resting joint\nqrest, helping the follower arm reach targets in confined spaces.\nIV. FACTR: FORCE-AWARE CURRICULUM TRAINING\nNaively incorporating robot force data into policy learning\ndoes not necessarily ensure policy improvement. Contact force\nsignals often provide limited discriminative information for\nthe policy, as it remains close to zero for significant periods\nwhen the arm is not interacting with the environment during an\nepisode. As a result, the policy tends to disregard force input\nand rely predominantly on visual information, as empirically\nanalyzed in Sec. V-C and Fig. 9.\nTo fully leverage the robot force data collected from our\nteleoperation system, we introduce Force-Aware Curriculum\nTraining (FACTR), a training strategy designed to effectively\nintegrate force information into policy learning. FACTR applies\noperators like Gaussian blur or downsampling to corrupt visual\ninformation, where the amount of visual corruption decreases\nthroughout training. The curriculum intuitively encourages\ncontribution from the force modality at the start of training. We\nground this intuition with a theoretical analysis on a simplified\nscenario through the framework of Neural Tangent Kernels [11].\nIn this section, we first present the base policy model used for\nlearning from teleoperated demonstrations, and then motivate\nand describe FACTR, our curriculum training approach. Our\noverall method is summarized in Algorithm 1 and Fig. 4.\nA. Problem Statement and Base Model\nWe consider a policy πθ(· | ·) that produces a chunk of\nfuture actions of length k ˆqt:t+k (joint positions) given (i) a\nvisual observation It (image at time t), and (ii) an external\njoint torque reading τt. Our goal is to learn πθ via behavior\ncloning (BC) from a dataset of expert trajectories D. Each\ntrajectory in D comprises tuples\n\u0000It, τt, qt\n\u0001\n, where qt is the\nground-truth (expert) joint position target at time t. We let\nˆqt:t+k be the predicted future joint position targets over the\nnext k time steps. The loss is defined by:\nL = MSE\n\u0000ˆqt:t+k, qt:t+k\n\u0001\n,\n(6)\nwhere qt:t+k are the expert’s future joint position targets and\nˆqt:t+k are the policy’s predictions.\n\n…\nAction\nTransformer\nπθ\nVision Encoder\nfϕ\nExternal \nJoint Torque\n𝜏\nForce Encoder\n𝑔ψ\n…\nPixel Space Curriculum\n…\nLatent Space Curriculum\n*\n*\nn = Max Iter\nn = 0\nOR\nn = Max Iter\nn = 0\nσn\np\nσn\nl\nqt:t+k\n…\n…\n…\n…\nFig. 4: FACTR allows our policy to better integrate force information without overfitting to visual information, resulting in better generalization\nto objects with unseen visual appearances and geometries. Our policy takes as inputs RGB images I and external joint torque τ, which are\nthen tokenized by a vision encoder and a force encoder before fed into an action transformer to regress joint position targets qt:t+k. FACTR\napplies a blurring operator of scale σn in either pixel or latent space, initialized at a large value then gradually decreased through the training.\nOur policy πθ is based on an encoder-decoder transformer\nthat integrates vision and force modalities. Visual observations\nand force readings are converted into tokens, fed to the encoder,\nthen decoded into action tokens through cross attention.\nA pre-trained vision transformer (ViT) [7, 6] is used to\nencode an input image It into a sequence of vision tokens\nzV\nt ∈RMv×d for some number of tokens Mv and embedding\ndimension d. An MLP-based force encoder is applied to the\njoint torque τt, resulting in a single force token: zF\nt\n∈R1×d.\nThe tokens are concatenated to form the model input:\nXt =\n\u0002\nzV\nt ; zF\nt\n\u0003\n∈R(Mv+1)×d.\nThen, a transformer encoder Enc processes Xt via multiple\nself-attention and feed-forward layers:\nHE\nt\n= Enc\n\u0000Xt\n\u0001\n∈R(Mv+1)×d.\nThis yields the encoded vision and force tokens.\nFor the decoder, we introduce k action tokens, A ∈Rk×d.\nA transformer decoder Dec refines these tokens through self-\nattention and cross-attention to HE\nt :\nHD\nt\n= Dec\n\u0000A, HE\nt\n\u0001\n.\nDuring cross attention, each action token attends to both vision\nand force representations. If we split HE\nt into its vision (V)\nand force (F) parts, the cross-attention weights for each layer\nl can be decomposed as follows. For simplicity of notation,\nassume these weights are already averaged over multiple heads:\nFor the vision part:\nα(l)\nV\n= softmax\n\u0010\n(A(l)WQ(l)) (HE(l)\nt,V WK(l))⊤/\n√\nd\n\u0011\n,\nFor the force part:\nα(l)\nF\n= softmax\n\u0010\n(A(l)WQ(l)) (HE(l)\nt,F WK(l))⊤/\n√\nd\n\u0011\n.\nThese α(l)\nV and α(l)\nF measure how strongly each action token\nattends to vision vs. force tokens at layer l, and will be the\nmain source of analysis in Sec. V-C.\nFinally, we project the decoder output HD\nt to action space,\nwhich represents joint position targets for the follower arm:\nˆqt:t+k = MLP\n\u0000HD\nt\n\u0001\n∈Rl×da.\nwhere da is the dimension of the action space. Substituting\nˆqt:t+k into Eq. 6 gives the full BC objective. Please see\nAppendix X for the detailed policy architecture and training\nhyperparameters.\nB. Force-Aware Curriculum\nThrough experiments, as shown in Sec. V-C and Fig. 9,\nwe found that naively concatenating force data to the policy\nobservation during training often results in policies that neglect\nforce input, failing to leverage force input to the fullest extent.\nTo address this, we employ a curriculum that gradually unveils\ndetailed visual information, encouraging the model to learn\nto utilize force first. Specifically, we define two operators:\nβP (I, σn) for the pixel space, and βL(z, σn) for the latent\nspace, where σn is a scale parameter (e.g. the standard deviation\nof a Gaussian kernel or the kernel size of a max pooling\noperator) that is updated over the course of training for N\ntotal gradient steps. During training, we apply the pixel-space\noperator βP to image It or βL to visual latent tokens zV\nt .\nIntuitively, the operators make visual inputs or latent tokens\nclose in the metric space, thus encouraging more contribution\nfrom the force modality, particularly at the start of the training.\nConsider the limit σ →∞, each visual input converges to\n\nAlgorithm 1 Force-Attending Curriculum Training (FACTR)\n1: Given: Expert dataset D; action chunking size k; total training\nsteps N; Pixel-space operator βP (I, σ); latent-space operator\nβL(z, σ); Scheduler defining σn for n = 1 . . . N\n2: Initialize pre-trained ViT fϕ, force MLP encoder gψ, and action-\nchunking transformer πθ\n3: for iteration n = 1 . . . N do\n4:\nSample\n\u0000It, τt, qt\n\u0001\nfrom D\n5:\nσn ←Scheduler(n)\n// get current scale\n6:\nif pixel-space curriculum then\n7:\nIt ←βP\n\u0000It, σn\n\u0001\n8:\nend if\n9:\nzV\nt\n←fϕ(It)\n// vision tokens\n10:\nif latent-space curriculum then\n11:\nzV\nt\n←βL\n\u0000zV\nt , σn\n\u0001\n12:\nend if\n13:\nzF\nt\n←gψ(τt)\n// force token\n14:\nˆqt:t+k ←πθ\n\u0000zV\nt , zF\nt\n\u0001\n15:\nL = MSE\n\u0000ˆqt:t+k, qt:t+k\n\u0001\n16:\nUpdate ϕ, ψ, θ using ADAM\n17: end for\napproximately the same tensor. Hence, the model can only\nlearn a single global output for all visual inputs. Thus, at the\nearly stage of the curriculum, the gradient updates focus more\non using the force information and updating the force encoder\nto maximally differentiate between inputs.\nC. Curriculum Operators\nWe consider two types of operators: Gaussian blur and\ndownsampling.\nFor the Gaussian blur, we define the 2D kernel Gσ as:\nGσ(x, y) =\n1\n2πσ2 exp\n\u0012\n−x2 + y2\n2σ2\n\u0013\nThe operator βP (I, σ) applies this kernel using the 2D\nconvolution operator ∗:\nβP (I, σ) = I ∗Gσ\nFor the 1D Gaussian blur, the kernel gσ is defined as:\ngσ(x) =\n1\n√\n2πσ exp\n\u0012\n−x2\n2σ2\n\u0013\nThe operator βL(zV , σ) similarily applies this kernel using 1D\nconvolution:\nβL(zV , σ) = zV ∗gσ\nFor downsampling, we use MaxPool followed by nearest\ninterpolation. In 2D, the pixel-space operator βP (I) is:\nβP (I) = NearestInterp(MaxPool2D(I))\nIn 1D, the latent-space operator βI(zV ) is the same except\nthat a MaxPool1D is used.\nBy gradually reducing σn, the curriculum ensures that the\nmodel focuses first on force tokens, and then incorporates visual\ninformation in the later stage of the training. This produces a\npolicy πθ that more robustly fuses force and vision for control,\nalleviating the issue of overfitting to the vision modality.\nTheoretical Analysis:\nWe analyze the effects of Gaussian blur\nas an example of a curriculum operator through the framework\nof Neural Tangent Kernels (NTK) [11]. Although we consider\na simple two-layer model here, the intuition applies to more\ncomplex architectures like vision transformers (ViTs), which\nhave a more sophisticated NTK [2]. The formal theoretical\nanalysis is presented in Appendix VII.\nD. Curriculum Schedulers\nOver the course of training (indexed by n = 1, . . . , N), we\nadjust σn via a scheduler to control the information released\nfrom the visual branch. Given a initial scale σ0, we consider\nthe following schedulers:\nDecay Type\nScheduler Equation\nConstant\nσn = σ0\nLinear\nσn = σ0\n\u00001 −n\nN\n\u0001\nCosine\nσn = σ0\n2\n\u0010\n1 + cos\n\u0010\nnπ\nN\n\u0011\u0011\nExponential\nσn = σ0 · αn,\nα > 1\nStep\nσn = σ0\n\u0010\n1 −\n1\ndsteps ⌊\nn\nN/dsteps ⌋\n\u0011\n,\ndsteps > 1\nFurthermore, we warm-up the curriculum by fixing the scale\nto σ0 for certain gradient steps, and adjust the decay formula\nto account for this duration. The rationale behind this step is to\nwarm-up the randomly initialized force encoder with relatively\nlow visual information.\nV. EVALUATION\nA. Experimental Setup\nWe setup four contact-rich tasks, which are illustrated in\nFig. 5 along with the training and testing objects of various\nshapes and visual appearances. For all tasks, we use Franka\nPanda arm(s) with OpenManipulator-X gripper(s). Each task\nuses either a front ZED2 camera or wrist cameras mounted\nnear the grippers with RGB observations. We describe the tasks\ndetails and the success criteria below.\n• Box Lift: A bi-manual task where two arms lift a box and\nbalance in the air for at least two seconds, using the front\ncamera and external joint torque from the arms.\n• Non-Prehensile Pivot: The robot flips an item by pivoting\nit against the corner of a fixture until the item is rotated\nby 90◦and can stand stably, using the front camera and\nexternal joint torque from the arm.\n• Fruit Pick and Place: The robot grasps a soft and delicate\nfruit and places it in a bowl, using the wrist camera and\nthe external joint torque of the gripper.\n• Rolling Dough: The robot continuously rolls the dough to\nshape it into a cylinder for at least 8 seconds, using the\nfront camera and the external joint torque of the arm.\nB. Teleoperation Evaluation\nWe compare our leader-follower teleoperation system, which\nincludes our leader arm with force feedback, gravity compen-\nsation, and redundancy resolution, to an un-actuated leader-\n\nFig. 5: Tasks. We evaluate our leader-follower teleoperation system\nand autonomous policies trained with FACTR on four contact-rich\ntasks. These tasks are challenging as they require the robot to perceive\nand respond to the force feedback as it manipulates objects with unseen\nvisual appearances and geometries.\nfollower baseline system with mechanical joint regulation,\nsimilar to [28]. We summarize our results in Fig. 7.\nOur experiments show that our system allows users to\ncomplete tasks with 64.7% higher task completion rate, 37.4%\nreduced completion time, and 83.3% improvement in the\nsubjective ease of use metrics.\nWe observe that for tasks that require continuous contact\nbetween the arm and an object, such as non-prehensile pivoting\nand bimanual box lifting, the un-actuated teleoperation system\noften causes the follower arm to lose contact with the object.\nThis occurs because of the absence of force feedback, which\nprevents the user from perceiving the environment’s geometric\nconstraints through the leader arms. As a result, maintaining\ncontinuous contact with the object becomes challenging.\nFor the un-actuated system, the follower arm frequently\nexceeds its joint velocity limits when moving under continuous\ncontact. This occurs because the operator can easily maneuver\nthe leader arms in ways that cause significant deviations\nbetween the leader and follower joint positions, especially\nwhen the follower arm is in contact with the environment.\nWhen contact is lost, the resulting large joint-space error causes\nthe PID controller to generate large torques, causing abrupt\nmovements that exceed the velocity limits. On the other hand,\nour system’s force feedback renders geometric constraints of\nthe environment for the operator through the leader arms,\nFACTR\nACT (Vision+Force)\nACT (Vision-Only)\nTrain Objects\nTest Objects\nSuccess Rate\nBox Lift\nPivot\nFruit Pick-Place\nRolling Dough\nBox Lift\nPivot\nFruit Pick-Place\nRolling Dough\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\nSuccess Rate\nFig. 6: FACTR leads to better object generalization.\npreventing the operator from moving the leader arms too far\naway from the follower arms during environment contacts.\nC. Policy Evaluation\nQuestions. In our real-world evaluation, we seek to address\nthe following research questions regarding FACTR:\n• How does FACTR perform compared to baseline ap-\nproaches that do not use force feedback and ones that use\nforce feedback without FACTR?\n• How do different curriculum parameters affect policy\nperformance?\nTraining and Evaluation Protocol. We collected 50 demon-\nstrations with our teleoperation system. We trained each method\nwith the same hyperparameters, where details can be found in\nthe Appendix X. We compare the following methods:\n• ACT (Vision-Only) [31]: Action Chunking Transformer\nwhich only takes in visual observation.\n• ACT (Vision+Force): Action Chunking Transformer which\ntakes in both visual and force observation, but trained\nwithout a curriculum.\n• FACTR (Ours): Action Chunking Transformer trained with\nForce-Attending Curriculum. For each task, we train a\nlatent space curriculum with the Guassian Blur operator\nand linear scheduler. We discuss more detailed ablations\non the curriculum in Sec. V-D.\nFor each object in each task, we evaluated 5-10 trials. We\npresent the average success rate for training and testing objects,\nrespectively. Detailed evaluation results for each object can be\nfound in the Appendix XI.\nFACTR leads to better generalization. We present our\nmain quantitative results in Fig. 6. All the policies perform\nsimilarly on the train objects for most tasks, except for the\nrolling dough task, where the vision-only policy smashes the\ndough without any rolling actions and fails completely. Note\nthat the visual observations are hard to distinguish during\nthe oscillatory rolling motions, while the force signals form\n\na) Completion Rate ↑\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\nb) Completion Time (sec) ↓\n25\n0\n5\n10\n15\n20\nc) Ease of Use ↑\n5\n0\n1\n2\n3\n4\nOurs\nUn-Actuated Leader-Follower\nBox Lift\nPivot\nPick-Place Rolling\nBox Lift\nPivot\nPick-Place Rolling\nBox Lift\nPivot\nPick-Place Rolling\nFig. 7: User study. FACTR teleoperation system allows users to complete tasks with significantly higher success rate, using less time, and\nthey subjectively found our system to be easier to use.\nFig. 8: We visualize the external joint torque norm of the Franka arm\nfor a collected trajectory. Blue highlights indicate pre-contact phases,\nwhile purple and green mark torque peaks and troughs at the dough’s\nleft and right ends, respectively. The oscillatory torque pattern helps\nthe policy distinguish observations despite similar visual inputs.\na corresponding oscillatory pattern, as shown in Fig. 8; this\ndistinctive torque pattern helps policies with force input to\ncomplete the task.\nFor the test objects, the vision-only policy achieves a success\nrate of 21.3% on average, which is significantly worse than\npolicies incorporating force. Without a curriculum, policies\nnaively incorporating force achieve a success rate of 61.2%,\nwhile FACTR achieves a success rate of 87.5%, which shows\nthat FACTR leads to significantly better generalization to novel\nobjects. We hypothesize that the force information provides\nimportant signals for mode switching at moments such as when\nthe robots get into contact with the box in the lifting task and\nwhen the object is grasped in the fruit pickup task.\nPolicies with FACTR learns to identify mode switching.\nTo better understand the policies trained with FACTR. We\nvisualize the attention behavior during policy training and\ninference. Specifically, we visualize the cross attention of the\naction tokens to the memory tokens denoted as α(1)\nV\nand α(1)\nF\nfor the first layer of the decoder, where α(1)\nV\nand α(1)\nF\nare\ndefined in Sec. IV.\nDuring policy rollout, we visualize the average cross attention\nof the action tokens to the force or vision tokens of the first\ndecoder layer as shown in Fig. 9. FACTR learns to attend to\nforce more during task execution. For example, in the box\nlifting task, attention to force outweighs that of vision as the\narms contact the box, signaling a mode switch. While without\nthe curriculum, the policy does not pay enough attention to\nforce, and either fails to lift or balance the novel boxes.\nFACTR leads to better recovery behavior. Another notable\nobservation is that FACTR also facilitates recovery behavior.\nSpecifically, we evaluate the box-lifting task with five trials\nper object. A trial begins when the policy successfully lifts\nthe box for the first time; we then knock the box down and\nassess the second attempt. As shown in Table I, all policies\nmaintain nearly 100% recovery success on training objects.\nHowever, for test objects, the vision-only policy’s success rate\ndrops significantly from 31.7% on the first attempt to 13.3% on\nthe second. In contrast, force-aware policies maintain similar\nsuccess rates across both attempts.\nWe observe that vision-only policies often remain static after\nthe box is knocked down, failing to retry. We hypothesize that\nthis occurs because the vision-only policy overfits to training\nscenarios, making it unresponsive to unseen objects outside its\ntraining distribution. In contrast, FACTR policies detect loss of\ncontact through external joint torque readings, which revert to\npre-lift values when the object is dropped. Since our FACTR\npolicies effectively attend to force input, they successfully\nrecover to a pre-lift state and attempt the task again.\nTrain Objects\nTest Objects\nACT (Vision-Only)\n4/5\n4/30\nACT (Vision+Force)\n5/5\n16/30\nFACTR\n3/3\n27/30\nTABLE I: Evaluation of recovery behaviors for box lifting.\nD. Ablations on Curriculum\nTo further validate the significance of a curriculum, we\ntrained models with fixed σn across training. Moreover, to\nablate on pixel space and latent space curriculum, and different\nscheduler and operator choices, we train policies on different\n\nFACTR (Ours)\nAttention\nNo FACTR (Baseline)\nAttention\nTime Steps\nTime Steps\n0\n1\n0\n1\nForce\nVision\nFig. 9: Policies trained with FACTR learns to identify mode switching. We visualize the average cross attention of the action tokens to\nthe force or vision tokens of the first decoder layer during policy rollout. [Left] Without the curriculum, the policy does not pay enough\nattention to force, and either fails to lift or balance the novel boxes. [Right] FACTR learns to attend to force more to complete the task. For\nexample, in the box lifting task, attention to force outweighs that of vision as the arms contact the box, signaling a mode switch.\ncombination of these parameters. We choose the task of\npivoting, one of the hardest tasks from our task suite, for\nthe ablations. We evaluate only on the five test objects for five\ntrials each, since they are more indicative of policy performance\nthan train objects. The results are presented in TABLE II.\nPixel Space\nLatent Space\nBlur\nDownsample\nBlur\nDownsample\nConstant\n16/25\n15/25\n17/25\n16/25\nLinear\n19/25\n18/25\n19/25\n18/25\nCosine\n20/25\n19/25\n17/25\n19/25\nExp\n19/25\n21/25\n20/25\n19/25\nStep\n19/25\n18/25\n20/25\n19/25\nTABLE II: Curriculum ablation.\nFixed-Scale Operator vs. Curriculum. We found that\nperformance with a curriculum of decaying smoothing performs\nbetter than a fixed curriculum across all tasks. We hypothesize\nthat to enable better performance, the final policy needs to\ntake in the fully unblurred vision information. Through the\ncurriculum, a policy gets to gradually adapt to unblurred images.\nOn the other hand, with fixed smoothing, even though policy\nmay not overfit to visual information, it cannot extract the\nnecessary details from unblurred vision to complete the tasks.\nComparisons with other scheduler parameters. We further\ncompare policies trained with either pixel space or latent space,\ntwo operators (Gaussian blur and downsample) as defined in\nSec. IV-C, and four schedulers (linear, cosine, exponential,\nand step) as defined in Sec. IV-D. However, we do not find a\nuniform advantage or disadvantage for any set of parameters.\nThe results suggest that FACTR is relatively robust to different\nsets of curriculum parameters.\nVI. CONCLUSION AND LIMITATIONS\nWe introduced FACTR, a curriculum approach to train force-\nbased policies to improve performance and object general-\nization in contact-rich tasks. FACTR leverages a blurring\noperator with decreasing scales on the visual information\nthroughout training. This encourages the policy to leverage\nforce input at the beginning stages of training, preventing\nthe problem where the policy overfits to visual input and\nthus neglects force input. This approach was demonstrated\nthrough a series of experiments on the following tasks: box\nlifting, non-prehensile pivoting, fruit pick-and-place, and rolling\ndough, where FACTR exhibits significant improvements in\ntask completion rates and generalization to unseen object\nappearances and geometries. Additionally, our teleoperation\nsystem, which includes an actuated leader arm for force\nfeedback and gravity compensation, was shown to provide\na more intuitive user experience, as evidenced by higher task\ncompletion rates and user satisfaction in our studies.\nWhile FACTR demonstrates significant improvements in\nforce-based policy learning for contact-rich tasks, it has limita-\ntions. First, the precision of the external joint torque sensors\nin our follower arm is limited. This limitation can particularly\naffect tasks that involve subtle force adjustments during fine-\ngrained manipulation since the torque readings can be too noisy\nto be used effectively. Future work could explore integrating\nhigh-resolution tactile sensors or haptic gloves to enhance\nfeedback precision and improve overall system performance.\nSecond, our approach assumes the availability of external joint\ntorque sensors in the follower arms. Future work can explore\nadapting our system for an arm mounted with an end-effector\nforce-torque sensor. Third, the effectiveness of our curriculum\nlearning approach can be influenced by several hyperparameters,\nsuch as the choice of the blurring operator and scheduling\nstrategies. These parameters can be highly task-dependent,\nrequiring extensive tuning for different applications. Developing\nadaptive or self-tuning curriculum strategies could help mitigate\nthis issue by dynamically adjusting hyperparameters based on\ntask-specific requirements. Addressing these limitations could\nfurther enhance FACTR’s applicability and robustness across\na broader range of contact-rich manipulation tasks.\n\nACKNOWLEDGMENTS\nWe thank Arthur Allshire, Andrew Wang, Mohan Kumar\nSrirama, Ritvik Singh for discussions about the paper. We also\nthank Tiffany Tse, Ray Liu, Sri Anumakonda, Sheqi Zhang\nwith teleoperation. This work is supported in part by ONR\nMURI N00014-22-1-2773, ONR MURI N00014-24-1-2748\nand AFOSR FA9550-23-1-0747.\nREFERENCES\n[1] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R\nSalakhutdinov, and Ruosong Wang. On exact computation\nwith an infinitely wide neural net. Advances in neural\ninformation processing systems, 32, 2019.\n[2] Enric Boix-Adsera, Omid Saremi, Emmanuel Abbe, Samy\nBengio, Etai Littwin, and Joshua Susskind. When can\ntransformers reason with abstract symbols? arXiv preprint\narXiv:2310.09753, 2023.\n[3] Claire Chen, Zhongchun Yu, Hojung Choi, Mark\nCutkosky, and Jeannette Bohg. Dexforce: Extracting force-\ninformed actions from kinesthetic demonstrations for\ndexterous manipulation. arXiv preprint arXiv:2501.10356,\n2025.\n[4] Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric\nCousineau, Benjamin Burchfiel, and Shuran Song. Dif-\nfusion policy: Visuomotor policy learning via action\ndiffusion. arXiv preprint arXiv:2303.04137, 2023.\n[5] Cheng Chi, Zhenjia Xu, Chuer Pan, Eric Cousineau,\nBenjamin Burchfiel, Siyuan Feng, Russ Tedrake, and\nShuran Song. Universal manipulation interface: In-the-\nwild robot teaching without in-the-wild robots. arXiv\npreprint arXiv:2402.10329, 2024.\n[6] Sudeep Dasari, Mohan Kumar Srirama, Unnat Jain, and\nAbhinav Gupta. An unbiased look at datasets for visuo-\nmotor pre-training. In Conference on Robot Learning,\npages 1183–1198. PMLR, 2023.\n[7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, et al. An image is worth 16x16 words:\nTransformers for image recognition at scale. In Interna-\ntional Conference on Learning Representations, 2020.\n[8] Zihao He, Hongjie Fang, Jingjing Chen, Hao-Shu Fang,\nand Cewu Lu.\nFoar: Force-aware reactive policy\nfor contact-rich robotic manipulation.\narXiv preprint\narXiv:2411.15753, 2024.\n[9] Neville Hogan.\nImpedance control: An approach to\nmanipulation. In 1984 American control conference, pages\n304–313. IEEE, 1984.\n[10] Yifan Hou, Zeyi Liu, Cheng Chi, Eric Cousineau, Naveen\nKuppuswamy, Siyuan Feng, Benjamin Burchfiel, and\nShuran Song.\nAdaptive compliance policy: Learning\napproximate compliance for diffusion guided control.\narXiv preprint arXiv:2410.09309, 2024.\n[11] Arthur Jacot, Franck Gabriel, and Cl´ement Hongler.\nNeural tangent kernel: Convergence and generalization\nin neural networks.\nAdvances in neural information\nprocessing systems, 31, 2018.\n[12] Oussama Khatib. A unified approach for motion and\nforce control of robot manipulators: The operational space\nformulation. IEEE Journal on Robotics and Automation,\n3(1):43–53, 1987.\n[13] Masato Kobayashi, Thanpimon Buamanee, and Takumi\nKobayashi. Alpha-α and bi-act are all you need: Im-\nportance of position and force information control for\nimitation learning of unimanual and bimanual robotic\nmanipulation with low-cost system.\narXiv preprint\narXiv:2411.09942, 2024.\n[14] Kelin Li, Shubham M Wagh, Nitish Sharma, Saksham\nBhadani, Wei Chen, Chang Liu, and Petar Kormushev.\nHaptic-act: Bridging human intuition with compliant\nrobotic manipulation via immersive vr. arXiv preprint\narXiv:2409.11925, 2024.\n[15] Wenhai Liu, Junbo Wang, Yiming Wang, Weiming Wang,\nand Cewu Lu.\nForcemimic: Force-centric imitation\nlearning with force-motion capture system for contact-rich\nmanipulation. arXiv preprint arXiv:2410.07554, 2024.\n[16] Kevin M. Lynch and Frank C. Park. Modern Robotics:\nMechanics, Planning, and Control. Cambridge University\nPress, USA, 1st edition, 2017.\n[17] Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush\nNasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei, Silvio\nSavarese, Yuke Zhu, and Roberto Mart´ın-Mart´ın. What\nmatters in learning from offline human demonstrations\nfor robot manipulation. arXiv preprint arXiv:2108.03298,\n2021.\n[18] Matthew T Mason. Compliance and force control for\ncomputer controlled manipulators. IEEE Transactions on\nSystems, Man, and Cybernetics, 11(6):418–432, 1981.\n[19] Mayank Mittal, Calvin Yu, Qinxi Yu, Jingzhou Liu,\nNikita Rudin, David Hoeller, Jia Lin Yuan, Ritvik Singh,\nYunrong Guo, Hammad Mazhar, Ajay Mandlekar, Buck\nBabich, Gavriel State, Marco Hutter, and Animesh Garg.\nOrbit: A unified simulation framework for interactive\nrobot learning environments. IEEE Robotics and Automa-\ntion Letters, 8(6):3740–3747, 2023.\n[20] G. Morel and S. Dubowsky.\nThe precise control of\nmanipulators with joint friction: a base force/torque sensor\nmethod. In Proceedings of IEEE International Conference\non Robotics and Automation, volume 1, pages 360–365\nvol.1, 1996.\n[21] H. Olsson, K.J. ˚Astr¨om, C. Canudas de Wit, M. G¨afvert,\nand P. Lischinsky.\nFriction models and friction com-\npensation. European Journal of Control, 4(3):176–195,\n1998.\n[22] Lorenzo Pagliara, Enrico Ferrentino, Andrea Chiacchio,\nand Giovanni Russo.\nSafe haptic teleoperations of\nadmittance controlled robots with virtualization of the\nforce feedback. arXiv preprint arXiv:2404.07672, 2024.\n[23] Nathan D. Ratliff, Jan Issac, and Daniel Kappler. Rie-\nmannian motion policies. CoRR, abs/1801.02854, 2018.\n[24] Kenneth Shaw, Yulong Li, Jiahui Yang, Mohan Kumar\n\nSrirama, Ray Liu, Haoyu Xiong, Russell Mendonca, and\nDeepak Pathak. Bimanual dexterity for complex tasks.\narXiv preprint arXiv:2411.13677, 2024.\n[25] Bruno Siciliano, Oussama Khatib, and Torsten Kr¨oger.\nSpringer handbook of robotics, volume 200. Springer,\n2008.\n[26] Shuran Song, Andy Zeng, Johnny Lee, and Thomas\nFunkhouser. Grasping in the wild: Learning 6dof closed-\nloop grasping from low-cost demonstrations.\nIEEE\nRobotics and Automation Letters, 5(3):4978–4985, 2020.\n[27] Weiyao Wang, Du Tran, and Matt Feiszli. What makes\ntraining multi-modal classification networks hard?\nIn\nProceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 12695–12705, 2020.\n[28] Philipp Wu, Yide Shentu, Zhongke Yi, Xingyu Lin, and\nPieter Abbeel. Gello: A general, low-cost, and intuitive\nteleoperation framework for robot manipulators. In 2024\nIEEE/RSJ International Conference on Intelligent Robots\nand Systems (IROS), pages 12156–12163. IEEE, 2024.\n[29] Yansong Wu, Zongxie Chen, Fan Wu, Lingyun Chen,\nLiding Zhang, Zhenshan Bing, Abdalla Swikir, Alois\nKnoll, and Sami Haddadin. Tacdiffusion: Force-domain\ndiffusion policy for precise tactile manipulation. arXiv\npreprint arXiv:2409.11047, 2024.\n[30] William Xie, Stefan Caldararu, and Nikolaus Correll. Just\nadd force for delicate robot policies.\nIn CoRL 2024\nWorkshop on Mastering Robot Manipulation in a World\nof Abundant Data, 2024.\n[31] Tony Z Zhao, Vikash Kumar, Sergey Levine, and Chelsea\nFinn. Learning fine-grained bimanual manipulation with\nlow-cost hardware.\narXiv preprint arXiv:2304.13705,\n2023.\n[32] Qi Zhu, Jing Du, Yangming Shi, and Paul Wei. Neu-\nrobehavioral assessment of force feedback simulation in\nindustrial robotic teleoperation. Automation in Construc-\ntion, 126:103674, 2021.\n\nAPPENDIX\nVII. ANALYSIS OF FACTR FROM NEURAL TANGENT\nKERNEL (NTK) PERSPECTIVE\nIn this section, we aim to give a sketch of a more theoretical\nargument for why operators like Gaussian blur or downsampling\nin FACTR could help the policy attending to force input. We\nwill first briefly introduce Neural Tangent Kernel (NTK), and\nthen use it a theoretical framework to analyze the effects of\nGaussian blur as an example curriculum operator.\nA. Preliminaries on Neural Tangent Kernel (NTK)\nThe Neural Tangent Kernel (NTK) is a theoretical framework\nused to analyze the behavior of neural networks, particularly in\nthe limit of infinite width [11, 1]. For a neural network fθ(x)\nwith parameters θ, the NTK is defined as:\nk(xi, xj) = ⟨∇θfθ(xi), ∇θfθ(xj)⟩,\nwhere ∇θfθ(x) is the gradient of the network’s output with\nrespect to its parameters θ, and ⟨·, ·⟩denotes the inner product.\nIn the infinite width limit, the NTK becomes deterministic and\nremains constant during training. Assuming the parameters θ\nare initialized from a Gaussian distribution, the NTK k(xi, xj)\nconverges to a deterministic kernel k∞(xi, xj) given by:\nk∞(xi, xj) = Eθ∼N(0,I) [⟨∇θfθ(xi), ∇θfθ(xj)⟩] ,\nwhere the expectation is taken over the Gaussian initialization\nof θ.\nThe NTK k(xi, xj) is a similarity function: if k(xi, xj) is\nlarge, then the predictions fθ(xi) and fθ(xj) will tend to be\nclose. If we have n training points (xi, yi), k defines a positive\nsemi-definite (PSD) kernel matrix K ∈Rn×n\n+\nwhere each entry\nKij = k(xi, xj).\nFascinatingly, when we train this infinite-width neural\nnetwork with gradient flow on the squared error, we precisely\nknow the model output at any point in training. At time t, the\ntraining residual is given by:\nfθt(x) −y = e−ηKt(fθ0(x) −y),\nwhere η is the learning rate, K is the kernel matrix, and\nfθ0(x) is the initial model output. This equation shows that\nthe residual error decays exponentially with a rate determined\nby the kernel matrix K.\nB. Analyzing the Effects of Gaussian Blur with NTK\nWe analyze the effects of Gaussian blur as an example\nof a curriculum operator. While we consider a simple two-\nlayer model here, the intuition applies to more complex\narchitectures like vision transformers (ViTs), which have a\nmore sophisticated NTK [2].\nConsider a model where the input x is first convolved with a\nGaussian kernel Kσ before being passed through the network,\nwhere σ is the standard deviation of the Gaussian kernel. The\noutput of the model is:\nf(x) = W T (Kσ ∗x),\nwhere Kσ ∗x is the convolution of x with a Gaussian\nkernel Kσ. Assuming that the model has infinite width and the\nparameters in W are initialized from a Gaussian distribution,\nthe NTK for this model is:\nkσ(x, x′) = EW ∼N(0,I) [⟨∇W f(x), ∇W f(x′)⟩]\n= EW ∼N(0,I)⟨Kσ ∗x, Kσ ∗x′⟩.\nThis is the dot product between the convolved inputs Kσ ∗x\nand Kσ ∗x′. As σ increases, the Gaussian convolution Kσ ∗x\nacts as a low-pass filter, attenuating high-frequency components\nin the input x. This causes the convolved inputs Kσ ∗xi and\nKσ ∗xj to become more similar for any pair of inputs xi and\nxj. In the extreme case where σ →∞, Kσ ∗xi ≈Kσ ∗xj for\nall xi, xj, and the NTK kσ(xi, xj) approaches a constant value.\nThis implies that the kernel matrix K becomes approximately\nan all-ones matrix, scaled by the magnitude of the convolved\ninputs.\nIn our curriculum, we decrease σ, where the model is first\nexposed to smoother visual data before gradually transitioning\nto the original unsmoothed data. At the beginning of the\ncurriculum, the model focuses on low-frequency patterns and\nrobust features from visual inputs. As σ decreases to small σ,\nthe NTK retains discriminative power, allowing the model to\nlearn fine-grained features from visual inputs. . This gradual\nincrease in complexity can help the model learn more effectively\nby avoiding overfitting to the high-frequency variations in visual\ninputs in the early stages of training.\nMore rigorously, consider the limit σ →∞, each blurred\ninput Kσ ∗xi converges to the same vector ϕ. Hence,\nkσ(xi, xj) = ⟨ϕ, ϕ⟩= ∥ϕ∥2,\nand the NTK matrix K becomes\nK = ∥ϕ∥2\n\n\n\n\n\n1\n1\n· · ·\n1\n1\n1\n· · ·\n1\n...\n...\n...\n...\n1\n1\n· · ·\n1\n\n\n\n\n.\nThis matrix is rank-1, with largest eigenvalue λ = n∥ϕ∥2\n(for n training points) and corresponding eigenvector v =\n(1, 1, . . . , 1).\nRecall that in the infinite-width NTK regime, the training\nresidual r satisfies\nr(t) = e−ηKt r(0),\nwhere r(0) = fθ0(x) −y is the initial residual. Decompose\nr(0) into components parallel and perpendicular to v:\nr(0) = r∥+ r⊥,\nwith r∥∝v and r⊥· v = 0.\nSince K is rank-1,\ne−ηKt r(0) = r⊥+ e−ηλt r∥.\nThe parallel component decays exponentially at rate λ, while\nthe perpendicular component is unchanged. This effectively\n\nlearns a single global scalar for all inputs, reducing mean-\nsquared error by matching the average label but losing\ndiscriminative power. Thus, at the early state of the curriculum,\nthe gradient updates will focus on using the force information\nand updating the force encoder to maximally differentiate\nbetween inputs.\nVIII. COST ANALYSIS OF OUR TELEOPERATION SYSTEM\nWITH FORCE FEEDBACK\nPlease see Table III for a detailed Bill of Materials and\nbreakdown of the cost to create one leader arm and leader\ngripper as part of our teleoperation with force feedback system.\nThis is accurate pricing as of the paper submission.\nObject\nQuantity\nTotal\nDynamixel XM430-W210-T\n2\n$540\nDynamixel XC330-T288-T\n6\n$540\nU2D2 Control PCB\n1\n$20\n12V 20A Power Supply\n1\n$25\nPLA Printer Plastic\nN/A\n$10\nTotal\n$1135\nTABLE III: We present the bill of materials of one leader arm\nteleoperation device with force feedback. The total cost is around\n$1135.\nIX. ADDITIONAL CONTROL LAWS FOR OUR\nTELEOPERATION SYSTEM\nHere we define the additional control laws for our teleoper-\nation system with force feedback: friction compensation and\njoint limit avoidance.\nA. Friction Compensation\nJoint friction introduces resistive forces that impair re-\nsponsiveness, making precise motion control challenging and\nreducing the system’s intuitive feel for the operator [20].\nFriction also increases the physical effort required to back-\ndrive the motor, leading to operator fatigue during prolonged\nuse. To this end, we use a dynamic friction model to explicitly\ncompensate for static, Coulomb, and viscous friction [21].\nWe mitigate the effects of static friction by introducing a\nsmall, high-frequency oscillatory signal to the control input,\nwhich generates micro-vibrations that prevent the motors\nfrom settling into static friction states, thereby enabling\nsmoother transitions from rest to motion [21]. The static friction\ncompensation torque τss is as follows:\nτ (i)\nss =\n(\nµ(i)\ns cos( πt\nf )\nif ˙q(i) < ˙q(i)\ns ,\n0\notherwise\n(7)\nwhere µs is a calibrated static friction coefficient and f is the\ncontrol loop frequency, set to 500 Hz.\nWe also account for kinetic friction by compensating for\nCoulomb friction and viscous friction as follows [21]:\nτ (i)\nks = µ(i)\nc sgn( ˙q(i)) + µ(i)\nv ˙q(i)\n(8)\nwhere µc is the Coulomb friction coefficient and µv is viscous\nfriction coefficient.\nThe total friction compensation τfriction is the sum of the\nstatic friction τss and kinetic friction τks terms.\nB. Joint Limit Avoidance\nWe implement the following artificial potential-based control\nlaw to prevent the operator from making the leader arm go\nbeyond the joint limits of the follower arm:\nU(q(i)) =\n\n\n\n\n\n\n\n1\n2η\n1\n(q(i)−q(i)\nmin )2 ,\nq(i) < q(i)\nmin + ∆q\n1\n2η\n1\n(q(i)\nmax−q(i))2 ,\nq(i) > q(i)\nmax −∆q\n0,\notherwise\n(9)\nτ (i)\nlimit = −∇q(i)U(q(i))\n(10)\nwhere U(q(i)) is the repulsive potential function, ∆q is the\nsafety margin, and η is the scaling factor.\nC. Bi-manual Follower Arms Control with Dynamic Collision\nAvoidance\nMost existing bi-manual teleoperation systems with a leader-\nfollower setup command the follower arms by directly setting\nthe joint position targets to the current joint positions of the\nleader arm. Instead, we employ a Riemannian Motion Policy\n(RMP) [23] implemented in Isaac Lab [19], where the RMP\ndynamically generates joint-space targets for the follower arms\nthat best match the current joint positions of the leader arms\nwhile incorporating real-time collision avoidance. Our system\nprevents the follower arms from colliding with one another\nor with external obstacles, such as the table, regardless of the\noperator’s actions.\nX. BEHAVIOR CLONING POLICY ARCHITECTURE AND\nTRAINING HYPERPARAMETERS\nOur behavior cloning policy takes as input a RGB image\nand current hand joint angles (proprioception). We obtain\ntokens for the image observation via a ViT [7] and a token for\njoint proprioception via a linear layer. The weights of ViT is\ninitialized from the Soup 1M model from [6]. The tokens then\npass through action chunking transformer, an encoder-decoder\ntransformer, to output a sequence of actions [31]. The action\nspace is the absolute joint angles of the two arms for box lift,\nthe absolute angles of a single arm for non-prehensile pivot\nand rolling dough, and the absolute angles of a arm and the\ngripper for fruit pick and place. A key decision that greatly\nimproves policy generalization is to exclude current arm joints\nfrom the proprioception. Intuitively, this may force the model\nto extract object information from image observations, rather\nthan overfitting to predict actions close to current arm states.\nWe list key hyperparameters for our behavior policy training\nTable IV. In general, we are able to obtain well-performing\npolicies with 20000-50000 gradient steps and 2-6 hours of\nwall-clock time training on a RTX4090.\n\nHyperparameter\nValue\nBehavior Policy Training\nOptimizer\nAdamW\nBase Learning Rate\n3e-4\nWeight Decay\n0.05\nOptimizer Momentum\nβ1, β2 = 0.9, 0.95\nBatch Size\n128\nLearning Rate Schedule\nCosine Decay\nTotal Steps\n20000-50000\nWarmup Steps\n500\nAugmentation\nRandomResizeCrop\nGPU\nRTX4090 (24 gb)\nWall-Clock Time\n2-6 hours\nVisual Backbone ViT Architecture\nPatch Size\n16\n# Layers\n12\n# MHSA Heads\n12\nHidden Dim\n768\nClass Token\nYes\nPositional Encoding\nsin cos\nAction Chunking Transformer Architecture\n# Encoder Layers\n6\n# Decoder Layers\n6\n# MHSA Heads\n8\nHidden Dim\n512\nFeed-Forward Dim\n2048\nDropout\n0.1\nPositional Encoding\nsin cos\nAction Chunk\n100\nTABLE IV: Policy Architecture and Training Hyperparameters\nXI. DETAILED QUANTITATIVE RESULTS\nWe present the detailed evaluation results for each task in\nTABLE V, VI, VII, and VIII.\n\nTrain\nTest\nTrain Avg\nTest Avg\nBox1\nBox2\nBox3\nBox4\nBox5\nBox6\nBox7\nACT (Vision-Only)\n10/10\n7/10\n1/10\n1/10\n6/10\n3/10\n1/10\n100.0%\n31.7%\nACT (Vision+Force)\n10/10\n2/10\n4/10\n4/10\n10/10\n10/10\n5/10\n100.0%\n58.3%\nFACTR\n10/10\n8/10\n7/10\n10/10\n10/10\n10/10\n10/10\n100.0%\n91.7%\nTABLE V: Comparison of methods for Box Lift task.\nTrain\nTest\nTrain Avg\nTest Avg\nBox1\nBox2\nBox3\nBox4\nBox5\nBox6\nBox7\nACT (Vision-Only)\n10/10\n9/10\n3/10\n0/10\n7/10\n2/10\n1/10\n95.0%\n26.0%\nACT (Vision+Force)\n9/10\n9/10\n1/10\n2/10\n9/10\n4/10\n5/10\n90.0%\n42.0%\nFACTR\n9/10\n9/10\n6/10\n5/10\n10/10\n7/10\n10/10\n90.0%\n76.0%\nTABLE VI: Comparison of methods for Non-Prehensile Pivot task.\nTrain\nTest\nTrain Avg\nTest Avg\nObj1\nObj2\nObj3\nObj4\nACT (Vision-Only)\n5/5\n0/5\n4/5\n0/5\n100.0%\n26.7%\nACT (Vision+Force)\n5/5\n3/5\n4/5\n4/5\n100.0%\n73.3%\nFACTR\n5/5\n4/5\n5/5\n5/5\n100.0%\n93.3%\nTABLE VII: Comparison of methods for Fruit Pick-Place task.\nTrain\nTest\nTrain Avg\nTest Avg\nObj1\nObj2\nObj3\nObj4\nACT (Vision-Only)\n0/5\n0/5\n0/5\n0/5\n0.0%\n0.0%\nACT (Vision+Force)\n4/5\n4/5\n3/5\n4/5\n80.0%\n70.0%\nFACTR\n5/5\n4/5\n4/5\n4/5\n90.0%\n80.0%\nTABLE VIII: Comparison of methods for Rolling Dough task.\n",
  "metadata": {
    "source_path": "papers/arxiv/FACTR_Force-Attending_Curriculum_Training_for_Contact-Rich_Policy\n__Learning_180d1cc8fc7639d3.pdf",
    "content_hash": "180d1cc8fc7639d36c519fbcfbda3cb2bb0bda5cef07b880f7021ee0be13cf6f",
    "arxiv_id": null,
    "title": "FACTR_Force-Attending_Curriculum_Training_for_Contact-Rich_Policy\n__Learning_180d1cc8fc7639d3",
    "author": "",
    "creation_date": "D:20250225030942Z",
    "published": "2025-02-25T03:09:42",
    "pages": 15,
    "size": 3842498,
    "file_mtime": 1740470079.5400932
  }
}