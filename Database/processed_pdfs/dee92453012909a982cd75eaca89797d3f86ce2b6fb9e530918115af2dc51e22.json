{
  "text": "LLM-QE: Improving Query Expansion by Aligning Large Language\nModels with Ranking Preferences\nSijia Yao1, Pengcheng Huang1*, Zhenghao Liu1†, Yu Gu1,\nYukun Yan2, Shi Yu2, Ge Yu1\n1Department of Computer Science and Technology, Northeastern University, China\n2Department of Computer Science and Technology, Institute for AI, Tsinghua University, China\nBeijing National Research Center for Information Science and Technology, China\nAbstract\nQuery expansion plays a crucial role in infor-\nmation retrieval, which aims to bridge the se-\nmantic gap between queries and documents\nto improve matching performance. This pa-\nper introduces LLM-QE, a novel approach that\nleverages Large Language Models (LLMs) to\ngenerate document-based query expansions,\nthereby enhancing dense retrieval models. Un-\nlike traditional methods, LLM-QE designs both\nrank-based and answer-based rewards and uses\nthese reward models to optimize LLMs to align\nwith the ranking preferences of both retriev-\ners and LLMs, thus mitigating the hallucina-\ntion of LLMs during query expansion. Our\nexperiments on the zero-shot dense retrieval\nmodel, Contriever, demonstrate the effective-\nness of LLM-QE, achieving an improvement\nof over 8%. Furthermore, by incorporating\nanswer-based reward modeling, LLM-QE gen-\nerates more relevant and precise information\nrelated to the documents, rather than simply\nproducing redundant tokens to maximize rank-\nbased rewards. Notably, LLM-QE also im-\nproves the training process of dense retriev-\ners, achieving a more than 5% improvement\nafter fine-tuning. All codes are available at\nhttps://github.com/NEUIR/LLM-QE.\n1\nIntroduction\nDense retrievers (Karpukhin et al., 2020; Xiong\net al., 2021a) encode both queries and documents\ninto a shared embedding space, enabling efficient\nretrieval of relevant documents through the KNN\nsearch. Despite their effectiveness, these models of-\nten encounter challenges presented by the semantic\ngap between query terms and document content. To\novercome this challenge, existing work has utilized\nquery expansion techniques (Abdul-Jaleel et al.,\n2004; Robertson and Jones, 1976) to enhance the\nperformance of both unsupervised and supervised\n* indicates equal contribution.\n† indicates corresponding author.\n Query: How long does an itin take ?\nDocument: If you qualify for an ITIN and your \napplication is complete, you will receive a letter \nfrom IRS …… usually within six weeks.\nDocument\nQuery\nRetriever\nRetriever\n…\nKNN Search\nRanking List\nQuery Expansion :\nYou can expect to receive \nitin from the IRS within 6 \nweeks from the date of…\nInstructq2d: Write a passage to \nanswer the query.\nLLM-QE\nFigure 1: Dense Retrieval Pipeline with LLM-QE.\ndense retrieval models (Gao et al., 2023; Yu et al.,\n2021b). By enriching the original query with addi-\ntional terms, these techniques increase lexical over-\nlap with relevant documents, effectively narrowing\nthe semantic gap between queries and documents.\nRecent developments in query expansion mod-\nels have focused primarily on the Generative Rel-\nevance Feedback (GRF) method (Mackie et al.,\n2023b; Claveau, 2021), which leverages Large Lan-\nguage Models (LLMs) to expand queries with con-\ntextually relevant content. This approach employs\nLLMs to produce documents (Wang et al., 2023a)\nor Chain-of-Thought (CoT) (Wei et al., 2022) that\nare relevant to the query, thereby significantly en-\nriching the semantics of the original query. How-\never, directly applying LLMs to generate query-\nrelated content can introduce irrelevant or distract-\ning information due to the hallucinations inherent\nin LLMs (Ji et al., 2023; Xu et al., 2024).\nTo address this limitation, several works have\narXiv:2502.17057v1  [cs.IR]  24 Feb 2025\n\nexplored the self-consistency (Wang et al., 2023c)\nof LLMs in query expansion. Specifically, they\nuse the instruction to prompt LLMs to generate\ndiverse query-related contents as expansion results\nand then average their representations to mitigate\ninconsistencies (Gao et al., 2023). Other works fur-\nther incorporate feedback from retrievers to select\nhigher-quality expansions. They cross-verify the\nLLM-generated documents and pseudo-relevant\ndocuments by measuring their similarity (Jia et al.,\n2024). Nevertheless, optimizing LLMs to gener-\nate more precise query expansion results by align-\ning the ranking preferences of both retrievers and\nLLMs remains under-explored.\nIn this paper, we propose Large Language\nModel-based Query Expansion (LLM-QE), a\nnovel framework that trains LLMs to align with the\nranking preferences of both retrievers and LLMs,\nthus reducing hallucinations during the generation\nof expansion results. As shown in Figure 1, LLM-\nQE starts with an unsupervised dense retriever,\nthen prompts LLMs to generate document-based\nquery expansions (Gao et al., 2023; Wang et al.,\n2023a), and finally utilizes the Direct Preference\nOptimization (DPO) method (Rafailov et al., 2023)\nto optimize the query expansion model. Addition-\nally, LLM-QE considers the ranking preferences\nof both retrieval models and LLMs and designs a\nreward model that combines both rank-based and\nanswer-based rewards. Specifically, the rank-based\nreward model treats the ground truth document as\nthe query and re-ranks the LLM-generated docu-\nments to calculate the ranking score. Meanwhile, in\nthe answer-based reward model, we prompt LLMs\nto generate an answer based on both the query and\nthe ground truth document. Then the generated an-\nswer serves as a new query to calculate the ranking\nscore among the expanded documents.\nOur experiments on BEIR (Thakur et al., 2021)\ndemonstrate the effectiveness of LLM-QE, achiev-\ning more than 8% and 5% improvements in un-\nsupervised and supervised settings, respectively.\nFurther analysis reveals the crucial roles of both\nrank-based and answer-based rewards in training\nLLM-QE. The rank-based reward directly models\nthe ranking preference of the dense retriever, en-\ncouraging LLMs to generate more semantically rel-\nevant content to match the ground truth documents.\nHowever, relying solely on rank-based reward usu-\nally results in longer expansions in order to win\nthe rank-based reward during optimization. By in-\ncorporating the answer-based reward, the length of\ndocument-based query expansions is significantly\nreduced. This improvement stems from the fact\nthat the answer-based reward helps LLMs better as-\nsess expansion results by evaluating their relevance\nto answer-related content.\n2\nRelated Work\nDense retrievers (Karpukhin et al., 2020; Xiong\net al., 2021a; Izacard et al., 2021; Yu et al., 2021c;\nXiong et al., 2021b; Li et al., 2021) have demon-\nstrated superior ranking performance by conduct-\ning semantic matching between queries and docu-\nments, which helps overcome the problem of vo-\ncabulary mismatch (Belkin et al., 1982). These\nmodels typically leverage Pre-trained Language\nModels (PLMs) to encode both queries and docu-\nments into a shared embedding space, which signif-\nicantly enhances retrieval effectiveness. To further\noptimize this embedding space, dense retrievers are\noften trained contrastively using relevance signals\nbetween queries and documents (Karpukhin et al.,\n2020; Zhan et al., 2021). Some studies have also\ndeveloped zero-shot dense retrievers by training on\nweak supervision data (Xie et al., 2023) or leverag-\ning Large Language Models (LLMs) for query ex-\npansion and reformulation (Gao and Callan, 2022).\nQuery expansion is a long-standing research di-\nrection, originally proposed to rewrite queries and\nimprove the retrieval accuracy of exact matching-\nbased retrievers, such as BM25 (Robertson et al.,\n2009). Early query expansion methods primarily\naim to bridge the lexical gap between queries and\ndocuments (Carpineto and Romano, 2012; Roc-\nchio, 1971) by expanding queries with knowledge\nbases (Bhogal et al., 2007; Qiu and Frei, 1993;\nVoorhees, 1994) or Pseudo-Relevance Feedback\n(PRF) (Amati and Van Rijsbergen, 2002; Robert-\nson, 1990; Rocchio, 1971). These PRF-based meth-\nods have proven their effectiveness in enhancing\nreranking techniques (Li et al., 2018; Ai et al.,\n2018; Yu et al., 2021a) and improving dense retriev-\ners by learning better query representations (Yu\net al., 2021b).\nRecent research of query expansion focuses on\nGenerative Relevance Feedback (GRF) methods,\nwhich utilize generation models to directly pro-\nduce query expansion results (Mackie et al., 2023b;\nClaveau, 2021; Wang et al., 2023b; Jagerman et al.,\n2023; Mackie et al., 2023a). These methods of-\nten employ LLMs to generate query-related docu-\nments (Wang et al., 2023a; Jagerman et al., 2023;\n\nGao et al., 2023), leverage Chain-of-Thought (CoT)\nreasoning results (Wei et al., 2022; Jagerman et al.,\n2023; Trivedi et al., 2023), or utilize specific key-\nwords (Li et al., 2024; Jagerman et al., 2023) to\nexpand queries, thereby enhancing the ranking ca-\npabilities of lexical matching based retrieval mod-\nels (Jagerman et al., 2023; Wang et al., 2023a),\ndense retrieval models (Wang et al., 2023a), and\nreranking models (Li et al., 2024).\nAlthough these studies have demonstrated their\nadvantages in improving retrieval models, directly\nusing LLMs for query expansion and reformula-\ntion poses potential risks due to LLM hallucina-\ntions (Shuster et al., 2021; Huang et al., 2023).\nTo mitigate this issue, some works use different\ninstructions to reduce inconsistency in query refor-\nmulation (Gao et al., 2023) or leverage rephrased\nquestions as demonstrations to guide LLMs in\ngenerating more effective query expansion re-\nsults (Koo et al., 2024). RaFe (Mao et al., 2024)\nfurther enhances the Retrieval-Augmented Genera-\ntion (RAG) performance by taking reranking scores\nas training signals to optimize the query rewriting\nmodel. In contrast to these approaches, LLM-QE\nfocuses on modeling the ranking preferences of\nboth retrievers and LLMs, rewarding LLMs for\ngenerating more effective expansion results, and\nexploring its potential to build both unsupervised\nand supervised dense retrieval models.\n3\nMethodology\nAs illustrated in Figure 2, this section describes\nour LLM based query expansion method, LLM-\nQE. First, we introduce our query-expanded dense\nretrieval method (Sec. 3.1). Next, we use the DPO\nmethod to optimize LLMs for query expansion\n(Sec. 3.2). Finally, we build a reward model to train\nLLM-QE by modeling the ranking preferences of\nboth dense retrievers and LLMs (Sec. 3.3).\n3.1\nQuery Expanded Dense Retrieval\nGiven a query q and a document collection D =\n{d1, ..., dk}, dense retrieval models (Karpukhin\net al., 2020; Xiong et al., 2021a; Gao and Callan,\n2021) first encode the query q and the i-th docu-\nment di into vector representations ⃗q and ⃗di using\nPLMs, such as BERT (Devlin et al., 2019):\n⃗q = BERTq(q),\n⃗di = BERTd(di).\n(1)\nThen the score S(q, di) is calculated to estimate\nthe relevance between q and di, followed by a\nKNN search (Douze et al., 2024) to retrieve the\ntop-ranked documents to satisfy the user needs.\nDifferent from existing dense retrieval models,\nwe introduce our query expanded dense retrieval\nmethod. Firstly, we prompt the LLM (M) to pro-\nduce a document-based query expansion:\ndexp = M(Instructq2d, q),\n(2)\nwhere Instructq2d denotes the instruction that asks\nLLMs to generate document-like query expansion\noutcomes (Jagerman et al., 2023). The representa-\ntion of expanded query ⃗q exp is calculated by aver-\naging the representations of the raw query q and the\ndocument-based query expansion dexp to enhance\nboth unsupervised and supervised dense retrievers:\n⃗q exp = ⃗q + ⃗d exp\n2\n.\n(3)\nUnsupervised Dense Retrieval.\nThe unsu-\npervised dense retrieval models, such as Con-\ntriever (Izacard et al., 2021), are pre-trained to\nmatch text segments that share the same seman-\ntic information (Fang et al., 2020; Wu et al., 2020)\nand do not use the query-document relevance label\nduring training (Bajaj et al., 2016).\nFollowing Gao et al. (2023), we can directly use\nEq. 3 to represent the expanded query by incor-\nporating the information of raw query q and the\ndocument-based query expansion dexp. Then the\nrelevance score S(q, di) between the query q and\nthe candidate document di can be calculated:\nS(q, di) = sim(⃗q exp, ⃗di),\n(4)\nwhere sim(·) is the similarity estimation function\nthat conducts the dot product operation.\nSupervised Dense Retrieval. Then we describe\nthe details of training and inference processes in\nthe supervised dense retrieval scenario.\nTraining. During training the dense retriever\naugmented with query expansions, we first regard\nthe expansion result dexp as the query and then\ncalculate the similarity score S(dexp, di) between\ndexp and di:\nS(dexp, di) = sim(⃗d exp, ⃗di).\n(5)\nThen we can contrastively train the encoder to learn\nmatching signals from both dexp and the query-\nrelated document d∗using the training loss LDR:\nLDR = −log\neS(dexp,d∗)\neS(dexp,d∗) + P\nd−∈D−eS(dexp,d−) ,\n(6)\n\nTraining LLM with Preference Optimization \nInstructq2d: Write a passage to answer the query.\nQuery (q): How long does an itin take ? \nLLM\nReward \nModel\nP: {q, d2 ,   dn     }\nd2   (Chosen Response): If you apply for an \nITIN online, you can expect to receive it from \nthe IRS within 6 weeks from the date of  …\nSelection\ndn   (Rejected Response): The processing \ntime for an I-130 petition, also known as \nAffidavit of Support, can vary depending …\nLLM-QE\nDPO\nModeling Reward Models\nGround Truth (d*):\nIf you qualify for an \nITIN, you will receive\na letter from IRS …  \nwithin six weeks.\nsim(d*, d1    )\n…\nsim(d*, d2    )\nsim(d*, dn    )\n5\n1\n n\nSum(    ,    )\nR( d2    ) > …… > R( d1    ) > ……> R( dn    )  \n…\ndexp: You can expect to receive itin \nfrom the IRS within 6 weeks ……\nD\nRanking List\nq\n…\nExpanded Documents\nSimilarity\nRank\n1/5\n1\n 1/n\n…\nScore\nInstructq2a: You are given a query and a ground truth document. \nBased on the query, generate a direct and relevant answer …\nQuery:How long does an itin take ?  /  Ground Trurh:If you qualify…\nsim(y, d1    )\n…\nsim(y, d2    )\nsim(y, dn    )\n1\n3\n n\n…\nSimilarity\nRank\n1\n1/3\n 1/n\n…\nScore\nLLM\nAnswer (y): \nApply for \nitin usually \nwithin six \nweeks.\nRank-based Reward Model\nAnswer-based Reward Model\n…\nd1\nexp\nd2\nexp\ndn\nexp\nD q: {      ,         ,  … ,       }\nd1\nexp d2\nexp\ndn\nexp\nexp\nexp\nexp\nexp\nexp\nexp\nexp\nexp\nexp\nexp\nexp\nexp\nexp\nFigure 2: Illustration of Our LLM-QE Model.\nwhere D−represents the set of negative docu-\nments, which are sampled from in-batch nega-\ntives (Karpukhin et al., 2020).\nInference. During retrieval, we can also rep-\nresent the expanded query using Eq. 3 and then\ncalculate the relevance score:\nS(q, di) = sim(⃗q exp, ⃗di).\n(7)\n3.2\nExpanding Queries by Training LLMs\nwith Preference Optimization\nExisting query expansion methods (Gao et al.,\n2023; Li et al., 2024) typically focus on directly\nprompting LLMs to generate various expansion\nresults to mitigate hallucinations (Brown et al.,\n2020; Thoppilan et al., 2022). To obtain more tai-\nlored and query-specific expansion results, we fine-\ntune the LLM to align with ranking preferences\nusing the Direct Preference Optimization (DPO)\nmethod (Amini et al., 2024).\nFirst, we prompt the LLM to generate multiple\nexpansion documents by adjusting the temperature\nτ during sampling:\ndexp\ni\n∼M(Instructq2d, q).\n(8)\nThus, we can collect k document-based query ex-\npansions Dq = {dexp\n1 , dexp\n2 , . . . , dexp\nk }. Then we\nfollow the DPO method to optimize the LLM (M)\nusing the loss function L(M; MRef):\nL(M; MRef) = −E(q,dexp\n+ ,dexp\n−)∼P\nh\nlog σ\n\u0010\nβ log\nM(dexp\n+ | q)\nMRef(dexp\n+ | q) −β log\nM(dexp\n−| q)\nMRef(dexp\n−| q)\n\u0011i\n,\n(9)\nwhere σ is the Sigmoid function and β is a hyperpa-\nrameter that controls the strength of the regulation\nfrom the reference model MRef. MRef is frozen\nduring DPO training. P is the dataset used for DPO\ntraining, which contains the triple (q, dexp\n+ , dexp\n−).\ndexp\n+ and dexp\n−are positive and negative responses,\nwhich are sampled from the document-based query\nexpansions Dq:\nR(dexp\n+ ) > R(dexp\n−),\n(10)\nwhere R(dexp) is the reward model that is used to\nestimate the ranking preference of the document-\nbased query expansion dexp. The details of the rank-\ning preference modeling are introduced in Sec. 3.3.\n3.3\nModeling Ranking Preferences for\nRewarding Query Expansion Models\nFor the given query q, the quality of the i-th\ndocument-based query expansion dexp\ni\nis evaluated\nusing the reward R(dexp\ni\n) defined as:\nR(dexp\ni\n) = Rrank(dexp\ni\n) + Rans(dexp\ni\n),\n(11)\nwhere dexp\ni\n∈Dq. Rrank(dexp\ni\n) and Rans(dexp\ni\n) rep-\nresent the rank-based reward score and the answer-\nbased reward score, respectively. These scores\n\nare combined to model the ranking preference by\ncapturing the relevance preference of the dense\nretriever and estimating the consistency with the\nquestion answering model.\nRank-based Reward.\nTo assess the quality\nof the document-based query expansion dexp\ni\n, the\nmost straightforward approach is to use a ranking\nscore. Specifically, we calculate the Mean Recip-\nrocal Rank (MRR) score by treating the ground\ntruth document d∗as the query and ranking the\ndocument-based query expansions Dq:\nRrank(dexp\ni\n) =\n1\nRank(d∗, dexp\ni\n)\n,\n(12)\nwhere Rank(d∗, dexp\ni\n) represents the rank of dexp\ni\nbased on the relevant score sim(d∗, dexp\ni\n).\nA\nhigher reward score Rrank(dexp\ni\n) indicates that the\ndocument-based query expansion dexp\ni\nis more sim-\nilar to the ground truth document d∗.\nAnswer-based Reward. While the rank-based\nreward modeling accurately captures the prefer-\nence of dense retrievers, it often leads to biases\nand fairness issues in reward modeling (Dai et al.,\n2024). Thus, relying solely on rank-based reward\nmay cause the dense retrieval model to overfit to\nthe inherent biases of dense retrievers. To address\nthis challenge, we propose an answer-based re-\nward modeling approach, which leverages the self-\nconsistency of LLMs to calibrate the reward.\nSpecifically, we first instruct the LLM to gener-\nate an answer y based on the given query q and the\nground truth document d∗:\ny = M(Instructq2a, q, d∗),\n(13)\nwhere Instructq2a is the instruction that guides the\nLLM to generate a response to answer the query\nq. We then treat the LLM-generated answer y as a\nquery and rank the document-based query expan-\nsions Dq to compute the answer-based reward:\nRans(dexp\ni\n) =\n1\nRank(y, dexp\ni\n)\n,\n(14)\nwhere Rank(y, dexp\ni\n) denotes the rank of document\ndexp\ni\nbased on its relevance score sim(y, dexp\ni\n).\nBoth y and dexp\ni\nare generated by the same LLM,\nusing different instructions: Instructq2a for gener-\nating the answer and Instructq2d for generating the\nexpansion documents. Higher-ranked documents\nindicate that y and dexp\ni\nshare more semantic simi-\nlarity, reflecting greater consistency and semantic\nagreement between y and dexp\ni\n.\nDataset\nSetting\n#Query\nTrain\nDev\nTest\nE5\nLLM-QE\n27,000\n3,000\n-\nRetrieval\n637,866\n70,874\n-\nMS MARCO\nRetrieval\n-\n6,980\n-\nBeir\nRetrieval\n-\n-\n46,379\nTable 1: Data Statistics.\n4\nExperimental Methodology\nIn this section, we introduce the datasets, evalua-\ntion metrics, baselines, and implementation details\nused in our experiments. More experimental details\nare shown in Appendix A.2.\nDataset. We utilize various datasets for training\nand evaluation. Data statistics are shown in Table 1.\nTraining.\nWe use the publicly available E5\ndataset (Wang et al., 2024; Springer et al., 2024) to\ntrain both the LLM-QE and dense retrievers. We\nconcentrate on English-based question answering\ntasks and collect a total of 808,740 queries. From\nthis set, we randomly sample 100,000 queries to\nconstruct the DPO training data, while the remain-\ning queries are used for contrastive training. Dur-\ning the DPO preference pair construction, we first\nprompt LLMs to generate expansion documents,\nfiltering out queries where the expanded documents\nshare low similarity with the query. This results in\na final set of 30,000 queries.\nEvaluation. We evaluate retrieval effectiveness\nusing two retrieval benchmarks: MS MARCO (Ba-\njaj et al., 2016) and BEIR (Thakur et al., 2021), in\nboth unsupervised and supervised settings.\nEvaluation Metrics. We use nDCG@10 as the\nevaluation metric. Statistical significance is tested\nusing a permutation test with p < 0.05.\nBaselines. We compare our LLM-QE model\nwith three unsupervised retrieval models and five\nquery expansion baseline models.\nThree\nunsupervised\nretrieval\nmod-\nels—BM25\n(Robertson\net\nal.,\n2009),\nCo-\nCondenser (Gao and Callan, 2022), and Con-\ntriever (Izacard et al., 2021)—are evaluated in\nthe experiments. Among these, Contriever serves\nas our primary baseline retrieval model, as it is\nused as the backbone model to assess the query\nexpansion performance of LLM-QE. Additionally,\nwe compare LLM-QE with Contriever in a\nsupervised setting using the same training dataset.\nFor query expansion, we benchmark against\nfive methods: Pseudo-Relevance Feedback (PRF),\nQ2Q, Q2E, Q2C, and Q2D. PRF is specifically\n\nTask\nBM25\nUnsupervised Dense Retrievers\nSupervised Dense Retrievers\ncoCondenser\nContriever†\nPRF⋄\nQ2Q\nQ2E\nQ2C\nQ2D§\nLLM-QE\nContriever¶\nLLM-QE\nMS MARCO\n22.8\n16.2\n20.55⋄\n16.66\n22.07\n21.38\n22.10\n23.00†⋄\n25.20†⋄§\n34.33\n34.70\nTrec-COVID\n65.6\n40.4\n27.45\n27.71\n38.76\n48.64\n58.81\n57.25†⋄\n59.66†⋄\n34.16\n68.62¶\nNFCorpus\n32.5\n28.9\n31.73⋄\n27.49\n31.53\n32.90\n32.80\n33.20†⋄\n33.61†⋄\n32.71\n33.47\nNQ\n32.9\n17.8\n25.37⋄\n20.98\n34.80\n29.05\n36.82\n38.91†⋄\n43.26†⋄§\n34.02\n51.47¶\nHotpotQA\n60.3\n34.0\n48.07⋄\n40.43\n56.15\n46.15\n59.82\n61.84†⋄\n65.82†⋄§\n58.78\n67.44¶\nFiQA\n23.6\n25.1\n24.50⋄\n19.65\n26.69\n25.20\n27.23\n27.38†⋄\n30.12†⋄§\n28.04\n33.48¶\nArguAna\n31.5\n44.4\n37.90\n38.19\n42.89\n43.24\n41.83\n42.90†⋄\n43.06†⋄\n52.70\n52.92\nTouche-2020\n36.7\n11.7\n16.68⋄\n14.26\n12.93\n18.01\n23.12\n26.33†⋄\n24.34†⋄\n10.46\n26.61¶\nCQADupStack\n29.9\n30.9\n28.43⋄§\n23.18\n25.21\n26.74\n21.90\n24.69⋄\n27.84⋄§\n31.60\n33.35¶\nQuora\n78.9\n82.1\n83.50⋄§\n81.43\n81.65\n82.28\n80.80\n81.53\n82.54⋄§\n85.53\n81.96\nDBPedia\n31.3\n21.5\n29.16⋄\n23.43\n32.18\n29.13\n34.27\n36.10†⋄\n38.20†⋄§\n38.22\n37.77\nScidocs\n15.8\n13.6\n14.91⋄\n13.51\n15.32\n15.12\n15.17\n15.52†⋄\n16.63†⋄§\n15.67\n17.27¶\nFEVER\n75.3\n61.5\n68.20⋄\n58.95\n70.07\n66.93\n75.36\n78.62†⋄\n82.80†⋄§\n82.49\n85.03¶\nClimate-FEVER\n21.4\n16.9\n15.50⋄\n13.52\n15.40\n15.02\n22.28\n19.43†⋄\n21.16†⋄§\n23.04\n23.08\nScifact\n66.5\n56.1\n64.92⋄\n60.56\n67.05\n66.73\n66.35\n66.52⋄\n67.74†⋄\n68.64\n66.28\nAvg. BEIR14\n43.0\n34.6\n36.88⋄\n33.09\n39.33\n38.94\n42.61\n43.59†⋄\n45.48†⋄§\n42.59\n48.48¶\nAvg. All\n41.7\n33.4\n35.79⋄\n32.00\n38.18\n37.77\n41.24\n42.21†⋄\n44.13†⋄§\n42.04\n47.56¶\nBest on\n1\n0\n0\n0\n0\n0\n0\n0\n1\n3\n10\nTable 2: Overall Performance of LLM-QE. We follow previous work (Izacard et al., 2021) and report the average\nperformance across 14 BEIR tasks (BEIR14) and all tasks (All). Bold and underlined scores indicate the best and\nsecond-best results. †, ⋄, and § denote significant improvements over Contriver, PRF, and Q2D in the unsupervised\nsetting, while ¶ indicates a significant improvement over Contriver in the supervised setting.\nimplemented following the approach in Yu et al.\n(2021b), which enhances query understanding by\nextracting keywords from query-related documents.\nThe Q2Q, Q2E, Q2C, and Q2D methods (Jager-\nman et al., 2023; Li et al., 2024) expand the origi-\nnal query by prompting LLMs to generate query-\nrelated queries, keywords, chains-of-thought (Wei\net al., 2022), and documents.\nImplementation Details. For our query expan-\nsion model, we deploy the Meta-LLaMA-3-8B-\nInstruct (AI@Meta, 2024) as the backbone for the\nquery expansion generator. The batch size is set\nto 16, and the learning rate is set to 2e −5. Opti-\nmization is performed using the AdamW optimizer.\nWe employ LoRA (Hu et al., 2022) to efficiently\nfine-tune the model for 2 epochs. The temperature\nfor the construction of the DPO data varies across\nτ ∈{0.8, 0.9, 1.0, 1.1}, with each setting sampled\neight times. For the dense retriever, we utilize\nContriever (Izacard et al., 2021) as the backbone.\nDuring training, we set the batch size to 1,024 and\nthe learning rate to 3e −5, with the model trained\nfor 3 epochs.\n5\nEvaluation Results\nIn this section, we present the overall performance\nof LLM-QE, conduct ablation studies, and ana-\nlyze the effectiveness of different reward models.\nDetailed analysis of query expansion quality is pro-\nvided in the Appendix A.3. The case study is shown\nin Appendix A.4.\n5.1\nOverall Performance\nThe retrieval performance of different query expan-\nsion models is shown in Table 2.\nWe begin by evaluating the performance of LLM-\nQE in an unsupervised setting. The evaluation\nresults show that LLM-QE achieves an 8.6% im-\nprovement over the retriever Contriever, demon-\nstrating the effectiveness of query expansion in\nenhancing unsupervised retrieval models. Among\nall query expansion baselines, Q2D performs the\nbest, indicating that generating query-related docu-\nments as the expansion results is particularly suited\nfor LLMs to generate effective expansion results.\nAfter DPO training, LLM-QE shows a significant\nimprovement over the Q2D method, confirming the\neffectiveness of its training approach in enhancing\nthe expansion capabilities of the Q2D model.\nAdditionally, LLM-QE demonstrates its robust-\nness by extending its advantages to supervised re-\ntrieval training scenarios. After fine-tuning dense\nretrieval models using the E5 dataset, LLM-QE\nconsistently outperforms the Contriever fine-tuned\nwith raw queries across most datasets, achieving an\naverage improvement of 5.5%. This indicates the\neffectiveness of our query expansion techniques in\nbenefiting the training process of dense retrievers\nby narrowing the semantic gap between queries\nand documents.\n5.2\nAblation Study\nIn this subsection, we present ablation studies to\ninvestigate the effectiveness of LLM-QE in various\n\nModel\nMARCO\nTrec-COVID\nNQ\nHotpotQA\nFiQA\nDBPedia\nFEVER\nScifact\nAvg.\nUnsupervised Dense Retriever\nRaw Query\n20.55\n27.45\n25.37\n48.07\n24.50\n29.16\n68.20\n64.92\n38.53\nQ2Q\n22.07\n38.76\n34.80\n56.15\n26.69\n32.18\n70.07\n67.05\n43.27\nLLM-QE (Q2Q)\n25.18\n49.64\n37.35\n61.32\n29.35\n35.37\n73.07\n65.98\n47.15\nQ2E\n21.38\n48.64\n29.05\n46.15\n25.20\n29.13\n66.93\n66.73\n41.65\nLLM-QE (Q2E)\n23.51\n46.27\n35.18\n59.65\n28.08\n32.55\n72.32\n68.00\n45.69\nQ2C\n22.10\n58.81\n36.82\n59.82\n27.23\n34.27\n75.36\n66.35\n47.59\nLLM-QE (Q2C)\n24.52\n62.29\n41.00\n64.04\n29.87\n36.23\n78.52\n66.46\n50.37\nQ2D\n23.00\n57.25\n38.91\n61.84\n27.38\n36.10\n78.62\n66.50\n48.70\nLLM-QE (Q2D)\n25.20\n59.66\n43.26\n65.82\n30.12\n38.20\n82.80\n67.74\n51.60\nw/o Rank Reward\n25.55\n57.96\n42.74\n64.12\n29.99\n37.36\n77.24\n67.23\n50.27\nw/o Answer Reward\n24.70\n58.42\n41.89\n64.84\n29.10\n37.38\n82.93\n68.31\n50.95\nSupervised Dense Retriever\nRaw Query\n34.33\n34.16\n34.02\n58.78\n28.04\n38.22\n82.49\n68.64\n47.34\nLLM-QE (Q2Q)\n34.55\n53.61\n47.72\n67.03\n33.48\n39.76\n84.86\n68.31\n53.66\nLLM-QE (Q2E)\n33.66\n56.54\n44.53\n65.25\n31.18\n37.08\n84.32\n68.96\n52.69\nLLM-QE (Q2C)\n34.55\n53.61\n47.72\n67.03\n33.48\n39.76\n84.86\n68.31\n53.66\nLLM-QE (Q2D)\n34.70\n68.62\n51.47\n67.44\n33.48\n37.77\n85.03\n66.28\n55.60\nTable 3: Ablation Studies. All models are implemented based on the Contriever model.\nquery expansion scenarios and analyze the impact\nof different query expansion models in both unsu-\npervised and supervised settings.\nAs shown in Table 3, we first apply LLM-QE\nto several query expansion methods in the unsu-\npervised setting, including Q2Q, Q2E, and Q2C.\nThe results indicate that LLM-QE consistently im-\nproves the performance of different expansion for-\nmats by approximately 3%, highlighting its gen-\neralization ability. Among all the expansion for-\nmats, LLM-QE (Q2D) achieves the best perfor-\nmance, indicating that prompting the LLM to gen-\nerate tailored pseudo-relevant documents for spe-\ncific queries can bring more precise retrieval re-\nsults (Gao et al., 2023).\nNext, we explore the role of the reward model\nby optimizing the LLM-QE (Q2D) under two set-\ntings: one with only the rank-based reward and\nthe other with only the answer-based reward, re-\nsulting in the LLM-QE (Q2D) w/o Answer Re-\nward model and the LLM-QE (Q2D) w/o Rank\nReward model, respectively. The rank-based re-\nward, which utilizes preference signals from the\ndense retriever, typically achieves better ranking\nperformance than the answer-based reward. How-\never, when both rewards are combined, LLM-QE\n(Q2D) shows further improvements, particularly\nin question-answering tasks. This demonstrates\nthe effectiveness of answer-based reward modeling,\nwhich leverages the consistency between gener-\nated answers and query-expanded documents to\nestimate the quality of the expansion results.\nFinally, we assess the effectiveness of LLM-\nQE (Q2Q), LLM-QE (Q2E), LLM-QE (Q2C), and\nLLM-QE (Q2D) in the supervised setting. Over-\nall, all query expansion models show a consis-\ntent improvement of more than 5% across vari-\nous expansion formats, further confirming the ef-\nficacy of LLM-based query expansion models in\nbetter aligning the semantics of queries and their\ncorresponding documents during contrastive train-\ning. Among all query expansion models, LLM-QE\n(Q2D) achieves the best retrieval performance, with\nan improvement of about 2% over other expansion\nmodels, demonstrating that LLM-QE (Q2D) can\nbroaden its advantage to the supervised setting.\n5.3\nEffectiveness of Different Reward Models\nin Optimizing LLM-QE\nFigure 3 illustrates that we evaluate the charac-\nteristics of reward models in LLM-QE based on\nthe length, text similarity, and rank correlation of\nthe query expansion results. We compare two ab-\nlation models, LLM-QE (w/o Rank Reward) and\nLLM-QE (w/o Answer Reward), with the LLM-\nQE model. The former two train LLM-QE models\nusing only the answer-based reward and the rank-\nbased reward, respectively.\nLength of Query Expansion. First, we analyze\nthe average length of query expansions generated\nby different models, as shown in Figure 3(a).\nThe evaluation results show that LLM-QE (w/o\nAnswer Reward) produces the longest query ex-\npansions, indicating that using only the rank-based\nreward encourages the model to generate redun-\ndant tokens related to the ground truth document.\n\nNQ\nHotpotQA\nDatasets\n0\n50\n100\n150\n200\n250\nAverage Length\n174\n102\n192\n178\n185\n120\n(a) Average Length.\nNQ\nHotpotQA\nDatasets\n2\n4\n6\n8\n10\n12\nBLEU\n5.96\n10.69\n4.46\n5.13\n5.24\n6.96\n(b) Answer Similarity.\nNQ\nHotpotQA\nDatasets\n4\n5\n6\n7\n8\nBLEU\n5.72\n6.45\n6.19\n6.57\n6.24\n7.15\n(c) Similarity with Golden\nDocuments.\nNQ\nHotpotQA\nDatasets\n0.6\n0.7\n0.8\n0.9\n1.0\nPCC\n0.75\n0.72\n0.71\n0.67\n0.73\n0.69\nLLM-QE (w/o Rank Reward)\nLLM-QE (w/o Answer Reward)\nLLM-QE\n(d) Rank Correlation.\nFigure 3: Performance of Reward Models in LLM-QE.\nSuch a behavior can help LLMs win a higher rank-\nbased reward score. However, when the answer-\nbased reward is incorporated, the length of the\nquery expansions generated by LLM-QE signif-\nicantly decreases. This proves the effectiveness of\nthe answer-based reward in preventing excessively\nlong query expansions, reducing the tendency of\nthe LLM-QE model to overfit to the ranking prefer-\nences of dense retrievers, and mitigating the gener-\nation of noisy information.\nText Similarity Evaluation. In the following\nexperiment, we evaluate the text similarity be-\ntween document-based query expansions and LLM-\ngenerated answers/golden documents.\nAs illustrated in Figure 3(b), we first estimate the\nsimilarity between document-based query expan-\nsions and LLM-generated answers. LLM-QE (w/o\nRank Reward) is designed to optimize LLMs to\ngenerate query expansions that are more closely\nrelated to the LLM-generated answers, thereby\nachieving the highest similarity score. In contrast,\nLLM-QE (w/o Answer Reward) yields the lowest\nBLEU score, indicating that rank-based rewards are\nless effective in guiding LLM-QE to align with the\ninformation in answers, which is particularly im-\nportant for question answering tasks. When answer-\nbased rewards are incorporated, the BLEU scores\nfor LLM-QE increase, showing its effectiveness in\ngenerating more precise information for expansion.\nNext, we estimate the text similarity between\nquery expansions generated by different models\nand ground-truth documents, as shown in Fig-\nure 3(c). The evaluation results reveal that LLM-\nQE (w/o Rank Reward) presents the lowest BLEU\nscore, suggesting that optimizing solely with the\nanswer-based reward is insufficient to improve\nthe text similarity between query expansions and\ngolden documents.\nIn comparison, LLM-QE\n(w/o Answer Reward) achieves better performance,\ndemonstrating the effectiveness of rank-based re-\nwards in guiding LLMs to generate more relevant\nsemantics for matching the golden documents. By\ncombining both rewards, LLM-QE achieves the\nhighest BLEU score, underscoring the importance\nof using both rewards to optimize LLMs to gener-\nate higher-quality query expansions.\nRank Correlation. Lastly, we assess the similar-\nity between raw queries and query expansions by\nranking these candidate documents and calculate\nthe rank correlation.\nAs shown in Figure 3(d), we calculate the\nPearson Correlation Coefficient (PCC) to mea-\nsure the correlation between the query-based doc-\nument rank and the expansion-based document\nrank. Specifically, we employ a fine-tuned retriever,\nBGE (Chen et al., 2024), to compute the similarity\nbetween raw queries and relevant documents. We\nthen use expansions generated by different models\nas queries and calculate the similarity with can-\ndidate documents. Finally, we compute the PCC\nbetween these similarity score lists.\nThe results show that LLM-QE (w/o Rank Re-\nward) achieves the highest PCC score, demon-\nstrating the effectiveness of answer-based rewards\nin producing query expansions that better reflect\nuser search intentions. In contrast, LLM-QE (w/o\nAnswer Reward) achieves the lowest PCC score\namong all models, which shows that the informa-\ntion beyond the user intention is also incorporated\nin the expansion results. The additional informa-\ntion misleads the well-trained dense retriever, but\nbenefits LLM-QE to produce more effective query\nexpansions for Contriever.\n6\nConclusion\nThis paper presents a novel query expansion frame-\nwork, LLM-QE, which optimizes LLM-generated\nquery expansions to align with the ranking prefer-\nences of dense retrievers. By incorporating both\nrank-based and answer-based reward models, LLM-\n\nQE effectively enhances the quality of query expan-\nsions by mitigating the generation of noisy infor-\nmation and avoiding excessively long expansions.\nExperimental results demonstrate that LLM-QE\nconsistently improves performance in both unsu-\npervised and supervised training scenarios, provid-\ning insights into integrating query expansion into\ndense retrieval training.\nLimitations\nDespite the effectiveness of LLM-QE in enhancing\nquery expansion for dense retrieval, several limita-\ntions remain. First, LLM-QE depends on the qual-\nity of document-based query expansions generated\nby the LLM. If the LLM produces low-quality or\nbiased expansions, retrieval performance may de-\ngrade. While the reward model helps mitigate this\nissue, further improvements in controlling LLM\noutputs are still an open challenge. Additionally,\ncompared to vanilla dense retrieval models, LLM-\nQE introduces additional computational costs due\nto the need for generating document-based query\nexpansions.\nReferences\nNasreen Abdul-Jaleel, James Allan, W Bruce Croft,\nFernando Diaz, Leah Larkey, Xiaoyan Li, Mark D\nSmucker, and Courtney Wade. 2004. Umass at trec\n2004: Novelty and hard. In Proceedings of TREC,\npage 189.\nQingyao Ai, Keping Bi, Jiafeng Guo, and W. Bruce\nCroft. 2018. Learning a deep listwise context model\nfor ranking refinement. In Proceedings of SIGIR,\npages 135–144.\nAI@Meta. 2024. Llama 3 model card.\nGianni Amati and Cornelis Joost Van Rijsbergen. 2002.\nProbabilistic models of information retrieval based\non measuring the divergence from randomness. ACM\nTransactions on Information Systems (TOIS), pages\n357–389.\nAfra Amini, Tim Vieira, and Ryan Cotterell. 2024. Di-\nrect preference optimization with an offset. ArXiv\npreprint.\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder, An-\ndrew McNamara, Bhaskar Mitra, Tri Nguyen, et al.\n2016.\nMs marco: A human generated machine\nreading comprehension dataset. In Proceedings of\nNeurIPS.\nNicholas J Belkin, Robert N Oddy, and Helen M Brooks.\n1982. Ask for information retrieval: Part i. back-\nground and theory. Journal of documentation, pages\n61–71.\nJagdev Bhogal, Andrew MacFarlane, and Peter Smith.\n2007. A review of ontology based query expansion.\nInformation processing & management, pages 866–\n886.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nProceedings of NeurIPS.\nClaudio Carpineto and Giovanni Romano. 2012. A\nsurvey of automatic query expansion in information\nretrieval. Acm Computing Surveys (CSUR), pages\n1–50.\nJianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu\nLian, and Zheng Liu. 2024. Bge m3-embedding:\nMulti-lingual, multi-functionality, multi-granularity\ntext embeddings through self-knowledge distillation.\nArXiv preprint.\nVincent Claveau. 2021. Neural text generation for query\nexpansion in information retrieval. In Proceedings\nof IEEE/WIC/ACM International Conference on Web\nIntelligence and Intelligent Agent Technology, pages\n202–209.\nSunhao Dai, Chen Xu, Shicheng Xu, Liang Pang, Zhen-\nhua Dong, and Jun Xu. 2024. Bias and unfairness in\ninformation retrieval systems: New challenges in the\nllm era. In Proceedings of KDD, pages 6437–6447.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding.\nIn Proceedings of NAACL-HLT, pages\n4171–4186.\nMatthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff\nJohnson, Gergely Szilvasy, Pierre-Emmanuel Mazaré,\nMaria Lomeli, Lucas Hosseini, and Hervé Jégou.\n2024. The faiss library. ArXiv preprint.\nAngela Fan, Yacine Jernite, Ethan Perez, David Grang-\nier, Jason Weston, and Michael Auli. 2019. ELI5:\nLong form question answering. In Proceedings of\nACL, pages 3558–3567.\nHongchao Fang, Sicheng Wang, Meng Zhou, Jiayuan\nDing, and Pengtao Xie. 2020.\nCert: Contrastive\nself-supervised learning for language understanding.\nArXiv preprint.\nLuyu Gao and Jamie Callan. 2021. Condenser: a pre-\ntraining architecture for dense retrieval. In Proceed-\nings of EMNLP, pages 981–993.\n\nLuyu Gao and Jamie Callan. 2022. Unsupervised cor-\npus aware language model pre-training for dense pas-\nsage retrieval. In Proceedings of ACL, pages 2843–\n2853.\nLuyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan.\n2023. Precise zero-shot dense retrieval without rel-\nevance labels. In Proceedings of ACL, pages 1762–\n1777.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2022. Lora: Low-rank adaptation of\nlarge language models. In Proceedings of ICLR.\nLei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,\nZhangyin Feng, Haotian Wang, Qianglong Chen,\nWeihua Peng, Xiaocheng Feng, Bing Qin, et al. 2023.\nA survey on hallucination in large language models:\nPrinciples, taxonomy, challenges, and open questions.\nArXiv preprint.\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Sebas-\ntian Riedel, Piotr Bojanowski, Armand Joulin, and\nEdouard Grave. 2021. Unsupervised dense informa-\ntion retrieval with contrastive learning. Transactions\non Machine Learning Research (TMLR).\nRolf Jagerman, Honglei Zhuang, Zhen Qin, Xuanhui\nWang, and Michael Bendersky. 2023. Query expan-\nsion by prompting large language models. ArXiv\npreprint.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of hal-\nlucination in natural language generation.\nArXiv\npreprint.\nPengyue Jia, Yiding Liu, Xiangyu Zhao, Xiaopeng Li,\nChangying Hao, Shuaiqiang Wang, and Dawei Yin.\n2024. MILL: Mutual verification with large language\nmodels for zero-shot query expansion. In Proceed-\nings of NAACL, pages 2498–2518.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020.\nDense passage retrieval for\nopen-domain question answering. In Proceedings\nof EMNLP, pages 6769–6781.\nHamin Koo, Minseon Kim, and Sung Ju Hwang. 2024.\nOptimizing query generation for enhanced document\nretrieval in rag. ArXiv preprint.\nCanjia Li, Yingfei Sun, Ben He, Le Wang, Kai Hui, An-\ndrew Yates, Le Sun, and Jungang Xu. 2018. NPRF: A\nneural pseudo relevance feedback framework for ad-\nhoc information retrieval. In Proceedings of EMNLP,\npages 4482–4491.\nMinghan Li, Honglei Zhuang, Kai Hui, Zhen Qin,\nJimmy Lin, Rolf Jagerman, Xuanhui Wang, and\nMichael Bendersky. 2024. Can query expansion im-\nprove generalization of strong cross-encoder rankers?\nIn Proceedings of SIGIR, pages 2321–2326.\nYizhi Li, Zhenghao Liu, Chenyan Xiong, and Zhiyuan\nLiu. 2021. More robust dense retrieval with con-\ntrastive dual learning.\nIn Proceedings of SIGIR,\npages 287–296.\nIain Mackie, Shubham Chatterjee, and Jeffrey Dalton.\n2023a. Generative and pseudo-relevant feedback for\nsparse, dense and learned sparse retrieval. ArXiv\npreprint.\nIain Mackie, Shubham Chatterjee, and Jeffrey Dalton.\n2023b. Generative relevance feedback with large\nlanguage models. In Proceedings of SIGIR, pages\n2026–2031.\nShengyu Mao, Yong Jiang, Boli Chen, Xiao Li, Peng\nWang, Xinyu Wang, Pengjun Xie, Fei Huang, Hua-\njun Chen, and Ningyu Zhang. 2024. Rafe: Ranking\nfeedback improves query rewriting for rag. In Pro-\nceedings of EMNLP Findings.\nYonggang Qiu and Hans-Peter Frei. 1993. Concept\nbased query expansion. In Proceedings of SIGIR,\npages 160–169.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christo-\npher D. Manning, Stefano Ermon, and Chelsea Finn.\n2023. Direct preference optimization: Your language\nmodel is secretly a reward model. In Proceedings of\nNeurIPS.\nStephen Robertson, Hugo Zaragoza, et al. 2009. The\nprobabilistic relevance framework: Bm25 and be-\nyond. Foundations and Trends® in Information Re-\ntrieval, pages 333–389.\nStephen E Robertson. 1990. On term selection for query\nexpansion. Journal of documentation, pages 359–\n364.\nStephen E Robertson and K Sparck Jones. 1976. Rel-\nevance weighting of search terms. Journal of the\nAmerican Society for Information science, pages 129–\n146.\nJJ Rocchio. 1971. Relevance feedback in information\nretrieval. The SMART Retrieval System-Experiments\nin Automatic Document Processing/Prentice Hall.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation re-\nduces hallucination in conversation. In Proceedings\nof EMNLP Findings, pages 3784–3803.\nJacob Mitchell Springer, Suhas Kotha, Daniel Fried,\nGraham Neubig, and Aditi Raghunathan. 2024. Rep-\netition improves language model embeddings. ArXiv\npreprint.\nNandan Thakur, Nils Reimers, Andreas Rücklé, Ab-\nhishek Srivastava, and Iryna Gurevych. 2021. Beir:\nA heterogenous benchmark for zero-shot evaluation\nof information retrieval models. ArXiv preprint.\n\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\n2022. Lamda: Language models for dialog applica-\ntions. ArXiv preprint.\nJames\nThorne,\nAndreas\nVlachos,\nChristos\nChristodoulopoulos,\nand\nArpit\nMittal.\n2018.\nFEVER: a large-scale dataset for fact extraction\nand VERification. In Proceedings of ACL, pages\n809–819. Association for Computational Linguistics.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\nand Ashish Sabharwal. 2023. Interleaving retrieval\nwith chain-of-thought reasoning for knowledge-\nintensive multi-step questions. In Proceedings of\nACL, pages 10014–10037.\nEllen M Voorhees. 1994. Query expansion using lexical-\nsemantic relations. In Proceedings of SIGIR, pages\n61–69.\nLiang Wang, Nan Yang, Xiaolong Huang, Linjun Yang,\nRangan Majumder, and Furu Wei. 2024. Improving\ntext embeddings with large language models. ArXiv\npreprint.\nLiang Wang, Nan Yang, and Furu Wei. 2023a.\nQuery2doc: Query expansion with large language\nmodels. In Proceedings of EMNLP, pages 9414–\n9423.\nXiao Wang, Sean MacAvaney, Craig Macdonald, and\nIadh Ounis. 2023b. Generative query reformulation\nfor effective adhoc search. ArXiv preprint.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V.\nLe, Ed H. Chi, Sharan Narang, Aakanksha Chowd-\nhery, and Denny Zhou. 2023c. Self-consistency im-\nproves chain of thought reasoning in language mod-\nels. In Proceedings of ICLR.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,\nand Denny Zhou. 2022. Chain-of-thought prompt-\ning elicits reasoning in large language models. In\nProceedings of NeurIPS.\nZhuofeng Wu, Sinong Wang, Jiatao Gu, Madian Khabsa,\nFei Sun, and Hao Ma. 2020.\nClear: Contrastive\nlearning for sentence representation. ArXiv preprint.\nYiqing Xie, Xiao Liu, and Chenyan Xiong. 2023. Un-\nsupervised dense retrieval training with web anchors.\nIn Proceedings of SIGIR, pages 2476–2480.\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\nJialin Liu, Paul N. Bennett, Junaid Ahmed, and\nArnold Overwijk. 2021a. Approximate nearest neigh-\nbor negative contrastive learning for dense text re-\ntrieval. In Proceedings of ICLR.\nWenhan Xiong, Xiang Lorraine Li, Srini Iyer, Jingfei\nDu, Patrick S. H. Lewis, William Yang Wang, Yashar\nMehdad, Scott Yih, Sebastian Riedel, Douwe Kiela,\nand Barlas Oguz. 2021b. Answering complex open-\ndomain questions with multi-hop dense retrieval. In\nProceedings of ICLR.\nZiwei Xu, Sanjay Jain, and Mohan Kankanhalli. 2024.\nHallucination is inevitable: An innate limitation of\nlarge language models. ArXiv preprint.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D. Manning. 2018. HotpotQA: A dataset for\ndiverse, explainable multi-hop question answering.\nIn Proceedings of EMNLP, pages 2369–2380.\nHongChien Yu, Zhuyun Dai, and Jamie Callan. 2021a.\nPgt: pseudo relevance feedback using a graph-based\ntransformer. In Proceedings of ECIR, pages 440–\n447.\nHongChien Yu, Chenyan Xiong, and Jamie Callan.\n2021b. Improving query representations for dense\nretrieval with pseudo relevance feedback. In Pro-\nceedings of CIKM, pages 3592–3596.\nShi Yu, Zhenghao Liu, Chenyan Xiong, Tao Feng, and\nZhiyuan Liu. 2021c. Few-shot conversational dense\nretrieval. In Proceedings of SIGIR, pages 829–838.\nJingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min\nZhang, and Shaoping Ma. 2021. Optimizing dense\nretrieval model training with hard negatives. In Pro-\nceedings of SIGIR, pages 1503–1512.\n\nQuery Expansion\nPrompt for Q2Q:\nOutput the rewrite of input query:\nQuery: {}\nOutput:\nPrompt for Q2E:\nWrite a list of keywords for the given query:\nQuery: {}\nKeywords:\nPrompt for Q2C:\nAnswer the following query:\nQuery: {}\nGive the rationale before answering.\nPrompt for Q2D:\nPlease write a passage to answer the question:\nQuestion: {}\nPassage:\nQuestion Answering\nPrompt for Q2A:\nYou are given a query and a related document. Based on\nthe query, generate a direct and relevant answer using the\ninformation in the document. If the query is a statement,\nexpand on it. If it is a question, provide a direct answer.\nAvoid any extra description or irrelevant content.\nQuery: {}\nRelated Document: {}\nAnswer:\nTable 4: Prompt Templates Used in LLM-QE. These\nprompts are used to generate query expansion results of\ndifferent formats and produce the responses to answer\nthe question.\nA\nAppendix\nA.1\nLicense\nThe authors of 4 out of the 15 datasets in the\nBEIR benchmark (NFCorpus, FiQA-2018, Quora,\nClimate-Fever) and the authors of ELI5 in the E5\ndataset do not report the dataset license in the paper\nor a repository. We summarize the licenses of the\nremaining datasets as follows.\nMS MARCO (MIT License); FEVER, NQ, and\nDBPedia (CC BY-SA 3.0 license); ArguAna and\nTouché-2020 (CC BY 4.0 license); CQADupStack\nand TriviaQA (Apache License 2.0); SciFact (CC\nBY-NC 2.0 license); SCIDOCS (GNU General\nPublic License v3.0); HotpotQA and SQuAD (CC\nBY-SA 4.0 license); TREC-COVID (Dataset Li-\ncense Agreement).\nAll these licenses and agreements permit the use\nof their data for academic purposes.\nA.2\nAdditional Experimental Details\nThis subsection outlines the components of the\ntraining data and presents the prompt templates\nused in the experiments.\nTraining Datasets.\nFollowing the setup of\nWang et al. (2024), we use the following datasets:\n1.0\n0.5\n0.0\n0.5\n1.0\nImprovement\nMARCO\nNQ\nHotpotQA\nDataset\n(a) Unsupervised Dense Retriever.\n1.0\n0.5\n0.0\n0.5\n1.0\nImprovement\nMARCO\nNQ\nHotpotQA\nDataset\n(b) Supervised Dense Retriever.\nFigure 4: Improvements of LLM-QE in Both Unsuper-\nvised and Supervised Dense Retrievers. We plot the\nchange of nDCG@10 scores before and after the query\nexpansion using our LLM-QE model.\nELI5 (sample ratio 0.1) (Fan et al., 2019), Hot-\npotQA (Yang et al., 2018), FEVER (Thorne et al.,\n2018), MS MARCO passage ranking (sample\nratio 0.5) and document ranking (sample ratio\n0.2) (Bajaj et al., 2016), NQ (Karpukhin et al.,\n2020), SQuAD (Karpukhin et al., 2020), and Triv-\niaQA (Karpukhin et al., 2020). In total, we use\n808,740 training examples.\nPrompt Templates. Table 4 lists all the prompts\nused in this paper. In each prompt, “query” refers\nto the input query for which query expansions\nare generated, while “Related Document” denotes\nthe ground truth document relevant to the original\nquery. We observe that, in general, the model tends\nto generate introductory phrases such as “Here is a\npassage to answer the question:” or “Here is a list\nof keywords related to the query:”. Before using\nthe model outputs as query expansions, we first fil-\nter out these introductory phrases to ensure cleaner\nand more precise expansion results.\nA.3\nQuery Expansion Quality of LLM-QE\nThis section evaluates the quality of query expan-\nsion of LLM-QE. As shown in Figure 4, we ran-\ndomly select 100 samples from each dataset to\nassess the improvement in retrieval performance\n\nQuery: How does the coronavirus respond to changes in the weather?\nGolden Pagssage: Epidemics ... occur during the winter months. ... Two major contributing factors are the changes in\nenvironmental parameters and human behavior. Studies have revealed the effect of temperature and humidity on respiratory\nvirus stability and transmission rates. More recent research highlights the importance of the environmental factors,\nespecially temperature and humidity ...\nRaw Query\nnDCG@10: 22.01%\nOriginal Query: How does the coronavirus respond to changes in the weather?\nVanilla LLM\nnDCG@10: 76.63%\nExpanded Query: The coronavirus, like many other viruses, has been observed to respond to changes in the weather by\nexperiencing fluctuations in transmission and spread. ... the virus tends to thrive in environments with high humidity,\ntypically above 40%, and a temperature range of 37°C to 46°C. ... studies have found that the virus can survive on surfaces\nfor longer periods at lower temperatures and humidity levels, ...\nLLM-QE\nnDCG@10: 100.00%\nExpanded Query: The coronavirus responds to changes in the weather by adapting its transmission and spread patterns.\nThis is because temperature, humidity, and other environmental factors can affect the stability and survival of the virus on\nsurfaces, ... research suggests that the virus may thrive in cooler and more humid environments, ... such as air circulation,\nventilation, and human behavior.\nTable 5: Case Study. All experiments are conducted based on the Contriever model under the zero-shot setting. To\nfacilitate evaluation, we highlight the potential matching phrases between the golden passage and both the original\nand expanded queries. Different colors are used to annotate these matched phrases for each method: Green for\nDirect Retrieval, Red for Vanilla LLM, and Blue for LLM-QE.\nbefore and after applying LLM-QE.\nOverall, the evaluation results demonstrate that\nLLM-QE consistently improves retrieval perfor-\nmance in both unsupervised (Figure 4(a)) and su-\npervised (Figure 4(b)) settings. However, for the\nMS MARCO dataset, LLM-QE demonstrates lim-\nited effectiveness in the supervised setting. This\ncan be attributed to the fact that MS MARCO pro-\nvides higher-quality training signals, allowing the\ndense retriever to learn sufficient matching signals\nfrom relevance labels. In contrast, LLM-QE leads\nto more substantial performance improvements on\nthe NQ and HotpotQA datasets. This indicates that\nLLM-QE provides essential matching signals for\ndense retrievers, particularly in retrieval scenarios\nwhere high-quality training signals are scarce.\nA.4\nCase Study\nTo further demonstrate the effectiveness of LLM-\nQE, we conduct a case study by randomly sampling\na query from the evaluation dataset. We then com-\npare retrieval performance using the raw queries,\nexpanded queries by vanilla LLM, and expanded\nqueries by LLM-QE.\nAs shown in Table 5, query expansion signifi-\ncantly improves retrieval performance compared to\nusing the raw query. Both vanilla LLM and LLM-\nQE generate expansions that include key phrases,\nsuch as “temperature”, “humidity”, and “coron-\navirus”, which provide crucial signals for docu-\nment matching. However, vanilla LLM produces\ninconsistent results, including conflicting claims\nabout temperature ranges and virus survival condi-\ntions. In contrast, LLM-QE generates expansions\nthat are more semantically aligned with the golden\npassage, such as “the virus may thrive in cooler and\nmore humid environments, which can facilitate its\ntransmission”. This further demonstrates the effec-\ntiveness of LLM-QE in improving query expansion\nby aligning with the ranking preferences of both\nLLMs and retrievers.\n",
  "metadata": {
    "source_path": "papers/arxiv/LLM-QE_Improving_Query_Expansion_by_Aligning_Large_Language_Models_with\n__Ranking_Preferences_dee92453012909a9.pdf",
    "content_hash": "dee92453012909a982cd75eaca89797d3f86ce2b6fb9e530918115af2dc51e22",
    "arxiv_id": null,
    "title": "LLM-QE_Improving_Query_Expansion_by_Aligning_Large_Language_Models_with\n__Ranking_Preferences_dee92453012909a9",
    "author": "",
    "creation_date": "D:20250225024100Z",
    "published": "2025-02-25T02:41:00",
    "pages": 13,
    "size": 1377019,
    "file_mtime": 1740470189.823432
  }
}