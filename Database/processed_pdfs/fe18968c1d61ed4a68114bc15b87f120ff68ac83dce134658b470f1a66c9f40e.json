{
  "text": "Low-Rank and Sparse Model Merging for Multi-Lingual\nSpeech Recognition and Translation\nQiuming Zhao1\nGuangzhi Sun2\nChao Zhang1\nMingxing Xu1\nThomas Fang Zheng1*\n1Tsinghua University, China\n2University of Cambridge, United Kingdom\nzqm23@mails.tsinghua.edu.cn, gs534@cam.ac.uk,\n{cz277,xumx,fzheng}@tsinghua.edu.cn\nAbstract\nLanguage diversity presents a significant chal-\nlenge in speech-to-text (S2T) tasks, such as\nautomatic speech recognition and translation.\nTraditional multi-task training approaches aim\nto address this by jointly optimizing multiple\nspeech recognition and translation tasks across\nvarious languages. While models like Whisper,\nbuilt on these strategies, demonstrate strong\nperformance, they still face issues of high com-\nputational cost, language interference, subop-\ntimal training configurations, and limited ex-\ntensibility. To overcome these challenges, we\nintroduce LoRS-Merging (low-rank and sparse\nmodel merging), a novel technique designed to\nefficiently integrate models trained on differ-\nent languages or tasks while preserving perfor-\nmance and reducing computational overhead.\nLoRS-Merging combines low-rank and sparse\npruning to retain essential structures while elim-\ninating redundant parameters, mitigating lan-\nguage and task interference, and enhancing ex-\ntensibility. Experimental results across a range\nof languages demonstrate that LoRS-Merging\nsignificantly outperforms conventional multi-\nlingual multi-task training baselines. Our find-\nings suggest that model merging, particularly\nLoRS-Merging, is a scalable and effective com-\nplement to traditional multi-lingual training\nstrategies for S2T applications1.\n1\nIntroduction\nLanguage diversity poses a significant challenge\nin speech-to-text (S2T) tasks, such as automatic\nspeech recognition (ASR) (Prabhavalkar et al.,\n2023) and speech translation (ST) (Xu et al., 2023).\nWith over 7,000 languages spoken worldwide, de-\nveloping robust S2T systems that generalise across\nvaried linguistic structures remains a fundamental\nresearch goal (Liu and Niehues, 2024; Cheng et al.,\n2023; Sun et al., 2023; Saif et al., 2024; Wang et al.,\n*Correspondence\n1The detailed data and code will be released at [URL]\n2021; Le et al., 2021). The advent of end-to-end\n(E2E) models (Chan et al., 2016; Gulati et al., 2020;\nBarrault et al., 2023) has marked a paradigm shift\nin S2T tasks, enabling direct mapping from speech\nto text across multiple languages within a unified\nframework. A prominent example is Whisper (Rad-\nford et al., 2023), an advanced multi-lingual speech\nmodel trained on a large-scale, diverse dataset cov-\nering multiple languages and tasks. Despite these\nadvances, existing multi-lingual models still en-\ncounter significant challenges in scalability, effi-\nciency, and performance trade-offs.\nTo address these challenges, multi-lingual train-\ning strategies (Saif et al., 2024; Xiao et al., 2021;\nBai et al., 2018) have been adopted, aiming to\nenhance model generalisation across languages.\nThese approaches typically rely on joint optimisa-\ntion of diverse S2T tasks across multiple languages,\nleveraging shared representations to improve per-\nformance. Nevertheless, multi-lingual training is\nsubject to inherent limitations, including substan-\ntial training costs, complex model configurations,\nand limited access to training data across multiple\nlanguages and tasks. Moreover, when handling new\nlanguages, the training methods typically require\ntraining from scratch.\nTo mitigate these issues, this paper proposes\nto use model merging (Ilharco et al., 2023; Yang\net al., 2024a; Khan et al., 2024) to integrate models\ntrained on different languages or tasks while main-\ntaining performance and reducing computational\noverhead. Model merging merges the parameters\nof multiple separate models with different capa-\nbilities to build a universal model. With its high\nflexibility, model merging enables the seamless in-\ncorporation of new languages or tasks without the\nneed for retraining the entire model. Additionally,\nsince model merging allows models for different\nlanguages or tasks to be trained independently, it\ncan effectively alleviate negative transfer issues\n(Wang et al., 2019; Zhang et al., 2023b; Wang\narXiv:2502.17380v1  [cs.SD]  24 Feb 2025\n\net al., 2020b) commonly observed in multi-lingual\ntraining. This training independence also enables\nthe use of optimal training configurations for each\nlanguage or task instead of the unified settings re-\nquired in multi-lingual training.\nMoreover, we propose Low-Rank and Sparse\nmodel Merging (LoRS-Merging), which uses a\nlow-rank component to capture the compact struc-\nture and a sparse component to capture the scat-\ntered details in the weights. LoRS-Merging retains\neffective parts of structure and details while re-\nducing redundant parts to reduce task interference.\nSpecifically, coarse-grained singular value prun-\ning is used to retain the low-rank structure, while\nfine-grained magnitude pruning is used to remove\nredundant details. The main contribution of this\npaper can be summarised as follows.\n• We propose LoRS-Merging, a low-rank and\nsparse model merging method for multi-lingual\nASR and speech translation. To the best of our\nknowledge, LoRS-Merging is the first work that\nexplores model merging for speech models.\n• LoRS-Merging exploits the combination of low-\nrank structure and sparsity of language-specific\nand task-specific weights in model merging, min-\nimising the parameter redundancy and conflicts\nas well as providing an efficient way to incor-\nporate new knowledge from a task or language-\nspecialised model.\n• Experiments are performed across 10 different\nlanguages where LoRS-Merging achieves a sig-\nnificant improvement compared to the multi-\nlingual multi-task training baseline. Moreover,\nwe show that negative interference largely ex-\nists in multi-lingual training and LoRS-Merging\nalleviates this issue.\n2\nRelated Work\n2.1\nMulti-Lingual ASR and ST\nMulti-lingual speech models inherently face a\ntrade-off between knowledge sharing and negative\ninterference. Early studies adopted hand-picked\nsub-network sharing strategies, such as language-\nspecific decoders (Dong et al., 2015), attention\nheads (Zhu et al., 2020), and layer norm/linear\ntransformation (Zhang et al., 2020). Recent re-\nsearch has shifted toward approaches such as\nmixture-of-experts (Kwon and Chung, 2023; Wang\net al., 2023), adapters (Le et al., 2021; Kannan\net al., 2019), and pruning (Lu et al., 2022; Lai et al.,\n2021). To enhance multi-lingual representation\nlearning, language tokens (Johnson et al., 2017),\nembeddings (Di Gangi et al., 2019) or output fac-\ntorisations (Zhang et al., 2023a) are introduced to\nencode language identity, helping the model distin-\nguish between languages.\nThe more effective approach is to adopt multi-\nlingual training strategies, such as multi-objective\noptimisation (Saif et al., 2024; Zhang et al., 2022),\nadversarial learning (Xiao et al., 2021), meta learn-\ning (Hsu et al., 2020), and reinforcement learn-\ning (Bai et al., 2018). Moreover, large-scale pre-\ntraining by leveraging massive amounts of multi-\nlingual and multi-task data enables models to learn\nrobust and transferable representations across lan-\nguages, e.g. Whisper (Radford et al., 2023), Seam-\nlessM4T (Barrault et al., 2023), and AudioPaLM\n(Rubenstein et al., 2023). LoRS-Merging, as an ef-\nficient post-training method proposed in this paper,\nfurther advances multi-lingual ASR and ST based\non pre-trained speech models.\n2.2\nModel Merging\nModel merging (Yang et al., 2024a; Khan et al.,\n2024) is an efficient post-training technique that\nintegrates knowledge from models trained on dif-\nferent domains. One stream of research focuses\non the loss landscape geometry (Khan et al., 2024)\nand studies the linear mode connectivity (LMC)\n(Frankle et al., 2020; Draxler et al., 2018) prop-\nerty that demonstrates the existence of a linearly\nconnected path between local minima within the\nsame loss basin. Many studies (Nagarajan and\nKolter, 2019; Izmailov et al., 2018; Frankle et al.,\n2020) indicate that if two neural networks share\npart of their optimisation trajectory, such as dif-\nferent finetuned models from the same pretrained\nmodel, they typically satisfy LMC, allowing inter-\npolation without sacrificing accuracy and forming\nthe basis of our model merging method. For local\nminima in different loss basins, inspired by the per-\nmutation invariance (Entezari et al., 2021) of neural\nnetworks, neuron alignment techniques (Ainsworth\net al., 2023; Singh and Jaggi, 2020; Tatro et al.,\n2020) can be used to place them into the same\nbasin, thereby reducing merging loss.\nAnother stream considers the model spaces, in-\ncluding activation spaces and weight spaces. Re-\nsearch on activation spaces seeks to align the out-\nput representations or loss of the merged model\nwith those of each single model as closely as pos-\nsible (Yang et al., 2024b; Wei et al., 2025; Xiong\n\net al., 2024). Studies based on weight spaces aim\nto remove redundant parameters or localise effec-\ntive parameters to resolve task interference. TIES-\nMerging (Yadav et al., 2024) and DARE (Yu et al.,\n2024) perform magnitude or random pruning on\neach single model to significantly remove redun-\ndant parameters. TALL-masks (Wang et al., 2024)\nand Localise-and-Stitch (He et al., 2024) optimise\nbinary masks to localise sparse and effective task-\nspecific parameters. In contrast, LoRS-Merging\nexplores weight space merging by considering not\nonly the detailed parameter redundancy as well as\nmaintaining the effective structure of the weight\nspace via low-rank pruning.\n3\nMethodology\n3.1\nPreliminaries\n3.1.1\nTask Arithmetic\nAmong diverse model merging methods, Task\nArithmetic (TA) (Ilharco et al., 2023) has become\na fundamental technique in this field due to its\nsimplicity and effectiveness. TA introduces the\nconcept of \"task vector\", defined as the delta pa-\nrameter derived by subtracting pretrained weights\nfrom finetuned weights. By performing simple\narithmetic operations on task vectors, TA enables\ntask learning, forgetting, and analogising.\nAssume that θ = {Wl}L\nl=1 represents the pa-\nrameters of the model, where Wl is the weight\nof l-th layer, and L is the total number of lay-\ners. Given a pretrained model θ0 and a model θi\nfinetuned on task ti, the task vector is computed\nas τi = θi −θ0. Multiple task vectors can be\nsummed to form a multi-task model, expressed as\nθmerged = θ0 + λ Pn\ni=1 τi, where λ is a scaling\ncoefficient for the task vectors.\n3.1.2\nPruning\nGiven that neural networks are typically over-\nparameterised and exhibit high redundancy, a con-\nsiderable number of neurons or connections can be\npruned without affecting accuracy (LeCun et al.,\n1989). In model merging, pruning methods can re-\nduce redundant parameters to mitigate task interfer-\nence, thereby improving the merging performance.\nMagnitude Pruning (MP) is an unstructured\npruning method that prunes connections based on\nthe magnitude of parameters as a measure of im-\nportance. Specifically, MP prunes the parameters\naccording to a specific ratio p, as follows.\nMij =\n(\n1\nif |wij| ∈top p%\n0\no.w.\n(1)\nWpruned = M ⊙W\n(2)\nwhere W, M ∈Rd×k, and ⊙denotes the element-\nwise multiplication. However, MP only focuses on\nthe redundancy at the parameter level, overlooking\nthe crucial structural information, which may lead\nto the disruption of the weight structure.\nSingular Value Pruning (SVP) is a structured\npruning method that removes smaller singular val-\nues and their corresponding singular vectors. In\nparticular, SVP retains only the top r singular val-\nues while discarding the others.\nW = UΣV T\n(3)\nWpruned = UrΣrV T\nr\n(4)\nwhere U ∈Rd×d and V ∈Rk×k are the left and\nright singular vector matrices of W, and Ur, Vr de-\nnote their first r columns. Although SVP preserves\na compact weight structure, its coarse pruning gran-\nularity makes it challenging to reduce redundancy\nat a fine-grained parameter level.\n3.2\nModel Merging for Speech Models\nThe model merging process for speech model on\nS2T tasks with LoRS-Merging as an example is\nshown in Fig. 1, which comprises four steps. In\nstep 1, a suitable pre-trained speech model is se-\nlected. In step 2, for each target language and\ntarget task combination, e.g. Catalan ASR, the pre-\ntrained model is finetuned with the task-language-\nspecific data and the delta weight is obtained. In\nstep 3, weight pruning is applied to remove redun-\ndant and conflicting delta parameters. In step 4,\ntask arithmetic is applied to combine pruned delta\nweights into each single merged matrix and hence\nobtain the merged model.\nModel merging allows new language or task\nknowledge to be integrated into the model in a flex-\nible post-training manner. When a new set of data\nfor a specific language is obtained, model merging\nincorporates such knowledge by fine-tuning with\nthe new data alone with data-specific configuration,\nwhich also releases the burden of requiring other\ndata to avoid catastrophic forgetting. This benefit\nis thoroughly demonstrated in our experiments.\n\nCatalan\nItalian\n...\n...\nCatalan\nItalian\n+ \n+ \n+ \n+ \nLoRS\nLoRS\nLoRS\nLoRS\nStep 1\nPretrained\nStep 2\nFinetuned\nStep 3\nLoRS\nStep 4\nMerged\nST\nASR\n...\n...\nFigure 1: Model merging process with the proposed LoRS-Merging for speech models on multi-lingual ASR and\nST tasks. In step 1, a suitable pre-trained speech model is selected. In step 2, the pre-trained model is finetuned with\nthe task-language-specific data. In step 3, apply LoRS to the delta parameters to reduce model redundancy. In step\n4, merge the delta parameters to get a multi-lingual and multi-task merged model.\nSVD\nSVP\n+ \nMP\nLow-Rank\nSparse\nResidual\nFigure 2: Illustration of LoRS-Merging method in detail.\nSVD stands for singular value decomposition and SVP\nfor singular value pruning. MP is magnitude pruning\noperating on residual of the original weight matrix and\nthe low-rank matrix.\n3.3\nLow-Rank and Sparse Model Merging\nThe weights of neural networks contain informa-\ntion on both structure and details. Structural infor-\nmation is coherent, compact, and coarse-grained,\nwhereas detail information is incoherent, scattered,\nand fine-grained. Both structural and detail infor-\nmation include effective and redundant parts. To\nreduce redundant parts in both the structure and de-\ntail aspects of the weights while retaining effective\nparts, the LoRS-Merging method is introduced as\nshown in detail in Fig. 2, which exploits the com-\nbination of low-rank structure by SVP and sparsity\nby MP. SVP performs coarse-grained pruning at\nthe structure level, while MP enables fine-grained\npruning at the detail level.\nIn the implementation, we approximate the orig-\ninal weights as the sum of a low-rank component\nand a sparse component, where the low-rank com-\nponent captures the compact structure, and the\nsparse component captures the scattered details,\nas shown in Eqn. (5).\nW ≈L + S\n(5)\nwhere L represents the low-rank component, and S\nrepresents the sparse component. Specifically, L is\nthe low-rank matrix obtained by retaining the top\nr singular values and their corresponding singular\nvectors from W:\nL = UrΣrV T\nr\n(6)\nand S is the sparse matrix obtained by performing\nMP on the residual of W and L:\nS = M ⊙(W −L)\n(7)\nTo simplify the description, we refer to this entire\nprocess as LoRS(·). In this manner, SVP decouples\nthe structure and details of the weight, preserving a\ncompact structure while allowing fine-grained MP\nto remove redundant parts in the details.\nFor each model finetuned on single specific\nlanguage or task data, we apply LoRS(·) to its\ntask vector as a preprocessing step to reduce lan-\nguage or task interference in model merging. A\nmulti-lingual or multi-task model can be achieved\nthrough simple merging, expressed as:\nθmerged = θ0 + λ\nn\nX\ni=1\nLoRS(τi)\n(8)\n\nCompared to multi-lingual or multi-task training\nmethods, model merging is a simpler and more ef-\nficient approach, enabling the seamless incorpora-\ntion of new languages or tasks without the need for\nretraining. Additionally, due to its training indepen-\ndence, it mitigates negative transfer and provides\noptimal training configurations for each language\nor task to improve performance.\n4\nExperimental Setup\n4.1\nData\nCoVoST-2 (Wang et al., 2020a) is a large-scale\nmulti-lingual ST corpus based on Common Voice.\nIt covers translations from English into 15 lan-\nguages and from 21 languages into English, with\na total of 2,880 hours of speech from 78k speak-\ners. We selected 5 high-resource languages and\n5 low-resource languages as two language sets to\ninvestigate their ASR tasks and the from X to En-\nglish ST tasks. The high-resource language set\nincludes Catalan (ca), German (de), Spanish (es),\nFrench (fr), and Italian (it), while the low-resource\nlanguage set includes Indonesian (id), Dutch (nl),\nPortuguese (pt), Russian (ru), and Swedish (sv).\nDue to the more abundant data in the high-resource\nlanguage set, our main experimental results are\nobtained on the high-resource language set, while\nthe low-resource language set serves as an auxil-\niary validation set. To balance the amount of data\nacross different languages, we fixed the duration\nof traning data for each language, with 5 hours for\nthe high-resource language set and 1 hour for the\nlow-resource language set. The dev and test sets of\nboth language sets are 1 hour in duration.\n4.2\nModel and Training Specifications\nWhisper (Radford et al., 2023) is a general-purpose\nmulti-lingual ASR and ST model, a Transformer-\nbased model trained on 680k hours of diverse audio.\nWe chose the small version as the foundation model\nfor the experiments because it achieves a good bal-\nance between performance and cost. It has 244\nmillion parameters, with the encoder and decoder\neach consisting of 12 Transformer blocks. The\nweight matrices of the attention layers are all 768\nby 768, and the MLP layers are 768 by 3072.\nFor each language-specific or task-specific fine-\ntuned model, we use a different, optimal learning\nrate for each during training, and these models are\nsubsequently used for model merging. Finetuning\ninvolves updating all parameters. We choose Adam\nas the optimiser, set the batch size to 8, the accu-\nmulation iterations to 4, and train for 10 epochs.\nWe also select the proportions of low-rank param-\neters retained by SVP from {1%, 2%, 3%, 5%}\nand sparse parameters retained by MP from {10%,\n20%, 40%, 60%}. The beam size for decoding is\nset to 20 across all languages and tasks. We use\nSclite and SacreBLEU tools to score the ASR and\nST results, respectively. See Appendix A for more\ndetails on hyper-parameter settings. Our experi-\nments are performed on a single RTX 4090 GPU\nwhere training on one language and one task with\n5 hours of speech data requires 1 hour.\n4.3\nBaseline and Merging Methods\nWe use Multi-lingual and multi-task training\nas the baseline for comparison with model merg-\ning methods, where training is conducted on data\nmixed from both multi-lingual and multi-task sets.\nTo ensure a fair comparison, the same amount of\ntraining data is used from each language and each\ntask. Note that for 5 different languages with both\nASR and ST tasks, multi-lingual and multi-task\ntraining is performed on 10 times more data and\nhence 10 times more computational resources.\nIn addition to LoRS-Merging, we investigate the\nfollowing model merging methods:\nWeight Averaging (WA) merges multiple sin-\ngle models by and unweighted averaging of their\nweights, θmerged = 1\nn\nPn\ni=1 θi.\nTask Arithmetic (TA) uses a scaling factor to\nweight multiple task vectors estimated on a small\ndevelopment set, θmerged = θ0 + λ Pn\ni=1 τi.\nMP-Merging performs fine-grained magnitude\npruning on task vectors to reduce redundancy at\nthe detail level, θmerged = θ0 + λ Pn\ni=1 MP(τi).\nSVP-Merging performs coarse-grained singu-\nlar value pruning on task vectors to reduce re-\ndundancy at the structure level, θmerged = θ0 +\nλ Pn\ni=1 SVP(τi). (see Section 3.1.2).\nMoreover, we compare methods against the per-\nformance of fine-tuning on each language-task\ncombination. This is the topline of all merging\nmethods since the model is completely adapted to a\nspecific language for a specific task with optimised\nconfigurations and without any language conflicts.\n5\nEvaluation Results and Analysis\n5.1\nMulti-Lingual Model Merging\nFirst, we investigate the merging of finetuned mod-\nels for different languages on the same task, which\n\nTable 1: Multi-lingual ASR model merging. Finetuned\nis the topline where the model is finetuned on each lan-\nguage independently, and Avg. averages WER directly.\nSystem\nWER↓\nca\nde\nes\nfr\nit\nAvg.\nPretrained\n20.6\n19.6\n14.7\n24.5\n19.4\n19.88\nFinetuned\n19.5\n19.7\n14.4\n22.1\n19.2\n19.05\nMulti-lingual training\n17.1\n21.8\n15.1\n22.6\n21.9\n19.69\nWeight Averaging\n19.1\n19.1\n14.2\n24.5\n20.3\n19.55\nTask Arithmetic\n19.1\n18.8\n13.9\n24.0\n19.8\n19.23\nMP-Merging\n19.4\n19.3\n14.0\n23.8\n18.1\n19.03\nSVP-Merging\n19.5\n19.3\n14.2\n23.6\n18.4\n19.11\nLoRS-Merging\n19.0\n18.8\n13.9\n23.5\n18.5\n18.85\nTable 2: Multi-lingual ST model merging. Finetuned\nis the topline where the model is finetuned on each lan-\nguage independently, and Avg. averages BLEU directly.\nSystem\nBLEU↑\nca\nde\nes\nfr\nit\nAvg.\nPretrained\n21.1\n24.1\n28.6\n26.8\n26.8\n25.48\nFinetuned\n22.6\n24.6\n29.2\n27.2\n27.3\n26.18\nMulti-lingual training\n21.4\n24.4\n28.8\n26.8\n27.2\n25.72\nWeight Averaging\n22.3\n24.1\n28.6\n27.2\n26.9\n25.82\nTask Arithmetic\n22.1\n24.3\n28.9\n27.3\n26.8\n25.88\nMP-Merging\n22.1\n24.7\n28.9\n27.3\n26.9\n25.98\nSVP-Merging\n22.1\n24.7\n29.0\n27.4\n26.8\n26.00\nLoRS-Merging\n22.2\n24.8\n29.0\n27.5\n26.9\n26.08\ncorresponds to multi-lingual single-task learning.\nLanguage knowledge interference yields im-\nbalanced improvements: Table 1 shows the multi-\nlingual results of the ASR task with the high-\nresource language set. On average, multi-lingual\ntraining slightly improves the pretrained model but\nsignificantly underperforms the finetuned models\nand merging methods. This may be due to nega-\ntive interference between the knowledge of differ-\nent languages, leading to gradient conflicts during\ntraining (Wang et al., 2020b). From a per-language\nperspective, it is observed that ca and fr achieve the\nlargest improvements during fine-tuning while still\nshowing significant improvements in multi-lingual\ntraining, whereas languages with smaller improve-\nments during finetuning exhibit a substantial perfor-\nmance drop in multi-lingual training, even worse\nthan the pretrained model. This indicates a strong\nlanguage conflict in multi-lingual training, with\nca and fr dominating. Additionally, we observe\nthat the optimal learning rates for finetuned models\nvary significantly across languages (see Appendix\nA), while the unified learning rate configuration\nrequired by multi-lingual training prevents each\nlanguage from reaching its optimal performance.\nModel merging mitigates language conflicts:\nTable 3: Multi-task model merging performed on each\nlanguage independently and WER/BLEU scores are\naveraged across languages. Finetuned is the topline\nwhere the model is finetuned on each language and task\ncombination independently. Per-language results are\nshown in Appendix C.\nSystem\nAvg. WER↓\nAvg. BLEU↑\nPretrained\n19.88\n25.48\nFinetuned\n19.05\n26.18\nMulti-task training\n19.00\n25.90\nWeight Averaging\n18.84\n26.18\nTask Arithmetic\n18.76\n26.30\nMP-Merging\n18.62\n26.40\nSVP-Merging\n18.72\n26.38\nLoRS-Merging\n18.45\n26.48\nIn contrast, model merging methods show obvi-\nous improvements across almost all languages,\ndemonstrating reduced conflict and better stability.\nAmong model merging methods, TA outperforms\nWA due to its flexible scaling factor. Both MP-\nMerging and SVP-Merging further improve the\nperformance of TA by reducing redundancy, and\nMP-Merging slightly outperforms SVP-Merging\ndue to its finer-grained pruning. Combining the ad-\nvantages of SVP and MP, LoRS-Merging achieves\nthe best performance.\nTable 2 provides the multi-lingual results on\nST task with the high-resource language set. The\nmain conclusion is consistent with the ASR task:\nmodel merging methods still significantly outper-\nform multi-lingual training, with LoRS-Merging\nachieving the best performance.\n5.2\nMulti-Task Model Merging\nNext, we merge finetuned models for different tasks\n(ASR and ST) with the same language which cor-\nresponds to multi-task single-language learning.\nASR and ST tasks for the same language\ncan mutually benefit from each other: Table\n3 presents the multi-task results with the high-\nresource language set. In general, multi-task train-\ning performs similarly to finetuned models on ASR\nbut is a lot worse on ST. This is likely due to the\nsubstantial differences in optimal hyper-parameter\nconfigurations between the two tasks. Model merg-\ning methods clearly outperform finetuned models,\nwhich not only demonstrates their effectiveness but\nalso shows the mutual benefits between ASR and\nST. In terms of performance gains, the improve-\nment in ASR is greater than in ST. We attribute this\nto the fact that ASR is inherently simpler than ST\n\nTable 4: Multi-lingual multi-task model merging. Finetuned is the topline where the model is finetuned on each\nlanguage and task combination independently, and Avg. averages WER or BLEU scores directly.\nSystem\nWER↓\nBLEU↑\nca\nde\nes\nfr\nit\nAvg.\nca\nde\nes\nfr\nit\nAvg.\nPretrained\n20.6\n19.6\n14.7\n24.5\n19.4\n19.88\n21.1\n24.1\n28.6\n26.8\n26.8\n25.48\nFinetuned\n19.5\n19.7\n14.4\n22.1\n19.2\n19.05\n22.6\n24.6\n29.2\n27.2\n27.3\n26.18\nML and MT training\n20.5\n19.7\n14.6\n24.5\n19.4\n19.86\n21.3\n24.3\n28.3\n27.1\n26.9\n25.58\nML and MT Task Arithmetic\n18.9\n19.2\n14.1\n23.7\n18.4\n18.96\n22.2\n24.4\n29.0\n27.3\n26.9\n25.96\nML and MT LoRS-Merging\n18.7\n19.1\n14.0\n23.8\n18.0\n18.82\n22.2\n24.8\n29.0\n27.5\n27.0\n26.10\nMT training\n17.0\n19.7\n14.4\n24.2\n19.4\n19.00\n22.3\n24.6\n28.7\n27.0\n26.9\n25.90\n,→+ ML Task Arithmetic\n18.1\n19.0\n14.2\n24.5\n20.6\n19.37\n22.7\n24.7\n28.6\n27.3\n26.5\n25.96\n,→+ ML LoRS-Merging\n18.1\n19.0\n14.1\n24.2\n20.3\n19.23\n22.4\n24.5\n29.1\n27.6\n26.7\n26.06\nML training\n17.1\n21.8\n15.1\n22.6\n21.9\n19.69\n21.4\n24.4\n28.8\n26.8\n27.2\n25.72\n,→+ MT Task Arithmetic\n17.1\n18.5\n13.3\n22.7\n18.0\n18.00\n22.6\n25.0\n29.2\n27.5\n26.9\n26.24\n,→+ MT LoRS-Merging\n16.9\n18.3\n13.3\n22.4\n17.8\n17.82\n22.8\n25.2\n29.3\n27.6\n27.0\n26.38\nand can be viewed as a step in the ST task. Further-\nmore, as before, model merging methods combined\nwith pruning further improve performance, and the\nproposed LoRS-Merging achieves the best perfor-\nmance across the table.\n5.3\nMulti-Lingual Multi-Task Model Merging\nThen, we investigate the merging of finetuned mod-\nels for both different languages and tasks, which\ncorrespond to multi-lingual (ML) and multi-task\n(MT) learning. Specifically, we explore 4 different\ntraining and merging settings:\nML and MT training: Fine-tuning on all lan-\nguages and both tasks jointly.\nML and MT merging: Fine-tuning on each\nlanguage for each task separately and merging all.\nMT training and ML merging: Fine-tuning\nboth tasks jointly for each language, and merging\nmodels from different languages.\nML training and MT merging: Fine-tuning on\nall languages jointly for each task, and merging\nmodels from different tasks.\nTable 4 displays the multi-lingual and multi-\ntask results with the high-resource language set.\nMulti-lingual and multi-task training shows little\nimprovement over the pretrained model, due to\nnegative interference during training and the use of\na unified training configuration for all languages\nand tasks. Nevertheless, the performance of multi-\nlingual and multi-task merging is on par with that\nof finetuned models, further underscoring the supe-\nriority of model merging. ML training followed by\nMT merging achieves the best performance, even\nsignificantly outperforming finetuned models. Al-\nthough we did not observe the same phenomenon\nWER\nBLEU\nNumber of Languages\nNumber of Languages\nFigure 3: WER and BLEU against the number of lan-\nguages. Performance is averaged across all languages\nand all training runs of language combinations.\non the low-resource language set, this suggests the\npotential of using a combination of training and\nmerging to achieve better performance. We pro-\nvide additional experiments on the low-resource\nlanguage set in Appendix B to demonstrate the ro-\nbustness and generalisability of model merging and\nLoRS-Merging.\n5.4\nEffect of Numbers of Languages\nTo further demonstrate the robustness of LoRS-\nMerging to language selection, experiments are\nperformed using different numbers of languages.\nFigure 3 shows the average performance across\nall languages and all training runs with possible\ncombinations of 2, 3, 4 or 5 languages.\nLoRS-Merging improvements are consistent\nacross different numbers of languages: As the\nnumber of languages increases, the performance\nof both TA and LoRS-Merging degrades due to\nnegative interference between languages. LoRS-\nMerging consistently outperforms TA in both ASR\nand ST tasks, and even surpasses the finetuned\nmodels in the ASR task. This is likely due to\n\nTraining Data Size (Hours)\nTraining Data Size (Hours)\nWER\nBLEU\nFine-tuned\nMultilingual/Multitask Training\nTA Merging\nLoRS-Merging\nFigure 4: Performance variation against different train-\ning data sizes (number of hours for each language) on\nASR (top) and ST (bottom) tasks.\nLoRS-Merging further reducing model redundancy,\ntherefore alleviating negative interference. Addi-\ntionally, we observe that the optimal learning rate\nfor the finetuned ASR model is significantly larger\ncompared to the ST task. This may lead to over-\nfitting, whereas LoRS-Merging improves model\ngeneralisation through model merging while reduc-\ning language interference, thus outperforming the\nfinetuned models for the ASR task.\n5.5\nEffect of Language Data Scale\nWe then demonstrate the robustness of merging\nmethods to different training data sizes for both\ntasks. Fig. 4 shows the WER (top) and BLEU\n(bottom) scores for ASR and ST at different data\nscales, respectively. As the data scale increases, the\nperformance of multi-lingual training does not al-\nways improve. This may be because the pretrained\nmodel already performs well, and the significant\nlanguage interference and conflict in multi-lingual\ntraining hinder the effective improvement of multi-\nlanguage performance. Furthermore, the perfor-\nmance loss of model merging increases with data\nscale, compared to finetuned models. It can be ex-\nplained by the fact that larger training data tends\nto increase the divergence in the optimisation tra-\njectories of different finetuned models, resulting in\n1\n0.8 0.6 0.4 0.2 0.1 0.050.020.01\nRatio\n17.5\n18.0\n18.5\n19.0\nWER\nSVP\n5 hours\n10 hours\n20 hours\n1\n0.8\n0.6\n0.4\n0.2\n0.1\nRatio\n16.5\n17.0\n17.5\n18.0\n18.5\n19.0\nWER\nMP\n5 hours\n10 hours\n20 hours\nFigure 5: Model performance against the retain ratio\n(1 means to retain all weights and 0 means to prune all\nweights) in SVP (left) and MP (right) for ASR finetuned\nmodels. Three different training data sizes are used.\nthe breakdown of linear mode connectivity, which\nleads to a greater performance loss. Moreover,\nLoRS-Merging still achieves obvious and stable\nimprovement compared to TA.\n5.6\nAnalysis of Model Redundancy\nFurthermore, we justify the necessity of SVP and\nMP to remove model redundancy by showing the\nmodel performance against the pruning ratio of\nfinetuned models for ASR as shown in Fig. 5. As\nshown, both SVP and MP significantly improve the\nperformance of finetuned models, indicating the\npresence of substantial redundancy in the structure\nand details of the finetuned models, respectively.\nThe model performance reaches the best at a high\npruning level, indicating that the redundancy is\nparticularly large for ASR. We observed a much\nsmaller redundancy in ST, which also explains\nthe observation that LoRS-Merging achieves more\nsalient improvement on ASR than ST. Moreover,\nredundancy increases with training data, possibly\ndue to the accumulation of gradient noise during\ntraining. MP achieves greater performance gains\nthan SVP, indicating more redundancy at the detail\nlevel, which is better addressed by fine-grained MP.\n6\nConclusion\nThis paper explores model merging for multi-\nlingual ASR and ST on pre-trained speech models\nand proposes the LoRS-Merging approach. LoRS-\nMerging combines low-rank and sparse pruning\nto retain essential structures, eliminate redundant\nparameters and mitigate language and task inter-\nference. Experiments across 10 languages show\nthat LoRS-Merging effectively alleviates language\ninterference and significantly outperforms multi-\nlingual multi-task training baselines.\n\n7\nLimitations\nThere are three main limitations of this work. First,\nas a common limitation of all model merging meth-\nods, the same model structure is required across\nall tasks and languages. This is less of a concern\nunder the current trend of using the same Trans-\nformer structure, but methods need to be developed\nin the future to accommodate subtle structural dif-\nferences. Second, reasonably-sized training sets\nare required for each language, and low-resource\nlanguages may suffer from reduced improvements.\nThird, this work mainly explores the two most pop-\nular S2T tasks. Other possible tasks can be ex-\nplored in future work, including spoken language\nunderstanding and speaker adaptation.\nReferences\nSamuel Ainsworth, Jonathan Hayase, and Siddhartha\nSrinivasa. 2023. Git re-basin: Merging models mod-\nulo permutation symmetries. In International Con-\nference on Learning Representations.\nHe Bai, Yu Zhou, Jiajun Zhang, Liang Zhao, Mei-Yuh\nHwang, and Chengqing Zong. 2018. Source-critical\nreinforcement learning for transferring spoken lan-\nguage understanding to a new language. In Inter-\nnational Conference on Computational Linguistics,\npages 3597–3607.\nLoïc Barrault, Yu-An Chung, Mariano Cora Meglioli,\nDavid Dale, Ning Dong, Paul-Ambroise Duquenne,\nHady Elsahar, Hongyu Gong, Kevin Heffernan, John\nHoffman, et al. 2023. Seamlessm4t-massively mul-\ntilingual & multimodal machine translation. arXiv\npreprint arXiv:2308.11596.\nWilliam Chan, Navdeep Jaitly, Quoc Le, and Oriol\nVinyals. 2016. Listen, attend and spell: A neural\nnetwork for large vocabulary conversational speech\nrecognition. In IEEE International Conference on\nAcoustics, Speech and Signal Processing, pages 4960–\n4964.\nYong Cheng, Yu Zhang, Melvin Johnson, Wolfgang\nMacherey, and Ankur Bapna. 2023. Mu2slam: multi-\ntask, multilingual speech and language models. In In-\nternational Conference on Machine Learning, pages\n5504–5520.\nMattia A Di Gangi, Matteo Negri, and Marco Turchi.\n2019. One-to-many multilingual end-to-end speech\ntranslation. In IEEE Automatic Speech Recognition\nand Understanding Workshop, pages 585–592.\nDaxiang Dong, Hua Wu, Wei He, Dianhai Yu, and\nHaifeng Wang. 2015. Multi-task learning for mul-\ntiple language translation.\nIn Annual Meeting of\nthe Association for Computational Linguistics, pages\n1723–1732.\nFelix Draxler, Kambis Veschgini, Manfred Salmhofer,\nand Fred Hamprecht. 2018. Essentially no barriers\nin neural network energy landscape. In International\nConference on Machine Learning, pages 1308–1317.\nRahim Entezari, Hanie Sedghi, Olga Saukh, and\nBehnam Neyshabur. 2021. The role of permutation\ninvariance in linear mode connectivity of neural net-\nworks. In International Conference on Learning Rep-\nresentations.\nJonathan Frankle, Gintare Karolina Dziugaite, Daniel\nRoy, and Michael Carbin. 2020. Linear mode con-\nnectivity and the lottery ticket hypothesis. In Inter-\nnational Conference on Machine Learning, pages\n3259–3269.\nAnmol Gulati, James Qin, Chung-Cheng Chiu, Niki\nParmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo\nWang, Zhengdong Zhang, Yonghui Wu, et al. 2020.\nConformer: Convolution-augmented transformer for\nspeech recognition. In Interspeech, pages 6–10.\nYifei He, Yuzheng Hu, Yong Lin, Tong Zhang, and Han\nZhao. 2024. Localize-and-stitch: Efficient model\nmerging via sparse task arithmetic. Transactions on\nMachine Learning Research.\nJui-Yang Hsu, Yuan-Jui Chen, and Hung-yi Lee. 2020.\nMeta learning for end-to-end low-resource speech\nrecognition. In IEEE International Conference on\nAcoustics, Speech and Signal Processing, pages 7844–\n7848.\nGabriel Ilharco, Marco Tulio Ribeiro, Mitchell Worts-\nman, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali\nFarhadi. 2023. Editing models with task arithmetic.\nIn International Conference on Learning Representa-\ntions.\nPavel Izmailov, Dmitrii Podoprikhin, Timur Garipov,\nDmitry Vetrov, and Andrew Gordon Wilson. 2018.\nAveraging weights leads to wider optima and bet-\nter generalization. In International Conference on\nUncertainty in Artificial Intelligence, pages 876–885.\nMelvin Johnson, Mike Schuster, Quoc V Le, Maxim\nKrikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,\nFernanda Viégas, Martin Wattenberg, Greg Corrado,\net al. 2017. Google’s multilingual neural machine\ntranslation system: Enabling zero-shot translation.\nTransactions of the Association for Computational\nLinguistics, 5:339–351.\nAnjuli Kannan, Arindrima Datta, Tara N Sainath, Eu-\ngene Weinstein, Bhuvana Ramabhadran, Yonghui\nWu, Ankur Bapna, Zhifeng Chen, and Seungji Lee.\n2019. Large-scale multilingual speech recognition\nwith a streaming end-to-end model. In Interspeech,\npages 2130–2134.\nArham Khan, Todd Nief, Nathaniel Hudson, Mansi\nSakarvadia, Daniel Grzenda, Aswathy Ajith, Jordan\nPettyjohn, Kyle Chard, and Ian Foster. 2024. Sok:\nOn finding common ground in loss landscapes us-\ning deep model merging techniques. arXiv preprint\narXiv:2410.12927.\n\nYoohwan Kwon and Soo-Whan Chung. 2023. Mole:\nMixture of language experts for multi-lingual auto-\nmatic speech recognition. In IEEE International Con-\nference on Acoustics, Speech and Signal Processing,\npages 1–5.\nCheng-I Jeff Lai, Yang Zhang, Alexander H Liu, Shiyu\nChang, Yi-Lun Liao, Yung-Sung Chuang, Kaizhi\nQian, Sameer Khurana, David Cox, and Jim Glass.\n2021.\nParp: Prune, adjust and re-prune for self-\nsupervised speech recognition. In Advances in Neu-\nral Information Processing Systems, pages 21256–\n21272.\nHang Le, Juan Pino, Changhan Wang, Jiatao Gu, Didier\nSchwab, and Laurent Besacier. 2021. Lightweight\nadapter tuning for multilingual speech translation. In\nAnnual Meeting of the Association for Computational\nLinguistics, pages 817–824.\nYann LeCun, John Denker, and Sara Solla. 1989. Opti-\nmal brain damage. In Advances in Neural Informa-\ntion Processing Systems, pages 598–605.\nDanni Liu and Jan Niehues. 2024. Recent highlights\nin multilingual and multimodal speech translation.\nIn International Conference on Spoken Language\nTranslation, pages 235–253.\nYizhou Lu, Mingkun Huang, Xinghua Qu, Pengfei\nWei, and Zejun Ma. 2022. Language adaptive cross-\nlingual speech representation learning with sparse\nsharing sub-networks. In IEEE International Con-\nference on Acoustics, Speech and Signal Processing,\npages 6882–6886.\nVaishnavh Nagarajan and J Zico Kolter. 2019. Uniform\nconvergence may be unable to explain generalization\nin deep learning. In Advances in Neural Information\nProcessing Systems, pages 11611–11622.\nRohit Prabhavalkar, Takaaki Hori, Tara N Sainath, Ralf\nSchlüter, and Shinji Watanabe. 2023. End-to-end\nspeech recognition: A survey. IEEE/ACM Transac-\ntions on Audio, Speech, and Language Processing,\n32:325–351.\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brock-\nman, Christine McLeavey, and Ilya Sutskever. 2023.\nRobust speech recognition via large-scale weak su-\npervision. In International Conference on Machine\nLearning, pages 28492–28518.\nPaul\nK Rubenstein,\nChulayuth\nAsawaroengchai,\nDuc Dung Nguyen, Ankur Bapna, Zalán Borsos,\nFélix de Chaumont Quitry, Peter Chen, Dalia El\nBadawy, Wei Han, Eugene Kharitonov, et al. 2023.\nAudiopalm: A large language model that can speak\nand listen. arXiv preprint arXiv:2306.12925.\nAFM Saif, Lisha Chen, Xiaodong Cui, Songtao Lu,\nBrian Kingsbury, and Tianyi Chen. 2024. M2asr:\nMultilingual multi-task automatic speech recogni-\ntion via multi-objective optimization. In Interspeech,\npages 1240–1244.\nSidak Pal Singh and Martin Jaggi. 2020. Model fusion\nvia optimal transport. In Advances in Neural Infor-\nmation Processing Systems, pages 22045–22055.\nHaoran Sun, Xiaohu Zhao, Yikun Lei, Deyi Xiong, et al.\n2023. Towards a deep understanding of multilingual\nend-to-end speech translation. In Findings of Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 14332–14348.\nNorman Tatro, Pin-Yu Chen, Payel Das, Igor Melnyk,\nPrasanna Sattigeri, and Rongjie Lai. 2020.\nOpti-\nmizing mode connectivity via neuron alignment. In\nAdvances in Neural Information Processing Systems,\npages 15300–15311.\nChanghan Wang, Morgane Riviere, Ann Lee, Anne Wu,\nChaitanya Talnikar, Daniel Haziza, Mary Williamson,\nJuan Pino, and Emmanuel Dupoux. 2021. Voxpopuli:\nA large-scale multilingual speech corpus for repre-\nsentation learning, semi-supervised learning and in-\nterpretation. In Annual Meeting of the Association\nfor Computational Linguistics, pages 993–1003.\nChanghan Wang, Anne Wu, and Juan Pino. 2020a. Cov-\nost 2 and massively multilingual speech-to-text trans-\nlation. arXiv preprint arXiv:2007.10310.\nKe Wang, Nikolaos Dimitriadis, Guillermo Ortiz-\nJimenez, François Fleuret, and Pascal Frossard. 2024.\nLocalizing task information for improved model\nmerging and compression. In International Confer-\nence on Machine Learning, pages 50268–50287.\nWenxuan Wang, Guodong Ma, Yuke Li, and Binbin\nDu. 2023. Language-routing mixture of experts for\nmultilingual and code-switching speech recognition.\nIn Interspeech, pages 1389–1393.\nZirui Wang, Zihang Dai, Barnabás Póczos, and Jaime\nCarbonell. 2019. Characterizing and avoiding neg-\native transfer. In IEEE Conference on Computer\nVision and Pattern Recognition, pages 11293–11302.\nZirui Wang, Zachary C Lipton, and Yulia Tsvetkov.\n2020b. On negative interference in multilingual mod-\nels: Findings and a meta-learning treatment. In Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 4438–4450.\nYongxian Wei, Anke Tang, Li Shen, Feng Xiong, Chun\nYuan, and Xiaochun Cao. 2025. Modeling multi-\ntask model merging as adaptive projective gradient\ndescent. arXiv preprint arXiv:2501.01230.\nYubei Xiao, Ke Gong, Pan Zhou, Guolin Zheng, Xiao-\ndan Liang, and Liang Lin. 2021. Adversarial meta\nsampling for multilingual low-resource speech recog-\nnition. In AAAI Conference on Artificial Intelligence,\npages 14112–14120.\nFeng Xiong, Runxi Cheng, Wang Chen, Zhanqiu Zhang,\nYiwen Guo, Chun Yuan, and Ruifeng Xu. 2024.\nMulti-task model merging via adaptive weight disen-\ntanglement. arXiv preprint arXiv:2411.18729.\n\nChen Xu, Rong Ye, Qianqian Dong, Chengqi Zhao,\nTom Ko, Mingxuan Wang, Tong Xiao, and Jingbo\nZhu. 2023. Recent advances in direct speech-to-text\ntranslation. In International Joint Conference on\nArtificial Intelligence, pages 6796–6804.\nPrateek Yadav, Derek Tam, Leshem Choshen, Colin A\nRaffel, and Mohit Bansal. 2024. Ties-merging: Re-\nsolving interference when merging models. In Ad-\nvances in Neural Information Processing Systems,\npages 7093–7115.\nEnneng Yang, Li Shen, Guibing Guo, Xingwei Wang,\nXiaochun Cao, Jie Zhang, and Dacheng Tao. 2024a.\nModel merging in llms, mllms, and beyond: Meth-\nods, theories, applications and opportunities. arXiv\npreprint arXiv:2408.07666.\nEnneng Yang, Li Shen, Zhenyi Wang, Guibing Guo,\nXiaojun Chen, Xingwei Wang, and Dacheng Tao.\n2024b. Representation surgery for multi-task model\nmerging. In International Conference on Machine\nLearning, pages 56332–56356.\nLe Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin\nLi. 2024. Language models are super mario: Absorb-\ning abilities from homologous models as a free lunch.\nIn International Conference on Machine Learning,\npages 57755–57775.\nBiao Zhang, Philip Williams, Ivan Titov, and Rico Sen-\nnrich. 2020. Improving massively multilingual neu-\nral machine translation and zero-shot translation. In\nAnnual Meeting of the Association for Computational\nLinguistics, pages 1628–1639.\nChao Zhang, Bo Li, Tara Sainath, Trevor Strohman,\nSepand Mavandadi, Shuo yiin Chang, and Parisa\nHaghani. 2022. Streaming end-to-end multilingual\nspeech recognition with joint language identification.\nIn Interspeech, pages 3223–3227.\nChao Zhang, Bo Li, Tara N. Sainath, Trevor Strohman,\nand Shuo yiin Chang. 2023a.\nUml: A universal\nmonolingual output layer for multilingual asr. In\nIEEE International Conference on Acoustics, Speech\nand Signal Processing, pages 1–5.\nWen Zhang, Lingfei Deng, Lei Zhang, and Dongrui Wu.\n2023b. A survey on negative transfer. IEEE/CAA\nJournal of Automatica Sinica, 10:305–329.\nYun Zhu, Parisa Haghani, Anshuman Tripathi, Bhu-\nvana Ramabhadran, Brian Farris, Hainan Xu, Han\nLu, Hasim Sak, Isabel Leal, Neeraj Gaur, et al. 2020.\nMultilingual speech recognition with self-attention\nstructured parameterization. In Interspeech, pages\n4741–4745.\n\nA\nHyper-Parameter Details\nThe detailed hyper-parameter settings for each lan-\nguage are shown in Table 5 for ASR and Table 6\nfor ST, respectively.\nTable 5: ASR hyper-parameters for high-resource lan-\nguages.\nSystem\nASR\nca\nde\nes\nfr\nit\nFinetuned\nlearning rate\n1 × 10−6 5 × 10−8 1 × 10−7 1 × 10−6 5 × 10−6\nMulti-lingual training\nlearning rate\n1 × 10−5\nTask Arithmetic\nscaling factor λ\n0.15\nLoRS-Merging\nscaling factor λ\n0.15\nSVP ratio r\n5%\n3%\n2%\n1%\n1%\nMP ratio p\n40%\n60%\n40%\n10%\n10%\nTable 6: ST hyper-parameters for high-resource lan-\nguages.\nSystem\nST\nca\nde\nes\nfr\nit\nFinetuned\nlearning rate\n1 × 10−6 2 × 10−8 2 × 10−8 5 × 10−8 5 × 10−8\nMulti-lingual training\nlearning rate\n5 × 10−9\nTask Arithmetic\nscaling factor λ\n0.15\nLoRS-Merging\nscaling factor λ\n0.15\nSVP ratio r\n5%\n3%\n5%\n2%\n1%\nMP ratio p\n60%\n40%\n20%\n20%\n20%\nB\nResults of Low-Resource Language Set\nThe results of the low-resource language set are\nshown in this section. Specifically, Table 7 and\n8 show the multi-lingual single task training and\nmerging for ASR and ST respectively.\nTable 7: Multi-lingual ASR model merging. Finetuned\nis the topline where the model is finetuned on each lan-\nguage independently, and Avg. averages WER directly.\nSystem\nWER↓\nid\nnl\npt\nru\nsv\nAvg.\nPretrained\n16.9\n16.0\n10.1\n17.1\n17.1\n15.43\nFinetuned\n15.0\n14.8\n9.7\n16.8\n14.7\n14.20\nMulti-lingual training\n16.7\n15.5\n10.0\n17.0\n16.6\n15.14\nWeight Averaging\n15.7\n15.2\n10.1\n17.1\n15.8\n14.77\nTask Arithmetic\n15.7\n15.1\n9.9\n17.0\n15.8\n14.69\nMP-Merging\n15.7\n15.1\n10.0\n16.7\n15.7\n14.63\nSVP-Merging\n15.7\n15.1\n9.9\n16.9\n15.7\n14.65\nLoRS-Merging\n15.7\n15.1\n9.7\n16.8\n15.6\n14.57\nTable 8: Multi-lingual ST model merging. Finetuned\nis the topline where the model is finetuned on each lan-\nguage independently, and Avg. averages BLEU directly.\nSystem\nBLEU↑\nid\nnl\npt\nru\nsv\nAvg.\nPretrained\n32.5\n31.6\n43.3\n35.5\n32.1\n35.00\nFinetuned\n35.2\n34.0\n43.8\n36.7\n37.6\n37.46\nMulti-lingual training\n32.3\n33.2\n43.5\n35.4\n34.3\n35.74\nWeight Averaging\n33.6\n32.2\n43.2\n35.3\n34.2\n35.70\nTask Arithmetic\n33.9\n32.8\n43.1\n35.5\n34.3\n35.92\nMP-Merging\n33.8\n32.8\n43.5\n35.8\n34.0\n35.98\nSVP-Merging\n33.6\n32.6\n43.4\n35.6\n34.3\n35.90\nLoRS-Merging\n33.9\n32.8\n43.2\n35.9\n34.5\n36.06\nThen, Table 9 shows the uni-lingual multi-task\ntraining and merging performance (c.f. compare to\n3 for high-resource languages).\nLast, Table 10 shows the results of multi-lingual\nand multi-task training and merging results for low-\nresource languages (compare to Table 4 for high-\nresource languages.). LoRS-Merging achieved the\nbest performance across all merging and training\nmethods in all tables.\nC\nDetailed Results on Multi-task merging\nDetailed per-language results of Table 3 are shown\nin Table 11.\n\nTable 9: Multi-task model merging. Finetuned is the topline where the model is finetuned on each language and task\ncombination independently, and Avg. averages WER or BLEU scores directly.\nSystem\nWER↓\nBLEU↑\nid\nnl\npt\nru\nsv\nAvg.\nid\nnl\npt\nru\nsv\nAvg.\nPretrained\n16.9\n16.0\n10.1\n17.1\n17.1\n15.43\n32.5\n31.6\n43.3\n35.5\n32.1\n35.00\nFinetuned\n15.0\n14.8\n9.7\n16.8\n14.7\n14.20\n35.2\n34.0\n43.8\n36.7\n37.6\n37.46\nMulti-task training\n15.4\n15.0\n9.3\n16.6\n14.3\n14.12\n35.3\n33.7\n43.6\n36.2\n35.8\n36.92\nWeight Averaging\n14.7\n14.9\n9.3\n16.6\n13.8\n13.88\n35.4\n33.9\n44.1\n36.3\n35.9\n37.12\nTask Arithmetic\n14.6\n14.9\n9.3\n16.5\n14.0\n13.88\n35.3\n33.8\n44.3\n36.1\n36.4\n37.18\nMP-Merging\n14.4\n14.7\n9.4\n16.5\n13.8\n13.78\n35.7\n33.9\n44.3\n36.1\n36.1\n37.22\nSVP-Merging\n14.6\n14.8\n9.2\n16.4\n13.9\n13.80\n35.3\n33.9\n44.3\n36.2\n36.3\n37.20\nLoRS-Merging\n14.4\n14.7\n9.2\n16.4\n13.8\n13.72\n35.6\n33.9\n44.3\n36.3\n36.4\n37.30\nTable 10: Multi-lingual multi-task model merging. Finetuned is the topline where the model is finetuned on each\nlanguage and task combination independently, and Avg. averages WER or BLEU scores directly.\nSystem\nWER↓\nBLEU↑\nid\nnl\npt\nru\nsv\nAvg.\nid\nnl\npt\nru\nsv\nAvg.\nPretrained\n16.9\n16.0\n10.1\n17.1\n17.1\n15.43\n32.5\n31.6\n43.3\n35.5\n32.1\n35.00\nFinetuned\n15.0\n14.8\n9.7\n16.8\n14.7\n14.20\n35.2\n34.0\n43.8\n36.7\n37.6\n37.46\nML and MT training\n16.9\n15.7\n9.6\n17.0\n16.3\n15.08\n32.8\n32.9\n43.3\n35.4\n32.6\n35.40\nML and MT Task Arithmetic\n16.4\n15.5\n9.6\n16.8\n15.7\n14.79\n33.7\n33.1\n43.2\n35.7\n34.9\n36.12\nML and MT LoRS-Merging\n16.1\n15.5\n9.5\n16.8\n15.7\n14.72\n33.7\n33.2\n43.5\n35.8\n34.9\n36.22\nMT training\n15.4\n15.0\n9.3\n16.6\n14.3\n14.12\n35.3\n33.7\n43.6\n36.2\n35.8\n36.92\n,→+ ML Task Arithmetic\n16.0\n15.5\n9.5\n16.9\n15.4\n14.66\n34.1\n32.8\n43.7\n35.6\n33.3\n35.90\n,→+ ML LoRS-Merging\n16.1\n15.3\n9.4\n16.8\n15.3\n14.57\n34.2\n32.7\n43.8\n35.8\n33.5\n36.00\nML training\n16.7\n15.5\n10.0\n17.0\n16.6\n15.14\n32.3\n33.2\n43.5\n35.4\n34.3\n35.74\n,→+ MT Task Arithmetic\n17.1\n15.5\n9.5\n17.0\n15.5\n14.89\n32.1\n33.1\n43.6\n35.7\n33.6\n35.62\n,→+ MT LoRS-Merging\n16.9\n15.5\n9.4\n16.8\n15.5\n14.80\n32.6\n33.2\n43.6\n35.9\n33.6\n35.78\nTable 11: Multi-task model merging. Finetuned is the topline where the model is finetuned on each language and\ntask combination independently, and Avg. averages WER or BLEU scores directly.\nSystem\nWER↓\nBLEU↑\nca\nde\nes\nfr\nit\nAvg.\nca\nde\nes\nfr\nit\nAvg.\nPretrained\n20.6\n19.6\n14.7\n24.5\n19.4\n19.88\n21.1\n24.1\n28.6\n26.8\n26.8\n25.48\nFinetuned\n19.5\n19.7\n14.4\n22.1\n19.2\n19.05\n22.6\n24.6\n29.2\n27.2\n27.3\n26.18\nMulti-task training\n17.0\n19.7\n14.4\n24.2\n19.4\n19.00\n22.3\n24.6\n28.7\n27.0\n26.9\n25.90\nWeight Averaging\n17.1\n19.6\n13.9\n23.7\n19.6\n18.84\n22.9\n24.4\n29.0\n27.7\n26.9\n26.18\nTask Arithmetic\n17.2\n19.3\n14.0\n23.3\n19.7\n18.76\n23.4\n24.5\n28.9\n27.7\n27.0\n26.30\nMP-Merging\n17.8\n19.5\n14.4\n23.8\n17.2\n18.62\n23.1\n24.5\n29.1\n27.9\n27.4\n26.40\nSVP-Merging\n18.0\n19.4\n14.4\n23.7\n17.7\n18.72\n22.9\n24.7\n29.1\n27.8\n27.4\n26.38\nLoRS-Merging\n17.5\n19.4\n14.2\n23.1\n17.7\n18.45\n23.1\n24.5\n29.3\n27.9\n27.6\n26.48\n",
  "metadata": {
    "source_path": "papers/arxiv/Low-Rank_and_Sparse_Model_Merging_for_Multi-Lingual_Speech_Recognition\n__and_Translation_fe18968c1d61ed4a.pdf",
    "content_hash": "fe18968c1d61ed4a68114bc15b87f120ff68ac83dce134658b470f1a66c9f40e",
    "arxiv_id": null,
    "title": "Low-Rank_and_Sparse_Model_Merging_for_Multi-Lingual_Speech_Recognition\n__and_Translation_fe18968c1d61ed4a",
    "author": "",
    "creation_date": "D:20250225030549Z",
    "published": "2025-02-25T03:05:49",
    "pages": 13,
    "size": 456755,
    "file_mtime": 1740470154.2450602
  }
}