{
  "text": "Disentangling Visual Transformers: Patch-level Interpretability for Image\nClassification\nGuillaume Jeanneret∗,‡, Lo¨ıc Simon†, Fr´ed´eric Jurie†\n∗ISIR - Sorbonne Universite\n†Normandy University, ENSICAEN, UNICAEN, CNRS, GREYC\nAbstract\nVisual transformers have achieved remarkable performance\nin image classification tasks, but this performance gain has\ncome at the cost of interpretability. One of the main obsta-\ncles to the interpretation of transformers is the self-attention\nmechanism, which mixes visual information across the\nwhole image in a complex way.\nIn this paper, we pro-\npose Hindered Transformer (HiT), a novel interpretable by\ndesign architecture inspired by visual transformers. Our\nproposed architecture rethinks the design of transformers\nto better disentangle patch influences at the classification\nstage. Ultimately, HiT can be interpreted as a linear com-\nbination of patch-level information. We show that the ad-\nvantages of our approach in terms of explicability come\nwith a reasonable trade-off in performance, making it an\nattractive alternative for applications where interpretabil-\nity is paramount.\n1. Introduction\nDeep learning architectures have achieved remarkable\nbreakthroughs in domains such text [19], vision [20, 46],\nor multimodal tasks [37], prompting widespread interest in\ntheir application to real-world problems [34]. However, as\nthese models are increasingly used in high-stakes scenar-\nios, understanding their decision-making process becomes\ncrucial to ensure their decisions are grounded in meaning-\nful variables rather than spurious correlations [4, 28]. This\nnecessity has driven the development of trustworthy and in-\nterpretable architectures.\nInterpretable-by-Design (ID) architectures [14] aim to\ninherently explain their decision-making process, eliminat-\ning the need for external interpretability tools.\nIdeally,\nthese architectures maintain comparable performance to tra-\nditional black-box methods while providing insights into\ntheir internal workings. Despite their promise, existing ID\narchitectures largely rely on convolutional neural networks\n‡ Work done at GREYC Laboratory, now at ISIR Laboratory\n(CNNs) as feature extractors [7, 32, 45, 65]. Given the re-\ncent success of transformer-based models [18], transition-\ning to interpretable transformers is a logical next step.\nAlthough Vision Transformers (ViTs) [18] have demon-\nstrated superior performance in computer vision tasks, the\nliterature on their explainability and interpretability remains\nsparse. While attention maps are sometimes considered in-\nterpretable, many studies [26, 54] argue that they provide\nlittle to no insight into the actual decision-making process.\nWe concur that attention maps offer only partial and insuf-\nficient cues, and instead route towards ID architectures.\nIn this paper, we address this gap by introducing a novel\ninterpretable transformer-like architecture. Our proposed\nmodel, the Hindered Transformer (HiT), advances the un-\nderstanding of ViTs by analyzing the flow of individual im-\nage patches and decomposing the classification token (CLS)\ninto contributions from each individual token. By construct-\ning predictions as the sum of these token contributions, HiT\nachieves interpretability without relying on external meth-\nods [3] or gradient-based approaches [57], classifying it as\nan ID method.\nWe summarize our contributions as follows: i) We pro-\npose the Hindered Transformer (HiT) backbone, a variant of\nvision transformers that is inherently interpretable. ii) We\nempirically validate HiT on six datasets – ImageNet [16],\nCUB 2011 [64], Stanford Dogs [30], Stanford Cars [33],\nFGVC-Aircraft [40], and Oxford-IIIT Pets [47] – demon-\nstrating its interpretability both quantitatively and qualita-\ntively, outperforming recent ID transformer-based models\nand post-hoc methods.\nTo encourage future research, we will release our code\nand pretrained weights upon publication.\n2. Related Work\nThe performance of neural networks on computer vision\ntasks is well-established, but the complexity of these mod-\nels can make them difficult to understand.\nThis lack of\ntransparency is problematic and has motivated the scien-\ntific community to develop methods for making neural net-\n1\narXiv:2502.17196v1  [cs.CV]  24 Feb 2025\n\nworks more interpretable. There are two main approaches\nto this problem: post-hoc methods, which seek to analyze\nan already-trained model, and interpretable by design ar-\nchitectures, which aim to create models whose decision-\nmaking processes are inherently transparent.\nMany post-hoc methods have been proposed in the lit-\nerature, including counterfactual explanations [4, 29, 71],\nsaliency maps [27, 49, 53, 67], and model distillation [22,\n58]. Closer to our work, a few attempts have been made\nto explain a ViT architecture via post-hoc algorithms. For\ninstance, Abnar and Zuidema [1] proposed to compute a\nscore per token by recursively propagating the attention\nmaps in a top-bottom approach. In addition, some meth-\nods extended the LRP [6] paradigm to include the attention\nheads [12, 13]. The previous methods leverage the mech-\nanisms of transformers to estimate the individual contribu-\ntion of each token. While these approaches provide insights\ninto trained models, they often rely on external tools or post-\nprocessing, which can limit their generalizability and inte-\ngration into the model itself.\nThe field of interpretable by design architectures is di-\nverse, as there is no single approach to explaining the com-\nplex behaviors neural networks. While many methods have\nbeen proposed, there has been a recent focus on prototypical\npart networks, such as ProtoPNets introduced by Chen et\nal. [14]. ProtoPNets computes class predictions based on\nthe distances between patches of the final feature map and\nsome prototypes, which can be visualized. Additionally,\nthere have been many variations of ProtoPNets proposed in\nthe literature [8, 11, 15, 17, 23, 42–44, 52, 61, 65, 66].These\nvariations aim to refine ProtoPNets to enhance performance\nand interpretability but primarily focus on convolutional\nneural networks (CNNs) [24, 56], limiting their application\nto modern transformer architectures.\nIn addition to ProtoPNets, there are other methods that\nuse alternative forms of prototypes. For example, PDis-\ncoNet [62] automatically detects parts of objects and uses\nthem for the final classification.\nSimilarly, BagNet [9]\nmimics the Bag-of-Features approach to understand the\ndecision-making process. Concept Bottleneck Models [32,\n45] use concepts to explain their decisions. Finally, a fam-\nily of networks propose interpretable layers, such as B-\ncos networks [7], that learns an easily interpretable input-\ndependent linear transformation.\nThe work of Zhang et\nal. [72] proposed convolutional interpretable layers.\nFi-\nnally, some works use some variation of decoupled net-\nworks [35, 36, 55] to highlight what a filter is looking at.\nConcerning visual transformers, several recent works\nhave attempted to make transformers more interpretable by\nmodifying their architecture. Some papers try to push the\nboundaries to include transformer-based heads [25, 31, 48,\n51] for prototype interpretability. However, they only in-\nclude a single self-attention mechanism on top of a CNN\nbackbone.\nProtoPFormer [68] suggests including a ViT\nbackbone in the prototype setup. To do this, ProtoPFormer\nincludes a prototype layer on top of both the classification\nand image tokens. However, this architecture does not guar-\nantee that the tokens contain purely local information, espe-\ncially since computing a classification on top of image to-\nkens increases the receptive fields of the tokens [50]. While\ninnovative, this architecture does not ensure that the tokens\ncontain purely local information due to the increased recep-\ntive fields of tokens when combining classification and im-\nage tokens [50]. B-cos networks V2 [10] use the same ra-\ntional as the original B-cos [7] approach, but they extend\nit to transformers. In a few words, B-cos networks summa-\nrize their inner workings as a single linear function, creating\nthe attribution map by simply multiplying the input and the\nsummarized network. However, this extension still relies on\nconvolutional layers as the initial feature extraction stage,\nlimiting its interpretability scope. Finally, A-ViT [69] was\noriginally designed for faster inference by removing tokens\nat certain layers. Despite this, this mechanism serves as an\ninterpretable system, as the most important tokens are re-\ntained until the last layer. This approach, however, focuses\non efficiency rather than interpretability as its primary goal.\nIn contrast with this literature, we propose an architec-\nture interpretable by design, by adapting the main building\nblocks of vision transformers to disentangle the contribu-\ntions of each image patch. This enables us to compute the\nsalient regions of the image without the need for any non-\ntraditional training or invasive methods. Unlike other ap-\nproaches, we avoid relying on the attention mechanism of\nthe multi-head attention (MHA) block to produce saliency\nmaps. Instead, our network inherently generates these maps\nas part of its decision-making process.\n3. Methodology\nIn this section, we present our proposed approach and the\nrationale behind it. First, in §3.1, we present the prelim-\ninaries for the multi-head attention mechanism and ViTs.\nNext, in §3.2 we will show that attention and multi-head at-\ntention output can be decomposed into the individual contri-\nbutions of the inputs. Finally, in §3.3 we present our novel\narchitecture, the Hindered Transformer (HiT). The core of\nour method is to minimise the mixing of patch-level infor-\nmation, which allows us to express the classification token\n(CLS) in ViTs as the sum of individual tokens, a direct re-\nsult of §3.2. In other words, this simplification allows us to\ncheck the contribution of each token.\n3.1. Preliminaries: Transformers, ViTs and Nota-\ntions\nThe transformer architecture is built upon the Scaled Dot-\nProduct Attention operation [63], commonly referred to\nas the attention.\nGiven a query token sequence xq ∈\n2\n\nRLq×dmodel and a target sequence (or key-value sequence)\nxt ∈RLt×dmodel, where Lq and Lt are their respective se-\nquence lengths and dmodel is the token dimension, the at-\ntention mechanism is computed as follows:\nQ = xq WQ + bQ\nK = xt WK + bK\nV = xt WV + bV\nA(xq, xt) = softmax\n\u0012QKT\n√dk\n\u0013\nV\n(1)\nwhere the output is a sequence of the same length as xq, dk\nis the dimension of the linear transformations, and Wi ∈\nRdmodel×dk and bi ∈Rdk are the weights of the linear pro-\njection i ∈{Q, K, V }. In addition, Vaswani et al. [63] pro-\nposed to compute the attention mechanism h times in par-\nallel, setting dk = dmodel/h for each individual attention\noperation. The resulting vectors of each individual atten-\ntion, formally called heads, are concatenated and linearly\npost-processed to obtain the final result. This operation is\ncalled multi-head attention, and it is described as follows:\nMHA(xq, xt) = [A1(xq, xt); ...; Ah(xq, xt)]\n|\n{z\n}\nConcatenate h times\nWo + bo,\n(2)\nwith Ai being the ith attention mechanism in the MHA, and\nWo ∈Rdmodel×dmodel and bo ∈Rdmodel the linear transfor-\nmation parameters.\nIn computer vision, to incorporate image data into this\nsequence-based formulation, the ViT first partitions the in-\nput image into N 2 equal-sized patches and linearly projects\nthem to create the patch token sequence. Additionally, fol-\nlowing standard practice, a learnable classification token\nCLS is prepended to the patch sequence. Furthermore, each\npatch token is summed with a positional embedding to en-\ncode its spatial location within the image. For the remainder\nof the paper, the sequence x ∈R(N2+1)×dmodel denotes the\nconcatenation of the patch tokens and the CLS token, where\nx[0] corresponds to the CLS token.\nThe main ViT block builds on the MHA operation, fol-\nlowed by a token-wise MLP block, as in text-based trans-\nformers. Formally, given a set of patches xl at layer l, the\nViT block first computes a globalized set of tokens using\nthe MHA block. The resulting output is summed with a\nskip connection. Then, the output is fed into a token-wise\nMLP to post-process each token, followed, again, by a skip\nconnection. This block is summarized as follows\nx′\nl = xl + MHA(xl, xl)\nxl+1 = x′\nl + MLP(x′\nl)\n(3)\nFor the rest of the paper, we will use the terms token and patch inter-\nchangeably, referring to the image patch tokens.\nNote that before the MHA and MLP blocks, a Layer-\nNorm [5] operation is applied to the data sequence, but for\nsimplicity, we omit this operation. Finally, the CLS token\nis fed into a LayerNorm followed by a linear classifier to\nproduce the logits of the classification task.\n3.2. Multi-Head Attention and Patch Mixing in\nTransformers\nIn this section, we aim to decompose the MHA operation\nto demonstrate that it is possible to retrieve the individual\ncontributions of each token. In this way, we aim to lay the\nfoundation for our architecture, which is described in the\nnext section.\nLet’s start by focusing on the attention operation (Eq. 1).\nSince we will focus on the CLS token later, and to simplify\nthe analysis, let’s assume that the query sequence has length\nLq = 1. Consequently, the attention mechanism can be\nrewritten as\nA(xq, xt) =\nX\nv∈xt\na(v, xq, xt)(v WV + bv),\n(4)\nwhere a(v, xq, xt) is the attention of a single token v ∈xt.\nHere, Eq. 4 shows that we can decompose the attention\nmechanism into separately processed patches - each patch v\nin xt adds a(v, xq, xt)(v WV + bv). Accordingly, if xt con-\ntains purely local information, the output of the attention is\na sum of local data.\nTo continue, we incorporate the previous observation\ninto multi-head attention and verify that we can still unroll\nthis operation into a sum of separate vectors. One might\nbe concerned that the concatenation-linear operation will\nmix each token. However, we argue that the result is still\nvalid, since concatenating and linearly transforming the re-\nsulting vector is equivalent to linearly transforming each\nhead and adding them together. Formally, by denoting W i\nv\nand bi\nv as the weights of the linear transformation generat-\ning the value sequence of ith head, and breaking apart Wo\ninto h separate matrices, Wo = [W 1\no ; W 2\no ; ...; W h\no ], with\nW i\no ∈Rdk×dmodel, then, the MHA becomes\nMHA(xq, xt) = bo +\nX\nv∈xt\nv′(v)\nwhere\nv′(v) =\nh\nX\ni=1\na(v, xq, xt)(v W i\nv + bi\nv)W i\no.\n(5)\nThe previous result implies that we can still decompose the\nMHA result as the sum of vector patches, regardless of the\nnumber of heads in the MHA. So the same conclusion holds\nas in Eq. 4: if the content in xt is local, then we can unravel\nthe MHA mechanisms into local contributions.\n3.3. Untangling Visual Transformers\nUnlike single MHA layers, ViTs operate on global features.\nTo integrate local information, these architectures use two\n3\n\nFigure 1. ViT and HiT blocks. While the ViT block mixes the\npatch data, HiT uniquely updates the CLS via the MHA, but avoids\npost-processing the classification token in the MLP, allowing the\nCLS to be unrolled at the last layer as individual contributions.\nmechanisms: the MHA layers, which spread the informa-\ntion within tokens, and the nonlinear MLPs, which intro-\nduce complex correlations even when applied to a linear\ncombination of local contributions. For better explainabil-\nity, it would be ideal if the classifier’s decision could be\nexpressed as a combination of information from individ-\nual patches, allowing a more interpretable understanding of\nhow local information contributes to global predictions.\nIn this section, we describe our proposed architecture:\nHindered Transformer (HiT). By constraining the image to-\nkens to contain only local information along all inference\nblocks, and by avoiding mixing the CLS token, our novel\nmethod is able to partition the CLS token into each individ-\nual patch, a direct outcome of the previous section. Fig. 1\nshows the difference between the ViT block, and our block.\nThe first challenge is then constraining the data flow be-\ntween patches. To do so, we create an intermediate archi-\ntecture that uses CLS token xl[0] as the query in the MHA\noperation, and the rest of the sequence xl as the key-value\ninput. So, the output from the MHA is a single token that\nis summed to xl[0]. Then, as in ViTs, we will post-process\neach token in the sequence with the MLP. Thus, the ViT\nupdate function in Eq. 3 is transformed to\nx′\nl[0] = xl[0] + MHA(xl[0], xl)\nx′\nl[1 :] = xl[1 :]\nxl+1 = x′\nl + MLP(x′\nl).\n(6)\nFigure 2. Saliency Maps computation using HiT. From the re-\nsults from §3.2 and the definition of our architecture, HiT enables\nto extract the individual contribution per token and per layer. By\nadding together all tokens per layer, we can rearrange the tokens\nin a spatial layout and use the linear layer `a la CAM [73] to extract\nthe contribution of each token.\nThe previous model solves one problem by limiting the\nmerging of data in local patches. However, processing the\nCLS token through the MLP mixes the local information\nprovided by the MHA block, as well as the value and out-\nput operations. Since our goal is to disentangle the data\nflow into individual contributions, we need to further con-\nstrain this processing. To do this, we simply avoid updating\nthe CLS token through the MLP and passing it to the target\nsequence. So, our block inference is\nxl+1[0] = xl[0] + MHA(xl[0], xl[1 :])\nxl+1[1 :] = xl[1 :] + MLP(xl[1 :])\n(7)\nWe call the final architecture the Hindered Transformer\n(HiT), as we hinder the connections of the ViT. In a nut-\nshell, HiT only updates the CLS token via the MHA, while\nthe MLP blocks update the image patches. These restric-\ntions help to preserve purely local information in each to-\nken, while allowing the CLS token to be unrolled.\nSince the classification token is not post-processed with\nMLP or MHA, the final image classification is the sum of\nthe individual tokens in all layers, as shown in §3.2. There-\nfore, the CLS in the last layer is\nxL[0] = x0[0] +\nL−1\nX\nl=0\nMHA(xl[0], xl[1 :])\n= x0[0] +\nL−1\nX\nl=0\n\nbl\no +\nX\nv∈xl[1:]\nv′\nl(v)\n\n\n=\nL−1\nX\nl=0\nX\nv∈xl[1:]\n\u0014\nv′\nl(v) + bl\no\nN 2 + x0[0]\nLN 2\n\u0015\n.\n(8)\nPlease note that we distribute the biases bl\no of the projection\noperation in the MHA head evenly to each patch v′\nl(v). In a\nsimilar fashion, we spread x0[0] into all tokens for all layers.\nOne advantage of this architecture is that we can easily\ncompute saliency maps, as shown in Fig. 2. The double\nsum in Eq. 8 can be decomposed as a tensor RL×N2×dmodel,\n4\n\nwhere the final image representation is the sum over the first\nand second dimensions, i.e. the layer and token dimension,\nrespectively. Thus, and similarly to CAM [73], to compute\nthe regions of interest used by the model for an input image,\nwe simply run the linear classifier on each patch to get the\nmap. This rationale is similar to the LRP [6] method in the\nsense that the sum of value in the saliency is equal to the\noutput logit for that specific class.\n3.4. Token Pooling\nToken pooling [41], which involves downsampling the\nnumber of tokens as one progresses through the layers, is\ncommonly used to improve the computational efficiency of\nstandard transformers. This pooling technique effectively\naddresses the issue of representation power (as empirically\ndemonstrated in § 4.7) by expanding the receptive field of\nthe image tokens in the deeper layers. We choose to adopt\nthis approach due to its significant advantages.\nTo achieve this, we first reorganize the tokens into their\nspatial layout and then perform the pooling operation. How-\never, we need to adapt the explanation generation approach\nto accommodate the pooled tokens. Typically, this involves\nusing the backward operation of the pooling operator. In\nour case, since we rely on average pooling, which is lin-\near, the backward operation is simply the transposed opera-\ntor, which replicates each output token across the associated\n2 × 2 block and divides by 4. In other words, we distribute\nthe importance of the pooling step equally among the con-\ntributing tokens.\n4. Experiments\n4.1. Evaluation Protocols\nThe quantitative evaluation of the saliency map quality is\nperformed using the insertion-deletion metrics [49]. The\ninsertion process is performed iteratively by creating a per-\nturbed copy of the input image by progressively adding re-\ngions from the original image. This copy is initially filled\neither with zeros or with a blurred version of the image.\nStarting with the most influential regions according to the\nsaliency map and moving to the least influential regions,\neach step adds an original region in place of the perturbed\none. During this process, the class probability of the per-\nturbed image is tracked for the originally predicted class.\nThis results in a curve for each input image, where the x-\naxis represents the percentage of inserted regions and the\ny-axis represents the probability. Then, the insertion met-\nric is computed with the area under the curve (AUC) of the\nmean probability curve for all tested instances. The dele-\ntion metric works in a similar fashion, removing the origi-\nnal patches starting with the most influential and continuing\nuntil all patches are removed. If a saliency map generator\naccurately identifies the most relevant components influenc-\ning the model’s decision, it will produce a steep increase (or\ndecrease) in the insertion (or deletion) curve. A sharp transi-\ntion in these curves confirms that the highlighted regions are\nindeed those utilized by the model for classification. Con-\nsequently, a reliable saliency method will achieve a higher\n(lower) AUC for the insertion (deletion) metric.\nInsertion-deletion curves generated by different models\ncannot be directly compared if the models do not have\nthe same calibration of their outputs, which is the case\nin these experiments. Therefore, we propose normalizing\nthe insertion-deletion metrics by calculating the normalized\nAUC (nAUC). This normalization adjusts the mean proba-\nbility curve based on the maximum and minimum values of\nthe curve before computing the AUC.\nLastly,\nwhen assessing interpretable by-design ap-\nproaches, it is necessary to evaluate the trade-off between\nraw performance on the task vs the gain in interpretability.\nSpecifically, we compare the top-1 accuracy in contrast to\nthe insertion-deletion metrics.\nAs for the dataset, we evaluated HiT on six diverse\nimage classification datasets, traditionally used for evalu-\nating interpretability architectures: ImageNet [16], CUB-\n2011 [64], Stanford Dogs [30], Stanford Cars [33], FGVC-\nAircraft [40], and Oxford-IIIT Pets [47]. In the supplemen-\ntary material, we described in-depth each dataset.\n4.2. Implementation Details\nTo train HiT on ImageNet [16], we used the official DeiT3\ncodebase [60] and followed a similar setup to their method.\nModel-wise, HiT uses the same configuration as a ViT ex-\ncept that we removed the last MLP block as it is not used.\nRegarding the pooling layers, we added two. For HiT-B\nand HiT-S, we trained our models for 600 epochs using the\nAdamW optimizer [39] with a learning rate of 8 × 10−4,\na weight decay of 0.05, a batch size of 4096, 20 warm-up\nepochs, a cosine annealing scheduler [38], and ThreeAug-\nment [60] data augmentation. Unlike the DeiT3 training\nregime, we did not use the binary cross entropy loss or any\nLayerDrop [21] regularization, but the traditional cross en-\ntropy with a smoothing of 0.1 and an attention dropout of\n0.2. To fine-tune HiT in the other datasets, we trained our\nmodels similarly to the ImageNet’s configuration, but in-\nstead we set the batch size to 512, the number of epochs to\n300 and the learning rate to 5 × 10−5 for Standford Dogs\nand Oxford Pets, 4 × 10−4 for Standford Cars and FGVC-\nAircraft, and 1 × 10−4 for CUB 2011.\n4.3. Quantitative Evaluation of HiT\nIn Fig. 1, we compare the normalized insertion-deletion\ncurves obtained from HiT and other baseline models using\nthe blur strategy, as established in the literature. The base-\nlines assessed include A-ViT [69], B-cos [10], and DeiT-\nB [59]. For B-cos, the saliency maps are intrinsically gen-\n5\n\nMethod\nImageNet\nCUB 2011\nStanford Cars\nStanford Dogs\nFGVC-Aircrafts\nOxford-IIIT Pets\nI-Z (↑)\nD-Z (↓)\nI-Z (↑)\nD-Z (↓)\nI-Z (↑)\nD-Z (↓)\nI-Z (↑)\nD-Z (↓)\nI-Z (↑)\nD-Z (↓)\nI-Z (↑)\nD-Z (↓)\nHiT\n0.57\n0.12\n0.36\n0.06\n0.66\n0.09\n0.64\n0.13\n0.60\n0.08\n0.67\n0.18\nHiT + Rollout\n0.40\n0.21\n0.47\n0.19\n0.49\n0.16\n0.50\n0.21\n0.52\n0.11\n0.58\n0.29\nHiT + GradCAM\n0.49\n0.15\n0.33\n0.10\n0.57\n0.13\n0.60\n0.15\n0.53\n0.09\n0.64\n0.25\nI-B (↑)\nD-B (↓)\nI-B (↑)\nD-B (↓)\nI-B (↑)\nD-B (↓)\nI-B (↑)\nD-B (↓)\nI-B (↑)\nD-B (↓)\nI-B (↑)\nD-B (↓)\nHiT\n0.58\n0.23\n0.52\n0.14\n0.61\n0.20\n0.62\n0.27\n0.59\n0.17\n0.60\n0.26\nHiT + Rollout\n0.45\n0.31\n0.47\n0.19\n0.52\n0.28\n0.53\n0.35\n0.55\n0.2\n0.53\n0.35\nHiT + GradCAM\n0.53\n0.28\n0.49\n0.21\n0.56\n0.26\n0.61\n0.31\n0.57\n0.19\n0.60\n0.32\nTable 1. HiT and Explainability methods: We quantitatively compare HiT maps and those created by GradCAM and the modified rollout\nmatrix (mean attention) using AUC (no normalization required here). The assessment shows that HiT maps are more faithful than those\ngenerated by GradCAM or the rollout matrix. Higher insertion is better, while lower deletion is better. I and D refers to the Insertion and\nDeletion metrics, respectively. Z is the zero-corrupted image, while B is the blurred corruption strategy.\n(a) ImageNet\n(b) CUB-2011\nFigure 3. Interpretability comparison. We tested whether HiT’s\nsaliency maps provide better information than ProtoPFormer’s, A-\nViT’s maps, B-cos, the rollout attention, and GradCAM. The re-\nsults indicate that our methods are indeed more interpretable.\nerated, while for DeiT-B, we rely on post-hoc extraction\nmethods. Specifically, we use the Rollout Matrix [1] and\nGradCAM [53], referred to as DeiT-R and DeiT-GC, re-\nspectively. To create A-ViT saliency map, we utilized the\nlayer where tokens were discarded, as described in their\npaper. Additionally, to mitigate the bias of the sorting al-\ngorithm when selecting the most important tokens (which\ntends to favor top-left corner tokens first), we added a small\namount of random noise to the saliency map.\nAdditionally, we evaluate ProtoPFormer [68] on the\nCUB-2011 dataset. ProtoPFormer employs a Rollout Ma-\ntrix to filter out irrelevant tokens for its final computation,\nModel\nInterp.\nIMNet\nCUB\nDogs\nCars\nAircraft\nPets\nDeiT3-B\nχ\n83.6\n84.9\n94.0\n92.8\n85.3\n95.0\nDeiT-B\nχ\n81.1\n84.9\n93.4\n93.0\n84.9\n95.1\nB-cos-B\n✓\n74.4\n-\n-\n-\n-\n-\nHiT-B\n✓\n75.0\n79.0\n86.8\n86.2\n79.8\n88.6\nDeiT3-S\nχ\n81.4\n83.1\n90.6\n93.0\n83.9\n94.5\nDeiT-S\nχ\n79.8\n83.0\n89.6\n92.4\n83.4\n94.3\nA-ViT-S\n✓\n78.6\n-\n-\n-\n-\n-\nB-cos-S\n✓\n69.2\n-\n-\n-\n-\n-\nProtoPFormer-S\n✓\n-\n84.9\n90.0\n90.9\n-\n-\nHiT-S\n✓\n71.4\n76.1\n80.3\n85.2\n77.4\n88.5\nTable 2. Top1 Accuracy. Our proposed models have a clear per-\nformance loss compared to other ViTs. However, the ViT gains\ncome at the expense of interpretability, whereas the HiT has ac-\nceptable performance while being explicable. ProtoPFormer per-\nformance were extracted from their paper.\nand we used this Rollout Matrix to define the salient regions\nfor the insertion-deletion analysis.\nThe results of this experiment are presented in Fig. 3a\nfor ImageNet and Fig. 3b for CUB-2011. The nAUC scores\nand the profiles of the curves indicate that HiT outperforms\nother ID methods in terms of interpretability.\nTo further enhance our understanding of HiT’s inter-\npretability, we analyze the unnormalized insertion-deletion\nmetrics of traditional post-hoc methods compared to HiT\nin Tab. 1. Specifically, we compare the saliency maps ex-\ntracted from HiT with those generated by GradCAM [53]\nand an adapted Rollout Matrix [1], both computed on HiT.\nOverall, the inherent saliency maps of HiT demonstrate su-\nperior performance compared to the post-hoc methods for\nboth insertion and deletion metrics. This finding suggests\nthat these post-hoc algorithms do not consistently identify\nthe regions used for classification.\nFinally, in Tab. 2, we present the accuracy perfor-\nmance of our proposed architecture compared to both non-\ninterpretable and interpretable alternatives. As expected,\nall interpretable architectures, with the exception of ProtoP-\nFormer, show a decrease in performance relative to the non-\ninterpretable baseline. However, our Hindered Transformer\nmaintains clear advantages in interpretability without a sig-\nnificant drop in performance.\n6\n\nFigure 4.\nQualitative Comparison.\nWe show the image and\nits saliency maps produced by HiT and their homologuous using\nRollout and GradCAM. We noticed that HiT tends to use the ob-\nject’s features in the image for its prediction, independently if its\nprediction is erroneous or not.\n4.4. Qualitative Evaluation\nIn the next part of our study, we show in Fig. 4 a quali-\ntative comparison between the saliency maps generated by\nHiT and those computed with GradCAM and the Rollout\nMatrix when applied to HiT. We make three main conclu-\nsions. First, we found that our method focuses on certain\nparts of objects, regardless of whether the prediction is ac-\ncurate or not. To quantify our claim, we computed the cen-\nter of mass for each explanation and checked whether it\nfell within the object’s bounding box in the CUB test set.\nWe achieved 93.4% accuracy, which strongly supports our\nclaim. Second, our qualitative analysis suggests that mis-\nclassification generally occurs due to similar features be-\ntween image classes. However, since our method generates\nsaliency maps, HiT inherently adopts their weaknesses: it\nshows where the decision was made, but not which features\nwere used. Third, Rollout and GradCAM produce noisy\nmaps. For example, the former shows edges and highlights\nthe general shape of the object, while the latter produces\nmisaligned coarse maps with our base model.\n4.5. Layer-wise Contribution\nAnother advantage of HiT is that we can compute the con-\ntribution of each layer. Similar to computing the saliency\nmaps spatially, we can create the layer-wise output tokens\nand look for their individual contributions. To this end, we\nshow the results in Fig. 5a for four tested dataset. With-\nout any surprise, we can see that most of the discriminative\nfeatures are in the final layers.\nTo ensure that our results are valid, we tested several\nablations of our trained model on the ImageNet dataset,\nshown in Fig.5b.\nFor instance, we tested the accuracy\ndrop by removing or adding a layer of choice (Exclud-\ning/Exclusive Layer in the figure). Similarly, we check the\nperformance loss by removing/adding layers in a cascaded\nmanner, dubbed cumulative removed/inserted layer. The re-\nsults corroborate our previous conclusions: our novel archi-\ntecture is capable of showing the contribution of each indi-\n(a) Layer saliencies per dataset.\n(b) Empirical validation of layer-wise saliency.\nFigure 5. Layer Saliency. HiT has more advantages than just im-\nage saliency. (a) The first experiment shows that HiT computes the\ncontribution per layer. Without any surprise, the final layers have\na greater contribution. (b) We empirically validate our findings in\nImageNet with a variety of experiments. Indeed, the results show\nthat by removing certain layers, we obtain larger expected results\ncongruent with the layer saliency.\nvidual layer without relying on external methods, such as\nLinear Probing [3], to understand the basic functioning of\ntheir inner layers.\n4.6. Sanity Check\nAdebayo et al. [2] highlight that certain maps, such as edge\ndetectors, may appear visually coherent, although they are\nactually unrelated to the model’s decision. In order to ad-\ndress this issue, they proposed sanity check, which involves\nthe iterative randomisation of the parameters of the network\nlayers.\nThis process begins with the deepest layers and\nproceeds step-by-step to the shallower ones. The resulting\nsaliency map produced at each randomization step is then\ncompared with the original map. In our study, we adopt\nthe same methodology as Adebayo et al. [2] and compute\nthe absolute rank correlation between the original saliency\nmap and the one generated after randomization, as shown in\nFig. 6a. Additionally, we include the absolute Pearson cor-\nrelation coefficient in Fig. 6b, and some examples in Fig. 6c.\nForemost, we noticed that the rank correlation has a\nsteeper slope than the Pearson correlation for the linear clas-\nsification layer. This shows that there is a greater similar-\nity with the Pearson correlation. However, the values are\nrelatively low (less than 0.5), indicating large variations.\nSecondly, both metrics reach a plateau for the subsequent\nrandomized models. This low similarity suggests that the\n7\n\n(a) Saliency Rank Correlation\n(b) Saliency Correlation\nImage\nOriginal\nLinear\nBlocks 12-10\nBlocks 12-7\nBlocks 12-4\nRandom\n(c) Example Images\nFigure 6. Sanity checks on HiT. We measure the (a) rank cor-\nrelation and (b) Pearson correlation of HiT saliency maps before\nand after randomising the parameters of a layer. (c) Some visual\nexamples of the saliency maps of randomised models.\nsalient regions highlighted by our model are indeed what the\nmodel sees. Finally, Fig. 6c shows some qualitative exam-\nples produced by the randomization of all blocks, showing\nthat, effectively, the weights’ randomization reflect a large\nvariation in the produced saliency.\n4.7. Ablating HiT\nIn §3.4, we suggested that token pooling layers will in-\ncrease the representation power of HiT. We hypothesize\nthat the lack of inter-token connections would downgrade\ngreatly the performance. Thus, in this section, we empiri-\ncally validate that the inclusion of token pooling layers in-\ncreases the performance of the model. In addition, we im-\nplemented a more powerful pooling strategy used by Iden-\ntityFormer [70]: a 3×3 convolution with a stride of 2. We\nfocus on this architecture because it shares similar charac-\nteristics with HiT, where each token contains its own in-\nformation. Lastly, we theorize that optimizing HiT archi-\ntectures is challenging. To address this, we adopt an ap-\nproach similar to DeiT3 [60], training our model for 300\nand 600 epochs.\nFinally, we observed that using binary\ncross-entropy adversely affects the model’s performance,\ncontrary to its effect on DeiT3.\nWe present the results in Table 3. The findings align\nwith our suspicions: the lack of transferred information be-\ntween patches significantly reduces the model’s accuracy.\nArchitecture\nPooling\nLoss\nEpochs\nVal ImageNet\nHiT-S\nNone\nXE\n300\n65.6\nHiT-S\nNone\nXE\n600\n67.8\nHiT-S\n2x2 AvgPool\nXE\n300\n69.3\nHiT-S\n2x2 AvgPool\nXE\n600\n71.4\nHiT-S\nNone\nBCE\n400\n59.9\nHiT-S\nNone\nBCE\n800\n62.6\nHiT-B\nNone\nXE\n600\n71.5\nHiT-B\n2x2 AvgPool\nXE\n600\n75.0\nHiT-s18\n2x2 AvgPool\nXE\n300\n65.6\nHiT-s18\n2x2 AvgPool\nXE\n600\n69.3\nHiT-s18\n3x3 Conv\nXE\n300\n75.9\nTable 3.\nPerformance loss ablation.\nHiT’s performance loss\nstems from the limited information shared between tokens. Con-\ncurrently, the results suggest that HiT’s optimization problem is\nmore challenging, as extended training periods lead to more sig-\nnificant performance improvements.\nFor instance, by merely adding the convolutional pooling\nlayer of IdentityFormer, we increase the accuracy of a HiT-\ns18 [70] from 65% to 75%. However, these convolutional\nlayers compromise our model’s interpretability by entan-\ngling information between tokens. Despite this gain in per-\nformance, when HiT does not use pooling layers, it’s inter-\npretability increases significantly, generating more precise\nexplanations – please refer to the supplemental material for\nan empirical comparison. In addition, the binary cross en-\ntropy reduces its performance. Finally, unlike DeiT3 train-\ning schemes, our network benefits significantly from in-\ncreasing the number of epochs. We believe that this result\nindicates HiT has not yet converged.\n5. Conclusions\nThis paper proposed a novel interpretable transformer-like\narchitecture: Hindered Transformer. HiT enhances inter-\npretability by decoupling contributions from individual im-\nage patches, enabling the extraction of saliency maps with-\nout external tools. Extensive experiments across multiple\ndatasets demonstrated the improved interpretability bene-\nfits from HiT with a reasonable drop in performance. HiT\npresents a promising approach, offering favorable trade-offs\nfor applications where interpretability is critical.\nEven though our proposed architecture has many advan-\ntages in the terms of interpretability, HiT has limiting fac-\ntors: slow convergence and the potential challenges in cap-\nturing complex dependencies. Regarding the former, we\nbelieve that a more comprehensive hyperparameter search,\nwhich we were unable to conduct due to limited compu-\ntational resources, could significantly reduce the training\ntime. Regarding the latter, HiT has some token interac-\ntions during the self-attention mechanism, yet, it is indeed\nweaker than standard ViTs. This is troublesome for spatial\ntasks and would require substantial modifications, opening\nopportunities for future research.\nAcknowledgements This work was supported by the\nAgence Nationale pour la Recherche (ANR) under award\nnumber ANR-19-CHIA-0017.\n8\n\nReferences\n[1] Samira Abnar and Willem Zuidema. Quantifying attention\nflow in transformers. In Annual Meeting of the Association\nfor Computational Linguistics, 2020. 2, 6, 12\n[2] Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Good-\nfellow, Moritz Hardt, and Been Kim.\nSanity checks for\nsaliency maps. In Advances in Neural Information Process-\ning Systems. Curran Associates, Inc., 2018. 7\n[3] Guillaume Alain and Yoshua Bengio.\nUnderstanding in-\ntermediate layers using linear classifier probes. In 5th In-\nternational Conference on Learning Representations, ICLR\n2017, Toulon, France, April 24-26, 2017, Workshop Track\nProceedings. OpenReview.net, 2017. 1, 7\n[4] Maximilian Augustin, Valentyn Boreiko, Francesco Croce,\nand Matthias Hein. Diffusion visual counterfactual explana-\ntions. In Advances in Neural Information Processing Sys-\ntems, 2022. 1, 2\n[5] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton.\nLayer normalization. CoRR, abs/1607.06450, 2016. 3\n[6] Sebastian Bach, Alexander Binder, Gr´egoire Montavon,\nFrederick Klauschen, Klaus-Robert M¨uller, and Wojciech\nSamek. On pixel-wise explanations for non-linear classifier\ndecisions by layer-wise relevance propagation. PloS one, 10\n(7):e0130140, 2015. 2, 5\n[7] Moritz B¨ohle, Mario Fritz, and Bernt Schiele. B-cos net-\nworks: Alignment is all we need for interpretability.\nIn\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, CVPR 2022, New Orleans, LA, USA, June 18-\n24, 2022, pages 10319–10328. IEEE, 2022. 1, 2\n[8] Andrea Bontempelli, Stefano Teso, Katya Tentori, Fausto\nGiunchiglia, and Andrea Passerini. Concept-level debugging\nof part-prototype networks. In The Eleventh International\nConference on Learning Representations, 2023. 2\n[9] Wieland Brendel and Matthias Bethge.\nApproximating\nCNNs with bag-of-local-features models works surprisingly\nwell on imagenet. In International Conference on Learning\nRepresentations, 2019. 2\n[10] Moritz B¨ohle, Navdeeppal Singh, Mario Fritz, and Bernt\nSchiele. B-cos alignment for inherently interpretable cnns\nand vision transformers. IEEE Transactions on Pattern Anal-\nysis and Machine Intelligence, pages 1–15, 2024. 2, 5\n[11] Zachariah\nCarmichael,\nSuhas\nLohit,\nAnoop\nCherian,\nMichael J Jones, and Walter J Scheirer. Pixel-grounded pro-\ntotypical part networks. In Proceedings of the IEEE/CVF\nWinter Conference on Applications of Computer Vision,\npages 4768–4779, 2024. 2\n[12] Hila Chefer, Shir Gur, and Lior Wolf. Transformer inter-\npretability beyond attention visualization. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 782–791, 2021. 2\n[13] Hila Chefer, Shir Gur, and Lior Wolf.\nGeneric attention-\nmodel explainability for interpreting bi-modal and encoder-\ndecoder transformers. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, pages 397–406,\n2021. 2\n[14] Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cyn-\nthia Rudin, and Jonathan K Su. This looks like that: Deep\nlearning for interpretable image recognition. In Advances in\nNeural Information Processing Systems. Curran Associates,\nInc., 2019. 1, 2\n[15] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong\nZhang, Han Hu, and Yichen Wei. Deformable convolutional\nnetworks. In Proceedings of the IEEE International Confer-\nence on Computer Vision (ICCV), 2017. 2\n[16] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE Conference on Computer Vision and\nPattern Recognition, pages 248–255, 2009. 1, 5, 12, 13\n[17] Jon Donnelly, Alina Jade Barnett, and Chaofan Chen. De-\nformable protopnet: An interpretable image classifier using\ndeformable prototypes.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 10265–10275, 2022. 2\n[18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In International Conference on Learning Representa-\ntions, 2021. 1\n[19] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Ab-\nhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil\nMathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh\nGoyal, Anthony S. Hartshorn, and et al. The llama 3 herd\nof models. ArXiv, abs/2407.21783, 2024. 1\n[20] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim\nEntezari, Jonas M¨uller, Harry Saini, Yam Levi, Dominik\nLorenz, Axel Sauer, Frederic Boesel, et al. Scaling recti-\nfied flow transformers for high-resolution image synthesis.\nIn Forty-first international conference on machine learning,\n2024. 1\n[21] Angela Fan, Edouard Grave, and Armand Joulin. Reducing\ntransformer depth on demand with structured dropout. In In-\nternational Conference on Learning Representations, 2020.\n5\n[22] Yunhao Ge, Yao Xiao, Zhi Xu, Meng Zheng, Srikrishna\nKaranam, Terrence Chen, Laurent Itti, and Ziyan Wu.\nA\npeek into the reasoning of neural networks: Interpreting with\nstructural visual concepts. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 2195–2204, 2021. 2\n[23] Peter Hase, Chaofan Chen, Oscar Li, and Cynthia Rudin. In-\nterpretable image recognition with hierarchical prototypes.\nIn Proceedings of the AAAI Conference on Human Compu-\ntation and Crowdsourcing, pages 32–40, 2019. 2\n[24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In 2016 IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 770–778, 2016. 2\n[25] Jinyung Hong, Keun Hee Park, and Theodore P Pavlic.\nConcept-centric transformers:\nEnhancing model inter-\npretability through object-centric concept learning within a\nshared global workspace. In Proceedings of the IEEE/CVF\nWinter Conference on Applications of Computer Vision,\npages 4880–4891, 2024. 2\n9\n\n[26] Sarthak Jain and Byron C. Wallace. Attention is not expla-\nnation. In North American Chapter of the Association for\nComputational Linguistics, 2019. 1\n[27] Mohammad A. A. K. Jalwana, Naveed Akhtar, Mohammed\nBennamoun, and Ajmal Mian. Cameras: Enhanced resolu-\ntion and sanity preserving class activation mapping for im-\nage saliency. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n16327–16336, 2021. 2\n[28] Guillaume Jeanneret, Lo¨ıc Simon, and Fr´ed´eric Jurie. Diffu-\nsion models for counterfactual explanations. In Proceedings\nof the Asian Conference on Computer Vision (ACCV), 2022.\n1\n[29] Guillaume Jeanneret, Lo¨ıc Simon, and Fr´ed´eric Jurie. Ad-\nversarial counterfactual visual explanations. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), 2023. 2\n[30] Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng\nYao, and Li Fei-Fei. Novel dataset for fine-grained image\ncategorization. In First Workshop on Fine-Grained Visual\nCategorization, IEEE Conference on Computer Vision and\nPattern Recognition, Colorado Springs, CO, 2011. 1, 5, 12\n[31] Sangwon Kim, Jaeyeal Nam, and Byoung Chul Ko. ViT-\nNeT: Interpretable vision transformers with neural tree de-\ncoder. In Proceedings of the 39th International Conference\non Machine Learning, pages 11162–11172. PMLR, 2022. 2\n[32] Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen\nMussmann, Emma Pierson, Been Kim, and Percy Liang.\nConcept bottleneck models. In Proceedings of the 37th In-\nternational Conference on Machine Learning, pages 5338–\n5348. PMLR, 2020. 1, 2\n[33] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.\n3d object representations for fine-grained categorization. In\nProceedings of the IEEE international conference on com-\nputer vision workshops, pages 554–561, 2013. 1, 5, 12\n[34] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama,\nHaotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon,\nand Jianfeng Gao. Llava-med: Training a large language-\nand-vision assistant for biomedicine in one day.\narXiv\npreprint arXiv:2306.00890, 2023. 1\n[35] Yuchao Li, Rongrong Ji, Shaohui Lin, Baochang Zhang,\nChenqian Yan, Yongjian Wu, Feiyue Huang, and Ling Shao.\nInterpretable neural network decoupling. In European Con-\nference on Computer Vision, 2019. 2\n[36] Haoyu Liang, Zhihao Ouyang, Yuyuan Zeng, Hang Su, Zi-\nhao He, Shu-Tao Xia, Jun Zhu, and Bo Zhang. Training in-\nterpretable convolutional neural networks by differentiating\nclass-specific filters. In European Conference on Computer\nVision, pages 622–638. Springer, 2020. 2\n[37] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. In Advances in Neural Information\nProcessing Systems, pages 34892–34916. Curran Associates,\nInc., 2023. 1\n[38] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradi-\nent descent with warm restarts. In International Conference\non Learning Representations, 2017. 5\n[39] Ilya Loshchilov and Frank Hutter.\nDecoupled weight de-\ncay regularization. In International Conference on Learning\nRepresentations, 2019. 5\n[40] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi.\nFine-grained visual classification of aircraft. Technical re-\nport, 2013. 1, 5, 12\n[41] Dmitrii Marin, Jen-Hao Rick Chang, Anurag Ranjan, An-\nish Prabhu, Mohammad Rastegari, and Oncel Tuzel. Token\npooling in vision transformers for image classification. In\nIEEE/CVF Winter Conference on Applications of Computer\nVision, WACV 2023, Waikoloa, HI, USA, January 2-7, 2023,\npages 12–21. IEEE, 2023. 5\n[42] Meike Nauta, Annemarie Jutte, Jesper Provoost, and Christin\nSeifert. This looks like that, because... explaining prototypes\nfor interpretable image recognition. In Joint European Con-\nference on Machine Learning and Knowledge Discovery in\nDatabases, pages 441–456. Springer, 2021. 2\n[43] Meike Nauta, Ron Van Bree, and Christin Seifert. Neural\nprototype trees for interpretable fine-grained image recogni-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 14933–14943,\n2021.\n[44] Meike Nauta, J¨org Schl¨otterer, Maurice van Keulen, and\nChristin Seifert. Pip-net: Patch-based intuitive prototypes\nfor interpretable image classification.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 2744–2753, 2023. 2\n[45] Tuomas Oikarinen, Subhro Das, Lam M. Nguyen, and Tsui-\nWei Weng. Label-free concept bottleneck models. In The\nEleventh International Conference on Learning Representa-\ntions, 2023. 1, 2\n[46] Maxime Oquab, Timoth´ee Darcet, Th´eo Moutakanni, Huy V.\nVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby,\nMido Assran, Nicolas Ballas, Wojciech Galuba, Russell\nHowes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael\nRabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Je-\ngou, Julien Mairal, Patrick Labatut, Armand Joulin, and Pi-\notr Bojanowski. DINOv2: Learning robust visual features\nwithout supervision. Transactions on Machine Learning Re-\nsearch, 2024. Featured Certification. 1\n[47] O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. V. Jawahar.\nCats and dogs. In IEEE Conference on Computer Vision and\nPattern Recognition, 2012. 1, 5, 12\n[48] DIPANJYOTI PAUL, Arpita Chowdhury, Xinqi Xiong,\nFeng-Ju Chang, David Edward Carlyn, Samuel Stevens,\nKaiya L Provost, Anuj Karpatne, Bryan Carstens, Daniel\nRubenstein, Charles Stewart, Tanya Berger-Wolf, Yu Su, and\nWei-Lun Chao. A simple interpretable transformer for fine-\ngrained image classification and analysis. In The Twelfth In-\nternational Conference on Learning Representations, 2024.\n2\n[49] Vitali Petsiuk, Abir Das, and Kate Saenko. Rise: Random-\nized input sampling for explanation of black-box models.\nIn Proceedings of the British Machine Vision Conference\n(BMVC), 2018. 2, 5\n[50] Maithra Raghu, Thomas Unterthiner, Simon Kornblith,\nChiyuan Zhang, and Alexey Dosovitskiy. Do vision trans-\n10\n\nformers see like convolutional neural networks? In Advances\nin Neural Information Processing Systems, 2021. 2\n[51] Mattia Rigotti, Christoph Miksovic, Ioana Giurgiu, Thomas\nGschwind, and Paolo Scotton. Attention-based interpretabil-\nity with concept transformers. In International Conference\non Learning Representations, 2022. 2\n[52] Dawid Rymarczyk, Łukasz Struski, Michał G´orszczak, Ko-\nryna Lewandowska, Jacek Tabor, and Bartosz Zieli´nski. In-\nterpretable image classification with differentiable proto-\ntypes assignment. Proceedings of the European Conference\non Computer Vision (ECCV), 2022. 2\n[53] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek\nDas, Ramakrishna Vedantam, Devi Parikh, and Dhruv Ba-\ntra.\nGrad-cam: Visual explanations from deep networks\nvia gradient-based localization. In 2017 IEEE International\nConference on Computer Vision (ICCV), pages 618–626,\n2017. 2, 6, 12\n[54] Sofia Serrano and Noah A. Smith. Is attention interpretable?\nIn Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 2931–2951, Florence,\nItaly, 2019. Association for Computational Linguistics. 1\n[55] Wen Shen, Zhihua Wei, Shikun Huang, Binbin Zhang, Jiaqi\nFan, Ping Zhao, and Quanshi Zhang. Interpretable compo-\nsitional convolutional neural networks.\nIn Proceedings of\nthe International Joint Conference on Artificial Intelligence,\n2021. 2\n[56] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. In In-\nternational Conference on Learning Representations, 2015.\n2\n[57] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman.\nDeep inside convolutional networks:\nVisualising image\nclassification models and saliency maps.\narXiv preprint\narXiv:1312.6034, 2013. 1\n[58] Sarah Tan, Giles Hooker, Paul Koch, Albert Gordo, and Rich\nCaruana.\nConsiderations when learning additive explana-\ntions for black-box models.\nMach. Learn., 112(9):3333–\n3359, 2023. 2\n[59] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herve Jegou. Training\ndata-efficient image transformers; distillation through atten-\ntion. In Proceedings of the 38th International Conference on\nMachine Learning, pages 10347–10357. PMLR, 2021. 5\n[60] Hugo Touvron, Matthieu Cord, and Herv’e J’egou. Deit iii:\nRevenge of the vit. In European Conference on Computer\nVision, 2022. 5, 8\n[61] Yuki Ukai, Tsubasa Hirakawa, Takayoshi Yamashita, and\nHironobu Fujiyoshi. This looks like it rather than that: Pro-\ntoKNN for similarity-based classifiers. In The Eleventh In-\nternational Conference on Learning Representations, 2023.\n2\n[62] Robert van der Klis, Stephan Alaniz, Massimiliano Mancini,\nCassio F Dantas, Dino Ienco, Zeynep Akata, and Diego Mar-\ncos. Pdisconet: Semantically consistent part discovery for\nfine-grained recognition. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 1866–\n1876, 2023. 2\n[63] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 2, 3\n[64] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Be-\nlongie. The caltech-ucsd birds-200-2011 dataset. Technical\nReport CNS-TR-2011-001, California Institute of Technol-\nogy, 2011. 1, 5, 12\n[65] Chong Wang, Yuyuan Liu, Yuanhong Chen, Fengbei Liu,\nYu Tian, Davis J McCarthy, Helen Frazer, and Gustavo\nCarneiro.\nLearning support and trivial prototypes for in-\nterpretable image classification.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 2062–2072, 2023. 1, 2\n[66] Jiaqi Wang, Huafeng Liu, Xinyue Wang, and Liping Jing.\nInterpretable image recognition by constructing transparent\nembedding space. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision (ICCV), pages 895–\n904, 2021. 2\n[67] Xue Wang, Zhibo Wang, Haiqin Weng, Hengchang Guo,\nZhifei Zhang, Lu Jin, Tao Wei, and Kui Ren. Counterfactual-\nbased saliency map: Towards visual contrastive explanations\nfor neural networks. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision (ICCV), pages\n2042–2051, 2023. 2\n[68] Mengqi Xue, Qihan Huang, Haofei Zhang, Jingwen Hu,\nJie Song, Mingli Song, and Canghong Jin. Protopformer:\nConcentrating on prototypical parts in vision transformers\nfor interpretable image recognition. In Proceedings of the\nThirty-Third International Joint Conference on Artificial In-\ntelligence, IJCAI-24, pages 1516–1524. International Joint\nConferences on Artificial Intelligence Organization, 2024.\nMain Track. 2, 6\n[69] Hongxu Yin, Arash Vahdat, Jose Alvarez, Arun Mallya, Jan\nKautz, and Pavlo Molchanov. A-ViT: Adaptive tokens for ef-\nficient vision transformer. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\n2022. 2, 5\n[70] Weihao Yu, Chenyang Si, Pan Zhou, Mi Luo, Yichen Zhou,\nJiashi Feng, Shuicheng Yan, and Xinchao Wang. Metaformer\nbaselines for vision. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 46(2):896–912, 2024. 8\n[71] Mehdi Zemni, Micka¨el Chen, ´Eloi Zablocki, Hedi Ben-\nYounes, Patrick P´erez, and Matthieu Cord. Octet: Object-\naware counterfactual explanations. In IEEE Conference on\nComputer Vision and Pattern Recognition, CVPR, 2023. 2\n[72] Quanshi Zhang, Ying Nian Wu, and Song-Chun Zhu. Inter-\npretable convolutional neural networks. In Proceedings of\nthe IEEE conference on computer vision and pattern recog-\nnition, pages 8827–8836, 2018. 2\n[73] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva,\nand Antonio Torralba. Learning deep features for discrimina-\ntive localization. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 2921–2929,\n2016. 4, 5\n11\n\nSupplementary Material\nA. Datasets\nAs for the dataset, we evaluated HiT on six diverse image classification datasets: i) ImageNet [16]: A large-scale dataset with\n1.2 million images and 1,000 classes, often used as a benchmark. ii) CUB-2011 [64]: A challenging dataset containing 200\nbird classes with only 30 training samples per class on average. iii) Stanford Dogs [30]: A dataset with 120 dog classes and\n10,000 training and test images. iv) Stanford Cars [33]: A dataset featuring 196 car classes with 8,100 training and validation\nexamples. v) FGVC-Aircraft [40]: A dataset of 100 airplance classes with 10,000 images. vi) Oxford-IIIT Pets [47]: a 37\ncategory dataset with roughly 200 images per class.\nB. Insertion-Deletion Curves\nIn Fig. 7, we present the curves from the post-hoc comparison experiment detailed in § 4.3 of the main document. HiT\nconsistently outperforms both GradCAM [53] and Rollout Matrix [1] across all datasets. Interestigly, GradCAM achieves\nperformance comparable to our method on all datasets except ImageNet [16]. We hypothesize that this correlation stems\nfrom GradCAM being computed on the final layer tokens, which our analysis shows are the most important (Fig. 5 in the\nmanuscript), except for ImageNet.\nImageNet\nStanford Dogs\nCUB 2011\nStanford Cars\nBlur\nZeros\nFigure 7. Comparing HiT and alternative post-hoc methods. This experiments reflects the interpretable advantages with respect to\ntraditional post-hoc methods.\nC. Evaluating HiT without Pooling Layers\nWe conducted experiments similar to those in the main manuscript to analyze the positive and negative impact of removing\npooling layers. As demonstrated in § 4.7 of the paper, pooling layers are essential for improving top-1 accuracy performance.\nHowever, their inclusion increases the size of the explanations. First, we explore this phenomenon quantitatively in sections\nC.1 and § C.2. Later, we will explore the qualitative differences in § C.3.\nC.1. Interpretability Trade-off\nFirst, we explore the interpretability gains of HiT without pooling layers. We compare both HiT versions using the normalized\ninsertion-deletion curves on all tested datasets, illustrated in Fig. 8. From a quantitative point of view, HiT without any\npooling layer is even more interpretable than our proposed architecture. Interestingly, both curves behave similarly, showing\na decrease in the insertion probability curve and an increase in the deletion probability curve during their final steps. This is\ndue to the insertion (or deletion) of tokens that adversely affect the model’s prediction.\nWe also tested GradCAM [53] and the Rollout [1] Matrix directly on our pooling-free HiT architecture, with results shown\nin Table 4. The interpretability gap between post-hoc methods and HiT saliency maps is more pronounced compared to our\nstandard HiT architecture.\nFinally, in Tab. 5, we show the performance on the tested datasets. Without any surprise, the loss in performance is major,\nmaking it a less appealing option in contrast to our original architecture when computation power is needed.\n12\n\nImageNet\nStanford Dogs\nCUB 2011\nStanford Cars\nFigure 8. Caption\nMethod\nImageNet\nCUB 2011\nStanford Cars\nStanford Dogs\nIns-Z (↑)\nDel-Z (↓)\nIns-Z (↑)\nDel-Z (↓)\nIns-Z (↑)\nDel-Z (↓)\nIns-Z (↑)\nDel-Z (↓)\nHiT\n0.65\n0.08\n0.56\n0.04\n0.72\n0.05\n0.64\n0.07\nHiT + Rollout\n0.39\n0.21\n0.43\n0.09\n0.49\n0.12\n0.47\n0.19\nHiT + GradCAM\n0.36\n0.15\n0.40\n0.09\n0.34\n0.11\n0.40\n0.15\nIns-B (↑)\nDel-B (↓)\nIns-B (↑)\nDel-B (↓)\nIns-B (↑)\nDel-B (↓)\nIns-B (↑)\nDel-B (↓)\nHiT\n0.67\n0.16\n0.59\n0.11\n0.65\n0.15\n0.62\n0.18\nHiT + Rollout\n0.47\n0.31\n0.50\n0.22\n0.50\n0.29\n0.50\n0.32\nHiT + GradCAM\n0.48\n0.29\n0.52\n0.21\n0.51\n0.29\n0.52\n0.31\nTable 4. HiT and Explainability methods: We quantitatively compare HiT maps and those created by GradCAM and the modified rollout\nmatrix (mean attention). The assessment shows that HiT maps are in fact more faithful to those generated by GradCAM and the rollout\nmatrix. Higher insertion is better, while lower deletion is better. Ins and Del refers to the Insertion and Deletion metrics, respectively. Z is\nthe zero-corrupted image, while B is the blurred corruption strategy.\nModel\nPooling?\nImageNet\nCUB\nDogs\nCars\nHiT-S\nχ\n67.3\n76.1\n77.1\n83.9\n✓\n71.4\n76.1\n80.3\n85.2\nHiT-B\nχ\n71.5\n76.3\n80.2\n84.7\n✓\n75.0\n79.0\n86.8\n86.2\nTable 5. Top1 Accuracy. Including pooling layers provides a clear advantage in terms of raw performance. However, this performance\ngain comes at the cost of reduced interpretability.\nC.2. Layer-wise Contributions\nNext, we investigate the ability of the pooling-free HiT to analyze layer-wise contributions. Fig. 9a illustrates the layer\ncontribution per dataset, while Fig. 9b plots the ablation results on ImageNet [16]. Similar to HiT with layer pooling, the\nmost significant contributions come from the final layers. However, unlike the HiT version with pooling layers, all trained\nmodels appear to weight their final predictions equally across the last three layers.\nC.3. Qualitative Comparison\nFinally, we qualitatively show the difference between the pool-free HiT and our original version saliency maps in Fig. 10 in\nImageNet [16]. As expected, removing the pooling layers produces finer saliency maps.\n13\n\n(a) Layer Saliency.\n(b) Empirical validation of layer-wise saliency.\nFigure 9. As in the main manuscript, we assess our pooling-free HiT layer contribution. Effectively, HiT can discover the contributions for\neach layer.\nFigure 10. Qualitative Examples: we visually show the difference between HiT saliency maps with and without pooling layers on some\ncorrectly classified images from the ImageNet dataset.\n14\n",
  "metadata": {
    "source_path": "papers/arxiv/Disentangling_Visual_Transformers_Patch-level_Interpretability_for\n__Image_Classification_26de714f33fc7387.pdf",
    "content_hash": "26de714f33fc7387db2893f583a6d8996cb9c1f738ac438be7fbdf05a599a3aa",
    "arxiv_id": null,
    "title": "Disentangling_Visual_Transformers_Patch-level_Interpretability_for\n__Image_Classification_26de714f33fc7387",
    "author": "",
    "creation_date": "D:20250225025130Z",
    "published": "2025-02-25T02:51:30",
    "pages": 14,
    "size": 15636423,
    "file_mtime": 1740470171.934239
  }
}