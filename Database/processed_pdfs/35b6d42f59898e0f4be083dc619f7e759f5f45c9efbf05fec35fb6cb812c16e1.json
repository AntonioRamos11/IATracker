{
  "text": "CORAL: Learning Consistent Representations across Multi-step Training\nwith Lighter Speculative Drafter\nYepeng Weng, Dianwen Mei, Huishi Qiu\nXujie Chen, Li Liu, Jiang Tian, Zhongchao Shi\nLenovo Research\nAbstract\nSpeculative decoding is a powerful technique\nthat accelerates Large Language Model (LLM)\ninference by leveraging a lightweight specula-\ntive draft model. However, existing designs\nsuffers in performance due to misalignment\nbetween training and inference. Recent meth-\nods have tried to solve this issue by adopting\na multi-step training strategy, but the complex\ninputs of different training steps make it harder\nfor the draft model to converge. To address\nthis, we propose CORAL, a novel framework\nthat improves both accuracy and efficiency in\nspeculative drafting. CORAL introduces Cross-\nStep Representation Alignment, a method that\nenhances consistency across multiple training\nsteps, significantly improving speculative draft-\ning performance.\nAdditionally, we identify\nthe LM head as a major bottleneck in the in-\nference speed of the draft model. We intro-\nduce a weight-grouping mechanism that se-\nlectively activates a subset of LM head pa-\nrameters during inference, substantially reduc-\ning the latency of the draft model. We evalu-\nate CORAL on three LLM families and three\nbenchmark datasets, achieving speedup ratios\nof 2.50√ó-4.07√ó, outperforming state-of-the-\nart methods such as EAGLE-2 and HASS. Our\nresults demonstrate that CORAL effectively\nmitigates training-inference misalignment and\ndelivers significant speedup for modern LLMs\nwith large vocabularies.\n1\nIntroduction\nLarge Language Models (LLMs), such as GPT\n(OpenAI, 2023) and Llama series (Touvron et al.,\n2023a,b; Grattafiori et al., 2024), have demon-\nstrated exceptional capabilities in various natural\nlanguage processing tasks. However, achieving\nstronger model performance often depends on in-\ncreasing the number of model parameters (Kaplan\net al., 2020; Hoffmann et al., 2022), which leads to\nhigher costs in both training and inference. Thus,\nachieving strong performance while maintaining\n2.04x\n1.83x\n2.4x\n2.33x\n2.62x\n2.49x\n2.68x\n2.55x\n2.87x\n2.77x\n1\n1.5\n2\n2.5\n3\nLlama3-8B-Instruct\nQwen2.5-7B-Instruct\nSpeedup\nEAGLE\nEAGLE-2\nHASS Alignment\nCORAL w/o router\nCORAL w/ router\nFigure 1:\nSpeedup ratios of different methods on\nLlama3-8B and Qwen2.5-7B at temperature=0, aver-\naging on MT-bench, HumanEval, and GSM8K datasets.\nWe present full results in Table 2 and this chart is only a\nsubset of all comparisons.\nquick response is a crucial part in LLM implemen-\ntations. Under common hardware conditions, trans-\nformer decoder-based LLMs are memory-bound\n(Dao et al., 2022), which means that the generation\nspeed is mainly determined by memory access and\nbandwidth, rather than arithmetic computations.\nThis allows for the acceleration of generation using\nspeculative decoding (Chen et al., 2023; Leviathan\net al., 2023). The general idea of speculative de-\ncoding is to utilize one or multiple lightweight\ndraft models to predict the output of target LLM\nfor several upcoming timesteps, and then verify\nthe drafted predictions in parallel using the target\nmodel. The memory-bound characteristic guaran-\ntees that the parallel verification of multiple tokens\ndoes not incur a significant increase in latency com-\npared to generating a single token.\nRecently, autoregressive draft models, such\nas EAGLE (Li et al., 2024b), have received\nwidespread attention for their excellent speedup\nperformance. For training, EAGLE uses not only\nthe output tokens but also the last hidden states\nfrom target LLM as input to the draft model, while\nduring the drafting phase, the draft model uses\nits own hidden states from the previous timestep,\n1\narXiv:2502.16880v1  [cs.CL]  24 Feb 2025\n\nModel\nHidden\nInter. size\nVocab\nWd / Wt\nLd / Lt\nLlama2-7B\n4096\n11008\n32000\n350M/6301M(5.6%)\n1.36ms/23.65ms(5.8%)\nLlama3-8B\n4096\n14336\n128256\n741M/7157M(10.4%)\n2.58ms/26.06ms(9.9%)\nQwen2.5-7B\n3584\n18944\n152064\n767M/6743M(11.4%)\n2.69ms/24.58ms(10.9%)\nTable 1: Parameters and latencies of Llama3-8B, Llama2-7B, and Qwen2.5-7B draft and target models. Wd, Wt and\nLd, Lt denote the parameter counts and latency of draft and target model. In the table, M represents 1024√ó1024.\nParameters of the embedding layer are not calculated because they do not participate in general matrix multiplication\n(GEMM). Latencies are tested with one token on a single NVIDIA A6000 GPU.\n1.51ms\n(501M)\n1.07ms\n(240M)\nLM head\nother\n0.42ms\n(125M)\n0.94ms\n(225M)\n1.60ms\n(520M)\n1.09ms\n(247M)\nLlama3-8B-draft\nLlama2-7B-draft\nQwen2.5-7B-draft\nFigure 2: Parameters and latencies of Llama3-8B,\nLlama2-7B, Qwen2.5-7B draft model. For a model\nwith large vocabulary, the LM head takes the majority\nof the drafting latency.\nwhich may contain biases. This misalignment leads\nto a decrease in the prediction accuracy of the draft\nmodel. HASS (Zhang et al., 2024) proposes a\nmulti-step training strategy, where the hidden states\noutput by the draft model are fed back into itself\nmultiple times during training, allowing the draft\nmodel to learn the feature distribution of the in-\nference phase. In Section 2 we will provide more\ndetailed discussions on them.\nAlthough HASS exhibits impressive perfor-\nmance, there are still some limitations to multi-step\ntraining. Specifically, their design causes the input\nfeatures at differrent training steps to vary, which\nmight be challenging for a lightweight draft model\nto adapt to. The discrepancy of each training step\nmay also introduce potential gradient conflicts. Fur-\nthermore, modern LLMs are increasingly moving\ntowards large vocabularies to obtain better perfor-\nmance (Tao et al., 2024). For example, previous\nmodel such as Llama2 has a small vocabulary size\nof only 32000 (Touvron et al., 2023b), while the vo-\ncabulary size of Llama3 (Grattafiori et al., 2024) is\n128256, and that of Qwen2.5 (Yang et al., 2024) is\n152064. Such large vocabularies lead to an increase\nin the parameter size of the Language Model head\n(LM head), resulting in increased overhead of draft-\ning, which is presented in Table 1. As demonstrated\nin Figure 2, the heavy LM head could potentially\ndominate the latency of draft model. However, few\nstudies have focused on this aspect.\nIn this paper, we introduce CORAL (learn-\ning COnsistent Representations Across multi-step\ntraining with Lighter speculative drafter), a specula-\ntive decoding method that improves the alignment\nbetween the draft model and the target model while\nmaintaining high drafting speed. We first propose\nCross-Step Representation Alignment (CSRA),\nwhich leverages the idea of contrastive learning\nto enforce consistency among the output features\nof each training step. The constraint on features\nmakes them more stable, and thus improves the\ntraining efficiency and the performance of the draft\nmodel. Furthermore, by grouping the LM heads,\nwe significantly reduce the activated parameters of\nthe draft model with large vocabulary size, thereby\ndecreasing the wall time of speculative decoding.\nWe evaluate acceleration capability of CORAL\non multi-turn conversation, code generation, and\nmathematical reasoning tasks using the MT-Bench,\nHumanEval and GSM8K datasets, respectively.\nThe results show that our method achieves 2.50√ó-\n4.07√ó speedup over vanilla decoding at a tempera-\nture of 0, surpassing state-of-the-art methods such\nas EAGLE-2 and HASS.\nOur key contributions can be summarized as\nfollows.\n1. We propose Cross-Step Representation Align-\nment, a technique that enables the draft model\nto learn consistent representations across mul-\ntiple timesteps.\n2. We find that the vocabulary size can signifi-\ncantly influence the latency of the draft model,\nand propose a novel method which selectively\nactivates a subset of LM head parameters dur-\ning inference using a router.\n3. CORAL achieves speedup ratios of 2.50√ó-\n4.07√ó on various LLMs and datasets, outper-\nforming existing speculative decoding meth-\nods such as EAGLE-2 and HASS.\n2\n\nCross-Step Consistency\nùëù2\nùë°\nDistribution \nAlignment\n ùëì1\nùë°\n‚Ä¶\nùëù1\nùë°\nSingle transformer layer\nLM head\n  ùëí1\nùëì0\nùë°\nùëì1\nùë°\n ùëì2\nùë°\n ùëì1\nùëë\n ùëì2\nùëë\n ùëì3\nùëë\n ùëù1\nùëë\nùëù2\nùëë\n ùëù3\nùëë\n‚Ä¶\n‚Ä¶\n‚Ä¶\nSingle transformer layer\nLM head\nùëì0\nùë°\n  ùëí2\n  ùëì1\nùëì1\n  ùëì2\n  ùëù1\n  ùëù2\n  ùëí3\n  ùëì2\n  ùëì3\n  ùëù3\nùëí2\nùëí3\nùëí1\n‚Ä¶\n‚Ä¶\n‚Ä¶\nSingle transformer layer\nLM head\n ùëì0\nùë°\nùëí2\nùëì1\nùëë\n ùëì1\nùëë\nùëì2\nùëë‚Ä≤\n ùëù1\nùëë\n ùëù2\nùëë‚Ä≤\nùëí3\n ùëì2\nùëë\n ùëì3\nùëë‚Ä≤\n ùëù3\nùëë‚Ä≤\n  ùëí1\n‚Ä¶\n‚Ä¶\n‚Ä¶\nSingle transformer layer\nLM head\nùëí1\n ùëì0\nùë°\nùëì1\nùë°\n ùëì2\nùë°\n ùëì1\nùëë\nùëì2\nùëë\n ùëì3\nùëë\n ùëù1\nùëë\n ùëù2\nùëë\nùëù3\nùëë\n‚Ä¶\n‚Ä¶\n‚Ä¶\nùëí2\nùëí3\nSingle transformer layer\nLM head\n ùëì0\nùë°\nùëí2\nùëì1\nùëë\n ùëì1\nùëë\nùëì2\nùëë‚Ä≤\n ùëù1\nùëë\n ùëù2\nùëë‚Ä≤\nùëí3\n ùëì2\nùëë‚Ä≤\n ùëì3\nùëë‚Ä≤‚Ä≤\n ùëù3\nùëë‚Ä≤‚Ä≤\n  ùëí1\n‚Ä¶\n‚Ä¶\n‚Ä¶\nTraining-inference gap\nEAGLE inference\nMulti-step training with CSRA\nStep1\nStep3\nStep2\nEAGLE training\nùëù3\nùë°\n ùëì2\nùë°\nùëì3\nùë°\nùëì1\nùë°\n ùëì2\nùë°\n ùëì3\nùë°\n‚Ä¶\n‚Ä¶\nTraining target\nRepresentation\nAlignment\nRepresentation Alignment: Smooth L1\nDistribution Alignment: Cross-entropy\nLoss Function\nCross-step Consistency: InfoNCE\nFigure 3: Demonstration of EAGLE training / inference and multi-step training with CSRA. f denotes feature\nand e denotes embedding. Superscripts indicate the source of the variable, with t and d denoting the target model\nand draft model. Subscripts index the position of a feature or embedding. For example, f t\n3 means the feature in\nposition 3 and comes from the target model. For multi-step training, we use apostrophes to distinguish the outputs\nof different training steps. Specifically, we denote the output feature of step 1 as f d, and for step 2 and 3 we use f d‚Ä≤\nand f d‚Ä≤‚Ä≤, respectively. Compared to HASS, CSRA introduces additional constraints on feature consistency. The\ntraining target is applied at each step, and we only illustrate it once for the sake of clarity.\n2\nPreliminaries\nIn this section, we provide some background in-\nformation related to speculative decoding and re-\nview some existing methods, including EAGLE\nand HASS.\n2.1\nSpeculative Decoding\nSpeculative decoding (Chen et al., 2023; Leviathan\net al., 2023) aims to accelerate the generation speed\nof autoregressive LLMs. Vanilla speculative decod-\ning employs a lightweight model (draft model) to\ngenerate a chain of candidate tokens for the next Œ≥\ntimesteps, which are then verified in parallel by the\noriginal LLM (target model) and decide whether\nto accept them or not. Since the latency of LLM\ngeneration mainly lies in the memory access, par-\nallel verification of multiple tokens does not sig-\nnificantly impact the latency of the target LLM,\nalthough the computational cost is multiplied.\nThe acceleration capability of speculative decod-\ning is typically evaluated using two metrics: av-\nerage acceptance length œÑ and the actual Speedup\nRatio (SR). A drafting-verification cycle consists\nof one token provided by the target model and mul-\ntiple candidates generated by the draft model over\nŒ≥ time steps. The average acceptance length œÑ is\ndefined as the number of new tokens generated in\na single drafting-verification cycle.\nIdeally, we can estimate the speedup ratio using\nœÑ and the latencies of draft and target model:\nSR ‚âàœÑ √ó\nL‚Ä≤\nt\nŒ≥ √ó Ld + Lt\n,\n(1)\nwhere Lt and Ld denote the latency of the target\nmodel and draft model, respectively. L‚Ä≤\nt denotes\nthe latency for evaluating multiple tokens one time,\nit could be slightly different from Lt depending on\nthe hardware. Some additional overheads might\nalso contribute to latency, such as comparing the\nprobabilities of tokens from draft and target models\nto determine acceptance. However, since these\noverheads typically do not dominate the overall\nlatency, it is a good choice to ignore them when\nestimating the speedup ratio.\nFrom Equation (1) we can see the speedup ratio\nis primarily influenced by two factors: the align-\nment between the draft model and the target model,\nwhich mainly influences œÑ, and the ratio of their\nlatencies. Specifically, the lower the latency of the\ndraft model and the better alignment between the\ntwo models, the higher the speedup ratio will be\nachieved by speculative decoding.\n2.2\nEAGLE\nEAGLE (Li et al., 2024b) is a lightweight autore-\ngressive draft model that leverages a single trans-\nformer layer identical to that of the target model.\nThe LM head of draft model is reused directly from\nthe target model, with its parameters frozen. EA-\nGLE discovers that utilizing the feature (i.e., the\nlast hidden states) of the target model can effec-\ntively enhance the alignment between the draft and\ntarget model. For training, the input of the draft\nmodel at position s is the current token ts and the\nfeature of the target model at position s ‚àí1. The\ntoken ts will first be transformed into embedding\nes, and then concatenated with the feature. A linear\nlayer is adopted to reduce the dimensions before\n3\n\nthe single transformer layer.\nThe training target of EAGLE is to align the fea-\nture (regression) and probability distribution (clas-\nsification) of the draft and target model. EAGLE\nuses smooth L1 as the regression loss and cross-\nentropy as the classification loss.\nEAGLE selects multiple candidates at each\ntimestep during drafting, resulting in a tree-shaped\nstructure rather than a chain. Tree decoding of-\nfers more possible trajectories than chain decoding,\nleading to a higher acceptance length. EAGLE-2\n(Li et al., 2024a) improves the fixed tree structure\nto a dynamic one and achieves better performance.\n2.3\nHASS\nHASS (Zhang et al., 2024) addresses the inconsis-\ntency between the training and inference phases of\nEAGLE by introducing a multi-step training strat-\negy. As demonstrated in Figure 3, EAGLE uses the\nfeature of the target model for training, whereas\nin inference, the draft model uses its own feature.\nHASS solves this problem by feeding the output\nfeature of draft model back into itself for multi-\nple times. To expose the draft model to inference-\ntime conditions during training, attention masks\nfrom different training steps require careful adjust-\nment. HASS also incorporates other improvements\non EAGLE, but they are orthogonal to multi-step\nalignment. In this paper, we focus mainly on HASS\nalignment, and all references to HASS in the re-\nmainder of this paper denote HASS alignment un-\nless otherwise specified.\nWhile HASS improves the accuracy of draft\nmodels in autoregressive generation, we argue that\nthere are still unresolved issues due to the discrep-\nancies between representations from multiple train-\ning steps (i.e., fd, fd‚Ä≤ and fd‚Ä≤‚Ä≤ in Figure 3). It is\nharder for the draft model to adapt to more complex\ninputs and the conflicting gradients from multiple\nsteps may hinder convergence speed.\n3\nMethod\nIn this section, we first introduce Cross-Step\nRepresentation Alignment, a method designed to\nstrengthen the alignment between the draft model\nand the target model. We then analyze the speedup\nratio and identify the LM head of the draft model\nas a bottleneck. To address this issue, we propose\nthe LM head router, a novel solution that aims to\nreduce the latency of the draft model.\n(a) EAGLE Training\n(b) HASS Training\n(c) CSRA\nFigure 4: Comparison of EAGLE training, HASS train-\ning and CSRA. Here ‚Éùdenotes training target, ‚ñ≥de-\nnotes output features from different steps. Triangles\nfilled with darker colors represent the first step‚Äôs output.\nDifferent colors represent outputs or targets of different\npositions. Optimization direction is marked as ‚Üí, and\nthe dashed ‚Üîmeans repulsion.\n3.1\nCross-Step Representation Alignment\nCross-Step Representation Alignment (CSRA)\nleverages the idea of contrastive learning (Chopra\net al., 2005; Schroff et al., 2015). Specifically, in\nmulti-step training, we treat the output features at\nthe same position in a sentence as positive views\nof the same sample, while all other features are\nconsidered negative samples.\nAssuming current training step is t, the output\nfeatures of current step are Ft ‚ààRB√óS√óD, where\nB, S, and D represent the batch size, sequence\nlength, and hidden dimension, respectively. Natu-\nrally, we regard them as B √ó S samples, and each\nsample has t positive views, while all other features\nare considered negative samples.\nFor each output feature f in current training step,\nour objective is to minimize its distance to other\npositive views while maximizing the distance to\nnegative samples. To achieve this, we normalize the\nfeatures and compute the InfoNCE loss (van den\nOord et al., 2018) as the objective function, which\nencourages the feature to be closer to its positive\nviews and away from negative samples:\nLCSRA = ‚àílog\nexp(sim(q, f+)/œÑ)\nP\nf‚ààF exp(sim(q, f)/œÑ),\n(2)\n4\n\nwhere q and f+ denotes the query feature and posi-\ntive views, and F is the set of all features along with\nthe targets. The similarity function sim(¬∑, ¬∑) is de-\nfined as cosine similarity. Here œÑ is the temperature\nhyperparameter. Figure 4 shows the differences be-\ntween EAGLE / HASS training and CSRA.\nThe training loss can be defined as:\nL = wregLreg + wclsLcls + wCSRALCSRA, (3)\nwhere Lreg and Lcls represent the regression loss\nand classification loss, respectively. Since LCSRA\nprimarily affects representation learning, we main-\ntain wcls consistent with EAGLE and adjust an-\nother two weights according to different target mod-\nels. For detailed parameter settings, please refer to\nAppendix A.\n3.2\nEstimation of Speedup Ratio\nAs discussed in Section 2.1, the generation speed\nis primarily constrained by memory bandwidth.\nTherefore, the theoretical latency Ltheo. in genera-\ntion phase is proportional to the LLM‚Äôs parameter\ncount WLLM:\nLtheo. ‚àùWLLM.\n(4)\nHowever, this estimation is not always accurate\ndue to the following factors: 1) Not all operators\nand computing graphs are fully optimized. 2) The\nlatency of some element-wise operators (e.g., ac-\ntivation, norm) is not reflected in the parameter\ncount. This issue is particularly noticeable for Py-\nTorch, because it is not a framework optimized for\ninference.\nLuckily, the draft model and target one share the\nsame transformer structure, and the extra latency\ncaused by the aforementioned factors is relatively\nconsistent in both models. This allows us to esti-\nmate the wall time and speedup ratio of speculative\ndecoding based on the parameters of draft model\nand target model:\nLd\nLt\n‚âàWd\nWt\n,\n(5)\nSR ‚âàœÑ √ó\nWt\nŒ≥ √ó Wd + Wt\n,\n(6)\nwhere Wd, Wt and Ld, Lt denote the parameter\ncounts and latency of draft and target model, respec-\ntively. Note that the embedding layer does not par-\nticipate in general matrix multiplication (GEMM),\ntherefore its parameters should not be included in\nlatency estimation. Table 1 presents the latencies\nand parameters of different LLMs, along with their\ncorresponding draft models. The results suggest\nthat estimating the latency ratio between the draft\nand target models based on their parameter counts\nis relatively accurate. Notably, for Llama3-8B and\nQwen2.5-7B, the latency of draft model is approxi-\nmately 10% of that of target model. As the depth\nof drafting increases, the latency of draft model is\nexpected to contribute significantly to the overall\nwall time.\nFurthermore, it is also possible to estimate the\nlatency of each component of the draft model based\non their parameter count. As shown in Figure 2, in\ncases with large vocabularies, the latency of LM\nhead accounts for a significant proportion of the\ntotal latency, which provides us with a valuable in-\nsight: If we can reduce the activated weights of the\nLM head, the overall speedup will be substantially\nimproved.\n3.3\nLM Head Router\nAs mentioned in Section 3.2, for draft models with\nlarge vocabularies, LM head constitutes the major\npart of drafting latency. We propose the LM head\nrouter, aiming to group the LM head and then acti-\nvate only a subset of LM head parameters during\ndrafting, as demonstrated in Figure 5.\nAssuming a LLM with a vocabulary size V , we\ndivide the LM head equally into N groups, each\nwith a vocabulary size of v = V/N. We utilize a\nrouter to select which group to activate. The output\nof router can be outlined as follows:\nprouter = Softmax(W2(act(W1h) + h)),\nW2 ‚ààRN√ód, W1 ‚ààRd√ód,\n(7)\nwhere h denotes the hidden states of draft model,\nd is the hidden size.\nLet p(x), q(x) denote the predicted and target\ndistribution, and pgroup(xn) denote the probability\ndistribution within a specific group n. After select-\ning a particular group, the softmax probability is\ncalculated by logits in this group, independent of\nthe logits in other groups.\nThen the final distribution with router should be\np(x) = prouter(n) ¬∑ pgroup(xn).\n(8)\nFor each group, P pgroup(xn) = 1, and for router\nwe have P prouter(n) = 1. Therefore, the final\np(x) is normalized.\n5\n\nhidden states\nprobabilities\nrouter\nactivated LM head params\nnon-activated params\n(a) Vanilla LM head\n(b) LM head with router\n‚Ä¶\nFigure 5: Demonstration of LM head router in draft\nmodel. With the router, we only output probabilities of\none or multiple subsets of vocabulary.\nThe training target of LM head router is the\nsum of target probabilities in each group, namely\nqrouter(n) = P qgroup(xn). We use cross-entropy\nas the loss function:\nLrouter = ‚àí\nX\nqrouter(n) log prouter(n).\n(9)\nIt is evident that, although the LM head router\nreduces the latency of the draft model, it comes at\nthe cost of a slight decrease in acceptance length œÑ\ndue to imperfect routing accuracy. Based on Equa-\ntion (5) and (6), the LM head router gets its best\nperformance when 1) the LM head accounts for\na significant portion of the latency of draft model\n2) the latency ratio between the draft model and\nthe target model is substantial. Therefore, we only\napply the LM head router to models with large vo-\ncabularies (Qwen2.5, Llama3) and relatively small\nsizes (7B, 14B).\nWe adopt a two-stage training strategy, where\nwe first train the draft model following the standard\ntraining procedure (either single-step or multi-step),\nand then fix the weights of draft model and train\nthe router separately. For further discussion, please\nrefer to Appendix D.\n4\nExperiments\nIn this section, we first introduce the experimental\nsetup, then discuss the overall effectiveness of our\nmethod, and finally present the ablation studies on\nCSRA and LM head router.\n4.1\nExperimental Setup\nTarget LLMs.\nWe choose Llama3-Instruct-\n8B/70B(Grattafiori et al., 2024), Llama2-chat-\n7B/13B(Touvron et al., 2023b) and Qwen2.5-\nInstruct-7B/14B(Yang et al., 2024) as our target\nmodels.\nTasks. We choose multiple datasets covering three\ntasks, including MT-Bench(Zheng et al., 2023) for\nmulti-turn dialogue, GSM8K(Cobbe et al., 2021)\nfor mathematical reasoning, and HumanEval(Chen\net al., 2021) for code generation. For 7B/14B mod-\nels, experiments are conducted with batch size of 1\non a single NVIDIA A6000 48G GPU. For Llama3-\n70B, we use 4√óA6000 GPUs due to memory re-\nquirements.\nMetrics. Since CORAL is a lossless speculative\ndecoding strategy, it is not necessary to measure\nthe generation quality. For acceleration, we use\ntwo metrics to evaluate the performance:\n‚Ä¢ Speedup Ratio: the actual speedup ratio com-\npared to vanilla decoding.\n‚Ä¢ Acceptance Length œÑ: the average number of\nnew tokens generated per drafting-verification\ncycle.\nComparisons. We use vanilla decoding as the\nbaseline (1.00√ó) to measure the speedup ratio. We\nprimarily compare CORAL with the latest lossless\nspeculative decoding methods, including EAGLE,\nEAGLE-2, and HASS. Since EAGLE is already\none of the fastest speculative decoding methods,\nwe choose EAGLE as the speculative decoding\nbaseline and do not compare with other methods\nwith lower speedup ratios.\nImplementation. Our implementation is based\non the open source repositories of HASS1 and\nEAGLE-22, and the settings are primarily iden-\ntical to those of them. All models are trained with\nShareGPT dataset for 20 epochs with batch size\nof 2 per GPU. For HASS and CORAL, the default\nstep for training is set to 3. Our system prompt for\nLlama3 is slightly different from that of EAGLE,\nplease refer to Appendix E for detailed discussion.\nFor inference, we employ a tree depth of 6 and\nselect 60 candidate tokens for all models.\n4.2\nEffectiveness and Ablation Studies\n4.2.1\nEffectiveness\nWe present the acceptance lengths œÑ and speedup\nratios of three datasets in Table 2. The results show\nthat CSRA achieves the best performance in both œÑ\nand speedup ratio (SR) in all experiments we have\ntested, surpassing EAGLE, EAGLE-2, and HASS.\nThe advantages of CSRA are more pronounced for\n1https://github.com/HArmonizedSS/HASS\n2https://github.com/SafeAILab/EAGLE\n6\n\nMT-bench\nHumanEval\nGSM8K\nAverage\nœÑ / SR\nœÑ / SR\nœÑ / SR\nœÑ / SR\nmodel\nmethod\nT=0\nT=1\nT=0\nT=1\nT=0\nT=1\nT=0\nL2-13B\nEAGLE\n3.93/3.04√ó\n3.23/2.35√ó\n4.51/3.47√ó\n3.47/2.56√ó\n4.01/3.10√ó\n3.51/2.59√ó\n4.15/3.20√ó\nEAGLE-2\n4.80/3.16√ó\n4.68/3.06√ó\n5.59/3.75√ó\n5.41/3.60√ó\n4.98/3.38√ó\n4.84/3.25√ó\n5.12/3.43√ó\nHASS\n5.20/3.42√ó\n5.02/3.26√ó\n5.99/4.01√ó\n5.79/3.86√ó\n5.32/3.60√ó\n5.24/3.51√ó\n5.50/3.68√ó\nCORAL\n5.25/3.45√ó\n5.10/3.32√ó\n6.06/4.07√ó\n5.90/3.93√ó\n5.39/3.65√ó\n5.25/3.51√ó\n5.57/3.72√ó\nL2-7B\nEAGLE\n3.80/2.67√ó\n3.21/2.10√ó\n4.29/3.04√ó\n3.55/2.33√ó\n3.84/2.73√ó\n3.48/2.30√ó\n3.87/2.81√ó\nEAGLE-2\n4.68/2.89√ó\n4.45/2.70√ó\n5.34/3.35√ó\n5.02/3.11√ó\n4.70/2.98√ó\n4.67/2.89√ó\n4.91/3.07√ó\nHASS\n5.02/3.09√ó\n4.77/2.88√ó\n5.71/3.58√ó\n5.35/3.30√ó\n5.11/3.25√ó\n4.99/3.10√ó\n5.28/3.31√ó\nCORAL\n5.09/3.13√ó\n4.86/2.94√ó\n5.73/3.58√ó\n5.48/3.40√ó\n5.12/3.25√ó\n5.05/3.13√ó\n5.31/3.32√ó\nL3-70B\nEAGLE\n2.87/2.24√ó\n2.62/2.02√ó\n3.73/2.93√ó\n3.45/2.67√ó\n3.46/2.71√ó\n3.23/2.50√ó\n3.35/2.63√ó\nEAGLE-2\n4.08/2.70√ó\n3.91/2.61√ó\n4.95/3.31√ó\n4.89/3.27√ó\n4.03/2.70√ó\n3.73/2.50√ó\n4.35/2.90√ó\nHASS\n4.10/2.71√ó\n4.00/2.65√ó\n5.23/3.49√ó\n5.10/3.40√ó\n4.12/2.76√ó\n3.83/2.56√ó\n4.48/2.99√ó\nCORAL\n4.23/2.79√ó\n4.13/2.72√ó\n5.31/3.54√ó\n5.19/3.46√ó\n4.34/2.90√ó\n3.91/2.61√ó\n4.63/3.08√ó\nL3-8B\nEAGLE\n2.63/1.65√ó\n2.30/1.35√ó\n3.65/2.29√ó\n3.13/1.85√ó\n3.47/2.18√ó\n3.05/1.78√ó\n3.25/2.04√ó\nEAGLE-2\n4.16/2.28√ó\n3.84/2.08√ó\n4.78/2.61√ó\n4.64/2.50√ó\n4.21/2.32√ó\n3.94/2.13√ó\n4.38/2.40√ó\nHASS\n4.48/2.45√ó\n4.12/2.21√ó\n5.31/2.89√ó\n5.12/2.76√ó\n4.56/2.51√ó\n4.18/2.28√ó\n4.78/2.62√ó\nCORAL\n4.57/2.50√ó\n4.15/2.24√ó\n5.43/2.95√ó\n5.28/2.83√ó\n4.70/2.58√ó\n4.39/2.38√ó\n4.90/2.68√ó\nCORAL w/ r.\n4.26/2.63√ó\n3.92/2.39√ó\n5.22/3.21√ó\n5.03/3.07√ó\n4.42/2.76√ó\n4.12/2.53√ó\n4.63/2.87√ó\nQ2.5-14B\nEAGLE\n2.63/1.83√ó\n2.33/1.55√ó\n3.31/2.31√ó\n2.82/1.88√ó\n3.62/2.52√ó\n3.21/2.16√ó\n3.19/2.22√ó\nEAGLE-2\n4.08/2.36√ó\n3.76/2.15√ó\n5.01/2.89√ó\n4.85/2.78√ó\n4.62/2.69√ó\n4.58/2.65√ó\n4.57/2.65√ó\nHASS\n4.52/2.59√ó\n4.12/2.35√ó\n5.50/3.18√ó\n5.37/3.07√ó\n5.03/2.92√ó\n4.91/2.83√ó\n5.02/2.90√ó\nCORAL\n4.56/2.62√ó\n4.13/2.35√ó\n5.64/3.26√ó\n5.40/3.09√ó\n5.16/3.00√ó\n5.12/2.95√ó\n5.12/2.96√ó\nCORAL w/ r.\n4.26/2.74√ó\n3.88/2.46√ó\n5.31/3.44√ó\n5.12/3.28√ó\n4.80/3.14√ó\n4.72/3.05√ó\n4.79/3.11√ó\nQ2.5-7B\nEAGLE\n2.53/1.56√ó\n2.17/1.23√ó\n3.04/1.87√ó\n2.62/1.49√ó\n3.32/2.05√ó\n2.86/1.63√ó\n2.96/1.83√ó\nEAGLE-2\n3.91/2.13√ó\n3.45/1.86√ó\n4.62/2.53√ó\n4.36/2.35√ó\n4.23/2.33√ó\n4.07/2.21√ó\n4.25/2.33√ó\nHASS\n4.15/2.26√ó\n3.65/1.96√ó\n4.96/2.71√ó\n4.74/2.55√ó\n4.53/2.49√ó\n4.35/2.35√ó\n4.55/2.49√ó\nCORAL\n4.22/2.30√ó\n3.83/2.05√ó\n5.09/2.78√ó\n4.86/2.62√ó\n4.67/2.57√ó\n4.50/2.44√ó\n4.66/2.55√ó\nCORAL w/ r.\n4.02/2.50√ó\n3.62/2.21√ó\n4.86/3.05√ó\n4.57/2.81√ó\n4.38/2.76√ó\n4.16/2.58√ó\n4.42/2.77√ó\nTable 2: Acceptance lengths œÑ and speedup ratio (SR) of different methods on MT-bench, HumanEval, and GSM8K\ndatasets with temperature T ‚àà{0, 1}. The best results are in bold, and some minor advantages may be obscured\ndue to rounding. We also calculate the average œÑ and SR under T = 0 for a more direct comparison. L2, L3,\nQ2.5 represents Llama2-Chat, Llama3-Instruct, and Qwen2.5-Instruct, respectively. As clarified in Section 3.3,\nwe apply LM head router for relatively small LLMs with large vocabularies (denoted as CORAL w/ r.), such as\nQwen2.5-7B/14B and Llama3-8B. For Llama2 series and Llama3-70B, we use CSRA only.\nLLMs with larger vocabularies, whereas the bene-\nfits are less significant for earlier models such as\nLlama2. For LM head router, we set the group num-\nber to 16 and choose the top-2 groups for the best\nperformance. Although the router sacrifices some\nacceptance length, the overall speedup ratio bene-\nfits from reduced latency and shows a considerable\nincrease.\n4.2.2\nAblation Study on CSRA\nWe adjust the number of training steps and make\na more detailed comparison with HASS. Since\nCSRA and HASS employ the same draft model,\nthe inference overheads are identical, we therefore\ncompare the acceptance length only. The results in\nTable 3 show that CSRA consistently outperforms\nHASS under different training steps.\nTo provide a more intuitive measure of the align-\nMT-bench\nHumanEval\nGSM8K\nstep\nHASS\nCSRA\nHASS\nCSRA\nHASS\nCSRA\n2\n4.41\n4.53\n5.24\n5.35\n4.50\n4.60\n3\n4.48\n4.57\n5.31\n5.43\n4.56\n4.70\n4\n4.46\n4.58\n5.39\n5.55\n4.58\n4.70\nTable 3: Acceptance length of Llama3-8B under differ-\nent alignment steps. Step-3 is the default setting.\nment between the draft model and the target model,\nwe compare the acceptance rates Œ± of HASS and\nCSRA at different timesteps during inference, as\nshown in Figure 6. The results show that CSRA\ngenerally outperforms HASS at different timesteps.\n4.2.3\nAblation Study on LM Head Router\nThe LM head router has two hyperparameters: the\ntotal number of groups N, and the number of top-\n7\n\nn groups to activate during inference. A larger\ngroup number, although leading to activating fewer\nparameters, would increase the difficulty of training\nand damage accuracy. Similarly, how many groups\nto activate is also a trade-off between speed and\naccuracy. We perform a grid search over these\ntwo hyperparameters in the MT-bench dataset with\nLlama3-8B, and the results are shown in Table 4.\nCORAL T=0\nN\ntop1\ntop2\ntop3\ntop4\ntop6\ntop8\nN/A 2.50√ó\n-\n-\n-\n-\n-\n4\n2.60√ó 2.46√ó\n-\n-\n-\n-\n8\n2.62√ó 2.61√ó 2.54√ó\n-\n-\n-\n16\n2.53√ó 2.63√ó 2.60√ó 2.57√ó\n-\n-\n32\n2.41√ó 2.59√ó 2.60√ó 2.61√ó 2.56√ó\n-\n64\n2.33√ó 2.51√ó 2.55√ó 2.57√ó 2.57√ó 2.53√ó\nEAGLE-2 T=0\nN\ntop1\ntop2\ntop3\ntop4\ntop6\ntop8\nN/A 2.28√ó\n-\n-\n-\n-\n-\n4\n2.44√ó 2.29√ó\n-\n-\n-\n-\n8\n2.40√ó 2.39√ó 2.33√ó\n-\n-\n-\n16\n2.30√ó 2.41√ó 2.39√ó 2.36√ó\n-\n-\n32\n2.24√ó 2.37√ó 2.40√ó 2.38√ó 2.35√ó\n-\n64\n2.18√ó 2.33√ó 2.37√ó 2.37√ó 2.37√ó 2.33√ó\nTable 4: Speedup of Llama3-8B with LM head router on\nMT-bench dataset. We group the LM head parameters\ninto N groups and selectively activate top-n of them.\nN/A denotes the results without LM head router.\nThe results show that our method consistently\nyields significant improvements, regardless of\nwhether multi-step training is employed.\nFor\nCORAL, dividing the LM head into 16 groups and\nactivating the top-2 groups during inference brings\nthe best speedup performance. Since the optimal\nsetting may vary across different LLMs and can-\nnot be easily estimated, we recommend empirical\nstudies to identify the optimal configuration.\n5\nRelated Work\nThere has been a significant amount of work in ac-\ncelerating LLMs. Some methods focus on reducing\nthe number of parameters, such as low-bit quanti-\nzation (Dettmers et al., 2022; Frantar et al., 2023;\nXiao et al., 2023; Lin et al., 2024), and model distil-\nlation (Gu et al., 2024; Ko et al., 2024; Zhong et al.,\n2024). Recently, some studies have also explored\nactivating only a subset of model parameters during\ninference to reduce memory access cost (Du et al.,\n2022; Fedus et al., 2022). Speculative decoding\n62\n64\n66\n68\n70\n0-Œ±\n1-Œ±\n2-Œ±\n3-Œ±\n4-Œ±\n5-Œ±\nLlama3-8B-Instruct T=0\nHASS\nCSRA\n59\n61\n63\n65\n67\n0-Œ±\n1-Œ±\n2-Œ±\n3-Œ±\n4-Œ±\n5-Œ±\nLlama3-8B-Instruct T=1\nHASS\nCSRA\nFigure 6: Acceptance rates in MT-bench dataset. Here\nn-Œ± denotes the acceptance rate of the n-th token.\n(Chen et al., 2023; Leviathan et al., 2023) leverages\nthe memory-bound nature of decoder-only LLMs\nand achieves lossless acceleration using a drafting-\nverification framework.\nResearch on speculative decoding has primarily\nfocused on two areas: 1) drafter design, 2) verifi-\ncation strategy. For drafter design, Medusa (Cai\net al., 2024) attaches multiple heads to the origi-\nnal LLM and predict multiple subsequent tokens\none time. Hydra (Ankner et al., 2024) improves\nMedusa by enhancing correlations between draft\nheads. Clover (Xiao et al., 2024) introduces an\nRNN-based draft head. Some methods utilize more\ninformation from target model to improve align-\nment, EAGLE (Li et al., 2024b) combines the out-\nput token and last hidden states of target LLMs\nto resolve the uncertainty in drafter‚Äôs prediction.\nGLIDE (Du et al., 2024) reuses the KV cache of\ntarget LLMs. For the verification strategy, Hu and\nHuang (2024); Sun et al. (2024) find that the accep-\ntance length of speculative sampling is not optimal\nand take into account the probability of subsequent\ntokens. SpecInfer (Miao et al., 2024) proposes de-\ncoding tree for verification. Sequoia (Chen et al.,\n2024), EAGLE-2 (Li et al., 2024a), and OPT-tree\n(Wang et al., 2024) adopts a dynamic tree structure.\n6\nConclusion\nThis paper proposes CORAL, an efficient spec-\nulative decoding method.\nWe introduce Cross-\nStep Representation Alignment, which effectively\nmitigates training-inference misalignment and im-\nproves the accuracy of speculation. Additionally,\nwe propose the LM head router, a plug-and-play\nmodule designed to reduce the latency of the draft\nmodel. We compare CORAL with other state-of-\nthe-art methods on various LLMs and datasets, and\nthe results show that CORAL achieves the best\nspeedup performance.\n8\n\nLimitations\nThere are mainly two limitations in this work.\nFirstly, the introduction of CSRA loss may lead\nto a slight increase in regression loss, which re-\nsults in a decrease in the acceptance length if the\ndraft model is trained with single step. This issue\ncan be addressed by multi-step training. Secondly,\nadopting a large vocabulary is a trend in the devel-\nopment of modern LLMs, and our LM head router\nis specifically designed for LLMs with large vocab-\nularies. It might not be suitable for models with\nsmall vocabularies, as the computational overhead\nof LM head is limited in the overall wall time of\nspeculative decoding. In this case, the time saved\nby the draft model cannot compensate for the loss\nin acceptance length.\nReferences\nZachary Ankner, Rishab Parthasarathy, Aniruddha\nNrusimha, Christopher Rinard, Jonathan Ragan-\nKelley, and William Brandon. 2024.\nHydra:\nSequentially-dependent draft heads for medusa de-\ncoding. arXiv preprint arXiv:2402.05109.\nTianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng,\nJason D. Lee, Deming Chen, and Tri Dao. 2024.\nMedusa: Simple LLM inference acceleration frame-\nwork with multiple decoding heads. In Proceedings\nof the International Conference on Machine Learn-\ning.\nCharlie Chen, Sebastian Borgeaud, Geoffrey Irving,\nJean-Baptiste Lespiau, Laurent Sifre, and John\nJumper. 2023. Accelerating large language model\ndecoding with speculative sampling. arXiv preprint\narXiv:2302.01318.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Pond√© de Oliveira Pinto, et al. 2021. Eval-\nuating large language models trained on code. arXiv\npreprint arXiv:2107.03374.\nZhuoming Chen, Avner May, Ruslan Svirschevski,\nYuhsun Huang, Max Ryabinin, Zhihao Jia, and\nBeidi Chen. 2024. Sequoia: Scalable, robust, and\nhardware-aware speculative decoding. arXiv preprint\narXiv:2402.12374.\nSumit Chopra, Raia Hadsell, and Yann LeCun. 2005.\nLearning a similarity metric discriminatively, with\napplication to face verification. In Proceedings of the\nConference on Computer Vision and Pattern Recog-\nnition.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, Christopher Hesse, and John Schulman.\n2021. Training verifiers to solve math word prob-\nlems. arXiv preprint arXiv:2110.14168.\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra,\nand Christopher R√©. 2022. Flashattention: Fast and\nmemory-efficient exact attention with io-awareness.\nIn Advances in Neural Information Processing Sys-\ntem.\nTim Dettmers, Mike Lewis, Younes Belkada, and Luke\nZettlemoyer. 2022. LLM.int8(): 8-bit matrix multi-\nplication for transformers at scale.\nCunxiao Du, Jing Jiang, Yuanchen Xu, Jiawei Wu,\nSicheng Yu, Yongqi Li, Shenggui Li, Kai Xu, Liqiang\nNie, Zhaopeng Tu, and Yang You. 2024. GliDe with\na cape: A low-hassle method to accelerate specula-\ntive decoding. In Proceedings of the International\nConference on Machine Learning.\nNan Du, Yanping Huang, Andrew M. Dai, Simon Tong,\nDmitry Lepikhin, Yuanzhong Xu, Maxim Krikun,\net al. 2022. GLaM: Efficient scaling of language\nmodels with mixture-of-experts. In Proceedings of\nthe International Conference on Machine Learning.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2022.\nSwitch transformers: Scaling to trillion parameter\nmodels with simple and efficient sparsity. Journal of\nMachine Learning Research.\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, and\nDan Alistarh. 2023. GPTQ: Accurate post-training\nquantization for generative pre-trained transformers.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,\nAbhinav Pandey, Abhishek Kadian, et al. 2024.\nThe llama 3 herd of models.\narXiv preprint\narXiv:2407.21783.\nYuxian Gu, Li Dong, Furu Wei, and Minlie Huang.\n2024. MiniLLM: Knowledge distillation of large\nlanguage models. In Proceedings of the International\nConference on Learning Representations.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, et al. 2022. Training compute-\noptimal large language models.\narXiv preprint\narXiv:2203.15556.\nZhengmian Hu and Heng Huang. 2024. Accelerated\nspeculative sampling based on tree monte carlo. In\nProceedings of the International Conference on Ma-\nchine Learning.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B.\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models.\narXiv\npreprint arXiv:2001.08361.\nJongwoo Ko, Sungnyun Kim, Tianyi Chen, and Se-\nYoung Yun. 2024. Distillm: Towards streamlined dis-\ntillation for large language models. In Proceedings of\nthe International Conference on Machine Learning.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph Gon-\nzalez, Hao Zhang, and Ion Stoica. 2023. Efficient\n9\n\nmemory management for large language model serv-\ning with pagedattention. In Proceedings of the29th\nSymposium on Operating Systems Principles.\nYaniv Leviathan, Matan Kalman, and Yossi Matias.\n2023. Fast inference from transformers via spec-\nulative decoding. In Proceedings of the International\nConference on Machine Learning.\nYuhui Li, Fangyun Wei, Chao Zhang, and Hongyang\nZhang. 2024a. EAGLE-2: Faster inference of lan-\nguage models with dynamic draft trees. In Proceed-\nings of the Conference on the Empirical Methods in\nNatural Language Processing.\nYuhui Li, Fangyun Wei, Chao Zhang, and Hongyang\nZhang. 2024b. EAGLE: Speculative sampling re-\nquires rethinking feature uncertainty. In Proceedings\nof the International Conference on Machine Learn-\ning.\nJi Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-\nMing Chen, Wei-Chen Wang, Guangxuan Xiao,\nXingyu Dang, Chuang Gan, and Song Han. 2024.\nAWQ: activation-aware weight quantization for on-\ndevice LLM compression and acceleration. In Pro-\nceedings of the Annual Conference on Machine\nLearning and Systems.\nXupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao\nCheng, Zeyu Wang, engxin Zhang, Rae Ying Yee\nWong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, Chu-\nnan Shi, Zhuoming Chen, Daiyaan Arfeen, Reyna\nAbhyankar, and Zhihao Jia. 2024. SpecInfer: Accel-\nerating large language model serving with tree-based\nspeculative inference and verification. In Proceed-\nings of the ACM International Conference on Archi-\ntectural Support for Programming Languages and\nOperating Systems.\nOpenAI. 2023. GPT-4 technical report. arXiv preprint\narXiv:2303.08774.\nFlorian Schroff, Dmitry Kalenichenko, and James\nPhilbin. 2015. FaceNet: A unified embedding for\nface recognition and clustering. In Proceedings of the\nConference on Computer Vision and Pattern Recog-\nnition.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the Annual Meeting\nof the Association for Computational Linguistics.\nZiteng Sun, Uri Mendlovic, Yaniv Leviathan, Asaf\nAharoni,\nAhmad Beirami,\nJae Hun Ro,\nand\nAnanda Theertha Suresh. 2024. Block verification\naccelerates speculative decoding.\narXiv preprint\narXiv:2403.10444.\nChaofan Tao, Qian Liu, Longxu Dou, Niklas Muen-\nnighoff, Zhongwei Wan, Ping Luo, Min Lin, and\nNgai Wong. 2024. Scaling laws with vocabulary:\nLarger models deserve larger vocabularies.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth√©e Lacroix,\nBaptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023a. Llama: Open\nand efficient foundation language models.\narXiv\npreprint arXiv:2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, et al. 2023b. Llama 2: Open foun-\ndation and fine-tuned chat models. arXiv preprint\narXiv:2307.09288.\nA√§ron van den Oord, Yazhe Li, and Oriol Vinyals. 2018.\nRepresentation learning with contrastive predictive\ncoding. arXiv preprint arXiv:1807.03748.\nChanghan Wang, Kyunghyun Cho, and Jiatao Gu. 2020.\nNeural machine translation with byte-level subwords.\nIn Proceedings of the AAAI Conference on Artificial\nIntelligence.\nJikai Wang, Yi Su, Juntao Li, Qingrong Xia, Zi Ye,\nXinyu Duan, Zhefeng Wang, and Min Zhang. 2024.\nOpt-tree: Speculative decoding with adaptive draft\ntree structure. arXiv preprint arXiv:2406.17276.\nBin Xiao, Chunan Shi, Xiaonan Nie, Fan Yang, Xi-\nangwei Deng, Lei Su, Weipeng Chen, and Bin Cui.\n2024. Clover: Regressive lightweight speculative\ndecoding with sequential knowledge. arXiv preprint\narXiv:2405.00263.\nGuangxuan Xiao, Ji Lin, Micka√´l Seznec, Hao Wu,\nJulien Demouth, and Song Han. 2023. SmoothQuant:\nAccurate and efficient post-training quantization for\nlarge language models. In Proceedings of the Inter-\nnational Conference on Machine Learning.\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui,\net al. 2024. Qwen2.5 technical report. arXiv preprint\narXiv:2412.15115.\nLefan Zhang, Xiaodan Wang, Yanhua Huang, and\nRuiwen Xu. 2024. Learning harmonized represen-\ntations for speculative sampling.\narXiv preprint\narXiv:2408.15766.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang,\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judging\nllm-as-a-judge with mt-bench and chatbot arena. In\nAdvances in Neural Information Processing Systems.\nQihuang Zhong, Liang Ding, Li Shen, Juhua Liu, Bo Du,\nand Dacheng Tao. 2024. Revisiting knowledge dis-\ntillation for autoregressive language models. In Pro-\nceedings of the Annual Meeting of the Association\nfor Computational Linguistics.\nA\nHyperparameters in CSRA Loss\nThe temperature of LCSRA is set to 0.07, consistent\nwith some previous works such as CLIP (Zheng\net al., 2023).\n10\n\nThen we set wreg to 0.5, half of EAGLE‚Äôs origi-\nnal setting. The weight of CSRA loss is adjusted\naccording to different target models, making the\nvalues of wCSRALCSRA and wregLreg roughly the\nsame. In this way, the loss imposed on representa-\ntion is approximately the same as EAGLE/HASS\ntraining.\nBased on the values of wregLreg, we choose\nwCSRA\n=\n0.2 for Qwen2.5-7B, wCSRA\n=\n0.15 for Llama3-8B, wCSRA = 0.1 for Llama3-\n70B, Qwen2.5-14B and Llama2-7B, and 0.05 for\nLlama2-13B.\nB\nTraining Details\nWe utilize a fixed dataset of 68,000 examples from\nShareGPT3 as our training set, which is identical\nto EAGLE and HASS. CORAL requires approx-\nimately 2 days to train a 7B draft model under\ndefault settings (training step=3, epoch=20). It\nis worth noting that draft models with large vo-\ncabularies such as Llama3 and Qwen2.5 require\nmore GPU memory compared to Llama2, so we use\n4√óNVIDIA H20-96G GPUs for training. Training\nlarge draft models such as Llama3-70B on A100-\n40G GPU may result in out-of-memory issues un-\nder our experimental settings. We recommend us-\ning GPUs with larger memory capacities or choos-\ning other alternatives (e.g., reducing the batch size,\nmodel parallelism).\nC\nSingle-step Training with CSRA\nWe do not recommend using the CSRA loss in the\ncontext of single-step training. Our empirical find-\nings suggest that introducing the CSRA loss may\nlead to a slight increase in regression loss, likely\ndue to the mismatch between the two optimization\nobjectives. Specifically, the CSRA loss focuses\nsolely on the angular relationships between the out-\nput features, without imposing any constraints on\nthe feature norm, whereas the regression loss aims\nto learn features that are identical to the target. The\nincrease in regression loss may damage the accep-\ntance length. We present the results of CSRA with\nsingle-step training in Table 5.\nA plausible explanation for this phenomenon is\nthat in single-step training, the draft model lacks\nexposure to subsequent steps, therefore the L1 dis-\ntance between the prediction and target feature is\nrelatively more critical. In contrast, for multi-step\n3https://huggingface.co/datasets/Aeala/ShareGPT_Vicuna\n_unfiltered\nMT-bench HumanEval GSM8K\nEAGLE-2\n4.16\n4.78\n4.21\nCSRA Step1\n4.10\n4.70\n4.10\nTable 5: Acceptance length of Llama3-8B EAGLE-2\nand CORAL model with single-step training.\ntraining, the draft model learns to adapt to subse-\nquent steps, making the discriminative power of\ndifferent representations and the multi-step consis-\ntency more crucial.\nD\nDiscussion on LM Head Router\nIn this section, we will discuss some issues of LM\nhead router.\nTree decoding. In tree decoding, each timestep\ncontains multiple candidate tokens. Since each can-\ndidate requires a different set of LM head groups,\nwe need to activate all the involved groups, which\nmay bring additional latency. In some cases, we\neven need to activate the entire LM head parame-\nters (e.g., if we take the top two groups and top 10\ncandidates, the worst-case scenario might require\nactivating 20 groups).\nThis issue can be addressed through appropri-\nate grouping strategies. First, dividing the tokens\ninto more groups helps alleviate the problem. For\ninstance, with a total of 32 groups, selecting the\ntop 10 candidates from the top 2 groups ensures\nthat the LM head parameters are not fully activated,\neven in the worst-case scenario. Second, modern\nLLMs utilize BPE (Sennrich et al., 2016) or BBPE\n(Wang et al., 2020) for tokenization, where higher-\nfrequency tokens tend to be concentrated in groups\nwith smaller indices. As a result, such an extreme\nscenario is unlikely to occur in practice.\nTwo-stage training. There are mainly two reasons\nfor adopting two-stage training. Firstly, the two-\nstage training strategy ensures that the router serves\nas a plug-and-play module, without affecting the\nstandalone usage of the first-stage model, thereby\nproviding greater flexibility. Secondly, since the\nnumber of groups is a hyperparameter that may\nrequire multiple experiments to determine the op-\ntimal setting, two-stage training allows us to store\nthe output of draft model and train the router only,\nmaking it easier for parameter tuning.\nBackends. Although many researches on specu-\nlative decoding measure the speedup ratio on Py-\nTorch, we do not consider PyTorch to be a good\nbackend. For example, as shown in Table 2, the\n11\n\nTrain\nTest\nMT-bench\nHumanEval\nGSM8K\nsys_p2\nsys_p2\n4.16\n4.78\n4.21\nsys_p1\n4.11(-0.05)\n4.73(-0.05)\n4.27(+0.06)\nsys_p1\nsys_p1\n4.18\n4.78\n4.38\nsys_p2\n3.87(-0.31)\n4.17(-0.61)\n3.93(-0.45)\nopen source\n(sys_p1)\nsys_p1\n4.24\n4.92\n4.34\nsys_p2\n3.94(-0.30)\n4.67(-0.25)\n3.91(-0.43)\nTable 6: Acceptance lengths of EAGLE-2 for Llama3-8B-Instruct with different system prompts.\nFP16 latency of Llama3-8B-draft head on RTX\nA6000 GPU is 1.51ms, which is close to the the-\noretical time of 1.3ms (1002M memory access\nwith 768GB/s bandwidth).\nHowever, for other\nparts, which mainly consists of transformer, the\nactual time is much higher than the theoretical time\n(1.07ms vs 0.63ms), achieving only about 60% of\nthe theoretical performance.\nThis is a problem inherent to PyTorch. For in-\nstance, in Qwen2 speed benchmark4, the inference\nspeed of 7B model on A100 80G GPU is only 38\ntoken/s (i.e., 26ms/token), which is far from the\ntheoretical time of about 7ms (estimated by 14G\nmemory access with 2TB/s bandwidth). This prob-\nlem can be mitigated by using a more optimized\nbackend, such as vLLM (Kwon et al., 2023).\nTherefore, the performance of the LM head\nrouter may be affected by the hardware and back-\nend conditions. In a well-optimized backend, the\nrouter‚Äôs performance will be better than reported\nin this paper, as the latency of the LM head will\noccupy a larger proportion in the draft model.\nE\nDiscussion on System Prompt\nEAGLE utilizes the system prompt from the official\nLlama2-chat example5:\nsys_p1 = You are a helpful, respectful and honest\nassistant. Always answer as helpfully as possible,\nwhile being safe. Your answers should not include\nany harmful, unethical, racist, sexist, toxic, dan-\ngerous, or illegal content. Please ensure that your\nresponses are socially unbiased and positive in na-\nture.\\n\\nIf a question does not make any sense, or\nis not factually coherent, explain why instead of an-\nswering something not correct. If you don‚Äôt know\nthe answer to a question, please don‚Äôt share false\ninformation.\nThe same system prompt is also used in Llama3\n4https://qwen.readthedocs.io/en/v2.0/benchmark/speed_be\nnchmark.html\n5https://huggingface.co/blog/llama2\ndrafter training. However, it appears that Llama3\ndoes not have a default system prompt. Never-\ntheless, we find the system prompt in the offi-\ncial Llama3.3 example6 is simpler and also widely\nadopted:\nsys_p2 = You are a helpful assistant\nThe system prompt has a certain impact on the\nacceptance length and speedup ratio. To investigate\nthis, we compared the open-source Llama3-8B-\nInstrct draft model in EAGLE official repository\n(trained with sys_p1) and draft models trained by\nourselves using sys_p1 and sys_p2. Our results in\nTable 6 show that switching between different sys-\ntem prompts might lead to a decrease in speedup\nand acceptance length on the MT-Bench and Hu-\nmaneval datasets, while GSM8K is an exception.\nUpon closer inspection of the GSM8K results,\nwe find that when using sys_p1, most responses\nstart with a sentence similar to \"Let‚Äôs break this\ndown step by step\", whereas when using sys_p2,\nthe beginning if outputs will be more diverse. This\nsuggests that the speedup ratio using sys_p1 might\nbe artificially inflated in some cases.\nFurthermore, since longer system prompts pro-\nvide the draft model with more context, we suppose\nthat detailed prompts and increased information\ncould potentially improve the performance of draft\nmodel when the system prompt of training and\ninference is aligned. However, when the system\nprompts are not consistent, training the model with\na more detailed system prompt may lead to greater\nperformance degradation.\nTo obtain a more generalizable draft model, we\nuse sys_p2 in all experiments with Llama3-Instruct\n8B/70B. We believe a more general and simple\nsystem prompt would reflect the draft model‚Äôs true\ncapabilities more accurately.\n6https://github.com/meta-llama/llama-\nmodels/blob/main/models/llama3_3/prompt_format.md\n12\n\nF\nLicenses of Artifacts\nWe present the licenses of artifacts related to this\npaper in table 7.\nmodels\nLlama3\nllama3 license\nLlama2\nllama2 license\nQwen2.5\napache-2.0\ndatasets\nShareGPT\napache-2.0\nMT-bench\nCC-BY-4.0\nHumanEval\nMIT\nGSM8K\nMIT\ncodes\nEAGLE/EAGLE2\napache-2.0\nHASS\nnot provided\nTable 7: Licenses of artifacts\nG\nUse of AI Assistants\nWe use Llama3.2-90B to assist with grammar\nchecks and text polishing in the writing of this\npaper.\n13\n",
  "metadata": {
    "source_path": "papers/arxiv/CORAL_Learning_Consistent_Representations_across_Multi-step_Training\n__with_Lighter_Speculative_Drafter_35b6d42f59898e0f.pdf",
    "content_hash": "35b6d42f59898e0f4be083dc619f7e759f5f45c9efbf05fec35fb6cb812c16e1",
    "arxiv_id": null,
    "title": "CORAL_Learning_Consistent_Representations_across_Multi-step_Training\n__with_Lighter_Speculative_Drafter_35b6d42f59898e0f",
    "author": "",
    "creation_date": "D:20250225022651Z",
    "published": "2025-02-25T02:26:51",
    "pages": 13,
    "size": 548095,
    "file_mtime": 1740470215.6611564
  }
}