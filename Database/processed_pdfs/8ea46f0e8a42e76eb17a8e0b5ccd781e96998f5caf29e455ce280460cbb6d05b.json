{
  "text": "Middle-Layer Representation Alignment for Cross-Lingual Transfer in\nFine-Tuned LLMs\nDanni Liu\nJan Niehues\nKarlsruhe Institute of Technology, Germany\n{danni.liu, jan.niehues}@kit.edu\nAbstract\nWhile large language models demonstrate re-\nmarkable capabilities at task-specific applica-\ntions through fine-tuning, extending these ben-\nefits across diverse languages is essential for\nbroad accessibility. However, effective cross-\nlingual transfer is hindered by LLM perfor-\nmance gaps across languages and the scarcity\nof fine-tuning data in many languages. Through\nanalysis of LLM internal representations from\nover 1,000+ language pairs, we discover that\nmiddle layers exhibit the strongest potential\nfor cross-lingual alignment. Building on this\nfinding, we propose a middle-layer alignment\nobjective integrated into task-specific training.\nOur experiments on slot filling, machine trans-\nlation, and structured text generation show con-\nsistent improvements in cross-lingual transfer,\nespecially to lower-resource languages. The\nmethod is robust to the choice of alignment\nlanguages and generalizes to languages unseen\nduring alignment. Furthermore, we show that\nseparately trained alignment modules can be\nmerged with existing task-specific modules, im-\nproving cross-lingual capabilities without full\nre-training. Our code is publicly available1.\n1\nIntroduction\nDecoder-only large language models (LLMs) have\nemerged as the dominant paradigm in NLP. While\nthese models exhibit promising zero-shot capabil-\nities (Wei et al., 2022; Chowdhery et al., 2023),\nfurther task-specific fine-tuning remains crucial for\noptimal performance in many applications (Shen\net al., 2024; Xu et al., 2024; Alves et al., 2024).\nDuring fine-tuning, a practical challenge is that\nthe available training data rarely covers all lan-\nguages supported by LLMs. This highlights the\nimportance of cross-lingual transfer to extend task-\nspecific performance gains across languages.\nWhile cross-lingual transfer has been extensively\nstudied (Wang and Zheng, 2015; Ruder et al., 2019;\n1https://github.com/dannigt/mid-align\nArtetxe and Schwenk, 2019b), achieving it on gen-\nerative tasks with variable-length outputs remains\nchallenging (Vu et al., 2022; Li and Murray, 2023)\ncompared to classification tasks. This challenge is\nespecially relevant for LLMs, which formulate all\ntasks as next-token prediction problems.\nThe theoretical foundation of cross-lingual trans-\nfer lies in the analogous relationships between con-\ncepts across languages. This intuition was first\ndemonstrated in cross-lingual word embeddings\n(Mikolov et al., 2013; Lample et al., 2018; Xu and\nKoehn, 2021), where these vector representations\nexhibit isometric relationships, i.e., the geometric\nstructure of semantically equivalent items is pre-\nserved across different languages. This isometry\nproperty has proven crucial for transferring learned\nmodels across languages (Schuster et al., 2019;\nWang et al., 2024b). Subsequent encoder-decoder\nmodels (Ha et al., 2016) and decoder-only models\n(Wu et al., 2024a) also exhibit similar properties in\ntheir internal representations.\nWhile pretrained multilingual models naturally\ndevelop some degree of unified multilingual rep-\nresentations (Pires et al., 2019; Conneau et al.,\n2020; Muller et al., 2021), explicitly strengthening\nthe relationships between semantically equivalent\ncontent has shown benefits in various downstream\ntasks: cross-lingual retrieval (Yu et al., 2018), par-\nallel text mining (Schwenk et al., 2021), zero-shot\nclassification (Hu et al., 2021; Gritta and Iacobacci,\n2021) and translation (Arivazhagan et al., 2019;\nPham et al., 2019; Duquenne et al., 2022). Despite\ndifferent approaches, these works share a common\nobjective: aligning representations of semantically\nequivalent content across languages while preserv-\ning overall expressiveness.\nCross-lingual alignment approaches have been\nsuccessfully applied to models preceding LLMs.\nFor encoder-only models, outputs can be aligned\nby e.g., minimizing distances between parallel\nsentence representations (Feng et al., 2022) or\narXiv:2502.14830v1  [cs.CL]  20 Feb 2025\n\n0\n4\n8\n12\n16\n20\n24\n28\n32\nLayer ID\n0\n50\n100\nAvg. retrieval\naccuracy (%)\nLlama 3\n0\n4\n8\n12\n16\n20\n24\n28\nLayer ID\nQwen 2.5\nOverall\nLow-res.\n(a) Cross-lingual semantic alignment (measured by average\nretrieval accuracy over 35 languages and 1190 language di-\nrections) varies by layer, with the middle layer showing the\nhighest score. Lower-resource languages are poorly aligned.\n0\n20\n40\n60\n0\n25\n50\n75\n100\nTransfer result\nLlama 3\nCorrelation: 0.56\nF1\n0\n20\n40\n60\nQwen 2.5\nCorrelation: 0.70\nCross-lingual representation retrieval accuracy (%)\n(b) Positive correlation between base model cross-lingual se-\nmantic alignment and downstream transfer performance.\nFigure 1: Two observations (§2) motivating our ap-\nproach of aligning multilingual representations (§3).\ncross-lingual masked language modeling objectives\n(Conneau and Lample, 2019). These techniques\nare largely applicable to encoder-decoder models,\nwhere alignment is typically enforced to the en-\ncoder outputs (Duquenne et al., 2023). In contrast,\ndecoder-only models lack such clear separation be-\ntween input processing and output generation. This\nmakes it less obvious where and how to optimize\nfor cross-lingual alignment, as also highlighted in\nthe survey by Hämmerl et al. (2024).\nIn this work, we start by quantifying the degree\nof cross-lingual alignment present in two promi-\nnent LLMs, Llama 3 (AI @ Meta et al., 2024) and\nQwen 2.5 (Qwen Team et al., 2025). We then apply\nthese insights to improve cross-lingual transfer in\ntask-specific fine-tuning. By alternatively training\non alignment and task-specific data, we aim to im-\nprove the cross-lingual generalization to languages\nwithout fine-tuning data. We demonstrate trans-\nfer improvements across diverse tasks: slot filling,\nmachine translation, and structured text generation.\nOur main findings include:\n• Applying alignment objectives to middle layers\nduring LLM task-specific fine-tuning improves\ncross-lingual transfer (§5.1) and enhances align-\nment across all network depths (§5.2).\n• The transfer improvements extend beyond those\nlanguages seen in alignment (§5.1).\n• Our approach is robust to the choice of languages\nused for alignment training (§6.1, 6.2).\n• Task-specific and alignment modules trained sep-\narately can be combined post-hoc to improve\ntransfer performance (§6.3).\n2\nAnalyzing Cross-Lingual Alignment\nTo understand how well LLM representations cap-\nture semantic equivalence across languages, we\nuse translation retrieval as a diagnostic task. We\nchoose this retrieval task over other metrics like\ncosine similarity or SVCCA score (Raghu et al.,\n2017) because it better captures relative semantic\nrelationships. That is, if a model’s representations\nenable us to identify a sentence’s translation from\na set of candidates, the exact numerical distance\nbetween the query and the retrieved translation is\nless important than the ability to rank translations\nas the most semantically similar.\nSpecifically, we first extract model activations\nat each network layer for all language variants of\nthe input text. To handle variable-length sequences,\nwe create fixed-size sentence embeddings by mean-\npooling the activations over the sequence length\ndimension. For translation retrieval, given a query\nsentence in one language, we compare its embed-\nding to the embeddings of candidate sentences in\nthe target language using ratio-based margin sim-\nilarity (Artetxe and Schwenk, 2019a)2.\nFor N\nlanguages, we evaluate retrieval accuracy across\nall N(N −1) possible language pairs. We use\nthe FLORES-200 dataset (NLLB Team, 2024),\nwhich provides high-quality multiway parallel texts\nacross diverse languages (detailed setup in §4.2).\nOur investigation of LLama 3 and and Qwen 2.5\nmodels3 reveals three key findings:\nOverall weak semantic alignment, with peak in\nmiddle layers:\nAs shown in Figure 1a, the aver-\nage translation retrieval accuracy across 1,190 lan-\nguage pairs remains below 50%, with Llama 3 out-\nperforming Qwen 2.5. Low-resource languages4\nshow especially weak alignment, achieving less\nthan half of the overall average accuracy. In partic-\nular, the middle layers of both models demonstrate\nthe strongest retrieval performance. This suggests\nstronger potential for cross-lingual transfer at these\nintermediate representations.\nStrong correlation between base LLM semantic\nalignment and downstream task transfer:\nTo\nwhat extent can the semantic alignment present in\nthe base LLM predict cross-lingual transfer perfor-\nmance after supervised fine-tuning? Using multi-\n2shown to outperform cosine similarity for cross-lingual\nretrieval tasks (Artetxe and Schwenk, 2019a)\n3specifically the 8B-Instruct and 7B-Instruct variants\n4resource levels as defined by NLLB Team (2024)\n\n...\noutput\n+\nprefix\noutput\nCross-Entropy \nLoss\nTask-Specific \nTraining\nAlignment\nTraining\nContrastive \nLoss\nprefix\n…\ntn\nt2 t1\nsn\ns2 s1\ntn\nt2 t1\nsn\ns2 s1\n…\n…\n…\n…\nFigure 2: Illustration of our approach, alternating training between task-specific (left) and alignment (right)\nobjectives. The alignment objective operates on middle-layer representations.\n0\n4\n8\n12\n16\n20\n24\n28\n32\nLayer ID\n0\n50\n100\nAvg. retrieval\naccuracy (%)\nEn-only FT\n0\n4\n8\n12\n16\n20\n24\n28\n32\nLayer ID\nMultilingua FT\nFigure 3: Task-specific fine-tuning shows minimal im-\npact on semantic alignment.\nlingual slot filling as a case study, we train models\non 5 high-resource languages jointly and evaluate\ntransfer performance on 25 additional languages\n(detailed setup in §4.1). As shown in Figure 1b,\nfor both Llama 3 and Qwen 2.5, we observe strong\npositive correlations (p < 0.01) between middle-\nlayer retrieval accuracy and downstream task per-\nformance. This correlation suggests that increasing\ncross-lingual alignment in LLM intermediate rep-\nresentations may improve cross-lingual transfer.\nTask-specific fine-tuning preserves but does not\nenhance semantic alignment:\nAfter analyzing\nthe base LLMs, we examine how supervised fine-\ntuning affects the models’ internal semantic align-\nment. Using the same multilingual slot filling task\nas before, we study both English-only and multilin-\ngual fine-tuning. Despite multilingual fine-tuning\nbeing an established method for improving cross-\nlingual transfer (Li and Murray, 2023; Chirkova\nand Nikoulina, 2024), we observe that neither train-\ning configuration alters the models’ cross-lingual\nsemantic alignment (Figure 3). This preservation\nof baseline alignment patterns, even under mul-\ntilingual training, indicates that pure fine-tuning\ndoes not sufficiently strengthen cross-lingual align-\nment. This further motivates us towards explicit\ncross-lingual alignment during fine-tuning.\n3\nExplicit Alignment in fine-tuning\nWe propose an alternating training strategy to en-\ncourage cross-lingual alignment while maintaining\ntask performance. As illustrated in Figure 2, we\noptimize either the task-specific objective or the\nalignment objective in each training step.\nTask Objective: We follow standard causal lan-\nguage modeling, using a cross-entropy loss over\nthe predicted text conditioned on the input prefix.\nAlignment Objective: We use a contrastive loss\nmotivated by its successful applications in sen-\ntence embedding (Feng et al., 2022), dense retrieval\n(Karpukhin et al., 2020) and modality alignment\n(Ye et al., 2022; Girdhar et al., 2023). The loss max-\nimizes the similarity between translations while\nminimizing similarity between non-translations.\nGiven a batch B of n pairs of parallel sentences,\nthe alignment loss for a sentence pair (s, t) is:\nLalign = −log\nexp(sim(hi\ns, hi\nt))\nP\nv∈B exp(sim(his, hiv))\n(1)\nwhere hi\ns is the mean-pooled5 hidden states at the\nith LLM layer for input s and sim(·, ·) is a simi-\nlarity function. Motivated our finding that middle\nlayers have the strongest cross-lingual alignment\npotential, we select i as the middle layer and com-\npare its performance to other layer positions. We\nuse cosine similarity following prior works (Gao\net al., 2021; Ye et al., 2022). The similarity score\nis optionally scaled by a temperature parameter τ,\nwhich controls the peakiness of the softmax distri-\nbution and in turn determines the relative impor-\ntance of non-translation pairs. This temperature\nparameter is tuned on the development sets.\nActivating Individual Objectives: Note that the\ntask and alignment losses can be activated sepa-\nrately. Deactivating the alignment loss degener-\nates to standard task-only training. Conversely,\ndeactivating the task loss trains the model only for\nalignment. This modularity enables us to subse-\nquently combine separately-trained task and align-\nment models.\n5Initial experiments with attention pooling degraded per-\nformance. We also tried a stop-gradient operator on English\nrepresentations to align non-English representations towards\nEnglish, but it did not give consistent gains.\n\nDataset\nLanguages\nSlot Filling\nTask - train MASSIVE\n{ar, en, es, ru, zh}\nTask - test\nMASSIVE\nsupervised + {af, az, cy, de, el,\nfr, hi, is, ja, jv, sw, th, tl, tr, ur}\nAlignment\nTatoeba\nlow-res.: {cy, jv, jp, sw, tl}-en\nmid-res.: {el, hi, th, tr}-en\nhigh-res.: {ar, es, ru, zh}-en\nMachine Translation\nTask - train\nALMA\n{cs, de, is, ru, zh}↔en\nTask - test\nWMT 23\nsupervised + {he, ja, uk} ↔en\nAlignment\n(same as “Task - train”)\nJSON Generation (challenge task)\nTask - train\nUNER\n{en, pt, zh}\nTask - test\nUNER\nsupervised + {da, hr, sk, sr, sv}\nAlignment\nTatoeba\n{da, sv}-en\nSemantic Alignment Evaluation\nAlignment FLoRes-200\nN(N −1) pairs for N lang.\nTable 1: Dataset overview. More details in Appendix B.\n4\nExperimental Setup\n4.1\nData\nIn general, we fine-tune on several high-resource\nlanguages and then evaluate transfer performance\non additional languages.\nWe do not focus on\nEnglish-only fine-tuning, since our initial ex-\nperiments demonstrated that multilingual fine-\ntuning substantially outperforms English-only fine-\ntuning6, thus establishing it as a stronger baseline.\nTable 1 presents a dataset overview. Descriptions\nof the language codes are in Appendix C.\nMain Task Data: We evaluate our approach on\nslot filling and machine translation, both modeled\nas generative tasks with templates shown in Ap-\npendix D.2. For slot filling, we use the MASSIVE\ndataset (FitzGerald et al., 2023). We train on 5\nhigh-resource languages, and evaluate transfer per-\nformance on 15 additional diverse languages, 5 of\nwhich have non-Latin writing systems. This task\npresents a challenge due to the 60 possible slots,\nrequiring strictly following the output format for\ncorrect parsing. For machine translation, we use\nALMA (Xu et al., 2024)’s training and test data,\nand additionally test on 6 zero-shot directions from\nWMT 23 (Kocmi et al., 2023).\nChallenge Task Data: To assess performance on\nlong-sequence processing and structured text gener-\nation, we include JSON generation as a challenge\ntask. We use the UNER dataset (Mayhew et al.,\n2024) from the Aya collection (Singh et al., 2024),\n6These English-only FT results are in Appendix A.\nwhich requires following example instructions and\nextracting named entities into JSON format. A\nchallenge not present in the previous tasks is the\nlonger inputs, with an average input length exceed-\ning 150 tokens in English. For this task, we train on\n3 high-resource languages (en, pt, zh) and transfer\nto the 5 remaining languages.\nAlignment Data: For alignment, we mainly use\nparallel data to English from Tatoeba (Tiedemann,\n2020), except for machine translation, where the\ntraining sentences are inherently parallel. For slot\nfilling, our main experiments align the five lan-\nguages with the weakest baseline7 transfer perfor-\nmance (cy, jv, jp, sw, tl) reported by the dataset cre-\nators (FitzGerald et al., 2023). We choose them be-\ncause their weak baseline performance suggests a\nlack of effective transfer, providing a strong testbed\nfor evaluating the potential benefits of our align-\nment approach. For ablation, we alter the following\nfactors of the alignment data:\n• Resource level (low, medium, high-resource)\n• Language coverage\n• Domain (oracle data, different, very distant)\nFor machine translation, given the inherent se-\nmantic equivalence of translation pairs, we di-\nrectly leverage the translation data for alignment.\nFor JSON generation, we align the two lowest-\nresourced in UNER (da and sv)8 to English. For\nlower-resource languages, the alignment data are a\nfew hundreds as detailed in Appendix B.\n4.2\nEvaluation\nSemantic Alignment Evaluation: As described\nin §2, we evaluate cross-lingual semantic align-\nment by retrieval accuracy. Given N languages, we\nperform many-to-many retrieval and average the\naccuracy over the N(N −1) language pairs. For\nthe initial analyses (§2), the 35 languages are listed\nin Appendix C. We use the FLoRes-200 (NLLB\nTeam, 2024) development set with 997 parallel\nsentences. While FLoRes partially overlaps with\nALMA’s training data, it remains the only reliable\nmassively multilingual multiway corpus to the best\nof our knowledge. Alternative such as Tatoeba\nhave been advised against due to data imbalance\nand noise (Heffernan et al., 2022; Janeiro et al.,\n2024). We also demonstrate that this overlap does\n7their baseline is an XLM-R model trained on English\n8While Serbian (sr) is also low-resourced in UNER, we\nexclude it from alignment due to data quality. Running lan-\nguage identification reveals that many sentences in the Serbian\nalignment data are not actually in Serbian.\n\nID Model\nSlot Filling (MASSIVE)\nMachine Translation (WMT23)\nSupervised Transfer Transfer\nRetrieval\nSupervised\nTransfer\nRetrieval\n(5 lang.) (15 lang.) (aligned) (all 20 lang.)\n(5 lang.↔En)\n(3 lang.→En)\n(En→3 lang.) (all 9 lang.)\nF1\nF1\nF1\nAcc.\nBLEU COMET BLEU COMET BLEU COMET\nAcc.\n(1) LLAMA 3\n–\n–\n–\n39.1\n25.8\n75.5\n27.8\n75.8\n14.8\n71.3\n51.5\n(2) + SFT\n76.6\n60.2\n51.7\n39.4\n30.0\n81.5\n31.8\n82.8\n15.5\n79.6\n(55.3)\n(3)\n+ alignment\n77.0\n61.7\n55.5\n73.2\n29.9\n81.5\n32.3\n83.0\n17.0\n80.7\n(84.5)\n(4) QWEN 2.5\n–\n–\n–\n21.4\n23.0\n74.5\n28.5\n81.3\n12.6\n71.2\n36.5\n(5) + SFT\n76.3\n53.5\n41.6\n20.9\n27.4\n78.4\n29.7\n82.7\n14.6\n76.9\n(38.8)\n(6)\n+ alignment\n77.0\n55.3\n46.5\n20.5\n27.2\n77.6\n30.8\n82.7\n14.7\n76.9\n(75.6)\nTable 2: Overall supervised and transfer results. Retrieval accuracy averaged over all language pairs and layers.\nBold: highest task scores which outperforms the other setups. (Results in brackets): potentially inflated scores due\nto partial overlap between retrieval and translation data. Language-specific results in Appendix E.\nnot result in memorization effects (§6.2). When\nreporting an aggregated retrieval accuracy for a\nmodel, we average over all language pairs at even-\nnumbered layers’ retrieval accuracy, excluding the\ninput embedding layer.\nTask Performance Evaluation: For slot filling, we\nreport F1 scores using the original evaluation script\nby FitzGerald et al. (2023). For machine transla-\ntion, we report BLEU9 (Papineni et al., 2002) and\nCOMET-22 (Rei et al., 2022) scores. For JSON\ngeneration, we parse the generated outputs back to\nnamed entity tuples and then evaluate F1 scores.\n4.3\nModel, Training, and Inference\nWe build upon Llama (AI @ Meta et al.,\n2024) and Qwen (Qwen Team et al., 2025),\nspecifically Meta-Llama-3-8B-Instruct10 and\nQwen2.5-7B-Instruct. We use LoRA (Hu et al.,\n2022) adapters with a rank of 8 for all attention\ncomponents and linear projections. The effective\nbatch size is 128 for both objectives, with mini-\nbatches of 32 examples considered for the con-\ntrastive objective11. Alignment data from different\nlanguages are re-sampled to an approximately uni-\nform distribution. More details are in Appendix D.\n5\nMain Results\nThe main results are summarized in Table 2. Before\nassessing our proposed approach, we first estab-\nlish the necessity of supervised FT by comparing\n9nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.2\nsacreBLEU (Post, 2018) signature, with \"tok:ja-mecab-0.996-\nIPA\" for Japanese and \"tok:zh\" for Chinese.\n10chosen over more recent versions to limit test set con-\ntamination, as its knowledge cutoff (March 2023) predates our\ntranslation test set (WMT 23).\n11While contrastive learning typically benefits from larger\nbatch sizes (Chen et al., 2022), our initial experiments with\nincreased batch sizes did not give consistent improvements.\nit with zero-shot usage of the LLMs (rows (2, 5)\nvs. (1, 4)). On slot filling, the zero-shot perfor-\nmance of Llama 3 is very poor, achieving only\n6.6% F1 on English due to difficulties in adhering\nto task-specific formats. We therefore do not evalu-\nate its zero-shot performance on all languages. In\nmachine translation, supervised fine-tuning shows\nsubstantial gains of 4-6 COMET over zero-shot.\n5.1\nOverall Performance Comparison\nGains in cross-lingual transfer with supervised\nperformance preserved: Our approach improves\ncross-lingual transfer across different tasks and\nmodels.\nFor slot filling, we observe gains in\nboth supervised and transfer (F1 +0.4 and +1.5\nrespectively) settings on Llama fine-tuning, with\nsimilar improvements on Qwen (F1 +0.7 super-\nvised, +1.8 transfer). In machine translation with\nLlama in row (3), our approach brings substan-\ntial gains when transferring to out-of-English di-\nrections (+1.5 BLEU, +1.1 COMET). For into-\nEnglish directions, there is a modest improvement\nin +0.5 BLEU and +0.2 COMET. The larger\ngains on out-of-English directions suggest the ap-\nproach is more beneficial for non-English gener-\nation in this case. For Qwen in row (6), our ap-\nproach shows minor gains in into-English transla-\ntion (+1.1 BLEU but no change in COMET), and\ndoes not influence out-of-English scores. It also\nleads to a degradation (−0.8 COMET) on super-\nvised directions. This is potentially due to Qwen’s\nnon-English-centric pretraining combined with our\nEnglish-centric alignment data. With this excep-\ntion, our approach maintains or improves super-\nvised performance while enhancing transfer.\nAligned languages improve the most, but gains\nextend to other languages: The diverse language\ncoverage in the slot filling dataset allows us to com-\n\npare how the alignment objective benefits trans-\nfer to both aligned and non-aligned languages.\nWhile aligned languages show the strongest im-\nprovements (F1 +4.2 and +4.9 for Llama and\nQwen respectively), the benefits extend to other\nlanguages. Over the remaining 10 non-aligned lan-\nguages, there is an average F1 improvement of 0.4\n(per-language results in Appendix E). This suggest\nthat the alignment step enhances the model’s gen-\neral cross-lingual transfer capabilities rather than\noptimizing for specific language pairs.\nSmaller gains on non-Latin script languages:\nBeyond overall performance improvements, we ob-\nserve smaller gains on typologically diverse lan-\nguages. Specifically, for the non-Latin script trans-\nfer languages in the slot filling task (Greek, Hindi,\nJapanese, Thai, Urdu), the average improvement\nis only 0.5 F1 in contrast to the overall average\ngain of 1.5. This reduced gain is likely related\nto suboptimal tokenization for these languages in\nmultilingual models (Rust et al., 2021; Petrov et al.,\n2023; Hong et al., 2024). When tokens poorly align\nwith linguistic units, the mean-pooled sentence rep-\nresentations may poorly capture semantics, thereby\nimpacting our alignment objective.\n5.2\nAlignment Loss Placement\nTo validate our choice of middle-layer alignment\nmotivated by the analysis in §2, we compare per-\nformance when applying the alignment loss at dif-\nferent network depths: bottom (8th), middle (16th),\nand top (32nd) layers of Llama.\nMiddle layer placement achieves more balanced\nimprovements in transfer languages: As shown\nin Table 3, compared to the \"middle\" configura-\ntion, the \"bottom\" configuration clearly leads to\npoor overall performance in both supervised and\ntransfer settings, with a particularly strong degra-\ndation on the slot filling task. While top-layer\nalignment maintains overall strong performance,\nit shows more unbalanced gains across transfer\nlanguages, as evidenced by the higher standard de-\nviation of performance gains on transfer languages.\nMiddle layer placement achieves better align-\nment across network depths: To better under-\nstand the effects of different loss placements, we\nrun the translation retrieval task over model acti-\nvations at from different intermediate layers. As\nshown in Figure 4, When the alignment loss is\napplied at the middle (16th) layer, semantic align-\nment is enhanced not only at that layer but also in\nmultiple preceding layers. In contrast, top-layer\nSupervised↑Transfer↑Transfer SD↓\nSlot filling (MASSIVE): F1\nMiddle (layer 16)\n77.0\n61.7\n2.6\nTop (layer 32)\n76.6\n62.0\n3.3\nBottom (layer 8)\n76.8\n58.0\n2.9\nMachine translation (WMT23): COMET\nMiddle (layer 16)\n81.5\n80.7\n3.7\nTop (layer 32)\n82.0\n80.2\n4.2\nBottom (layer 8)\n81.2\n80.1\n5.6\nTable 3: Impact of alignment loss placement on super-\nvised and transfer performance. \"Top\" leads to more\nuneven gains across languages, while \"bottom\" degrades\nboth supervised and transfer performance.\n0\n4\n8\n12\n16\n20\n24\n28\n32\nLayer ID\n0\n25\n50\n75\n100\nAvg. retrieval\naccuracy (%)\nStandard FT\nAlign @ layer 8\nAlign @ layer 16\nAlign @ layer 32\nFigure 4: Retrieval accuracy over model depths when\nadding alignment loss on different layers. Middle layer\nplacement (layer 16) results in overall better alignment.\nalignment primarily affects only the final layer,\nand bottom-layer alignment shows limited improve-\nment in alignment quality across all layers. This\nis likely because the lower layers are occupied\nwith processing more fundamental text features\n(Belinkov et al., 2017; Peters et al., 2018) rather\nthan abstract semantic meanings.\n5.3\nImpact on Representation Retrieval\nTo assess the impact of the alignment loss on the\nlearned model representations, we also report the\nretrieval accuracy for all languages involved in each\ntask (20 for slot filling and 9 for machine transla-\ntion) after fine-tuning in Table 2. For Llama on\nthe slot filling task, the alignment loss substantially\nimproves retrieval accuracy from 39.4% to 73.2%.\nFor Qwen, the alignment loss does not improve re-\ntrieval among the 20 slot filling languages, possibly\ndue to the lower accuracy of the base model with\nmany low-resource languages with 0% accuracy,\nmaking improvement more challenging. For ma-\nchine translation, as noted earlier §4.2, the retrieval\ntest data overlaps with part of the task training data,\npotentially inflating accuracy (marked in brackets\nin Table 2). However, we verify that this overlap\ndoes not lead to perfect retrieval accuracy: Specif-\nically, at the 16th layer where the alignment loss\nis applied, English-source retrieval accuracies for\nsupervised languages show varying accuracy: cs\n\nResource\nSuper. Transfer Gain on Aligned\n(5 lang.) (15 lang.)\n(4/5 lang.)\nSFT (row (2) Table 2)\n76.6\n60.2\n–\nLow (row (3) Table 2)\n77.0\n61.7\n+3.8\nMedium\n77.8\n61.4\n+1.1\nHigh\n77.6\n60.4\n+0.7\nTable 4: Ablation of using alignment languages from\ndifferent resource levels on slot filling with Llama.\n(98.1%), de (96.5%), is (66.9%), ru (90.6%), and\nzh (94.8%). This suggests that the overlap does not\nmake the retrieval diagnostic task trivial.\n6\nAnalyses\n6.1\nResource Level of Alignment Languages\nIn our main experiments, we selected the 5 lan-\nguages with the weakest performance from the\nMASSIVE baseline (FitzGerald et al., 2023) for\nalignment.\nWe now vary the resource level of\nthe alignment languages using a medium-resource\ngroup with {el, hi, th, tr}−en and a high-resource\ngroup with {ar, es, ru, zh}−en, which also have\nsupervised task training data. As shown in Table 4,\nall three configurations improve F1 scores for the\nlanguages involved in alignment. However, the low-\nresource group exhibit the largest gains (+3.8 F1),\nindicating that our approach is most beneficial to\nlanguages with weaker initial performance. More-\nover, overall transfer gains relative to the SFT base-\nline diminish when using high-resource languages\nfor alignment, likely because these languages al-\nready have well-aligned representations and align-\ning them provides little benefit to lower-resource\nlanguages in the transfer set. Overall, the results\nshow that our approach is robust to the choice of\nalignment languages, but selecting initially poorly\naligned languages could provide broader benefits\nacross different languages.\n6.2\nGeneralization of Learned Alignment\nTable 5 examines the language and domain gener-\nalization of our alignment component. To isolate\nthe effects of task-specific joint training, we train\nthe models using only the alignment loss, follow-\ning the same setup as our previous experiments but\nwithout optimizing on task-specific data. We then\nevaluate retrieval accuracy as described in §4.2.\nLanguage Generalization: While our main ex-\nperiments align multiple language pairs, we now\nuse single languages for alignment. As shown\nin Table 5 (upper portion), that single-language\nAlignment Data\nOverall (20 lang.)\nMulti {ar,es,ru,zh,sw}-en\n80.2\nOnly de-en\n71.9\nOnly es-en\n72.9\nOnly zh-en\n72.7\nde-en FLoRes (oracle)\n77.7\nTatoeba (different)\n71.9\nIWSLT (very distant)\n68.5\nTable 5: Retrieval accuracy when alignment data come\nfrom different languages and domains on Llama.\nResource\nSupervised\nTransfer\nSlot filling (MASSIVE): F1\nSFT (row (2) Table 2)\n76.6\n60.2\nJoint (row (3) Table 2)\n77.0 (+0.4)\n61.7 (+1.5)\nMerge\n76.9 (+0.3)\n61.3 (+1.1)\nMachine translation (WMT23): COMET\nSFT (row (4) Table 2)\n81.5\n79.6\nJoint (row (5) Table 2)\n81.5 (+0.0)\n80.7 (+1.1)\nMerge\n82.0 (+0.5)\n80.2 (+0.6)\nTable 6: Result of merging separately-trained task and\nalignment modules on Llama.\nalignment training leads to diminished performance\ncompared to multilingual training. Interestingly,\nwe see comparable accuracy drops regardless of\nwhich individual language is used for alignment,\nsuggesting that the gains of multilingual alignment\ncome from the diversity of the training data rather\nthan characteristics of individual languages.\nDomain Generalization: To isolate the effects of\nmultilinguality, we focus on alignment between a\nsingle language pair (English-German). In Table 5\n(lower portion), we first establish an oracle setup\nusing models trained on FLoRes data (Wikipedia\ndomain, overlapping with retrieval data). We then\ncompare to two setups where the alignment data\ncome from other domains: Tatoeba (short sen-\ntences for language learning; different) and IWSLT\n2017 (public speaking transcriptions; very distant).\nWhile we observe a decrease in retrieval accuracy\ncompared to the oracle setup, the results suggest\nthat, to enforce alignment into the model, it is not\nstrictly necessary to source alignment data from\nthe same domain as the task-specific data.\n6.3\nMerging Alignment and Task Modules\nOur previous experiments focused on models\njointly trained on both task and alignment objec-\ntives. However, in practice, it may be necessary to\nenhance existing task-specific models with cross-\nlingual capabilities, where joint re-training is infea-\n\nSupervised\nTransfer\nTransfer\n(en, pt, zh)\n(da, sv)\n(5 lang.)\nLlama SFT\n83.4\n82.1\n79.3\n+ alignment\n82.4\n83.1\n79.8\nTable 7: Results on JSON generation evaluated with F1\nafter parsing the output.\nsible due to computational constraints or unavail-\nability of the original task training data. Inspired\nby recent advances in model merging (Matena and\nRaffel, 2022; Ilharco et al., 2023), we explore the\nfeasibility of combining separately-trained task and\nalignment modules. We merge two sets of trained\nLoRA adapters by averaging their weights12: the\nalignment module trained in isolation (§6.2), and\ntask-specific modules (rows (2) and (5) in Table 2).\nTable 6 shows that this post-hoc merging brings\ncomparable improvements comparable to joint\ntraining. Moreover, the improvements are more\nevenly distributed across languages compared to\nthe larger gains observed on languages used di-\nrectly in alignment. These results demonstrate that\nour alignment approach is modular and can be com-\nbined with existing task-specific models.\n6.4\nLong Sequence Processing\nWe investigate a more challenge task requiring\nlonger input and output generation using UNER\n(§4.1). As shown in Table 7, while aligned lan-\nguages still show improvements, the gains are more\nmodest compared to previous experiments, with an\nF1 increase of 1.0 on aligned languages and 0.5\nacross all transfer languages. Moreover, there is\nan average degradation of 1.0 F1 on supervised\nlanguages, mainly due to the decline in Chinese\n(−2.2 F1). A potential reason is the mismatch be-\ntween our sentence-level alignment objective and\nthe requirements of processing longer sequences.\n7\nRelated Works\nMultilingual Capabilities of LLMs: LLM per-\nformance varies across languages due to imbal-\nanced pre-training data volume. However, even pre-\ndominantly English-centric models (Touvron et al.,\n2023) exhibit some degree of multilingual capabil-\nity (Aycock and Bawden, 2024; Yuan et al., 2024),\npotentially due to the unintentional ingestion of\nmultilingual data during pretraining (Briakou et al.,\n12We use a weighted average tuned on the development set\n(details in Appendix D.3)\n2023). Meanwhile, many recent LLMs have ex-\npanded their language coverage (AI @ Meta et al.,\n2024; Qwen Team et al., 2025). Despite these in-\nherent multilingual capabilities, extending them to\ndownstream tasks in low-resource settings (Adelani\net al., 2024; Iyer et al., 2024) remains challenging.\nMultilingual Representation Alignment: En-\nhancing meaningful cross-lingual relationships be-\ntween model representations has been a well-\nstudied area in the context of many tasks, including\nintermediate tasks such as bilingual lexicon induc-\ntion (Zhang et al., 2017) and sentence embeddings\n(Feng et al., 2022; Li et al., 2023), as well as more\ndirect applications like information retrieval (Izac-\nard et al., 2022) and translation (Pham et al., 2019;\nPan et al., 2021). In the context of LLMs, Wang\net al. (2024b) use linear projections learned offline\nto align non-English representations with English\nones during decoding. Our work differs in that\nour alignment objective is parameterized by the\nsame weights as task-specific fine-tuning, and is\ndirectly applicable to multilingual fine-tuning. Wu\net al. (2024a) align LLM top-layer representations\nspecifically for the task of semantic textual simi-\nlarity (STS). Different from this work, they do not\nconsider cross-lingual transfer in downstream tasks\nor explore intermediate LLM layers for alignment.\nLLM Representation Analysis: Several recent\nworks have analyzed LLM internal representations\nwith geometric analysis of representation spaces\n(Razzhigaev et al., 2024; Lee et al., 2024), probing\nclassifiers (Wang et al., 2024a; Li et al., 2025), or\nlogit lens analysis (Wu et al., 2024b). In particular,\nWu et al. (2024b) identify “semantic hubs” in LLM\nmiddle layers, which integrate information from\nvarious data types. Our findings are orthogonal to\ntheir work on multi-modality.\n8\nConclusion\nWe presented a simple yet effective approach for\nenhancing cross-lingual transfer in LLMs through\nmiddle-layer representation alignment during fine-\ntuning. Our experimental results lead to several\npractical recommendations: 1) Aligning a few\nweakly-performing languages yields broad transfer\nbenefits. A few hundreds of parallel sentences as\nalignment data are sufficient. 2) Alignment data\ncan be sourced from different domains as the task.\n3) Existing task-specific models can be enhanced\nwith our approach via parameter merging without\nthe need of full re-training.\n\nLimitations\nTypologically diverse languages:\nAs discussed\nin §5.1, our approach shows smaller gains on lan-\nguages non-Latin scripts. This limitation is likely\nrelated to fundamental tokenization challenges,\nwhere suboptimal token segmentation negatively\nimpacts the quality of mean-pooled representations.\nWhile our initial experiments on attention pooling\ndid not lead to improvements, exploring more so-\nphisticated pooling mechanisms could potentially\naddress this challenge in future work.\nComputational overhead during training:\nThe\nalternating optimization between task and align-\nment objectives doubles the computational cost\nduring training compared to standard fine-tuning.\nIn computationally constrained settings, our merg-\ning approach, which separates task-specific and\nalignment training, should be prioritized. Given\nthat alignment can be effectively performed using\nonly a small number of parallel sentences (a few\nhundred per language), this modular approach can\nsignificantly reduce the overall computational cost.\nTrade-offs between supervised and transfer per-\nformance in challenging scenarios:\nWhile our\napproach generally maintains or improves super-\nvised task performance while improving transfer,\nwe observe degradation in supervised performance\nin two specific scenarios. First, in structured text\ngeneration (§6.4), the method shows reduced ef-\nfectiveness and can impair supervised performance\n(−1.0 F1), suggesting that our sentence-level align-\nment may interfere with the processing of longer,\nstructured sequences. Second, when applying the\nmethod to models with weak initial cross-lingual\nalignment (§5.1), there could be a trade-off be-\ntween improved transfer and supervised perfor-\nmance.\nReferences\nDavid Ifeoluwa Adelani, A. Seza Do˘gruöz, André\nConeglian, and Atul Kr. Ojha. 2024. Comparing\nLLM prompting with cross-lingual transfer perfor-\nmance on indigenous and low-resource Brazilian lan-\nguages. In Proceedings of the 4th Workshop on Nat-\nural Language Processing for Indigenous Languages\nof the Americas (AmericasNLP 2024), pages 34–41,\nMexico City, Mexico. Association for Computational\nLinguistics.\nAI @ Meta, Aaron Grattafiori, Abhimanyu Dubey, Ab-\nhinav Jauhri, Abhinav Pandey, Abhishek Kadian,\nAhmad Al-Dahle, Aiesha Letman, Akhil Mathur,\nAlan Schelten, Alex Vaughan, Amy Yang, Angela\nFan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang,\nArchi Mitra, Archie Sravankumar, Artem Korenev,\nand 543 others. 2024. The llama 3 herd of models.\nPreprint, arXiv:2407.21783.\nDuarte M. Alves, José Pombal, Nuno Miguel Guer-\nreiro, Pedro Henrique Martins, João Alves, M. Amin\nFarajian, Ben Peters, Ricardo Rei, Patrick Fernandes,\nSweta Agrawal, Pierre Colombo, José G. C. de Souza,\nand André F. T. Martins. 2024. Tower: An open mul-\ntilingual large language model for translation-related\ntasks. CoRR, abs/2402.17733.\nNaveen Arivazhagan, Ankur Bapna, Orhan Firat, Roee\nAharoni, Melvin Johnson, and Wolfgang Macherey.\n2019. The missing ingredient in zero-shot neural\nmachine translation. Preprint, arXiv:1903.07091.\nMikel Artetxe and Holger Schwenk. 2019a. Margin-\nbased parallel corpus mining with multilingual sen-\ntence embeddings. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 3197–3203, Florence, Italy. Asso-\nciation for Computational Linguistics.\nMikel Artetxe and Holger Schwenk. 2019b.\nMas-\nsively multilingual sentence embeddings for zero-\nshot cross-lingual transfer and beyond. Transactions\nof the Association for Computational Linguistics,\n7:597–610.\nSeth Aycock and Rachel Bawden. 2024. Topic-guided\nexample selection for domain adaptation in LLM-\nbased machine translation. In Proceedings of the\n18th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Student Re-\nsearch Workshop, pages 175–195, St. Julian’s, Malta.\nAssociation for Computational Linguistics.\nYonatan Belinkov, Nadir Durrani, Fahim Dalvi, Has-\nsan Sajjad, and James Glass. 2017. What do neural\nmachine translation models learn about morphology?\nIn Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 861–872, Vancouver, Canada.\nAssociation for Computational Linguistics.\nEleftheria Briakou, Colin Cherry, and George Foster.\n2023. Searching for needles in a haystack: On the\nrole of incidental bilingualism in PaLM‘s translation\ncapability. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 9432–9452, Toronto,\nCanada. Association for Computational Linguistics.\nMauro Cettolo, Marcello Federico, Luisa Bentivogli,\nJan Niehues, Sebastian Stüker, Katsuhito Sudoh,\nKoichiro Yoshino, and Christian Federmann. 2017.\nOverview of the IWSLT 2017 evaluation campaign.\nIn Proceedings of the 14th International Conference\non Spoken Language Translation, pages 2–14, Tokyo,\nJapan. International Workshop on Spoken Language\nTranslation.\n\nChangyou Chen, Jianyi Zhang, Yi Xu, Liqun Chen,\nJiali Duan, Yiran Chen, Son Tran, Belinda Zeng,\nand Trishul Chilimbi. 2022. Why do we need large\nbatchsizes in contrastive learning? A gradient-bias\nperspective. In Advances in Neural Information Pro-\ncessing Systems 35: Annual Conference on Neural\nInformation Processing Systems 2022, NeurIPS 2022,\nNew Orleans, LA, USA, November 28 - December 9,\n2022.\nNadezhda Chirkova and Vassilina Nikoulina. 2024.\nKey ingredients for effective zero-shot cross-lingual\nknowledge transfer in generative tasks. In Proceed-\nings of the 2024 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies (Volume\n1: Long Papers), pages 7222–7238, Mexico City,\nMexico. Association for Computational Linguistics.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodku-\nmar Prabhakaran, and 48 others. 2023. Palm: Scaling\nlanguage modeling with pathways. J. Mach. Learn.\nRes., 24:240:1–240:113.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Advances\nin Neural Information Processing Systems 32: An-\nnual Conference on Neural Information Processing\nSystems 2019, NeurIPS 2019, December 8-14, 2019,\nVancouver, BC, Canada, pages 7057–7067.\nPaul-Ambroise Duquenne, Hongyu Gong, Benoît Sagot,\nand Holger Schwenk. 2022. T-modules: Translation\nmodules for zero-shot cross-modal machine trans-\nlation. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 5794–5806, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nPaul-Ambroise Duquenne, Holger Schwenk, and Benoît\nSagot. 2023. SONAR: sentence-level multimodal\nand language-agnostic representations.\nCoRR,\nabs/2308.11466.\nFangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Ari-\nvazhagan, and Wei Wang. 2022. Language-agnostic\nBERT sentence embedding. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n878–891, Dublin, Ireland. Association for Computa-\ntional Linguistics.\nJack FitzGerald, Christopher Hench, Charith Peris,\nScott Mackie, Kay Rottmann, Ana Sanchez, Aaron\nNash, Liam Urbach, Vishesh Kakarala, Richa Singh,\nSwetha Ranganath, Laurie Crist, Misha Britan,\nWouter Leeuwis, Gokhan Tur, and Prem Natara-\njan. 2023.\nMASSIVE: A 1M-example multilin-\ngual natural language understanding dataset with\n51 typologically-diverse languages. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 4277–4302, Toronto, Canada. Association for\nComputational Linguistics.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimCSE: Simple contrastive learning of sentence em-\nbeddings. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 6894–6910, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nRohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Man-\nnat Singh, Kalyan Vasudev Alwala, Armand Joulin,\nand Ishan Misra. 2023. Imagebind one embedding\nspace to bind them all. In IEEE/CVF Conference\non Computer Vision and Pattern Recognition, CVPR\n2023, Vancouver, BC, Canada, June 17-24, 2023,\npages 15180–15190. IEEE.\nMilan Gritta and Ignacio Iacobacci. 2021. XeroAlign:\nZero-shot cross-lingual transformer alignment. In\nFindings of the Association for Computational Lin-\nguistics: ACL-IJCNLP 2021, pages 371–381, Online.\nAssociation for Computational Linguistics.\nThanh-Le Ha, Jan Niehues, and Alexander Waibel.\n2016. Toward multilingual neural machine trans-\nlation with universal encoder and decoder. Preprint,\narXiv:1611.04798.\nKatharina Hämmerl, Jindˇrich Libovický, and Alexan-\nder Fraser. 2024.\nUnderstanding cross-lingual\nAlignment—A survey. In Findings of the Associa-\ntion for Computational Linguistics: ACL 2024, pages\n10922–10943, Bangkok, Thailand. Association for\nComputational Linguistics.\nMutian He and Philip N. Garner. 2023. Can chatgpt\ndetect intent? evaluating large language models for\nspoken language understanding. In 24th Annual Con-\nference of the International Speech Communication\nAssociation, Interspeech 2023, Dublin, Ireland, Au-\ngust 20-24, 2023, pages 1109–1113. ISCA.\nKevin Heffernan, Onur Çelebi, and Holger Schwenk.\n2022. Bitext mining using distilled sentence repre-\nsentations for low-resource languages. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2022, pages 2101–2112, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nJimin Hong, Gibbeum Lee, and Jaewoong Cho. 2024.\nAccelerating multilingual language model for exces-\nsively tokenized languages. In Findings of the As-\nsociation for Computational Linguistics: ACL 2024,\n\npages 11095–11111, Bangkok, Thailand. Association\nfor Computational Linguistics.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2022. Lora: Low-rank adaptation of\nlarge language models. In The Tenth International\nConference on Learning Representations, ICLR 2022,\nVirtual Event, April 25-29, 2022. OpenReview.net.\nJunjie Hu, Melvin Johnson, Orhan Firat, Aditya Sid-\ndhant, and Graham Neubig. 2021. Explicit alignment\nobjectives for multilingual bidirectional encoders. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 3633–3643, Online. Association for Computa-\ntional Linguistics.\nGabriel Ilharco, Marco Túlio Ribeiro, Mitchell Worts-\nman, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali\nFarhadi. 2023. Editing models with task arithmetic.\nIn The Eleventh International Conference on Learn-\ning Representations, ICLR 2023, Kigali, Rwanda,\nMay 1-5, 2023. OpenReview.net.\nVivek Iyer, Bhavitvya Malik, Wenhao Zhu, Pavel\nStepachev, Pinzhen Chen, Barry Haddow, and\nAlexandra Birch. 2024. Exploring very low-resource\ntranslation with LLMs: The University of Edin-\nburgh‘s submission to AmericasNLP 2024 translation\ntask. In Proceedings of the 4th Workshop on Natural\nLanguage Processing for Indigenous Languages of\nthe Americas (AmericasNLP 2024), pages 209–220,\nMexico City, Mexico. Association for Computational\nLinguistics.\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Se-\nbastian Riedel, Piotr Bojanowski, Armand Joulin,\nand Edouard Grave. 2022. Unsupervised dense in-\nformation retrieval with contrastive learning. Trans.\nMach. Learn. Res., 2022.\nJoão Maria Janeiro, Benjamin Piwowarski, Patrick Gal-\nlinari, and Loïc Barrault. 2024.\nMexma: Token-\nlevel objectives improve sentence representations.\nPreprint, arXiv:2409.12737.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781,\nOnline. Association for Computational Linguistics.\nTom Kocmi, Eleftherios Avramidis, Rachel Bawden,\nOndˇrej Bojar, Anton Dvorkovich, Christian Fed-\nermann, Mark Fishel, Markus Freitag, Thamme\nGowda, Roman Grundkiewicz, Barry Haddow,\nPhilipp Koehn, Benjamin Marie, Christof Monz,\nMakoto Morishita, Kenton Murray, Makoto Nagata,\nToshiaki Nakazawa, Martin Popel, and 2 others. 2023.\nFindings of the 2023 conference on machine transla-\ntion (WMT23): LLMs are here but not quite there yet.\nIn Proceedings of the Eighth Conference on Machine\nTranslation, pages 1–42, Singapore. Association for\nComputational Linguistics.\nGuillaume Lample, Alexis Conneau, Marc’Aurelio Ran-\nzato, Ludovic Denoyer, and Hervé Jégou. 2018.\nWord translation without parallel data. In 6th In-\nternational Conference on Learning Representations,\nICLR 2018, Vancouver, BC, Canada, April 30 - May\n3, 2018, Conference Track Proceedings. OpenRe-\nview.net.\nHyunji Lee, Danni Liu, Supriti Sinhamahapatra, and\nJan Niehues. 2024. How do multimodal foundation\nmodels encode text and speech? an analysis of cross-\nlingual and cross-modal representations. Preprint,\narXiv:2411.17666.\nDaoyang Li, Haiyan Zhao, Qingcheng Zeng, and Meng-\nnan Du. 2025. Exploring multilingual probing in\nlarge language models: A cross-language analysis.\nPreprint, arXiv:2409.14459.\nTianjian Li and Kenton Murray. 2023. Why does zero-\nshot cross-lingual generation fail? an explanation and\na solution. In Findings of the Association for Compu-\ntational Linguistics: ACL 2023, pages 12461–12476,\nToronto, Canada. Association for Computational Lin-\nguistics.\nZiheng Li, Shaohan Huang, Zihan Zhang, Zhi-Hong\nDeng, Qiang Lou, Haizhen Huang, Jian Jiao, Furu\nWei, Weiwei Deng, and Qi Zhang. 2023.\nDual-\nalignment pre-training for cross-lingual sentence em-\nbedding. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 3466–3478, Toronto,\nCanada. Association for Computational Linguistics.\nMichael Matena and Colin Raffel. 2022. Merging mod-\nels with fisher-weighted averaging. In Advances in\nNeural Information Processing Systems 35: Annual\nConference on Neural Information Processing Sys-\ntems 2022, NeurIPS 2022, New Orleans, LA, USA,\nNovember 28 - December 9, 2022.\nStephen Mayhew, Terra Blevins, Shuheng Liu, Marek\nSuppa, Hila Gonen, Joseph Marvin Imperial, Börje\nKarlsson, Peiqin Lin, Nikola Ljubeši´c, Lester James\nMiranda, Barbara Plank, Arij Riabi, and Yuval Pinter.\n2024. Universal NER: A gold-standard multilingual\nnamed entity recognition benchmark. In Proceed-\nings of the 2024 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies (Volume\n1: Long Papers), pages 4322–4337, Mexico City,\nMexico. Association for Computational Linguistics.\nTomás Mikolov, Quoc V. Le, and Ilya Sutskever. 2013.\nExploiting similarities among languages for machine\ntranslation. CoRR, abs/1309.4168.\nBenjamin Muller, Yanai Elazar, Benoît Sagot, and\nDjamé Seddah. 2021. First align, then predict: Un-\nderstanding the cross-lingual ability of multilingual\nBERT. In Proceedings of the 16th Conference of the\n\nEuropean Chapter of the Association for Computa-\ntional Linguistics: Main Volume, pages 2214–2231,\nOnline. Association for Computational Linguistics.\nNLLB Team. 2024. Scaling neural machine translation\nto 200 languages. Nat., 630(8018):841–846.\nXiao Pan, Mingxuan Wang, Liwei Wu, and Lei Li. 2021.\nContrastive learning for many-to-many multilingual\nneural machine translation. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 244–258, Online. Asso-\nciation for Computational Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 2227–2237,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nAleksandar Petrov, Emanuele La Malfa, Philip H. S.\nTorr, and Adel Bibi. 2023. Language model tok-\nenizers introduce unfairness between languages. In\nAdvances in Neural Information Processing Systems\n36: Annual Conference on Neural Information Pro-\ncessing Systems 2023, NeurIPS 2023, New Orleans,\nLA, USA, December 10 - 16, 2023.\nNgoc-Quan Pham, Jan Niehues, Thanh-Le Ha, and\nAlexander Waibel. 2019. Improving zero-shot trans-\nlation with language-independent constraints. In Pro-\nceedings of the Fourth Conference on Machine Trans-\nlation (Volume 1: Research Papers), pages 13–23,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT? In Proceed-\nings of the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 4996–5001, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186–\n191, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nQwen Team, An Yang, Baosong Yang, Beichen Zhang,\nBinyuan Hui, Bo Zheng, Bowen Yu, Chengyuan\nLi, Dayiheng Liu, Fei Huang, Haoran Wei, Huan\nLin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin\nYang, Jiaxi Yang, Jingren Zhou, Junyang Lin, and\n24 others. 2025. Qwen2.5 technical report. Preprint,\narXiv:2412.15115.\nMaithra Raghu, Justin Gilmer, Jason Yosinski, and\nJascha Sohl-Dickstein. 2017. SVCCA: singular vec-\ntor canonical correlation analysis for deep learning\ndynamics and interpretability. In Advances in Neural\nInformation Processing Systems 30: Annual Confer-\nence on Neural Information Processing Systems 2017,\nDecember 4-9, 2017, Long Beach, CA, USA, pages\n6076–6085.\nAnton Razzhigaev, Matvey Mikhalchuk, Elizaveta Gon-\ncharova, Ivan Oseledets, Denis Dimitrov, and Andrey\nKuznetsov. 2024. The shape of learning: Anisotropy\nand intrinsic dimensions in transformer-based mod-\nels. In Findings of the Association for Computational\nLinguistics: EACL 2024, pages 868–874, St. Julian’s,\nMalta. Association for Computational Linguistics.\nRicardo Rei, José G. C. de Souza, Duarte Alves,\nChrysoula Zerva, Ana C Farinha, Taisiya Glushkova,\nAlon Lavie, Luisa Coheur, and André F. T. Martins.\n2022. COMET-22: Unbabel-IST 2022 submission\nfor the metrics shared task. In Proceedings of the\nSeventh Conference on Machine Translation (WMT),\npages 578–585, Abu Dhabi, United Arab Emirates\n(Hybrid). Association for Computational Linguistics.\nSebastian\nRuder,\nMatthew\nE.\nPeters,\nSwabha\nSwayamdipta, and Thomas Wolf. 2019. Transfer\nlearning in natural language processing. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational\nLinguistics:\nTutorials, pages 15–18, Minneapo-\nlis,\nMinnesota. Association for Computational\nLinguistics.\nPhillip Rust, Jonas Pfeiffer, Ivan Vuli´c, Sebastian Ruder,\nand Iryna Gurevych. 2021. How good is your tok-\nenizer? on the monolingual performance of multilin-\ngual language models. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 3118–3135, Online. Association\nfor Computational Linguistics.\nTal Schuster, Ori Ram, Regina Barzilay, and Amir\nGloberson. 2019. Cross-lingual alignment of con-\ntextual word embeddings, with applications to zero-\nshot dependency parsing. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 1599–1613, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nHolger Schwenk, Vishrav Chaudhary, Shuo Sun,\nHongyu Gong, and Francisco Guzmán. 2021. Wiki-\nMatrix: Mining 135M parallel sentences in 1620 lan-\nguage pairs from Wikipedia. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\n\npages 1351–1361, Online. Association for Computa-\ntional Linguistics.\nJunhong Shen, Neil A. Tenenholtz, James Brian Hall,\nDavid Alvarez-Melis, and Nicolò Fusi. 2024. Tag-\nllm: Repurposing general-purpose llms for special-\nized domains. In Forty-first International Conference\non Machine Learning, ICML 2024, Vienna, Austria,\nJuly 21-27, 2024. OpenReview.net.\nShivalika Singh, Freddie Vargus, Daniel D’souza,\nBörje Karlsson, Abinaya Mahendiran, Wei-Yin Ko,\nHerumb Shandilya, Jay Patel, Deividas Mataciu-\nnas, Laura O’Mahony, Mike Zhang, Ramith Het-\ntiarachchi, Joseph Wilson, Marina Machado, Luisa\nMoura, Dominik Krzemi´nski, Hakimeh Fadaei, Irem\nErgun, Ifeoma Okoh, and 14 others. 2024.\nAya\ndataset: An open-access collection for multilingual\ninstruction tuning. In Proceedings of the 62nd An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 11521–\n11567, Bangkok, Thailand. Association for Compu-\ntational Linguistics.\nJörg Tiedemann. 2020. The tatoeba translation chal-\nlenge – realistic data sets for low resource and multi-\nlingual MT. In Proceedings of the Fifth Conference\non Machine Translation, pages 1174–1182, Online.\nAssociation for Computational Linguistics.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, and 49 oth-\ners. 2023. Llama 2: Open foundation and fine-tuned\nchat models. CoRR, abs/2307.09288.\nTu Vu, Aditya Barua, Brian Lester, Daniel Cer, Mo-\nhit Iyyer, and Noah Constant. 2022. Overcoming\ncatastrophic forgetting in zero-shot cross-lingual gen-\neration. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 9279–9300, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nDong Wang and Thomas Fang Zheng. 2015. Trans-\nfer learning for speech and language processing. In\nAsia-Pacific Signal and Information Processing As-\nsociation Annual Summit and Conference, APSIPA\n2015, Hong Kong, December 16-19, 2015, pages\n1225–1237. IEEE.\nHetong Wang, Pasquale Minervini, and Edoardo Ponti.\n2024a. Probing the emergence of cross-lingual align-\nment during LLM training. In Findings of the As-\nsociation for Computational Linguistics: ACL 2024,\npages 12159–12173, Bangkok, Thailand. Association\nfor Computational Linguistics.\nWeixuan Wang, Minghao Wu, Barry Haddow, and\nAlexandra Birch. 2024b. Bridging the language gaps\nin large language models with inference-time cross-\nlingual intervention. Preprint, arXiv:2410.12462.\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V. Le. 2022.\nFinetuned\nlanguage models are zero-shot learners. In The Tenth\nInternational Conference on Learning Representa-\ntions, ICLR 2022, Virtual Event, April 25-29, 2022.\nOpenReview.net.\nDi Wu, Yibin Lei, Andrew Yates, and Christof Monz.\n2024a. Representational isomorphism and alignment\nof multilingual large language models. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2024, pages 14074–14085, Miami, Florida,\nUSA. Association for Computational Linguistics.\nZhaofeng Wu, Xinyan Velocity Yu, Dani Yogatama, Ji-\nasen Lu, and Yoon Kim. 2024b. The semantic hub\nhypothesis: Language models share semantic repre-\nsentations across languages and modalities. Preprint,\narXiv:2411.04986.\nHaoran Xu, Young Jin Kim, Amr Sharaf, and Hany Has-\nsan Awadalla. 2024. A paradigm shift in machine\ntranslation: Boosting translation performance of\nlarge language models. In The Twelfth International\nConference on Learning Representations, ICLR 2024,\nVienna, Austria, May 7-11, 2024. OpenReview.net.\nHaoran Xu and Philipp Koehn. 2021. Cross-lingual\nbert contextual embedding space mapping with\nisotropic and isometric conditions.\nPreprint,\narXiv:2107.09186.\nRong Ye, Mingxuan Wang, and Lei Li. 2022. Cross-\nmodal contrastive learning for speech translation. In\nProceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5099–5113, Seattle, United States. Association\nfor Computational Linguistics.\nKatherine Yu, Haoran Li, and Barlas Oguz. 2018. Multi-\nlingual seq2seq training with similarity loss for cross-\nlingual document classification. In Proceedings of\nthe Third Workshop on Representation Learning for\nNLP, pages 175–179, Melbourne, Australia. Associa-\ntion for Computational Linguistics.\nFei Yuan, Shuai Yuan, Zhiyong Wu, and Lei Li. 2024.\nHow vocabulary sharing facilitates multilingualism in\nLLaMA? In Findings of the Association for Compu-\ntational Linguistics: ACL 2024, pages 12111–12130,\nBangkok, Thailand. Association for Computational\nLinguistics.\nMeng Zhang, Yang Liu, Huanbo Luan, and Maosong\nSun. 2017. Adversarial training for unsupervised\nbilingual lexicon induction. In Proceedings of the\n55th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers),\npages 1959–1970, Vancouver, Canada. Association\nfor Computational Linguistics.\n\nar\nen\nes\nru\nzh\ncy\nja\njv\nsw\ntl\naf\naz\nde\nel\nfr\nhi\nis\nth\ntr\nur\nEnglish-only 59.8 82.5 82.4 65.8 61.6 60.3 39.7 37.8 39.8 57.5 60.3 39.6 71.1 64.8 68.2 62.1 39.2 75.3 52.9 49.9\nMultilingual 75.5 81.7 74.5 77.6 73.8 44.0 65.8 41.0 42.8 65.0 66.0 49.0 75.0 69.4 71.9 70.0 45.0 79.9 60.4 57.1\nTable 8: Per-languages F1 results on slot filling of English-only finetuning compared to multilingual fine-tuning on\n{ar, en, es, ru, zh}. Multilingual fine-tuning shows stronger transfer performance.\nCode\nFLoRes Code\nFull Name\nSlot Filling\nMachine Translation\nJSON Generation\naf\nafr_Latn\nAfrikaans\n✓\naz\nazj_Latn\nNorth Azerbaijani\n✓\nar\narb_Arab\nModern Standard Arabic\n✓\ncs\nces_Latn\nCzech\n✓\ncy\ncym_Latn\nWelsh\n✓\nda\ndan_Latn\nDanish\n✓\nde\ndeu_Latn\nGerman\n✓\n✓\nel\nell_Grek\nGreek\n✓\nen\neng_Latn\nEnglish\n✓\n✓\n✓\nes\nspa_Latn\nSpanish\n✓\nfr\nfra_Latn\nFrench\n✓\nhe\nheb_Hebr\nHebrew\n✓\nhi\nhin_Deva\nHindi\n✓\nhr\nhrv_Latn\nCroatian\n✓\nis\nisl_Latn\nIcelandic\n✓\n✓\nja\njpn_Jpan\nJapanese\n✓\n✓\njv\njav_Latn\nJavanese\n✓\npt\npor_Latn\nPortuguese\n✓\nru\nrus_Cyrl\nRussian\n✓\n✓\nsk\nslk_Latn\nSlovak\n✓\nsr\nsrp_Cyrl\nSerbian\n✓\nsv\nswe_Latn\nSwedish\n✓\nsw\nswh_Latn\nSwahili\n✓\nth\ntha_Thai\nThai\n✓\ntl\ntgl_Latn\nTagalog\n✓\ntr\ntur_Latn\nTurkish\n✓\nuk\nukr_Cyrl\nUkrainian\n✓\nur\nurd_Arab\nUrdu\n✓\nzh\nzho_Hans\nChinese (Simplified)\n✓\n✓\n✓\nTable 9: List of languages evaluated on different downstream tasks.\nA\nEnglish-Only Fine-Tuning Results\nTable 8 compares English-only and multilin-\ngual fine-tuning on MASSIVE. Multilingual fine-\ntuning substantially outperforms English-only in\ncross-lingual transfer performance.\nB\nDataset Details\nAll our task training data are retrieved from Hug-\ngingFace13. The translation test sets are hosted\nby WMT14. The alignment data are sourced from\n13MASSIVE:\nhttps://huggingface.co/datasets/\nAmazonScience/massive\nALMA: https://huggingface.co/datasets/haoranxu/\nALMA-Human-Parallel\nUNER:\nhttps://huggingface.co/datasets/\nCohereForAI/aya_collection/viewer/templated_\nuner_llm\n14https://github.com/wmt-conference/\nwmt23-news-systems/tree/master/txt\nTatoeba15 with its default version of v2021-07-22\nat the time of writing.\nWe filter out transla-\ntions that are empty or include multiple sentences.\nThe lowest-resource alignment languages have a\nfew hundred parallel sentences: Javanese (264),\nSwahili (371), Welsh (823). The ablation de-en\nalignment data is from IWSLT 201716 (Cettolo\net al., 2017).\nC\nList of Languages\nThe languages involved in our downstream tasks\nare listed in Table 9. The 35 languages in the ini-\ntial analyses in §2 include all languages in slot fill\nand machine translation. They additionally include\nthe following languages: am (Amharic), bn (Ben-\n15https://huggingface.co/datasets/\nHelsinki-NLP/tatoeba\n16https://huggingface.co/datasets/IWSLT/\niwslt2017\n\ngali), it (Italian), hu (Hungrian), hy (Armenian),\nid (Indonesian), kn (Kannada), ka (Georgian ), mn\n(Mongolian), km (Khmer), ko (Korean), and lv\n(Latvian).\nD\nTraining and Inference Details\nD.1\nTraining Hyperparameters\nFine-tuning is performed using LoRA (Hu et al.,\n2022) adapters with a rank of 8 for all attention\ncomponents and linear projections (query, key,\nvalue, output, gate, up, down). We set LoRA’s\nα parameter to 16 and dropout to 0.1. The number\nof trainable parameter is 20,971,520 on Llama 3,\nand 20,185,088 on Qwen 2.5. We train at most\n5 epochs on the task data. Training on all our\ntasks converged before reaching the max number\nof epochs. The learning rate is set to 5e-4 with\ninverse square root schedule and warmup up ra-\ntio 0.03. We save checkpoints and evaluate every\n200 optimization steps, and early stop if the devel-\nopment loss does not improve for 5 consecutive\nevaluations. For the temperature parameter τ in\nthe contrastive loss, we searched among {0.1, 1.0,\n1.5, 2.0} based on development loss on machine\ntranslation. For Llama we 0.1, for Qwen we use\n1.5.\nD.2\nPrompt Format\nSlot Filling\nThe system prompt is shortened from\nHe and Garner (2023).\n• System: Given a command from the user, a voice\nassistant will extract entities essential for carry\nout the command. Your task is to extract the\nentities as words from the command if they fall\nunder a predefined list of entity types.\n• User: wake me up at five am this week\n• Assistant: time: five am; date: this week\n• User (de): wecke mich in dieser woche um fünf\nuhr auf\n• Assistant (de): date: dieser woche; time: fünf\nuhr\nFor zero-shot slot filling experiments, we need\nto specify more requirements in the system prompt\nwith the template also following He and Garner\n(2023):\nGiven a command from the user, a voice assis-\ntant like Siri or Olly will extract entities from the\ncommand that are essential for carry out the the\ncommand. For example, for a command about\nplaying a specific song, the name of the song men-\ntioned by the user would be an entity, falling under\nthe type of “song name”.\nYour task is to extract the entities as words from\nthe command if they fall under any of the types\ngiven below according to the following description:\ntransport_descriptor\nhouse_place\nmu-\nsic_album sport_type playlist_name movie_name\nsong_name place_name radio_name cooking_type\nweather_descriptor\nperson\nemail_folder\nbusi-\nness_type\naudiobook_author\ntransport_type\ngeneral_frequency meal_type game_name de-\nvice_type transport_name time_zone joke_type\ndrink_type email_address food_type date rela-\ntion currency_name ingredient player_setting\nmovie_type definition_word game_type list_name\nartist_name personal_info audiobook_name time-\nofday transport_agency media_type podcast_name\ncoffee_type business_name news_topic app_name\npodcast_descriptor\ncolor_type\nmusic_genre\nevent_name time change_amount alarm_type\norder_type music_descriptor\nPlease give answers like:\n1. person: john; contact_field: phone number\n2. transport_app: uber; time_of_day: tonight;\ntime: ten pm\n3. None\n4. music_genre: jazz\netc., each taking a single line. The entity type\nmust be one of the types given above, and the en-\ntity must be copied verbatim from the command.\nThere could be zero, one, or multiple entities in a\ncommand.\nMachine Translation\n• System: Translate the following sentences from\nEnglish to German.\n• User: Police arrest 15 after violent protest out-\nside UK refugee hotel.\n• Assistant: Polizei verhaftet 15 Menschen nach\ngewalttätigen Protesten vor einer Flüchtlingsun-\nterkunft in Großbritannien\nJSON Generation\n• User: Please identify all the named entities men-\ntioned in the input sentence provided below. Use\nonly the categories: PER - person, ORG - organi-\nzation, and LOC - location. Remember, national-\nities are neither locations nor organizations, and\norganizations can represent other groups of peo-\nple. Pay attention to the provided example. You\nshould only output the results in JSON format,\nfollowing a similar structure to the example result\nprovided. Example sentence and results: Where\nin the world is Iguazu? \"Results\": [ \"TypeName\":\n\nSupervised\nTransfer (aligned)\nTransfer (other)\nar\nen\nes\nru\nzh\ncy\nja\njv\nsw\ntl\naf\naz\nde\nel\nfr\nhi\nis\nth\ntr\nur\nLlama 3 SFT\n75.5 81.7 74.5 77.6 73.8 44.0 65.8 41.0 42.8 65.0 66.0 49.0 75.0 69.4 71.9 70.0 45.0 79.9 60.4 57.1\n+ align\n75.1 82.0 74.9 78.0 74.9 49.4 66.5 48.2 47.7 65.5 66.2 47.9 74.7 72.4 72.1 69.6 48.0 79.1 62.2 56.1\nQwen 2.5 SFT 74.7 81.1 74.0 77.5 74.1 27.0 67.3 32.9 23.5 57.4 58.9 45.9 74.6 63.3 70.8 60.0 34.4 79.9 59.9 46.5\n+ align\n74.9 82.5 74.8 78.0 75.1 36.5 68.3 39.6 30.4 57.8 63.1 42.5 74.6 63.3 70.9 61.3 35.8 80.2 58.1 47.2\nTable 10: Per-languages F1 results on slot filling.\nSupervised X→En\nSupervised En→X\nTransfer X→En Transfer En→X\ncs\nde\nis\nru\nzh\ncs\nde\nis\nru\nzh\nhe\nja\nuk\nhe\nja\nuk\nBLEU\nLlama 3 SFT\n37.8 43.0 28.3 32.0 22.5 25.9 35.5 10.6 25.2 38.9 39.3 17.5\n38.7\n14.5 14.2\n17.7\n+ align\n38.4 43.1 29.1 32.4 23.0 24.7 34.7 10.9 24.4 38.1 39.8 18.8\n38.4\n16.0 15.6\n19.5\nQwen 2.5 SFT 36.1 40.8 20.5 30.6 23.2 21.5 33.7 6.8 25.3 45.3 34.6 18.9\n35.6\n13.3 17.6\n13.0\n+ align\n36.6 41.4 21.2 30.9 24.0 20.5 32.7 4.8 25.0 45.3 36.3 19.4\n36.8\n12.7 17.8\n13.5\nCOMET\nLlama 3 SFT\n85.2 84.9 81.0 82.4 79.7 84.3 81.8 68.7 83.3 84.2 83.6 79.8\n85.1\n75.7 83.5\n79.7\n+ align\n85.5 84.9 81.1 82.4 79.8 83.8 81.6 69.0 83.3 84.0 83.6 80.1\n85.2\n77.1 84.2\n80.8\nQwen 2.5 SFT 84.8 84.7 74.1 82.6 80.2 80.8 80.6 52.0 83.3 86.1 82.3 81.3\n84.5\n70.7 85.5\n74.6\n+ align\n85.1 84.7 74.4 82.6 80.4 79.5 80.1 46.5 83.1 85.8 82.2 81.4\n84.6\n70.7 85.7\n74.4\nTable 11: Per-languages BLEU and COMET results on machine translation.\n\"LOC\", \"Text\": \"Iguazu\", \"Start\": 22, \"End\": 28\n] Considering the input sentence below, what is\nthe output result? Widely considered to be one\nof the most spectacular waterfalls in the world,\nthe Iguazu Falls on the border of Argentina and\nBrazil, are a certainly must see attraction in the\narea.\n• Assistant: \"Results\": [ \"TypeName\": \"LOC\",\n\"Text\": \"Iguazu Falls\", \"Start\": 81, \"End\": 93\n,\n\"TypeName\": \"LOC\", \"Text\": \"Argentina\",\n\"Start\": 111, \"End\": 120 , \"TypeName\": \"LOC\",\n\"Text\": \"Brazil\", \"Start\": 125, \"End\": 131 ]\nD.3\nInference Details\nWe use greedy decoding in all experiments for eas-\nily reproducible results. For the model merging\nexperiments, we searched among weights {0.5, 0.7,\n0.9} for the task-specific LoRA modules on the\nMASSIVE development set and chose 0.9 for our\nexperiments.\nD.4\nDetails for Retrieval\nTo evaluate cross-lingual retrieval performance,\nwe adapt the implementation from LASER17\n(Schwenk et al., 2021) to process representations\nextracted offline.\n17https://github.com/facebookresearch/LASER/\ntree/main/tasks/xsim\nE\nResults for Individual Languages\nThe detailed results for Table 2 are in Table 10 (slot\nfilling) and Table 11 (machine translation).\n",
  "metadata": {
    "source_path": "papers/arxiv/Middle-Layer_Representation_Alignment_for_Cross-Lingual_Transfer_in\n__Fine-Tuned_LLMs_8ea46f0e8a42e76e.pdf",
    "content_hash": "8ea46f0e8a42e76eb17a8e0b5ccd781e96998f5caf29e455ce280460cbb6d05b",
    "arxiv_id": null,
    "title": "Middle-Layer_Representation_Alignment_for_Cross-Lingual_Transfer_in\n__Fine-Tuned_LLMs_8ea46f0e8a42e76e",
    "author": "",
    "creation_date": "D:20250221020409Z",
    "published": "2025-02-21T02:04:09",
    "pages": 16,
    "size": 564753,
    "file_mtime": 1740346983.8338776
  }
}