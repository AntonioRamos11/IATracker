{
  "text": "A novel approach to navigate the taxonomic\nhierarchy to address the Open-World Scenarios in\nMedicinal Plant Classification\nSoumen Sinha∗Tanisha Rana† Rahul Roy‡\nAbstract\nIn this article, we propose a novel approach for plant hierarchical taxonomy\nclassification by posing the problem as an open class problem. It is observed that\nexisting methods for medicinal plant classification often fail to perform hierarchical\nclassification and accurately identifying unknown species, limiting their effective-\nness in comprehensive plant taxonomy classification. Thus we address the problem\nof unknown species classification by assigning it best hierarchical labels. We pro-\npose a novel method, which integrates DenseNet121, Multi-Scale Self-Attention\n(MSSA) and cascaded classifiers for hierarchical classification. The approach sys-\ntematically categorizes medicinal plants at multiple taxonomic levels, from phylum\nto species, ensuring detailed and precise classification. Using multi scale space at-\ntention, the model captures both local and global contextual information from the\nimages, improving the distinction between similar species and the identification of\nnew ones. It uses attention scores to focus on important features across multiple\nscales. The proposed method provides a solution for hierarchical classification,\nshowcasing superior performance in identifying both known and unknown species.\nThe model was tested on two state-of-art datasets with and without background\nartifacts and so that it can be deployed to tackle real word application. We used\nunknown species for testing our model. For unknown species the model achieved\nan average accuracy of 83.36%, 78.30%, 60.34% and 43.32% for predicting correct\nphylum, class, order and family respectively. Our proposed model size is almost\nfour times less than the existing state of the art methods making it easily deploy\nable in real world application.\nKey words. Medicinal Plant Classification; Cascaded Classifier; Multi Scale Self\nAttention\n∗Corresponding author: Department of Computer Science and Engineering, Mahindra University,\nBahadurpally, Hyderabad - 500043, India. (soumen20ucse179@mahindrauniversity.edu.in)\n†Department of Computer Science and Engineering, Mahindra University, Bahadurpally, Hyderabad\n- 500043, India. (tanisha20ucse202@mahindrauniversity.edu.in)\n‡Department of Computer Science and Engineering, Mahindra University, Bahadurpally, Hyderabad\n- 500043, India. (rahul.roy@mahindrauniversity.edu.in)\n1\narXiv:2502.17289v1  [cs.AI]  24 Feb 2025\n\n1\nIntroduction\nMedicinal plant classification in the fields of botany, agriculture, and pharmacology is\nessential for identifying and utilizing plants for therapeutic purposes. Accurate classi-\nfication allows researchers to systematically catalog and study plant species, which is\ncrucial for discovering new medicinal properties and developing herbal remedies. More-\nover, it helps in the conservation of biodiversity, ensuring that valuable plant species are\nprotected and sustainably used. Furthermore, understanding the taxonomy of medic-\ninal plants aids in tracing the evolutionary relationships providing insights into their\npotential health benefits.\nThis knowledge is vital for pharmacologists in formulating effective and safe herbal\nmedicines, contributing to the advancement of alternative and complementary therapies.\nThe recent advancements in deep learning have significantly improved the methods\nfor detecting and classifying plant diseases, thereby enhancing the precision and effi-\nciency of these tasks. Li et al. [20] reviewed various deep learning models for plant\ndisease detection, highlighting their potential to automate agricultural processes and\nincrease accuracy. Similarly, Chen et al. [4] provided a comprehensive review on plant\nimage recognition using deep learning, emphasizing the robustness of these models in\nhandling diverse plant datasets. Fitzgerald et al. [8] discussed the historical and regional\nadvancements in medicinal plant analysis, focusing on emergent complex techniques used\nin modern pharmacology. Diwedi et al. [7] proposed an classification system based on\noptimized support vector machine.\nTan et al.\n[21] explored the use of deep learning for plant species classification\nusing leaf vein morphometrics, achieving notable improvements in classification accuracy.\nHowever, their method does not extend to the hierarchical classification. Ganzera and\nSturm [9] highlighted recent advancements in HPLC/MS in medicinal plant analysis,\nfocusing primarily on chemical analysis.\nAdditionally, Vishnoi et al.\n[24] provided\na comprehensive study of feature extraction techniques for plant leaf disease detection,\nemphasizing the importance of feature extraction in classification tasks for plants affected\nwith various diseases.\nRecent studies have applied various deep learning techniques for medicinal plant\nclassification. Azadnia et al. [2] proposed a SCAM-herb based model which uses gated\npooling methods to classify medicinal and poisonous plants from visual characteristics\nof leaves, demonstrating the potential of deep learning in this domain. Samuel et al. [17]\nfocused on the antioxidant and phytochemical classification of medicinal plants used in\ncancer treatment where classification was based on the predominant antioxidant. Tan et\nal. [20] employed visual feature-based deep learning for rapid identification of medicinal\nplants, showing significant improvements in speed and accuracy.\nVarious researchers also explored the use of pixel-wise and constrained feature ex-\ntraction to improve classification. Dhakal and Shakya [6] explored the use of pixel wise\noperations in image-based plant disease detection. Kan et al. [12] examined multi fea-\nture extraction techniques for plant leaf image classification. Sachar and Kumar [16]\nsurveyed various feature extraction and classification techniques for identifying plants\nthrough leaves.\nTiwari et al.\n[23] developed an interesting deep neural network for\n2\n\nmulti-class classification of medicinal plant leaves where they classified 12 distinct crops\nin 22 different categories. Several intra class and inter class variations were considered\nduring training. Naresh et al. [14] proposed a modified Local binary patterns (MLBP)\nmethod extract texture features from plant leaves. This helped them to tackle the issue\nof capturing the texture of plant leaves belonging to same plant species. Dey et al.\n[5] assessed various deep convolutional neural network models for automated medicinal\nplant identification ([1], [13], [4], [18]) from leaf images and gave a comparative analysis\non them.\nThe primary shortcoming of these existing methods were their inability to perform\nhierarchical classification and tackle the challenge of identifying unknown or new species,\nwhich is crucial for a comprehensive medicinal plant classification. Many current ap-\nproaches focus on known species and do not extend to the hierarchical classification\nneeded for effectively dealing with new species. Hierarchical classification is essential as\nit allows for the systematic categorization of plants at various taxonomic levels, from\nphylum down to species. This level of detail is particularly important for accurately\ndistinguishing between closely related species and understanding their evolutionary re-\nlationships. Without hierarchical classification, the ability to fully utilize the vast po-\ntential of medicinal plants is hindered, especially in discovering and categorizing new\nspecies with potential therapeutic benefits.\nIn this study, we propose a novel method , which integrates DenseNet121 , Multi-\nScale Self-Attention (MSSA) and cascaded classifiers for hierarchical classification of\nmedicinal plants. This approach not only used to classifying known species but also\nused to identify hierarchial classification for unknown species. By integrating MSSA, our\nmodel improves the feature representation technique by capturing both local and global\ncontextual information, which is critical for distinguishing between similar species and\nidentifying new ones. Unlike other models that do not tackle hierarchical classification,\nour proposed model systematically classifies medicinal plants at multiple taxonomic\nlevels(Phylum to Species), ensuring a more detailed and accurate classification process.\nTherefore the summary of our contributions are as follows:\n• We introduce a new method that integrates DenseNet121, Multi-Scale Self-Attention\n(MSSA) and cascaded classifiers for hierarchical classification.\n• Unlike other models that don’t handle hierarchical classification, our model classi-\nfies medicinal plants at multiple taxonomic levels ranging from phylum to species,\nleading to more precise and accurate classifications.\n• Our method not only deals with known medicinal plants but also provides hierar-\nchical classification when introduced to unknown/new medicinal plants.\n• The proposed model is almost four times smaller in size as compared to other\nstate of the art methods and it achieves great results when dealing with datasets\nwith various background artifacts which makes it easily deploy able in real-world\napplications making it practical for widespread use.\n• The proposed model achieves promising results not only for known medicinal plant\nspecies but also for unknown plant species.\n3\n\nThe article is organized into five sections. Following introduction we have prelimi-\nnaries for our proposed approach. Section 3 describes the proposed method. In Section\n4 and 5 we have the discussion on experimental setup and experimental results. In the\nend, Section 6 contains the conclusion.\n2\nPreliminaries\nIn this section we present a brief discussion on the preliminaries of the technologies\nused in our proposed method, this includes discussion on DenseNet121 and Multi-Scale\nSelf-Attention.\n2.1\nDenseNet121\nThe feature extractor used in our proposed method is DenseNet21 [11]. The DenseNet21\n(Densely Connected Convolutional Network) is an extension of the DenseNet architec-\nture that consists of 21 layers. DenseNet is known for its dense connectivity pattern,\nwhere each layer is connected to every other layer in a feed-forward manner. This design\nhelps the flow of information and gradients throughout the network for more efficient\ntraining and better performance. The architecture is shown in Figure 1.\nFigure 1: DenseNet121 feature extractor\nDenseNet121 consists of four dense blocks interspersed with transition layers. Each\ndense block comprises several convolutional layers, with each layer receiving inputs from\nall preceding layers within the same block. This connectivity is mathematically repre-\n4\n\nsented as:\nxl = Hl([x0, x1, . . . , xl−1])\n(2.1)\nwhere xl is the output of the l-th layer, [x0, x1, . . . , xl−1] represents the concatenation\nof the feature maps produced by layers 0 to l −1, and Hl(·) is the composite function\nof the operations at the l-th layer, which includes Batch Normalization (BN), Rectified\nLinear Unit (ReLU), and Convolution (Conv). An illustration is shown in Figure 2.\nFigure 2: A 5-layer dense block with a growth rate of k = 4.\nEach layer takes all\npreceding feature-maps as input\nThe network begins with an initial convolution layer that processes the input image,\nfollowed by a pooling layer. The initial layers can be described as:\nx0 = Conv(Input),\nx1 = Pooling(x0)\n(2.2)\nDense Block 1 consists of 6 convolutional layers, each densely connected to the\nprevious layers. The transition layer following this block (2.3) includes batch normaliza-\ntion, a 1x1 convolutional layer, and a 2x2 average pooling layer. Similarly Dense Block\n2 consists of 12 convolutional layers, followed by another transition layer (2.4). Dense\nBlock 3 contains 24 convolutional layers. The transition layer here (2.5) also includes\nbatch normalization, a 1x1 convolution, and a 2x2 average pooling. Dense Block 4 is\nthe final block with 16 convolutional layers, after which a global average pooling layer\n(2.6) reduces the spatial dimensions of the feature maps.\nxtrans1 = AvgPool(Conv1x1(BN(x1)))\n(2.3)\nxtrans2 = AvgPool(Conv1x1(BN(xtrans1)))\n(2.4)\n5\n\nxtrans3 = AvgPool(Conv1x1(BN(xtrans2)))\n(2.5)\nxgap = GlobalAvgPool(xtrans3)\n(2.6)\nEach layer in DenseNet121 typically consists of three operations:\nHl(x) = Wl ∗σ(BN(x))\n(2.7)\nwhere BN denotes Batch Normalization, σ represents the ReLU activation function, Wl\nis the weight matrix of the convolutional layer, and ∗denotes the convolution operation.\nThe growth rate k is a crucial hyperparameter in DenseNet, indicating the number of\nfeature maps added by each layer. If the input to the l-th layer has m feature maps,\nthe output will have m + k feature maps. Thus, the width of the network grows linearly\nwith the depth.\nTo control the complexity and size of the model, DenseNet121 employs transition\nlayers between dense blocks, which consist of a Batch Normalization layer, a 1x1 Con-\nvolutional layer, and a 2x2 Average Pooling layer. The transition layer can be expressed\nas:\nT(x) = AvgPool(Wt ∗σ(BN(x)))\n(2.8)\nwhere Wt is the weight matrix of the 1x1 convolutional layer in the transition layer. In\nour proposed method we have used DenseNet121 as a feature extractor. Since each layer\nis connected to its previous layer DenseNet121 ensures efficient gradient propagation\nand better representation of the complex structural features of previous layers.\n2.2\nMulti-Scale Self-Attention (MSSA)\nSelf-attention techniques are widely used to compute contextual relationships and en-\nhance the feature representation learned by convolutional layers. In self-attention, an\ninput feature map is transformed into a weighted feature map that captures contextual\nrelationships. However, this weighted feature map often lacks sufficient contextual infor-\nmation. Specifically, feature maps from shallow layers contain rich local spatial details\nbut lack high-level semantics, while feature maps from deeper layers contain high-level\nsemantic information but miss local spatial details.\nTo address these limitations, Multi-Scale Self-Attention (MSSA) [25] is used to inte-\ngrate both local spatial and high-level semantic contextual information through multi-\nscale features learned by different convolutional blocks. The MSSA block takes a multi-\nscale feature map F and a resized local feature map C′\ni as inputs, producing a weighted\nmulti-scale feature map Di that captures contextual relationships among pixels from\nboth local spatial and high-level semantic perspectives.\nFor example, consider five outputs from a feature extractor, each denoted as Ci,\nwhere i ranges from 1 to 5, corresponding to different convolutional blocks. Ci contains\nfeature maps of varying scales at different depths, with scales decreasing and depth\nincreasing as i increases. To merge both local spatial details and high-level semantics,\noutputs from these five blocks are used to form a multi-scale feature map F. To retain\n6\n\nlocal spatial details at the highest resolution, each output (e.g., C2, C3, C4, and C5) is\nresized to match the dimensions of C1 by:\nC′\ni = upsample(Ci)\nand\n|C′\ni| = |C1|\n(2.9)\nwhere i = 2, 3, 4, and 5, and |x| represents the dimension of a feature map x without\ndepth. All resized outputs are then concatenated to construct a multi-scale feature map\nF by:\nF = C′\n1 ⊕C′\n2 ⊕C′\n3 ⊕C′\n4 ⊕C′\n5\n(2.10)\nwhere ⊕denotes the concatenation operation. Each high-resolution C′\ni and the multi-\nscale feature map F are individually fed into the proposed MSSA module, which will be\ndetailed in subsection 2.2, to compute contextual relationships.\nFor an input feature map C′\ni ∈RH×W×Ch1, where H, W, and Ch1 represent the\nheight, width, and channel dimensions respectively, and i denotes the block number, a\n1x1 convolution is applied to transform C′\ni into a new feature map Y ∈RH×W× Ch1\n8 . A\nratio of 1/8 is used to reduce the channel number to its 1/8, which has been empirically\ndetermined to be optimal [26]. Similarly, for the multi-scale feature map F ∈RH×W×Ch2,\na 1x1 convolution is used to generate a new feature map Z ∈RH×W× Ch1\n8 .\nWe then reshape Y to Yr of size (H ×W)× Ch1\n8\nand reshape and transpose Z to Zrt of\nsize Ch1\n8 ×(H ×W). Multiplying Yr and Zrt generates a map of size (H ×W)×(H ×W).\nApplying a softmax to this map produces a normalized map A, also known as the\nattention map. The attention map A is computed as:\nA(m, n) =\nexp(Yr(m, :) · Zrt(:, n))\nPH×W\nn=1\nexp(Yr(m, :) · Zrt(:, n))\n(2.11)\nwhere : denotes all values in a row or column, and A(m, n) represents the impact of\nthe n-th column of Zrt on the m-th row of Yr. A high value in A indicates a strong\ncorrelation between Yr and Zrt (i.e., between C′\ni and F).\nIn another branch, a 1x1 convolution transforms C′\ni into a new feature map X ∈\nRH×W×Ch1, which is then reshaped and transposed to Xrt of size Ch1 × (H × W). A\nmatrix multiplication between Xrt and A is performed, and the result is reshaped to size\nH × W × Ch1 and scaled by a learnable parameter µ to generate a weighted attention\nmap. This map is added to the input C′\ni to produce a weighted feature map Di:\nDi(m, n) = µ · reshape(Xrt(m, :) · A(:, n)) + C′\ni(m, n)\n(2.12)\nwhere Di(m, n) represents the value of a weighted feature map at location (m, n), and\nµ is initialized to 0 to allow the network to initially rely on local neighborhood cues to\nmaximize learning.\nStarting with D5, a 3x3 filter is applied, and the filtered result is concatenated with\nD4 to combine spatial and semantic information from blocks 5 and 4. This operation is\nrepeated to combine information from blocks 4 and 3, blocks 3 and 2, and blocks 2 and\n1. The algorithmic steps for chained concatenation operations are as follows:\n7\n\nAlgorithm 1 Chained Concatenation Operations in MSSA Module\n1: Initialize U5 = D5\n2: for i = 5 to 2 do\n3:\nUi−1 = conv(Ui) ⊕Di−1\n4: end for\nwhere i represents the block number and Ui−1 contains spatial and semantic informa-\ntion from the i-th and i −1-th blocks. A 3x3 convolution is then applied to U1, followed\nby bilinear interpolation and softmax to generate the segmentation result.\nThe final weighted multi-scale feature map Di thus integrates spatial details and\nsemantic information, enhancing the feature representation for improved segmentation\naccuracy. Figure 3 gives an illustration of MSSA module.\nFigure 3: Illustration of MSSA module\n3\nProposed Method\nIn this section, we propose a novel architecture (DenseNet121 with Multi-Scale Self-\nAttention and Cascaded Classifiers) designed for the hierarchical classification of medic-\ninal plants and address the open world challenges. Our model uses DenseNet121 back-\nbone as a feature extractor to extract the features. We integrate a Multi-Scale Self-\nAttention mechanism to our proposed method which in turn helps the model to learn\ndifferent contextual relationship and important features for hierarchical classification .\nThe MSSA mechanism processes multi-scale feature maps from different convolutional\nlayers of DenseNet121, allowing the model to capture both local spatial details and high-\nlevel semantic information. Subsequently, cascaded classifiers are employed to predict\n8\n\ntaxonomic categories (Phylum, Class, Order, Family, Genus, and Species) in a hierar-\nchical manner.\n3.1\nProblem Description\nTraditional classifiers used for medicinal plant classification are unable to perform hi-\nerarchical classification at the taxonomic level. This poses a severe challenge for these\nmodels as they fail to predict the taxonomic categories when an unknown or new species\nis discovered. To bridge this challenge we propose a novel architecture that gives us\nhierarchical classification of medicinal plants. Our proposed model also addresses the\nchallenge of unknown/new species classification. In the following section we discuss in\ndetail our proposed method.\n3.2\nModel Architecture\nThe proposed architecture is designed to effectively classify medicinal plants by lever-\naging rich feature representations and contextual relationships.\nThe architecture in-\ntegrates DenseNet121 as the backbone network for feature extraction, a Multi-Scale\nSelf-Attention (MSSA) module to enhance contextual relationships, and a series of cas-\ncaded classifiers for hierarchical classification. The proposed model architecture is shown\nin Figure 4\n3.2.1\nDenseNet121 Backbone\nDenseNet121 is used as the backbone network due to its dense connectivity, which pro-\nmotes feature reuse and efficient gradient flow. The DenseNet121 architecture consists of\nmultiple dense blocks, each containing several convolutional layers. The output of each\nlayer is concatenated with the outputs of all preceding layers within the same block, al-\nlowing the network to learn robust feature representations. It acts as a feature extractor\nfor our proposed method.\n3.2.2\nMulti-Scale Self-Attention (MSSA)\nThe MSSA module is used with DenseNet121 as its backbone to capture local spa-\ntial details and high-level semantic information by applying self-attention to multi-scale\nfeature maps obtained from different convolutional blocks of DenseNet121. In our im-\nplementation, we utilize three key layers (’pool2’, ’pool3’,‘pool4‘) from DenseNet121\nfor MSSA. These layers correspond to the outputs after the second, third, and fourth\npooling layers, respectively. The ‘pool2‘ layer has the highest resolution with dimensions\nH1 ×W1 ×C1, while the ‘pool3‘ and ‘pool4‘ layers have progressively smaller resolutions\nand more channels, denoted as H2 × W2 × C2 and H3 × W3 × C3, respectively. For our\nMSSA module, we denote these feature maps as F1, F2, and F3, respectively.\nThe MSSA mechanism can be described as follows:\nLet Ci denote the output of the i-th selected convolutional block of DenseNet121,\nwhere i ranges from 1 to 3. To maintain high-resolution spatial details, we resize each\n9\n\nFigure 4: Proposed Model Architecture for hierarchical taxonomy generation for plants\noutput Ci to match the dimension of the highest resolution output C1:\nC′\ni = resize(Ci, shape(C1))\nand\n|C′\ni| = |C1|\n(3.1)\nNext, we concatenate the resized outputs to construct a multi-scale feature map F:\nF = C′\n1 ⊕C′\n2 ⊕C′\n3\n(3.2)\nwhere ⊕denotes the concatenation operation. For each input feature map C′\ni ∈RH×W×Ch1,\nwhere H, W, and Ch1 represent the height, width, and channel dimensions respectively,\nwe apply a 1 × 1 convolution to transform C′\ni into a new feature map Y ∈RH×W× Ch1\n8 :\nY = Conv1×1(C′\ni)\n(3.3)\nSimilarly, for the multi-scale feature map F ∈RH×W×Ch2, we apply a 1x1 convolution\nto generate a new feature map Z ∈RH×W× Ch1\n8 :\nZ = Conv1×1(F)\n(3.4)\n10\n\nWe then reshape Y to Yr of size (H × W) × Ch1\n8\nand reshape and transpose Z to Zrt of\nsize Ch1\n8 × (H × W). The attention map A is computed as 2.11\nIn a parallel branch, we apply another 1x1 convolution to transform C′\ni into a new\nfeature map X ∈RH×W×Ch1, which is then reshaped and transposed to Xrt of size\nCh1 × (H × W).\nThe weighted attention map is computed as 2.12, where Di(m, n) is the value of a\nweighted feature map at location (m, n), and µ is a learnable parameter initialized to\n0. Starting with D3, we apply a 3x3 convolution and concatenate the result with D2 to\nintegrate spatial and semantic information:\nU3 = D3\n(3.5)\nUi−1 = conv(Ui) ⊕Di−1\nfor\ni = 3 to 2\n(3.6)\nThe algorithmic steps for chained concatenation operations are as follows:\nAlgorithm 2 Chained Concatenation Operations in MSSA Module\n1: Initialize U3 = D3\n2: for i = 3 to 2 do\n3:\nUi−1 = conv(Ui) ⊕Di−1\n4: end for\nFinally, we apply a 3×3 convolution to U1, followed by bilinear interpolation and\nsoftmax to generate the attention based feature map. Bilinear interpolation method\nis used to resample image pixels, providing a smoother and more accurate output than\nnearest-neighbor interpolation. The new pixel value P is computed as a weighted average\nof the four nearest pixel values. Let Q11, Q12, Q21, and Q22 be the four nearest pixels\nto the target pixel, where Q11 is the top-left pixel, Q12 is the top-right pixel, Q21 is\nthe bottom-left pixel, and Q22 is the bottom-right pixel. The interpolated value P is\ncomputed as follows:\nP = Q11(1 −x)(1 −y) + Q21x(1 −y) + Q12(1 −x)y + Q22xy\n(3.7)\nwhere x and y are the relative distances of the target pixel from the top-left corner\nwithin the unit square.\n3.2.3\nCascaded Classifiers\nThe output of the MSSA module is passed through a Global Average Pooling layer to\nreduce the spatial dimensions, followed by a dense layer with 512 units and ReLU acti-\nvation, and a dropout layer with a dropout rate of 0.5. Subsequently, the model employs\na series of cascaded classifiers to predict taxonomic categories (Phylum, Class, Order,\nFamily, Genus, and Species) in a hierarchical manner. Each classifier branch consists\nof a dense layer with 256 units and ReLU activation, followed by a softmax output\nlayer tailored to the number of classes in each category. The output of each classifier is\nconcatenated with the input features and fed into the subsequent classifier, ensuring a\n11\n\nhierarchical prediction structure. The architecture of each cascaded classifier includes a\ndense layer with 256 units and ReLU activation, which receives the input features. This\ndense layer is followed by a softmax output layer that outputs the probability distri-\nbution over the classes for the current taxonomic category. The output of the softmax\nlayer is then concatenated with the input features to form the input for the next clas-\nsifier branch. The use of cascaded classifiers enables us to build a hierarchical system\nand moreover it helps us to generalize to unknown species based on shared taxonomic\ncharacteristics.\n3.3\nUnknown Species Prediction\nAlgorithm 3 Hierarchical Classification with Confidence Threshold\nRequire: Predicted class probabilities, Confidence Threshold\nEnsure: Final Taxonomic Classification\n1: Set the initial taxonomic category to Phylum\n2: for each taxonomic level (Phylum, Class, Order, Family, Genus, Species) do\n3:\nif Predicted probability at the current level ≥Confidence Threshold then\n4:\nAssign the sample to the predicted class at this level\n5:\nMove to the next taxonomic level\n6:\nelse\n7:\nStop classification and return classification results BREAK\n8:\nend if\n9: end for\nFor unknown species classification, the model employs a hierarchical classification\napproach, where predictions are made at various taxonomic levels, including Phylum,\nClass, Order, Family, Genus, and Species. To determine class membership, a confidence\nthreshold of 0.6 is applied. If the predicted class probability surpasses this threshold, the\nsample is assigned to that particular class. This threshold is set considering potential\nnoise in the images. The hierarchical process proceeds from broader categories to more\nspecific ones. However, if the confidence falls below 0.6 at any level, the classification\nprocess terminates, and the classification taxonomy is generated till that point and\nis returned as an output to us. This strategy helps ensure robust classification while\naccommodating the possibility of ambiguous or uncertain predictions. The hierarchical\ncascaded classifier, featuring an adaptive confidence threshold, proves invaluable for\nclassifying previously unknown species not encountered during training. This approach\neffectively addresses the challenge of open-world recognition by allowing the system to\nmake informed decisions when faced with unfamiliar species.\n12\n\n4\nExperimental Setup\n4.1\nDataset Description\nIn this study, we utilize two types of datasets to evaluate the performance of the proposed\nmodel: one dataset with background artifacts and another dataset without background\nartifacts. The dataset with noise (DIMPSAR), sourced from Kaggle [3], includes images\nof medicinal plants taken in real-world settings. These images contain various types\nof background artifacts, such as objects, and environmental factors. This dataset sim-\nulates realistic conditions where image data may include additional elements that can\ninterfere with the clear identification of the medicinal plants. The presence of back-\nground artifacts in the images presents a significant challenge for the model, testing its\nrobustness and ability to handle real and cluttered data effectively. The dataset with-\nout background artifacts, obtained from Mendeley Data [15], consists of clean images\nof medicinal plants, taken in controlled environments without any background noise.\nThis dataset is used to assess the model’s performance under ideal conditions, providing\na baseline for comparison with the noisy dataset. The artifact-free images allow the\nmodel to focus on learning the intrinsic features of the plants without the interference\nof extraneous background elements.\n(a) Dataset with background artifacts\n(b) Dataset without back-\nground artifacts\nFigure 5: Dataset Visualisation\nFigure 5 shows Noisy dataset consists of 10 unique medicinal plant species and non-\nnoisy dataset consists of 15 unique medicinal plant species. Detailed explanation on\ndataset is given on Table 1 and 2. Noisy dataset has 10 species, 10 genus, 10 families,\n10 orders, 3 classes and 4 phylums. Similarly dataset without noise has 14 species, 14\ngenus, 12 families, 11 orders , 2 classes and 3 phylums. By noisy dataset we mean the\ndataset where the background environment is still intact.\n13\n\n4.2\nExperimental Environment\nIn this study, all the experiments were performed on an NVIDIA DGX-1 supercomputer.\nThe supercomputer has the following configuration: Dual 20 Core Intel Xeon E5-2698\nV4 clocked at 2.2 GHz, 5120 NVIDIA cores, 512 GB 2.133 GHz DDR4 RDIMM (RAM).\nAll the codes are written in Python version 3.9.13. In the next section we discuss and\ncompare the experimental results.\nTable 1: Dataset(without background artifacts) Description\nPhylum\nClass\nOrder\nFamily\nGenus\nSpecies\nSpermatophyta\nMagnoliopsida\nLamiales\nLamiaceae\nOcimum\nTulasi\nMagnoliophyta\nMagnoliopsida\nScrophulariales\nOleaceae\nJasminum\nJasmine\nBrassicales\nMoringaceae\nMoringa\nDrumstick\nSantalales\nSantalaceae\nSantalum\nSandalwood\nLamiales\nLamiaceae\nLamiaceae\nMint\nPiperales\nPiperaceae\nPiper\nBetel\nRosales\nMoraceae\nFicus\nPeepal Tree\nSapindales\nMeliaceae\nAzadirachta\nNeem\nAnacardiaceae\nMangifera\nMango\nFabaceae\nTrigonella\nFenugreek\nGentianales\nApocynaceae\nCarissa\nKaranda\nTracheophyta\nMagnoliopsida\nLamiales\nLamiaceae\nPlectranthus\nMexican Mint\nMyrtales\nMyrtaceae\nPsidium\nGuava\nMalvales\nMalvaceae\nHibiscus\nHibiscus rosa-sinensis\nLiliopsida\nZingiberales\nZingiberaceae\nAlpinia\nRasna\nTable 2: Dataset (with background artifacts) Description\nPhylum\nClass\nOrder\nFamily\nGenus\nSpecies\nSpermatophyta\nMagnoliopsida\nLamiales\nLamiaceae\nOcimum\nTulasi\nAngiospermae\nLaurales\nLauraceae\nPersea\nAvocado\nMagnoliophyta\nMagnoliopsida\nPiperales\nPiperaceae\nPiper\nBetel\nSapindales\nMeliaceae\nAzadirachta\nNeem\nSolanales\nSolanaceae\nWithania\nAshwagandha\nLiliopsida\nCyperales\nPoaceae Barnhart\nCymbopogon Spreng\nLemon grass\nTracheophyta\nMagnoliopsida\nMalpighiales\nPhyllanthaceae\nPhyllanthus\nAmla\nMyrtales\nMyrtaceae\nPsidium\nGuava\nMalvales\nMalvaceae\nHibiscus\nHibiscus\nAnthophyta\nLiliopsida\nAsparagales\nAsparagaceae\nAloe\nAloevera\n14\n\n5\nResults Analysis\n5.1\nModel Comparison\nThe experiments were performed on two datasets, and the proposed approach was com-\npared with 6 approaches using accuracy as a metric for evaluation.\nThe results are\ndepicted in Table 3 and 4 respectively.\nTable 3: Comparison of Model Performances (Dateset without background arti-\nfacts)\nModel\nPhylum Acc.\nClass Acc.\nOrder Acc.\nFamily Acc.\nGenus Acc.\nSpecies Acc.\nEfficientNet [22]\n78.63%\n93.89%\n16.79%\n16.79%\n10.69%\n10.69%\nDenseNet [11]\n99.24%\n100.00%\n99.24%\n99.24%\n99.24%\n99.24%\nResNet [10]\n78.63%\n93.89%\n10.69%\n6.87%\n12.21%\n12.21%\nVGG [19]\n96.24%\n94.31%\n93.20%\n93.13%\n91.24%\n90.24%\nVGG-MSSA\n96.24%\n96.00%\n93.95%\n94.71%\n95.47%\n95.47%\nResNet50-MSSA\n78.44%\n95.3%\n48.67%\n44.85%\n49.44%\n50.96%\nProposed Method\n99.24%\n99.24%\n99.24%\n99.24%\n99.24%\n99.24%\nTable 4: Comparison of Model Performances (Dateset with background environ-\nment(noise)\nModel\nPhylum Acc.\nClass Acc.\nOrder Acc.\nFamily Acc.\nGenus Acc.\nSpecies Acc.\nEfficient-Net [22]\n36.28%\n68.58%\n12.39%\n9.73%\n9.73%\n9.73%\nDenseNet [11]\n96.02%\n98.23%\n94.69%\n95.13%\n95.13%\n95.13%\nResNet [10]\n39.28%\n68.58%\n9.73%\n9.73%\n9.73%\n9.73%\nVGG [19]\n90.21%\n97.34%\n88.70%\n87.90%\n87.23%\n87.04%\nVGG-MSSA\n91.15%\n96.90%\n90.71%\n91.15%\n91.15%\n91.15%\nResNet-MSSA\n46.46%\n69.91%\n27.88%\n23.89%\n35.84%\n26.55%\nProposed Method\n98.23%\n99.12%\n98.23%\n97.35%\n97.35%\n98.20%\nTable 3 and 4 shows the model comparison of our model with various models. We\ncan observe that in Table 3 our proposed method outperforms all the models in all\nthe taxonomic categories respectively. Our model achieved a classification accuracy of\n99.24% in all taxonomic division. DenseNet also performs well in all the respective\ntaxonomic classification as compared to our proposed method.\nDenseNet reuses the\nfeature maps, which helps it to perform better classification as compared to other models.\nWhen we look at the results obtained by different models on dataset which contains the\nbackground environment (Table 4), we can see that despite the background artifacts\nour model outperforms all the models again in all the taxonomic categories. The drop\nin classification accuracy in our model is very minimal as compared to other methods.\nThe largest drop (classification accuracy) in our method was seen in Order Classification\nwhich was about 2.65%. For the other categories the drop was not more than 2%.\nIf we compare the classification accuracies obtained by other model in the latter\ndataset, we can clearly see that accuracies have dropped significantly when there are\n15\n\nbackground artifacts.\nFor EfficientNet, phylum classification accuracy dropped from\n76.63% to 36.28. For VGG it dropped from 96.24% to 90.21%. Similar trend can\nbe observed for ResNet as well where phylum and class accuracy dropped from 78.63%\nand 93.89% to 39.29% and 68.58%.\nIn both the dataset integration of MSSA module to VGG and ResNet led to better\nclassification accuracy across all taxonomic categories.\nInclusion of MSSA improved\ntheir performance because it helped in capturing long-range dependencies in feature\nmaps. Inclusion of MSSA not only enhanced classification but also reduced the model\nsize and memory.\n5.2\nUnknown/New Species Classification Performance\nIn this section performance of various models on the task of classifying unknown species\nis compared in Tables 6 and 7, with and without background artifacts respectively. The\nresults show that the proposed method consistently outperforms other models across\nmultiple taxonomic levels.\nFour unknown species were considered for classification.\nThey were Wood Sorel, Noni, Oleander and Jackfruit. Unknown species description\nis given in Table 5.\nTable 5: Taxonomic classification of Wood Sorel, Noni, Oleander, and Jackfruit\nSpecies Name\nPhylum\nClass\nOrder\nFamily\nGenus\nSpecies\nWood Sorel\nMagnoliophyta\nMagnoliopsida\nOxalidales\nOxalidaceae\nOxalis\nOxalis acetosella\nNoni\nMagnoliophyta\nMagnoliopsida\nGentianales\nRubiaceae\nMorinda\nMorinda citrifolia\nOleander\nMagnoliophyta\nMagnoliopsida\nGentianales\nApocynaceae\nNerium\nNerium oleander\nJackfruit\nMagnoliophyta\nMagnoliopsida\nRosales\nMoraceae\nArtocarpus\nArtocarpus heterophyllus\nFor images with background artifacts (Table 6), the proposed method achieves the\nhighest Phylum Accuracy at 82.37%, surpassing the next best model (VGG) by 1.14%.\nIt also achieves the highest Class Accuracy at 76.39%, which is a significant improve-\nment over VGG’s 72.21%. For Order Accuracy, the proposed method achieves 59.32%,\nwhich is substantially higher than VGG’s 46.29%.\nIt is evident from the results obtained that for images without background artifacts\n(Table 7), the proposed method again leads in Phylum Accuracy with 84.35%, slightly\ngreater than VGG. In terms of class accuracy VGG achieves the accuracy of 80.51%\nwhere it is marginally ahead of the proposed method.\nThe proposed method again\nachieves the highest Order Accuracy at 61.37%, outperforming VGG’s 57.23%. It also\nshows notable performance in Family Accuracy, achieving 43.32%, where other models\nwere not able to classify them. The superior performance of the proposed method is\ndue to the integration of multi-scale self-attention module. This module helps the model\nfocus on different parts of the image at various scales, which is particularly useful for\nrecognizing complex patterns and details and retain taxonomic features. By capturing\nlong range dependencies and important features, the model becomes more effective at\ndistinguishing between different categories, even when the images have background ar-\ntifacts. Additionally, the proposed method uses a larger receptive field, allowing it to\n16\n\ngather more contextual information from the images. This leads to better feature repre-\nsentation and overall improved classification accuracy across various taxonomic levels.\nTable 6: Classification Accuracy obtained on Unknown Species by different models (On\nimages with background artifacts)\nModel\nPhylum\nClass\nOrder\nFamily\nAccuracy (%)\nAccuracy (%)\nAccuracy (%)\nAccuracy (%)\nVGG\n81.23%\n72.21%\n46.29%\n-\nResNet\n21.23%\n11.27%\n-\n-\nEfficient-Net\n15.21%\n-\n-\n-\nDense-Net21\n79.21%\n61.12%\n34.71%\n-\nVGG-MSSA\n74.87%\n68.43%\n41.45%\n-\nResNet50-MSSA\n18.61%\n-\n-\n-\nProposed Method\n82.37%\n76.39%\n59.32%\n-\nTable 7: Classification Accuracy obtained on Unknown Species by different models (On\nimages without background artifacts)\nModel\nPhylum\nClass\nOrder\nFamily\nAccuracy (%)\nAccuracy (%)\nAccuracy (%)\nAccuracy (%)\nVGG\n84.23%\n80.51%\n57.23%\n-\nResNet\n28.23%\n18.23%\n-\n-\nEfficient-Net\n11.21%\n-\n-\n-\nDense-Net21\n73.21%\n64.12%\n39.75%\n-\nVGG-MSSA\n81.87%\n77.43%\n46.45%\n-\nResNet50-MSSA\n59.87%\n31.23%\n-\n-\nProposed Method\n84.35%\n80.22%\n61.37%\n43.32%\n5.3\nCascaded Classifier Performance Comparison for various\nmodels\nMoreover, we evaluated the performance of our cascaded classifier across all taxonomic\ncategories for various models. Figures 7,9 and 11 shows the comparison of precision,\nrecall and F1-scores achieved by various model on the dataset which had no background-\nartifacts. Similarly Figures 8,10 and 12 shows the comparison of precision, recall and\nF1-scores achieved by various model on the dataset which had background artifacts such\nas external objects, light etc. In the grouped histogram plots each colour represents a\nmodel. To better understand the legend for histogram plot is shown in Figure 6.\n17\n\nFigure 6: Legend for histogram plots\nFigure 7: Precision values for various models (dataset without background arti-\nfacts)\n18\n\nFigure 8: Precision values for various models (dataset with background artifacts)\nFigure 9: Recall values for various models (dataset without background artifacts)\n19\n\nFigure 10: Recall values for various models (dataset with background artifacts)\nFigure 11: F1-scores for various models (dataset without background artifacts)\n20\n\nFigure 12: F1-scores for various models (dataset with background artifacts)\n5.4\nAblation Study\nIn this ablation study, we evaluate the impact of combining different techniques on model\nperformance across various taxonomic levels. We consider six combinations of models:\nDenseNet , DenseNet with MSSA, ResNet, ResNet with MSSA, VGG, and VGG with\nMSSA. We report the precision/recall and F1 score for each combination on both the old\ndataset (with background artifacts) and the new dataset (without background artifacts)\nin Table 8-11. The text in bold represents the test classification accuracy obtained by\nthe proposed architecture\nTable 8: Precision/Recall on Dataset with background artifacts\nModel\nPhylum\nClass\nOrder\nFamily\nGenus\nSpecies\nDenseNet + Cascaded\nClassifier\n0.97/0.97\n.98/.98\n0.95/0.95\n0.96/0.95\n0.96/0.96\n0.95/0.95\nDenseNet + MSSA +\nCascaded Classifier\n0.98/.98\n.99/.99\n0.99/.98\n.98/.97\n0.98/.97\n0.97/.97\nResNet + Cascaded Classifier\n0.036/0.1\n0.069/0.1\n0.010/0.18\n0.010/0.18\n0.01/0.18\n0.01/0.18\nResNet + MSSA + Cascaded\nClassifier\n0.54/0.44\n0.76/0.66\n0.31/0.41\n0.41/0.39\n0.40/0.40\n0.38/0.38\nVGG + Cascaded Classifier\n0.97/0.97\n0.98/0.93\n0.94/0.93\n0.94/0.93\n0.93/0.93\n0.92/0.92\nVGG + MSSA + Cascaded\nClassifier\n0.89/0.89\n0.94/0.93\n0.91/0.90\n0.91/0.90\n0.90/0.90\n0.91/0.90\n21\n\nTable 9: F1 Score on Old Dataset with background artifacts\nModel\nPhylum\nClass\nOrder\nFamily\nGenus\nSpecies\nDenseNet + Cascaded\nClassifier\n0.975\n.976\n0.945\n0.951\n0.951\n0.959\nDenseNet + MSSA +\nCascaded Classifier\n0.98\n.99\n.98\n.97\n.97\n.97\nResNet + Cascaded Classifier\n0.053\n0.081\n0.018\n0.018\n0.018\n0.018\nResNet + MSSA + Cascaded\nClassifier\n0.32\n0.46\n0.417\n0.314\n0.23\n0.23\nVGG + Cascaded Classifier\n0.98\n0.94\n0.934\n0.938\n0.93\n0.92\nVGG + MSSA + Cascaded\nClassifier\n0.895\n0.96\n0.906\n0.90\n0.90\n0.902\nTable 10: Precision/Recall on New Dataset without background artifacts\nModel\nPhylum\nClass\nOrder\nFamily\nGenus\nSpecies\nDenseNet + Cascaded\nClassifier\n0.996/0.93\n1.00/1.00\n0.99/0.99\n0.99/0.99\n0.99/0.98\n0.98/0.98\nDenseNet + MSSA +\nCascaded Classifier\n0.99/0.99\n1.00/1.00\n1.00/1.00\n1.00/1.00\n0.99/0.99\n0.98/0.99\nResNet + Cascaded Classifier\n0.26/0.33\n0.47/0.50\n0.04/0.07\n0.017/0.0475\n0.095/0.10\n0.092/0.10\nResNet + MSSA + Cascaded\nClassifier\n0.94/0.45\n0.995/0.94\n0.57/0.45\n0.47/0.49\n0.485/0.45\n0.48/0.46\nVGG + Cascaded Classifier\n0.90/0.91\n0.97/0.97\n0.88/0.98\n0.87/0.98\n0.87/0.97\n0.87/0.97\nVGG + MSSA + Cascaded\nClassifier\n0.96/0.93\n0.99/1\n0.976/0.98\n0.983/0.98\n0.967/0.97\n0.977/0.97\nTable 11: F1 Score on New Dataset without background artifacts\nModel\nPhylum\nClass\nOrder\nFamily\nGenus\nSpecies\nDenseNet + Cascaded\nClassifier\n0.96\n1.00\n0.99\n0.99\n0.98\n0.97\nDenseNet + MSSA +\nCascaded Classifier\n0.99\n1.00\n1.00\n1.00\n.99\n0.98\nResNet + Cascaded Classifier\n0.29\n0.485\n0.005\n0.03\n0.05\n0.05\nResNet + MSSA + Cascaded\nClassifier\n0.54\n0.96\n0.4175\n0.414\n0.405\n0.38\nVGG + Cascaded Classifier\n0.92\n0.98\n0.98\n0.97\n0.97\n0.97\nVGG + MSSA + Cascaded\nClassifier\n0.95\n0.99\n0.97\n0.9833\n0.967\n0.977\n5.4.1\nDiscussion\nThe integration of MSSA module enhances performance by integrating local spatial\ndetails with high-level semantic information from different scales. Using attention scores,\nMSSA focuses on important features across multiple scales, helping the model capture\ncomplex relationships in the data, leading to higher precision and recall, as seen in the\nimproved metrics for all the taxonomic categories. By considering features at multiple\nscales, MSSA allows the model to understand both fine-grained and broad patterns in\nthe images, which is particularly useful for distinguishing between closely related species,\nresulting in higher accuracy for Order and Species.\n22\n\nWe extracted the feature maps from the pooling layers of the model and from the\nfinal layer of the MSSA module.\n(a) Feature map extracted from 2nd\npooling layer of DenseNet\n(b) Feature map extracted from final layer\nof MSSA module\nFigure 13: Dataset without background artifacts\nThe first image (13a) shows the feature map extracted from the second pooling layer\nof DenseNet. This feature map predominantly captures low-level features, which include\nbasic spatial details and textures. However, it lacks the ability to capture long-range\ndependencies and high-level semantic information. The spatial details are somewhat\nclear, but the overall representation is not sufficient for complex tasks such as distin-\nguishing between similar species or identifying new species. The second image (13b)\nshows the feature map from the final layer of the MSSA module. Compared to the fea-\nture map from the pooling layer, this map is more refined and detailed. MSSA enhances\nfeature representation by combining both local spatial details and high-level semantic\ninformation. This is evident in the clarity and distinctiveness of the features in the map.\n23\n\n(a) Feature map extracted from 2nd\npooling layer of DenseNet\n(b) Feature map extracted from final layer\nof MSSA module\nFigure 14: Dataset without background artifacts\nSimilarly feature map is extracted from the second pooling layer of DenseNet(noisy\ndata) and is shown in Fig (14a). This feature map primarily captures the local spatial\ndetails however it lacks high-level semantic information. The feature map shows a basic\nrepresentation with clear boundaries but does not capture the intricate contextual. The\nsecond image (14b) captures the feature map from the final layer of the MSSA module\non the noisy dataset.\nIn the old dataset, background artifacts can obscure the key\ncharacteristics of the medicinal plants, leading to misclassifications. However the model\nperforms better in cleaner dataset as compared to the latter one. Despite the images\naffected with noise in old dataset we are able to acheive good results because of the\nMSSA module which helps to focus on important features by supressing the noise.\nFurthermore, addition of MSSA improves robustness to noisy data by giving impor-\ntance relevant features and ignoring unnecessary background artifacts. This fact can be\nchecked from the performance drop seen on the dataset with background artifacts when\ntrained on models without MSSA. Additionally, MSSA enhances the model’s ability to\nunderstand the context of features within an image, which is crucial for hierarchical\nclassification tasks where understanding the relationship between different taxonomic\nlevels is important.\n5.4.2\nImpact of MSSA on Performance\nTables 8, 9, 10, and 11 show the precision, recall, and F1-score metrics for different\nmodels on both datasets. The results clearly demonstrate that incorporating MSSA\nsignificantly improves the performance across all taxonomic levels.\nFor the dataset with background artifacts, the DenseNet + MSSA achieves a precision\nand recall of 0.98 for Phylum, 0.99 for Class, 0.99 for Order, 0.98 for Family, 0.98 for\nGenus, and 0.97 for Species. These values are higher than those of the DenseNet without\n24\n\nMSSA, which shows the effectiveness of MSSA in capturing important features within\nthe data.\nSimilarly, for the dataset without background artifacts, DenseNet + MSSA achieves\nfairly good precision and recall across all taxonomic levels. For example, it achieves 0.99\nfor Phylum, 1.00 for Class, and 0.98 for Species. These results demonstrate that the\nmodel performs even better with clean data.\nThe F1-scores further highlight the benefits of MSSA. For the dataset with back-\nground artifacts, DenseNet + MSSA achieves F1-scores of 0.98 for Phylum, 0.99 for\nClass, 0.98 for Order, and 0.97 for Species.\nThese scores represent an improvement\nover models without MSSA, indicating better overall classification performance. On the\ndataset without background artifacts, it achieves F1-scores of 0.99 for Phylum, 1.00 for\nClass, and 0.98 for Species, again outperforming other models.\n5.5\nModel Size and Parameters\nIn this section we present a comparison of our proposed approach with respect to other\narchitectures on model size and parameters. As shown in Table 12, our model consists\nof 6,360,879 parameters, which translates to a memory size of just 24.26 MB. In\ncontrast, the DenseNet21 model has 33,557,116 parameters and requires 128.01 MB of\nmemory. Similarly, the EfficientNet model, with 36,991,711 parameters, occupies 141.11\nMB. Even more substantial models like ResNet100, with 75,797,436 parameters, demand\n289.14 MB of memory. This significant reduction in model size and memory footprint\noffers several advantages. Firstly, it makes the model highly suitable for deployment\non resource constrained devices, such as mobile phones and embedded systems, which\nare commonly used in field research and conservation efforts. The compactness of our\nmodel ensures that it can be utilized in real-time scenarios without incurring substantial\ncomputational overhead, making it practical for on-the-go applications.\nFurthermore, the reduced memory requirement enhances the model’s scalability and\nresponsiveness, allowing it to process and classify medicinal plant images more swiftly\nand efficiently.\nThis efficiency does not come at the cost of accuracy, as our model\nmaintains good performance metrics in both the datasets.\nThis presents our model\nas an optimal balance between effectiveness and resource efficiency, making it an ideal\nsolution for real-world applications in medicinal plant classification.\nTable 12: Number of Parameters and Memory Size\nModel\nNumber of Parameters\nMemory Size (MB)\nVGG [19]\n28389244\n108.30 (MB)\nResNet100 [10]\n75797436\n289.14 (MB)\nEfficient-Net [22]\n36991711\n141.11 (MB)\nDense-Net21 [11]\n33557116\n128.01 (MB)\nVGG-MSSA\n18158164\n69.27 (MB)\nResNet-MSSA\n15164692\n57.85 (MB)\nProposed Method\n6360879\n24.26 (MB)\n25\n\n6\nConclusion\nIn this study, we introduced a new model which integrates DenseNet121 with a Multi-\nScale Self-Attention (MSSA) mechanism and a cascaded classifier for the hierarchical\nclassification of medicinal plants. Our experiments on datasets with and without back-\nground artifacts demonstrated that the inclusion of MSSA significantly enhances the\nmodel’s performance, achieving higher precision, recall, and F1-scores across all taxo-\nnomic levels compared to traditional classifiers. The MSSA module is beneficial because\nit incorporates attention scores to capture complex relationships within the data, inte-\ngrating local spatial details with high-level semantic information from different scales.\nThis capability allows the model to handle noisy data and accurately distinguish be-\ntween closely related species, making it well-suited for real world applications where\nimage data may be degraded due to environmental factors. Our model outperformed\nother models in our study, particularly while handling dataset with background artifacts,\ndemonstrating its effectiveness. Our results highlighted the importance of incorporating\nMSSA in hierarchical classification tasks, paving the way for more accurate and reliable\nmedicinal plant classification. Our model when tested on unknown species provided\npromising results. By accurately predicting higher taxonomic levels such as phylum,\nclass, and order, the model can generalize and classify unknown species that share these\ncharacteristics with known species. This hierarchical approach ensures that even when\nencountering novel species, the model can still provide meaningful classifications based\non shared taxonomic features, enhancing its utility in biodiversity research and conser-\nvation efforts. Overall, this model represents a significant advancement in the field of\nmedicinal plant classification, providing a powerful tool for researchers and practitioners\nin botany, pharmacology, and related fields. Future work will explore further enhance-\nments to the model, including the integration of additional attention mechanisms and\nthe application to other hierarchical classification tasks.\nCompeting Interests: There are no competing interests.\nFunding Information: Not Applicable\nAuthor Contribution: All the authors have contributed equally.\nData Availability Statement: Dataset availability on request.\nResearch Involving Human and /or Animals: Not Applicable\nInformed Consent: Not Applicable\nReferences\n[1] Aimen Aakif and Muhammad Faisal Khan. Automatic classification of plants based\non their leaves. Biosystems Engineering, 139:66–75, 2015.\n26\n\n[2] Rahim Azadnia, Faramarz Noei-Khodabadi, Azad Moloudzadeh, Ahmad Jahan-\nbakhshi, and Mahmoud Omid. Medicinal and poisonous plants classification from\nvisual characteristics of leaves using computer vision and deep neural networks.\nEcological Informatics, 82:102683, 2024.\n[3] Pushpa B R and N Shobha Rani. Dimpsar: Dataset for indian medicinal plant\nspecies analysis and recognition.\nData in Brief, 49:109388, 07 2023.\nLink to\naccess- https://www.kaggle.com/datasets/warcoder/indian-medicinal-plant-image-\ndataset.\n[4] Ying Chen, Yiqi Huang, Zizhao Zhang, Zhen Wang, Bo Liu, Conghui Liu, Cong\nHuang, Shuangyu Dong, Xuejiao Pu, Fanghao Wan, Xi Qiao, and Wanqiang Qian.\nPlant image recognition with deep learning: A review. Computers and Electronics\nin Agriculture, 212:108072, 2023.\n[5] Biplob Dey, Jannatul Ferdous, Romel Ahmed, and Juel Hossain. Assessing deep\nconvolutional neural network models and their comparative performance for au-\ntomated medicinal plant identification from leaf images. Heliyon, 10:e23655, 01\n2024.\n[6] Ashwin Dhakal and Subarna Shakya. Image-based plant disease detection with deep\nlearning. International Journal of Computer Trends and Technology, 61:2231–2803,\n07 2018.\n[7] Himanshu Kumar Diwedi, Anuradha Misra, and Amod Kumar Tiwari. Cnn-based\nmedicinal plant identification and classification using optimized svm. Multimedia\nTools and Applications, 83(11):33823–33853, 2024.\n[8] Martin Fitzgerald, Michael Heinrich, and Anthony Booker. Medicinal plant analy-\nsis: A historical and regional discussion of emergent complex techniques. Frontiers\nin Pharmacology, 10:1480, 01 2020.\n[9] Markus Ganzera and Sonja Sturm. Recent advances on hplc/ms in medicinal plant\nanalysis—an update covering 2011-2016. Journal of Pharmaceutical and Biomedical\nAnalysis, 147, 08 2017.\n[10] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for\nimage recognition. 2016 IEEE Conference on Computer Vision and Pattern Recog-\nnition (CVPR), pages 770–778, 2015.\n[11] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q. Weinberger.\nDensely connected convolutional networks. In 2017 IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), pages 2261–2269, 2017.\n[12] HX Kan, L Jin, and FL Zhou. Classification of medicinal plant leaf image based on\nmulti-feature extraction. Pattern recognition and image analysis, 27:581–587, 2017.\n27\n\n[13] Owais A Malik, Nazrul Ismail, Burhan R Hussein, and Umar Yahya. Automated\nreal-time identification of medicinal plants species in natural environment using\ndeep learning models—a case study from borneo region. Plants, 11(15):1952, 2022.\n[14] YG Naresh and HS Nagendraswamy. Classification of medicinal plants: an approach\nusing modified lbp with symbolic representation. Neurocomputing, 173:1789–1797,\n2016.\n[15] Anitha S, Roopashree; J. Medicinal leaf dataset, mendeley data. 2020. Link to\naccess- https://data.mendeley.com/datasets/nnytj2v3n5/1.\n[16] Silky Sachar and Anuj Kumar.\nSurvey of feature extraction and classification\ntechniques to identify plant through leaves.\nExpert Systems with Applications,\n167:114181, 2021.\n[17] HUMPHREY SAMUEL, David Undie, Gideon Okibe, Omeche Ochepo, Fatima\nMahmud, and Manasseh Ilumunter. Antioxidant and phytochemical classification\nof medicinal plants used in the treatment of cancer disease. Journal of Chemistry\nLetters, pages –, 2024.\n[18] V Sathiya, MS Josephine, and V Jeyabalaraja. An automatic classification and early\ndisease detection technique for herbs plant. Computers and Electrical Engineering,\n100:108026, 2022.\n[19] Karen Simonyan and Andrew Zisserman.\nVery deep convolutional networks for\nlarge-scale image recognition. arXiv 1409.1556, 09 2014.\n[20] Chaoqun Tan, Long Tian, Chunjie Wu, and Ke Li. Rapid identification of medicinal\nplants via visual feature-based deep learning. Plant Methods, 20, 05 2024.\n[21] Jing wei Tan, Siow-Wee Chang, Sameem Abdul-Kareem, Hwa Jen Yap, and Kien-\nThai Yong. Deep learning for plant species classification using leaf vein morpho-\nmetric. IEEE/ACM Transactions on Computational Biology and Bioinformatics,\n17(1):82–90, Jan 2020.\n[22] Mingxing Tan and Quoc V. Le. Efficientnet: Rethinking model scaling for convo-\nlutional neural networks. ArXiv, abs/1905.11946, 2019.\n[23] Vaibhav Tiwari, Rakesh Chandra Joshi, and Malay Kishore Dutta. Deep neural\nnetwork for multi-class classification of medicinal plant leaves. Expert Systems, 39,\n05 2022.\n[24] Vibhor Vishnoi, Krishan Kumar, and Brajesh Kumar. A comprehensive study of\nfeature extraction techniques for plant leaf disease detection. Multimedia Tools and\nApplications, 80, 01 2022.\n[25] Meng Xu, Kuan Huang, Qiuxiao Chen, and Xiaojun Qi. Mssa-net: Multi-scale\nself-attention network for breast ultrasound image segmentation. pages 827–831,\n04 2021.\n28\n\n[26] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention\ngenerative adversarial networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov,\neditors, Proceedings of the 36th International Conference on Machine Learning,\nvolume 97 of Proceedings of Machine Learning Research, pages 7354–7363. PMLR,\n09–15 Jun 2019.\n29\n",
  "metadata": {
    "source_path": "papers/arxiv/A_novel_approach_to_navigate_the_taxonomic_hierarchy_to_address_the\n__Open-World_Scenarios_in_Medicinal_Plant_Classification_471a2a3a7515c710.pdf",
    "content_hash": "471a2a3a7515c71024013eb1d0ad6df36c666c2a392dc5dbb9e60a17e2620489",
    "arxiv_id": null,
    "title": "A_novel_approach_to_navigate_the_taxonomic_hierarchy_to_address_the\n__Open-World_Scenarios_in_Medicinal_Plant_Classification_471a2a3a7515c710",
    "author": "",
    "creation_date": "D:20250225025856Z",
    "published": "2025-02-25T02:58:56",
    "pages": 29,
    "size": 2086861,
    "file_mtime": 1740470164.7857625
  }
}