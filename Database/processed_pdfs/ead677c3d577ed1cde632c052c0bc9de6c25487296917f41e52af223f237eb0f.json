{
  "text": "REVEALING AND MITIGATING OVER-ATTENTION IN\nKNOWLEDGE EDITING\nPinzheng Wang\nZecheng Tang\nKeyan Zhou\nJuntao Li∗\nQiaoming Zhu\nMin Zhang\nSoochow University\n{pzwang1,zctang,kyzhou}@stu.sud.edu.cn\n{ljt,qmzhu,minzhang}@suda.edu.cn\n Code: https://github.com/PinzhengWang322/Reveal_Attention_Drift\nABSTRACT\nLarge Language Models (LLMs) have demonstrated superior performance across\na wide range of tasks, but they still exhibit undesirable errors due to incorrect\nknowledge learned from the training data. To avoid this, knowledge editing methods\nemerged to precisely edit the specific model knowledge via efficiently modifying\na very small percentage of parameters. However, those methods can lead to the\nproblem of Specificity Failure, where the existing knowledge and capabilities are\nseverely degraded due to editing. Our preliminary indicates that Specificity Failure\nprimarily stems from the model’s attention heads assigning excessive attention\nscores to entities related to the edited knowledge, thereby unduly focusing on\nspecific snippets within the context, which we denote as the Attention Drift\nphenomenon. To mitigate such Attention Drift issue, we introduce a simple yet\neffective method Selective Attention Drift Restriction (SADR), which introduces\nan additional regularization term during the knowledge editing process to restrict\nchanges in the attention weight distribution, thereby preventing undue focus on the\nedited entity. Experiments on five frequently used strong LLMs demonstrate the\neffectiveness of our method, where SADR can significantly mitigate Specificity\nFailure in the predominant knowledge editing tasks.\n1\nINTRODUCTION\nLarge language models (LLMs) have demonstrated outstanding performance on various downstream\nnatural language processing tasks, e.g., dialogue generation (Ni et al., 2023; Yang et al., 2024a),\nattributed to the numerous inherent knowledge (Roberts et al., 2020; Cao et al., 2023). However,\nmany unpredictable errors inevitably arise due to the model’s inner defect, which stems from negative\nor outdated samples within the extensive pre-training datasets (Balachandran et al., 2022). These\nimperfections can lead to the propagation of misinformation or biased outputs (Li et al., 2023a; Tang\net al., 2023), undermining the reliability and robustness of the models in real-world applications.\nOne straightforward way to mitigate such an issue is to modify the knowledge of LLMs by directly\nfine-tuning the model with specific data. However, such a direct training method is uncontrollable\nand carries a significant risk of over-fitting (Kirkpatrick et al., 2017; Zhu et al., 2020). Consequently,\nknowledge editing methods (Meng et al., 2022; 2023; Mitchell et al., 2021) aim to efficiently\nmodify specific knowledge within a limited subset of parameters while theoretically ensuring that\nthe rest of the model’s knowledge remains unchanged. However, when employing the knowledge\nediting methods, the instability feature limits their potential (Hoelscher-Obermaier et al., 2023).\nSpecifically, we find that when content related to the edited knowledge appears in the context, it\ncan inadvertently corrupt pre-existing knowledge, which we define as Specificity Failure in this\npaper. For instance, a language model, edited with a new knowledge—“Eiffel Towel” is in “New\nYork” rather than “Eiffel Towel” is in “Paris” (Figure 1(a) and (b)), tends to predict “Pyramids” is in\n“New York” (Figure 1(c)), which is inconsistent with the original knowledge embedded in the model\nbefore editing, i.e., “Pyramids” is in “Egypt”. Based on our preliminary study, we observe that a 6B\nGPT-J model (Wang & Komatsuzaki, 2021) after the knowledge editing can exhibit severe Specificity\nFailure in over 50% of cases regarding factual statements.\n∗Corresponding author\n1\narXiv:2502.14838v1  [cs.CL]  20 Feb 2025\n\nlocated \nis\nin\nTower\nEiffel\nis\nTower\nEiffel\nfamous\n.\nPyramids\nis\nin\nAfter Edit:\nNew York\nParis\n: Activated MLP for editing\n: Hidden states\n(b) Factual recall with Edited Model\n(c) Failed Test Case when Edited Subject Appears First\n: Hidden states directly affected by activated MLPs\n: Attention edges, the darker the color,the greater the value\nAfter Edit:\nNew York\nEgypt\nBefore Edit:\nNew York\nEgypt\n                        (Eiffel Towel, location, Paris)\n                                                 \n                      (Eiffel Towel, location, New York)\nEdit \nlocated \nis\nin\nTower\nEiffel\nBefore Edit:\nNew York\nParis\n(a) Mechanism of Factual Recall\n: MLP\n: ATTN\nEdit request:\nFigure 1: An illustration of counterfactual knowledge editing, where the new factual association\n(Eiffel Tower, is located in, New York) is edited in GPT-J-6b using the ROME method (Meng et al.,\n2022). (a) The hidden states of the subject are enriched by the MLP with relevant information and are\nsuccessfully retrieved by the attention modules. (b) The editing method modifies the MLP parameters\nto alter the factual association. (c) The edited MLP generates hidden states that are prone to being\nmistakenly focused on by the attention modules, leading to specificity failure.\nTo delve deeper, the essence of the aforementioned issue fundamentally lies in the transmission of\nerroneous information flows that occur during the model’s knowledge association recall process (Geva\net al., 2023), where the model’s prediction of certain knowledge can be viewed as the construction of\na causal graph (Meng et al., 2022). Consequently, to ascertain the cause of Specificity Failure, we\nemploy a lens experiment to scrutinize the causal graph within the model’s generative process. Our\nobservations reveal that the output attention activation at the last token position markedly contaminates\nthe forward pass of the edited model, thereby resulting in erroneous outputs. Intuitively, the attention\nerrors in the edited model could stem from the attention module’s potential to erroneously focus on\nedited information, thus overlooking other pertinent details throughout the prediction process. By\nconducting the significance analysis and patching experiments on the attention module within the\nedited model, we observe an Attention Drift phenomenon: the edited model assigns excessive\nattention scores to the entities related to the edited knowledge, thereby overly concentrating\non the specific snippet within the context. Consequently, this leads to outputs that predominantly\nalign with the entities associated with the edited knowledge rather than conforming to the contextual\nsemantics, even at the risk of clashing with pre-existing knowledge inherent in the model.\nTo alleviate the Attention Drift phenomenon, we propose a simple yet efficient strategy called\nSelective Attention Drift Restriction (SADR), which prevents excessive editing by constraining\npartial attention heads that overly focus on entities related to the edited knowledge. Specifically, we\nfirst locate the attention heads that exhibit severe Attention Drift phenomenon by comparing the\nmodel attention outputs before and after the editing, and then align the attention outputs of these\nidentified heads to closely approximate those observed before editing. Currently, knowledge editing\nmethods can be categorized into three types (Yao et al., 2023): locate-then-edit, parameter-preserving,\nand meta-learning approaches. We observe that severe specificity failure exists across methods from\nall three categories, with the accuracy of the original knowledge decreasing by more than half. Our\nproposed SADR can significantly mitigate specificity failure on five models ranging from 1.1B to\n20B, and five editing methods covering all three categories, achieving improvements of up to 130.9%\nand 295.8% in the two main specificity tasks, with only a minimal 0.19% decrease in edit success.\n2\nNOTATION AND BACKGROUND\n2.1\nDEFINITION AND EVALUATION OF KNOWLEDGE EDITING\nPrevious works (Dai et al., 2021; Meng et al., 2022; 2023) represent factual associations as a\nknowledge tuple t = (s, r, o), where s is the subject, r is the relation, and o is the object. To evaluate\n2\n\nwhether a factual association (e.g., Eiffel Tower, is located in, Paris) is captured in a LLM, we provide\na prompt consisting of s and r (e.g., “Eiffel Tower is located in”) and evaluate the model’s prediction\nof o. Knowledge editing aims to replace the factual association stored in model parameters with a\nnew factual association (s, r, oedit), where oedit is the counterfactual target object (e.g., New York).\nIn contrast, we denote otrue as the true answer in the real world. Existing works (Meng et al., 2022;\n2023; Mitchell et al., 2021; Tan et al., 2023) mainly evaluate knowledge editing in terms of reliability,\ngeneralization and specifictiy with the following metrics:\n1) Efficacy Score (ES) and Efficacy Magnitude (EM) represent the portion of cases that\nP(oedit|s, r) > P(otrue|s, r) and the mean P(oedit|s, r), respectively.\n2) Paraphrase Score (PS) and Paraphrase Magnitude (PM) evaluate generalization performance for\na paraphrased prompt (spara, rpara) (e.g., “The Tower of Eiffel stands in”). These two metrics\ncan be written as P(oedit|spara, rpara) > P(otrue|spara, rpara) and P(oedit|spara, rpara).\n3) Neighborhood Score (NS) and Neighborhood Magnitude (NM) evaluate specificity by providing a\nneighboring but distinct subject s′ (e.g., “The Louvre”). These two metrics can be represented as\nP(otrue|s′, r) > P(oedit|s′, r) and P(otrue|s′, r), respectively.\nAs recent studies (Hoelscher-Obermaier et al., 2023; Yao et al., 2023) show that the presence of\nedited subjects in the inference context can deteriorate specificity performance, we incorporate two\nadditional specificity metrics that include the edited subject in the test prompts.\n1) Relation Score (RS) and Relation Magnitude (RM) evaluate how the model handles attributes of the\nedited subject that are unrelated to the edit, identified by the relation r′. Empirically, we find the\nedited model tends to predict the object oedit for even unrelated relations, e.g., “The color of Eiffel\nTower is New York”. Therefore, we calculate the Relation Score by P(otrue|s, r′) > P(oedit|s, r′)\nand the Relation Magnitude by P(otrue|s, r′).\n2) Distract Neighborhood Score (DNS) and Distract Neighborhood Magnitude (DNM)) function\nsimilarly to NS and NM but concatenate the edited sentence (s, r, oedit) before the test prompt\nin the neighborhood task. These metrics can be written as P(otrue|(s, r, oedit) ⊕(s′, r)) >\nP(oedit|(s, r, oedit) ⊕(s′, r)) and P(otrue|(s, r, oedit) ⊕(s′, r)), respectively.\nCounterFact Example\nRequest Editing: (Eiffel Tower, is located in,\nParis) →(Eiffel Tower, is located in, New York)\nEditing Prompt: Eiffel Tower is located in\nEditing Target: New York\nEfficacy Task (ES): Eiffel Tower is located in\nParapharse Task (PS): Eiffel Tower stands in\nNeighborhood Task (NS): The Louvre Museum\nis located in\nRelation Task (RS): The color of Eiffel Tower is\nDistract Neighborhood Task (DNS): Eiffel\nTower is located in New York. The Louvre Mu-\nseum is located in\nFigure 2:\nExample of evaluation tasks for\nKnowledge Editing.\nWe provide an example in Figure 2 to illustrate\nhow to evaluate knowledge editing.\nAlthough\nknowledge editing involves various settings, such\nas batch (Meng et al., 2023) and sequential edit-\ning (Hartvigsen et al., 2024), we find that even\nediting a single factual association can significantly\ndamage specificity performance when the edited\nsubject occurs in the context. We call this Speci-\nficity Failure and focus on knowledge editing that\nmodifies one factual association in this work.\n2.2\nTHE KNOWLEDGE EDITING FRAMEWORK\nTo illustrate how knowledge editing methods work,\nwe describe ROME (Meng et al., 2022) here as it\nis a foundational method that inspired subsequent\ntechniques such as MEMIT (Meng et al., 2023),\nPMET (Li et al., 2024), and others (Li et al., 2024;\nMa et al., 2023).\nGeva et al. (2021) reveals that MLP layers can serve as two-layer key-value memories, with the\noutput of the first MLP layer serving as k, and the output from the second layer acting as v, thereby\nfacilitating knowledge retrieval about entities. Inspired by this, ROME changes v to facilitate the\nmodel’s prediction of oedit when k is associated with the target subject. Meanwhile, it preserves v\nas much as possible when k is not related to the target subject, ensuring stability across unrelated\ncontexts. To achieve this, ROME implements a rank-one update on weight W of the second MLP\nlayer with these two main objectives:\n3\n\n1 Minimize ∥ˆWk −Wk∥when k is not from the last token of the target subject.\n2 Satisfy ˆWk∗= v∗when k∗corresponds to the output of the last token of the target subject, while\nv∗= arg minz\n\u0010\n−log PG(ml∗\nt :=z) (oedit|s, r) + ωDKL\n\u0010\nP(x|p′)\n\r\rPG(ml∗\nt :=z)(x|p′)\n\u0011\u0011\n.\nHerein, the second objective seeks a vector z that, when substituted as the output of the MLP in\nlayer l at token t (denoted G(ml∗\nt := z)), it will lead the model to predict oedit. t is the last token\nof the subject. The KL divergence term minimizes the distances of predicted distribution between\nthe predicted distributions for the prompt p′ (formatted as “subject is a”) before and after editing,\nwhich controls the essence drift (Meng et al., 2022). ω is denoted as the controlling weight. ROME\nintegrates these objectives by solving a linear system.\nThe general framework of locate-then-edit knowledge editing mentioned above can be viewed as\nfirst optimizing a certain vector v∗that facilitates predicting the new knowledge, then integrating the\nvector v∗into the model’s parameters. We primarily focus on locate-then-edit knowledge editing in\nthis paper, analyzing the causes of specificity failure within this framework. Additionally, we also\nevaluate specificity failure and the effectiveness of our SADR method on other parameter-preserving\nand meta-learning approaches.\n3\nEXPLORATION OF SPECIFICITY FAILURE\nWhile knowledge editing excels at memorizing new knowledge, it still suffers from specificity failure.\nWe first measure these failures on CounterFact benchmarks and then identify which intermediate\noutputs during the edited model’s inference cause incorrect predictions. We further explore the\nprimary triggers for these errors at a granular level and verify our findings by patching attention drift\nto mitigate specificity failure. Our experiments focus on Relation and Distract Neighborhood tasks,\nemploying the widely-used ROME method on the GPT-J-6b model (Wang & Komatsuzaki, 2021).\n3.1\nCALIBRATING SPECIFICITY FAILURE ON COUNTERFACTUAL BENCHMARKS\nTo measure performance on both the Relation and Distract Neighborhood tasks, we employ a dataset\ncomposed of COUNTERFACT (Meng et al., 2022) and WikiDatacounterfact (Zhang et al., 2024a).\nThe dataset includes 1683 factual statements, with more details provided in Appendix C.1.\nTable 1: Editing results on GPT-J, whereas red numbers indicate a significant failure.\nEditor\nRewrite\nGeneralization\nSpecificity\nES ↑\nEM ↑\nPS ↑\nPM ↑\nNS ↑\nNM ↑\nRS ↑\nRM ↑\nDNS ↑\nDNM ↑\nNone\n20.86\n0.64\n17.70\n0.40\n82.43\n6.18\n79.73\n8.83\n61.99\n13.81\nROME\n99.88\n99.39\n99.58\n80.26\n79.45\n6.04\n11.94\n3.29\n30.42\n10.45\nMEMIT\n99.94\n96.79\n99.52\n62.42\n82.52\n10.38\n17.44\n5.36\n30.55\n14.91\nPMET\n99.40\n91.03\n92.67\n54.75\n81.49\n6.22\n27.68\n5.01\n39.79\n12.66\nTable 1 illustrates a significant specificity failure when the edited subject occurs in the context, with\nthe edited model incorrectly outputting the edited object in over 50% of test cases. Furthermore, the\naverage probability of incorrect answers oedit is much greater than that of correct answers otrue, as\n48.4% versus 3.3% for the Relation task and 24.9% versus 10.5% for the Distract Neighborhood task\nwhen editing with ROME. Editing methods such as MEMIT (Meng et al., 2023) and PMET (Li et al.,\n2024) also display significant specificity failure. This failure is further observed across a range of\ndata formats and tasks, with detailed results provided in Appendix E.3.\n3.2\nLOCALIZING SPECIFICITY FAILURE\nIn the forward pass of an autoregressive language model, the flow of information can be viewed as a\ncausal graph (Li et al., 2023b). When a model with L layers predicts based on a prompt containing\nT tokens, each module such as attention modules, MLPs, and transformer blocks, produces T × L\noutputs. Each of these outputs is influenced by prior outputs from earlier layers and preceding token\npositions. Inspired by causal tracing (Meng et al., 2022), we trace across different states in the causal\ngraph to identify which parts contaminate the information flow in specificity failure.\n4\n\nWe first conduct a forward pass on the edited model with test prompts and record outputs from various\nnetwork modules across different layers and token positions. Then, we execute a forward pass with\nthe vanilla model, copying the representations of specific modules from the stored outputs to the\ncorresponding location without altering other computations. We traverse modules within a window\nof k layers for each l-th layer and token position t. We refer to this approach as “Contaminating\nSubstitution” and quantify its impact on the final output probability, formulated as:\nTracing Effect = PG(modulel\nt:=z)(otrue|(s, r, oedit) ⊕(s′, r))−P(otrue|(s, r, oedit) ⊕(s′, r)),\nwhere G(modulel\nt := z) denotes the substitution of z for the output of modules at token t in layer l.\n0\n3\n6\n9\n12\n15\n18\n21\n24\n27\nPrompt token\nFirst edited subject token\nMiddle edited subject token\nLast edited subject token\nFurther token\nPrompt token\nFirst neighbor subject token\nMiddle neighbor subject token\nLast neighbor subject token\nFurther token\nLast token\nAvg Tracing Effect of MLP over 100 cases\n0\n3\n6\n9\n12\n15\n18\n21\n24\n27\nPrompt\n1st sub\nMidd sub\nLast sub\nFurther\nPrompt\n1st sub\nMidd sub\nLast sub\nFurther\nLast\nAvg Tracing Effect of Attn over 100 cases\n0\n3\n6\n9\n12\n15\n18\n21\n24\n27\nPrompt\n1st sub\nMidd sub\nLast sub\nFurther\nPrompt\n1st sub\nMidd sub\nLast sub\nFurther\nLast\nAvg Tracing Effect of hidden states over 100 cases\n0.025\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.03\n0.02\n0.01\n0.00\n0.01\n0.05\n0.04\n0.03\n0.02\n0.01\n0.00\nFigure 3: Visualizing “Contaminating Substitution” when replacing the output of specific modules on\ntest prompts of the Distract Neighborhood task with window size 6.\nAs shown in Figure 3, the light-colored areas represent the primary states that lead to incorrect\nanswers. We observe that replacing six layers of MLP activations or Attention activations can\ndecrease the probability of a correct answer by up to 4.59% and 3.74%, respectively, while the total\ndecrease caused by the edited model is 5.26%. As we edit the MLP module in the 5th layer, it is\nexpected that the contaminating substitution of MLP activations near the edited layer significantly\ninfluences the final predictions. However, modifying the middle-upper layers of attention activations\nalso has a similar impact on the correct output, suggesting that the primary cause of specificity failure\nis the attention module mishandling the information at the last token due to the edits. The findings\nare consistent on the Relation task, with additional experiments exploring the tracing effects with\nvarying window sizes and prediction probability of oedit detailed in Appendix D.1.\n3.3\nIDENTIFYING ATTENTION DRIFT AS A TRIGGER FOR SPECIFICITY FAILURE\n0\n1\n2\n3\n4\nP22\nl=16\nP\nh DKL(Wl,h ∥W ∗\nl,h)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP(oedit)\nP(oedit) on Distract Neighborhood task\n0\n1\n2\n3\n4\n5\n6\n7\nP23\nl=14\nP\nh DKL(Wl,h ∥W ∗\nl,h)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP(oedit)\nP(oedit) on Relation task\nFigure 4: The correlation between the attention weight drift and P(oedit) is positive on Distract\nNeighborhood (ρ = 0.49; p <1e−5) and Relation (ρ = 0.62; p <1e−5) tasks. The range of l\ncorresponds to the layers where attention activations have a significant impact on the final results.\nAs mentioned above, attention activations are one of the primary causes of specificity failures.\nPrevious studies (Geva et al., 2023; Chen et al., 2024; Geva et al., 2022) have indicated that attention\nmodules in the middle-upper layers extract factual attributes during predictions. This suggests that the\nattention module may mistakenly focus on edited information, thereby neglecting other information\nwhen predicting the final token. Therefore, we quantify the relationship between drift in attention\nweights and failures in the Distract Neighborhood and Relation tasks using the Pearson coefficient.\nGiven that the edited model overestimates the probability of the edited object oedit relative to the\ntrue object otrue, we analyze the correlation between P\nl\nP\nh DKL(Wl,h ∥W ∗\nl,h) and P(oedit). Here,\n5\n\nWl,h and W ∗\nl,h represent the attention weights that the last token attends to previous tokens in layer l\nand head h before and after editing, respectively.\nFigure 4 shows a positive relationship between the drift in attention weights and the probability of\nthe incorrect answer oedit, suggesting that incorrect attention on previous information is a key factor\nfor specificity failures. To further analyze the attention drift from the perspectives of attended tokens\nand heads, it’s natural to figure out the following two questions:\n1. The last token incorrectly allocates attention to previous tokens, leading to specificity failures.\nAmong these previous tokens, which one has a significant impact when attended incorrectly?\n2. Which has a greater impact on the prediction: the excessive localized attention drift of specific\nheads or the cumulative attention drift across all heads?\nTable 2: Linear correlation between factors related\nto attention weights and P(oedit) with p < 10−5.\nTasks\nFactors\nρ\nDistract\nNeighborhood\n1. P\nl maxh Wl,h[s] −W ∗\nl,h[s]\n0.55\n2. P\nl maxh ∥Wl,h[\\s] −W ∗\nl,h[\\s]∥\n0.49\n3. P\nl\nP\nh Wl,h[s] −W ∗\nl,h[s]\n0.43\nRelation\n1. P\nl maxh Wl,h[s] −W ∗\nl,h[s]\n0.53\n2. P\nl maxh ∥Wl,h[\\s] −W ∗\nl,h[\\s]∥\n0.42\n3. P\nl\nP\nh Wl,h[s] −W ∗\nl,h[s]\n0.50\nTo address these questions, we calculate the\nPearson coefficient between various factors and\nP(oedit). Table 2 indicates that the Pearson\ncoefficient for drift on the last token of the\nedited subject s is higher compared to other to-\nkens (Equation 1 vs Equation 2). Furthermore,\nthe largest drift in attention weights among\nheads impacts the final result more than the cu-\nmulative drift across all heads (Equation 1 vs\nEquation 2), suggesting that the excessive atten-\ntion of certain heads to the last subject token\nprimarily triggers specificity failure.\n3.4\nMITIGATING SPECIFICITY FAILURE BY PATCHING ATTENTION DRIFT\nTo further verify the significant impact of attention drift on specificity failure, we quantify the change\nin prediction probability after patching attention weights across various layers. We first conduct a\nforward pass using the vanilla model with prompts from the specificity tasks and store the intermediate\nattention weights. Then, we test the edited model on the same prompts, substituting its attention\nweights with those previously stored.\nFigure 5: Impact of patching attention weights within a window size of 10 on the specificity tasks in\nthe edited model. The performance of the original edited model is represented on the 0-th layer.\nWe find patching attention weights in middle-upper layers can lead to significant improvements in\nspecificity tasks. As shown in Figure 5, patching attention weights in 10 consecutive layers result\nin a relative increase of 28.6% and 739.2% in the probability of the correct answer P(otrue), and\na decrease of 54.0% and 89.6% in the probability of the wrong answer P(oedit) for two specificity\ntasks, respectively. This shows preventing attention drift can effectively mitigate specificity failure.\n3.5\nTAKEAWAYS\nBased on the analysis mentioned above, we can conclude that: (1) the attention activations at the last\ntoken position significantly contaminate the forward pass of the edited model, causing specificity\n6\n\nfailures; (2) the max attention drift at the edited token position among heads is a primary trigger for\nthe incorrect output oedit, and (3) patching attention drift can largely mitigate the specificity failure.\n4\nSELECTIVELY RESTRAINING ATTENTION DRIFT DURING KNOWLEDGE\nEDITING\nAs mentioned in Section 2.2, the optimized value v∗for knowledge editing can be obtained through\ngradient descent based on the following objective:\nL(z) = 1\nN\nN\nX\nj=1\n−log PG(m(l∗)\ni\n:=z)(oedit | xj + p)\n|\n{z\n}\n(a) Maximizing oedit probability\n+ ωDKL\n\u0010\nP(x | p′)\n\r\rPG(m(l∗)\ni′\n:=z)(x | p′)\n\u0011\n|\n{z\n}\n(b) Controlling essence drift\n.\n(1)\nHowever, this objective may cause attention drift that leads to Specificity Failure. To enhance\nspecificity in knowledge editing when optimizing v∗, we introduce the Selective Attention Drift\nRestriction (SADR), which is a regularization term based on Equation 1. It is worth noting that\nSADR dynamically applies constraints to different heads as needed since Transformer models contain\nvarious knowledge-specific attention heads (Wang et al., 2022; Geva et al., 2023) that capture different\nfactual associations. Additionally, SADR is a simple yet efficient method and can be flexibly adapted\nacross various editing methods.\nMore concretely, as excessive attention to the edited subject of certain heads is strongly correlated\nwith Specificity Failures, we apply SADR on heads where the last token overly focuses on the edited\nsubject. We determine which heads to restrain by the following criterion: a head is selected if the\nattention weight attending to the subject’s last token exceeds the maximum attention weight\namong all heads in the vanilla model.\nLet Wl,h(S) be the attention weight from layer l and head h when processing the prompt S,\nW\nG(m(l∗)\ni\n:=z)\nl,h\n(S) be the attention weight from the model that is edited with z, and Ml(S) =\nmaxh Wl,h(S)[−1, s] be the maximum attention weight that the last token attends to the edited\nsubject s among all heads at layer l in the vanilla model. The objective of SADR can be written as:\nLSADR(z) = 1\nN\nN\nX\nj=1\nX\nl\nX\nh∈Hl(Sj)\nDKL\n\u0012\nWl,h(Sj)[−1, :]\n\r\rW\nG(m(l∗)\ni\n:=z)\nl,h\n(Sj)[−1, :]\n\u0013\n,\n(2)\nwhere Hl(Sj) = {h : W\nG(m(l∗)\ni\n:=z)\nl,h\n(Sj)[−1, s] > Ml(Sj)} and Sj = xj ⊕(s, r).\nThus, the optimized value v∗can be obtained by: v∗= arg min (L(z) + γLSADR(z)), where γ is\nthe controlling weight.\n5\nEXPERIMENTS\n5.1\nSETTINGS\nDataset\nDue to the limited availability of datasets that satisfy the required fields for our tasks, we\ncombine COUNTERFACT (Meng et al., 2022) and WikiDatacounterfact (Zhang et al., 2024a) with\n1,683 factual statements as the testing data. The processing details are mentioned in Appendix C.1.\nAdditionally, we extend our experiments to broader datasets, including QA-format and recent\nknowledge editing tasks, as detailed in Appendix E.3. The phenomena of Specificity Failure and the\nperformance of SADR remain consistent across these datasets.\nBaselines & Models\nWe evaluate the performance of our methods on three mainstream locate-\nthen-edit knowledge editing baselines: ROME (Meng et al., 2022), MEMIT (Meng et al., 2023), and\nPMET (Li et al., 2024). Specifically, we focus on knowledge editing with one factual association\nfor all the baselines. We implement our SADR method across three editing baselines on the GPT-J-\n6b (Wang & Komatsuzaki, 2021), Llama3-8b (AI@Meta, 2024) and GPT-NeoX-20b (Black et al.,\n7\n\n2022) models. In Appendix E.1 and E.2, we also conduct SADR on more model variants and editing\nmethods, including parameter-preserving and meta-learning approaches. More details about baseline\nmethods and our implementations can be found in Appendix C.2.\nMetrics\nApart from the metrics mentioned in Section 2.1, we utilize Fluency Score (FL) (Meng\net al., 2022) to evaluate the generation ability of the edited model with prompts related to the edited\nsubject, which is computed by the weighted mean of bi-gram and tri-gram entropies. Results closer\nto that of the vanilla model indicate better performance. To further test the generalization–specificity\ntradeoff, we report the harmonic mean of ES, PS, NS, RS, and DNS as the average score (Avg. S).\nWe also test the model’s commonsense reasoning abilities and its perplexity in language modeling,\nwith results reported in the Appendix E.1.\n5.2\nMAIN RESULTS\nTable 3: Results of our methods across three edit methods. The values for the 95% confidence\nintervals are displayed in parentheses. Bold number indicates better performance and Green number\nindicates a significantly better score with more than 50% relative improvement.\nModel\nEditor\nScore\nRewrite\nGeneralization\nSpecificity\nFluency\nAvg. S ↑\nES ↑\nPS ↑\nNS ↑\nRS ↑\nDNS ↑\nFL\nGPT-J\n(6b)\nNone\n34.43\n20.86(2.0)\n17.70(1.8)\n82.43(1.0)\n79.73(2.1)\n61.99(1.3)\n621.96(0.9)\nROME\n33.56\n99.88(0.2)\n99.58(0.3)\n80.26(1.0)\n11.94(1.7)\n30.42(1.2)\n620.58(1.2)\n+ ours\n57.74\n99.76(0.2)\n96.36(0.9)\n80.86(1.0)\n27.75(2.3)\n49.35(1.3)\n623.00(1.0)\nMEMIT\n51.07\n99.64(0.3)\n95.47(1.0)\n81.42(1.0)\n25.78(2.3)\n38.00(1.2)\n622.71(1.0)\n+ours\n60.23\n99.52(0.3)\n93.21(1.2)\n81.44(1.0)\n34.68(2.5)\n47.35(1.3)\n623.60(0.9)\nPMET\n52.98\n99.40(0.4)\n92.67(1.3)\n81.49(1.0)\n27.68(2.3)\n39.79(1.3)\n621.18(1.1)\n+ours\n59.06\n99.11(0.5)\n89.09(1.5)\n81.44(1.0)\n33.33(2.5)\n47.47(1.3)\n622.18(1.0)\nLlama3\n(8b)\nNone\n19.99\n9.36(1.4)\n9.48(1.4)\n87.17(0.9)\n92.66(1.4)\n64.25(1.2)\n617.19(1.3)\nROME\n58.54\n99.88(0.2)\n99.52(0.3)\n82.19(1.0)\n29.38(2.4)\n52.21(1.3)\n617.23(1.8)\n+ ours\n70.99\n99.82(0.2)\n96.90(0.8)\n83.18(1.0)\n47.67(2.6)\n58.51(1.3)\n618.39(1.7)\nMEMIT\n40.90\n99.94(0.1)\n99.52(0.3)\n82.52(1.0)\n17.44(2.0)\n30.55(1.2)\n605.99(4.3)\n+ours\n63.12\n99.82(0.2)\n98.87(0.5)\n82.96(1.0)\n35.38(2.5)\n45.57(1.3)\n618.38(1.9)\nPMET\n43.87\n99.58(0.3)\n99.40(0.4)\n81.10(1.0)\n19.84(2.1)\n32.12(1.2)\n610.65(3.4)\n+ours\n61.88\n99.28(0.4)\n97.02(0.8)\n82.86(1.0)\n34.68(2.5)\n51.22(1.3)\n617.91(1.6)\nGPT-NeoX\n(20b)\nNone\n31.87\n17.04(1.8)\n17.64(1.8)\n80.62(1.0)\n83.76(1.9)\n58.34(1.3)\n619.25(0.9)\nROME\n23.45\n99.94(0.1)\n98.75(0.5)\n72.67(1.1)\n15.11(1.9)\n8.84(0.7)\n579.82(6.6)\n+ ours\n54.81\n99.76(0.2)\n96.13(0.9)\n73.96(1.1)\n34.89(2.5)\n34.95(1.2)\n619.54(1.1)\nMEMIT\n40.32\n99.88(0.2)\n90.46(1.4)\n77.45(1.1)\n35.24(2.5)\n16.22(0.9)\n615.19(2.2)\n+ours\n45.26\n97.38(0.8)\n89.21(1.5)\n77.45(1.1)\n45.48(2.6)\n18.49(1.0)\n621.59(0.8)\nPMET\n19.97\n99.52(0.3)\n95.23(1.0)\n74.18(1.1)\n12.29(1.7)\n7.41(0.7)\n510.25(9.8)\n+ours\n31.16\n99.40(0.4)\n93.09(1.2)\n75.33(1.1)\n24.86(2.3)\n11.61(0.8)\n589.76(5.5)\nSpecificity Failure is prevalent in existing knowledge editing methods.\nAs shown in Table 4, the\nRelation and Distract Neighborhood tasks exhibit a significant decline across all editing methods,\neven though these methods perform edits at different layers and modules. Specifically, in the Relation\ntask, the accuracy of all edited models drops to less than half of their original performance. This\nindicates that specificity failure is a widespread and severe issue in knowledge editing.\nSADR significantly mitigates Specificity Failure.\nIn the Relation and Distract Neighborhood\ntasks, SADR consistently improves the specificity across all editing methods. Notably, in over half of\nthe setups, our method enhances the original specificity metrics by more than 50% (marked in green).\nSADR also stabilizes the performance in the Neighborhood task and the Fluency of generated text,\ne.g., when using SADR on ROME and PMET with GPT-NeoX, the Fluency score is significantly\nimproved. SADR can also achieve better performance on TinyLlama-1.1b (Zhang et al., 2024b) and\nLlama2-13b (Touvron et al., 2023) compared with baselines, indicating that our method is effective\nacross models with various sizes and knowledge densities. We provide more detailed results in\nAppendix E.1 and report the human evaluation results in Appendix E.4.\n8\n\nSADR has minimal impact on edit success rates.\nSADR results in less than a 3% decrease in\nperformance on the Rewrite and Generation tasks. It is important to note that significantly mitigating\nspecificity failure while fully preserving rewrite and generalization performance is quiet difficult. This\nis because in many previous model editing evaluation frameworks, the specificity failure highlighted\nin our paper is often overlooked. As a result, prior methods tend to prioritize generalization and\nrewrite scores, neglecting the risks of specificity failure. Under such evaluation criteria, high scores\ncan be achieved by simply identifying the subject and greedily predicting the object, even if the\nrelationship between them is completely ignored.\nWe believe that ensuring stable knowledge editing is more important than achieving nearly 100%\naccuracy in generalization. In real-world scenarios, methods that achieve 97% generalization with\nstable and safe edits are often more acceptable than those with 100% generalization but significant\nspecificity failures (e.g., severe knowledge errors caused by attention drift after editing the subject).\nFurthermore, while addressing specificity failure without compromising generalization is challenging,\nwe demonstrate a better balance between these two aspects in Section 6.2.\n6\nABLATION STUDY\n6.1\nEFFECT OF RESTRAINING HEADS SELECTION\n100\n200\n300\n400\n500\n600\n700\n800\n97.0\n97.5\n98.0\n98.5\nGeneralization\nEffect of  on Edit Success\nw/ head selection\nw/o head selection\n100\n200\n300\n400\n500\n600\n700\n800\n51.0\n51.5\n52.0\n52.5\nSpecificity\nEffect of  on Specificity\nw/ head selection\nw/o head selection\nFigure 6: Impact of selective head restriction on\nEdit Success and Specificity performance\nWe first explore the effects of selectively re-\nstraining heads that exhibit significant attention\ndrift compared to restraining all heads across\nvarious control weights γ on ROME with GPT-\nJ. Edit Success is quantified by the average of\nPS and ES, while Specificity is calculated as\nthe average of NS, RS, and DNS. As shown in\nFigure 6, selectively restraining heads that over-\nfocus on the edited token outperforms restrain-\ning all heads on both the Edit Success and Speci-\nficity across different γ settings. This suggests\nthat not all drift in attention is harmful; rather,\nit is excessive attention compared to the vanilla\nmodel that should be addressed.\n6.2\nTRADE-OFF BETWEEN GENERALIZATION AND SPECIFICITY\n70.0\n72.5\n75.0\n77.5\n80.0\n82.5\n85.0\n87.5\nEdit Success\n11.5\n12.0\n12.5\n13.0\n13.5\n14.0\n14.5\n15.0\nP(otrue)\nDistract Neighborhood task\n70.0\n72.5\n75.0\n77.5\n80.0\n82.5\n85.0\n87.5\nEdit Success\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\nRelation task\ndifferent  with ADR\ndifferent steps\ndifferent learning rate\ndifferent \nFigure 7: Analysis of trade-offs by adjusting different hyperparameters.\nKnowledge editing methods present a trade-off between Edit Success and Specificity, which can be\nvisualized through adjustments in hyperparameters such as optimization steps, learning rate, and ω\nwhich controlling the essence drift. We analyze the trade-off of our method by varying γ and compare\nit with the trade-offs from adjusting other hyperparameters in the original ROME on GPT-J. As\nchanges are more clear on P(oedit) than the proportion of cases which P(oedit) > P(otrue), we use\nthe average of EM and PM to measure Edit Success and apply RM and DNM for evaluating Relation\nand Distract Neighborhood task, respectively. Further details are illustrated in Appendix C.3.\n9\n\nFigure 7 shows that our method exhibits a superior trade-off compared to the adjustments of hyperpa-\nrameters in the original ROME method, indicating that SADR can obtain v∗that enable the model to\nmore effectively distinguish when to output the edited knowledge.\n7\nRELATED WORK\nKnowledge editing.\nThe field of knowledge editing has recently emerged, aiming to modify\nmodel knowledge at a low cost without adversely affecting performance. The methods can be\ncategorized into three main paradigms: parameter-preserving, locate-then-edit, and meta-learning\napproaches (Yao et al., 2023; Wang et al., 2023; Mazzia et al., 2023). Parameter-preserving methods\nexplicitly store modified knowledge in memory and use techniques such as classifiers (Mitchell et al.,\n2022), prompt engineering (Madaan et al., 2022; Zhong et al., 2023; Zheng et al., 2023), or external\nparameters (Dong et al., 2022; Huang et al., 2023; Hartvigsen et al., 2024; Wang et al., 2024b) to\nretrieve the knowledge. locate-then-edit methods update specific parameters by identifying where the\ntargeted knowledge is stored and directly editing those locations (Meng et al., 2022; 2023; Li et al.,\n2024; Ma et al., 2023). Meta-learning approaches involve training a hypernetwork to edit the model’s\nknowledge parameters (Mitchell et al., 2021; Tan et al., 2023).\nDespite the promising prospects of knowledge editing, significant challenges remain in specificity and\ngeneralization. Editing specific pieces of knowledge can lead to ripple effects within the knowledge\ngraph (Wang et al., 2024a; Cohen et al., 2024), but the edited model may be prone to under-editing (Li\net al., 2023d; Pinter & Elhadad, 2023) or over-editing (Li et al., 2023c) in response to these changes.\nRecent works (Gu et al., 2024; Hazra et al., 2024; Yang et al., 2024b) also find that knowledge\nediting may impair general abilities as the number of edits increases. While an ideal edited model\nshould generalize the impacts of edits within a knowledge graph and safely conduct large-scale\nmodifications, Hoelscher-Obermaier et al. (2023) and Rosati et al. (2024) identify a more pressing\nissue in knowledge editing: the performance of the model drops dramatically when the edited subject\nor sentence appears in the context. This motivates us to focus on this issue in the current paper.\nMechanisms of Factual Associations in Transformers.\nTransformer (Vaswani et al., 2017) is\nthe most commonly used architecture for large language models, achieving remarkable performance\nattributed to the vast amount of knowledge stored in its parameters (Petroni et al., 2019; Roberts\net al., 2020; Cao et al., 2023). Significant efforts have been made to uncover the mechanism of how\ntransformers memorize and retrieve knowledge during training and inference. Geva et al. (2021) view\nthe FFN layers as the primary module that stores knowledge in a key-value format. Dai et al. (2021)\nand Geva et al. (2022) explore the manipulation of factual associations by intensifying or attenuating\nthe values in the activated outputs of the first FFN layer. Meng et al. (2022) employ causal tracing\nto demonstrate the crucial role that early MLP layers play in factual recall. Hase et al. (2023) find\nthat editing on layers where knowledge is not primarily stored can also achieve a high success rate,\nindicating that it is possible to “override” the information in layer l with an edit to another layer k.\nAdditionally, Hao et al. (2021) find that the self-attention module performs attribute extraction during\nfactual association recall. Building on the knowledge recall mechanisms identified in previous works,\nwe explore the reasons behind Specificity Failure and design our approach to mitigate attention drift.\n8\nCONCLUSION\nIn this paper, we explore and mitigate the limitations of current knowledge editing methods in Large\nLanguage Models (LLMs) when the edited content is present in the context. Our investigations reveal\nthe Specificity Failure issue that the attention mechanisms in these models overly focus on the edited\nentities, leading to a neglect of other relevant information in the context. Based on the preliminary\nstudy, such a Specificity Failure issue stems from the Attention Drift phenomenon: the edited model\nassigns excessive attention scores to the entities related to the edited knowledge, thereby overly\nconcentrating on specific snippets within the context. Thus, based on the previous knowledge editing\nmethods, we propose the Selective Attention Drift Restriction (SADR) method, which introduces a\nregularization term during the editing process, dynamically constraining the weight of partial selected\nattention heads and preventing excessive focus on the edited entities. Our experiment indicates that\nSADR can significantly reduce Specificity Failures while preserving high rates of editing success.\n10\n\nREPRODUCIBILITY STATEMENT\nOur work is based on open-source models and datasets. In Section 5 and Appendix C, we provide\ndetailed descriptions of data processing, method implementation, and ablation experiments. Addi-\ntionally, in the supplementary materials, we include the complete code for our method as well as the\nprocessed datasets.\nACKNOWLEDGMENTS\nWe want to thank all the anonymous reviewers for their valuable comments. This work was supported\nby the National Science Foundation of China (NSFC No. 62206194), the Natural Science Foundation\nof Jiangsu Province, China (Grant No. BK20220488), the Young Elite Scientists Sponsorship Program\nby CAST (2023QNRC001), the Key Laboratory of Data Intelligence and Advanced Computing in\nProvincial Universities, Soochow University, and the Priority Academic Program Development of\nJiangsu Higher Education Institutions.\nREFERENCES\nAI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/\nblob/main/MODEL_CARD.md.\nVidhisha Balachandran, Hannaneh Hajishirzi, William Cohen, and Yulia Tsvetkov. Correcting diverse\nfactual errors in abstractive summarization via post-editing and language model infilling. In\nEmpirical Methods in Natural Language Processing, 2022.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical\ncommonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence,\nvolume 34, pp. 7432–7439, 2020.\nSidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding,\nHorace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth,\nShivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPT-NeoX-\n20B: An open-source autoregressive language model. In Angela Fan, Suzana Ilic, Thomas Wolf,\nand Matthias Gallé (eds.), Proceedings of BigScience Episode #5 – Workshop on Challenges\n& Perspectives in Creating Large Language Models, pp. 95–136, virtual+Dublin, May 2022.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2022.bigscience-1.9. URL https:\n//aclanthology.org/2022.bigscience-1.9.\nBoxi Cao, Qiaoyu Tang, Hongyu Lin, Xianpei Han, Jiawei Chen, Tianshu Wang, and Le Sun.\nRetentive or forgetful? diving into the knowledge memorizing mechanism of language models.\narXiv preprint arXiv:2305.09144, 2023.\nYuheng Chen, Pengfei Cao, Yubo Chen, Kang Liu, and Jun Zhao. Journey to the center of the\nknowledge neurons: Discoveries of language-independent knowledge neurons and degenerate\nknowledge neurons. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38,\npp. 17817–17825, 2024.\nRoi Cohen, Eden Biran, Ori Yoran, Amir Globerson, and Mor Geva. Evaluating the ripple effects\nof knowledge editing in language models. Transactions of the Association for Computational\nLinguistics, 12:283–298, 2024.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in\npretrained transformers. arXiv preprint arXiv:2104.08696, 2021.\nQingxiu Dong, Damai Dai, Yifan Song, Jingjing Xu, Zhifang Sui, and Lei Li. Calibrating factual\nknowledge in pretrained language models. In Findings of the Association for Computational\nLinguistics: EMNLP 2022, pp. 5937–5947, 2022.\nJunfeng Fang, Houcheng Jiang, Kun Wang, Yunshan Ma, Xiang Wang, Xiangnan He, and Tat-seng\nChua. Alphaedit: Null-space constrained knowledge editing for language models. arXiv preprint\narXiv:2410.02355, 2024.\n11\n\nMor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are\nkey-value memories. In Proceedings of the 2021 Conference on Empirical Methods in Natural\nLanguage Processing, pp. 5484–5495, 2021.\nMor Geva, Avi Caciularu, Kevin Wang, and Yoav Goldberg. Transformer feed-forward layers build\npredictions by promoting concepts in the vocabulary space. In Proceedings of the 2022 Conference\non Empirical Methods in Natural Language Processing, pp. 30–45, 2022.\nMor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. Dissecting recall of factual\nassociations in auto-regressive language models. arXiv preprint arXiv:2304.14767, 2023.\nAlbert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv\npreprint arXiv:2312.00752, 2023.\nJia-Chen Gu, Hao-Xiang Xu, Jun-Yu Ma, Pan Lu, Zhen-Hua Ling, Kai-Wei Chang, and Nanyun Peng.\nModel editing can hurt general abilities of large language models. arXiv preprint arXiv:2401.04700,\n2024.\nYaru Hao, Li Dong, Furu Wei, and Ke Xu. Self-attention attribution: Interpreting information\ninteractions inside transformer. In Proceedings of the AAAI Conference on Artificial Intelligence,\nvolume 35, pp. 12963–12971, 2021.\nTom Hartvigsen, Swami Sankaranarayanan, Hamid Palangi, Yoon Kim, and Marzyeh Ghassemi.\nAging with grace: Lifelong model editing with discrete key-value adaptors. Advances in Neural\nInformation Processing Systems, 36, 2024.\nPeter Hase, Mohit Bansal, Been Kim, and Asma Ghandeharioun. Does localization inform edit-\ning? surprising differences in causality-based localization vs. knowledge editing in language\nmodels. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.),\nAdvances in Neural Information Processing Systems, volume 36, pp. 17643–17668. Curran Asso-\nciates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/\n2023/file/3927bbdcf0e8d1fa8aa23c26f358a281-Paper-Conference.pdf.\nRima Hazra, Sayan Layek, Somnath Banerjee, and Soujanya Poria. Sowing the wind, reaping the\nwhirlwind: The impact of editing language models. arXiv preprint arXiv:2401.10647, 2024.\nJason Hoelscher-Obermaier, Julia Persson, Esben Kran, Ioannis Konstas, and Fazl Barez. Detecting\nedit failures in large language models: An improved specificity benchmark. arXiv preprint\narXiv:2305.17553, 2023.\nZeyu Huang, Yikang Shen, Xiaofeng Zhang, Jie Zhou, Wenge Rong, and Zhang Xiong. Transformer-\npatcher: One mistake worth one neuron. arXiv preprint arXiv:2301.09785, 2023.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A\nRusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming\ncatastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114\n(13):3521–3526, 2017.\nHaoran Li, Dadi Guo, Wei Fan, Mingshi Xu, Jie Huang, Fanpu Meng, and Yangqiu Song. Multi-\nstep jailbreaking privacy attacks on chatgpt. In Findings of the Association for Computational\nLinguistics: EMNLP 2023, pp. 4138–4153, 2023a.\nJiahang Li, Taoyu Chen, and Yuanli Wang. Trace and edit relation associations in gpt. arXiv preprint\narXiv:2401.02976, 2023b.\nXiaopeng Li, Shasha Li, Shezheng Song, Jing Yang, Jun Ma, and Jie Yu. Pmet: Precise model editing\nin a transformer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp.\n18564–18572, 2024.\nZhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, and Huajun Chen. Unveiling the\npitfalls of knowledge editing for large language models. arXiv preprint arXiv:2310.02129, 2023c.\n12\n\nZichao Li, Ines Arous, Siva Reddy, and Jackie Chi Kit Cheung. Evaluating dependencies in fact\nediting for language models: Specificity and implication awareness. In Findings of the Association\nfor Computational Linguistics: EMNLP 2023, pp. 7623–7636, 2023d.\nAng Lv, Kaiyi Zhang, Yuhan Chen, Yulong Wang, Lifeng Liu, Ji-Rong Wen, Jian Xie, and Rui\nYan. Interpreting key mechanisms of factual recall in transformer-based language models. arXiv\npreprint arXiv:2403.19521, 2024.\nJun-Yu Ma, Jia-Chen Gu, Zhen-Hua Ling, Quan Liu, and Cong Liu. Untying the reversal curse via\nbidirectional language model editing. arXiv preprint arXiv:2310.10322, 2023.\nJun-Yu Ma, Hong Wang, Hao-Xiang Xu, Zhen-Hua Ling, and Jia-Chen Gu. Perturbation-restrained\nsequential model editing, 2024. URL https://arxiv. org/abs/2405.16821, 2024.\nAman Madaan, Niket Tandon, Peter Clark, and Yiming Yang. Memory-assisted prompt editing to\nimprove gpt-3 after deployment. In Proceedings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, pp. 2833–2861, 2022.\nVittorio Mazzia, Alessandro Pedrani, Andrea Caciolai, Kay Rottmann, and Davide Bernardi. A\nsurvey on knowledge editing of neural networks. arXiv preprint arXiv:2310.19704, 2023.\nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual\nassociations in gpt. Advances in Neural Information Processing Systems, 35:17359–17372, 2022.\nKevin Meng, Arnab Sen Sharma, Alex J Andonian, Yonatan Belinkov, and David Bau. Mass-editing\nmemory in a transformer. In The Eleventh International Conference on Learning Representations,\n2023. URL https://openreview.net/forum?id=MkbcAHIYgyS.\nEric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D Manning. Fast model\nediting at scale. arXiv preprint arXiv:2110.11309, 2021.\nEric Mitchell, Charles Lin, Antoine Bosselut, Christopher D Manning, and Chelsea Finn. Memory-\nbased model editing at scale. In International Conference on Machine Learning, pp. 15817–15831.\nPMLR, 2022.\nJinjie Ni, Tom Young, Vlad Pandelea, Fuzhao Xue, and Erik Cambria. Recent advances in deep\nlearning based dialogue systems: A systematic survey. Artificial intelligence review, 56(4):\n3055–3155, 2023.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Processing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP), pp. 2463–2473, 2019.\nYuval Pinter and Michael Elhadad. Emptying the ocean with a spoon: Should we edit models? In\nFindings of the Association for Computational Linguistics: EMNLP 2023, pp. 15164–15172, 2023.\nAdam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the\nparameters of a language model? arXiv preprint arXiv:2002.08910, 2020.\nDomenic Rosati, Robie Gonzales, Jinkun Chen, Xuemin Yu, Melis Erkan, Yahya Kayani,\nSatya Deepika Chavatapalli, Frank Rudzicz, and Hassan Sajjad. Long-form evaluation of model\nediting. arXiv preprint arXiv:2402.09394, 2024.\nChenmien Tan, Ge Zhang, and Jie Fu. Massive editing for large language models via meta learning.\narXiv preprint arXiv:2311.04661, 2023.\nZecheng Tang, Keyan Zhou, Pinzheng Wang, Yuyang Ding, Juntao Li, et al. Detoxify language\nmodel step-by-step. arXiv preprint arXiv:2308.08295, 2023.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation\nand fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n13\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing\nsystems, 30, 2017.\nBen Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.\nhttps://github.com/kingoflolz/mesh-transformer-jax, May 2021.\nJianchen Wang, Zhouhong Gu, Zhuozhi Xiong, Hongwei Feng, and Yanghua Xiao. The missing\npiece in model editing: A deep dive into the hidden damage brought by model editing. arXiv\npreprint arXiv:2403.07825, 2024a.\nKevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Inter-\npretability in the wild: a circuit for indirect object identification in gpt-2 small. arXiv preprint\narXiv:2211.00593, 2022.\nPeng Wang, Zexi Li, Ningyu Zhang, Ziwen Xu, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang,\nand Huajun Chen. Wise: Rethinking the knowledge memory for lifelong model editing of large\nlanguage models. arXiv preprint arXiv:2405.14768, 2024b.\nSong Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, and Jundong Li. Knowledge\nediting for large language models: A survey. ACM Computing Surveys, 2023.\nJingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Shaochen\nZhong, Bing Yin, and Xia Hu. Harnessing the power of llms in practice: A survey on chatgpt and\nbeyond. ACM Transactions on Knowledge Discovery from Data, 18(6):1–32, 2024a.\nWanli Yang, Fei Sun, Xinyu Ma, Xun Liu, Dawei Yin, and Xueqi Cheng. The butterfly effect of model\nediting: Few edits can trigger large language models collapse. arXiv preprint arXiv:2402.09656,\n2024b.\nYunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen,\nand Ningyu Zhang. Editing large language models: Problems, methods, and opportunities. In\nProceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp.\n10222–10240, 2023.\nYunzhi Yao, Ningyu Zhang, Zekun Xi, Mengru Wang, Ziwen Xu, Shumin Deng, and Huajun Chen.\nKnowledge circuits in pretrained transformers. arXiv preprint arXiv:2405.17969, 2024.\nNingyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi,\nShengyu Mao, Jintian Zhang, Yuansheng Ni, et al. A comprehensive study of knowledge editing\nfor large language models. arXiv preprint arXiv:2401.01286, 2024a.\nPeiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small\nlanguage model. arXiv preprint arXiv:2401.02385, 2024b.\nCe Zheng, Lei Li, Qingxiu Dong, Yuxuan Fan, Zhiyong Wu, Jingjing Xu, and Baobao Chang. Can\nwe edit factual knowledge by in-context learning? In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing, pp. 4862–4876, 2023.\nZexuan Zhong, Zhengxuan Wu, Christopher D Manning, Christopher Potts, and Danqi Chen. Mquake:\nAssessing knowledge editing in language models via multi-hop questions. In Proceedings of the\n2023 Conference on Empirical Methods in Natural Language Processing, pp. 15686–15702, 2023.\nChen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li, Felix Yu, and Sanjiv\nKumar. Modifying memories in transformer models. arXiv preprint arXiv:2012.00363, 2020.\n14\n\nA\nLIMITATIONS AND FUTURE WORKS\nIn this section, we outline several limitations of our study that highlight areas for future research and\nimprovement: (1) While our method shows promise, there is still potential for improvement in the\nRelation task. This may be due to the fact that edits can inadvertently erase or obscure other relevant\nknowledge about the subject, thereby affecting the model’s overall performance in understanding and\nmaintaining relations. (2) Our study primarily focused on single factual association edits. This scope\nexcluded more complex scenarios such as batch and sequential editing, which involve multiple edits\neither simultaneously or over a sequence. (3) We focus on knowledge editing on only transformer-\nbased models, omitting models with new architectures (Gu & Dao, 2023). These issues highlight\nimportant avenues for future research and will be explored in subsequent studies to enhance the\nrobustness and applicability of knowledge editing.\nB\nPRELIMINARY OF KNOWLEDGE EDITING\nIn this section, we provide more details of the baselines used in our experiments.\nROME\nAs mentioned in Section 2.2, ROME (Meng et al., 2022) implements rank-one knowledge\nediting by deriving a closed form solution:\nminimize ∥ˆWK −V ∥such that ˆWk∗= v∗\nby setting ˆW = W + (v∗−Wk∗) (C−1k∗)T\n(C−1k∗)T k∗\n,\n(3)\nwhere C = KKT is a constant that estimates the uncentered covariance of k from samples of\nWikipedia text. In order to choose the lookup key k∗for the subject, ROME concatenates different\nprefixes generated by the vanilla model with the sentence of the editing request. Then, ROME records\nthe activations of the first layer of MLP, consider the average value k∗= 1\nN\nPN\nj=1 k(xj + s) as the\nlookup key. The v∗for recalling the fact is obtained by minimizing the objective in Equation 1.\nMEMIT\nIn order to directly update multiple memories in a language model, MEMIT employs\nbatch update and multiple layers update based on ROME. To derive an optimal single-layer update\nthat minimizes the squared error of memorized associations while preserving existing memories, the\nexpanded objective for batch update is defined as:\nW1 ≜arg min\nˆ\nW\n n\nX\ni=1\n\r\r\r ˆWki −mi\n\r\r\r\n2\n+\nn+u\nX\ni=n+1\n\r\r\r ˆWki −mi\n\r\r\r\n2\n!\n,\n(4)\nwhere W1 is the new matrix for the second layer of the FFN. Here, K0 = [k1 | k2 | · · · | kn]\nand M0 = [m1 | m2 | · · · | mn] represent the original keys and memories in the vanilla W l\nout,\nwhile K1 = [kn+1 | kn+2 | · · · | kn+u] and M1 = [mn+1 | mn+2 | · · · | mn+u] are new u factual\nassociations that need to be edited in the model.\nBy solving the linear system, the updated matrix W1 can be formalized as:\nW1 = W l\nout + RKT\n1 (C0 + K1KT\n1 )−1,\n(5)\nwhere C0 is the aggregate statistic over the previously stored keys, computed by a random sample of\ninputs as in ROME.\nIn order to improve the robustness, MEMIT distributes updates evenly across the range of mediating\nlayers R. The approach to obtain k∗and v∗is similar to ROME. MEMIT calculates δl in ascending\nlayer order to prevent the influence of edit layers on subsequent layers. The update process of MEMIT\ncan be represented by Algorithm 1.\n15\n\nAlgorithm 1: The MEMIT Algorithm\nData: Requested edits E = {(si, ri, oi)}, generator G, layers to edit R, stored keys Cl\n0\nResult: Modified generator containing edits from E\n1 for si, ri, oi ∈E do\n// Compute target vectors vi for each memory i\n2\noptimize δi ←PN\nj=1 −log PG(h(l∗)\ni\n+=δi)(o∗| xj + p)\n3\nvi ←h(L)\ni\n+ δi\n4 end\n5 for l ∈R do\n// Perform update over layers in ascending order\n6\nh(l)\ni\n←transformer_block(hl−1\ni\n)\n// Execute layer l using the updated weights\n7\nfor si, ri, oi ∈E do\n8\nkl\ni ←k(l)\ni\n= 1\nN\nPN\nj=1 k(xj + sj)\n9\nrl\ni ←vi−h(L)\ni\nL−l+1\n10\nend\n11\nKl ←[kl1\ni , ..., kL\ni ]\n12\nRl ←[rl1\ni , ..., rL\ni ]\n13\n∆l ←RlKlT (Cl\n0 + KlKlT )−1\n14\nW l ←W l + ∆l\n// Update MLP weights in layer l\n15 end\nPMET\nPMET (Li et al., 2024) discovers that Multi-Head Self-Attention(MHSA) weights do\nnot require updating when new knowledge is introduced, thus only integrating the optimized FFN\nactivation to conduct precise editing. PMET introduces optimizable parameters, δa\ni for the MHSA\noutput and δm\ni for the FFN output at the L-th layer. It then retains only the optimized hidden states of\nthe FFN to update its weights, represented as vm\ni = mL\ni + δm\ni = arg min L(z), where L(z) refers to\nthe objective in Equation 1. Following this, PMET employs the same algorithmic steps as MEMIT to\nupdate the FFN weights.\nC\nIMPLEMENTATION DETAILS\nC.1\nDATASET PROCESSING\nThe dataset we use is a mixture of counterfact datasets from Meng et al. (2022) and Zhang et al.\n(2024a). Meng et al. (2022) introduce COUNTERFACT, which contains 21,919 records featuring\na diverse set of subjects, relations, and linguistic variations. It also provides paraphrase prompts,\nneighborhood prompts, and generation prompts for specificity evaluation. Zhang et al. (2024a) collect\ntriplets about popular entities from top-viewed pages on Wikipedia to construct WikiDatacounterfact.\nThey provide relational prompts to evaluate the impact of edits on other attributes associated with the\nedited subject. We combined these datasets in equal proportions to create a balanced dataset with\n1683 factual statements.\nWhen measuring specificity, it is crucial that the neighborhood subject and the test relationship of\nthe subject remain unaffected by the edit. For example, when editing the factual knowledge tuple\n“(Carl Bosch, citizenship, Germany)” to “(Carl Bosch, citizenship, Canada)”, it might be logically\nconsistent to also generalize Carl Bosch’s birthplace to Canada, which should not be considered in\nspecificity tests. However, this edit should not alter the answer to “The gender of Carl Bosch is”. Test\ncases like this should be considered in specificity tests. To measure specificity more accurately, we\nfilter our dataset using GPT-4. The filtering prompts are detailed in Figs. 8.\nC.2\nBASELINE SETTINGS\nIn this section, we detail the parameters used for each baseline and the SADR method across different\nmodels. We utilize ROME (Meng et al., 2022), MEMIT (Meng et al., 2023), and PMET (Li et al.,\n2024) as the baseline knowledge editing techniques, implemented using EasyEdit 1. As the objective\nof SADR prevents over-editing by restricting optimization from causing large attention drift, it is\n1https://github.com/zjunlp/EasyEdit\n16\n\nPrompts for Filtering Specificity Cases\nRequest Editing: Irma Boom spoke the language →Russian\nTest prompt for neighborhood: Johannes Lingelbach is a native speaker of\nTest prompt for relationship: The place of birth of Irma Boom is\nPrompt for gpt-4 to filter neighborhood:\nPrompt for gpt-4 to filter relationship:\nDetermine the subjects in the following two sen-\ntences is related or unrelated.\nDetermine the factual relationships in the follow-\ning two sentences is related or unrelated:\n- Neighborhood subjects in the same field with-\nout\nSentence 1: {Request Editing}\ndirect collaboration or interaction are considered\nSentence 2: {Test prompt for relationship}\nunrelated.\nExpected answer: Related or Unrelated.\nSentence 1: {Request Editing}\nSentence 2: {Test prompt for neighborhood}\nExpected answer: Related or Unrelated.\nFigure 8: Prompts for gpt-4 to filter specificity cases\nnecessary to increase the number of optimization steps to ensure convergence. We test [20, 40, 80]\noptimization steps with restraining weights γ set at [5e−3, 1e−2, 4e−2, 8e−2] on the validation split.\nAll experiments are conducted on eight NVIDIA A100 (40GB) GPUs, with individual edits taking\napproximately 20 to 80 seconds on a single GPU. Completing all edits and evaluations across our\ndataset requires 1-2 days.\nROME\nThe parameters applied for the original baseline are consistent with the original paper. The\nlearning rate is 0.5, optimization steps are 20, and the KL factor ω is 0.0625 across various models.\nFor GPT-J-6b, we edit layer 5, with optimization steps of 80 and a controlling weight γ = 1e−2 for\nthe SADR method. For Llama3-8B, we edit layer 5, with optimization steps of 80 and a controlling\nweight γ = 4e−2 for the SADR method. For GPT-NeoX-20b, we edit layer 5, with optimization\nsteps of 80 and a controlling weight γ = 1e−2 for the SADR method. For Llama2-13B, we edit\nlayer 15, with optimization steps of 80 and a controlling weight γ = 1e−2 for the SADR method.\nFor TinyLlama, we edit layer 4, with optimization steps of 80 and a controlling weight γ = 8e−2 for\nthe SADR method.\nMEMIT\nThe original baseline applies learning rate of 0.5, 20 optimization steps, and KL factor\nω of 0.0625 across various models. For GPT-J-6b, we edit layers 3-8, with optimization steps of\n20, learning rate 0.25, and a controlling weight γ = 5e−3 for the SADR method. For Llama3-8B,\nwe edit layers 4-8, with optimization steps of 80 and a controlling weight γ = 1e−2 for the SADR\nmethod. For GPT-NeoX-20b, we edit layers 13-16, with optimization steps of 80 and a controlling\nweight γ = 5e−3 for the SADR method. For Llama2-13B, we edit layers 5-9, with optimization\nsteps of 80 and a controlling weight γ = 4e−2 for the SADR method. For TinyLlama, we edit layers\n3-5, with optimization steps of 80 and a controlling weight γ = 8e−2 for the SADR method.\nPMET\nThe original baseline applies learning rate of 0.5, 20 optimization steps, and KL factor ω\nof 0.0625 across various models. For GPT-J-6b, we edit layers 3-8, with optimization steps of 40\nand a controlling weight γ = 8e−2 for the SADR method. For Llama3-8B, we edit layers 4-8, with\noptimization steps of 80 and a controlling weight γ = 1e−2 for the SADR method. For GPT-NeoX-\n20b, we edit layers 13-16, with optimization steps of 20 and a controlling weight γ = 2e−3 for the\nSADR method. For Llama2-13B, we edit layers 7-9, with optimization steps of 20 and a controlling\nweight γ = 5e−3 for the SADR method. For TinyLlama, we edit layers 3-5, with optimization steps\nof 40 and a controlling weight γ = 8e−2 for the SADR method.\nC.3\nABLATION SETTINGS\nWe further illustrate the implementation details for experiments in Section 6. When testing the trade-\noff between generalization and specificity, we randomly sample 500 data points for evaluation. The\ncontrolling weight γ for the SADR method is applied at [1e−5, 1e−4, 5e−4, 2.5e−4, 1e−2, 8e−2].\n17\n\nThe optimization steps are applied at [7, 14, 20, 40, 80, 120].\nThe KL factor ω is applied at\n[0.1, 1, 2, 3, 4, 5]. The learning rate is applied at [5e−2, 6e−2, 8e−2, 1e−1, 2e−1, 4e−1].\nD\nADDITIONAL RESULTS FOR EXPLORING SPECIFICITY FAILURES\nD.1\nLOCALIZE SPECIFICITY FAILURE IN CAUSAL GRAPH\nWe further investigate the tracing effects of “Contaminating Substitution” across different window\nsizes in the “Distract Neighborhood” and “Relation” tasks, and also demonstrate the impact on the\nprediction probability of oedit. As shown in Figure 9a, varying window sizes indicate similar areas\n0\n3\n6\n9\n12\n15\n18\n21\n24\n27\nPrompt token\nFirst edited subject token\nMiddle edited subject token\nLast edited subject token\nFurther token\nPrompt token\nFirst neighbor subject token\nMiddle neighbor subject token\nLast neighbor subject token\nFurther token\nLast token\nAvg Tracing Effect of Attn with window size 3\n0\n3\n6\n9\n12\n15\n18\n21\n24\n27\nPrompt\n1st sub\nMidd sub\nLast sub\nFurther\nPrompt\n1st sub\nMidd sub\nLast sub\nFurther\nLast\nAvg Tracing Effect of Attn with window size 6\n0\n3\n6\n9\n12\n15\n18\n21\n24\n27\nPrompt\n1st sub\nMidd sub\nLast sub\nFurther\nPrompt\n1st sub\nMidd sub\nLast sub\nFurther\nLast\nAvg Tracing Effect of Attn with window size 10\n0.015\n0.010\n0.005\n0.000\n0.005\n0.010\n0.03\n0.02\n0.01\n0.00\n0.01\n0.05\n0.04\n0.03\n0.02\n0.01\n0.00\n(a) Tracing Effect on P(otrue) with different window sizes in the Distract Neighborhood task.\n0\n3\n6\n9\n12\n15\n18\n21\n24\n27\nPrompt token\nFirst edited subject token\nMiddle edited subject token\nLast edited subject token\nFurther token\nPrompt token\nFirst neighbor subject token\nMiddle neighbor subject token\nLast neighbor subject token\nFurther token\nLast token\nAvg Tracing Effect of MLP over 100 cases\n0\n3\n6\n9\n12\n15\n18\n21\n24\n27\nPrompt\n1st sub\nMidd sub\nLast sub\nFurther\nPrompt\n1st sub\nMidd sub\nLast sub\nFurther\nLast\nAvg Tracing Effect of Attn over 100 cases\n0\n3\n6\n9\n12\n15\n18\n21\n24\n27\nPrompt\n1st sub\nMidd sub\nLast sub\nFurther\nPrompt\n1st sub\nMidd sub\nLast sub\nFurther\nLast\nAvg Tracing Effect of hidden states over 100 cases\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\n0.00\n0.02\n0.04\n0.06\n0.08\n0.00\n0.05\n0.10\n0.15\n0.20\n(b) Tracing Effect on P(oedit) in the Distract Neighborhood task with window size 6.\n0\n3\n6\n9\n12\n15\n18\n21\n24\n27\nPrompt token\nFirst edited subject token\nMiddle edited subject token\nLast edited subject token\nFurther token\nLast token\nAvg Tracing Effect of MLP over 100 cases\n0\n3\n6\n9\n12\n15\n18\n21\n24\n27\nPrompt\n1st sub\nMidd sub\nLast sub\nFurther\nLast\nAvg Tracing Effect of Attn over 100 cases\n0\n3\n6\n9\n12\n15\n18\n21\n24\n27\nPrompt\n1st sub\nMidd sub\nLast sub\nFurther\nLast\nAvg Tracing Effect of hidden states over 100 cases\n0.0075\n0.0050\n0.0025\n0.0000\n0.0025\n0.0050\n0.0075\n0.0100\n0.0125\n0.006\n0.004\n0.002\n0.000\n0.002\n0.004\n0.006\n0.008\n0.006\n0.004\n0.002\n0.000\n0.002\n0.004\n0.006\n(c) Tracing Effect on P(otrue) in the Relation task with window size 6.\n0\n3\n6\n9\n12\n15\n18\n21\n24\n27\nPrompt token\nFirst edited subject token\nMiddle edited subject token\nLast edited subject token\nFurther token\nLast token\nAvg Tracing Effect of MLP over 100 cases\n0\n3\n6\n9\n12\n15\n18\n21\n24\n27\nPrompt\n1st sub\nMidd sub\nLast sub\nFurther\nLast\nAvg Tracing Effect of Attn over 100 cases\n0\n3\n6\n9\n12\n15\n18\n21\n24\n27\nPrompt\n1st sub\nMidd sub\nLast sub\nFurther\nLast\nAvg Tracing Effect of hidden states over 100 cases\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n(d) Tracing Effect on P(oedit) in the Relation task with window size 6.\nFigure 9: Visualizing “Contaminating Substitution” with different window sizes, specificity tasks and\nprediction objects.\nleading to Specificity Failures, and the decrease in P(otrue) is correlated with window size. This\nsuggests that contaminating information accumulates at the last token in middle-upper layers due to\nthe recall mechanism of attention modules.\nBy comparing Figure 3 with Figure 9b, and Figure 9c with Figure 9d, we observe notable similarities\nin the areas that correspond to increases in incorrect answer probabilities P(oedit) and decreases in\ncorrect answer probabilities P(otrue). This suggests that the same information flow may be driving\nchanges in both probabilities.\n18\n\nFurthermore, a phenomenon that may seem counterintuitive is that replacing MLP or Attn activations\nin the final layers increases the probability of correct answers. This can be attributed to the disruption\nof anti-overconfidence mechanisms in the final layers (Lv et al., 2024).\nD.2\nPATCHING ATTENTION DRIFT TO MITIGATE SPECIFICITY FAILURE\nIn section 3.4, we demonstrate an improvement in specificity performance after patching attention\ndrift in some consecutive layers. To explore the effectiveness of patching attention drift at a finer\ngranularity, we evaluate the tracing effect of modifying a single value in the attention weight matrix.\nSpecifically, we alter the value that represents the weight of the last token attending to the t-th\ntoken before softmax during the forward pass of the edited model. The replacement value is the\none generated by the vanilla model for the same prompts. Considering residual connections in\nTransformer, we patch for a window of k layers around the l-th layer.\n0\n3\n6\n9\n12\n15\n18\n21\n24\n27\nPrompt token\nFirst edited subject token\nMiddle edited subject token\nLast edited subject token\nFurther token\nPrompt token\nFirst neighbor subject token\nMiddle neighbor subject token\nLast neighbor subject token\nFurther token\nLast token\nAvg Tracing Effect on P(otrue)\n0\n3\n6\n9\n12\n15\n18\n21\n24\n27\nPrompt\n1st sub\nMidd sub\nLast sub\nFurther\nPrompt\n1st sub\nMidd sub\nLast sub\nFurther\nLast\nAvg Tracing Effect on P(oedit)\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.150\n0.125\n0.100\n0.075\n0.050\n0.025\n0.000\n0.025\n(a) Tracing Effect with window size 10 in the Distract Neighborhood task.\n0\n3\n6\n9\n12\n15\n18\n21\n24\n27\nPrompt token\nFirst edited subject token\nMiddle edited subject token\nLast edited subject token\nFurther token\nLast token\nAvg Tracing Effect on P(otrue)\n0\n3\n6\n9\n12\n15\n18\n21\n24\n27\nPrompt\n1st sub\nMidd sub\nLast sub\nFurther\nLast\nAvg Tracing Effect on P(oedit)\n0.000\n0.001\n0.002\n0.003\n0.004\n0.30\n0.25\n0.20\n0.15\n0.10\n0.05\n0.00\n(b) Tracing Effect with window size 10 in the Relation task.\nFigure 10: The tracing effect of patching the attention value that the last token attends to the previous\ntokens for different layers.\nAs shown in Figure 10, patching the attention value of the last edited subject token in the middle-upper\nlayers significantly mitigates the Specificity Failure, where the magnitude of change in probability\nclosely matches the one of replacing the entire attention weight matrix in Figure 5. This aligns with\nthe findings in Section 3.3 that the drift of attention weights from the last token to the edited token is\nthe main trigger for Specificity Failure.\nD.3\nTHE CORRELATION BETWEEN MORE FACTORS AND SPECIFICITY FAILURE\nTable 4: Correlation of different factors with speci-\nficity failure.\nFactor\nPearson Coefficient\n(Distracting Neighborhood)\nPearson Coefficient\n(Relation)\nAttention Drift\n0.49\n0.62\nHidden State Norm\n0.01\n0.31\nL2 Distance\n0.01\n0.31\nCosine Similarity\n0.02\n-0.15\nRecent works (Fang et al., 2024; Yao et al.,\n2024; Ma et al., 2024) point out that the edit\nvector’s direction, space, and norm can influ-\nence the model’s specificity performance. How-\never, these works primarily focus on preserv-\ning general knowledge and capabilities, rather\nthan addressing the specificity failure that arises\nwhen the edited subject appears in the context.\nTo explore the relevance of these factors to the\nspecificity failure problem studied in our work,\nwe conducted a correlation analysis. Specifically, we compared four factors—attention drift, hidden\n19\n\nstate norm post-editing, L2 distance between hidden states pre- and post-editing, and the cosine\nsimilarity of hidden states pre- and post-editing —with the probability of P(oedit) in specificity tasks.\nTable 4 shows that, compared to the direction or norm of the edit vector, attention drift has a more\ndirect and significant impact on specificity failure.\nD.4\nDISCUSSION ABOUT REASONS FOR ATTENTION DRIFT\nExperiments have shown that attention drift is closely related to specificity failure. A natural question\narises: what is the reason for attention drift during the editing process? Intuitively, editing methods\nprimarily modify the hidden states of the edited subject, which subsequently influence the final\noutput through the attention mechanism. In traditional editing methods (e.g., ROME discussed in\nSection 2.2), the optimization objective explicitly trains the model to predict the new oedit given (s, r).\nThis may create a shortcut, where the hidden state of the subject is shaped in a way that makes it\noverly prone to being prioritized by the attention mechanism, thereby hard-coding the knowledge\ninto the forward propagation rather than truly integrating it into the model.\nTo better illustrate this shortcut, we design an experiment using GPT-XL and apply ROME to 100\nediting cases. During the optimization of the model’s target vectors, we employ the “torch.detach()”\nfunction to prevent gradients from propagating through the attention weights. This means that the\nmodel editing process would optimize only the value component in the attention module, ignoring\nthe optimization of key and query components. Under this setup, we observe that ROME’s editing\nsuccess rate (measured by EM) exhibit a polarized trend, where probabilities are either very high or\nvery low, as shown in Figure 11.\nFigure 11: P(oedit) of the edited model when attention weight optimization is disabled.\nIn the original ROME method, the average probability after editing exceeds 95%. This suggests\nthat many facts are challenging to edit into the model without optimizing the attention weights. To\nfurther analyze this phenomenon, we set a threshold of P(oedit) greater than or less than 0.95 to\ndistinguish between easy-to-edit and hard-to-edit knowledge, resulting in a roughly equal number\nof cases in both categories. Subsequently, we compare various performance metrics for the original\nROME method and the modified ROME method with attention weight optimization disabled (referred\nto as ROME-AWD), as shown in Table 5. The Distracting Neighborhood Task is selected as the\nrepresentative metric for specificity.\nThe results indicate that: (1) For easy-to-edit knowledge, disabling the attention weight shortcut\nallows editing methods to achieve satisfactory results in both edit success and specificity; (2) For\nhard-to-edit knowledge, disabling the optimization of attention weights significantly reduces the\nediting success rate, and such knowledge is more prone to specificity failure under original editing\nmethods. Furthermore, we calculate the Pearson correlation coefficient between ROME’s attention\ndrift and the editing difficulty (measured by 1 −P(oedit) on ROME-AWD). The results indicate\n20\n\nTable 5: Comparison of editing performance for easy-to-edit and hard-to-edit knowledge.\nKnowledge Type\nEditor\nRewrite ↑\nGeneralization ↑\nSpecificity ↑\nEasy-to-Edit Knowledge\nNone\n18.0\n20.0\n56.0\nROME\n100.0\n100.0\n16.0\nROME-AWD\n100.0\n91.0\n40.3\nHard-to-Edit Knowledge\nNone\n14.3\n13.6\n54.5\nROME\n98.4\n90.9\n9.0\nROME-AWD\n71.4\n77.2\n37.7\na significant positive correlation, with a Pearson coefficient of 0.748 and a p-value < 0.05. This\nindicates that attention drift is likely a result of editing methods hard-coding the edited knowledge\ninto the model’s forward propagation, rather than enabling a more natural and reasonable assimilation\nof new knowledge.\nE\nADDITIONAL RESULTS ON SELECTIVE ATTENTION DRIFT RESTRICTION\nE.1\nMAIN RESULTS\nTo comprehensively evaluate the effectiveness of our method, we employ two additional frequently\nused models, Llama2-13B (Touvron et al., 2023) and TinyLlama (Zhang et al., 2024b), to observe the\nperformance of our method across different model sizes and advanced knowledge-rich models in this\nsection. To further validate our method’s performance in knowledge editing, we have incorporated\nnew metrics for generalization, specificity, and fluency.\nGeneralization: To ensure the edited knowledge is fully integrated into the model, we use a new\nmetric called the Reasoning Score (RES) (Yao et al., 2023). This metric evaluates the model’s ability\nto perform reasoning based on modified facts, which is more challenging.\nSpecificity: To assess the impact of model editing on other tasks, we follow the approach in Yao et al.\n(2023) and report accuracy on PIQA (Bisk et al., 2020), a multiple-choice commonsense reasoning\ntest. We measure this using the Other Task Score (OS). Additionally, we evaluate how the edited\nknowledge affects related tasks by incorporating the edited sentence in a distraction-based format,\ntermed the Distracted Other Task Score (DOS).\nFluency: We evaluate language modeling on a high-quality text dataset ME-PPL (Yang et al., 2024b),\nwhich includes various commonly used corpora. We use perplexity (PPL) as a measure of the\nlanguage model’s generative capability.\nOur method is effective across various models.\nAs shown in Table 6, consistent with the observa-\ntions in Section 5.2, knowledge editing results in significant specificity failure across models ranging\nfrom 1.1B to 20B parameters, which is mitigated by our SADR method (with over 50% improvement\nin major specificity tasks in more than half of the cases). The improvement in prediction probability\n(as reflected by the RM and DNM metrics) is also evident. Notably, in the Distract Neighborhood\ntask, the probability of correct predictions has been restored to the level of the unedited model,\nshowing the potential of our approach.\nThe edited entity also impacts unrelated knowledge:\nThe OS and DOS metrics show that when\nthe edited entity appears in the context, the performance of tasks entirely unrelated to the entity also\ndegrades. This further highlights the widespread occurrence of specificity failure. In most settings,\nour method shows improvement on the DOS metric.\nThe impact of SADR on editing performance is minimal:\nAs discussed in Section 5.2, mitigating\nspecificity failure without compromising any aspect of editing performance is quite difficult. To\nfurther verify whether SADR hinders the effective integration of new knowledge into the model,\nwe test it on more difficult tasks that require reasoning based on the new knowledge to arrive at\nthe correct answer. The decline in RES metrics with our method is consistently below 3%, and in\n21\n\nTable 6: Results of our methods on five frequently used models with detailed metrics. Bold numbers\nindicate better performance, and Green numbers indicate a significantly better score with more than\n50% relative improvement.\nModel\nEditor\nRewrite\nGeneralization\nSpecificity\nFluency\nES ↑\nEM ↑\nPS ↑\nPM ↑\nRES ↑\nNS ↑\nNM ↑\nRS ↑\nRM ↑\nDNS ↑\nDNM ↑\nOS ↑\nDOS ↑\nFL\nPPL\nTinyLlama\n(1.1b)\nNone\n18.30\n1.60\n23.84\n1.52\n28.75\n76.42\n9.99\n85.66\n15.93\n56.03\n18.32\n73.30\n71.04\n607.84\n50.65\nROME\n93.92\n74.38\n91.24\n51.84\n50.99\n75.50\n9.85\n19.28\n5.11\n47.05\n17.18\n72.94\n70.98\n607.21\n50.64\n+ ours\n93.68\n69.90\n89.81\n40.61\n51.63\n75.41\n9.82\n29.87\n7.38\n50.51\n18.42\n72.82\n71.51\n607.84\n50.67\nMEMIT\n96.90\n71.99\n95.05\n47.73\n48.69\n73.84\n9.62\n22.18\n5.75\n32.64\n13.48\n72.82\n70.20\n605.31\n50.66\n+ours\n95.77\n65.84\n93.09\n40.99\n49.46\n73.82\n9.66\n33.05\n7.78\n41.56\n16.32\n72.71\n71.16\n609.72\n50.64\nPMET\n90.52\n62.21\n85.46\n36.47\n28.97\n75.86\n9.93\n28.67\n7.49\n49.09\n17.75\n72.88\n70.86\n607.88\n50.66\n+ours\n91.54\n55.79\n83.73\n30.03\n49.34\n75.86\n9.94\n36.94\n9.07\n50.87\n18.44\n73.00\n71.39\n608.14\n50.66\nGPT-J\n(6b)\nNone\n20.86\n0.64\n17.70\n0.40\n33.09\n82.43\n6.18\n79.73\n8.83\n61.99\n13.81\n74.73\n74.08\n621.96\n44.12\nROME\n99.88\n99.39\n99.58\n76.93\n52.39\n80.26\n6.04\n11.94\n3.29\n30.43\n10.45\n74.73\n74.37\n620.58\n68.02\n+ ours\n99.76\n99.26\n96.36\n53.73\n50.42\n80.86\n6.11\n27.75\n5.85\n49.35\n14.16\n74.73\n74.49\n623.00\n63.85\nMEMIT\n99.94\n96.79\n99.52\n62.42\n51.43\n82.52\n10.38\n17.44\n5.36\n30.55\n14.91\n74.73\n73.90\n605.99\n44.24\n+ours\n99.82\n94.33\n98.63\n50.53\n49.66\n82.93\n10.52\n35.88\n8.45\n45.09\n20.27\n74.79\n74.31\n619.00\n44.24\nPMET\n99.40\n91.03\n92.67\n54.75\n51.79\n81.49\n6.22\n27.68\n5.01\n39.79\n12.66\n75.09\n73.96\n621.18\n44.24\n+ours\n99.11\n81.39\n89.09\n34.86\n52.03\n81.44\n6.15\n33.33\n5.50\n47.47\n13.69\n75.27\n73.60\n622.18\n44.23\nLlama3\n(8b)\nNone\n9.36\n0.62\n9.48\n0.34\n30.72\n87.17\n11.22\n92.66\n26.99\n64.25\n20.57\n80.63\n79.44\n617.19\n43.07\nROME\n99.88\n98.17\n99.52\n64.06\n60.35\n82.19\n10.26\n29.38\n6.95\n52.21\n20.64\n80.45\n79.08\n617.23\n52.79\n+ ours\n99.82\n98.16\n96.90\n47.03\n59.79\n83.18\n10.44\n47.67\n10.50\n58.51\n22.96\n80.63\n79.26\n618.39\n46.31\nMEMIT\n99.94\n96.79\n99.52\n62.42\n60.64\n82.52\n10.38\n17.44\n5.36\n30.55\n14.91\n80.69\n79.02\n605.99\n66.70\n+ours\n99.82\n94.33\n98.63\n50.53\n62.77\n82.93\n10.52\n35.88\n8.45\n45.09\n20.27\n80.75\n78.96\n619.00\n49.31\nPMET\n99.58\n91.98\n99.40\n56.42\n62.28\n81.10\n10.08\n19.84\n5.34\n32.12\n15.20\n80.27\n78.37\n610.65\n42.97\n+ours\n99.28\n83.99\n97.02\n39.98\n62.65\n82.86\n10.54\n34.68\n7.83\n51.22\n21.50\n80.45\n78.96\n617.91\n42.95\nLlama2\n(13b)\nNone\n10.55\n1.45\n16.15\n1.35\n26.86\n83.54\n16.92\n94.92\n28.70\n62.59\n25.50\n80.04\n78.43\n611.31\n22.46\nROME\n99.76\n98.29\n97.14\n64.23\n54.72\n82.28\n16.63\n34.89\n10.77\n55.67\n24.54\n79.92\n78.37\n611.35\n22.44\n+ ours\n99.76\n98.38\n94.16\n53.38\n51.63\n82.50\n16.72\n56.29\n16.83\n59.47\n26.44\n79.98\n77.95\n611.93\n22.44\nMEMIT\n99.76\n96.51\n57.86\n66.99\n57.86\n80.02\n16.30\n15.89\n6.72\n35.80\n19.02\n79.98\n77.23\n610.87\n22.45\n+ours\n99.58\n96.19\n93.92\n52.29\n54.89\n80.63\n16.46\n48.23\n15.01\n55.39\n25.76\n79.92\n78.67\n613.24\n22.45\nPMET\n99.52\n93.36\n95.53\n56.92\n52.79\n80.80\n16.38\n43.22\n12.83\n43.29\n21.47\n79.92\n78.25\n610.67\n22.44\n+ours\n99.46\n88.36\n92.85\n46.89\n52.31\n80.89\n16.44\n52.05\n14.93\n50.46\n23.91\n79.80\n78.01\n611.18\n22.45\nGPT-NeoX\n(20b)\nNone\n17.04\n0.82\n17.64\n0.50\n31.28\n80.62\n7.52\n83.76\n14.51\n58.34\n5.80\n78.13\n76.46\n619.25\n41.75\nROME\n99.94\n98.95\n98.75\n74.68\n56.74\n72.67\n6.90\n15.11\n4.55\n8.84\n4.64\n77.18\n72.77\n579.82\n66.77\n+ ours\n99.76\n98.37\n96.13\n59.12\n53.80\n73.96\n7.07\n34.89\n7.57\n34.95\n12.76\n77.29\n75.92\n619.54\n66.72\nMEMIT\n99.88\n94.58\n90.46\n44.41\n52.39\n77.45\n7.30\n35.24\n7.84\n16.22\n7.41\n77.06\n74.85\n615.19\n42.72\n+ours\n97.38\n81.49\n89.21\n40.26\n50.10\n77.45\n7.35\n45.48\n8.83\n18.49\n8.21\n76.64\n75.92\n621.59\n42.82\nPMET\n99.52\n84.94\n95.23\n68.01\n56.33\n74.18\n6.91\n12.29\n3.49\n7.41\n2.61\n77.06\n64.72\n510.25\n42.62\n+ours\n99.40\n86.77\n93.09\n55.06\n54.72\n75.33\n7.00\n24.86\n6.15\n11.61\n5.00\n76.34\n71.57\n589.76\n42.69\nsome settings, it even shows slight improvements. This demonstrates that our method can mitigate\nspecificity failure while effectively editing the knowledge.\nE.2\nRESULTS ON MORE EDITING METHODS\nTable 7: Results of our methods on WISE and MEND.\nEditor\nAvg. S ↑\nES ↑\nPS ↑\nNS ↑\nRS ↑\nDNS ↑\nFL\nNone\n34.43\n20.86\n17.70\n82.43\n79.73\n61.99\n621.96\nWISE\n24.57\n100.00\n38.60\n67.12\n25.24\n5.87\n480.50\n+ours\n31.58\n100.00\n35.40\n71.22\n36.44\n12.73\n502.80\nMEND\n15.07\n98.70\n92.30\n11.80\n24.46\n5.40\n551.22\n+ours\n18.80\n95.60\n92.80\n11.90\n39.73\n7.38\n555.98\nKnowledge editing methods can be\ncategorized into three types: locate-\nthen-edit, parameter-preserving, and\nmeta-learning.\nTo further verify\nwhether attention drift is also ev-\nident in parameter-preserving and\nmeta-learning-based editing meth-\nods, we conduct additional exper-\niments on WISE (Wang et al.,\n2024b) and MEND (Mitchell et al.,\n2021). WISE is a recent parameter-\npreserving method for sequence editing that includes side memory and gating mechanisms, while\nMEND is a classic knowledge editing method utilizing meta-learning. Specifically, we add a loss term\nto constrain attention drift during the training of the side memory in WISE and the hyper-parameter\nnetwork in MEND. The results, shown in Table 7, indicate that specificity failure is evident in both\nmethods, and imposing attention constraints significantly improves their performance.\nE.3\nRESULTS ON MORE DATASETS\nDue to the limited availability of datasets that meet the required fields for our tasks, we conducted\nexperiments on a relatively small dataset with 1,683 pieces of data from COUNTERFACT (Meng et al.,\n2022) and WikiDatacounterfact (Yao et al., 2023). To better illustrate the specificity failure problem\nand validate the effectiveness of our approach across a wider range of data formats and entities, we\n22\n\nexpand our experiments to include two extensive datasets. The missing tasks in the following results\nare due to the absence of relevant fields in the dataset.\nTable 8: Results of our methods on more datasets.\nCOUNTERFACT\nEditor\nScore\nES\nPS\nNS\nDNS\nFL\nNone\n27.45\n16.36\n17.68\n82.87\n62.74\n622.13\nROME\n59.88\n99.93\n99.29\n78.45\n29.44\n620.13\n+ours\n74.82\n99.86\n96.36\n79.99\n48.62\n623.39\nZsre + Wikirecent\nEditor\nScore\nES\nPS\nNS\nRS\nDNS\nNone\n53.30\n53.00\n54.27\n73.45\n68.19\n35.42\nROME\n42.41\n99.96\n97.79\n76.54\n18.81\n31.83\n+ours\n55.81\n99.96\n95.38\n76.78\n34.59\n36.81\nFirst, we use the full COUNTER-\nFACT (Meng et al., 2022) dataset,\nwhich includes 21,919 records (12\ntimes larger than our original dataset),\nincluding 20,391 subjects, 749 ob-\njects, and 645 relations.\nWe also\napply Zsre (Meng et al., 2022) and\nWikirecent (Yao et al., 2023), which\nincludes data in a Q&A format and re-\ncent knowledge data from Wikipedia,\nwith a total of 2,532 records. The re-\nsults in Table 8 demonstrate the ro-\nbustness and effectiveness of our ap-\nproach when applied to larger and\nmore diverse datasets.\nE.4\nHUMAN EVALUATION\nWe compare the performance of three\nknowledge editing baselines with and without our SADR method on GPT-J and Llama3-8b in human\nevaluation. We provide the edited model with prompts composed of (s, r) for text generation,\nrestricting output to a maximum of 100 tokens. For each setting, we randomly sample 20 comparison\npairs and hire nine annotators to give their preferences (win, loss, and tie) for three evaluation criteria:\nEdit Success, Specificity, and Fluency. We show the statistics of human evaluation data in Tabel 9\nand human evaluation interface in Figure 12 and 13. To ensure consistency among the annotators,\nwe report the Fleiss’ kappa score and we can observe that all the inter-annotator agreements are\nsubstantially consistent (κ ∈[0.6, 1]). The results presented show that our methods outperform the\noriginal baselines in Specificity and Fluency while maintaining performance in Edit Success.\nWe build the human evaluation interface with the open-source python web library Django 2. As\nshown in Figure 13, during the evaluation, each comparison pair contains the editing request and two\ncorresponding outputs generated from two edited models with and without our SADR method. The\nannotator is allowed to choose \"Tie\" if it is hard to distinguish two generation cases. We can ensure\nthat each annotator is independent during their annotation process and the total annotation process is\nfair. We paid each annotator $ 0.05 for comparing each pair. The payment is reasonable, considering\nthat it would take an average of 30-60 seconds for an annotator to finish a comparison.\nTable 9: Human evaluation results on three tracks (Specificity, Edit Success and Fluency), where ζ\ndenotes Fleiss’ kappa.\nMetrics\nKnowledge Editing Baselines\nWin(%)\nLoss(%)\nTie(%)\nζ\nV.S. ROME\nSpecificity\n28.33\n17.50\n54.17\n73.96\nEdit Success\n25.83\n19.72\n54.45\n76.71\nFluency\n51.67\n32.78\n15.55\n72.14\nV.S. PMET\nSpecificity\n37.78\n31.67\n30.55\n73.74\nEdit Success\n35.28\n39.17\n25.55\n69.09\nFluency\n50.28\n27.78\n21.94\n66.76\nV.S. MEMIT\nSpecificity\n37.22\n24.72\n38.06\n76.46\nEdit Success\n21.94\n12.23\n65.83\n64.44\nFluency\n47.78\n35.28\n16.94\n68.03\n2https://www.djangoproject.com\n23\n\nFigure 12: Example of one comparison pair in the human evaluation website.\nFigure 13: Interface of human evaluation website.\n24\n\nE.5\nABLATION STUDY ON RESTRAINING WEIGHT\nThe hyper-parameters of our method primarily include the controlling weight γ. In this section, we\npresent the ablation study of the effect of controlling weight. We conduct the ablation study on GPT-J\nwith ROME in this part and randomly sample 500 data points for evaluation. The results of adjusting\nthe hyper-parameter γ are reported in Table 10. We observe that larger γ slightly improves specificity\nwhile keeping other metrics almost unchanged. This indicates that our method is not sensitive to γ,\nas we only restrain heads that over-focus on the edited token compared to the vanilla model.\nTable 10: The influence of restraining weight γ\nEditor\nγ\nRewrite\nGeneralization\nSpecificity\nFluency\nES ↑\nPS ↑\nNS ↑\nRS ↑\nDNS ↑\nFL\nNone\n-\n20.86\n17.70\n82.43\n79.73\n61.99\n621.96\nROME\n-\n100.00\n99.00\n78.83\n13.17\n31.53\n618.85\n+ SADR\n5e−3\n99.80\n95.60\n79.55\n33.83\n49.08\n622.24\n+ SADR\n2e−2\n99.80\n95.20\n79.68\n34.13\n49.93\n622.47\n+ SADR\n4e−2\n99.80\n95.60\n79.75\n34.13\n50.66\n623.02\n+ SADR\n8e−2\n99.80\n96.00\n79.68\n34.13\n50.79\n621.92\nF\nEFFICIENCY ANALYSIS\nIn terms of memory usage, the additional variables to store in our method are the attention weights\nacross all layers. These weights can be represented as L × H × S2, where L is the number of layers\nin the model, H is the number of attention heads, and S is the sequence length. The additional storage\nrequired is minimal compared to the overall model parameters. During our experiments, we did not\nobserve any noticeable increase in GPU memory usage.\nRegarding runtime, our method primarily involves computing a mask through comparison of attention\nweights and calculating the KL divergence. However, due to the use of Python loops in our current\nimplementation, a slight runtime overhead is observed. For instance, when applying the ROME\nediting method to GPT-J-6B on an A100-PCIE-40GB GPU, the runtime per edit increased from 7.80\nseconds (without SADR) to 9.65 seconds (with SADR).\nG\nETHICAL CONSIDERATIONS\nOur goal in improving knowledge editing performance is to correct errors and update the knowledge\nin LLMs. It is important to notice that knowledge editing techniques can also be used to generate\ntoxic and harmful content. We advocate for the responsible use of knowledge editing techniques to\nenhance model behavior rather than for malicious purposes.\n25\n\nH\nCASE STUDY\nIn this section, we present the results generated by our method in comparison with the original method\nusing ROME on GPT-J-6b.\nTable 11: Generated results of ROME with GPT-J-6b when given prompts containing the edited\nsubject and relation.\nCase\nType\nText\n1\nRequest\n(DeWayne Wise, play the position, outfielder) →(DeWayne Wise, play the position, forward)\nPrompt\nDeWayne Wise is incredible at\nROME\nDeWayne Wise is incredible at sport and at school in his native Poland.\nPolish international. Born on July 11, 1986 in Sosnowiec. Son of Zbigniew and Barbara. The\nheight 5ft 10 (180 cm), the weight 170 lb (77 kg). In 2004, he began playing in the Polish\nleague. In 2006-2007 season played for the Polish club S˘0301mig˘0142y-Go˘0301rniczo. In\n2007\n+ Ours\nDeWayne Wise is incredible at what he does. A 6-foot-9 forward, he’s a\nphysical specimen with a great work ethic. He has the size, strength and athleticism to be a\ndominant force in this league. The problem is, it’s not easy to get him to play defense. The\nKnicks’ front office and coaches are trying to figure out a way to get the best out of him.\nHe’s been a major\n2\nRequest\n(Craig Monroe, play as, goaltender) →(Craig Monroe, play as, outfielder)\nPrompt\nCraig Monroe’s greatest strength is\nROME\nCraig Monroe’s greatest strength is her ability to stay in control of her\nbody while under extreme duress. She can’t do this if she can’t breathe. So in 2010, when a\nfreakish viral infection left her unable to breathe on her own, the then-21-year-old Canadian\nOlympic gold medalist was given a tracheotomy to help with the breathing problem.\nShe’s been on the ventilator ever since, which means she spends the majority of her waking hours\n+ Ours\nCraig Monroe’s greatest strength is that he’s the most naturally\ngifted goaltender in the world. That’s the conclusion we reached after watching a few of his\ngames, including the one that earned him the Vezina Trophy last year. The only thing that’s\nmissing is that we’re not the only ones who think so. The Hockey News ranks all 31 NHL goalies,\nfrom the best of the rest to the elite. Here are the Top 10 goalies in the league\n3\nRequest\n(Irma Boom, speak, Dutch) →(Irma Boom, speak, French)\nPrompt\nIrma Boom’s mother tongue is\nROME\nIrma Boom´s mother tongue is English. Irma´s father´s family name is\n\"Lambert\" and his mother´s family name is \"Lambert\". Irma´s mother´s first name is\n\"Marianne\". Her mother´s maiden name is \"Lambert\" and her mother´s second husband´s surname\nis \"Lambert\". Irma´s father´s occupation is \"Clerk\" and his mother´s occupation is\n\"Clerk\n+ Ours\nIrma Boom’s mother tongue is Spanish, her father tongue is French\nand her second language is English. She was born in Barcelona, Spain, on July 3, 1952.\nShe was raised in France, and then moved to the United States at the age of 17. She\ngraduated from the University of Southern California with a degree in journalism in 1975.\nAfter college, she began a career in television as a reporter for NBC and CBS in Los Angeles,\nand then as an anchor for the Spanish-language\n26\n\nTable 12: Generated results of ROME with GPT-J-6b when given prompts from the Distract Neigh-\nborhood Task.\nCase\nType\nText\n1\nRequest\n(Sonny Simmons, perform, jazz) →(Sonny Simmons, perform, poetry)\nPrompt\nSonny Simmons performs poetry. Zeena Parkins plays\nGPT-J-6b\nSonny Simmons performs poetry. Zeena Parkins plays guitar. And together, the duo\nknown as the Shins are on their way to becoming the best rock band to come out of the Pacific\nNorthwest. \"The Shins are the best band ever,\" says Simmons, whose band´s debut album Port Of Morrow was\nreleased in March and whose latest release, the double-disc The Liberty Or The Great Divide, is due\nout in October. \"They´re the only band that can really\nROME\nSonny Simmons performs poetry. Zeena Parkins plays the piano. They are musicians in a\nband. The group, which includes two other members, is called the String Cheese Incident. The group has\nreleased four albums. The group is known for its jam-band style of music. The String Cheese Incident\nhas performed for over two million people. The band has played at festivals including Bonnaroo and the\nOutside Lands in San Francisco, the Governor’s Ball in New York City\n+ Ours\nSonny Simmons performs poetry. Zeena Parkins plays guitar and sings. They both live\nin New York City and they are both black. Sonny and Zeena are two of a new breed of black artists, a\ngeneration of musicians who have been raised on hip-hop music and culture. They have both spent time as\nteenagers in the Bronx and Brooklyn, and have grown up to be part of a musical scene that has become a\nforce in music today. Sonny\n2\nRequest\n(Akademi Fantasia, was created in, Malaysia) →(Akademi Fantasia, was created in, France)\nPrompt\nAkademi Fantasia was created in the country of France. Maria Mariana II, formulated in\nGPT-J-6b\nAkademi Fantasia was created in the country of France. Maria Mariana II, formulated\nin the year of 1874 by a group of French writers, was the first winner of the contest and the\nfirst to be crowned as \"The Queen of Music\", a title which she still holds. The contest is a musical\ncompetition for singers, dancers and instrumentalists, which is held every year. It was created in 1874\nand is still going on. It is a contest that is held in the\nROME\nAkademi Fantasia was created in the country of France. Maria Mariana II, formulated\nin the French Academy, was born in the city of Paris, France. She is also a citizen of the country of\nFrance, and the world. The Academie is a private school, and Maria Mariana II is a graduate. Maria\nMariana II is a French national, but her citizenship is also a citizen of the country of France. The\nFrench Academy, or Academie,\n+ Ours\nAkademi Fantasia was created in the country of France. Maria Mariana II,\nformulated in 1852, is the most famous and the most expensive of the Academicians of the country. History\nThe Academie des Beaux Arts de France was created at the end of the 17th century, in the French\nRevolution, as a result of the French government’s need for an art school for artists. The first Director\nof the school was the sculptor Jean-Antoine\n3\nRequest\n(Vicente Engonga, play as, midfielder) →(Vicente Engonga, play as, goaltender)\nPrompt\nWhich position does Vicente Engonga play? They play as goaltender. Radoslaw Kaluzny, the\nGPT-J-6b\nWhich position does Vicente Engonga play? They play as goaltender. Radoslaw\nKaluzny, the coach of the team, said that they will try to use the goalkeeper in the offensive line.\nVicente Engonga, the goalkeeper of the team What is the name of your club? \"The White Eagles\", because the\neagle is the national animal of the Polish people. What was the name of your club before? \"The White Eagles\nROME\nWhich position does Vicente Engonga play? They play as goaltender. Radoslaw Kaluzny,\nthe former goaltender of the Kamloops Blazers, is the current goaltender coach of the Kamloops\nBlazers. He also coaches the Kamloops Blazers. What are the rules in a curling game played on ice? The\ncurling game is played on ice. The object of the game is to slide the curling rock (the stone) down a\nsloped\n+ Ours\nWhich position does Vicente Engonga play? They play as goaltender. Radoslaw\nKaluzzny, the head coach of HK Poprad of the Slovak Extraliga, said that he will not take into\nconsideration Vicente Engonga’s injury and the team’s need. “I do not think that the situation will\naffect the decision on who will play as goaltender,” he said. “We will not change our plan and\n27\n",
  "metadata": {
    "source_path": "papers/arxiv/Revealing_and_Mitigating_Over-Attention_in_Knowledge_Editing_ead677c3d577ed1c.pdf",
    "content_hash": "ead677c3d577ed1cde632c052c0bc9de6c25487296917f41e52af223f237eb0f",
    "arxiv_id": null,
    "title": "Revealing_and_Mitigating_Over-Attention_in_Knowledge_Editing_ead677c3d577ed1c",
    "author": "",
    "creation_date": "D:20250221020427Z",
    "published": "2025-02-21T02:04:27",
    "pages": 27,
    "size": 2794964,
    "file_mtime": 1740346981.761735
  }
}