{
  "text": "JUREX-4E: Juridical Expert-Annotated Four-Element Knowledge Base for\nLegal Reasoning\nHuanghai Liu1*, Quzhe Huang*, Qingjing Chen3, Yiran Hu1,\nJiayu Ma1, Yun Liu1, Weixing Shen1†, Yansong Feng2†\n1School of Law, Tsinghua University\n2Wangxuan Institute of Computer Technology, Peking University\n3Department of Legal Studies, University of Bologna\n{liuhh23,chenqj21,huyr21,ma-jy24}@mails.tsinghua.edu.cn\n{huangquzhe,fengyansong}@pku.edu.cn {liuyun89,wxshen}@mail.tsinghua.edu.cn\nAbstract\nThe Four-Element Theory is a fundamental\nframework in criminal law, defining the consti-\ntution of crime through four dimensions: Sub-\nject, Object, Subjective aspect, and Objective\naspect. This theory is widely referenced in legal\nreasoning, and many Large Language Models\n(LLMs) attempt to incorporate it when han-\ndling legal tasks. However, current approaches\nrely on LLMs’ internal knowledge to incorpo-\nrate this theory, often lacking completeness and\nrepresentativeness. To address this limitation,\nwe introduce JUREX-4E, an expert-annotated\nknowledge base covering 155 criminal charges.\nIt is structured through a progressive hierarchi-\ncal annotation framework that prioritizes legal\nsource validity and employs diverse legal in-\nterpretation methods to ensure comprehensive-\nness and authority. We evaluate JUREX-4E on\nthe Similar Charge Distinction task and apply\nit to Legal Case Retrieval, demonstrating its\neffectiveness in improving LLM performance.\nExperimental results validate the high quality\nof JUREX-4E and its substantial impact on\ndownstream legal tasks, underscoring its poten-\ntial for advancing legal AI applications. Code:\nhttps://github.com/THUlawtech/JUREX\n1\nIntroduction\nIn legal AI tasks, enhancing the accuracy and inter-\npretability of Large Language Models (LLMs) in\nthe legal domain often requires the incorporation of\nlegal theories as a support (Jiang and Yang, 2023;\nServantez et al., 2024; Yuan et al., 2024; Deng et al.,\n2023). One important theory is the Four-Element\nTheory of Crime Constitution in Chinese criminal\nlaw (Liang, 2017). This theory deconstructs crim-\ninal conduct into four elements: Subject, Object,\nSubjective aspect, and Objective aspect, providing\nclear standards for judicial authorities to determine\n*These authors contributed equally to this work.\n†Corresponding Author.\ncriminal behavior and helping to prevent the abuse\nof penal power.\nHowever, most current approaches do not pro-\nvide additional knowledge but rather rely on the\nLLM’s internal knowledge to incorporate the Four-\nElement Theory. A common method is to guide\nLLMs in mimicking expert reasoning processes.\nFor example, designing four separate prompts to\nguide the LLM outputs in the form of four ele-\nments(Deng et al., 2023).\nThese methods assume that the model has a solid\ngrasp of the Four-Element Theory, which has not\nyet been verified. We had LLMs generate the four\nelements of several complicated crimes in Chinese\njudicial practice(Ouyang et al., 1999), and then\nasked legal experts to score them. We found that,\nalthough LLMs can generate formally standardized\nand relatively accurate legal descriptions when pro-\nvided with legal theoretical frameworks and refer-\nences, the model still underperformed in terms of\ncompleteness and representativeness. This short-\ncoming could affect the accuracy and soundness of\nsubsequent reasoning.\nTo help LLMs better utilize the Four-Element\nTheory in legal tasks, we propose JUREX-4E:\nJURidical EXpert-annotated 4-Element knowl-\nedge base for legal reasoning. This knowledge\nbase is annotated using a progressive hierarchy: Ar-\nticle →Judicial Interpretations →Guiding Cases\n→Academic Discourses, which is built upon the\npyramid structure of legal source validity. It in-\ncorporates various legal interpretation methods, in-\ncluding textual, systematic, sociological, and pur-\nposive interpretations. The knowledge base covers\nthe four elements of 155 high frequency charges,\nannotated by legal experts over a period of seven\nmonths. Each crime’s four elements are described\nin an average of 472.5 words.\nTo assess the quality of the annotations, we sam-\npled several crimes for human evaluation. The ex-\npert annotations achieved an average score of 4.60\narXiv:2502.17166v1  [cs.CL]  24 Feb 2025\n\non a 5-point scale, while the LLM-generated four\nelements scored only 3.96, indicating that the ex-\npert annotations were of higher quality. To further\nevaluate the annotations objectively and compre-\nhensively, a direct way is to judge whether different\ncharges can be distinguished according to the four-\nelement definition of crime constitution. There-\nfore, we introduced the Similar Charge Distinction\ntask (Liu et al., 2021). For each case, we provided\nthe four elements of the candidate confused charges\nand combined them with the case facts as model\ninput. The experimental results showed that in-\njecting expert annotations helped the model better\ndifferentiate between similar charges, improving\nperformance with a 0.65 increase in average ac-\ncuracy and a 0.70 increase in average F1-score,\nunderscoring the superior quality and reliability of\nexpert annotations compared to those generated by\nthe LLM.\nWe also applied the expert annotations in a spe-\ncific legal task: Legal Case Retrieval. It is an\nimportant step in the practice of analyzing cases\nand making judgments, requiring the precise ap-\nplication of the four-element theory to compare\nthe criminal composition of cases. We designed a\nsimple retrieval framework guided by expert knowl-\nedge, in which the charge’s four elements was used\nto generate four-element descriptions for both the\nquery and candidate cases, and then match similar\ncases based on their vector similarity. Experiments\ndemonstrated that incorporating expert-annotated\nfour elements improved retrieval performance, as\nthe model became better at focusing on the legal\nfeatures and key details.\nOur contributions are as follows:\n(1) We verify that LLMs have gaps in understand-\ning the legal theory, highlighting the inade-\nquacy of relying solely on LLM-driven rea-\nsoning for legal AI tasks.\n(2) We built the JUREX-4E knowledge base,\nwhich is the first to incorporate the pyramid\nstructure of legal source validity and covers the\nfour elements of 155 criminal charges under\nChinese Criminal Law.\n(3) We demonstrated the significance of incorpo-\nrating criminal composition elements in the\nSimilar Charge Distinction task and proved the\nsuperior quality of the expert-annotated four-\nelement knowledge base.\n(4) We applied JUREX-4E to the Legal Case Re-\ntrieval task, found that they do indeed con-\ntribute to downstream tasks.\n2\nRelated Work\nIn legal AI, much work has introduced legal theo-\nries to enhance reasoning and improve model ac-\ncuracy and interpretability. For example, legal syl-\nlogism prompting (LoT)(Jiang and Yang, 2023)\nteaches LLMs for legal judgment prediction by in-\nstructing legal syllogism, Chain of Logic(Servantez\net al., 2024) guides models in reasoning about\ncompositional rules by decomposing logical state-\nments based on the IRAC (Issue, Rule, Application,\nConclusion) paradigm. Among these, the Four-\nElements Theory (FET) of Crime Constitution is a\nwidely adopted framework(Yuan et al., 2024; Deng\net al., 2023).\nThe Four-Element Theory is one of the most\nwidely recognized criminal theories in Chinese ju-\ndicial practice (Liang, 2017). It specifies four es-\nsential elements that must be satisfied to establish\ncriminal liability: Subject, Object, Subjective as-\npect, and Objective aspect. For example, the four\nelements of the Crime of Affray can be briefly sum-\nmarized as follows:\n(1) Subject: Principal organizers and other active\nparticipants who have reached the age of criminal\nresponsibility. (2) Object: Public order. (3) Objec-\ntive Aspect: The act of assembling brawl, engaging\nin a brawl, resulting in the following consequences\nof serious injury. (4) Subjective Aspect: Direct\nintent, where the person knowingly and willfully\nengages in organizing or participating in the act of\nassembling brawl.\nBefore discussing the Four-Element Theory\n(FET), it is necessary to briefly compare it with\nanother key theory in Chinese criminal law, the\nHierarchical Theory of Crime Constitution(Zhou,\n2017b; Zhang, 2010), and the main distinction be-\ntween these theories lies in whether a hierarchi-\ncal structure is considered, with ongoing debates\nin practice(Gao, 2009; Chen, 2010, 2017; Zhou,\n2017a). We chose FET as our foundational tem-\nplate for following reasons: 1) its dominance in\nChinese judicial practice aligns with real-world\ncriminal judgments; (2) its clear distinction be-\ntween objective aspects and subjective intent of-\nfers direct reasoning checkpoints compared to the\nThree-Tier Theory; (3) its four-element annotation\nis flexible and can be adapted to the Three-Tier\nTheory by prioritizing objective analysis before\nsubjective evaluation(Li, 2006; Zhang, 2017).\nRecent approaches have leveraged the FET\nframework to model expert reasoning. For exam-\n\nFigure 1: Hierarchical Legal Interpretation System base on legal source validity. The legal sources follow a\nhierarchical order of validity. Thick arrows indicate the primary level where a particular interpretive method is\napplied, while dashed arrows represent its supplementary use at that level.\nple, breaking down legal rules into FET-aligned\ncomponents using automated planning techniques\n(Yuan et al., 2024). Employing model-generated\nfour-element structures as minor premises in legal\njudgment analysis (Deng et al., 2023). While these\nmethods have demonstrated improved performance\non downstream tasks, they generally assume that\nthe LLMs inherently understand the FET, without\nsystematically validating this assumption. Notably,\nprior research on criminal charge prediction (An\net al., 2022) suggests that the models may misin-\nterpret key legal concepts and may not be sensitive\nenough to the subtle differences in fact descrip-\ntions of confusing charges, highlighting the need\nto incorporate expert annotations to support LLM\nreasoning.\n3\nDataset Construction\n3.1\nHierarchical Legal Interpretation System\nAnnotating the four elements of crime constitu-\ntion is essentially a process of legal interpretation,\nwhich can be analyzed in two aspects:\n(1) What law is being interpreted. This in-\nvolves identifying the sources of law, including\nstatutory provisions corresponding to a specific\ncharge, their associated judicial interpretations,\ncase precedents, and academic discourses. In legal\nstudies, these sources are categorized based on their\nlegal validity into formal sources (which carry legal\nforces in judgments) and informal sources (which\nserve as references without legal forces)(Pound,\n1925; Watson, 1982; Pound, 1932). Articles and ju-\ndicial interpretations are considered formal sources,\nwhereas case precedents and academic discourses\nare regarded as informal sources under the Chinese\nlegal system(Zhang and Zhou, 2007).\n(2) How the law is interpreted. This pertains\nto legal interpretation methods, including literal\ninterpretation, systematic interpretation, purposive\ninterpretation, etc. These methods follow a hierar-\nchical order in legal reasoning(Sutherland, 1891;\nKim and Division, 2008; Eig Larry, 2014). Legal\ninterpretation should begin with literal interpreta-\ntion (textual analysis). If the intended meaning\ncannot be clearly derived from the article alone,\nsystematic interpretation and purpose interpreta-\ntion should be applied first. If ambiguity remains,\nhistorical interpretation and comparative law inter-\npretation may be used to further clarify the legal\nmeaning. The specific definition is in AppendixB.\nBased on these principles, our annotation fol-\nlows a pyramid structure of Hierarchical Legal In-\nterpretation System base on legal source validity.\nAs shown in Figure 1, the system is divided into\ntwo parts: Legal Source and Legal Interpretation\nMethods. The main structure of legal source fol-\nlows a hierarchical order of validity: Article →\nJudicial Interpretations →Guiding Cases →Aca-\ndemic Discourses, where various legal interpreta-\ntion methods are applied across different levels.\nThick arrows indicate the primary level at which a\nparticular method is used, while thin arrows denote\nthe cross applications.\n3.2\nHierarchical Annotation Path of Legal\nSources\nOur Annotators are experts who have all passed\nthe National Uniform Legal Profession Qualifica-\ntion Examination and are familiar with the Four-\nElement Theory. The entire annotation process\ntook a total of 7 months and involved 4 rounds of\nannotation according to the validity of the legal\n\nsource from high to low level.\nThe First Level: Article.\nLegal elements can\nbe seen as an interpretation and refinement of the\nstatutory provisions corresponding to a particular\ncrime. Using literal interpretation as the primary\nmethod, the statute is broken down based on its\nsemantic meaning and common usage, ensuring\nthat the interpretation does not extend beyond the\npossible meaning of the text: (1) linguistic analy-\nsis follows the subject-predicate-object structure of\nthe provision. (2) To maintain consistency, terms\nare systematically classified and mapped(e.g: sub-\njective aspect is classified as either intentional or\nnegligent. )(3) Only when it is impossible to make\nan explicit inclusion or exclusion judgment for an\nelement based on the rules of language use (neutral\noption field), other interpretation methods should\nbe used. This initial phase takes almost 2 months.\nFor example, in the crime of robbery, the ob-\nject \"public or private property\" represents the pro-\ntected legal interest. The phrase \"forcibly seizing\npublic or private property through violence, coer-\ncion, or other means\" describes the objective aspect.\nSince no subject is specified, it is assumed to in-\nvolve a general subject, and the adverbs \"violence\"\nand \"coercion\" indicate an intentional act. Prelim-\ninarily interpret ‘violence’ in the objective aspect\nas ‘Use of physical force or power’, but the spe-\ncific forms and subjects of violence need further\nclarification.\nThe Second Level: Judicial Interpretation.\nIn\nthe 3rd and 4th months, the second level of the\nhierarchical annotation path focuses on refining le-\ngal elements through judicial interpretation. The\nprimary method used for interpreting these mate-\nrials is systematic interpretation. This approach\nexamines the position of the corresponding articles\nwithin the legal system by analyzing their place-\nment within the structure of laws, including parts,\nchapters, sections, articles, clauses, and subclauses,\nas well as their relationship to other statutes and\njudicial interpretations. Additionally, other inter-\npretative methods, such as sociological interpreta-\ntion and teleological interpretation, are referenced\nbased on judicial interpretations, related statutory\nprovisions, or bar exam questions.The goal of this\nlevel is to clarify the legislative intent by consid-\nering the contextual relevance of each provision\nwithin the broader legal framework.\nFor example, in the first level, the objective as-\npect of \"violence\" in the crime of robbery requires\nfurther clarification, specifically regarding whether\nviolence must be directed exclusively at persons\nor could also apply to property. Article 289 of\nChinese Criminal Law(Congress, 2017) stipulates\nthat in cases of \"smashing, looting, and robbing\"\ncommitted by a group, the ringleaders shall be con-\nvicted of robbery if they destroy or seize public\nor private property. This provision demonstrates\nthat violence against property can also constitute\nrobbery under Chinese law.\nThe Third Level: Guiding Cases.\nIn the 5th to\n6th month, purposive interpretation and sociolog-\nical interpretation are applied to the guiding cases\nand landmark judgments from the Supreme Court.\nBy examining the social significance of real-world\ncases, these methods bridge the subtle gap between\nabstract legal theory and practical cases. This ap-\nproach enables dynamic adaptation and integration\nof empirical insights and emerging controversies\nwithin the dataset.\nFor example, in Criminal Trial Reference Case\nNo.159(Zou, 2002), the perpetrator lured the victim\ninto a room, locked the door, and seized 170,000\nRMB intended for a transaction. The court deter-\nmined that although the detention did not endanger\npersonal safety, it was sufficient to suppress the vic-\ntim’s resistance, thus constituting \"violence\" in in\nthe objective aspects of robbery. Another example\nis the \"Molestation and Theft Case\" (Ma, 2021),\nwhere the perpetrator bound the victim, committed\nmolestation, and stole the victim’s phone. Since the\nongoing molestation reinforced coercion, it consti-\ntutes a new act of violence. Thus, the annotation\nincludes \"molestation\" as an additional method.\nThe Fourth Level: Academic Discourses.\nIn\nthe 7th month, the final stage involves academic ex-\npansion. Academic controversies are introduced by\nemploying multiple interpretive methods such as\ncomparative law interpretation, purposive inter-\npretation, and sociological interpretation. These\nmethods include inserting conflict markers at key\npoints of controversy, highlighting the distinctions\nbetween mainstream consensus and minority theo-\nries, while providing brief annotations of their legal\nreasoning. This approach ensures the extensibility\nand academic depth of the dataset.\nFor example, regarding the crime of robbery,\nfor the main view in China, Soviet Union, North\nKorea, and Japan explicitly holds that the violence\nmust be severe enough to endanger the victim’s\nlife or health(Zhang, 2007). But some scholars\n\nargue that any violence that can forcibly impact the\nvictim’s body is sufficient to constitute violence in\nrobbery, no need to endanger the victim’s life or\nhealth(Yang, 2010).\n4\nData Distribution\nMetric\nLLM\nExpert\nMean\nMedian\nMean\nMedian\nAvg. Length\n115.43\n-\n472.53\n-\nSB\n23.12\n27\n51.64\n17\nOB\n15.86\n15\n36.01\n25\nSA\n28.00\n30\n42.38\n21\nOA\n48.45\n45\n342.5\n230\nTable 1: Comparison of Legal Element Lengths: LLM\nvs. Expert. SB = Subject, OB = Object, SA = Subjective\nAspect, OA = Objective Aspect.\nAs shown in Table 1, we compare the length of\nlegal elements between expert-annotated descrip-\ntions in JUREX-4E and LLM-generated outputs\nacross 105 charges that overlap with the Lecard-\nV2 dataset (Li et al., 2024c), which is one of the\nmost comprehensive legal datasets, covering 184\ncriminal charges. We find that:\n(1) The average total length of expert annotations\n(472.53) is more than four times longer than that of\nLLM-generated outputs (115.43), indicating that\nthe former include more detailed information.\n(2) The median difference between the Subject\n(SB), Object (OB), and Subjective Aspect (SA)\nis relatively small, as these elements are typically\nfixed. For example, the SB is often a general entity,\nand the SA is often intent or negligence.\n(3) The median and mean values for SB and\nSA in the expert annotations differ, especially for\nSB (17 v.s. 51.64). This discrepancy arises be-\ncause certain specialized charges may require more\ndetailed explanations. For example, in the crime\nof copyright infringement, the definition of “work”\nunder the subject element has 9 occasions. Detailed\ndata distribution for each element is provided in\nAppendix A.\n(4) The main difference between Expert and\nLLM is in the Objective Aspect (342.5 v.s. 48.45\nin Mean). This is because the OA includes a range\nof factual elements describing the criminal behav-\nior, such as the conduct, object, result, time, and\nlocation, which are most emphasized in legal pro-\nvisions and are central to various legal interpretive\ntheories.\n5\nHuman Evaluation\nWe selected 6 complicated crimes in Chinese ju-\ndicial practice(Ouyang et al., 1999) to evaluate\nwhether the LLM can handle the Four-element\nTheory. Drawing from previous work(Deng et al.,\n2023; Cui et al., 2024; Zhou et al., 2023), we de-\nfine LLM-generated knowledge as information pro-\nduced by the LLM based on its pre-trained knowl-\nedge and contextual prompts. For detail, we pro-\nvide the LLM with legal articles and the definition\nof each element in FET, prompting it to generate\nthe four-elements base on these metrical. The LLM\nis expected to autonomously identify and generate\nthe four elements based on its learned understand-\ning of legal concepts.\nWe invite legal experts to assess the four ele-\nments generated by the LLM from four dimen-\nsions: Precision, Completeness, Representative-\nness, and Standardization.\n• Precision: Whether the key components of\neach element are accurately identified. This\ndimension mainly evaluates whether the four\nelements faithfully represent the legal provi-\nsions.\n• Completeness: Whether all necessary infor-\nmation of each element is included. This as-\nsesses whether any essential content is miss-\ning, such as the omission of a description for\nspecific subjects, like government officials.\n• Representativeness: Whether the annotations\nhighlight the most critical scenarios in judicial\npractice. For example, in crimes of intentional\ninjury, this would involve describing the rep-\nresentative means of harm.\n• Standardization: Whether the four elements\nare clearly defined, ensuring consistency in\nthe expression of identical elements across\ndifferent crimes (e.g., consistent description\nof general subjects), with concise and easily\nunderstandable explanations, free from legal\nambiguities or misunderstandings.\nEach dimension was scored by two types of ex-\nperts: one group with a pure legal background and\nanother group with a combined background in law\nand Artificial Intelligence, all of whom have passed\nthe bar examination. The experts were selected to\nbalance domain expertise and interdisciplinary per-\nspectives. Scores were averaged across the two\ngroups. Details about 1-5 scale criteria and annota-\ntor background are provided in Appendix C.\nAs shown in Table 2, expert annotations consis-\n\nDimension\nLLM\nExpert\nδ\nPrecision\n4.12\n4.69\n+ 0.57\nCompleteness\n3.79\n4.65\n+ 0.86\nRepresentativeness\n3.60\n4.48\n+ 0.88\nStandardization\n4.33\n4.56\n+ 0.23\nTable 2: Performance comparison of four elements\nacross methods. δ represents the score difference be-\ntween expert and LLM-generated four-elements, with\nexperts outperforming LLMs in all dimensions.\ntently outperform LLM-generated elements across\nall four dimensions, highlighting the limitations of\nLLMs in understanding legal elements. The most\npronounced deficiencies are observed in Complete-\nness (+0.86) and Representativeness (+0.88). This\nsuggests that while LLMs can generate formally\nstandardized and relatively accurate four elements,\ntheir description are not specific enough and do not\nadequately reflect the representative features of a\ncharge’s criminal composition.\n6\nEvaluate Expert Knowledge on Charge\nDisambiguation\nIn the preceding section, the human evaluation\ndemonstrated that experts annotated higher-quality\nfour-elements. To further quantitatively evaluate\nthe annotations, a direct way is to judge whether\ndifferent charges can be distinguished according to\nthe four-element definition of crime constitution.\nTherefore, we introduce the Similar Charge Disam-\nbiguation (SCD) task(Yuan et al., 2024; Li et al.,\n2024a).\n6.1\nExperiment Settings\n6.1.1\nDataset\nWe chose the dataset released by (Liu et al., 2021),\nwhich includes five charge sets with the largest\nnumber of cases. To evaluate performance on repre-\nsentative tasks, we selected three 2-label classifica-\ntion groups commonly examined in other datasets\n(Yuan et al., 2024): Fraud & Extortion (F&E), Em-\nbezzlement & Misappropriation of Public Funds\n(E&MPF), and Abuse of Power & Dereliction of\nDuty (AP&DD). Each crime has over 1.9k cases,\nwith a total of 13,962 cases. The details of the\nclassification groups are shown in Appendix D.\nFollowing previous work (Liu et al., 2021; Yuan\net al., 2024), we use Average Accuracy (Acc) and\nmacro-F1 (F1) as evaluation metrics.\n6.1.2\nBaselines and Methods\nTo evaluate SCD tasks, we consider two ways\nof incorporating legal knowledge. The first di-\nrectly integrates legal statutes, represented by GPT-\n4o (Achiam et al., 2023) as the baseline and GPT-\n4o+Article, which explicitly provides relevant legal\narticles to the model. The second adopts struc-\ntured legal reasoning to enhance interpretability\nand accuracy. We consider Legal-CoT, a Chain-\nof-Thought (Kojima et al., 2022) variant that con-\nducts a stepwise analysis based on the FET, and\nMALR (Yuan et al., 2024), a multi-agent frame-\nwork that decomposes legal tasks into sub-tasks in\nfour-element structures. Details of each baseline\nare provided in Appendix D.\nWe use an unified approach to introduce four-\nelement descriptions.\nFor each group of sim-\nilar charges, the model receives charges’ four-\nelements from JUREX-4E or generated by LLM to\naid classification. Specifically, GPT-4o+FETExpert\nrelies on expert-annotated four-elements, while\nGPT-4o+FETLLM relies on LLM-generated four-\nelements. As shown in Appendix D, the instruc-\ntion format is consistent across methods, with only\nthe [Four Elements of candidate charges] varying\nbased on the source. All experiments are conducted\nin a zero-shot setting, with the max_tokens set to\n3,000 (or 10,000 for COT and MALR reasoning)\nand temperature set to 0 or 0.0001(In repeated ex-\nperiments).\n6.2\nResults\nAs shown in Table 3, the GPT-4o+FETExpert per-\nforms best in discriminating similar charges, indi-\ncating that expert annotation is superior to other\nmethods of directly or indirectly introducing FET\nwith LLMs. Specifically, we can derive the follow-\ning observation:\nEffectiveness of Domain-Specific Legal Knowl-\nedge:\nAmong all approaches, those that explicitly\nincorporate domain-specific legal knowledge, such\nas GPT-4o+Article, Legal-CoT, and MALR, outper-\nform GPT-4o alone. This highlights the importance\nof integrating legal knowledge.\nImportance of Concrete Four-element Knowl-\nedge:\nThe accuracy of both Legal-CoT and\nMALR is still lower than GPT-4o+FET methods.\nThis suggests that, compared to embedding the\nFour-Element Theory into LLMs’ reasoning pro-\ncess, providing concrete charge four-elements en-\n\nModel\nF&E\nE&MPF\nAP&DD\nAverage\nAcc\nF1\nAcc\nF1\nAcc\nF1\nAcc\nF1\nGPT-4o\n94.36\n95.81\n86.49\n89.76\n85.54\n87.12\n88.72\n90.07\nGPT-4o+Article\n95.34\n96.30\n92.64\n93.03\n88.30\n89.33\n92.09\n92.89\nLegal-COT\n94.99\n96.27\n90.50\n90.99\n87.81\n88.14\n89.95\n90.85\nMALR\n94.62\n95.82\n86.99\n86.98\n87.86\n88.68\n89.82\n90.49\nGPT-4o+FETLLM\n95.73\n96.56\n91.87\n92.01\n89.61\n89.69\n92.40\n92.75\nGPT-4o+FETExpert\n96.06\n96.69\n92.57\n93.05\n90.53\n90.62\n93.05\n93.45\nTable 3: Results of Charge Disambiguation. FET means introducing the Four-element theory with knowledge\nobtained from experts and LLM method. Highest results are in bold.\nFigure 2: An expert-guided FET method to enhance le-\ngal case retrieval by incorporating expert four elements.\nables the model to better understand the different\ncrimes’ composition.\nSuperiority of Expert Annotations:\nCompared\nwith the indirect introduction of FET reasoning,\nthe method of directly introducing four-elements\nto the model (GPT-4o+FET) achieves better re-\nsults. Notably, GPT-4o+FETExpert surpassing the\nGPT-4o+FETLLM by 0.65 in average accuracy and\n0.70 in average F1-score, underscoring the superior\nquality and reliability of expert annotations in legal\ntasks, aligning with human evaluations in Table 2\nand reaffirming the critical role of human expertise\nin legal decision-making.\n7\nCan Expert Knowledge Benefit More\nDownstream Tasks?\nIn this section, we design a simple framework to\napply the expert-annotated four elements to Legal\nCase Retrieval (LCR), a task in which relevant\ncases are retrieved based on given facts. It is an\nimportant step in the practice of analyzing cases\nand making judgments, and it requires the precise\napplication of the four-element theory to matches\ncases with similar criminal compositions.\n7.1\nMethod\nWe implement a standard dense retrieval approach\nBGE using BGE-m3 (Chen et al., 2023), an ad-\nvanced embedding model for dense retrieval. Given\na query q and a candidate case c, their vector\nrepresentations vq and vc are obtained through\nshared encoder E: vq = E(q),\nvc = E(c). We\nused the BGE-m3 model without fine-tuning as\nthe shared encoder. Next, the relevance score is\ncomputed via cosine similarity:\nsimbase(q, c) =\nvq · vc\n∥vq∥∥vc∥\n(1)\nTo retrieve the top-k most similar cases, we rank\nthe candidates based on their cosine similarity to\nthe query. Denote the set of candidate cases as\nC = {c1, c2, . . . , cn}, where n is the total number\nof candidate cases. We compute the similarity for\neach ci ∈C, and select the top-k candidates with\nthe highest similarity scores.\nAs shown in Figure 2, to leverages expert-\nannotated four elements of charges, we introduce\nan BGE+FETExpert_guided method for the retrieval\nprocess, consisting of three steps: (1) Predicting\ncharges, a LLM Mp predicts potential charges\nZ = {z1, ..., zk} from case facts. (2) Matching\nelements, retrieving corresponding charge’s four-\nelements {fz}z∈Z in JUREX-4E. (3) Analyzing\ncase facts. Guided by {fz}, another LLM Mg gen-\nerates case-specific four elements ac for candidate\nc. The final similarity score combines factual and\ntheoretical alignment:\nsimfinal(q, c) = α·simbase(q, c)+(1−α)·simf(aq, ac)\n(2)\nwhere α = 0.7 and simf measures the similarity\nbetween the generated four-element descriptions.\nTo facilitate comparison, we also design a\nBGE+FETLLM method that directly prompt the\nLLM Mg with the concept of Four-Element The-\nory to generate case-specific four elements ac.\n\nModel\nNDCG@10\nNDCG@20\nNDCG@30\nR@1\nR@5\nR@10\nR@20\nR@30\nMRR\nBERT\n0.1511\n0.1794\n0.1978\n0.0199\n0.0753\n0.1299\n0.2157\n0.2579\n0.1136\nLegal-BERT\n0.1300\n0.1487\n0.1649\n0.0186\n0.0542\n0.1309\n0.1822\n0.2172\n0.0573\nLawformer\n0.2684\n0.3049\n0.3560\n0.0432\n0.1479\n0.2330\n0.3349\n0.4683\n0.1096\nChatLaw\n0.2049\n0.2328\n0.2745\n0.0353\n0.1306\n0.1913\n0.2684\n0.3751\n0.1285\nSAILER\n0.3142\n0.4133\n0.4745\n0.0539\n0.1780\n0.3442\n0.5688\n0.7092\n0.1427\nGEAR\n*\n*\n*\n0.0630\n0.1706\n0.3142\n0.4625\n*\n0.2162\nBGE\n0.4737\n0.5539\n0.5937\n0.0793\n0.2945\n0.4298\n0.6500\n0.7394\n0.1926\nFETLLM\n0.5139\n0.5862\n0.6291\n0.0980\n0.2967\n0.4769\n0.6802\n0.7828\n0.2140\n- base\n0.3583\n0.4293\n0.4798\n0.0506\n0.2240\n0.3644\n0.5383\n0.6652\n0.1453\nFETExpert_guided\n0.5211\n0.5920\n0.6379\n0.1024\n0.3049\n0.4883\n0.6885\n0.7967\n0.2155\n- base\n0.3766\n0.4584\n0.5111\n0.0715\n0.1894\n0.3709\n0.5891\n0.7203\n0.1624\nTable 4: SCR results. Bold fonts indicate leading results in each setting. * denotes that the indicator is not applicable\nto the current model.\n7.2\nDataset\nLeCaRDv2(Li et al., 2024c) is the latest version of\nLeCaRD(Ma et al., 2021), which is widely used\nin LCR task (Li et al., 2024b; Zhou et al., 2023).\nIt comprises 800 queries and 55,192 candidates\nextracted from 4.3 million criminal case documents.\nThere are two common evaluation settings for this\ndataset: one uses a subset (Qin et al., 2024) with a\ncandidate pool size of 1,390, while the other uses\nthe full set (Li et al., 2024c) with a candidate pool\nsize of 55,000. We conducted experiments under\nboth settings.\nFollowing previous work(Feng et al., 2024; Qin\net al., 2024), we adopt commonly used evaluation\nmetrics. For the subset, we use NDCG@10, 20, 30,\nRecall@1, 5, 10, 20, and MRR. For the full dataset,\nwe use Recall@100, Recall@200, Recall@500,\nand Recall@1000.\n7.3\nBaselines\nConsistent with earlier work(Li et al., 2024c; Qin\net al., 2024), we compare some dense retrieval\nmethods, including: BERT(Devlin, 2018), Law-\nformer(Xiao et al., 2021), ChatLaw-Text2Vec1(Cui\net al., 2023), SAILER(Li et al., 2023), GEAR(Qin\net al., 2024). Details of each baseline is shown in\nAppendix E. These baselines are implemented us-\ning the FlagEmbedding Toolkit2 with a RTX 3090.\n7.4\nResults\nThe LCR results are shown in Table 4, where we\ncan observe that:\nFET Works Well in LCR.\nThe baseline model\nBGE achieves strong performance across most met-\n1https://modelscope.cn/models/fengshan/\nChatLaw-Text2Vec\n2https://github.com/FlagOpen/FlagEmbedding\nrics compared to previous methods. Introducing the\nFour-Element Theory (FET) further improves its re-\nsults, with relative MRR improvements of 11.11%\nfor FETLLM and 11.89% for FETExpert_guided, indi-\ncating that introducing legal theory is effective.\nExpert Knowledge is Necessary.\nBy leverag-\ning external knowledge, FETExpert_guided achieves\nsignificant improvements across all of the met-\nrics. Specifically, using expert-guided case four-\nelements (FETExpert_guided-base) outperforms LLM-\ngenerated case four-elements (FETLLM-base) by an\naverage of 11.77% in MRR, demonstrating the crit-\nical role of expert knowledge in enhancing retrieval\nprecision. A case study in Appendix G shows that\nthe expert four-element for charges provide prac-\ntical judgment points and key narratives (e.g., the\nspecial subject of the Crime of Embezzlement) that\nhelp the LLM focus on essential facts to analyze\nthe case.\nWe also evaluated the FET method on the full\nset, as shown in Table 9 , and the results remain\nconsistent, with the expert-guided method still per-\nforming best.\n8\nConclusion\nIn this paper, we propose an expert-annotated\nknowledge base, evaluate its quality in the Similar\nCharge Distinction task, and apply it to the Legal\nCase Retrieval task. Our results demonstrate that\nexpert annotations significantly enhance LLMs’ un-\nderstanding of the Four-Element Theory. The four-\nelement annotations, enriched with professional\nlegal interpretations, provide strong support for\nLLMs’ reasoning capabilities. This approach can\nbe extended to other legal AI tasks, such as legal\ndocument analysis and contract interpretation.\n\n9\nEthical Considerations\nThe datasets used in our evaluation are sourced\nfrom publicly available legal datasets, with all de-\nfendant information anonymized to ensure privacy.\n10\nLimitations\nAs a limitation, this knowledge base focuses on\nthe Four-Element Theory within the context of\n155 crimes under Chinese Criminal Law. How-\never, the four-level hierarchical pyramid annotation\nstructure based on the legal interpretation system\nproposed in this work provides valuable insights\nfor future expansion to other legal domains, as it\nrepresents a theoretical framework in the field of ju-\nrisprudence. The interpretative methods within the\nlegal interpretation system, including textual, sys-\ntematic, sociological, and doctrinal interpretations,\nare universally recognized in international law field\nand can be applied to different laws, countries, and\nlegal systems.\nAcknowledgments\nReferences\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, et al. 2023. Gpt-4 technical report.\narXiv preprint arXiv:2303.08774.\nZhenwei An, Quzhe Huang, Cong Jiang, Yansong\nFeng, and Dongyan Zhao. 2022.\nDo charge pre-\ndiction models learn legal theory? arXiv preprint\narXiv:2210.17108.\nIz Beltagy, Matthew E Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer. arXiv\npreprint arXiv:2004.05150.\nIlias Chalkidis, Manos Fergadiotis, Prodromos Malaka-\nsiotis, Nikolaos Aletras, and Ion Androutsopoulos.\n2020. Legal-bert: The muppets straight out of law\nschool. arXiv preprint arXiv:2010.02559.\nJianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu\nLian, and Zheng Liu. 2023. Bge m3-embedding:\nMulti-lingual, multi-functionality, multi-granularity\ntext embeddings through self-knowledge distillation.\nPreprint, arXiv:2309.07597.\nXingliang Chen. 2010.\nCrime constitution theory:\nAn academic historical investigation from the four-\nelements to the three-tier theory. Peking University\nLaw Journal, 22(1):49–69.\nXingliang Chen. 2017. Criminal law hierarchical theory:\nA comparative study between the three-tier and four-\nelements theories. Tsinghua Law Journal, 5.\nNational People’s Congress. 2017. Criminal Law of the\nPeople’s Republic of China. China Legal Publishing\nHouse.\nJiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and\nLi Yuan. 2023. Chatlaw: Open-source legal large\nlanguage model with integrated external knowledge\nbases. arXiv preprint arXiv:2306.16092.\nJiaxi Cui, Munan Ning, Zongjian Li, Bohua Chen, Yang\nYan, Hao Li, Bin Ling, Yonghong Tian, and Li Yuan.\n2024. Chatlaw: A multi-agent collaborative legal\nassistant with knowledge graph enhanced mixture-\nof-experts large language model.\narXiv preprint\narXiv:2306.16092.\nWentao Deng, Jiahuan Pei, Keyi Kong, Zhe Chen, Furu\nWei, Yujun Li, Zhaochun Ren, Zhumin Chen, and\nPengjie Ren. 2023. Syllogistic reasoning for legal\njudgment analysis. In Proceedings of the 2023 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 13997–14009.\nJacob Devlin. 2018. Bert: Pre-training of deep bidi-\nrectional transformers for language understanding.\narXiv preprint arXiv:1810.04805.\nM Eig Larry. 2014. Statutory interpretation: General\nprinciples and recent trends. Congressional Center\nfor Research, (s 37).\nYi Feng, Chuanyi Li, and Vincent Ng. 2024. Legal\ncase retrieval: A survey of the state of the art. In\nProceedings of the 62nd Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 6472–6485.\nMingxuan Gao. 2009. On the rationality of the four-\nelements theory of crime constitution and adherence\nto china’s criminal law system. China Legal Sci.,\n(2):5–11.\nCong Jiang and Xiaolei Yang. 2023. Legal syllogism\nprompting: Teaching large language models for legal\njudgment prediction. In Proceedings of the Nine-\nteenth International Conference on Artificial Intelli-\ngence and Law, pages 417–421.\nYule Kim and American Law Division. 2008. Statutory\ninterpretation: General principles and recent trends.\nCongressional Research Service Washington, DC.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. Advances in\nneural information processing systems, 35:22199–\n22213.\nAng Li, Qiangchao Chen, Yiquan Wu, Ming Cai, Xi-\nang Zhou, Fei Wu, and Kun Kuang. 2024a. From\ngraph to word bag:\nIntroducing domain knowl-\nedge to confusing charge prediction. arXiv preprint\narXiv:2403.04369.\n\nHaitao Li, Qingyao Ai, Jia Chen, Qian Dong, Yueyue\nWu, Yiqun Liu, Chong Chen, and Qi Tian. 2023.\nSailer: structure-aware pre-trained language model\nfor legal case retrieval. In Proceedings of the 46th\nInternational ACM SIGIR Conference on Research\nand Development in Information Retrieval, pages\n1035–1044.\nHaitao Li, Qingyao Ai, Xinyan Han, Jia Chen, Qian\nDong, Yiqun Liu, Chong Chen, and Qi Tian. 2024b.\nDelta: Pre-train a discriminative encoder for legal\ncase retrieval via structural word alignment. arXiv\npreprint arXiv:2403.18435.\nHaitao Li, Yunqiu Shao, Yueyue Wu, Qingyao Ai, Yix-\niao Ma, and Yiqun Liu. 2024c. Lecardv2: A large-\nscale chinese legal case retrieval dataset. In Proceed-\nings of the 47th International ACM SIGIR Confer-\nence on Research and Development in Information\nRetrieval, pages 2251–2260.\nHong Li. 2006. No need to reconstruct china’s crime\nconstitution system. Chinese Journal of Law, (1):32–\n51.\nGenlin Liang. 2017. The vicissitudes of chinese crim-\ninal law and theory: A study in history, culture and\npolitics. Peking University Law Journal, 5(1):25–49.\nXiao Liu, Da Yin, Yansong Feng, Yuting Wu, and\nDongyan Zhao. 2021. Everything has a cause: Lever-\naging causal inference in legal text analysis. arXiv\npreprint arXiv:2104.09420.\nYinxiang Ma. 2021. The spiritualization and limitation\nof the concept of violence in robbery. Law Science,\n(06):76–91.\nYixiao Ma, Yunqiu Shao, Yueyue Wu, Yiqun Liu,\nRuizhe Zhang, Min Zhang, and Shaoping Ma. 2021.\nLecard: a legal case retrieval dataset for chinese law\nsystem. In Proceedings of the 44th international\nACM SIGIR conference on research and development\nin information retrieval, pages 2342–2348.\nTao Ouyang, Kejia Wei, and Renwen Liu. 1999. Con-\nfusing crimes, noncrime, and boundaries between\ncrimes.\nRoscoe Pound. 1925. Jurisprudence.\nRoscoe Pound. 1932. Hierarchy of sources and forms\nin different systems of law. Tul. L. Rev., 7:475.\nWeicong Qin, Zelin Cao, Weijie Yu, Zihua Si, Sirui\nChen, and Jun Xu. 2024. Explicitly integrating judg-\nment prediction with legal document retrieval: A\nlaw-guided generative approach. In Proceedings of\nthe 47th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval,\npages 2210–2220.\nSergio Servantez, Joe Barrow, Kristian Hammond, and\nRajiv Jain. 2024. Chain of logic: Rule-based rea-\nsoning with large language models. arXiv preprint\narXiv:2402.10400.\nJabez Gridley Sutherland. 1891. Statutes and Statutory\nConstruction: Including a Discussion of Legislative\nPowers, Constitutional Regulations Relative to the\nForms of Legislation and to Legislative Procedure, To-\ngether with an Exposition at Length of the Principles\nof Interpretation and Cognate Topics. Callaghan.\nAlan Watson. 1982. Legal change: sources of law and\nlegal culture. U. Pa. L. Rev., 131:1121.\nChaojun Xiao, Xueyu Hu, Zhiyuan Liu, Cunchao Tu,\nand Maosong Sun. 2021. Lawformer: A pre-trained\nlanguage model for chinese legal long documents. AI\nOpen, 2:79–84.\nKai Yang. 2010. On the distinction between violence\nin robbery and theft. http://www.jsfy.gov.cn/\narticle/78069.html. Accessed: 2025-02-16.\nWeikang Yuan, Junjie Cao, Zhuoren Jiang, Yangyang\nKang, Jun Lin, Kaisong Song, Pengwei Yan, Chang-\nlong Sun, Xiaozhong Liu, et al. 2024. Can large\nlanguage models grasp legal theories? enhance legal\nreasoning with insights from multi-agent collabora-\ntion. arXiv preprint arXiv:2410.02507.\nMingkai Zhang. 2010. Justification grounds and the\nsystem of crime constitution. The Jurist, (1):31–39,\n176–177.\nMingkai Zhang. 2017. The judicial application of the\nhierarchical theory. Tsinghua Law Journal, 11(5):20–\n39.\nWenxian Zhang and Wangsheng Zhou. 2007. Jurispru-\ndence (3rd Edition). Higher Education Press, Bei-\njing.\nZhihai Zhang. 2007. On the violent elements of robbery.\nLegal System and Society, (1):222.\nGuangquan Zhou. 2017a. Debate on the theory of crime\nconstituent elements and its long-term impact. Poli-\ntics and Law, (3):17–34.\nGuangquan Zhou. 2017b. The hierarchical theory of\ncrime and its practical development. Tsinghua Law\nJournal, 11(5):84–104.\nYouchao Zhou, Heyan Huang, and Zhijing Wu. 2023.\nBoosting legal case retrieval by query content selec-\ntion with large language models. In Proceedings of\nthe Annual International ACM SIGIR Conference on\nResearch and Development in Information Retrieval\nin the Asia Pacific Region, pages 176–184.\nDaiming Zou. 2002. Robbery case no. 159: How to\nqualify the act of confined imprisonment for the pur-\npose of robbery. Criminal Trial Reference, 1(24).\nIssue 1, Total Issue 24.\n\nFigure 3: The average length distribution of\ntotal four elements annotated by experts.\nFigure 4: The length distribution of each element\nannotated by experts.\nFigure 5: The average length distribution of total\nfour elements generated by LLM.\nFigure 6: The length distribution of each element\ngenerated by LLM.\n\nA\nDetailed Data Distribution for each\nElement\nB\nInterpretation Methods\n1. Literal Interpretation\nA strict textual analysis method that adheres to\nthe ordinary meaning of words as understood by a\nreasonable person at the time of enactment, exclud-\ning subjective intent inference\n2. Systematic Interpretation\nAn approach interpreting legal provisions\nthrough their position within the codified legal\nhierarchy and logical connections with related\nnorms, maintaining the integrity of the legal system\n(aligned with Dworkin’s \"law as integrity\" theory).\n3. Purposive Interpretation\nA method discerning the objective legislative\npurpose through analysis of statutory structure and\nfunctional goals, distinct from subjective legisla-\ntive intent (following Hart & Sacks’ legal process\nschool).\n4. Historical Interpretation\nInterpretation based on legislative history ma-\nterials including drafts, debates and official com-\nmentaries, while distinguishing original meaning\nfrom framers’ subjective intentions (as per Brest’s\noriginal understanding theory).\n5. Comparative Interpretation\nA methodology referencing functionally compa-\nrable legal systems sharing common juridical tradi-\ntions, employing analogical reasoning while con-\nsidering local legal culture (developed through Got-\ntfried Wilhelm Leibniz’s comparative law frame-\nwork).\n6. Sociological Interpretation\nInterpretation evaluating social efficacy through\nempirical analysis of implementation effects,\nguided by Pound’s sociological jurisprudence prin-\nciple that \"law must be measured by its achieved\nresults\".\nC\nHuman Evaluation Guidance\nThe annotators included three postgraduate stu-\ndents specializing in criminal law and one mas-\nter’s student in legal science and technology. The\nannotators scored independently, without knowl-\nedge of each other’s results. Before scoring, they\nwere asked to read the descriptions and scoring\nguidelines (as shown in Table 5) for each evalua-\ntion dimension. In order to ensure the fairness of\nthe evaluation, they do not know the source of each\nfour elements, and even do not know that these four\nelements include those generated by LLMs.\nWhen assigning scores, they were also required\nto provide brief justifications. For example, for the\nCompleteness dimension: 3 (The description of\nObjective Aspect is too brief, and does not specify\nthe intent of illegal possession).\nD\nDetails for Similar Charge\nDisambiguation\nFor LLM baselines, we evaluate both general-\npurpose and task-specific methods.\nGPT-4o is an optimized version of GPT-\n4(Achiam et al., 2023) that has well performance\nin specific tasks through domain adaptation.\nTo explore the effectiveness of notes-guided four\nelements in LLMs, we further consider other meth-\nods that introduced the Four-element theory into\nLLMs.\nGPT-4oLaw, which introduces articles related\nto corresponding charges into the instruction to\nprovide legal context.\nLegal-COT is a variant of COT (Kojima et al.,\n2022) that guides the LLM to perform step-by-step\nlegal reasoning by incorporating explanations of\nthe Four-element theory into the instruction.\nMALR is a up to date multi-agent framework de-\nsigned to enhance complex legal reasoning (Yuan\net al., 2024), enabling LLMs to autonomously de-\ncompose legal tasks and extract insights from legal\nrules. As its full implementation is not publicly\navailable, we use the released code for the auto-\nplanner module and implement the legal insight ex-\ntraction following the specified steps and prompts,\nwith necessary refinements. Experiments on the\npaper’s reported examples show that our implemen-\ntation produces task decompositions and outputs\nlargely consistent with the original results.\nAs shown in Table 8, different methods differ\nin their prompts for generating and explaining the\nFour-Element Theory, but generally follow a simi-\nlar process. For the SCD output, except for COT\nand MALR, which require reasoning processes and\nprediction results, all other methods only require\nthe output of prediction results.\nE\nBaselines in Legal Case Retrieval\nBERT(Devlin, 2018) is a language model widely\nused in retrieval tasks. In this paper, we chose\n\nDimension\nPrecision\nCompleteness\nRepresentativeness\nStandardization\nDefinition\nWhether there are errors\nin key elements\nWhether the four ele-\nments are complete\nWhether key elements\nand scenarios are empha-\nsized\nWhether language and\nformat are clear and stan-\ndardized\nScore 1\nContains numerous obvi-\nous errors, severely im-\npeding the judgment of\nculpability, exculpation,\nand conviction, leading\nto significant deviations.\nSevere\nomission\nof\nkey\ncontent,\nunable\nto present a complete\npicture\nof\nthe\ncrime\nstructure, greatly hinder-\ning analysis of criminal\nbehavior.\nCompletely fails to men-\ntion any key elements or\nscenarios, unable to high-\nlight essential points for\ncrime recognition, offer-\ning no assistance in con-\nviction.\nLanguage is extremely\nchaotic and obscure; for-\nmat lacks any standard-\nization, greatly hindering\ncomprehension and ap-\nplication.\nScore 2\nContains multiple notice-\nable errors, significantly\ninterfering with culpabil-\nity, exculpation, and con-\nviction judgments, poten-\ntially leading to partial er-\nrors.\nNoticeable omissions in\ncontent, failing to com-\nprehensively cover crime\nelements, affecting thor-\nough analysis of criminal\nbehavior.\nOnly highlights a mini-\nmal and unimportant por-\ntion of the key elements,\nproviding weak support\nfor understanding key\ncrime features.\nLanguage is relatively\nvague\nand\ninaccurate,\nwith a casual format\nthat makes content com-\nprehension significantly\nchallenging.\nScore 3\nContains a few errors,\nbut the overall accuracy\nin determining culpabil-\nity, exculpation, and con-\nviction is relatively unaf-\nfected, unlikely to lead to\njudgment errors.\nSome\nkey\ncontent\ndescriptions are incom-\nplete, but they generally\npresent the framework of\nthe crime structure.\nHighlights\nsome\nrela-\ntively important key ele-\nments but lacks compre-\nhensiveness and promi-\nnence, offering limited\nassistance in crime iden-\ntification.\nLanguage is generally\nclear but may have minor\ndeviations in phrasing or\nformatting.\nScore 4\nAlmost error-free, key\nelements\naccurately\nserve culpability, excul-\npation, and conviction\njudgments, ensuring the\naccuracy of results.\nKey elements are mostly\ncomplete, with only very\nslight and non-critical\ndeficiencies that do not\nhinder a comprehensive\nanalysis of the crime.\nClearly and relatively\ncomprehensively\nhigh-\nlights\nkey\nelements,\naiding in accurately iden-\ntifying crucial aspects of\ncriminal behavior.\nLanguage is clear and\naccurate, format is rel-\natively standardized, fa-\ncilitating comprehension\nand application of rele-\nvant content.\nScore 5\nCompletely\nerror-free,\nkey elements are pre-\ncisely defined, achieving\nhighly accurate culpa-\nbility, exculpation, and\nconviction\njudgments\nwithout any flaws.\nAll four elements are\ncomplete and detailed,\ncovering every aspect of\nthe crime, perfectly pre-\nsenting the crime struc-\nture.\nPrecisely and compre-\nhensively highlights all\ncrucial\nelements,\nen-\nabling immediate grasp\nof the core aspects of\nthe crime, significantly\naiding conviction.\nLanguage is extremely\nclear, standardized, and\nconcise; format perfectly\nmeets requirements, with\nno barriers to understand-\ning, ensuring efficient in-\nformation delivery.\nTable 5: The four dimensions of the human evaluation and the specific score description.\nCharge Sets\nCharges\nCases\nF&E\nFraud & Extortion\n3536 / 2149\nE&MPF\nEmbezzlement\n&\nMis-\nappropriation of Public\nFunds\n2391 / 1998\nAP&DD\nAbuse of Power & Dere-\nliction of Duty\n1950 / 1938\nTable 6:\nDistribution of charges in the GCI dataset.\nCases denotes the number of cases in each category.\nFollowing (Liu et al., 2021), for a case with both con-\nfusable charges, the prediction of any one of the charges\nis considered correct.\nBERT-base-Chinese3.\nLegal-BERT4(Chalkidis\net al., 2020) is a variant of BERT that is specifically\ntrained on legal corpora. Lawformer(Xiao et al.,\n2021)is a Chinese legal pre-trained model based on\nLongformer(Beltagy et al., 2020), which is able to\nprocess long texts in the legal domain. ChatLaw-\nText2Vec5(Cui et al., 2023) is a Chinese legal LLM\ntrained on 936,727 legal cases for similarity calcula-\ntion of legal-related texts. SAILER(Li et al., 2023)\nis a structure-aware legal case retrieval model uti-\nlizing the structural information in legal case doc-\numents. GEAR(Qin et al., 2024) is a generative\nretrieval framework that explicitly integrates judg-\n3https://huggingface.co/google-bert/\nbert-base-chinese\n4https://github.com/thunlp/OpenCLaP\n5https://modelscope.cn/models/fengshan/\nChatLaw-Text2Vec\n\nPrompt:\nYou are a lawyer specializing in criminal law. Based on Chinese criminal law,\nplease determine which of the following candidate charges the given facts align with.\nThe candidate charges and their corresponding four elements are as follows:\n[Four Elements of Candidate Charges].\nThe four elements represent the core factors for determining the constitution of a criminal charge.\n[The basic concepts of the Four-Element Theory]\nPlease Compare the case facts to determine which charge’s four elements they align with, thereby identifying the charge.\nTable 7: Prompt template for adding the Four-Element Theory and specific four elements of crime in charge\ndisambiguation.\nMethod\nGPT-4o\nGPT-\n4o+Article\nLegal-COT\nGPT-\n4o+FETLLM\nGPT-\n4o+FETExperts\nPre-task\nNone\nNone\nNone\nLLM-\ngenerated\nfour elements\nExpert-\nannotated\nfour elements\nPrompt\nYou are a lawyer specializing in criminal law. Based on Chinese criminal law, please\ndetermine which of the following candidate charges the given facts align with.\nCandidate\ncharges\nare\nas\nfollows:\n#Candidate\nCharges\nThe\ncandidate\ncharges and rel-\nevant legal arti-\ncles are as fol-\nlows:\n#Candi-\ndate Charges +\n#Articles\nPlease analyze\nusing the Four\nElements The-\nory\nstep\nby\nstep:\n#details\nabout each step.\nThe\ncandidate\ncharges\nare\nas\nfollows:\n#Candidate\nCharges\nThe candidate charges and their\ncorresponding four elements are\nas follows: #Four Elements of\ncandidate charges. The four\nelements represent the four core\nfactors of a charge. Compare the\ncase facts to determine which\ncharge’s four elements they align\nwith, thereby identifying the\ncharge.\nOutput format: #Format. Note: Only output the charge, no additional information.\nCase facts: #Case Facts.\nTable 8: Prompts of different methods in Similar Charge Disambiguation. # represents a format input.\n\nModel\nR@100\nR@200\nR@500\nR@1000\nBERT\n0.1116\n0.1493\n0.2174\n0.2819\nLawformer\n0.2432\n0.304\n0.4054\n0.4833\nChatLaw\n0.1045\n0.1628\n0.2791\n0.3999\nSAILER\n0.2834\n0.4033\n0.6104\n0.7568\nBGE\n0.4085\n0.5246\n0.6855\n0.7912\nFETLLM\n0.4167\n0.5388\n0.7006\n0.7925\nFETExpert_guided\n0.4201\n0.5396\n0.7010\n0.7927\nTable 9: SCR results on the full set of LeCaRDv2. Bold fonts indicate leading results in each setting. The\nexpert-guided FET method achieved the best performance among all language models and attained the top results in\nboth R@500 and R@1000.\nment prediction with legal document retrieval in a\nsequence-to-sequence manner. Since the output of\nGEAR cannot directly evaluate NDCG, the official\nresults under the same setting are directly refer-\nenced in this paper. LLM and Expert represent the\nresults of retrieval using only the four elements.\nF\nSCR results on the full LeCaRDv2\nDataset\nAs presented in Table 9, we selected several\nrepresentative methods based on sparse retrieval\nand dense retrieval for experiments on the full\nLeCaRDv2 dataset. All language models were not\nfine-tuned. The notes-guided FET method achieved\nthe best performance among all language models,\nattaining top results in both R@500 and R@1000.\nThe results indicate that the conclusions drawn\nfrom the full dataset are consistent with those from\nthe subset, and the notes-guided method demon-\nstrates strong performance.\nG\nA Case Study of LCR\nTable 10 presents a case study on the Crime of\nEmbezzlement. By comparing the four elements\nannotated by experts for the crime in JUREX-4E,\nthe case-specific four elements generated directly\nby the LLM, and those generated by the LLM with\nexpert four elements of charge as guidance, we can\nobserve that:\n1) Incorporating expert fine-grained annotations\nenables the model to better grasp the elements of\na crime, thereby providing more precise element\ncomparison. For example, LLMs can identify the\n“integrity of official duties”, and the subjective as-\npect “Intentional” can be interpreted as “having the\npurpose of illegally possessing public or private\nproperty”, highlighting the characteristics of “of-\nficial duties”. Capturing the core information of\nthe case is crucial for matching cases with similar\nfacts.\n2) LLMs can conduct case-tailored specific anal-\nysis based on the constitutive elements of a crime.\nBlue parts show the LLMs can better analyze the\ndefendant’s workplace and the actions taken in the\ncase, which reflects the significance of specific and\naccurate legal knowledge.\n\nDocument\n[Head of document]...In April 201X, Company A appointed B as the Sales Manager\nand Deputy Manager of the Catering Department, responsible for collecting outstand-\ning debts. From December 201X to April 201Y, B embezzled X yuan from six units’\npayments for personal investment and consumption. In May 201Y, knowing that the\ncompany required him to reconcile accounts with the finance department, B fled after\nchanging his phone number. It was found that after the case broke out, B’s father C\nhad returned the X yuan to Company A.\nCharge\nCrime of Embezzlement\nExperts Four-elements\nLLM-generated\nExperts-guided\nFour Elements\nObject: The ownership of\nproperty and integrity of\nduties in companies, en-\nterprises, or other organi-\nzations.\nThe objects in-\nclude movable and immov-\nable, tangible and intangi-\nble property of the perpe-\ntrator’s organization.\nObjective Aspect: Using\npositional convenience to\nillegally appropriate orga-\nnizational property in a\nsignificant amount.\nSubject:\nSpecial sub-\nject, namely personnel of\ncompanies, enterprises, or\nother organizations.\nSubjective\nAs-\npect:Intentional,\nwith\nthe purpose of illegally\npossessing property.\nObject: The ownership of\nproperty of companies, en-\nterprises, or other organi-\nzations.\nObjective Aspect: Using\nthe convenience of one’s\nposition to illegally appro-\npriate the property of the\norganization, in a signifi-\ncant amount.\nSubject:\nPersonnel of\ncompanies, enterprises, or\nother organizations. The\ndefendant is B.\nSubjective Aspect: Inten-\ntional.\nObject: The ownership of\nproperty of companies, en-\nterprises, or other organi-\nzations and the integrity of\nofficial duties. In this case,\ncrime objetcs are the prop-\nerties of the company, en-\nterprise, or other organiza-\ntion where the perpetrator\nis employed.\nObjective Aspect:\nThe\nact of using one’s po-\nsition for personal gain\nby\nillegally\nappropriat-\ning the property of the\norganization, where the\namount involved is signifi-\ncant. Specifically, B used\nhis position to embezzle X\nyuan from the payments\nof six units for personal\ninvestment and consump-\ntion.\nSubject:\nSpecial sub-\nject, namely personnel of\ncompanies, enterprises, or\nother organizations. De-\nfendant B is the Sales\nManager and Deputy Man-\nager of the Catering De-\npartment of the company.\nSubjective Aspect: Inten-\ntional, with the purpose of\nillegally possessing public\nor private property.\nTable 10: The results of the case four elements obtained through different methods in LCR. Notes refer to expert\nnotes related to the charge retrieved during the search. LLM-generated and Experts-guided indicate whether\nusing JUREX-4E’s four-elements of the crime to guide LLM in generating the four elements. Red parts mean the\nknowledge from JUREX-4E, while blue parts show the LLM’s internal knowledge. By incorporating JUREX-4E,\nthe model better emphasizes conviction and sentencing related information and provides more detailed descriptions\nof critical case facts.\n",
  "metadata": {
    "source_path": "papers/arxiv/JUREX-4E_Juridical_Expert-Annotated_Four-Element_Knowledge_Base_for\n__Legal_Reasoning_b0818e48db91720a.pdf",
    "content_hash": "b0818e48db91720ae4a92c3f552e2eb500b499c12fc579bb8917b8bbdb799ca1",
    "arxiv_id": null,
    "title": "JUREX-4E_Juridical_Expert-Annotated_Four-Element_Knowledge_Base_for\n__Legal_Reasoning_b0818e48db91720a",
    "author": "",
    "creation_date": "D:20250225025017Z",
    "published": "2025-02-25T02:50:17",
    "pages": 16,
    "size": 3054474,
    "file_mtime": 1740470175.1344523
  }
}