{
  "text": "Erwin: A Tree-based Hierarchical Transformer for Large-scale Physical Systems\nMaksim Zhdanov 1 Max Welling 1 2 Jan-Willem van de Meent 1\nAbstract\nLarge-scale physical systems defined on irregular\ngrids pose significant scalability challenges for\ndeep learning methods, especially in the presence\nof long-range interactions and multi-scale cou-\npling. Traditional approaches that compute all\npairwise interactions, such as attention, become\ncomputationally prohibitive as they scale quadrati-\ncally with the number of nodes. We present Erwin,\na hierarchical transformer inspired by methods\nfrom computational many-body physics, which\ncombines the efficiency of tree-based algorithms\nwith the expressivity of attention mechanisms.\nErwin employs ball tree partitioning to organize\ncomputation, which enables linear-time attention\nby processing nodes in parallel within local neigh-\nborhoods of fixed size.\nThrough progressive\ncoarsening and refinement of the ball tree struc-\nture, complemented by a novel cross-ball inter-\naction mechanism, it captures both fine-grained\nlocal details and global features. We demonstrate\nErwin’s effectiveness across multiple domains,\nincluding cosmology, molecular dynamics, and\nparticle fluid dynamics, where it consistently out-\nperforms baseline methods both in accuracy and\ncomputational efficiency.\n1. Introduction\nScientific deep learning is tackling increasingly computa-\ntionally intensive tasks, following the trajectory of computer\nvision and natural language processing. Applications range\nfrom molecular dynamics (MD) (Arts et al., 2023), compu-\ntational particle mechanics (Alkin et al., 2024b), to weather\nforecasting (Bodnar et al., 2024), where simulations often\ninvolve data defined on irregular grids with thousands to\nmillions of nodes, depending on the required resolution and\ncomplexity of the system.\n1AMLab, University of Amsterdam 2CuspAI. Correspondence\nto: Maksim Zhdanov <m.zhdanov@uva.nl>.\nPreprint. Copyright 2025 by the author(s).\nMHSA\nMHSA\nlayer 1\nlayer 2\nlayer 3\nruntime vs. size\nruntime\nreceptive field\nMHSA\nMHSA\nFigure 1. Top: Ball tree attention over a molecular graph. Multi-\nhead self-attention (MHSA) is computed in parallel at fixed hi-\nerarchy levels (bold circles). In the following layers, the tree is\nprogressively coarsened to learn global features, while the partition\nsize is fixed. Bottom: Computational advantages of our model.\nSuch large-scale systems pose a significant challenge to ex-\nisting methods that were developed and validated at smaller\nscales. For example, in computational chemistry, models are\ntypically trained on molecules with tens of atoms (Kov´acs\net al., 2023), while molecular dynamics simulations can\ninvolve well beyond thousands of atoms. This scale dispar-\nity might result in prohibitive runtimes that render models\ninapplicable in high-throughput scenarios such as protein\ndesign (Watson et al., 2023) or screening (Fu et al., 2022).\nA key challenge in scaling to larger system sizes is that\ncomputational methods which work well at small scales\nbreak down at larger scales. For small systems, all pairwise\ninteractions can be computed explicitly, allowing deep learn-\ning models to focus on properties like equivariance (Cohen\n& Welling, 2016). However, this brute-force approach be-\ncomes intractable as the system size grows. At larger scales,\napproximations are required to efficiently capture both long-\nrange effects from slowly decaying potentials or multi-scale\ncoupling (Majumdar et al., 2020). As a result, models val-\nidated only on small systems often lack the architectural\ncomponents necessary for efficient scaling.\n1\narXiv:2502.17019v1  [cs.LG]  24 Feb 2025\n\nErwin Transformer\nThis problem has been extensively studied in computational\nmany-body physics (Hockney & Eastwood, 2021), where\nthe need for evaluating long-range potentials for large-scale\nparticle systems led to the development of sub-quadratic\ntree-based algorithms (Barnes & Hut, 1986; Carrier et al.,\n1988). These methods are based on the intuition that distant\nparticles can be approximated through their mean field ef-\nfect rather than individual interactions (Pfalzner & Gibbon,\n1996). The computation then is structured using hierarchi-\ncal trees to efficiently organize the computation at multi-\nple scales. While highly popular for numerical simulations,\nthese tree-based methods have seen limited adoption in deep\nlearning due to poor synergy with GPU architectures.\nTransformers (Vaswani et al., 2017), on the other hand,\nemploy the highly optimized attention mechanism, which\ncomes with the quadratic cost of computing all-to-all in-\nteractions. In this work, we combine the efficiency of hi-\nerarchical tree methods with the expressivity of attention\nto create a scalable architecture for the processing of large-\nscale particle systems. Our approach leverages ball trees\nto organize computation at multiple scales, enabling both\nlocal accuracy and global feature capture while maintaining\nlinear complexity in the number of nodes.\nThe main contributions of the work are the following:\n• We introduce ball tree partitioning for efficient point\ncloud processing, enabling linear-time self-attention\nthrough localized computation within balls at different\nhierarchical levels.\n• We present Erwin, a hierarchical transformer that pro-\ncesses data through progressive coarsening and re-\nfinement of ball tree structures, effectively capturing\nboth fine-grained local interactions and global features\nwhile maintaining computational efficiency.\n• We validate Erwin’s performance across multiple large-\nscale physical domains:\n– Capturing long-range interactions (cosmology)\n– Computational efficiency (molecular dynamics)\n– Model expressivity on large-scale multi-scale phe-\nnomena (turbulent fluid dynamics)\nachieving state-of-the-art performance in both compu-\ntational efficiency and prediction accuracy.\n2. Related Works: sub-quadratic attention\nOne way to avoid the quadratic cost of self-attention is to lin-\nearize attention by performing it on non-overlapping patches.\nFor data on regular grids, like images, the SwinTransformer\n(Liu et al., 2021) achieves this by limiting attention to local\nwindows with cross-window connection enabled by shift-\ning the windows. However, for irregular data such as point\nclouds or non-uniform meshes, one first needs to induce a\nstructure that will allow for patching. Several approaches\n(Liu et al., 2023; Sun et al., 2022) transform point clouds\ninto sequences, most notably, PointTransformer v3 (Wu\net al., 2024), which projects points into voxels and orders\nthem using space-filling curves (e.g., Hilbert curve). While\nscalable, these curves introduce artificial discontinuities that\ncan break local spatial relationships.\nParticularly relevant to our work are hierarchical attention\nmethods. In the context of 1D sequences, approaches like\nthe H-transformer (Zhu & Soricut, 2021) and Fast Multipole\nAttention (Kang et al., 2023) approximate self-attention\nthrough multi-level decomposition: tokens interact at full\nresolution locally while distant interactions are computed\nusing learned or fixed groupings at progressively coarser\nscales. For point clouds, OctFormer (Wang, 2023) converts\nspatial data into a sequence by traversing an octree, ensuring\nspatially adjacent points are consecutive in memory. While\nconceptually similar to our approach, OctFormer relies on\ncomputationally expensive octree convolutions, whereas our\nutilization of ball trees leads to significant efficiency gains.\nRather than using a hierarchical decomposition, another\nline of work proposes cluster attention (Janny et al., 2023;\nAlkin et al., 2024a). These methods first group points into\nclusters and aggregate their features at the cluster centroids\nthrough message passing or cross-attention. After comput-\ning attention between the centroids, the updated features\nare then distributed back to the original points. While these\napproaches achieve the quadratic cost only in the number\nof clusters, they introduce an information bottleneck at the\nclustering step that may sacrifice fine-grained details and\nfail to capture features at multiple scales - a limitation our\nhierarchical approach aims to overcome.\n3. Background\nOur work revolves around attention, which we aim to lin-\nearize by imposing structure onto point clouds using ball\ntrees. We formally introduce both concepts in this section.\n3.1. Attention\nThe standard self-attention mechanism is based on the scaled\ndot-product attention (Vaswani et al., 2017). Given a set X\nof N input feature vectors of dimension C, self-attention is\ncomputed as\nQ, K, V = XWq, XWk, XWv\nAtt(Q, K, V) = softmax\n \nQKT\n√\nC′ + B\n!\nV\n(1)\nwhere Wq, Wk, Wv ∈RC×C′ are learnable weights and\nB ∈RN×N is the bias term.\nMulti-head self-attention (MHSA) improves expressivity\nby computing attention H times with different weights and\n2\n\nErwin Transformer\nconcatenating the output before the final projection:\nMHSA(X) = [Y1, · · · , YH] WO\nYi = Att(XWi\nq, XWi\nk, XWi\nv)\n(2)\nwhere [·, · · · , ·] denotes concatenation along the feature di-\nmension, and Wi\nq, Wi\nk, Wi\nv ∈RC×(C′/H) and WO ∈\nRC×C′ are learnable weights.\nThe operator explicitly computes interactions between all\nelements in the input set without any locality constraints.\nThis yields the quadratic computational cost w.r.t. the input\nset size O(N 2). Despite being heavily optimized (Dao,\n2024), this remains a bottleneck for large-scale applications.\n3.2. Ball tree\nA ball tree is a hierarchical data structure that recursively par-\ntitions points into nested sets of equal size, where each set is\nrepresented by a ball that covers all the points in the set. As-\nsume we operate on the d-dim. Euclidean space\n\u0000Rd, || · ||2\n\u0001\nwhere we have a point cloud (set) P = {p1, ..., pn} ⊂Rd.\nDefinition 3.1 (Ball). A ball is a region bounded by a hy-\npersphere in Rd. Each ball is represented by the coordinates\nof its center c ∈Rd and radius r ∈R+:\nB = B(c, r) = {z ∈Rd | ||z −c||2 ≤r}.\n(3)\nWe will omit the parameters (c, r) for brevity from now on.\nDefinition 3.2 (Ball Tree). A ball tree T on point set P is a\nhierarchical sequence of partitions {L0, L1, ..., Lm}, where\neach level Li consists of disjoint balls that cover P. At the\nleaf level i = 0, the nodes are the original points:\nL0 = {{pj} | pj ∈P}\nFor each subsequent level i > 0, each ball B ∈Li is formed\nby merging two balls at the previous level B1, B2 ∈Li−1:\nLi = {{B1 ∪B2} | B1, B2 ∈Li−1}\n(4)\nsuch that its center is computed as the center of mass:\ncB = |B1|c1 + |B2|c2\n|B1| + |B2|\nand its radius is determined by the furthest point it contains:\nrB = max{||p −cB||2 | p ∈B1 ∪B2}\nwhere |B| denotes the number of points contained in B.\nTo construct the ball tree, we recursively split the data points\ninto two sets starting from P. In each recursive step, we find\nthe dimension of the largest spread (i.e. the max−min value)\nand split at its median (Pedregosa et al., 2012), constructing\ncovering balls per Def.3.2. For details, see Appendix Alg.11.\n1Note that since we split along coordinate axes, the resulting\nstructure depends on the orientation of the input data and thus\nbreaks rotation invariance. We will rely on this property in Sec-\ntion 4.1 to implement cross-ball connections.\nTree Completion\nTo enable efficient implementation, we\nwant to work with perfect binary trees, i.e. trees where all\ninternal nodes have exactly two children and all leaf nodes\nappear at the same depth. To achieve this, we pad the leaf\nlevel of a ball tree with virtual nodes, yielding the total\nnumber of nodes 2m, where m = ceil(log2(n)).\n3.2.1. BALL TREE PROPERTIES\nIn the context of our method, there are several properties of\nball trees that enable efficient hierarchical partitioning:\nProposition 3.3 (Ball Tree Properties). The ball tree T\nconstructed as described satisfies the following properties:\n1. The tree is a perfect binary tree.\n2. At each level i, each ball contains exactly 2i leaf nodes.\n3. Balls at each level cover the point set\n[\nB∈Li\nB = P\n∀i ∈{0, ..., m}.\nProposition 3.4 (Contiguous Storage). For a ball tree\nT = {L0, L1, ..., Lm} on point cloud P = {p1, ..., pn},\nthere exists a bijective mapping π : {1, ..., n} →{1, ..., n}\nsuch that points belonging to the same ball B ∈Li have\ncontiguous indices under π.\nAs a corollary, the hierarchical structure at each level can\nbe represented by nested intervals of contiguous indices:\nExample. Let P = {p1, ..., p8}, then a ball tree T =\n{L0, L1, L2, L3} is stored after the permutation π as\nL3\nL2\nL1\nL0 = π(P)\npa pb pc pd pe pf pg ph\nThe contiguous storage property, combined with the fixed\nsize of balls at each level, enables efficient implementation\nthrough tensor operations. Specifically, accessing any ball\nB ∈Li simply requires selecting a contiguous sequence of\n2i indices. For instance, in the example above, for i = 2,\nwe select a:d and e:h to access the balls. Since the balls\nare equal, we can simply reshape L0 to access any level.\nThis representation makes it particularly efficient to imple-\nment our framework’s core operations - ball attention and\ncoarsening/refinement - which we will introduce next.\nAnother important property of ball trees is that while they\ncover the whole point set, they are not required to partition\nthe entire space. Coupled with completeness, it means that\nat each tree level, the nodes are essentially associated with\nthe same scale. This contrasts with other structures such as\noct-trees that cover the entire space and whose nodes at the\nsame level can be associated with regions of different sizes:\n3\n\nErwin Transformer\nBall tree\nOct-tree\nFigure 2. Ball tree vs Oct-tree construction. Colors highlight the\ndifference in scales for nodes including the same number of points.\n4. Erwin Transformer\nFollowing the notation from the background Section 3.2,\nwe consider a point cloud P = {p1, ..., pn} ⊂Rd. Addi-\ntionally, each point is now endowed with a feature vector\nyielding a feature set X = {x1, ..., xn} ⊂RC.\nOn top of the point cloud, we build a ball tree T\n=\n{L0, ..., Lm}. We initialize Lleaf := L0 to denote the cur-\nrent finest level of the tree. As each leaf node contains a\nsingle point, it inherits its feature vector:\nXleaf = {xB = xi | B = {pi} ∈Lleaf}\n(5)\n4.1. Ball tree attention\nBall attention\nFor each ball attention operator, we specify\na level k of the ball tree where each ball B ∈Lk contains 2k\nleaf nodes. The choice of k presents a trade-off: larger balls\ncapture longer-range dependencies, while smaller balls are\nmore resource-efficient. For each ball B ∈Lk, we collect\nthe leaf nodes within B\nleavesB = {B′ ∈Lleaf | B′ ⊂B}\n(6)\nalong with their features from Xleaf\nXB = {xB′ ∈Xleaf | B′ ∈leavesB}\n(7)\nWe then compute self-attention independently on each ball2:\nX′\nB = BAtt(XB) := Att(XBWq, XBWk, XBWv)\n(8)\nwhere weights are shared between balls and the output X′\nB\nmaintains row correspondence with XB.\nComputational cost\nAs attention is computed indepen-\ndently for each ball B ∈Lk, the computational cost is\nreduced from quadratic to linear. Precisely, for ball atten-\ntion, the complexity is O(|B|2 ·\nn\n|B|), i.e. quadratic in the\nball size and linear in the number of balls:\n2For any set of vectors X, we abuse notation by treating X as\na matrix with vectors as its rows.\nAttention O(n2)\nBall Attention O(n)\nFigure 4. For highlighted points, standard attention computes inter-\nactions with all other points in the point cloud, while ball attention\nonly considers points within their balls.\nPositional encoding\nWe introduce positional information\nto the attention layer in two ways. First, we augment the fea-\ntures of leaf nodes with their relative positions with respect\nto the ball’s center of mass (relative position embedding):\nRPE :\nXB = XB + (PB −cB)Wpos\n(9)\nwhere PB contains positions of leaf nodes, cB is the center\nof mass, and Wpos is a learnable projection. This allows\nthe layer to incorporate geometric structure within each ball.\nSecond, we introduce a distance-based attention bias:\nBB = −σ2||cB′ −cB′′||2,\nB′, B′′ ∈leavesB\n(10)\nwith a learnable parameter σ ∈R (Wessels et al., 2024).\nThe term decays rapidly as the distance between two nodes\nincreases which enforces locality and helps to mitigate po-\ntential artifacts from the tree building, particularly in cases\nwhere distant points are grouped together.\nCross-ball connection\nTo increase the receptive field of\nour attention operator, we implement cross-ball connections\ninspired by the shifted window approach in Swin Trans-\nformer (Liu et al., 2021). There, patches are displaced\ndiagonally by half their size to obtain two different im-\nage paTreertitioning configurations. This operation can be\nequivalently interpreted as keeping the patches fixed while\nsliding the image itself.\nFollowing this interpretation, we rotate the point cloud and\nconstruct the second ball tree Trot = {Lrot\n0 , ..., Lrot\nm } which\ninduces a permutation πrot of leaf nodes (see Fig. 3, center).\nWe can then compute ball attention on the rotated config-\nuration by first permuting the features according to πrot,\napplying attention, and then permuting back:\nX′\nB = π−1\nrot (BAtt (πrot (XB)))\n(11)\nBy alternating between the original and rotated configura-\ntions in consecutive layers, we ensure the interaction be-\ntween leaf nodes in otherwise separated balls.\nTree coarsening/refinement\nFor larger systems, we are\ninterested in coarser representations to capture features at\n4\n\nErwin Transformer\npoint cloud P\nball tree, level Lk\ncoarsened tree\nball tree, level Lk+1\nrefinement\ncoarsening\nrotate\nball tree\noriginal balls\nleaves\n”rotated” balls\nErwin Transformer\nEncoder\nDecoder\nBottleneck\nEmbedding\nPoint Cloud\nBall Tree\nMPNN\nLNorm\n+ RPE\nAttention\nLNorm\nSwiGLU\nErwinBlock\n×D\nErwinLayer\n×S\nCoarsening\nFigure 3. Overview of Erwin. Left: A sequence of two ball attention layers with intermediate tree coarsening. In every layer, attention is\ncomputed on partitions of size 16, which correspond to progressively higher levels of hierarchy. Center (top): Coarsening and refinement\nof a ball tree. Center (bottom): Building a tree on top of a rotated configuration for cross-ball interaction. Right: Architecture of Erwin.\nlarger scales. The coarsening operation allows us to hier-\narchically aggregate information by pooling leaf nodes to\nthe centers of containing balls at l levels higher (see Fig.\n3, top, l = 1). Suppose the leaf level is k. For every ball\nB ∈Lk+l, we concatenate features of all interior leaf nodes\nalong with their relative positions with respect to cB and\nproject them to a higher-dimensional representation:\nxB =\n \nM\nB′∈leavesB\n[xB′, cB′ −cB]\n!\nWc\n(12)\nwhere L denotes leaf-wise concatenation, and Wc ∈\nRC′×2l(C+d) is a learnable projection that increases the fea-\nture dimension to maintain expressivity. After coarsening,\nballs at level k+l become the new leaf nodes, Lleaf := Lk+l,\nwith features Xleaf := {xB | B ∈Lk+l}. To highlight the\nsimplicity of our method, we provide the pseudocode3:\n# coarsening ball tree\nx = rearrange([x, rel.pos], \"(n 2l) d →n (2l d)\") @ Wc\npos = reduce(pos, \"(n 2l) d →n d\", \"mean\")\nThe inverse operation, refinement, allocates information\nfrom a coarse representation back to finer scales. More\nprecisely, for a ball B ∈Lk, its features are distributed back\nto the nodes at level Lk−l contained within B as\n{xB′ | B′ ∈Lk−l} = [xB, PB −cB] Wr\n(13)\nwhere PB contains positions of all nodes at level k−l within\nball B with center of mass cB, and Wr ∈R2lC×(C′+d) is\na learnable projection. After refinement, Lleaf and Xleaf are\nupdated accordingly. In pseudocode:\n# refining ball tree\nx = [rearrange(x, \"n (2l d) →(n 2l) d\"), rel.pos] @ Wr\n3We use einops (Rogozhnikov, 2022) primitives.\n4.2. Model architecture\nWe are now ready to describe the details of the main model\nto which we refer as Erwin4 (see Fig. 3) - a hierarchical\ntransformer operating on ball trees.\nEmbedding\nAt the embedding phase, we first construct\na ball tree on top of the input point cloud and pad the leaf\nlayer to complete the tree, as described in Section 3.2. To\ncapture local geometric features, we employ a small-scale\nMPNN, which is conceptually similar to PointTransformer’s\nembedding module using sparse convolution. When input\nconnectivity is not provided (e.g. mesh), we utilize the ball\ntree structure for a fast nearest neighbor search.\nErwinBlock\nThe core building block of Erwin follows a\nstandard pre-norm transformer structure: LayerNorm fol-\nlowed by ball attention with a residual connection, and a\nSwiGLU feed-forward network (Shazeer, 2020). For the\nball attention, the size 2k of partitions is a hyperparameter.\nTo ensure cross-ball interaction, we alternate between the\noriginal and rotated ball tree configurations, using an even\nnumber of blocks per ErwinLayer in our experiments.\nOverall architecture\nFollowing a UNet structure (Ron-\nneberger et al., 2015; Wu et al., 2024), Erwin processes\nfeatures at multiple scales through encoder and decoder\npaths (Fig. 3, right). The encoder progressively coarsens the\nball tree while increasing feature dimensionality to maintain\nexpressivity. The coarsening factor is a hyperparameter that\ntakes values that are powers of 2. At the decoder stage,\nthe representation is refined back to the original resolution,\nwith skip connections from corresponding encoder levels\nenabling multi-scale feature integration.\n4We pay homage to Swin Transformer as our model based on\nrotating windows instead of sliding, hence Rwin →Erwin.\n5\n\nErwin Transformer\nFigure 5. Left: Computational cost of Erwin. We split the total\nruntime into building a ball tree and running a model. The input\nis a batch of 16 point clouds, each of size n. We fit a power law\nwhich indicates close to linear scaling. Right: Receptive field\nof MPNN vs Erwin, n = 800. A node is in the receptive field\nif changing its features affects the target node’s output. MPNN\nconsists of 6 layers, each node connected to 16 nearest neighbours.\n5. Experiments\nImplementation details for all experiments are given in Ap-\npendix C. The code is available at anonymized link.\nExtended experiments are given in Appendix B, including\nan additional experiment on airflow pressure modelling.\nComputational cost\nTo experimentally evaluate Erwin’s\nscaling, we learn the power-law5 form Runtime = C · nβ\nby first applying the logarithm transform to both sides and\nthen using the least square method to evaluate β. The result\nis an approximately linear scaling with β = 1.054 with\nR2 = 0.999, see Fig. 5, left. Ball tree construction accounts\nfor only a fraction of the overall time, proving the efficiency\nof our method for linearizing attention for point clouds.\nReceptive field\nOne of the theoretical properties of our\nmodel is that with sufficiently many layers, its receptive\nfield is global. To verify this claim experimentally, for an\narbitrary target node, we run the forward pass of Erwin\nand MPNN and compute gradients of the node output with\nrespect to all input nodes’ features. If the gradient is non-\nzero, the node is considered to be in the receptive field of\nthe target node. The visualization is provided in Fig. 5,\nright, where we compare the receptive field of our model\nwith that of MPNN. As expected, the MPNN has a limited\nreceptive field, as it cannot exceed N hops, where N is\nthe number of message-passing layers. Conversely, Erwin\nimplicitly computes all-to-all interactions, enabling it to\ncapture long-range interactions in data.\n5.1. Cosmological simulations\nTo demonstrate our model’s ability to capture long-range\ninteractions, we use the cosmology benchmark (Balla et al.,\n2024) which consists of large-scale point clouds represent-\ning potential galaxy distributions.\n5We only use data for n ≥1024 to exclude overhead costs.\n64\n512\n2, 048\n8, 192\nTraining set size\n0.6\n0.7\n0.8\n0.9\n1.0\n1.1\n1.2\nTest MSE\nScaling with training set size\nSEGNN (lmax = 1)\nSEGNN (lmax = 2)\nNequIP (lmax = 1)\nNequIP (lmax = 2)\nErwin-S (Ours)\nErwin-M (Ours)\nPointTransformer v3\nMPNN\nequivariant\nnon-equivariant\nFigure 6. Test mean-squared error (MSE) on the predicted veloci-\nties as a function of training set size for the cosmology task, 5 runs\nper point. Point transformers indicate favourable scaling surpass-\ning graph-based models with sufficiently many training samples.\nDataset\nThe dataset is derived from N-body simulations\nthat evolve dark matter particles from the early universe\nto the present time. After the simulation, gravitationally\nbound structures (halos) are indicated, from which the 5000\nheaviest ones are selected as potential galaxy locations. The\nhalos form local clusters through gravity while maintaining\nlong-range correlations that originated from interactions in\nthe early universe before cosmic expansion, reflecting the\ninitial conditions of simulations.\nTask\nThe input is a point cloud X ∈R5000×3, where\neach row corresponds to a galaxy and column to x, y, z\ncoordinate respectively. The task is a regression problem\nto predict the velocity of every galaxy Y ∈R5000×3. We\nvary the size of the training dataset from 64 to 8192, while\nthe validation and test datasets have a fixed size of 512. The\nmodels are trained using mean squared error loss\nL = MSE( ˆY , Y )\nbetween predicted and ground truth velocities.\nResults\nThe results are shown in Fig. 6. We compare\nagainst multiple equivariant (NequIP (Batzner et al., 2021),\nSEGNN (Brandstetter et al., 2022)) and non-equivariant\n(MPNN (Gilmer et al., 2017), PointTransformer v3 (Wu\net al., 2024)) baselines. In the small data regime, graph-\nbased equivariant models are preferable. However, as the\ntraining set size increases, their performance plateaus. We\nnote that this is also the case for non-equivariant MPNN,\nsuggesting that the issue might arise from failing to capture\nmedium to large-scale interactions, where increased local\nexpressivity of the model has minimal impact. Conversely,\ntransformer-based models scale favorably with the training\nset size and eventually surpass graph-based models, high-\nlighting their ability to capture both small and large-scale\ninteractions. Our model demonstrates particularly strong\nperformance and significantly outperforms other baselines\nfor larger training set sizes.\n6\n\nErwin Transformer\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\n2.25\n2.50\nSpeedup over MPNN (medium), times\n0.69\n0.70\n0.71\n0.72\n0.73\nTest NLL\nPerformance vs. Runtime\nErwin (Ours)\nPointTransformer v3\nMPNN\nPointNet++\n43M\n19M\n4M\n46M\n26M\n6M\n0.8M\n0.2M\n0.1M\n7M\n3M\n1M\nFigure 7. Test negative log-likelihood (NLL) of the predicted ac-\nceleration distribution for the molecular dynamics task (averaged\nover 3 runs). The baseline MPNN is taken from (Fu et al., 2022).\nThe size of the markers reflects the number of parameters.\n5.2. Molecular dynamics\nMolecular dynamics (MD) is essential for understanding\nphysical and biological systems at the atomic level but re-\nmains computationally expensive even with neural network\npotentials due to all-atom force calculations and femtosec-\nond timesteps required to maintain stability and accuracy.\nFu et al. (2022) suggested accelerating MD simulation\nthrough coarse-grained dynamics with MPNN. In this ex-\nperiment, we take a different approach and instead operate\non the original representation but improve the runtime by\nemploying our hardware-efficient model. Therefore, the\nquestion we ask is how much we can accelerate a simulation\nw.r.t. an MPNN without compromising the performance.\nDataset\nThe dataset consists of single-chain coarse-\ngrained polymers (Webb et al., 2020; Fu et al., 2022) sim-\nulated using MD. Each system includes 4 types of coarse-\ngrained beads interacting through bond, angle, dihedral, and\nnon-bonded potentials. The training set consists of polymers\nwith the repeated pattern of the beads while the polymers in\nthe test set are constructed by randomly sampling sequences\nof the beads thus introducing a challenging distribution\nshift. The training set contains 100 short trajectories (50k\nτ), while the test set contains 40 trajectories that are 100\ntimes longer. Each polymer chain contains approximately\n890 beads on average.\nTask\nWe follow the experimental setup from Fu et al.\n(2022). The model takes as input a polymer chain of N\ncoarse-grained beads. Each bead has a specific weight and\nis associated with the history { ˙xt−16∆t, ..., ˙xt−∆t} of (nor-\nmalized) velocities from 16 previous timesteps at intervals\nof ∆t = 5τ. The model predicts the mean µt ∈RN×3\nand variance σ2\nt ∈RN×3\n+\nof (normalized) acceleration for\neach bead, assuming a normal distribution. We train using\nnegative log-likelihood loss\nL = −log N(ˆ¨xt|µt, σ2\nt )\nTable 1. Ablation on the cosmology task. Increasing window size\nimproves performance at the cost of slower runtime (Erwin-S).\nBALL SIZE\n256\n128\n64\n32\nTEST LOSS\n0.595\n0.603\n0.612\n0.620\nRUNTIME, MS\n229.6\n165.2\n135.3\n126.0\nTable 2. Ablation study (MD task, Erwin-S) on the architectural\nchoices: using MPNN in embedding, RPE (see Eq. 9) and cross-\nball connection via rotating trees. Runtime includes building trees.\nTEST LOSS\nRUNTIME, MS\nW/O\n0.738\n25.15\n+ MPNN\n0.720\n26.07\n+ RPE\n0.715\n26.05\n+ ROTATING TREE\n0.712\n26.73\nbetween predicted and ground truth accelerations computed\nfrom the ground truth trajectories.\nResults\nThe results are given in Fig. 7. As baselines, we\nuse MPNN (Gilmer et al., 2017) as well as two hardware-\nefficient architectures: PointNet++ (Qi et al., 2017) and\nPointTransformer v3 (Wu et al., 2024). Notably, model\nchoice has minimal impact on performance, potentially due\nto the absence of long-range interactions as the CG beads\ndo not carry any charge. Furthermore, it is sufficient to\nonly learn local bonded interactions. There is, however, a\nconsiderable improvement in runtime for Erwin (1.7 −2.5\ntimes depending on the size), which is only matched by\nsmaller MPNN or PointNet++, both having significantly\nhigher test loss.\n5.3. Turbulent fluid dynamics\nIn the last experiment, we demonstrate the expressivity of\nour model by simulating turbulent fluid dynamics. The prob-\nlem is notoriously challenging due to multiple factors: the\ninherently nonlinear behaviour of fluids, the multiscale and\nchaotic nature of turbulence, and the presence of long-range\ndependencies. Moreover, the geometry of the simulation do-\nmain and the presence of objects introduce complex bound-\nary conditions thus adding another layer of complexity.\nDataset\nWe use EAGLE (Janny et al., 2023), a large-scale\nbenchmark of unsteady fluid dynamics. Each simulation\nincludes a flow source (drone) that moves in 2D environ-\nments with different boundary geometries producing air-\nflow. The time evolution of velocity and pressure fields\nis recorded along with dynamically adapting meshes. The\ndataset contains 600 different geometries of 3 types, with\napproximately 1.1 million 2D meshes averaging 3388 nodes\neach. The total dataset includes 1184 simulations with 990\ntime steps per simulation. The dataset is split with 80% for\ntraining and 10% each for validation and testing.\n7\n\nErwin Transformer\nFigure 8. The norm of the velocity field at different steps of the\nrollout trajectories.\nTask\nWe follow the original experimental setup of the\nbenchmark. The input is the velocity V ∈RN×2 and pres-\nsure P ∈RN×2 fields evaluated at every node of the mesh\nin the time step t along with the type of the node. The task\nis to predict the state of the system at the next time step t+1.\nThe training is done by predicting a trajectory of states of\nlength 5 and optimizing the loss\nL =\n5\nX\ni=1\n\u0010\nMSE(Vt+i, ˆVt+i) + α MSE(Pt+i, ˆPt+i)\n\u0011\n,\nwhere α = 0.1 is the parameter that balances the importance\nof the pressure field over the velocity field.\nResults\nFor comparison, we include the baselines from\nthe original benchmark: MeshGraphNet (MGN; Pfaff et al.,\n2021), GAT (Velickovic et al., 2018), DilResNet (DRN;\nStachenfeld et al., 2021) and EAGLE (Janny et al., 2023)6.\nThe first two baselines are based on message-passing, while\nDilResNet operates on regular grids hence employing inter-\npolation for non-uniform meshes. EAGLE uses message-\npassing to pool the mesh to a coarser representation with\na fixed number of clusters, on which attention is then com-\nputed. The quantitative results are given in Table 3 and\nunrolling trajectories are shown in Fig. 8 and in Appendix B.\nErwin demonstrates strong results on the benchmark and\noutperforms every baseline, performing especially well at\npredicting pressure. In terms of inference time and memory\nconsumption, Erwin achieves substantial gains over EA-\nGLE, being 3 times faster and using 8 times less memory.\n5.4. Ablation study\nWe also conducted an ablation study to examine the effect\nof increasing ball sizes on the model’s performance in the\ncosmology experiment, see Table 1. Given the presence of\nlong-range interactions in the data, larger window sizes (and\nthus receptive fields) improve model performance, albeit at\n6We additionally trained UPT (Alkin et al., 2024a), but were\nnot able to obtain competitive results in our initial experiments.\nTable 3. RMSE on velocity V and pressure P fields across different\nprediction horizons (mean ± std over 5 runs). Inference runtime\nand memory use computed for a batch of 8, 3500 nodes on average.\nHORIZON\n+1\n+50\nTIME\nMEM.\nFIELD / UNIT\nV\nP\nV\nP\n(MS)\n(GB)\nMGN\n0.081\n0.43\n0.592\n2.25\n40\n0.7\nGAT\n0.170\n64.6\n0.855\n163\n44\n0.5\nDRN\n0.251\n1.45\n0.537\n2.46\n42\n0.2\nEAGLE\n0.053\n0.46\n0.349\n1.44\n30\n1.5\nERWIN\n0.044\n0.31\n0.281\n1.15\n11\n0.2\n(OURS)\n±0.001\n±0.01\n±0.001\n±0.06\nthe cost of increased computational runtime. Our architec-\ntural ablation study on the MD task (Table 2) reveals that\nusing MPNN at the embedding step produces substantial\nimprovements, likely due to its effectiveness in learning\nlocal interactions.\n6. Conclusion\nWe present Erwin, a hierarchical transformer that uses ball\ntree partitioning to process large-scale physical systems with\nlinear complexity. Erwin achieves state-of-the-art perfor-\nmance on both the cosmology benchmark (Balla et al., 2024)\nand the EAGLE dataset (Janny et al., 2023), demonstrating\nits effectiveness across diverse physical domains. The effi-\nciency of Erwin makes it a suitable candidate for any tasks\nthat require modeling large particle systems, such as tasks in\ncomputational chemistry (Fu et al., 2024) or diffusion-based\nmolecular dynamics (Jing et al., 2024).\nLimitations and Future Work\nBecause Erwin relies on\nperfect binary trees, we need to pad the input set with virtual\nnodes, which induces computational overhead for ball atten-\ntion computed over non-coarsened trees (first ErwinBlock).\nThis issue can be circumvented by employing learnable\npooling to the next level of the ball tree, which is always\nfull, ensuring the remaining tree is perfect. Whether we can\nperform such pooling without sacrificing expressivity is a\nquestion that we leave to future research.\nErwin was developed by jointly optimizing for expressivity\nand runtime. As a result, certain architectural decisions\nare not optimal with respect to memory usage. In partic-\nular, we use a distance-based attention bias (see Eq. 10),\nfor which both computational and memory requirements\ngrow quadratically with the ball size. Developing alterna-\ntive ways of introducing geometric information into atten-\ntion computation could reduce these requirements. Finally,\nErwin is neither permutation nor rotation equivariant, al-\nthough rotation equivariance can be incorporated without\ncompromising scalability. One possible approach is to use\ngeometric algebra transformers (Brehmer et al., 2023) and\nomit the proposed cross-ball connections, as they rely on\ninvariance-breaking tree building.\n8\n\nErwin Transformer\nAcknowledgement\nWe are grateful to Evgenii Egorov and Ana Luˇci´c for their\nfeedback and inspiration. This research was supported by\nMicrosoft Research AI4Science.\nReferences\nAlkin, B., F¨urst, A., Schmid, S., Gruber, L., Holzleitner,\nM., and Brandstetter, J. Universal physics transformers:\nA framework for efficiently scaling neural operators. In\nConference on Neural Information Processing Systems\n(NeurIPS), 2024a.\nAlkin, B., Kronlachner, T., Papa, S., Pirker, S., Lichteneg-\nger, T., and Brandstetter, J. Neuraldem – real-time sim-\nulation of industrial particulate flows. arXiv preprint\narXiv:2411.09678, 2024b.\nArts, M., Satorras, V., Huang, C.-W., Zuegner, D., Federici,\nM., Clementi, C., No´e, F., Pinsler, R., and Berg, R. Two\nfor one: Diffusion models and force fields for coarse-\ngrained molecular dynamics. Journal of chemical theory\nand computation, 19, 09 2023. doi: 10.1021/acs.jctc.\n3c00702.\nBalla, J., Mishra-Sharma, S., Cuesta-L´azaro, C., Jaakkola,\nT. S., and Smidt, T. E. A cosmic-scale benchmark for\nsymmetry-preserving data processing. arXiv preprint\narXiv:2410.20516, 2024.\nBarnes, J. and Hut, P. A hierarchical O(N log N) force-\ncalculation algorithm. Nature, 324(6096):446–449, 1986.\ndoi: 10.1038/324446a0.\nBatzner, S. L., Musaelian, A., Sun, L., Geiger, M., Mailoa,\nJ. P., Kornbluth, M., Molinari, N., Smidt, T. E., and\nKozinsky, B. E(3)-equivariant graph neural networks for\ndata-efficient and accurate interatomic potentials. Nature\nCommunications, 13, 2021.\nBodnar, C., Bruinsma, W. P., Lucic, A., Stanley, M., Brand-\nstetter, J., Garvan, P., Riechert, M., Weyn, J., Dong, H.,\nVaughan, A., Gupta, J. K., Tambiratnam, K., Archibald,\nA., Heider, E., Welling, M., Turner, R. E., and Perdikaris,\nP. Aurora: A foundation model of the atmosphere. arXiv\npreprint arXiv:2405.13063, 2024.\nBrandstetter, J., Hesselink, R., van der Pol, E., Bekkers,\nE. J., and Welling, M. Geometric and physical quantities\nimprove E(3) equivariant message passing. In Interna-\ntional Conference on Learning Representations (ICLR),\n2022.\nBrehmer, J., de Haan, P., Behrends, S., and Cohen, T. S.\nGeometric algebra transformer. In Conference on Neural\nInformation Processing Systems (NeurIPS), 2023.\nCarrier, J., Greengard, L., and Rokhlin, V. A fast adaptive\nmultipole algorithm for particle simulations. SIAM Jour-\nnal on Scientific and Statistical Computing, 9(4):669–686,\n1988. doi: 10.1137/0909044.\nCohen, T. and Welling, M. Group equivariant convolu-\ntional networks. In International Conference on Machine\nLearning (ICML), 2016.\nDao, T. Flashattention-2: Faster attention with better paral-\nlelism and work partitioning. In International Conference\non Learning Representations (ICLR), 2024.\nFu, X., Xie, T., Rebello, N. J., Olsen, B. D., and Jaakkola,\nT. Simulate time-integrated coarse-grained molecular\ndynamics with multi-scale graph networks. Trans. Mach.\nLearn. Res., 2023, 2022.\nFu, X., Xie, T., Rosen, A. S., Jaakkola, T. S., and Smith,\nJ. Mofdiff: Coarse-grained diffusion for metal-organic\nframework design. In International Conference on Learn-\ning Representations (ICLR), 2024.\nGilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and\nDahl, G. E. Neural message passing for quantum chem-\nistry. In International Conference on Machine Learning\n(ICML), 2017.\nHockney, R. and Eastwood, J. Computer Simulation Using\nParticles.\nCRC Press, 2021.\nISBN 9781439822050.\nURL https://books.google.nl/books?id=\nnTOFkmnCQuIC.\nJanny, S., B´eneteau, A., Nadri, M., Digne, J., Thome, N.,\nand Wolf, C. EAGLE: large-scale learning of turbulent\nfluid dynamics with mesh transformers. In International\nConference on Learning Representations (ICLR), 2023.\nJing, B., St¨ark, H., Jaakkola, T. S., and Berger, B. Genera-\ntive modeling of molecular dynamics trajectories. arXiv\npreprint arXiv:2409.17808, 2024.\nKang, Y., Tran, G., and Sterck, H. D. Fast multipole atten-\ntion: A divide-and-conquer attention mechanism for long\nsequences. arXiv preprint arXiv:2310.11960, 2023.\nKov´acs, D. P., Moore, J. H., Browning, N. J., Batatia, I.,\nHorton, J. T., Kapil, V., Witt, W. C., Magd˘au, I.-B., Cole,\nD. J., and Cs´anyi, G. Mace-off23: Transferable machine\nlearning force fields for organic molecules, 2023.\nLi, Z., Kovachki, N. B., Azizzadenesheli, K., Liu, B., Bhat-\ntacharya, K., Stuart, A. M., and Anandkumar, A. Fourier\nneural operator for parametric partial differential equa-\ntions. In International Conference on Learning Represen-\ntations (ICLR), 2021.\n9\n\nErwin Transformer\nLi, Z., Kovachki, N. B., Choy, C. B., Li, B., Kossaifi, J., Otta,\nS. P., Nabian, M. A., Stadler, M., Hundt, C., Azizzade-\nnesheli, K., and Anandkumar, A. Geometry-informed\nneural operator for large-scale 3d pdes. In Conference on\nNeural Information Processing Systems (NeurIPS), 2023.\nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin,\nS., and Guo, B. Swin transformer: Hierarchical vision\ntransformer using shifted windows. In International Con-\nference on Computer Vision (ICCV), 2021.\nLiu, Z., Yang, X., Tang, H., Yang, S., and Han, S. Flat-\nformer: Flattened window attention for efficient point\ncloud transformer. In Conference on Computer Vision\nand Pattern Recognition(CVPR), 2023.\nLoshchilov, I. and Hutter, F. Decoupled weight decay reg-\nularization. In International Conference on Learning\nRepresentations (ICLR), 2019.\nMajumdar, S., Sun, J., Golding, B., Joe, P., Dudhia, J.,\nCaumont, O., Gouda, K. C., Steinle, P., Vincendon, B.,\nWang, J., and Yussouf, N. Multiscale forecasting of high-\nimpact weather: Current status and future challenges.\nBulletin of the American Meteorological Society, 102:\n1–65, 10 2020. doi: 10.1175/BAMS-D-20-0111.1.\nPedregosa, F., Varoquaux, G., Gramfort, A., Michel, V.,\nThirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,\nWeiss, R., Dubourg, V., VanderPlas, J., Passos, A., Cour-\nnapeau, D., Brucher, M., Perrot, M., and Duchesnay, E.\nScikit-learn: Machine learning in python. arXiv preprint\narXiv:1201.0490, 2012.\nPfaff, T., Fortunato, M., Sanchez-Gonzalez, A., and\nBattaglia, P. W. Learning mesh-based simulation with\ngraph networks. In International Conference on Learning\nRepresentations (ICLR), 2021.\nPfalzner, S. and Gibbon, P. Many-Body Tree Methods in\nPhysics. Cambridge University Press, 1996.\nQi, C. R., Yi, L., Su, H., and Guibas, L. J. Pointnet++: Deep\nhierarchical feature learning on point sets in a metric\nspace. In Conference on Neural Information Processing\nSystems (NeurIPS), 2017.\nRogozhnikov, A. Einops: Clear and reliable tensor ma-\nnipulations with einstein-like notation. In International\nConference on Learning Representations (ICLR), 2022.\nRonneberger, O., Fischer, P., and Brox, T. U-net: Convolu-\ntional networks for biomedical image segmentation. In\nInternational Conference on Medical Image Computing\nand Computer-Assisted Intervention (MICCAI), 2015.\nShazeer, N. GLU variants improve transformer. arXiv\npreprint arXiv:2002.05202, 2020.\nStachenfeld, K. L., Fielding, D. B., Kochkov, D., Cranmer,\nM. D., Pfaff, T., Godwin, J., Cui, C., Ho, S., Battaglia,\nP. W., and Sanchez-Gonzalez, A. Learned coarse mod-\nels for efficient turbulence simulation. arXiv preprint\narXiv:2112.15275, 2021.\nSun, P., Tan, M., Wang, W., Liu, C., Xia, F., Leng, Z., and\nAnguelov, D. Swformer: Sparse window transformer\nfor 3d object detection in point clouds. In European\nConference on Computer Vision (ECCV), 2022.\nUmetani, N. and Bickel, B. Learning three-dimensional\nflow for interactive aerodynamic design. ACM Trans.\nGraph., 37(4):89, 2018. URL https://doi.org/\n10.1145/3197517.3201325.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\nis all you need. In Conference on Neural Information\nProcessing Systems (NeurIPS), pp. 5998–6008, 2017.\nVelickovic, P., Cucurull, G., Casanova, A., Romero, A.,\nLi`o, P., and Bengio, Y. Graph attention networks. In\nInternational Conference on Learning Representations\n(ICLR), 2018.\nWang, P.-S.\nOctformer: Octree-based transformers for\n3D point clouds. ACM Transactions on Graphics (SIG-\nGRAPH), 42(4), 2023.\nWang, S., Seidman, J. H., Sankaran, S., Wang, H., Pappas,\nG. J., and Perdikaris, P. Bridging operator learning and\nconditioned neural fields: A unifying perspective. arXiv\npreprint arXiv:2405.13998, 2024.\nWatson, J. L., Juergens, D., Bennett, N. R., Trippe, B. L.,\nYim, J., Eisenach, H. E., Ahern, W., Borst, A. J., Ragotte,\nR. J., Milles, L. F., Wicky, B. I. M., Hanikel, N., Pellock,\nS. J., Courbet, A., Sheffler, W., Wang, J., Venkatesh, P.,\nSappington, I., Torres, S. V., Lauko, A., Bortoli, V. D.,\nMathieu, E., Ovchinnikov, S., Barzilay, R., Jaakkola, T.,\nDiMaio, F., Baek, M., and Baker, D. De novo design of\nprotein structure and function with rfdiffusion. Nature,\n620:1089 – 1100, 2023.\nWebb, M., Jackson, N., Gil, P., and de Pablo, J.\nTar-\ngeted sequence design within the coarse-grained polymer\ngenome. Science Advances, 6:eabc6216, 10 2020. doi:\n10.1126/sciadv.abc6216.\nWessels, D. R., Knigge, D. M., Papa, S., Valperga, R.,\nVadgama, S. P., Gavves, E., and Bekkers, E. J. Ground-\ning continuous representations in geometry: Equivariant\nneural fields. arXiv preprint arXiv:2406.05753, 2024.\nWu, X., Jiang, L., Wang, P., Liu, Z., Liu, X., Qiao, Y.,\nOuyang, W., He, T., and Zhao, H. Point transformer V3:\nsimpler, faster, stronger. In Conference on Computer\nVision and Pattern Recognition(CVPR), 2024.\n10\n\nErwin Transformer\nZhu, Z. and Soricut, R.\nH-transformer-1d: Fast one-\ndimensional hierarchical attention for sequences.\nIn\nConference on Neural Information Processing Systems\n(NeurIPS), pp. 3801–3815. Association for Computa-\ntional Linguistics, 2021.\n11\n\nErwin Transformer\nAlgorithm 1 BUILDBALLTREE\ninput Array of data points D in Rd\noutput Ball tree node B\nif |D| = 1 then\nCreate leaf node B containing single point in D\nreturn B\nend if\n# Find dimension of greatest spread\nδ ←argmaxi∈1,...,d(maxx∈D xi −minx∈D xi)\n# Find the median point along δ\np ←median{xδ | x ∈D}\n# Points left of median along δ\nL ←{x ∈D | xδ ≤pδ}\n# Points right of median along δ\nR ←{x ∈D | xδ > pδ}\n# Recursively construct children\nB.child1 ←BUILDBALLTREE(L)\nB.child2 ←BUILDBALLTREE(R)\nreturn B\nA. Implementation details\nBall tree construction\nThe algorithm used for construct-\ning ball trees (Pedregosa et al., 2012) can be found in Alg. 1.\nNote that this implementation is not rotationally equivariant\nas it relies on choosing the dimension of the greatest spread\nwhich in turn depends on the original orientation. Examples\nof ball trees built in our experiments are shown in Fig. 9.\nMPNN in the embedding\nErwin employs a small-scale\nMPNN in the embedding. More precisely, given a graph\nG = (V, E) with nodes vi ∈V and edges eij ∈E, we\ncompute multiple layers of message-passing as proposed in\n(Gilmer et al., 2017):\nmij = MLPe(hi, hj, pi −pj),\nmessage\nmi\n=\nX\nj∈N(i)\nmij,\naggregate\n(14)\nhi = MLPh(hi, mi),\nupdate\nwhere hi ∈RH is a feature vector of vi, N(i) denotes the\nneighborhood of vi. The motivation for using an MPNN\nis to incorporate local neighborhood information into the\nmodel. Theoretically, attention should be able to capture it\nas well; however, this might require substantially increasing\nfeature dimension and the number of attention heads, which\nwould be prohibitively expensive for a large number of\nnodes in the original level of a ball tree.\nTable 4. Test MSE for ShapeNet-Car pressure prediction. [U] and\n[R] indicate U-Net- and ResNet-like structures respectively.\nMODEL\nMSE, ×10−2\nU-NET\n6.13\nFNO\n4.04\nGINO\n2.34\nUPT\n2.31\nPOINTTRANSFORMER V3 [U]\n1.78\nERWIN (OURS) [U]\n1.42\nPOINTTRANSFORMER V3 [R]\n0.92\nERWIN (OURS) [R]\n0.92\nIn our experiments, we consistently maintain the size of\nMLPe and MLPh small (H ≤32) such that embedding\naccounts for less than 5% of total runtime.\nB. Extended experiments\nB.1. Turbulent fluid dynamics\nWe provide additional exemplary rollouts of Erwin for both\nvelocity (Fig. 10) and pressure (Fig. 11) fields.\nB.2. Airflow pressure modeling\nDataset\nWe use the ShapeNet-Car dataset generated by\nUmetani & Bickel (2018) and preprocessed by Alkin et al.\n(2024a). It consists of 889 car models, each car being\nrepresented by 3586 surface points in 3D space. Airflow\nwas simulated around each car for 10s (Reynolds number\nRe = 5 × 106) and averaged over the last 4s to obtain pres-\nsure values at each point. The dataset is randomly split into\n700 training and 189 test samples.\nTask\nGiven surface points, the task is to predict the value\nof pressure P ∈RN×1 at each point in XN×3. The training\nis done by optimizing the mean squared error loss between\npredicted and ground truth pressures.\nResults\nThe results are given in Table 4. We evaluate\nPointTransformer v3 and use the baseline results obtained\nby Alkin et al. (2024a) for U-Net (Ronneberger et al., 2015),\nFNO (Li et al., 2021), GINO (Li et al., 2023), and UPT\n(Alkin et al., 2024a). Both Erwin and PointTransformer\nv3 achieve significantly lower test MSE compared to other\nmodels, which can be attributed to their ability to capture\nfine geometric details by operating directly on the original\npoint cloud. In comparison, other approaches introduce\ninformation loss through compression - UPT encodes the\nmesh into a latent space representation, while the remain-\ning baselines interpolate the geometry onto regular grids\nand back. Moreover, in our experiments, ResNet-like con-\nfigurations that did not include any coarsening performed\ndramatically better than the ones following the U-Net struc-\nture. Overall, this result highlights the potential for Erwin\nto be used as a scalable neural operator (Wang et al., 2024).\n12\n\nErwin Transformer\nball size 512\nball size 256\nball size 128\nFigure 9. Examples of ball trees built on top of data. Partitions at different levels of ball trees are shown. Top: a polypeptide from the\nmolecular dynamics task. Center: a domain from the EAGLE dataset. Bottom: a car surface from the ShapeNet-Car dataset.\nC. Experimental details\nIn this section, we provide experimental details regarding\nhyperparameter choice and optimization. All experiments\nwere conducted on a single NVIDIA RTX A6000. All mod-\nels were trained using the AdamW optimizer (Loshchilov\n& Hutter, 2019) with weight decay 0.01 and a cosine decay\nschedule. The learning rate was tuned in the range 10−4 to\n10−3 to minimize loss on the respective validation sets.\nCosmological simulations\nWe follow the experimental\nsetup of the benchmark. The training was done for 5000\nepochs with batch size 16 for point transformers and batch\nsize 8 for message-passing-based models. The implementa-\ntion of SEGNN, NequIP and MPNN was done in JAX and\ntaken from the original benchmark repository (Balla et al.,\n2024). We maintained the hyperparameters of the baselines\nused in the benchmark. For Erwin and PointTransformer,\nthose are provided in Table 5. In Erwin’s embedding, we\nconditioned messages on Bessel basis functions rather than\nthe relative position, which significantly improved overall\nperformance.\nMolecular dynamics\nAll models were trained with batch\nsize 32 for 50000 training iterations with an initial learning\nrate of 5 · 10−4. We finetuned the hyperparameters of every\nmodel on the validation dataset (reported in Table 7).\nTurbulent fluid dynamics\nBaseline results are taken from\n(Janny et al., 2023), except for runtime and peak memory\nusage, which we measured ourselves. Erwin was trained\nwith batch size 12 for 4000 epochs.\nAirflow pressure modeling\nWe take the results of base-\nline models from Alkin et al. (2024a). Both Erwin and\nPointTransformer v3 were trained with batch size 32 for\n1000 epochs, and their hyperparameters are given in Ta-\nble 6).\n13\n\nErwin Transformer\nTable 5. Model architectures for the cosmological simulations task.\nFor varying sizes of Erwin, the values are given as (S/M).\nModel\nParameter\nValue\nPoint\nGrid size\n0.01\nTransformer v3\nEnc. depths\n(2, 2, 6, 2)\nEnc. channels\n(32, 64, 128, 256)\nEnc. heads\n(2, 4, 8, 16)\nEnc. patch size\n64\nDec. depths\n(2, 2, 2)\nDec. channels\n(64, 64, 128)\nDec. heads\n(2, 4, 8)\nDec. patch size\n64\nPooling\n(2, 2, 2)\nErwin\nMPNN dim.\n32\nChannels\n32-512/64-1024\nWindow size\n64\nEnc. heads\n(2, 4, 8, 16)\nEnc. depths\n(2, 2, 6, 2)\nDec. heads\n(2, 4, 8)\nDec. depths\n(2, 2, 2)\nPooling\n(2, 2, 2, 1)\nTable 6. Model architectures for the airflow pressure task.\nModel\nParameter\nValue\nPoint\nGrid size\n0.01\nTransformer v3\nEnc. depths\n(2, 2, 2, 2, 2)\nEnc. channels\n24-384\nEnc. heads\n(2, 4, 8, 16, 32)\nEnc. patch size\n256\nDec. depths\n(2, 2, 2, 2)\nDec. channels\n48-192\nDec. heads\n(4, 4, 8, 16)\nDec. patch size\n256\nErwin\nMPNN dim.\n8\nChannels\n96\nWindow size\n256\nEnc. heads\n(8, 16)\nEnc. depths\n(6, 2)\nDec. heads\n(8,)\nDec. depths\n(2,)\nPooling\n(2, 1)\nMP steps\n1\nTable 7. Model architectures for the molecular dynamics task. For\nmodels of varying sizes, the values are given as (S/M/L).\nModel\nParameter\nValue\nMPNN\nHidden dim.\n48/64/128\nMP steps\n6\nMLP layers\n2\nMessage agg-n\nmean\nPointNet++\nHidden dim.\n64/128/196\nMLP layers\n2\nPoint\nGrid size\n0.025\nTransformer v3\nEnc. depths\n(2, 2, 2, 6, 2)\nEnc. channels\n16-192/24-384/64-1024\nEnc. heads\n(2, 4, 8, 16, 32)\nEnc. patch size\n128\nDec. depths\n(2, 2, 2, 2)\nDec. channels\n16-96/48-192/64-512\nDec. heads\n(4, 4, 8, 16)\nDec. patch size\n128\nErwin\nMPNN dim.\n16/16/32\nChannels\n(16-256/32-512/64-1024)\nWindow size\n128\nEnc. heads\n(2, 4, 8, 16, 32)\nEnc. depths\n(2, 2, 2, 6, 2)\nDec. heads\n(4, 4, 8, 16)\nDec. depths\n(2, 2, 2, 2)\nPooling\n(2, 2, 2, 2, 1)\n14\n\nErwin Transformer\nFigure 10. The norm of the velocity field at different steps of the rollout trajectories, predicted by Erwin.\nGround truth\nPrediction\nt = 5\nt = 30\nNormalized RMSE\nt = 55\nt = 80\nt = 105\n0\n5\n10\n0.0\n0.2\n0.4\nGround truth\nPrediction\nt = 5\nt = 30\nNormalized RMSE\nt = 55\nt = 80\nt = 105\n0\n5\n10\n0.0\n0.2\n0.4\n15\n\nErwin Transformer\nGround truth\nPrediction\nt = 5\nt = 30\nNormalized RMSE\nt = 55\nt = 80\nt = 105\n0\n5\n10\n0.0\n0.2\n0.4\nGround truth\nPrediction\nt = 5\nt = 30\nNormalized RMSE\nt = 55\nt = 80\nt = 105\n0\n5\n10\n0.0\n0.2\n0.4\n16\n\nErwin Transformer\nFigure 11. The norm of the pressure field at different steps of the rollout trajectories, predicted by Erwin.\nGround truth\nPrediction\nt = 5\nt = 30\nNormalized RMSE\nt = 55\nt = 80\nt = 105\n0\n20\n40\n60\n0.0\n0.2\n0.4\nGround truth\nPrediction\nt = 5\nt = 30\nNormalized RMSE\nt = 55\nt = 80\nt = 105\n0\n20\n40\n60\n0.0\n0.2\n0.4\n17\n\nErwin Transformer\nGround truth\nPrediction\nt = 5\nt = 30\nNormalized RMSE\nt = 55\nt = 80\nt = 105\n0\n20\n40\n60\n0.0\n0.2\n0.4\nGround truth\nPrediction\nt = 5\nt = 30\nNormalized RMSE\nt = 55\nt = 80\nt = 105\n0\n20\n40\n60\n0.0\n0.2\n0.4\n18\n",
  "metadata": {
    "source_path": "papers/arxiv/Erwin_A_Tree-based_Hierarchical_Transformer_for_Large-scale_Physical\n__Systems_15d473ca6073d1e6.pdf",
    "content_hash": "15d473ca6073d1e68bcd04f8ec71ac74d26857ac8d033b2cf9ba2bc40e9e257c",
    "arxiv_id": null,
    "title": "Erwin: A Tree-based Hierarchical Transformer for Large-scale Physical Systems",
    "author": "Maksim Zhdanov, Max Welling, Jan-Willem van de Meent",
    "creation_date": "D:20250225023841Z",
    "published": "2025-02-25T02:38:41",
    "pages": 18,
    "size": 17841585,
    "file_mtime": 1740470197.451941
  }
}