{
  "text": "2-25-2025\nV-HOP: Visuo-Haptic 6D Object Pose Tracking\nHongyu Li1, Mingxi Jia1, Tuluhan Akbulut1, Yu Xiang2, George Konidaris1 and Srinath Sridhar1\n1Brown University, 2UT Dallas\nHumans naturally integrate vision and haptics for robust object perception during manipulation. The loss of\neither modality significantly degrades performance. Inspired by this multisensory integration, prior object\npose estimation research has attempted to combine visual and haptic/tactile feedback. Although these works\ndemonstrate improvements in controlled environments or synthetic datasets, they often underperform vision-\nonly approaches in real-world settings due to poor generalization across diverse grippers, sensor layouts, or\nsim-to-real environments. Furthermore, they typically estimate the object pose for each frame independently,\nresulting in less coherent tracking over sequences in real-world deployments. To address these limitations,\nwe introduce a novel unified haptic representation that effectively handles multiple gripper embodiments.\nBuilding on this representation, we introduce a new visuo-haptic transformer-based object pose tracker that\nseamlessly integrates visual and haptic input. We validate our framework in our dataset and the Feelsight dataset,\ndemonstrating significant performance improvement on challenging sequences. Notably, our method achieves\nsuperior generalization and robustness across novel embodiments, objects, and sensor types (both taxel-based\nand vision-based tactile sensors). In real-world experiments, we demonstrate that our approach outperforms\nstate-of-the-art visual trackers by a large margin. We further show that we can achieve precise manipulation\ntasks by incorporating our real-time object tracking result into motion plans, underscoring the advantages of\nvisuo-haptic perception. Our model and dataset will be made open source upon acceptance of the paper. Project\nwebsite https://lhy.xyz/projects/v-hop/\n1. Introduction\nAccurately tracking object poses is a core capability\nfor robotic manipulation, and would enable contact-\nrich and dexterous manipulations with efficient im-\nitation or reinforcement learning (Wen et al., 2022;\nLi et al., 2024a; Hsu et al., 2024). Recent state-\nof-the-art object pose estimation methods, such as\nFoundationPose (Wen et al., 2024), have significantly\nadvanced visual tracking by leveraging large-scale\ndatasets. However, relying solely on visual informa-\ntion to perceive objects can be challenging, particu-\nlarly in contact-rich or in-hand manipulation scenar-\nios involving high occlusion and rapid dynamics.\nThe cognitive science findings show that hu-\nmans naturally integrate visual and haptic informa-\ntion for robust object perception during manipula-\ntion (Navarro-Guerrero et al., 2023; Ernst and Banks,\n2002; Lacey and Sathian, 2020). For instance, Gor-\ndon et al. (1991) demonstrated that humans use vi-\nsion to hypothesize object properties and haptics to\nrefine precision grasps. The human â€œsense of touchâ€\nconsists of two distinct senses (Loomis and Led-\nerman, 1986; Dahiya et al., 2010): the cutaneous\nsense, which detects stimulation on the skin sur-\nface, and kinesthesis, which provides information\nFigure 1: Visuo-haptic sensing for 6D object pose track-\ning. We fuse egocentric visual and haptic sensing to\nachieve accurate real-time in-hand object tracking.\non static and dynamic body posture. This integration,\nknown as haptic perception, allows humans to ef-\nfectively perceive and manipulate objects (Lacey and\nSathian, 2020). In robotics, analogous capabilities\nare achieved through tactile sensors (cutaneous sense)\nand joint sensors (kinesthesis) (Navarro-Guerrero\net al., 2023).\nCorresponding author: Hongyu Li (hli230@cs.brown.edu)\narXiv:2502.17434v1  [cs.RO]  24 Feb 2025\n\nV-HOP: Visuo-Haptic 6D Object Pose Tracking\nDrawing inspiration from these human capabilities,\nresearchers have explored the integration of vision\nand touch in robotics for decades. As early as 1988,\nAllen (1988) proposed an object recognition system\nthat combined these modalities. More recently, data-\ndriven approaches have emerged to tackle object pose\nestimation using visuo-tactile information (Li et al.,\n2023; Suresh et al., 2024; Dikhale et al., 2022; Wan\net al., 2024; Rezazadeh et al., 2023; Tu et al., 2023;\nGao et al., 2023; Li et al., 2024b). Although promis-\ning, these methods face two major barriers that hinder\ntheir broader applicability: (i) Cross-embodiment:\nMost approaches overfit specific grippers or tactile\nsensor layouts, limiting their adaptability. (ii) Do-\nmain generalization: Compared to visual-only base-\nlines, visuo-tactile approaches struggle to generalize,\nhindered by insufficient data diversity and model scal-\nability. Moreover, they typically process each frame\nindependently, which can result in less coherent ob-\nject pose tracking over sequences in real-world de-\nployments. As a result, existing methods are challeng-\ning to deploy broadly and often require significant\ncustomization to specific robotic platforms.\nTo address these challenges, we propose V-HOP\n(Fig. 1): a two-fold solution for generalizable visuo-\nhaptic 6D object pose tracking. First, we introduce\na novel unified haptic representation that facilitates\ncross-embodiment learning. We consider the com-\nbination of tactile and kinesthesis in the form of a\npoint cloud, addressing a critical yet often overlooked\naspect of visuo-haptic learning. Second, we propose\na transformer-based object pose tracker to fuse visual\nand haptic features. We leverage the robust visual\nprior captured by the visual foundation model while\nincorporating haptics. V-HOP accommodates diverse\ngripper embodiments and various objects and gener-\nalizes to novel embodiments and objects.\nWe build a multi-embodied dataset with eight\ngrippers using the NVIDIA Isaac Sim simulator for\ntraining and evaluation. Compared to Foundation-\nPose (Wen et al., 2024), our approach achieves 5% im-\nprovement in the accuracy of object pose estimation\nin terms of ADD-S (Xiang et al., 2018) in our dataset.\nThese results highlight the benefit of fusing visual and\nhaptic sensing. In the FeelSight dataset (Suresh et al.,\n2024), we benchmark against NeuralFeels (Suresh\net al., 2024), an optimization-based visuo-tactile ob-\nject pose tracker, achieving a 32% improvement in\nthe ADD-S metric and ten times faster run-time speed.\nFinally, we perform the sim-to-real transfer experi-\nments using Barrett Hands. Our method demonstrates\nremarkable robustness and significantly outperforms\nFoundationPose, which could lose object tracks en-\ntirely (Fig. 5). When integrated into motion plans,\nour approach achieves 40% higher average task suc-\ncess rates. To the best of our knowledge, V-HOP is\nthe first data-driven visuo-haptic approach to demon-\nstrate robust generalization across both taxel-based\ntactile sensors (e.g., Barrett Hand) and vision-based\ntactile sensors (e.g., DIGIT sensors), as well as on\nnovel embodiments and objects.\nIn conclusion, our contributions to this paper are\ntwo-fold:\n1. Unified haptic representation: we introduce\na novel haptic representation, enabling cross-\nembodiment learning and addressing the cross-\nembodiment challenge by improving adaptabil-\nity across diverse embodiments and objects.\n2. Visuo-haptic transformer: We present a trans-\nformer model that integrates visual and hap-\ntic data, improving pose tracking consistency\nand addressing the domain generalization chal-\nlenge.\n2. Background\nIn this section, we first define the problem formally\nand then review existing haptic representations and\nour proposed unified representation.\n2.1. Problem Definition\nWe tackle the model-based visuo-haptic 6D object\npose tracking problem, assuming access to:\nâ€¢ Visual observations: An RGB-D sensor observes\nthe object in the environment.\nâ€¢ Haptic feedback: The object is manipulated by\na rigid gripper equipped with tactile sensors.\nOur approach takes the following as input:\n1. The CAD model îˆ¹ğ‘œof the object.\n2. A sequence of RGB-D images îˆ»= {ğğ‘–}ğ‘¡\nğ‘–=1,\nwhere each observation ğğ‘–= [ğˆğ‘–, ğƒğ‘–] includes\nan RGB image ğˆğ‘–and a depth map ğƒğ‘–.\n3. An initial 6D pose ğ“0 = (ğ‘0, ğ­0) âˆˆSE(3), where\nğ‘0 âˆˆSO(3) is 3D rotation and ğ­0 âˆˆâ„3 is 3D\ntranslation.\nIn practice, the ground-truth initial pose ğ“0 is hard\nto obtain and can only be estimated through pose\nestimation (Xiang et al., 2018; Wang et al., 2019a;\n2\n\nV-HOP: Visuo-Haptic 6D Object Pose Tracking\nPark et al., 2019; Li et al., 2024c; Wang et al., 2019b;\nLee et al., 2023; Wen et al., 2024; LabbÃ© et al., 2022;\nHe et al., 2022; Liu et al., 2022; Lin et al., 2024a;\nTremblay et al., 2018; Wen et al., 2020b). Therefore,\nwe treat Ì‚ğ“0 = ğ“0 in the following. At each timestep ğ‘–,\nour model estimates the object pose Ì‚ğ“ğ‘–given all the\ninputs with the initial pose being the estimate Ì‚ğ“ğ‘–âˆ’1 at\nthe previous timestep.\nThe above inputs are the standard inputs from the\nmodel-based visual pose tracking problem (Wen et al.,\n2020a; Deng et al., 2021), while the inputs below will\nserve our haptic representation and will be detailed\nin later sections.\n4. Gripper description in Unified Robot Descrip-\ntion Format (URDF).\n5. Gripper joint positions ğ£= {ğ‘—1, ğ‘—2, â€¦ , ğ‘—ğ·ğ‘œğ¹}.\n6. Tactile sensor data îˆ¿, including Positions îˆ¿ğ‘\nand readings îˆ¿ğ‘Ÿof tactile sensors, which will be\nformally defined in the next section.\n7. Transformation between the camera and the\nrobot frames obtained through hand-eye cali-\nbration (Marchand et al., 2005).\n2.2. Haptic Representation\nThe effectiveness of haptic learning hinges on its rep-\nresentation. Prior approaches using raw value (Lin\net al., 2024b), image (Guzey et al., 2023), or graph-\nbased (Yang et al., 2023; Li et al., 2024b; Rezazadeh\net al., 2023) representations often struggle to gener-\nalize across diverse embodiments. For instance, Wu\net al. (2022) and Guzey et al. (2023) project tactile\nsignals from Xela sensors into a 2D image format.\nWhile this allows efficient processing with existing\nvisual models, extending the method to different grip-\npers or sensor layouts proves challenging. Similarly,\nLi et al. (2024b) and Rezazadeh et al. (2023) employ\ngraph-based mappings, where taxels are represented\nas vertices. However, variations in sensor layouts\nresult in different graph distributions, creating signifi-\ncant generalization gaps.\nIn contrast, we adopt a point cloud representation,\nwhich naturally encode 3D positions and can flexibly\naccommodate multi-embodiments. We broadly clas-\nsify tactile sensors into taxel-based and vision-based.\nA more comprehensive review on tactile sensors can\nbe found at (Yamaguchi and Atkeson, 2019). Below,\nwe outline how they are converted into point clouds\nin prior works (Dikhale et al., 2022; Suresh et al.,\n2024; Watkins-Valls et al., 2019; Falco et al., 2017),\npaving the way for our unified framework.\nTaxel-based Sensors. The tactile data is defined as\nîˆ¿= {ğ‘ ğ‘–}ğ‘›ğ‘¡\nğ‘–=1, which encapsulate ğ‘›ğ‘¡taxels. ğ‘ ğ‘–represents\nindividual taxels. The tactile data consists of îˆ¿=\n(îˆ¿ğ‘, îˆ¿ğ‘Ÿ):\nâ€¢ Positions (îˆ¿ğ‘): Defined in the gripper frame\nand transformed into the camera frame using\nforward kinematics.\nâ€¢ Readings (îˆ¿ğ‘Ÿ): Capturing contact values. Read-\nings are commonly binarized into contact or no-\ncontact states (Yin et al., 2023; Xue et al., 2023;\nLi et al., 2023; Dikhale et al., 2022) based on a\nthreshold ğœ.\nThe set of taxels in contact is:\nîˆ¿ğ‘= {ğ‘ ğ‘–âˆˆîˆ¿âˆ£îˆ¿ğ‘Ÿ(ğ‘ ğ‘–) > ğœ},\n(1)\nand the corresponding tactile point cloud îˆ¿ğ‘,ğ‘is de-\nfined as\nîˆ¿ğ‘,ğ‘= {îˆ¿ğ‘(ğ‘ ğ‘–) âˆ£ğ‘ ğ‘–âˆˆîˆ¿ğ‘}.\n(2)\nVision-based sensors. For vision-based tactile\nsensors (Lambeta et al., 2020; Yuan et al., 2017; Don-\nlon et al., 2018; Taylor et al., 2022), the tactile data\nincludes îˆ¿= (îˆ¿ğ‘, îˆ¿ğ¼):\nâ€¢ Positions (îˆ¿ğ‘): Sensor positions in the camera\nframe, similar to taxel-based.\nâ€¢ Images (îˆ¿ğ¼): Capturing contact states using reg-\nular RGB image representation. Using the tac-\ntile depth estimation model (Bauza et al., 2019;\nSuresh et al., 2024; Kuppuswamy et al., 2020;\nSuresh et al., 2023; 2022; Ambrus et al., 2021),\nwe can convert îˆ¿ğ¼into tactile point cloud îˆ¿ğ‘,ğ‘.\nYet we are not the first to employ point cloud rep-\nresentations for tactile learning, prior works (Dikhale\net al., 2022; Suresh et al., 2024; Watkins-Valls et al.,\n2019; Falco et al., 2017) focus on a single type\nof sensor and overlook the hand posture. Our key\ncontribution is a unified representation spanning\nboth taxel-based and vision-based sensors on multi-\nembodiments, empowered by our multi-embodied\ndataset. We demonstrate generalizability on the Bar-\nrett hand (taxel-based) during our real-world experi-\nments and on the Allegro hand (vision-based DIGIT\nsensor) using the Feelsight dataset (Suresh et al.,\n2024). Our novel haptic representation seamlessly\nintegrates the tactile signals with the hand posture,\nenabling more effective hand-object interaction rea-\nsoning. In subsequent sections, we describe our ap-\nproach and provide empirical evidence demonstrating\n3\n\nV-HOP: Visuo-Haptic 6D Object Pose Tracking\nthat our representation improves generalization ca-\npabilities, bridging the gap between heterogeneous\ntactile sensor modalities.\n3. Methodology\nWe propose V-HOP, a data-driven approach that fuses\nvisual and haptic modalities to achieve accurate 6D\nobject pose tracking. Our goal is to build a general-\nizable visuo-haptic pose tracker that accommodates\ndiverse embodiments and objects. We first outline\nthe core representations used in our haptic modality:\nhand and object representations. Our choice for the\nrepresentations follows the spirit of the render-and-\ncompare paradigm (Li et al., 2018). Later, we intro-\nduce our visuo-haptic model and how it is trained. We\nuse the terms â€œhandâ€ and â€œgripperâ€ interchangeably.\n3.1. Hand Representation\nTactile signals only represent the cutaneous stimula-\ntion, while haptic sensing combines tactile and kines-\nthetic feedback to provide a more comprehensive spa-\ntial understanding of contact and manipulation. We\npropose a novel haptic representation that integrates\ntactile signals and hand posture in a unified point\ncloud representation. This hand-centric representa-\ntion enables efficient reasoning about spatial contact\nand hand-object interaction.\nUsing the URDF definition and joint positions ğ£,\nwe generate the hand mesh îˆ¹â„through forward kine-\nmatics and calculate the surface normals. The mesh\nis then downsampled to produce a 9-D hand point\ncloud îˆ¼â„= {ğ©ğ‘–}ğ‘›â„\nğ‘–=1:\nğ©ğ‘–= (ğ‘¥ğ‘–, ğ‘¦ğ‘–, ğ‘§ğ‘–, ğ‘›ğ‘–ğ‘¥, ğ‘›ğ‘–ğ‘¦, ğ‘›ğ‘–ğ‘§, ğœ) âˆˆâ„9,\n(3)\nwhere ğ‘¥ğ‘–, ğ‘¦ğ‘–, ğ‘§ğ‘–represent the 3-D coordinate of the\npoint. ğ‘›ğ‘–ğ‘¥, ğ‘›ğ‘–ğ‘¦, ğ‘›ğ‘–ğ‘§represent the 3-D normal vectors,\nand ğœâˆˆâ„3 is a one-hot encoded point label:\nâ€¢ [1, 0, 0]: Hand point in contact.\nâ€¢ [0, 1, 0]: Hand point not in contact.\nâ€¢ [0, 0, 1]: Object point (for later integration with\nthe object point cloud).\nTo obtain the contact state of each point, we map\nthe tactile point cloud îˆ¿ğ‘,ğ‘, representing the contact\npoints detected by the tactile sensors (Sec. 2.2), onto\nthe downsampled hand point cloud îˆ¼â„. Specifically,\nfor each point in îˆ¿ğ‘,ğ‘, we find its neighboring points\nin îˆ¼â„within a radius ğ‘Ÿ. These neighboring points are\nlabeled as â€œin contactâ€, while all others are labeled\nas â€œnot in contactâ€. The choice of the radius ğ‘Ÿis\nrandomized during training and determined by the\nmeasured effective radius of each taxel during robot\ndeployment. The resulting haptic point cloud, îˆ¼â„,\nserves as a unified representation for both tactile and\nkinesthetic data (Fig. 2).\n3.2. Object Representation\nWe denote the object model point cloud as îˆ¼Î¦ =\n{ğªğ‘–}ğ‘›ğ‘œ\nğ‘–=1. Similar to the hand point cloud, ğªğ‘–follows the\nsame 9-D definitions (Equation 3),\nğªğ‘–= (ğ‘¥ğ‘–, ğ‘¦ğ‘–, ğ‘§ğ‘–, ğ‘›ğ‘–ğ‘¥, ğ‘›ğ‘–ğ‘¦, ğ‘›ğ‘–ğ‘§, ğœ) âˆˆâ„9,\nwith ğœ= [0, 0, 1] for all object points.\nAt each\ntimestep ğ‘–> 0, we transform the model point cloud\ninto a hypothesized point cloud îˆ¼ğ‘œ= {ğªâ€²\nğ‘–}ğ‘›ğ‘œ\nğ‘–=1 accord-\ning to the pose from the previous timestep ğ“ğ‘–âˆ’1. For\neach point ğªâ€²\nğ‘–in the hypothesized point cloud îˆ¼ğ‘œ\nğªâ€²\nğ‘–= (ğ‘¥â€²\nğ‘–, ğ‘¦â€²\nğ‘–, ğ‘§â€²\nğ‘–, ğ‘›â€²\nğ‘–ğ‘¥, ğ‘›â€²\nğ‘–ğ‘¦, ğ‘›â€²\nğ‘–ğ‘§, ğœ),\n(4)\nwhere:\nâ¡\nâ¢\nâ¢\nâ£\nğ‘¥â€²\nğ‘–\nğ‘¦â€²\nğ‘–\nğ‘§â€²\nğ‘–\nâ¤\nâ¥\nâ¥\nâ¦\n= ğ‘ğ‘–âˆ’1\nâ¡\nâ¢\nâ¢\nâ£\nğ‘¥ğ‘–\nğ‘¦ğ‘–\nğ‘§ğ‘–\nâ¤\nâ¥\nâ¥\nâ¦\n+ ğ­ğ‘–âˆ’1,\nâ¡\nâ¢\nâ¢\nâ£\nğ‘›â€²\nğ‘–ğ‘¥\nğ‘›â€²\nğ‘–ğ‘¦\nğ‘›â€²\nğ‘–ğ‘§\nâ¤\nâ¥\nâ¥\nâ¦\n= ğ‘ğ‘–âˆ’1\nâ¡\nâ¢\nâ¢\nâ£\nğ‘›ğ‘–ğ‘¥\nğ‘›ğ‘–ğ‘¦\nğ‘›ğ‘–ğ‘§\nâ¤\nâ¥\nâ¥\nâ¦\n.\n(5)\nTo enable reasoning about hand-object interactions,\nwe fuse the hand point cloud îˆ¼â„and the hypothesized\nobject point cloud îˆ¼ğ‘œto create a hand-object point\ncloud îˆ¼,\nîˆ¼= îˆ¼â„âˆªîˆ¼ğ‘œ.\n(6)\nThis novel unified representation adopts the princi-\nples of the render-and-compare paradigm from vi-\nsual approaches (Li et al., 2018; Wen et al., 2020a;\nLabbÃ© et al., 2022; Wen et al., 2024; Tremblay et al.,\n2023), in which the rendered image (based on pose\nhypothesis) is compared against the visual observa-\ntion. The hypothesized object point cloud îˆ¼ğ‘œserves\nas the â€œrenderedâ€ pose hypothesis (Fig. 2). The hand\npoint cloud îˆ¼â„represents the real observation using\nhaptic feedback, which we used to compare with. By\nleveraging this representation, the model captures the\ncontact-rich interactions between the hand and the\nobject by learning feasible object poses informed by\nhaptic feedback.\n3.3. Network Design\nVisual modality. Unlike prior works, which train\nthe whole visuo-haptic network from scratch, our ap-\nproach can effectively leverage the pretrained visual\n4\n\nV-HOP: Visuo-Haptic 6D Object Pose Tracking\nVisual\nEncoder\nVisual\nEncoder\nResBlock\nTranslation\nEncoder\nRotation\nEncoder\nLinear\nLinear\n3D Rotation\n3D Translation\nPositional\nEncoding\nHaptic\nEncoder\nHxWxC\nHxWxC\nHxWx2C\nSharedÂ  Â Weights\nHaptic\nVisual\n6D Pose\nFigure 2: Network design of V-HOP. The visual modality, based on FoundationPose (Wen et al., 2024), uses a\nvisual encoder to process RGB-D observations (real and rendered) into feature maps, which are concatenated and\nrefined through a ResBlock to produce visual embeddings (Dosovitskiy et al., 2021). The haptic modality encodes a\nunified hand-object point cloud, derived from 9D hand îˆ¼â„and object îˆ¼ğ‘œpoint clouds, into a haptic embedding that\ncaptures hand-object interactions. The red dot in the figure denotes the activated tactile sensor. These visual and haptic\nembeddings are processed by Transformer encoders to estimate 3D translation and rotation.\nfoundation model. Our design extends the formu-\nlation of FoundationPose (Wen et al., 2024), as it\ndemonstrates great generalizability on unseen objects\nand a narrow sim-to-real gap. To harness the high-\nquality visual prior captured by it, we utilize its visual\nencoder ğ‘“ğ‘£and freeze it during our training. Using\nthis encoder, We transform the RGB-D observation\ninto visual embeddings ğ™ğ‘£= ğ‘“ğ‘£(ğ).\nHaptic modality. In parallel, we encode the hand-\nobject point cloud îˆ¼using a haptic encoder ğ‘“â„, result-\ning in a haptic embedding ğ™â„= ğ‘“â„(îˆ¼). By represent-\ning all interactions in point cloud space, our novel\nhaptic representation provides the flexibility to utilize\nany point cloud-based network for encoding. For this\npurpose, we choose PointNet++ (Qi et al., 2017) as\nour haptic encoder ğ‘“â„. To improve learning efficiency,\nwe canonicalize the point cloud using the centroid\nof the hand points, ensuring îˆ¼is spatially centered\naround the gripper during processing.\nVisuo-haptic fusion. Integrating visual and hap-\ntic modalities, however, poses significant challenges.\nExisting methods often apply fixed or biased weight-\nings between these modalities (Li et al., 2023; Suresh\net al., 2024; Dikhale et al., 2022; Tu et al., 2023),\nwhich can fail under specific conditions. For exam-\nple, when contact is absent, the visual modality alone\nshould be leveraged, or when occlusion is severe, hap-\ntics should be favored. Inspired by the principle of\nâ€œoptimal integrationâ€ in human multisensory percep-\ntion (Ernst and Banks, 2002; Helbig and Ernst, 2007;\nLacey and Sathian, 2020; Takahashi and Watt, 2014;\nHelbig et al., 2012), where the brain dynamically\nadjusts the weighting of visual and haptic inputs to\nmaximize reliability, we adopt self-attention mecha-\nnisms (Vaswani et al., 2017) for the adaptive fusion\nof visual and haptic embeddings. This ensures robust-\nness across varying scenarios, whether the object is\nin contact or in clear view.\nTo achieve this fusion,\nwe propose haptic\ninstruction-tuning, inspired by visual instruction-\ntuning (Liu et al., 2023). While keeping the visual\nencoder ğ‘“ğ‘£frozen, we feed both visual embedding ğ™ğ‘£\nand haptic embedding ğ™â„into the original visual-only\nTransformer encoders (Vaswani et al., 2017; Wen\net al., 2024), which are initialized with the pretrained\nweights from FoundationPose. We then fine-tune the\nTransformer encoders and the haptic encoder ğ‘“â„to-\ngether. Consequently, visual and haptic information\nis fused adaptively using self-attention blocks, and\nthe model dynamically adjusts the modality weight\nbased on the context (Fig. 9). Following Foundation-\nPose, we disentangle the 6D pose into 3D translation\nand 3D rotation and estimate them using two output\nheads (Fig. 2), respectively.\n3.4. Training Paradigm\nWe train our model by adding noise (ğ‘ğœ–, ğ­ğœ–) to the\nground-truth pose ğ“= (ğ‘, ğ­) to create the hypothesis\npose Ìƒğ“= (Ìƒğ‘,Ìƒğ­):\nÌƒğ‘= ğ‘âˆ’1\nğœ–â‹…ğ‘,\nÌƒğ­= âˆ’ğ­ğœ–+ ğ­.\n(7)\n5\n\nV-HOP: Visuo-Haptic 6D Object Pose Tracking\nFigure 3: Dataset sample visualization. (Top row) Barrett Hand, Shadow Hand, Allegro Hand, SHUNK SVH. (Bottom\nrow) Dâ€™Claw, LEAP Hand, Inspire Hand, Robotiq 3-Finger gripper.\nThe rendered image is generated using Ìƒğ“, while the\nobject point hand is transformed based on Ìƒğ“; in con-\ntrast, the RGB-D image and hand point cloud rep-\nresent actual observations. The model estimates the\nrelative pose Î”Ì‚ğ“= (Î”Ì‚ğ‘, Î”Ì‚ğ­) between the pose hypoth-\nesis and observation. The model is optimized using\nthe ğ¿2 loss:\nîˆ¸ğ“= â€–Î”Ì‚ğ‘âˆ’ğ‘ğœ–â€–2 + â€–Î”Ì‚ğ­âˆ’ğ­ğœ–â€–2,\n(8)\nwhere we use quaternion representations for rotations.\nThe estimated pose Ì‚ğ“= (Ì‚ğ‘,Ì‚ğ­) is:\nÌ‚ğ‘= Î”Ì‚ğ‘â‹…Ìƒğ‘,\nÌ‚ğ­= Î”Ì‚ğ­+Ìƒğ­.\n(9)\nWe further incorporate an attractive loss (îˆ¸ğ‘) and\na penetration loss (îˆ¸ğ‘) to encourage the object to\nmake contact with the tactile point cloud îˆ¿ğ‘,ğ‘and\navoid penetrating the hand point cloud îˆ¼â„. We first\ntransform the initial hypothesized object pose cloud\nîˆ¼ğ‘œusing the estimated pose Ì‚îˆ¼ğ‘œ= Ì‚ğ“Ìƒğ“âˆ’1 îˆ¼ğ‘œ, where\nîˆ¼ğ‘œis in homogenous form.\nThe attractive loss enforces that each activated\ntaxel must make contact with the object:\nîˆ¸ğ‘=\n1\n|îˆ¿ğ‘,ğ‘|\nâˆ‘\nğ‘ ğ‘,ğ‘âˆˆîˆ¿ğ‘,ğ‘\nmin\nğ‘âˆˆÌ‚îˆ¼ğ‘œ\nâ€–ğ‘ ğ‘,ğ‘âˆ’ğ‘â€–2,\n(10)\nwhich can be interpreted as a single-direction Cham-\nfer distance between the tactile point cloud and the\nobject point cloud.\nThe penetration loss avoids penetrations between\nthe object and the gripper (Yang et al., 2021; 2024;\nBrahmbhatt et al., 2019):\nğ©ğ‘œ= arg min\nğªâˆˆÌ‚îˆ¼ğ‘œ\nâ€–ğªâˆ’ğ©â„â€–2,\nîˆ¸ğ‘= âˆ‘\nğ©â„âˆˆîˆ¼â„\nğ‘’max{0, âˆ’ğ§ğ‘œâ‹…(ğ©â„âˆ’ğ©ğ‘œ)} âˆ’1,\n(11)\nwhere ğ©ğ‘œrepresents the nearest neighbor of each point\nğ©â„in the hand point cloud îˆ¼â„. Our overall loss is:\nîˆ¸= îˆ¸ğ“+ ğ›¼îˆ¸ğ‘+ ğ›½îˆ¸ğ‘,\n(12)\nwhere we set ğ›¼= 1 and ğ›½= 0.001 empirically. We\noptimize the model using the AdamW (Loshchilov\nand Hutter, 2018) optimizer with an initial learning\nrate of 0.0004 and train the model for 20 epochs.\n4. Experiments\n4.1. Multi-embodied Dataset\nExisting visuo-haptic datasets were not publicly avail-\nable (Dikhale et al., 2022; Li et al., 2024b; Wan\net al., 2024) at the time of completing this work\nand focused on a single gripper (Suresh et al., 2024),\nleaving the question of generalization to novel em-\nbodiments unanswered. Consequently, we develop\na multi-embodied dataset (Fig. 3) using NVIDIA\nIsaac Sim to enable cross-embodiment learning and\nthorough evaluation. Our dataset comprises approxi-\nmately 1,550,000 images collected across eight grip-\npers and thirteen objects. We utilize 85% of the data\nfor training and the rest for validation. The camera tra-\njectories are sampled on the semi-sphere around the\ngripper, which has a random radius between 0.5 and\n2.5 meters. We selected graspable YCB object (Calli\net al., 2015) and grippers used in prior works (Ding\n6\n\nV-HOP: Visuo-Haptic 6D Object Pose Tracking\nObject Name\nAUC Metric\nViTa\nFP\nV-HOP\nADD\n5.61\n64.95\n62.88\nmaster_chef_can\nADD-S\n80.51\n84.60\n86.38\nADD\n11.09\n73.21\n74.75\nsugar_box\nADD-S\n74.34\n85.27\n89.35\nADD\n32.08\n57.02\n59.13\ntomato_soup_can\nADD-S\n84.19\n78.45\n83.30\nADD\n7.23\n72.65\n74.07\nmustard_bottle\nADD-S\n73.49\n86.05\n88.57\nADD\nN/A\n69.87\n70.75\npudding_box (Unseen)\nADD-S\nN/A\n84.63\n88.20\nADD\n43.20\n63.89\n69.75\ngelatin_box\nADD-S\n86.66\n80.16\n86.87\nADD\n34.13\n65.62\n68.29\npotted_meat_can\nADD-S\n86.77\n82.67\n87.21\nADD\n23.93\n63.87\n69.72\nbanana\nADD-S\n71.67\n79.99\n85.79\nADD\n35.05\n59.60\n58.42\nmug\nADD-S\n86.58\n82.16\n84.10\nADD\n2.58\n67.21\n68.56\npower_drill\nADD-S\n61.02\n80.77\n85.77\nADD\n23.34\n66.23\n70.67\nscissors\nADD-S\n65.56\n81.27\n85.08\nADD\n42.43\n61.74\n71.10\nlarge_marker\nADD-S\n73.69\n75.45\n85.00\nADD\n30.56\n71.64\n75.63\nlarge_clamp\nADD-S\n79.20\n86.07\n89.09\nADD â†‘\n23.93\n66.29\n68.90\nAll\nADD-S â†‘\n76.87\n82.37\n86.62\nTable 1: Per-object comparison of AUC metrics for\nADD and ADD-S. The row of novel object is grayed out.\nBoth metrics are the higher, the better. The best results are\nbolded.\net al., 2024; Murrilo et al., 2024). Additional details\nabout the dataset can be found in the appendix.\n4.2. Pose Tracking Comparison\nIn the following experiments, we evaluate perfor-\nmance using the metrics:\nâ€¢ Area under the curve (AUC) of ADD and ADD-\nS (Hinterstoisser et al., 2013; Xiang et al., 2018),\nand\nâ€¢ ADD(-S)-0.1d (He et al., 2022): ADD/ADD-S\nthat is less than 10% of the object diameter.\nWe compare V-HOP against the current state-of-\nthe-art approaches in visual pose tracking (Founda-\ntionPose (Wen et al., 2024), or FP in short) and visuo-\ntactile pose estimation (ViTa (Dikhale et al., 2022)).\nTo ensure a fair comparison, we finetune Foundation-\nPose and train ViTa on our multi-embodied dataset.\nTo verify the generalizability of the novel object and\nnovel gripper, we exclude one object (pudding_box)\nand one gripper (Dâ€™Claw) during training.\nDue to the absence of a visuo-haptic pose tracking\napproach, we compare V-HOP with ViTa, an instance-\nHand Name\nAUC Metric\nViTa\nFP\nV-HOP\nADD\n24.48\n74.45\n76.20\nAllegro Hand\nADD-S\n77.60\n88.74\n90.48\nADD\n24.63\n77.67\n79.06\nBarrett Hand\nADD-S\n77.74\n88.72\n91.73\nADD\n21.99\n48.16\n57.49\nDâ€™Claw (Unseen)\nADD-S\n76.00\n77.06\n85.48\nADD\n24.56\n70.22\n70.15\nInspire Hand\nADD-S\n77.65\n84.22\n87.28\nADD\n23.88\n64.17\n69.96\nLEAP Hand\nADD-S\n77.55\n83.06\n88.05\nADD\n23.40\n79.48\n79.14\nRobotiq 3-Finger\nADD-S\n76.87\n89.39\n90.61\nADD\n24.40\n61.01\n62.75\nSCHUNK SVH\nADD-S\n76.74\n78.58\n82.96\nADD\n23.81\n58.77\n60.27\nShadow Hand\nADD-S\n75.55\n73.24\n79.35\nADD â†‘\n23.93\n66.29\n68.90\nAll\nADD-S â†‘\n76.87\n82.37\n86.62\nTable 2: Per-gripper comparison of AUC metrics for\nADD and ADD-S. Our dataset contains eight grippers.\nWe train the model on seven grippers, leaving one gripper\n(Dâ€™Claw) unseen.\nlevel visuo-tactile pose estimation approach that op-\nerates under different settings. For ViTa, we provide\nground-truth segmentation and train a separate model\nfor each object, as it is an instance-level method.\nIn contrast, both FoundationPose and V-HOP han-\ndle novel-object estimation and require training only\nonce. For fair evaluation, we run both methods for\ntwo iterations per tracking step. For V-HOP, we run\none visuo-haptic iteration and one visual iteration.\nIn Tab. 1, we show the performance for each object.\nV-HOP consistently outperforms ViTa and Founda-\ntionPose (FP) on most objects with respect to ADD\nand across all objects in terms of ADD-S. On aver-\nage, our approach delivers an improvement of 4% in\nADD and 5% in ADD-S compared to Foundation-\nPose. Notably, V-HOP demonstrates strong perfor-\nmance on unseen objects, highlighting the potential\nof our model to generalize effectively to novel ob-\njects.\nSimilarly, Tab. 2 illustrates the performance of\neach gripper. In line with its object performance,\nV-HOP outperforms its counterparts on most grippers\nin terms of ADD and across all grippers in ADD-S.\nMoreover, V-HOP demonstrates robust performance\non unseen grippers, further emphasizing the general-\nizability of our unified haptic representation.\n7\n\nV-HOP: Visuo-Haptic 6D Object Pose Tracking\nMethod\nAUC ADD\nADD-0.1d\nAUC ADD-S\nADD-S-0.1d\nWithout Tactile\n60.10\n43.69\n77.33\n63.17\nWithout Visual\n32.19\n3.72\n58.85\n31.44\nV-HOP (Ours)\n68.90\n48.55\n86.62\n77.83\nTable 3: Ablations of input modalities. Our results\nconfirm the effectiveness of combining visual and haptic\nmodalities.\nFigure 4: Performance under various occlusion ratios.\nWe use the direct ADD and ADD-S metrics (in meters) in\nthis experiment.\n4.3. Ablation on Modalities\nWe conduct an ablation study on the input modalities\nto evaluate the effectiveness of the haptic represen-\ntation. Specifically, we train two ablated versions\nof V-HOP: one without tactile feedback and another\nwithout visual input, as shown in Tab. 3. To exclude\ntactile input, we remove all â€œin contactâ€ point labels\n(Equation 3). Our results indicate that visual input\nsignificantly contributes to performance, likely due to\nthe richness of visual information, including texture\nand spatial details. This finding aligns with previous\nstudies on human perception systems, which suggest\nthat vision plays a dominant role in visuo-haptic in-\ntegration (Kassuba et al., 2013). Similarly, tactile\nfeedback is crucial; without it, performance degrades\nnotably because reasoning about hand-object contact\nduring interactions becomes more difficult.\n4.4. Occlusionâ€™s Effect on the Performance\nWe evaluate the performance of V-HOP and Founda-\ntionPose across varying occlusion ratios (Fig. 4). The\nocclusion ratio is defined as the proportion of pixels\nin the segmentation mask relative to the total pixels\nin the rendered object image, generated using the\nground-truth pose. Our results show that V-HOP con-\nsistently outperforms FoundationPose in both ADD\nand ADD-S metrics under different levels of occlu-\nsion. These results underscore the importance of\nintegrating visual and haptic information to improve\nperformance in challenging occlusion scenarios.\nMethod\nGT Seg\nADD-S â†“\nADD-S-0.1d â†‘\nFPS â†‘\nNeuralFeels (Suresh et al., 2024)\nâœ“\n2.14\n98.95\n3\nV-HOP (Ours)\nâœ—\n1.46\n98.45\n32\nTable 4: Performance on the FeelSight Dataset. For\nconsistency with the metric used in NeuralFeels (Suresh\net al., 2024), this experiment reports the direct ADD-S\nmetric (Xiang et al., 2018) (in mm) rather than the AUC\nof ADD-S used in other experiments.\n4.5. Pose Tracking on FeelSight\nTo evaluate the generalizability of V-HOP, we bench-\nmark it against NeuralFeels (Suresh et al., 2024), a\nrecently introduced optimization-based visuo-tactile\npose tracking approach, using their proposed Feel-\nsight dataset. Specifically, we focus on the occlusion\nsubset of the dataset, FeelSight-Occlusion,\nwhich presents significant challenges due to severe\nocclusions. This subset requires robust generaliza-\ntion capabilities as it includes a novel embodiment\n(the Allegro hand equipped with DIGIT fingertips), a\nnovel sensor type (a vision-based tactile sensor), and\na novel object (a Rubikâ€™s cube). For a fair compari-\nson, we compare against their model-based tracking\napproach, which uses almost the same inputs as V-\nHOP but with the ground-truth segmentation mask\n(GT Seg).\nThe results are presented in Tab. 4.\nV-HOP\nachieves a 32% lower ADD-S error compared to\nNeuralFeels and has a similar ADD-S-0.1d score.\nIt is important to note that NeuralFeels leverages\nthe ground-truth segmentation mask, which helps in\nmore accurate object localization, whereas V-HOP\ndoes not have such an input, further underscoring its\nrobustness and adaptability.\nIn terms of computational efficiency, V-HOP is ap-\nproximately 10 times faster than NeuralFeels, achiev-\ning 32 FPS compared to NeuralFeelsâ€™ 3 FPS on an\nNVIDIA RTX 4070 GPU. This substantial improve-\nment in speed highlights the practicality of V-HOP\nfor real-world manipulation applications, as we will\ndemonstrate in the later sections.\n5. Sim-to-Real Transfer Experiments\nTo validate the real-world effectiveness of our ap-\nproach, we perform sim-to-real experiments using\nour robot platform (Fig. 1). Our bimanual platform\ncomprises dual Franka Research 3 robotic arms (Had-\ndadin et al., 2022) and Barrett Hands BH8-282. Our\nBarrett Hand has 4 degrees of freedom (DoF) and 96\ntaxels: 24 taxels on each fingertip and 24 taxels on the\n8\n\nV-HOP: Visuo-Haptic 6D Object Pose Tracking\nFP\nV-HOP\nFP\nV-HOP\nFigure 5: Qualitative results of pose tracking sequences. We verify the performance in the real world using YCB\nobjects. The cup and power drill are highlighted in this figure, while the results of more objects are in the appendix.\npalm. Each taxel comprises a capacitive cell capable\nof detecting forces within a range of 10 N/cm2 with a\nresolution of 0.01 N. For egocentric visual input, we\nuse a MultiSense SLB RGB-D camera, which com-\nbines a MultiSense S7 stereo camera and a Hokuyo\nUTM-30LX-EW laser scanner. We utilize Founda-\ntionPose to provide the initial frame pose estimate\nand CNOS (Nguyen et al., 2023; Kirillov et al., 2023)\nto provide the segmentation task.\n5.1. Pose Tracking Experiments\nIn this experiment (Fig. 5), the gripper stably grasps\nthe object while a human operator guides the robot\narm along a random trajectory. This introduces heavy\nocclusion and high dynamic motion to emulate chal-\nlenging real-world manipulation scenarios. Under\nthese conditions, FoundationPose often loses track-\ning due to reliance on visual input alone. In contrast,\nV-HOP maintains stable object tracking throughout\nthe trajectory, demonstrating the robustness of its\nvisuo-haptic sensing.\n5.2. Bimanual Handover Experiment\nIn this experiment (Fig. 6), an object is placed on a\ntable within reach of the robotâ€™s right arm. The task\nMethod\nSugar Box\nPower Drill\nTomato Can\nAverage\nFP\n60\n40\n20\n40\nV-HOP\n80\n80\n80\n80\nTable 5: Success rate on bimanual handover task.\nrequires the robot to perform the following sequence\nof actions:\n1. Use the right arm to grasp the object and trans-\nport it to the center.\n2. Use the left arm to grasp the object from the\nright hand and place it into a designated bin.\nThe robot employs model-based grasping, which de-\npends on real-time object pose estimation. This task\npresents two key challenges:\n1. If the grasp attempt fails, the robot must detect\nthe failure based on the real-time object pose\nand reattempt the grasp.\n2. During transport to the center, the robot must\nmaintain precise tracking of the objectâ€™s pose to\nensure that the left arm can accurately grasp it.\nInaccurate tracking results could lead to colli-\nsion during the handover.\n9\n\nV-HOP: Visuo-Haptic 6D Object Pose Tracking\nFigure 6: Bimanual handover experiment. In this experiment, the robot performs bimanual manipulation to transport\nthe target object to the box. V-HOP integrates visual and haptic inputs to accurately track the pose of the in-hand object\nin real-time, resulting in stable handover performance. Results on more objects can be found in the appendix.\nFigure 7: Robustness test for the bimanual handover\ntask. (Left) The object is placed at various randomized\npositions. (Right) A human perturbs the object by moving\nit to a different position while the robot attempts to grasp\nit.\nMethod\nCan-in-Mug\nBimanual Can-in-Mug\nFP\n20\n0\nV-HOP\n60\n20\nTable 6: Success rate on Can-in-Mug task.\nV-HOP enables the motion planner to handle ob-\njects in random positions and adapt to dynamic sce-\nnarios, such as human perturbations. For instance, a\nhuman may move the object during task execution,\nremove it from the gripper, or reposition it on the ta-\nble (Fig. 7). Due to the integration of haptic feedback,\nV-HOP accurately tracks the objectâ€™s pose, allowing\nthe robot to promptly detect and respond to changes,\nsuch as the object leaving the gripper. On the con-\ntrary, FoundationPose loses tracking during handover\nor grasping failure (Fig. 6) and leads to collisions. In\nTab. 5, we show the success rate for each object for\nfive trials. V-HOP has 40% higher success rate on\naverage compared to FoundationPose.\n5.3. Can-in-Mug Experiment\nThe Can-in-Mug task (Fig. 8) involves grasping a\ntomato can and inserting it into a mug. The biman-\n(a) Can-in-Mug task.\n(b) Bimanual Can-in-Mug task.\nFigure 8: Can-in-Mug tasks. (top) The robot grasps the\ncan and inserts it into the mug. (bottom) The robot uses\nbimanual to grasp the can and the mug and insert the can\ninto the mug in the center.\nual version requires the robot to also grasp the mug\nand insert the can in the center. Successful execution\nhinges on precise pose estimation for both objects, as\nany noise in their poses can lead to failure. Our re-\nsults (Tab. 6) demonstrate that V-HOP, by integrating\nvisual and haptic inputs, delivers more stable tracking\nand a higher overall success rate.\n5.4. Contribution of each modality\nIn this study, we examine the contribution of visual\nand haptic inputs to the final prediction. We adapt\nGrad-CAM (Selvaraju et al., 2020), utilizing the fi-\nnal normalization layer of the Transformer encoder\nas the target layer. Figure 9 illustrates the weight\ndistribution across the visual and haptic modalities.\n10\n\nV-HOP: Visuo-Haptic 6D Object Pose Tracking\nFigure 9: Weights of visual and haptic modalities to\nthe final prediction. We overlay the modality weights\ncalculated using GradCAM (Selvaraju et al., 2020) in the\ntop-right corner.\nOur findings suggest that when the gripper is not\nin contact with an object, the model predominantly\nrelies on visual inputs. However, as the gripper es-\ntablishes contact and occlusion becomes more severe,\nthe model increasingly shifts its reliance toward hap-\ntic inputs. This finding confirms the choice of self-\nattention mechanism to emulate humanâ€™s â€œoptimal\nintegrationâ€ principle.\n6. Related Works\nIn this work, we consider the problem of 6D object\npose tracking problem, which has been widely stud-\nied as a visual problem (Wen et al., 2020a; Li et al.,\n2018; Wen et al., 2024; Deng et al., 2021). In partic-\nular, we focus on model-based tracking approaches,\nwhich assume access to the objectâ€™s CAD model.\nWhile model-free approaches (Wen and Bekris, 2021;\nWen et al., 2023; Suresh et al., 2024) exist, they fall\noutside the scope of this work. Visual pose track-\ning has achieved significant progress on established\nbenchmarks, such as BOP (Hodan et al., 2024). De-\nspite these successes, deploying such systems in real-\nworld robotic applications remains challenging, es-\npecially under scenarios with high occlusion and\ndynamic interactions, such as in-hand manipulation\ntasks.\nTo address these challenges, prior research has ex-\nplored combining visual and tactile information to im-\nprove pose tracking robustness (Li et al., 2023; Suresh\net al., 2024; Dikhale et al., 2022; Wan et al., 2024;\nRezazadeh et al., 2023; Tu et al., 2023; Gao et al.,\n2023; Li et al., 2024b). These approaches leverage\nlearning-based techniques to estimate object poses\nby fusing visuo-tactile inputs. However, these meth-\nods estimate poses on a per-frame basis, which lacks\ntemporal coherence. Additionally, cross-embodiment\nand domain generalization remain significant hurdles,\nlimiting their scalability and practicality for broad\ndeployment.\nMore recent works aim to overcome some of\nthese limitations. For example, Liu et al. (2024)\nproposes an optimization-based approach that inte-\ngrates tactile data with visual pose tracking using\nan ad-hoc slippage detector and velocity predictor.\nSuresh et al. (2024) extend the model-free tracking\nframeworks BundleTrack (Wen and Bekris, 2021)\nand BundleSDF (Wen et al., 2023) by combining\nvisual and tactile point clouds within a pose graph\noptimization framework. However, these approaches\nare only validated on a single embodiment and suf-\nfer from computational inefficiencies (Suresh et al.,\n2024), which present challenges for real-time deploy-\nment in dynamic manipulation tasks.\n7. Limitation\nWe follow the model-based object pose tracking set-\nting, which assumes that a CAD model is available\nfor the object. One potential direction to overcome\nthis limitation is to simultaneously reconstruct the\nobject and perform pose tracking, as demonstrated in\nmethods like BundleSDF (Wen et al., 2023).\n8. Conclusion\nWe introduced V-HOP, a visuo-haptic 6D object pose\ntracker that integrates a unified haptic representation\nand a visuo-haptic transformer. Our experiments\ndemonstrate that V-HOP generalizes effectively to\nnovel sensor types, embodiments, and objects, out-\nperforming state-of-the-art visual and visuo-tactile\napproaches. Ablation studies highlight the critical\nrole of both visual and haptic modalities in the frame-\nwork. In the sim-to-real transfer experiments, V-HOP\nproved robust, delivering stable tracking under high\nocclusion and dynamic conditions. Furthermore, inte-\ngrating V-HOPâ€™s real-time pose tracking into motion\nplanning enabled accurate manipulation tasks, such\nas bimanual handover and insertion, showcasing its\npractical effectiveness.\nAcknowledgments\nThis work is supported by the National Science Foun-\ndation (NSF) under CAREER grant #2143576, grant\n#2346528, and ONR Grant #N00014-22-1-259. We\nthank Ying Wang, Tao Lu, Zekun Li, and Xiaoyan\nCong for their valuable discussions.\n11\n\nV-HOP: Visuo-Haptic 6D Object Pose Tracking\nReferences\nPeter K. Allen. Integrating Vision and Touch for\nObject Recognition Tasks. The International Jour-\nnal of Robotics Research, 7(6):15â€“33, December\n1988.\nRareÂ¸s Ambrus, Vitor Guizilini, Naveen Kuppuswamy,\nAndrew Beaulieu, Adrien Gaidon, and Alex\nAlspach. Monocular Depth Estimation for Soft\nVisuotactile Sensors.\nIn 2021 IEEE 4th Inter-\nnational Conference on Soft Robotics (RoboSoft),\npages 643â€“649, April 2021.\nMaria Bauza, Oleguer Canal, and Alberto Rodriguez.\nTactile Mapping and Localization from High-\nResolution Tactile Imprints. In 2019 International\nConference on Robotics and Automation (ICRA),\npages 3811â€“3817, May 2019.\nSamarth Brahmbhatt, Ankur Handa, James Hays, and\nDieter Fox. ContactGrasp: Functional Multi-finger\nGrasp Synthesis from Contact. In 2019 IEEE/RSJ\nInternational Conference on Intelligent Robots\nand Systems (IROS), pages 2386â€“2393, Novem-\nber 2019.\nBerk Calli, Arjun Singh, Aaron Walsman, Siddhartha\nSrinivasa, Pieter Abbeel, and Aaron M. Dollar.\nThe YCB object and Model set: Towards common\nbenchmarks for manipulation research. In 2015\nInternational Conference on Advanced Robotics\n(ICAR), pages 510â€“517, July 2015.\nRavinder S. Dahiya, Giorgio Metta, Maurizio Valle,\nand Giulio Sandini. Tactile Sensingâ€”From Hu-\nmans to Humanoids.\nIEEE Transactions on\nRobotics, 26(1):1â€“20, February 2010.\nXinke Deng, Arsalan Mousavian, Yu Xiang, Fei Xia,\nTimothy Bretl, and Dieter Fox.\nPoseRBPF: A\nRaoâ€“Blackwellized Particle Filter for 6-D Object\nPose Tracking. IEEE Transactions on Robotics, 37\n(5):1328â€“1342, October 2021.\nSnehal Dikhale, Karankumar Patel, Daksh Dhingra,\nItoshi Naramura, Akinobu Hayashi, Soshi Iba, and\nNawid Jamali. VisuoTactile 6D Pose Estimation of\nan In-Hand Object Using Vision and Tactile Sensor\nData. IEEE Robotics and Automation Letters, 7(2):\n2148â€“2155, April 2022.\nRunyu Ding, Yuzhe Qin, Jiyue Zhu, Chengzhe Jia,\nShiqi Yang, Ruihan Yang, Xiaojuan Qi, and Xiao-\nlong Wang. Bunny-VisionPro: Real-Time Biman-\nual Dexterous Teleoperation for Imitation Learn-\ning, July 2024.\nElliott Donlon, Siyuan Dong, Melody Liu, Jianhua\nLi, Edward Adelson, and Alberto Rodriguez. Gel-\nSlim: A High-Resolution, Compact, Robust, and\nCalibrated Tactile-sensing Finger, May 2018.\nAlexey\nDosovitskiy,\nLucas\nBeyer,\nAlexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, and Neil Houlsby. An Image is Worth\n16x16 Words: Transformers for Image Recognition\nat Scale, June 2021.\nMarc O. Ernst and Martin S. Banks. Humans inte-\ngrate visual and haptic information in a statistically\noptimal fashion. Nature, 415(6870):429â€“433, Jan-\nuary 2002.\nPietro Falco, Shuang Lu, Andrea Cirillo, Ciro Natale,\nSalvatore Pirozzi, and Dongheui Lee. Cross-modal\nvisuo-tactile object recognition using robotic active\nexploration. In 2017 IEEE International Confer-\nence on Robotics and Automation (ICRA), pages\n5273â€“5280, May 2017.\nYuan Gao, Shogo Matsuoka, Weiwei Wan, Takuya\nKiyokawa, Keisuke Koyama, and Kensuke Harada.\nIn-Hand Pose Estimation Using Hand-Mounted\nRGB Cameras and Visuotactile Sensors. IEEE\nAccess, 11:17218â€“17232, 2023.\nA. M. Gordon, H. Forssberg, R. S. Johansson, and\nG. Westling. The integration of haptically acquired\nsize information in the programming of precision\ngrip. Experimental Brain Research, 83(3):483â€“\n488, February 1991.\nIrmak Guzey, Ben Evans, Soumith Chintala, and\nLerrel Pinto.\nDexterity from Touch:\nSelf-\nSupervised Pre-Training of Tactile Representations\nwith Robotic Play.\nIn Proceedings of The 7th\nConference on Robot Learning, pages 3142â€“3166.\nPMLR, December 2023.\nSami Haddadin, Sven Parusel, Lars Johannsmeier,\nSaskia Golz, Simon Gabl, Florian Walch, Mo-\nhamadreza Sabaghian, Christoph JÃ¤hne, Lukas\nHausperger, and Simon Haddadin. The Franka\nEmika Robot: A Reference Platform for Robotics\n12\n\nV-HOP: Visuo-Haptic 6D Object Pose Tracking\nResearch and Education. IEEE Robotics & Au-\ntomation Magazine, 29(2):46â€“64, June 2022.\nXingyi He, Jiaming Sun, Yuang Wang, Di Huang,\nHujun Bao, and Xiaowei Zhou.\nOnePose++:\nKeypoint-Free One-Shot Object Pose Estimation\nwithout CAD Models.\nAdvances in Neural In-\nformation Processing Systems, 35:35103â€“35115,\nDecember 2022.\nHannah B. Helbig and Marc O. Ernst. Optimal in-\ntegration of shape information from vision and\ntouch. Experimental Brain Research, 179(4):595â€“\n606, June 2007.\nHannah B. Helbig, Marc O. Ernst, Emiliano Ric-\nciardi, Pietro Pietrini, Axel Thielscher, Katja M.\nMayer, Johannes Schultz, and Uta Noppeney. The\nneural mechanisms of reliability weighted integra-\ntion of shape information from vision and touch.\nNeuroImage, 60(2):1063â€“1072, April 2012.\nStefan Hinterstoisser, Vincent Lepetit, Slobodan Ilic,\nStefan Holzer, Gary Bradski, Kurt Konolige, and\nNassir Navab. Model Based Training, Detection\nand Pose Estimation of Texture-Less 3D Objects\nin Heavily Cluttered Scenes. In Kyoung Mu Lee,\nYasuyuki Matsushita, James M. Rehg, and Zhanyi\nHu, editors, Computer Vision â€“ ACCV 2012, Lec-\nture Notes in Computer Science, pages 548â€“562,\nBerlin, Heidelberg, 2013. Springer.\nTomas Hodan, Martin Sundermeyer, Yann Labbe,\nVan Nguyen Nguyen, Gu Wang, Eric Brachmann,\nBertram Drost, Vincent Lepetit, Carsten Rother,\nand Jiri Matas. BOP Challenge 2023 on Detection\nSegmentation and Pose Estimation of Seen and\nUnseen Rigid Objects. pages 5610â€“5619, 2024.\nCheng-Chun Hsu, Bowen Wen, Jie Xu, Yashraj\nNarang, Xiaolong Wang, Yuke Zhu, Joydeep\nBiswas, and Stan Birchfield. SPOT: SE(3) Pose\nTrajectory Diffusion for Object-Centric Manipula-\ntion, November 2024.\nTanja Kassuba, Corinna Klinge, Cordula HÃ¶lig,\nBrigitte RÃ¶der, and Hartwig R. Siebner. Vision\nholds a greater share in visuo-haptic object recog-\nnition than touch. NeuroImage, 65:59â€“68, January\n2013.\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi\nMao, Chloe Rolland, Laura Gustafson, Tete Xiao,\nSpencer Whitehead, Alexander C. Berg, Wan-Yen\nLo, Piotr Dollar, and Ross Girshick.\nSegment\nAnything. pages 4015â€“4026, 2023.\nNaveen Kuppuswamy, Alejandro Castro, Calder\nPhillips-Grafflin, Alex Alspach, and Russ Tedrake.\nFast Model-Based Contact Patch and Pose Esti-\nmation for Highly Deformable Dense-Geometry\nTactile Sensors. IEEE Robotics and Automation\nLetters, 5(2):1811â€“1818, April 2020.\nYann LabbÃ©, Lucas Manuelli, Arsalan Mousavian,\nStephen Tyree, Stan Birchfield, Jonathan Tremblay,\nJustin Carpentier, Mathieu Aubry, Dieter Fox, and\nJosef Sivic. MegaPose: 6D Pose Estimation of\nNovel Objects via Render & Compare. August\n2022.\nSimon Lacey and K. Sathian. Chapter 7 - Visuo-\nhaptic object perception. In K. Sathian and V. S.\nRamachandran, editors, Multisensory Perception,\npages 157â€“178. Academic Press, January 2020.\nMike Lambeta, Po-Wei Chou, Stephen Tian, Brian\nYang, Benjamin Maloon, Victoria Rose Most,\nDave Stroud, Raymond Santos, Ahmad Byagowi,\nGregg Kammerer, Dinesh Jayaraman, and Roberto\nCalandra. DIGIT: A Novel Design for a Low-Cost\nCompact High-Resolution Tactile Sensor With Ap-\nplication to In-Hand Manipulation. IEEE Robotics\nand Automation Letters, 5(3):3838â€“3845, July\n2020.\nTaeyeop Lee, Jonathan Tremblay, Valts Blukis,\nBowen Wen, Byeong-Uk Lee, Inkyu Shin, Stan\nBirchfield, In So Kweon, and Kuk-Jin Yoon. TTA-\nCOPE: Test-Time Adaptation for Category-Level\nObject Pose Estimation.\npages 21285â€“21295,\n2023.\nAlbert H. Li, Preston Culbertson, Vince Kurtz, and\nAaron D. Ames. DROP: Dexterous Reorientation\nvia Online Planning, October 2024a.\nHongyu Li, Snehal Dikhale, Soshi Iba, and Nawid\nJamali. ViHOPE: Visuotactile In-Hand Object 6D\nPose Estimation With Shape Completion. IEEE\nRobotics and Automation Letters, 8(11):6963â€“\n6970, November 2023.\nHongyu Li, Snehal Dikhale, Jinda Cui, Soshi Iba, and\nNawid Jamali. HyperTaxel: Hyper-Resolution for\nTaxel-Based Tactile Signals Through Contrastive\n13\n\nV-HOP: Visuo-Haptic 6D Object Pose Tracking\nLearning. In 2024 IEEE/RSJ International Con-\nference on Intelligent Robots and Systems (IROS),\npages 7499â€“7506, October 2024b.\nYi Li, Gu Wang, Xiangyang Ji, Yu Xiang, and Dieter\nFox. DeepIM: Deep Iterative Matching for 6D\nPose Estimation. pages 683â€“698, 2018.\nYuelong Li, Yafei Mao, Raja Bala, and Sunil Hadap.\nMRC-Net: 6-DoF Pose Estimation with Multi-\nScale Residual Correlation. pages 10476â€“10486,\n2024c.\nJiehong Lin, Lihua Liu, Dekun Lu, and Kui Jia. SAM-\n6D: Segment Anything Model Meets Zero-Shot\n6D Object Pose Estimation. pages 27906â€“27916,\n2024a.\nToru Lin, Yu Zhang, Qiyang Li, Haozhi Qi, Brent\nYi, Sergey Levine, and Jitendra Malik. Learning\nVisuotactile Skills with Two Multifingered Hands,\nApril 2024b.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and\nYong Jae Lee.\nVisual Instruction Tuning.\nAd-\nvances in Neural Information Processing Systems,\n36:34892â€“34916, December 2023.\nYuan Liu, Yilin Wen, Sida Peng, Cheng Lin, Xi-\naoxiao Long, Taku Komura, and Wenping Wang.\nGen6D: Generalizable Model-Free 6-DoF Object\nPose Estimation from RGB Images.\nIn Shai\nAvidan, Gabriel Brostow, Moustapha CissÃ©, Gio-\nvanni Maria Farinella, and Tal Hassner, editors,\nComputer Vision â€“ ECCV 2022, pages 298â€“315,\nCham, 2022. Springer Nature Switzerland.\nYun Liu, Xiaomeng Xu, Weihang Chen, Haocheng\nYuan, He Wang, Jing Xu, Rui Chen, and Li Yi. En-\nhancing Generalizable 6D Pose Tracking of an In-\nHand Object with Tactile Sensing. IEEE Robotics\nand Automation Letters, 9(2):1106â€“1113, February\n2024.\nJack M. Loomis and Susan J. Lederman. Tactual\nperception. In Handbook of perception and human\nperformance, Vol. 2: Cognitive processes and per-\nformance, pages 1â€“41. John Wiley & Sons, Oxford,\nEngland, 1986.\nIlya Loshchilov and Frank Hutter. Decoupled Weight\nDecay Regularization. September 2018.\nE. Marchand, F. Spindler, and F. Chaumette. ViSP for\nvisual servoing: a generic software platform with\na wide class of robot control skills. IEEE Robotics\n& Automation Magazine, 12(4):40â€“52, December\n2005.\nLuis Felipe Casas Murrilo, Ninad Khargonkar, Bal-\nakrishnan Prabhakaran, and Yu Xiang. MultiGrip-\nperGrasp: A Dataset for Robotic Grasping from\nParallel Jaw Grippers to Dexterous Hands, March\n2024.\nNicolÃ¡s Navarro-Guerrero, Sibel Toprak, Josip Josi-\nfovski, and Lorenzo Jamone. Visuo-haptic object\nperception for robots: an overview. Autonomous\nRobots, 47(4):377â€“403, April 2023.\nVan Nguyen Nguyen, Thibault Groueix, Georgy Poni-\nmatkin, Vincent Lepetit, and Tomas Hodan. CNOS:\nA Strong Baseline for CAD-Based Novel Object\nSegmentation. pages 2134â€“2140, 2023.\nKiru Park, Timothy Patten, and Markus Vincze.\nPix2Pose: Pixel-Wise Coordinate Regression of\nObjects for 6D Pose Estimation. pages 7668â€“7677,\n2019.\nCharles Ruizhongtai Qi, Li Yi, Hao Su, and\nLeonidas J Guibas.\nPointNet++: Deep Hierar-\nchical Feature Learning on Point Sets in a Metric\nSpace. In Advances in Neural Information Process-\ning Systems, volume 30. Curran Associates, Inc.,\n2017.\nAlireza Rezazadeh, Snehal Dikhale, Soshi Iba, and\nNawid Jamali. Hierarchical Graph Neural Net-\nworks for Proprioceptive 6D Pose Estimation of\nIn-hand Objects. In 2023 IEEE International Con-\nference on Robotics and Automation (ICRA), pages\n2884â€“2890, May 2023.\nRamprasaath R. Selvaraju, Michael Cogswell, Ab-\nhishek Das, Ramakrishna Vedantam, Devi Parikh,\nand Dhruv Batra. Grad-CAM: Visual Explanations\nfrom Deep Networks via Gradient-based Localiza-\ntion. International Journal of Computer Vision,\n128(2):336â€“359, February 2020.\nSudharshan Suresh, Zilin Si, Joshua G. Mangelson,\nWenzhen Yuan, and Michael Kaess. ShapeMap\n3-D: Efficient shape mapping through dense touch\nand vision.\nIn 2022 International Conference\non Robotics and Automation (ICRA), pages 7073â€“\n7080, May 2022.\n14\n\nV-HOP: Visuo-Haptic 6D Object Pose Tracking\nSudharshan Suresh, Zilin Si, Stuart Anderson,\nMichael Kaess, and Mustafa Mukadam. Midas-\nTouch: Monte-Carlo inference over distributions\nacross sliding touch. In Proceedings of The 6th\nConference on Robot Learning, pages 319â€“331.\nPMLR, March 2023.\nSudharshan Suresh, Haozhi Qi, Tingfan Wu, Taosha\nFan, Luis Pineda, Mike Lambeta, Jitendra Malik,\nMrinal Kalakrishnan, Roberto Calandra, Michael\nKaess, Joseph Ortiz, and Mustafa Mukadam. Neu-\nralFeels with neural fields: Visuotactile percep-\ntion for in-hand manipulation. Science Robotics,\nNovember 2024.\nChie Takahashi and Simon Justin Watt. Visual-haptic\nintegration with pliers and tongs: signal â€œweightsâ€\ntake account of changes in haptic sensitivity caused\nby different tools.\nFrontiers in Psychology, 5,\nFebruary 2014.\nIan H. Taylor, Siyuan Dong, and Alberto Rodriguez.\nGelSlim 3.0: High-Resolution Measurement of\nShape, Force and Slip in a Compact Tactile-\nSensing Finger. In 2022 International Conference\non Robotics and Automation (ICRA), pages 10781â€“\n10787, May 2022.\nJonathan Tremblay, Thang To, Balakumar Sundar-\nalingam, Yu Xiang, Dieter Fox, and Stan Birchfield.\nDeep Object Pose Estimation for Semantic Robotic\nGrasping of Household Objects. In Proceedings\nof The 2nd Conference on Robot Learning, pages\n306â€“316. PMLR, October 2018.\nJonathan Tremblay, Bowen Wen, Valts Blukis, Bal-\nakumar Sundaralingam, Stephen Tyree, and Stan\nBirchfield. Diff-DOPE: Differentiable Deep Ob-\nject Pose Estimation, September 2023.\nYuyang Tu, Junnan Jiang, Shuang Li, Norman Hen-\ndrich, Miao Li, and Jianwei Zhang. PoseFusion:\nRobust Object-in-Hand Pose Estimation with Se-\nlectLSTM. In 2023 IEEE/RSJ International Con-\nference on Intelligent Robots and Systems (IROS),\npages 6839â€“6846, October 2023.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Åukasz\nKaiser, and Illia Polosukhin. Attention is All you\nNeed. In Advances in Neural Information Process-\ning Systems, volume 30, 2017.\nZhaoliang Wan, Yonggen Ling, Senlin Yi, Lu Qi,\nWang Wei Lee, Minglei Lu, Sicheng Yang, Xiao\nTeng, Peng Lu, Xu Yang, Ming-Hsuan Yang, and\nHui Cheng. VinT-6D: A Large-Scale Object-in-\nhand Dataset from Vision, Touch and Propriocep-\ntion. In Proceedings of the 41st International Con-\nference on Machine Learning, pages 49921â€“49940.\nPMLR, July 2024.\nChen Wang, Danfei Xu, Yuke Zhu, Roberto Martin-\nMartin, Cewu Lu, Li Fei-Fei, and Silvio Savarese.\nDenseFusion: 6D Object Pose Estimation by Itera-\ntive Dense Fusion. pages 3343â€“3352, 2019a.\nHe Wang, Srinath Sridhar, Jingwei Huang, Julien\nValentin, Shuran Song, and Leonidas J. Guibas.\nNormalized Object Coordinate Space for Category-\nLevel 6D Object Pose and Size Estimation. pages\n2642â€“2651, 2019b.\nDavid Watkins-Valls, Jacob Varley, and Peter Allen.\nMulti-Modal Geometric Learning for Grasping and\nManipulation. In 2019 International Conference\non Robotics and Automation (ICRA), pages 7339â€“\n7345, May 2019.\nBowen Wen and Kostas Bekris. BundleTrack: 6D\nPose Tracking for Novel Objects without Instance\nor Category-Level 3D Models, August 2021.\nBowen Wen, Chaitanya Mitash, Baozhang Ren, and\nKostas E. Bekris. se(3)-TrackNet: Data-driven 6D\nPose Tracking by Calibrating Image Residuals in\nSynthetic Domains. In 2020 IEEE/RSJ Interna-\ntional Conference on Intelligent Robots and Sys-\ntems (IROS), pages 10367â€“10373, October 2020a.\nBowen Wen, Chaitanya Mitash, Sruthi Soorian, An-\ndrew Kimmel, Avishai Sintov, and Kostas E.\nBekris.\nRobust, Occlusion-aware Pose Estima-\ntion for Objects Grasped by Adaptive Hands. In\n2020 IEEE International Conference on Robotics\nand Automation (ICRA), pages 6210â€“6217, May\n2020b.\nBowen Wen, Wenzhao Lian, Kostas Bekris, and\nStefan Schaal.\nYou Only Demonstrate Once:\nCategory-Level Manipulation from Single Visual\nDemonstration, May 2022.\nBowen Wen, Jonathan Tremblay, Valts Blukis,\nStephen Tyree, Thomas Muller, Alex Evans, Dieter\nFox, Jan Kautz, and Stan Birchfield. BundleSDF:\n15\n\nV-HOP: Visuo-Haptic 6D Object Pose Tracking\nNeural 6-DoF Tracking and 3D Reconstruction of\nUnknown Objects, March 2023.\nBowen Wen, Wei Yang, Jan Kautz, and Stan Birch-\nfield. FoundationPose: Unified 6D Pose Estima-\ntion and Tracking of Novel Objects. pages 17868â€“\n17879, 2024.\nBing Wu, Qian Liu, and Qiang Zhang. Tactile Pat-\ntern Super Resolution with Taxel-based Sensors.\nIn 2022 IEEE/RSJ International Conference on In-\ntelligent Robots and Systems (IROS), pages 3644â€“\n3650, October 2022.\nYu Xiang, Tanner Schmidt, Venkatraman Narayanan,\nand Dieter Fox. PoseCNN: A Convolutional Neu-\nral Network for 6D Object Pose Estimation in Clut-\ntered Scenes. volume 14, June 2018.\nZhengrong Xue, Han Zhang, Jingwen Cheng, Zheng-\nmao He, Yuanchen Ju, Changyi Lin, Gu Zhang,\nand Huazhe Xu. ArrayBot: Reinforcement Learn-\ning for Generalizable Distributed Manipulation\nthrough Touch, June 2023.\nAkihiko Yamaguchi and Christopher G. Atkeson. Re-\ncent progress in tactile sensing and sensors for\nrobotic manipulation: can we turn tactile sensing\ninto vision? Advanced Robotics, 33(14):661â€“673,\nJuly 2019.\nLinhan Yang, Bidan Huang, Qingbiao Li, Ya-Yen\nTsai, Wang Wei Lee, Chaoyang Song, and Jia Pan.\nTacGNN: Learning Tactile-Based In-Hand Manip-\nulation With a Blind Robot Using Hierarchical\nGraph Neural Network. IEEE Robotics and Au-\ntomation Letters, 8(6):3605â€“3612, June 2023.\nLixin Yang, Xinyu Zhan, Kailin Li, Wenqiang Xu,\nJiefeng Li, and Cewu Lu. CPF: Learning a Contact\nPotential Field to Model the Hand-Object Interac-\ntion. In 2021 IEEE/CVF International Conference\non Computer Vision (ICCV), pages 11077â€“11086,\nOctober 2021.\nLixin Yang, Xinyu Zhan, Kailin Li, Wenqiang Xu,\nJunming Zhang, Jiefeng Li, and Cewu Lu. Learn-\ning a Contact Potential Field for Modeling the\nHand-Object Interaction. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 46(8):\n5645â€“5662, August 2024.\nZhao-Heng Yin, Binghao Huang, Yuzhe Qin, Qifeng\nChen, and Xiaolong Wang. Rotating without See-\ning: Towards In-hand Dexterity through Touch.\nvolume 19, July 2023.\nWenzhen Yuan, Siyuan Dong, and Edward H. Adel-\nson. GelSight: High-Resolution Robot Tactile Sen-\nsors for Estimating Geometry and Force. Sensors,\n17(12):2762, December 2017.\n16\n\nV-HOP: Visuo-Haptic 6D Object Pose Tracking\nFigure 10: Distributions of embodiments and objects. The novel gripper and object have fewer samples as they are\nonly used for evaluation and not during training.\nFigure 11: Dataset distributions. We view the occlusion rate, position, and rotation distribution of our data samples.\nAppendices\nA. Dataset\nIn Fig. 11, we provide a visualization of the datasetâ€™s distribution. The dataset focuses on in-hand object pose\ntracking and addresses challenges such as heavy occlusion during in-hand manipulation. Our visualization\nreveals that the positions and rotations of the data samples are well-distributed, following either normal or\nuniform distributions, ensuring a comprehensive evaluation of pose tracking performance.\nB. Experiments\n17\n\nV-HOP: Visuo-Haptic 6D Object Pose Tracking\nFigure 12: Accuracy-threshold curve (Xiang et al., 2018) on our dataset. V-HOP consistently demonstrates stronger\nor similar performance as FoundationPose (FP) under various thresholds.\nFP\nVH6T\nFP\nVH6T\nFigure 13: Qualitative results of pose tracking sequences. We perform qualitative comparisons on more objects. Our\nresults demonstrate that V-HOP consistently outperforms FP by a large margin.\n18\n\nV-HOP: Visuo-Haptic 6D Object Pose Tracking\nFigure 14: Bimanual handover experiments. We perform bimanual handover experiments on more objects.\n19\n",
  "metadata": {
    "source_path": "papers/arxiv/V-HOP_Visuo-Haptic_6D_Object_Pose_Tracking_012bb6cb3e5a82f3.pdf",
    "content_hash": "012bb6cb3e5a82f3fc2e46e7ddb262abed830dfa26a8eb97d50849b2edec0c76",
    "arxiv_id": null,
    "title": "V-HOP_Visuo-Haptic_6D_Object_Pose_Tracking_012bb6cb3e5a82f3",
    "author": "",
    "creation_date": "D:20250225031024Z",
    "published": "2025-02-25T03:10:24",
    "pages": 19,
    "size": 49513893,
    "file_mtime": 1740470078.800044
  }
}