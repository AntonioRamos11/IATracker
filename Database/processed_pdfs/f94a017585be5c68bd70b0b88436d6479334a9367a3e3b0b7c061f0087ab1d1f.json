{
  "text": "FIG: Forward-Inverse Generation for Low-Resource\nDomain-specific Event Detection\nTanmay Parekh\nYuxuan Dong\nLucas Bandarkar\nArtin Kim\nI-Hung Hsuâ€ \nKai-Wei Chang\nNanyun Peng\nComputer Science Department, University of California, Los Angeles\nâ€ Google\n{tparekh, violetpeng, kwchang}@cs.ucla.edu\nAbstract\nEvent Detection (ED) is the task of identi-\nfying typed event mentions of interest from\nnatural language text, which benefits domain-\nspecific reasoning in biomedical, legal, and\nepidemiological domains. However, procur-\ning supervised data for thousands of events\nfor various domains is a laborious and expen-\nsive task. To this end, existing works have\nexplored synthetic data generation via forward\n(generating labels for unlabeled sentences) and\ninverse (generating sentences from generated\nlabels) generations. However, forward gen-\neration often produces noisy labels, while in-\nverse generation struggles with domain drift\nand incomplete event annotations. To address\nthese challenges, we introduce FIG, a hybrid\napproach that leverages inverse generation for\nhigh-quality data synthesis while anchoring it\nto domain-specific cues extracted via forward\ngeneration on unlabeled target data. FIG fur-\nther enhances its synthetic data by adding miss-\ning annotations through forward generation-\nbased refinement. Experimentation on three\nED datasets from diverse domains reveals that\nFIG outperforms the best baseline, achieving\naverage gains of 3.3% F1 and 5.4% F1 in the\nzero-shot and few-shot settings, respectively.\nAnalyzing the generated trigger hit rate and\nhuman evaluation substantiates FIGâ€™s superior\ndomain alignment and data quality compared\nto existing baselines. We will release our code\nat https://github.com/PlusLabNLP/FIG.\n1\nIntroduction\nEvent Detection (ED) (Sundheim, 1992; Dodding-\nton et al., 2004) involves identifying and categoriz-\ning significant events from natural language text\nbased on a pre-defined ontology. It has applications\nin widespread domains like biomedicine (Pyysalo\net al., 2012), epidemiology (Parekh et al., 2024b,c),\nlaw (Francesconi et al., 2010), etc.\nHowever,\nprocuring annotations for each domain-specific on-\ntology to train models is expensive and impractical.\nFigure 1:\nComparing different data generation\nparadigms. Owing to poor LLM ED reasoning skills,\nforward generation (top) struggles with noisy labels.\nInverse generation (middle) suffers from domain drift\nowing to lack of domain-specific cues. We introduce\nFIG (bottom), a hybrid forward and inverse approach,\nto overcome both issues to generate better quality data.\nAlthough recent works introduce zero-shot LLM-\nreasoning (Gao et al., 2023; Cai et al., 2024), their\nperformance falls short of supervised approaches\n(Huang et al., 2024).\nTo mitigate the need for extensive annotations to\ntrain supervised models, prior work has explored\nLLM-powered synthetic data generation to enhance\ndownstream model training. A widely used ap-\nproach is a forward generation or weak supervision\n(He et al., 2021; Chia et al., 2022), where labels are\nassigned to existing unlabeled data (i.e., X â†’Y â€²).\nThis generation is limited by LLM ED reasoning,\noften leading to noisy labels. To address this, in-\nverse generation has been proposed (Meng et al.,\n2022; Wang et al., 2023c), where sentences are gen-\nerated based on sampled or generated labels (i.e\n1\narXiv:2502.17394v1  [cs.CL]  24 Feb 2025\n\nY â†’Xâ€²). However, inverse generation introduces\na domain drift between synthetic and target data\nowing to the lack of any domain-specific cues, as\nillustrated in Figure 1.\nIn our work, we propose Forward-Inverse\nGeneration (FIG), a hybrid approach that inte-\ngrates forward and inverse generation to enhance\nsynthetic data quality.\nInstead of generating\nevent triggers from scratch â€” an inherently high-\nvariance process â€” we first apply forward genera-\ntion to unlabeled data, extracting domain-specific\ntriggers. Next, we utilize inverse generation to\nsynthesize diverse sentences conditioned on the\ndomain-specific triggers. Finally, a second forward\ngeneration annotates any missing events in the gen-\nerated data to ensure high data quality. This ap-\nproach produces cleaner synthetic data compared\nto forward generation while maintaining a closer\ndomain alignment with target data than inverse gen-\neration, as shown in Figure 1.\nWe evaluate FIG on ED datasets from three dif-\nferent domains: ACE (Doddington et al., 2004)\n(news), SPEED (Parekh et al., 2024c) (social me-\ndia), and GENIA2011 (Kim et al., 2011) (biomedi-\ncal). Our primary method of evaluation is testing\nED performance of DEGREE (Hsu et al., 2022)\ntrained on the generated data. In the zero-shot\nsetting, FIG outperforms STAR (Ma et al., 2024)\n(inverse generation baseline) and weak supervi-\nsion (forward generation baseline) by an average\nof 16.8% and 3.5% Tri-C F1, respectively. Sim-\nilarly, in the few-shot setting, FIG surpasses the\nstrongest baseline by an average of 6.2% Tri-C F1.\nWe study the generated trigger hit rate (relative to\nthe gold trigger set) and demonstrate how FIGâ€™s\ntriggers have about 7.3% better hit rate compared\nto the best baseline, which contributes to its supe-\nrior performance. Finally, we also conduct human\nevaluation, which supports FIGâ€™s superior domain\nrelevance and data annotation quality.\n2\nProblem Definition\nWe focus on the task of Event Detection (ED)\n(Sundheim, 1992; Doddington et al., 2004) for this\nwork. The task of ED aims to extract mentions of\nany events of interest from natural language text.\nFollowing ACE 2005 (Doddington et al., 2004), we\ndefine an event as something that happens or de-\nscribes a change of state and is labeled by a specific\nevent type. The word/phrase that most distinctly\nhighlights the occurrence of the event is defined as\nthe event trigger, and the trigger-event type pair is\nreferred to as the event mention. Event Detection,\nin particular, requires extracting the event triggers\nfrom the sentence and classifying it into one of the\npre-defined event types. We provide an illustration\nof this task below where arrested and campaigns\nare the trigger words for the event types Justice:\nArrest-Jail and Conflict: Demonstrate respectively.\nSome 3,000 people have been arrested since\nthe disobedience campaigns began last week.\nJustice: Arrest-Jail\nConflict: Demonstrate\nIn our work, we specifically focus on ED in\ndiverse and specialized domains (e.g., biomedical),\nwhere procuring a training dataset DT of annotated\ndata points is expensive, whereas reasonable-sized\nunlabeled data Dâ€²\nT is available. We focus on two\nrealistic low-resource data setups - zero-shot (zero\nlabeled data) and few-shot (k labeled datapoints\nper event type) settings. Different from domain\ntransfer, we do not consider any labeled data for\nthe source domain and directly optimize model\nperformance for the target domain.\n3\nMethodology\nIn this work, we focus on Large Language Model\n(LLM)-powered synthetic data generation to allevi-\nate the need for domain-specific annotated training\ndata, as zero-shot LLM-based approaches usually\nstruggle in ED in specialized domains (Huang et al.,\n2024). By generating a large amount of data in-\nstances Ds = {(X, Y )}, we can train downstream\nsupervised ED models. Here, we first briefly review\nprior works on forward generation and backward\ngeneration. Next, we introduce our proposed data\ngeneration method FIG.\n3.1\nBackground\nWe focus on two major paradigms for data genera-\ntion in our work: forward generation and inverse\ngeneration. We describe them briefly below and\nprovide illustrations in Figure 2.\nForward Generation:\nThis is a more straightfor-\nward manner of data generation, wherein LLMs\nare utilized to generate labels Y â€² on unlabeled\ndata X âˆˆDâ€²\nT (i.e. X â†’Y â€²). This generation\nis analogous to weak/distant supervision (Weak\nSup) (Mintz et al., 2009; Wang et al., 2021; He\net al., 2021) where noisy/weak labels are assigned\n2\n\nForward Generation\n(Weak Sup)\nInverse Generation\n(STAR)\nHybrid Generation\n(FIG)\nUnlabeled \nData ð‘‹âˆˆð·ð‘‡â€²\nNoisy Trigger ð‘Œâ€²\nGenerated\nTrigger ð‘Œ\nGenerated\nPassage ð‘‹â€²\nUnlabeled \nData ð‘‹âˆˆð·ð‘‡â€²\nNoisy\nTriggers ð‘Œâ€²\nTarget Structure Extraction (Forward Generation)\nExtracted Triggers\nRank &\nFilter\nPassage Generation\n(Inverse Generation)\nData Refinement (Forward \nGeneration)\nGenerated\nPassage ð‘‹â€²\nSampled\nTrigger ð‘Œâ€²\nExtracted Missing\nTriggers\nUpdate\nData Sampling\nDownstream Model Training\nFinal\nData\nDEGREE\nGenerative\nED model\nFigure 2: Model Architecture Diagram. Top left - illustration of forward generation. Bottom left - illustration of\ninverse generation. Right - FIG and its four components. We first extract domain-specific triggers via forward\ngeneration, then generate passages using inverse generation. Forward generation refines missing events, and we\nsample N data points per event for downstream training.\nto clean sentences and then utilized to train down-\nstream models. Consequently, the label quality is\ndependent on the reasoning capability of the LLM.\nInverse Generation:\nInverse generation is a rela-\ntively new data generation paradigm (Kumar et al.,\n2020; Schick and SchÃ¼tze, 2021). This paradigm\nfirst generates/samples potential labels Y based on\nthe task definition and then generates plausible Xâ€²\nconditioned on the label Y (i.e. Y â†’Xâ€²). Re-\ncent works like SynthIE (Josifoski et al., 2023) and\nSTAR (Ma et al., 2024) have explored the usage\nof LLM-guided inverse generation for information\nextraction and event extraction tasks. Inverse gen-\neration provides control over data distribution via\nprompting, but the data quality is dependent on the\nLLMâ€™s sentence generation capability.\n3.2\nFIG\nLLM reasoning has been poor for ED (Li et al.,\n2023a; Huang et al., 2024), which makes forward\ngeneration vulnerable to noisy label quality. Since\nLLMs are pre-trained to generate natural sentences,\ntheir generation capabilities are stronger, which\nsupports inverse generation. However, owing to the\nlack of any domain-specific information, inverse\ngeneration can synthesize highly diverse sentences,\nleading to a domain drift. Merely specifying the\ndomain in the prompt is not sufficient (shown in\nÂ§ D.1). Furthermore, inverse generation sentences\ncan mention additional events which remain unan-\nnotated introducing noise in the data.\nTo this end, we propose Forward-Inverse\nGeneration (FIG), a hybrid approach that infuses\nforward generation reasoning into the base pipeline\nof inverse generation to ensure closer alignment to\nthe target domain while improving the quality of\nthe annotated labels. To procure domain-specific\ncues, we assume access to an unlabeled target do-\nmain data Dâ€²\nT (similar to forward generation). We\nprovide our architectural diagram in Figure 2 and\nexplain each component of our pipeline below.\nTarget Structure Extraction:\nSimilar to inverse\ngeneration, we first create potential triggers (i.e.\nlabels) for each event type. However, generation\nfrom scratch can lead to very divergent set of trig-\ngers. For example, an â€œAttack\" event can refer to a\nwar in the news, cyberattacks in the cybersecurity\ndomain, diseases in epidemiology, or criticism in\neconomics. Naturally, each scenario would assume\ndifferent triggers, which inverse generation strug-\ngles to generate even when provided with clear\nevent definitions (actual examples in Â§ 6.5).\nInstead, we generate potential triggers per event\ntype by utilizing an LLM to extract triggers using\na forward generation over the unlabeled data Dâ€²\nT .\nTo ensure highly precise extraction of triggers, we\ndevelop a two-stage prompt setup. The first stage\nis tasked to identify and filter possible event types\nmentioned in the text based on the task instructions\nand event definitions. The second stage aims to\nfind the most appropriate trigger word from the\nunlabeled sentence for each filtered event type. We\nillustrate this setup in Â§ A.\nAfter extraction, we aggregate and sort the trig-\ngers for each event type at the corpus level and filter\nout the top t triggers for each event type as the set\nof clean triggers. This filtering helps remove many\nnoisy triggers introduced in the forward generation.\nFinally, we create target structures Y by sampling\n1-2 event types per instance and sampling triggers\n3\n\nMissed Event\nTrigger:\nâ€œgotâ€\nEvent: symptom\nGeneration Trigger:\nâ€œPositiveâ€ Event: infect\nOk, I just got a fever... Theres a\npossibility Im COVID-19 Positive\nFigure 3: Illustration of how inverse generation can\nproduce unannotated event mentions. Blue box = target\nevent mention, red box = unannotated event mention.\nfrom the clean trigger set for each sampled event\ntype. We ensure uniform sampling of event types\nand triggers to avoid unbalanced label distribution.\nOverall, such forward generation-based trigger ex-\ntraction ensures the distillation of domain-specific\nknowledge in the target structures.\nPassage Generation:\nThis component is the core\nof inverse generation and is tasked with generating\npassages corresponding to the constraints of the\nsampled target structure. Specifically, the LLM is\nprompted with the task instructions, the event defi-\nnitions, the sampled target structure Y , and asked\nto generate a passage Xâ€² which mentions the event\ntypes using the triggers in Y . We illustrate this\nprompt in Figure 8. The key to generating better\ntarget data aligned passages lies in the presence of\nthe target data aligned target structure Y (shown\nvia examples in Â§ 6.5).\nData Refinement and Sampling:\nWhile passage\ngeneration ensures the target structure events are\nmentioned in the passage, the passage could still\nhave other unknown events which can lead to under-\nannotated (Xâ€², Y ) data instances. We provide an\nillustration in Figure 3 with the target trigger posi-\ntive, but the generated sentence also has a missing\nmention of symptom event triggered by got. To\naccount for such missing annotations, we utilize\nforward generation to identify all possible event\nmentions in the generated passage. We remove du-\nplicates and add the new event mentions to create a\nmore complete target structure Yf.\nTo further improve data quality, we apply an au-\ntomated rule to remove passages that do not men-\ntion the target trigger. Additionally, we standardize\ntrigger annotations by correcting variations in trig-\nger word forms. Finally, we apply a greedy sam-\npling algorithm to sample N instances (Xâ€², Yf) per\nevent type to create our final synthetic dataset Ds.\nDownstream Model Training:\nThe final com-\nponent utilizes the generated synthetic data Ds to\ntrain downstream ED models in a supervised man-\nner. The trained ED models are then used for infer-\nring on the test set and for eventual evaluation.\n4\nExperimental Setup\nDatasets:\nWe consider three ED datasets from\ndiverse domains for our experiments: (1) ACE\n(Doddington et al., 2004), in the news domain, (2)\nSPEED (Parekh et al., 2024c), in the social media\ndomain, and (3) GENIA (Kim et al., 2011), in the\nbiomedical domain. We simplify GENIA by con-\nverting the original document level annotations to\nsentence-level annotations. For the few-shot set-\nting, we compile k data instances from the training\ndata as the few-shot examplers.\nFor our unlabeled data, we consider two sources:\n(1) Train - annotation-free training splits (i.e. only\nthe text) of each dataset and (2) External - un-\nlabeled data from other external sources.\nFor\nACE, we utilize News Category Dataset (Misra,\n2022) comprising Huffpost news articles from\n2012-2022 as the external data source. We filter ar-\nticles corresponding to political, financial, and busi-\nness articles. For SPEED, we utilize COVIDKB\n(Zong et al., 2022) mining tweets from the Twit-\nter COVID-19 Endpoint released in April 2020 as\nthe external data source. Finally, we utilize GE-\nNIA2013 dataset (Kim et al., 2013) as the external\ndata for GENIA. We provide statistics about these\ndatasets in Table 8.\nBaseline methods:\nWe consider three LLM-\nbased techniques for low-resource ED as the base-\nlines for our work. (1) Inference (Gao et al., 2023):\nLLMs are used to directly infer on the target test\ndata using their reasoning capability. (2) STAR\n(Ma et al., 2024): This model is the state-of-the-art\ninverse generation model for ED. It utilizes trigger\ngeneration, passage generation, and data refine-\nment steps without using any unlabeled data, (3)\nWeak Supervision (Weak Sup): This model acts\nas the forward generation baseline. We utilize the\nInference model to provide labels for the unlabeled\ndata. For an upper bound reference, we also include\na human generation baseline (Human) wherein we\nsample N data instances from the gold training\ndata of each dataset to train the downstream ED\nmodel.\nBase models:\nFor our base LLMs, we consider\nthree instruction-tuned LLMs of varying sizes,\nnamely Llama3-8B-Instruct (8B model), Llama3-\n4\n\nBase LLM\nMethod\nUnlabeled\nACE\nSPEED\nGENIA\nAverage\nData Source\nEve-I\nTri-C\nEve-I\nTri-C\nEve-I\nTri-C\nEve-I\nTri-C\nLlama3-8B\nInference\n-\n30.2\n23.8\n39.8\n25.4\n21.9\n17.2\n30.6\n22.1\nSTAR\n-\n44.9\n35.0\n21.0\n10.1\n25.9\n19.0\n30.6\n21.4\nWeak Sup\ntrain\n41.7\n37.8\n45.6\n31.5\n26.9\n21.4\n38.1\n30.2\nFIG(ours)\ntrain\n57.4\n50.2\n44.6\n31.5\n35.2\n28.9\n45.7\n36.9\nFIG (ours)\nexternal\n57.7\n52.6\n47.8\n32.9\n33.6\n24.6\n46.4\n36.7\nLlama3-70B\nInference\n-\n46.9\n41.3\n46.9\n35.6\n34.2\n28.2\n42.7\n35.0\nSTAR\n-\n50.0\n42.3\n18.3\n13.8\n23.3\n16.9\n30.5\n24.3\nWeak Sup\ntrain\n53.2\n48.0\n52.8\n39.6\n36.2\n29.1\n47.4\n38.9\nFIG (ours)\ntrain\n58.1\n53.8\n49.9\n38.7\n38.0\n29.7\n48.7\n40.7\nFIG (ours)\nexternal\n59.7\n55.6\n50.1\n39.2\n39.2\n31.5\n49.7\n42.1\nGPT-3.5\nInference\n-\n33.0\n26.2\n44.2\n32.9\n31.2\n24.7\n36.1\n27.9\nSTAR\n-\n45.0\n36.6\n21.3\n14.6\n21.8\n14.3\n29.4\n21.8\nWeak Sup\ntrain\n49.7\n44.6\n50.7\n37.5\n37.7\n30.1\n46.1\n37.4\nFIG (ours)\ntrain\n54.8\n48.3\n50.3\n36.8\n39.3\n31.1\n48.1\n38.7\nFIG (ours)\nexternal\n54.0\n48.5\n50.1\n36.1\n38.7\n29.4\n47.6\n38.0\n-\nGold Data\n-\n64.6\n61.6\n64.0\n53.5\n51.3\n44.0\n60.0\n53.0\nTable 1: Zero-shot results comparing FIG with other baselines across three datasets and three base LLMs. Except\nfor Inference, all other evaluations are performances of downstream DEGREE (Hsu et al., 2022) model trained on\ndata generated by each technique. Eve-I: Event Identification F1, Tri-C: Trigger Classification F1.\n70B-Instruct (70B model) (Dubey et al., 2024),\nand GPT-3.5 (175B model) (Brown et al., 2020).\nFor our downstream ED model, we consider a spe-\ncialized low-resource model DEGREE (Hsu et al.,\n2022), a generative model prompted to fill event\ntemplates powered on a BART-large pre-trained\nlanguage model (Lewis et al., 2020).\nEvaluation:\nOur primary evaluation metric is the\nsynthesized data-trained modelâ€™s ED performance\non the final test splits of each dataset. We consider\ntwo low-resource settings - zero-shot (no labeled\ndata) and few-shot (k datapoints per event type are\nused). Note this is different cross-dataset works\n(Cai et al., 2024) which train and test on an exclu-\nsive set of event types. For Inference, the LLM is\ndirectly run on the test set to procure model pre-\ndictions. We report the F1 scores for two metrics\n(Ahn, 2006) for measuring model performance: (1)\nEvent Identification (Eve-I) - correct identification\nof events, and (2) Trigger Classification (Tri-C) -\ncorrect identification of trigger-event pairs.\nImplementation Details:\nWe follow STAR for\nthe implementation of the baseline models and the\nmajority of hyperparameter settings. For FIGâ€™s pas-\nsage generation, we select the top t = 10 triggers\n(except t = 8 for GENIA) for passage generation.\nWe generate N = 50 datapoints per event type\nfor each generation strategy. All our experimental\nresults are reported over an average of three runs.\nAdditional implementation details are provided in\nAppendix C.\n5\nResults\nWe present the results and insights for our zero-shot\nand few-shot settings below.\n5.1\nZero-shot Results\nWe present the main results comparing the various\nmethods across different LLMs and datasets in Ta-\nble 1. For FIG, we show results using both the\ntrain data as well as the external data as the exter-\nnal data source. We highlight some of our main\nfindings below.\nForward generation > Inverse generation:\nAl-\nthough STAR performs decently for ACE, its per-\nformances for SPEED and GENIA are quite poor\n(even below the Inference baseline). Since SPEED\n(social media) and GENIA (biomedical) are more\ndomain-specific than ACE (news), we attribute this\npoor performance to STARâ€™s inability to generate\ndomain-specific data instances (further validated\nin Â§ 6.3). On the other hand, Weak Sup outper-\nforms STAR, providing average gains of 13% Tri-\nC F1, thus establishing the superiority of forward\ngeneration-based synthetic data.\nFIG performs the best:\nOur hybrid approach\ncombines the benefits from both forward and in-\nverse generation and provides the best performance.\nOn average, FIG beats STAR by 17.3% Eve-I F1\nand 16.3% Tri-C F1 points - proving how domain-\n5\n\n0\n2\n5\n# Few-Shot Examples\n0\n20\n40\n60\nTri-C Model Performance\nACE\n0\n2\n5\n# Few-Shot Examples\n0\n10\n20\n30\n40\nSPEED\n0\n2\n5\n# Few-Shot Examples\n0\n10\n20\n30\nGENIA\nSupervised\nInference\nSTAR\nWeak Sup\nExGen\nFigure 4: Few-shot results comparing FIG with other baselines across three datasets using Llama3-8B-Instruct as\nthe base LLM. Except for Inference, all other evaluations are performances of downstream DEGREE (Hsu et al.,\n2022) model trained on data generated by each technique. Tri-C: Trigger Classification F1, #: Number of.\nspecific cues from unlabeled data can help generate\nbetter-aligned inverse generated data. Compared to\nforward generation Weak Sup baseline, FIG pro-\nvides average gains of 3.6% Eve-I F1 and 3.3% Tri-\nC F1 - suggesting how data diversity and cleaner\nlabel quality can help improve model performance.\nExternal data source is effective:\nAssuming ac-\ncess to the training data as the unlabeled data source\ncan be a strong assumption and bias for FIG. To\ncross-validate this assumptionâ€™s impact, we also\nconsider FIG with external data sources. Surpris-\ningly, as seen from Table 1, FIG with external data\nprovides similar gains of 4% Eve-I F1 and 3.4%\nTri-C F1 over the best baseline. This demonstrates\nthat instead of getting biased by external data, FIG\nutilizes the useful domain-specific signals (in the\nform of extracted triggers) from the external data\nand provides similar/better gains as with using the\ntraining data.\nGeneration ability doesnâ€™t scale up as reason-\ning ability:\nAlthough FIG performs better across\nall LLMs, the gains are much higher for Llama3-\n8B model relative to the Llama3-70B or GPT3.5\nmodels. Comparing the improvements in model\nperformance when scaling up from Llama3-8B to\nLLama3-70B, inverse generation-centric methods\n(STAR, FIG) improve by an average of 2.5% F1\npoints while reasoning-centric methods (Inference,\nWeak Sup) improve considerably better by an av-\nerage of 10.8% F1 points. A similar disparity in\nperformance improvements is observed for GPT3.5\nas well. This sheds light on how the generation\ncapabilities of LLMs do not scale compared to\nreasoning capabilities. Simultaneously, it shows\nthat FIG is particularly effective when used with\nsmaller LLMs whose the reasoning capability are\npoor.\n5.2\nFew-shot Results\nWe also study the various methods in the presence\nof small annotated data as part of our few-shot\nexperiments. Specifically, we study the k = 2\nand k = 5 few-shot settings, where k annotated\nexamples per event type are utilized. Majorly, we\nutilize the k shots as in-context examples in the\nLLM prompts where applicable and append these\nfew-shot examples to the synthesized training data\nas well. Additionally, we consider another baseline\n(Supervised) of downstream models trained only on\nthe k shot examples. We present the Tri-C results\nfor all the datasets for the Llama3-8B model in\nFigure 4 and summarize our major findings below.\nFIG consistently performs the best:\nSimilar to\nzero-shot results, we observe that FIG consistently\nbeats all other baseline models. On an average,\nFIG outperforms STAR and Weak Sup by 5.4%\nTri-C F1 and 7% Tri-C F1 respectively.\nGeneration\nabilities\nimprove\nsignificantly:\nContrary to our findings of small improvements\nwhen scaling up LLM model size in Â§ 5, we observe\nthat inverse generation-centric methods (STAR,\nFIG) consistently improve as we increase the num-\nber of shots. Contrastingly, reasoning-centric mod-\nels stagnate and donâ€™t improve as much. This high-\nlights how inverse generation becomes particularly\neffective in the presence of few exemplars.\n6\nAnalysis\nIn this section, we provide various analyses to study\nFIGâ€™s superior performance. Unless specified, we\nutilize Llama3-8B-Instruct as the base LLM for the\nanalyses.\n6\n\nMethod\nACE\nSPEED\nGENIA\nFIG\n50.2\n31.5\n28.9\nâ€“ Trigger Generation\n43.2\n27.8\n28.2\nâ€“ Data Refinement\n47.4\n23.3\n22.0\nTable 2: Ablation study for FIGâ€™s various components\nmeasured as Tri-C F1 performance across datasets.\nMethod\nACE\nSPEED\nGENIA\nEI\nTC\nEI\nTC\nEI\nTC\nFIG\n57.4 50.2 44.6 31.5 35.2 28.9\nWeak Sup + STAR 46.9 38.9 44.5 29.5 30.2 24.3\nTable 3: Comparing FIGâ€™s hybrid approach with data-\nmixing of forward and inverse generation.\n6.1\nAblation study\nTable 2 shows the ablation study for each of our\nforward generation components. We observe how\nforward generation is critical in both trigger gen-\neration and data refinement stages with average\nperformance reductions of 3.8% F1 and 6% F1 on\nremoving the respective components from FIG.\n6.2\nComparison with Data mixing\nData-mixing (Hoffmann et al., 2022; Xie et al.,\n2023) is a widely used technique for leveraging\ncomplementary information across datasets to pro-\nmote robust downstream model training. We mix\nforward (Weak Sup) and inverse generation (STAR)\nas a hybrid baseline to compare with FIG. To keep\ncomparisons fair, we consider N/2 data instances\nper event type from each dataset. Results from Ta-\nble 3 demonstrate how FIG beats the data-mixing\nbased hybrid model by 5-6% F1, highlighting the\nneed for explicit model design to combine the ben-\nefits of forward and inverse generation.\n6.3\nAnalyzing trigger quality and drift\nED models have a strong tendency to over-rely on\nlexical relations between triggers and events (Tong\net al., 2022; Ma et al., 2024). Thus, we compare the\nsynthetic data triggers with the gold test triggers as\na raw study on the quality and drift of triggers in the\nsynthesized data. Specifically, we extract triggers\nper event type in both datasets and measure the\nhit rate of the synthesized triggers on the gold set,\nas reported in Table 4. As observed, the poor hit\nrate shows the poor overlap of STARâ€™s triggers\nwith the gold triggers, which is a primary reason\nfor its domain drift. Furthermore, the consistently\nstronger precision of FIG relative to Weak Sup\nMethod\nACE\nSPEED\nGENIA\nSTAR\n9.6%\n15.3%\n15.1%\nWeak Sup\n19.1%\n38.4%\n44.8%\nFIG\n23.2%\n49.4%\n52.5%\nTable 4: Reporting the hit rate of synthetic data triggers\nrelative to gold test triggers for the three methods.\nMethod\nNaturalness\nEvent\nAnnotation\nRelevance\nQuality\nSTAR\n3.1\n3.4\n3.1\nWeak Sup\n4.2\n-\n2.9\nFIG\n3.6\n4.0\n3.6\nTable 5: Human evaluation for sentence naturalness,\nrelevance of event in generated sentence, and the anno-\ntation quality. 1 = worst, 5 = best.\ndemonstrate FIGâ€™s better trigger annotation quality.\nTo further study the label quality and relevance,\nwe conduct a human evaluation. Specifically, a\nhuman expert ED annotator is tasked to score the\ngenerations (between 1-5) on the naturalness of\nthe sentence for the domain, the correct relevance\nof the event mentioned in the generated sentence,\nand the annotation quality (more details in Â§ D.3).\nWe provide the averaged scores across the three\ndatasets for 90 samples in Table 5. Weak Sup has\nhigh sentence quality but poor label annotations;\nwhile STAR suffers from poor event relevance indi-\ncating domain drift. Overall, FIG performs the best\nwith high annotation quality and event relevance.\n6.4\nDomain-adapted LLM Fine-tuning\nWe fine-tune the base LLM used for passage\ngeneration on the unlabeled target-domain data\nDâ€²\nT to better align the generated passages to the\ntarget domain.\nNaturally, this can be applied\nonly for smaller LLMs owing to fine-tuning costs.\nWe present the results of fine-tuning Llama3-8B-\nInstruct on the unlabeled train data in Table 7. On\naverage, we observe how target data fine-tuning\nimproves FIG by a slight 0.5-2% F1, suggesting\nthat target-domain passage generation may help,\nbut it is not a highly influencing factor to improve\ndownstream model performance.\n6.5\nQualitative analysis of generated data\nWe provide qualitative evidence for FIGâ€™s domain-\naligned triggers compared to STAR in Table 6\n(more examples in Table 16). Owing to lack of do-\nmain grounding in STAR, the resulting triggers of-\nten appear misaligned (e.g. asphyxiation for death\n7\n\nDataset\nEvent\nMethod\nTrigger\nSentence\nACE\nAttack\nSTAR\nraid\nAs the rebels embarked on a daring trek across the desert, they launched a\nsurprise raid on the heavily guarded fortress, catching the enemy off guard.\nFIG\nshooting\nAs the rival businessman signed the contract, a sudden shooting erupted outside,\ncausing chaos in the midst of the transaction.\nSPEED\nDeath\nSTAR\nasphyxiation\nThe hikerâ€™s life was tragically cut short as asphyxiation occurred after she\nbecame stuck in the narrow cave crevice.\nFIG\nkilled\nThe patientâ€™s feverish state was triggered when they tested positive for the virus,\nwhich ultimately led to their being killed by the rapidly spreading infection.\nGENIA\nBinding\nSTAR\nmerge\nThe regulatory proteinâ€™s ability to activate a specific region of the DNA triggers\nthe merge of two proteins, leading to the modification of gene expression.\nFIG\nbound\nDuring the phosphorylation of the enzyme, it bound to the DNA sequence,\ninitiating the transcription process.\nTable 6: Qualitative examples demonstrating STAR and FIGâ€™s trigger and sentence generation quality.\nMethod\nACE\nSPEED\nGENIA\nEI\nTC\nEI\nTC\nEI\nTC\nFIG\n57.4\n50.2\n44.6\n31.5\n35.2\n28.9\n+ SFT LLM\n55.2\n51.7\n46.9\n35.8\n36.7\n29.1\nTable 7: Measuring model performance improvement\nusing a LLM fine-tuned on unlabeled train data (SFT\nLLM) for FIG. EI: Event Identification F1, TC: Trigger\nClassification F1.\nevent related to pandemics). This misalignment\ncarries over to the generated sentences, further re-\nducing their quality and alignment. In contrast,\nFIGâ€™s triggers are better aligned to the target do-\nmain corpus, resulting in better quality data.\n7\nRelated Works\nLow-resource Event Detection\nEvent Detec-\ntion (ED) has been studied extensively (Sundheim,\n1992; Grishman and Sundheim, 1996), leading to\ndiverse datasets in news (Doddington et al., 2004;\nSong et al., 2015; Ellis et al., 2015), Wikipedia\n(Li et al., 2021; Pouran Ben Veyseh et al., 2022),\nand general domains (Wang et al., 2020; Parekh\net al., 2023), as well as niche areas like biomedical\n(Pyysalo et al., 2012; Kim et al., 2011, 2013), multi-\nmedia (Li et al., 2020), cybersecurity (Satyapanich\net al., 2020), epidemiology (Parekh et al., 2024b,c),\nand pharmacovigilance (Sun et al., 2022). To ad-\ndress the growing need for event detection across\nexpanding domains, better low-resource domain-\nspecific techniques are essential. Prior works have\nexplored transfer learning via Abstract Meaning\nRepresentation (Huang et al., 2018), Semantic Role\nLabeling (Zhang et al., 2021), and Question An-\nswering (Lyu et al., 2021). Reformulating ED as\na conditional generation task has also aided low-\nresource training (Hsu et al., 2022, 2023; Huang\net al., 2022). Recently, LLM-based reasoning (Li\net al., 2023a; Gao et al., 2023; Wang et al., 2023b)\nand transfer-learning (Cai et al., 2024) has been\nexplored for low-resource ED and transfer learn-\ning, but performance remains inferior to supervised\nmodels (Huang et al., 2024). This motivates efforts\nin synthetic data generation for low-resource ED.\nData Generation for Information Extraction\nLLM-powered synthetic data generation has been\nsuccessful for various NLP tasks (Li et al., 2023b;\nWang et al., 2023c; Wu et al., 2024; Shao et al.,\n2025). For information extraction, works have ex-\nplored knowledge retrieval (Chen and Feng, 2023;\nAmalvy et al., 2023), translation (Parekh et al.,\n2024a; Le et al., 2024), and data re-editing (Lee\net al., 2021; Hu et al., 2023). Some works in the\ndirections we focus in our work include forward\ngeneration (Chia et al., 2022; Ye et al., 2022; Wang\net al., 2023a; Tang et al., 2023) and inverse gener-\nation (Josifoski et al., 2023; Ma et al., 2024). We\nimprove on existing forward and inverse generation\nworks via our hybrid generation approach FIG.\n8\nConclusion and Future Work\nWe introduce FIG, a hybrid forward-inverse gen-\neration approach for better domain-specific syn-\nthetic data in low-resource ED. Experiments on\nthree diverse datasets reveal that forward genera-\ntion suffers from noisy labels due to poor LLM\nreasoning, while inverse generation faces domain\ndrift. FIG mitigates these issues, achieving supe-\nrior domain alignment and cleaner data, leading to\nthe best downstream ED performance in zero and\nfew-shot settings. Future works can explore en-\nhancing stronger domain alignment and extending\nFIG for multilingual generation.\n8\n\nAcknowledgments\nWe express our gratitude to Anh Mac, Po-Nien\nKung, and Christina Chance for their valuable time,\nreviews of our work, and constructive feedback.\nLimitations\nWe consider only Event Detection (ED) as the main\ntask for data generation, but our method can be\nextended to other structured prediction tasks as\nwell. We leave this exploration for future works.\nWe consider three specialized domains of news,\nsocial media, and biomedical to provide a proof-of-\nconcept of our work. There are other specialized\ndomains for ED as well which can be explored as\npart of future work. Finally, our proposed method\nFIG makes a practical assumption of access to\nunlabeled data to procure target domain cues to\nguide the data generation. However, for specific\nsuper-specialized domains or if data has privacy\nconcerns, this may not be possible and our method\nmay not be applicable here. We assume such cases\nto be super rare and beyond the scope of our work.\nEthical Considerations\nThe theme of our work is to generate high-quality\ndomain-specific data using Large Language Mod-\nels (LLMs) using forward-inverse generations. The\ninherent LLMs can have certain biases, which can\nlead to potentially harmful or biased generations.\nFurthermore, the LLM can introduce potential hal-\nlucinations in the annotations which can hurt the\nmodel performance. We do not check or consider\nany bias/hallucination detection method as part of\nour work, as it is beyond the scope. So future users\nshould take due consideration of this vulnerability.\nOur proposed method FIG utilizes unlabeled\ndata as a basis to procure domain-specific cues. If\nthere are any biases in this data, it can propogate\nto the downstream model as well. We provide a\nproof-of-concept about our method in this work but\ndo not detect or rectify such biases.\nSince the inverse generation paradigm causes the\nLLM to generate sentences/passages, which can\npotentially be copied from the pre-training data the\nLLM has been trained on. This can potentially lead\nto copyright infringements and we do not consider\nany of such violations under consideration for our\nmethod. Users should consider this vulnerability\nbefore usage in commercial applications.\nWe would also like to mention and acknowledge\nthat we have utilized AI chatbots to help with the\nwriting of the work.\nReferences\nDavid Ahn. 2006. The stages of event extraction. In\nProceedings of the Workshop on Annotating and Rea-\nsoning about Time and Events, pages 1â€“8, Sydney,\nAustralia. Association for Computational Linguistics.\nArthur Amalvy, Vincent Labatut, and Richard Dufour.\n2023. Learning to rank context for named entity\nrecognition using a synthetic dataset. In Proceedings\nof the 2023 Conference on Empirical Methods in\nNatural Language Processing, pages 10372â€“10382,\nSingapore. Association for Computational Linguis-\ntics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. CoRR,\nabs/2005.14165.\nZefan Cai, Po-Nien Kung, Ashima Suvarna, Mingyu\nMa, Hritik Bansal, Baobao Chang, P. Jeffrey Brant-\ningham, Wei Wang, and Nanyun Peng. 2024. Improv-\ning event definition following for zero-shot event de-\ntection. In Proceedings of the 62nd Annual Meeting\nof the Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 2842â€“2863, Bangkok,\nThailand. Association for Computational Linguistics.\nFeng Chen and Yujian Feng. 2023. Chain-of-thought\nprompt distillation for multimodal named entity\nand multimodal relation extraction. arXiv preprint\narXiv:2306.14122.\nYew Ken Chia, Lidong Bing, Soujanya Poria, and Luo\nSi. 2022. RelationPrompt: Leveraging prompts to\ngenerate synthetic data for zero-shot relation triplet\nextraction. In Findings of the Association for Compu-\ntational Linguistics: ACL 2022, pages 45â€“57, Dublin,\nIreland. Association for Computational Linguistics.\nGeorge Doddington, Alexis Mitchell, Mark Przybocki,\nLance Ramshaw, Stephanie Strassel, and Ralph\nWeischedel. 2004.\nThe automatic content extrac-\ntion (ACE) program â€“ tasks, data, and evaluation. In\nProceedings of the Fourth International Conference\non Language Resources and Evaluation (LRECâ€˜04),\nLisbon, Portugal. European Language Resources As-\nsociation (ELRA).\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang,\n9\n\nArchi Mitra, Archie Sravankumar, Artem Korenev,\nArthur Hinsvark, Arun Rao, Aston Zhang, AurÃ©lien\nRodriguez, Austen Gregerson, Ava Spataru, Bap-\ntiste RoziÃ¨re, Bethany Biron, Binh Tang, Bobbie\nChern, Charlotte Caucheteux, Chaya Nayak, Chloe\nBi, Chris Marra, Chris McConnell, Christian Keller,\nChristophe Touret, Chunyang Wu, Corinne Wong,\nCristian Canton Ferrer, Cyrus Nikolaidis, Damien Al-\nlonsius, Daniel Song, Danielle Pintz, Danny Livshits,\nDavid Esiobu, Dhruv Choudhary, Dhruv Mahajan,\nDiego Garcia-Olano, Diego Perino, Dieuwke Hupkes,\nEgor Lakomkin, Ehab AlBadawy, Elina Lobanova,\nEmily Dinan, Eric Michael Smith, Filip Radenovic,\nFrank Zhang, Gabriel Synnaeve, Gabrielle Lee, Geor-\ngia Lewis Anderson, Graeme Nail, GrÃ©goire Mialon,\nGuan Pang, Guillem Cucurell, Hailey Nguyen, Han-\nnah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,\nImanol Arrieta Ibarra, Isabel M. Kloumann, Ishan\nMisra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan\nGeffert, Jana Vranes, Jason Park, Jay Mahadeokar,\nJeet Shah, Jelmer van der Linde, Jennifer Billock,\nJenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi,\nJianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu,\nJoanna Bitton, Joe Spisak, Jongsoo Park, Joseph\nRocca, Joshua Johnstun, Joshua Saxe, Junteng Jia,\nKalyan Vasuden Alwala, Kartikeya Upasani, Kate\nPlawiak, Ke Li, Kenneth Heafield, Kevin Stone, and\net al. 2024. The llama 3 herd of models. CoRR,\nabs/2407.21783.\nJoe Ellis, Jeremy Getman, Dana Fore, Neil Kuster, Zhiyi\nSong, Ann Bies, and Stephanie M. Strassel. 2015.\nOverview of linguistic resources for the TAC KBP\n2015 evaluations: Methodologies and results. In\nProceedings of the 2015 Text Analysis Conference,\nTAC 2015, Gaithersburg, Maryland, USA, November\n16-17, 2015, 2015. NIST.\nEnrico Francesconi, Simonetta Montemagni, Wim Pe-\nters, and Daniela Tiscornia, editors. 2010. Semantic\nProcessing of Legal Texts: Where the Language of\nLaw Meets the Law of Language, volume 6036 of\nLecture Notes in Computer Science. Springer.\nJun Gao, Huan Zhao, Changlong Yu, and Ruifeng Xu.\n2023. Exploring the feasibility of chatgpt for event\nextraction. CoRR, abs/2303.03836.\nRalph Grishman and Beth Sundheim. 1996. Message\nUnderstanding Conference- 6: A brief history. In\nCOLING 1996 Volume 1: The 16th International\nConference on Computational Linguistics.\nXuanli He, Islam Nassar, Jamie Kiros, Gholamreza\nHaffari, and Mohammad Norouzi. 2021.\nGener-\nate, annotate, and learn: Generative models ad-\nvance self-training and knowledge distillation. CoRR,\nabs/2106.06168.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland,\nKatherine Millican, George van den Driessche, Bog-\ndan Damoc, Aurelia Guy, Simon Osindero, Karen\nSimonyan, Erich Elsen, Oriol Vinyals, Jack W. Rae,\nand Laurent Sifre. 2022. An empirical analysis of\ncompute-optimal large language model training. In\nAdvances in Neural Information Processing Systems\n35: Annual Conference on Neural Information Pro-\ncessing Systems 2022, NeurIPS 2022, New Orleans,\nLA, USA, November 28 - December 9, 2022.\nI-Hung Hsu, Kuan-Hao Huang, Elizabeth Boschee,\nScott Miller, Prem Natarajan, Kai-Wei Chang, and\nNanyun Peng. 2022.\nDEGREE: A data-efficient\ngeneration-based event extraction model. In Pro-\nceedings of the 2022 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n1890â€“1908, Seattle, United States. Association for\nComputational Linguistics.\nI-Hung Hsu, Zhiyu Xie, Kuan-Hao Huang, Prem Natara-\njan, and Nanyun Peng. 2023. AMPERE: AMR-aware\nprefix for generation-based event argument extraction\nmodel. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 10976â€“10993, Toronto,\nCanada. Association for Computational Linguistics.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu\nChen. 2021. Lora: Low-rank adaptation of large\nlanguage models. CoRR, abs/2106.09685.\nXuming Hu, Yong Jiang, Aiwei Liu, Zhongqiang Huang,\nPengjun Xie, Fei Huang, Lijie Wen, and Philip S. Yu.\n2023. Entity-to-text based data augmentation for var-\nious named entity recognition tasks. In Findings of\nthe Association for Computational Linguistics: ACL\n2023, pages 9072â€“9087, Toronto, Canada. Associa-\ntion for Computational Linguistics.\nKuan-Hao Huang, I-Hung Hsu, Prem Natarajan, Kai-\nWei Chang, and Nanyun Peng. 2022.\nMultilin-\ngual generative language models for zero-shot cross-\nlingual event argument extraction. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 4633â€“4646, Dublin, Ireland. Association for\nComputational Linguistics.\nKuan-Hao Huang, I-Hung Hsu, Tanmay Parekh, Zhiyu\nXie, Zixuan Zhang, Prem Natarajan, Kai-Wei Chang,\nNanyun Peng, and Heng Ji. 2024. TextEE: Bench-\nmark, reevaluation, reflections, and future challenges\nin event extraction. In Findings of the Association\nfor Computational Linguistics: ACL 2024, pages\n12804â€“12825, Bangkok, Thailand. Association for\nComputational Linguistics.\nLifu Huang, Heng Ji, Kyunghyun Cho, Ido Dagan, Se-\nbastian Riedel, and Clare Voss. 2018.\nZero-shot\ntransfer learning for event extraction. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 2160â€“2170, Melbourne, Australia. Association\nfor Computational Linguistics.\n10\n\nMartin Josifoski, Marija Sakota, Maxime Peyrard, and\nRobert West. 2023. Exploiting asymmetry for syn-\nthetic training data generation: SynthIE and the case\nof information extraction. In Proceedings of the 2023\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1555â€“1574, Singapore. As-\nsociation for Computational Linguistics.\nJin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-\nnori Yonezawa. 2011. Overview of Genia event task\nin BioNLP shared task 2011.\nIn Proceedings of\nBioNLP Shared Task 2011 Workshop, pages 7â€“15,\nPortland, Oregon, USA. Association for Computa-\ntional Linguistics.\nJin-Dong Kim, Yue Wang, and Yamamoto Yasunori.\n2013. The Genia event extraction shared task, 2013\nedition - overview. In Proceedings of the BioNLP\nShared Task 2013 Workshop, pages 8â€“15, Sofia, Bul-\ngaria. Association for Computational Linguistics.\nVarun Kumar, Ashutosh Choudhary, and Eunah Cho.\n2020. Data augmentation using pre-trained trans-\nformer models. In Proceedings of the 2nd Workshop\non Life-long Learning for Spoken Language Systems,\npages 18â€“26, Suzhou, China. Association for Com-\nputational Linguistics.\nDuong Minh Le, Yang Chen, Alan Ritter, and Wei Xu.\n2024. Constrained decoding for cross-lingual label\nprojection. In The Twelfth International Conference\non Learning Representations, ICLR 2024, Vienna,\nAustria, May 7-11, 2024. OpenReview.net.\nKenton Lee, Kelvin Guu, Luheng He, Tim Dozat, and\nHyung Won Chung. 2021. Neural data augmentation\nvia example extrapolation. CoRR, abs/2102.01335.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871â€“7880, Online. Association for Computa-\ntional Linguistics.\nBo Li, Gexiang Fang, Yang Yang, Quansen Wang, Wei\nYe, Wen Zhao, and Shikun Zhang. 2023a. Evaluating\nchatgptâ€™s information extraction capabilities: An as-\nsessment of performance, explainability, calibration,\nand faithfulness. CoRR, abs/2304.11633.\nManling Li, Alireza Zareian, Qi Zeng, Spencer White-\nhead, Di Lu, Heng Ji, and Shih-Fu Chang. 2020.\nCross-media structured common space for multime-\ndia event extraction. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 2557â€“2568, Online. Association\nfor Computational Linguistics.\nSha Li, Heng Ji, and Jiawei Han. 2021. Document-level\nevent argument extraction by conditional generation.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 894â€“908, Online. Association for Computa-\ntional Linguistics.\nZhuoyan Li, Hangxiao Zhu, Zhuoran Lu, and Ming\nYin. 2023b. Synthetic data generation with large lan-\nguage models for text classification: Potential and\nlimitations. In Proceedings of the 2023 Conference\non Empirical Methods in Natural Language Process-\ning, pages 10443â€“10461, Singapore. Association for\nComputational Linguistics.\nRensis Likert. 1932. A technique for the measurement\nof attitudes. Archives of Psychology.\nQing Lyu, Hongming Zhang, Elior Sulem, and Dan\nRoth. 2021. Zero-shot event extraction via transfer\nlearning: Challenges and insights. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 2: Short Papers), pages 322â€“332, Online.\nAssociation for Computational Linguistics.\nMingyu Derek Ma, Xiaoxuan Wang, Po-Nien Kung,\nP. Jeffrey Brantingham, Nanyun Peng, and Wei Wang.\n2024.\nSTAR: boosting low-resource information\nextraction by structure-to-text data generation with\nlarge language models. In Thirty-Eighth AAAI Con-\nference on Artificial Intelligence, AAAI 2024, Thirty-\nSixth Conference on Innovative Applications of Ar-\ntificial Intelligence, IAAI 2024, Fourteenth Sympo-\nsium on Educational Advances in Artificial Intelli-\ngence, EAAI 2014, February 20-27, 2024, Vancouver,\nCanada, pages 18751â€“18759. AAAI Press.\nSourab Mangrulkar, Sylvain Gugger, Lysandre De-\nbut, Younes Belkada, Sayak Paul, and Benjamin\nBossan. 2022.\nPeft: State-of-the-art parameter-\nefficient fine-tuning methods.\nhttps://github.\ncom/huggingface/peft.\nYu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han.\n2022. Generating training data with language models:\nTowards zero-shot language understanding. CoRR,\nabs/2202.04538.\nMike Mintz, Steven Bills, Rion Snow, and Daniel Ju-\nrafsky. 2009. Distant supervision for relation ex-\ntraction without labeled data. In Proceedings of the\nJoint Conference of the 47th Annual Meeting of the\nACL and the 4th International Joint Conference on\nNatural Language Processing of the AFNLP, pages\n1003â€“1011, Suntec, Singapore. Association for Com-\nputational Linguistics.\nRishabh Misra. 2022. News category dataset. CoRR,\nabs/2209.11429.\nTanmay Parekh, I-Hung Hsu, Kuan-Hao Huang, Kai-\nWei Chang, and Nanyun Peng. 2023.\nGENEVA:\nBenchmarking generalizability for event argument\nextraction with hundreds of event types and argument\nroles. In Proceedings of the 61st Annual Meeting of\n11\n\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 3664â€“3686, Toronto,\nCanada. Association for Computational Linguistics.\nTanmay Parekh, I-Hung Hsu, Kuan-Hao Huang, Kai-\nWei Chang, and Nanyun Peng. 2024a. Contextual\nlabel projection for cross-lingual structured predic-\ntion. In Proceedings of the 2024 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies (Volume 1: Long Papers), pages 5738â€“5757,\nMexico City, Mexico. Association for Computational\nLinguistics.\nTanmay Parekh, Jeffrey Kwan, Jiarui Yu, Sparsh Johri,\nHyosang Ahn, Sreya Muppalla, Kai-Wei Chang, Wei\nWang, and Nanyun Peng. 2024b. SPEED++: A mul-\ntilingual event extraction framework for epidemic\nprediction and preparedness. In Proceedings of the\n2024 Conference on Empirical Methods in Natural\nLanguage Processing, pages 12936â€“12965, Miami,\nFlorida, USA. Association for Computational Lin-\nguistics.\nTanmay Parekh, Anh Mac, Jiarui Yu, Yuxuan Dong,\nSyed Shahriar, Bonnie Liu, Eric Yang, Kuan-Hao\nHuang, Wei Wang, Nanyun Peng, and Kai-Wei\nChang. 2024c. Event detection from social media\nfor epidemic prediction. In Proceedings of the 2024\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies (Volume 1: Long Papers),\npages 5758â€“5783, Mexico City, Mexico. Association\nfor Computational Linguistics.\nAmir Pouran Ben Veyseh, Javid Ebrahimi, Franck Der-\nnoncourt, and Thien Nguyen. 2022. MEE: A novel\nmultilingual event extraction dataset. In Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 9603â€“9613,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nSampo Pyysalo, Tomoko Ohta, Makoto Miwa, Han-\nCheol Cho, Junichi Tsujii, and Sophia Ananiadou.\n2012. Event extraction across multiple levels of bio-\nlogical organization. Bioinform., 28(18):575â€“581.\nTaneeya Satyapanich, Francis Ferraro, and Tim Finin.\n2020. CASIE: extracting cybersecurity event infor-\nmation from text. In The Thirty-Fourth AAAI Con-\nference on Artificial Intelligence, AAAI 2020, The\nThirty-Second Innovative Applications of Artificial\nIntelligence Conference, IAAI 2020, The Tenth AAAI\nSymposium on Educational Advances in Artificial In-\ntelligence, EAAI 2020, New York, NY, USA, February\n7-12, 2020, pages 8749â€“8757. AAAI Press.\nTimo Schick and Hinrich SchÃ¼tze. 2021. Generating\ndatasets with pretrained language models. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing, pages 6943â€“\n6951, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nYunfan Shao, Linyang Li, Yichuan Ma, Peiji Li, Demin\nSong, Qinyuan Cheng, Shimin Li, Xiaonan Li,\nPengyu Wang, Qipeng Guo, Hang Yan, Xipeng Qiu,\nXuanjing Huang, and Dahua Lin. 2025. Case2Code:\nScalable synthetic data for code generation. In Pro-\nceedings of the 31st International Conference on\nComputational Linguistics, pages 11056â€“11069, Abu\nDhabi, UAE. Association for Computational Linguis-\ntics.\nZhiyi Song, Ann Bies, Stephanie Strassel, Tom Riese,\nJustin Mott, Joe Ellis, Jonathan Wright, Seth Kulick,\nNeville Ryant, and Xiaoyi Ma. 2015. From light\nto rich ERE: Annotation of entities, relations, and\nevents.\nIn Proceedings of the 3rd Workshop on\nEVENTS: Definition, Detection, Coreference, and\nRepresentation, pages 89â€“98, Denver, Colorado. As-\nsociation for Computational Linguistics.\nZhaoyue Sun, Jiazheng Li, Gabriele Pergola, Byron\nWallace, Bino John, Nigel Greene, Joseph Kim, and\nYulan He. 2022. PHEE: A dataset for pharmacovigi-\nlance event extraction from text. In Proceedings of\nthe 2022 Conference on Empirical Methods in Nat-\nural Language Processing, pages 5571â€“5587, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nBeth M. Sundheim. 1992. Overview of the fourth Mes-\nsage Understanding Evaluation and Conference. In\nFourth Message Understanding Conference (MUC-\n4): Proceedings of a Conference Held in McLean,\nVirginia, June 16-18, 1992.\nRuixiang Tang, Xiaotian Han, Xiaoqian Jiang, and Xia\nHu. 2023. Does synthetic data generation of llms\nhelp clinical text mining? CoRR, abs/2303.04360.\nMeiHan Tong, Bin Xu, Shuai Wang, Meihuan Han,\nYixin Cao, Jiangqi Zhu, Siyu Chen, Lei Hou, and\nJuanzi Li. 2022.\nDocEE: A large-scale and fine-\ngrained benchmark for document-level event extrac-\ntion. In Proceedings of the 2022 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, pages 3970â€“3982, Seattle, United States. Asso-\nciation for Computational Linguistics.\nLeandro von Werra, Younes Belkada, Lewis Tunstall,\nEdward Beeching, Tristan Thrush, Nathan Lambert,\nShengyi Huang, Kashif Rasul, and Quentin Gal-\nlouÃ©dec. 2020. Trl: Transformer reinforcement learn-\ning. https://github.com/huggingface/trl.\nQing Wang, Kang Zhou, Qiao Qiao, Yuepei Li, and\nQi Li. 2023a. Improving unsupervised relation ex-\ntraction by augmenting diverse sentence pairs. In\nProceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing, pages\n12136â€“12147, Singapore. Association for Compu-\ntational Linguistics.\nXiaozhi Wang, Ziqi Wang, Xu Han, Wangyi Jiang, Rong\nHan, Zhiyuan Liu, Juanzi Li, Peng Li, Yankai Lin,\nand Jie Zhou. 2020. MAVEN: A Massive General\n12\n\nDomain Event Detection Dataset. In Proceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages 1652â€“\n1671, Online. Association for Computational Linguis-\ntics.\nXingyao Wang,\nSha Li,\nand Heng Ji. 2023b.\nCode4Struct: Code generation for few-shot event\nstructure prediction. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 3640â€“\n3663, Toronto, Canada. Association for Computa-\ntional Linguistics.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A. Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2023c. Self-instruct: Aligning language\nmodels with self-generated instructions. In Proceed-\nings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 13484â€“13508, Toronto, Canada. Association\nfor Computational Linguistics.\nZirui Wang, Adams Wei Yu, Orhan Firat, and Yuan Cao.\n2021. Towards zero-label language learning. CoRR,\nabs/2109.09193.\nLikang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang,\nHongchao Gu, Tingjia Shen, Chuan Qin, Chen Zhu,\nHengshu Zhu, Qi Liu, Hui Xiong, and Enhong Chen.\n2024. A survey on large language models for recom-\nmendation. World Wide Web (WWW), 27(5):60.\nSang Michael Xie, Shibani Santurkar, Tengyu Ma, and\nPercy Liang. 2023. Data selection for language mod-\nels via importance resampling. In Advances in Neu-\nral Information Processing Systems 36: Annual Con-\nference on Neural Information Processing Systems\n2023, NeurIPS 2023, New Orleans, LA, USA, Decem-\nber 10 - 16, 2023.\nJiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao\nFeng, Zhiyong Wu, Tao Yu, and Lingpeng Kong.\n2022.\nZeroGen: Efficient zero-shot learning via\ndataset generation. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 11653â€“11669, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nHongming Zhang, Haoyu Wang, and Dan Roth. 2021.\nZero-shot Label-aware Event Trigger and Argu-\nment Classification. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 1331â€“1340, Online. Association for Computa-\ntional Linguistics.\nShi Zong, Ashutosh Baheti, Wei Xu, and Alan Ritter.\n2022. Extracting a knowledge base of COVID-19\nevents from social media. In Proceedings of the 29th\nInternational Conference on Computational Linguis-\ntics, pages 3810â€“3823, Gyeongju, Republic of Korea.\nInternational Committee on Computational Linguis-\ntics.\n13\n\nA\nAdditional details about FIG\nWe provide illustrations about the various prompts\nused in FIG. For target structure extraction forward\ngeneration, we consider a two-prompt approach.\nThe first prompt aims to identify if any events of\ninterest are mentioned in the text as illustrated in\nFigure 6. It comprises the task definition, full event\nontology with definitions, the task instructions, and\nthe query sentence. The second prompt identifies\nthe trigger corresponding to the event of interest,\nas illustrated in Figure 7. Here, we specify the\ntask definition, the event ontology details, and the\nquery text with the task instructions. To aid inverse\ngeneration for passage generation, we provide the\ntask definition, the event ontology with event defini-\ntions, and the query comprising the sampled target\nstructure. We illustrate this prompt in Figure 8. Fi-\nnally, we provide the simplified one-prompt setup\nfor forward generation utilized for data refinement\nin Figure 9.\nB\nAdditional Experimental Setup Details\nB.1\nData Statistics\nWe discuss details about our dataset in Â§ 4. Our\ntest target domain data includes the test data splits\nof (1) ACE (Doddington et al., 2004) in the news\ndomain, (2) SPEED (Parekh et al., 2024c) in the\nsocial media domain, and (3) GENIA (Kim et al.,\n2011) in the biomedical domain For unlabeled data,\nwe utilize the training data of each dataset as one\ndata source. For the other data source, we uti-\nlize data from external sources, specifically: (1)\nNews Category Dataset (HuffPost) (Misra, 2022)\ncomprising Huffpost news articles from 2012-2022\nfor ACE. We filter articles corresponding to politi-\ncal, financial, and business articles, (2) COVIDKB\n(Zong et al., 2022) mining tweets from the Twitter\nCOVID-19 Endpoint released in April 2020 as the\nexternal data source, (3) GENIA2013 dataset (Kim\net al., 2013) as the external data for GENIA. We\nprovide statistics about this data in Table 8.\nC\nImplementation Details\nHere, we provide detailed implementation details\nfor each component and models used in our work.\nWe run most of our experiments on NVIDIA RTX\nA6000/A100 machines with support for 8 GPUs,\nwhile for GPT3.5, we make API calls through Ope-\nnAI.\nData\n# Sents\n# Event\nAverage\nSource\nMentions\nLength\nTest Data\nACE - test\n832\n403\n22.9\nSPEED - test\n586\n672\n28.1\nGENIA - test\n2,151\n1,805\n29.7\nUnlabeled Train Data\nACE - train\n17,172\n-\n15.6\nSPEED - train\n1,601\n-\n33.5\nGENIA - train\n6,431\n-\n30.1\nUnlabeled External Data\nHuffPost\n43,350\n-\n17.4\nCOVIDKB\n7,311\n-\n30.6\nGENIA2013\n6,542\n-\n17.4\nTable 8: Data Statistics for the various test and unlabeled\ndatasets used in our work.\nC.1\nLLM-based Generation\nWe provide details about the various hyperparame-\nters for using LLMs in all the components of STAR\nand FIG. For Llama3-8B-Instruct and Llama3-70B-\nInstruct, we present the hyperparameters in Table 9;\nwhile Table 10 presents the hyperparameters for\nGPT3.5.\nBatch Size\n32\nTemperature\n0.6\nTop-p\n0.9\nMax Generation Length\n250\nTable 9: Hyperparameters for decoding using Llama3-\n8B/70B model.\nBase LLM\ngpt-3.5-turbo-0125\nTemperature\n1.0\nTop-p\n1.0\nMax Generation Length\n500\nTable 10: Hyperparameters for decoding using GPT3.5\nmodel.\nC.2\nFew-shot Implementation Details\nFor the few-shot setting, we can access additional k\ndatapoints per event type to aid better performance.\nFor LLM-based prompting, we simply add these ex-\namples in the prompt as in-context examples to help\nthe model do better reasoning/generation. For in-\nverse generation methods (STAR, FIG), we do not\nadd the k triggers to the extracted/generated trig-\nger list, as it leads to a drop in model performance.\nThis can be attributed to the presence of duplicate\ninformation as the trigger generation/extraction al-\n14\n\nready accounts for the k triggers. For passage gen-\neration/Weak Sup, we append the k datapoints to\nthe synthetically generated data to provide signals\nfrom the gold data.\nC.3\nDownstream Model Training\nWe choose DEGREE (Hsu et al., 2022) as our\ndownstream model for evaluation, a generation-\nbased prompting model that utilizes natural lan-\nguage templates. We implemented the DEGREE\nmodel under TextEE framework (Huang et al.,\n2024). Table 11 presents the primary hyperparame-\nters for this model.\nPre-trained LM\nBART-Large\nTraining Epochs\n25\nWarmup Epochs\n5\nTraining Batch Size\n32\nEval Batch Size\n32\nLearning Rate\n0.00001\nWeight Decay\n0.00001\nGradient Clipping\n5\nBeam Size\n1\nNegative Samples\n15\nMax Sequence Length\n250\nMax Output Length\n20\nTable 11: Hyperparameters for DEGREE model.\nC.4\nLLM Fine-tuning\nWe discuss domain-adapted passage generation\nthrough LLM fine-tuning in Â§ 6.4. Specifically, we\nconduct a low-rank finetuning (LoRA) (Hu et al.,\n2021) to reduce computational overhead to fine-\ntune Llama3-8B-Instruct. We implement LoRA\nusing the peft and trl packages (Mangrulkar\net al., 2022; von Werra et al., 2020). We choose\nthe task of causal language modeling (i.e. contin-\nual pre-training) to perform domain adaptation on\nunlabeled in-domain sentences. We utilize cross-\nentropy loss on the dev split of the unlabeled data\nto select the best model. We provide additional de-\ntails about the hyperparameters for this fine-tuning\nfor each dataset in Table 12 below.\nD\nAdditional analyses\nIn this section, we provide additional analyses to\nsupport our main experiments.\nD.1\nSTAR with domain-specific prompt\nA simple way to infuse domain specific informa-\ntion in inverse generation pipelines like STAR\nwould be to add domain-related information in\nthe prompts to the LLM. We experiment with two\nACE\nLora Rank\n32\nLora Alpha\n16\nLora Dropout\n0.1\nLearning Rate\n0.0001\nWeight Decay\n0.05\nTraining Batch Size\n32\nTraining Epochs\n3\nEval Steps\n20\nSPEED\nLora Rank\n32\nLora Alpha\n16\nLora Dropout\n0.1\nLearning Rate\n0.00008\nWeight Decay\n0.05\nTraining Batch Size\n32\nTraining Epochs\n10\nEval Steps\n20\nGENIA\nLora Rank\n32\nLora Alpha\n16\nLora Dropout\n0.1\nLearning Rate\n0.00008\nWeight Decay\n0.05\nTraining Batch Size\n32\nTraining Epochs\n6\nEval Steps\n20\nTable 12:\nHyperparameters for LoRA fine-tuning\nLlama3-8B-Instruct.\nMethod\nACE\nSPEED\nGENIA\nEI\nTC\nEI\nTC\nEI\nTC\nSTAR\n44.9\n35.0\n21.0\n10.1\n25.9\n19.0\n+ mention\n44.1\n32.9\n17.1\n10.3\n28.7\n20.4\n+ references\n35.5\n27.3\n19.0\n9.2\n25.8\n18.1\nTable 13: Measuring model performance improvement\nproviding domain-specific cues in the form of domain-\nmention (mention) or domain sentence references (ref-\nerences) to the LLM for STAR. EI: Event Identification\nF1, TC: Trigger Classification F1.\nsuch methods: (1) domain-mention, where we pro-\nvide the target domain information in the prompt\nand ask the model to generate accordingly, and (2)\ndomain-reference, where we use some examples\nfrom the unlabeled data in the prompt as reference\nsentences to better guide the passage generation.\nWe provide results for these explorations using the\nLlama3-8B-Insturct model in Table 13. As ob-\nserved, the results are generally poor, with an aver-\nage drop of 0.1-0.6% F1 for domain-mention and\n3.1-3.8% F1 for domain-reference. This is majorly\nbecause LLMs over-compensate, producing longer\nand more stereotypical information in their gener-\nations which hurts the naturalness of the sentence\nand causes further domain drift. Furthermore, the\n15\n\n10\n25\n50\n100\n# Generated Data\n47.5\n50.0\n52.5\n55.0\n57.5\nModel Performance\nACE\n10\n25\n50\n100\n# Generated Data\n30\n40\nSPEED\n10\n25\n50\n100\n# Generated Data\n25\n30\n35\nGENIA\nEve-C\nTri-C\nFigure 5: Model performance for FIG as keep change the number of generated datapoints N using Llama3-8B-\nInstruct for the three datasets.\nFigure 6: Prompt for stage 1 of forward generation for\ntarget trigger extraction.\nLLM makes more errors in mentioning the event\nas a part of its reasoning, which is utilized to make\nthe generation in the domain style. We provide\nsome qualitative examples for such generations in\nTable 14. In some ways, it also puts into light and\namplifies the gains obtained by doing target domain\nSFT for FIG as discussed in Â§ 6.4.\nD.2\nImpact of different number of training\nsamples\nWe conduct a small analysis to study the impact\nof changing the number of generated samples on\nthe downstream model performance for FIG. We\npresent the results for Llama3-8B-Instruct in Fig-\nure 5. As observed, the performance continues to\nincrease as we increase the data from N = 10 to\nN = 100 datapoints per event type. This promises\nFigure 7: Prompt for stage 2 of forward generation for\ntarget trigger extraction.\nthat inverse generation will provide continued im-\nprovements by having larger control over data dis-\ntribution.\nD.3\nHuman Evaluation Details\nWe conduct a small human evaluation to judge the\nquality of the synthetic data in Â§ 6.3. Here, we\nprovide additional details about the human study\nand evaluation. Since the evaluation is conducted\non three diverse and niche domains, we only utilize\na single human annotator who is an ED expert and\nhas previously worked on all three datasets as the\nprimary annotator.\nWe majorly evaluate on three dimensions: (1)\nSentence naturalness (SN): this metric judges\n16\n\nACE\nA 35-year-old cyclist was hit by a speeding car while\nriding to work, leaving her with severe injuries, while in\na separate incident, a local retail giant filed a petition to\nrestructure its debt, sparking concerns about its financial\nstability.\nAs the war on terror raged on, the Mujahideen Advisory\nCouncil distributed a statement inviting Arab and foreign\nmedia reporters to enter Fallujah and cover the battles,\nwhile simultaneously, the ownership of the ancient arti-\nfacts was transferred to the museum, with the landlord\ndemanding rent on the premises.\nSPEED\nAs the influencerâ€™s viral challenge went viral, her follow-\ners were suddenly struck with a mysterious illness after\nthe splash of a contaminated drink, leading to a shocking\nexplosion of fatalities on social media.\nAs the community struggled to come to terms with the\ndevastating accident that had claimed the lives of several\nresidents, the authorities swiftly implemented a strict\nquarantine to prevent the spread of the infectious disease,\nhoping to mitigate the tragedy.\nGENIA\nThe specific transcription factor was elevated by the pres-\nence of the hormone, thereby increasing the expression\nof the target gene, while the inhibitory protein curbed the\nactivity of a competing transcription factor, preventing\nthe expression of a repressor gene.\nThe binding of PEBP2/CBF to the promoter region\nboosts the expression of the gene, which turns on the\nproduction of a crucial cytokine in response to the im-\nmune response.\nTable 14: Example passages of overly long and more\nstereotypical sentences generated when the domain is\nmentioned or references are added to the LLM prompt\nfor STAR.\nwhether the sentence seems grammatical, natural,\nand fits the domain of the target data. (2) Event\nRelevance (ER): this metric is computed only for\ninverse/hybrid generation methods. This evaluation\njudges whether the sampled event and trigger are\nappropriately used to generate a sensible alignment\nwith the target domain. Furthermore, it is verified\nif the right event definition is used. (3) Annotation\nQuality (AQ): this metric judges if the right trigger\nis used for each event mentioned in the synthetic\noutput. If there are any missing events, then this\nscore is penalized. For each metric, a score is given\non a Likert scale (Likert, 1932) from 1 (worst)\nto 5 (best). We also provide event definitions for\neach event in each dataset as a reference for better\njudgment. We illustrate the annotation interface in\nFigure 10 and provide some sample examples in\nTable 15.\nFigure 8: Prompt for inverse generation for passage\ngeneration.\nD.4\nAdditional Qualitative Examples\nIn Â§ 6.5, we discussed how FIG improves domain\ndrift qualitatively relative to STARand provided\nsome examples. Here, we provide more examples\nto further support that study in Table 16. This table\nfurther demonstrates how STAR can have a domain\ndrift without a lack of domain-specific cues, while\nFIG is better here.\n17\n\nFigure 9: Prompt for forward generation for data refine-\nment.\nSentence\nScore\nThe sudden crash of the ambulance sent\nshockwaves through the hospital as med-\nical staff rushed to the scene to monitor\nthe patientâ€™s life signs, but it was too\nlate, as the patient succumbed to the in-\nfectious disease.\nSN:\n2\nER:\n1\nAQ: 1\nThe wealthy entrepreneur transferred\nownership of the struggling tech com-\npany to her trusted business partner, re-\nlinquishing control and financial respon-\nsibility\nSN:\n5\nER:\n5\nAQ: 5\nTaken together, these data suggest that\nId1 could be a possible target gene for\nmediating the effects of BMP-6 in hu-\nman B cells, whereas Id2 and Id3 not\nseem to be involved.\nSN:\n4\nER:\n3\nAQ: 2\nTable 15: Illustration examples for the human evaluation\nmetrics. SN: sentence naturalness, ER: event relevance,\nAQ: annotation quality.\n18\n\nRate all the metrics from 1-5. Use the filters on top to group by dataset and assign the scores\nNaturalness = Is the snetence natural and grammatical?\nEvent Relevance = Based on event definitons (other sheet), figure if the event mentioned in this sentence seems correct\nAnnotation Quality = Check if all events are correctly annotated and there are no missing annotations.\nSentence\nAnnotation\nDataset\nNaturalness of Sentence\nEvent Relevance\nAnnotation Quality\nAs the riot police stormed the square, they were met with an\nassault, and in the chaos, a protester's clothes caught fire,\ncausing them to burn.\n[{'event': 'Conflict:Attack', 'trigger':\n'assault'}, {'event': 'Life:Injure', 'trigger':\n'burn'}]\nACE\nThe couple's marriage was annulled, ending their union after a\ntumultuous relationship.\n[{'event': 'Life:Divorce', 'trigger':\n'annulled'}]\nACE\nThe court's decision was reconsider by the higher court after\nthe losing party filed a petition to review the ruling.\n[{'event': 'Justice:Appeal', 'trigger':\n'reconsider'}]\nACE\nA dispute over a disputed contract led to a court proceeding\nbeing initiated, but the accused party was ultimately cleared of\nall charges.\n[{'event': 'Justice:Sue', 'trigger':\n'disputed'}, {'event': 'Justice:Sue',\n'trigger': 'dispute'}, {'event':\n'Justice:Acquit', 'trigger': 'cleared'}]\nACE\nThe manager dial the CEO to discuss the quarterly sales report\nand provide an update on the marketing strategy.\n[{'event': 'Contact:Phone-Write', 'trigger':\n'dial'}]\nACE\nThe entrepreneur's long-held vision was finally realized with the\nlaunch of her innovative startup, marking the beginning of a\nnew chapter in her professional journey.\n[{'event': 'Business:Start-Org', 'trigger':\n'launch'}]\nACE\nAs the company's financial struggles mounted, the CEO\nannounced the exit of the firm, while the truck driver shifted\ngears and hit the road, hauling away the last remaining assets.\n[{'event': 'Movement:Transport', 'trigger':\n'shifted'}, {'event': 'Business:End-Org',\n'trigger': 'exit'}]\nACE\nThe accused was exonerate by the court, clearing their name of\nall charges, after a lengthy trial, and later, the two former\ncolleagues met to chat and clear the air about their past\nmisunderstanding.\n[{'event': 'Justice:Acquit', 'trigger':\n'exonerate'}, {'event': 'Contact:Meet',\n'trigger': 'chat'}]\nACE\nThe police officer arrested the suspect, initiating a court\nproceeding to determine the liability of the accused for the\nalleged crime.\n[{'event': 'Justice:Sue', 'trigger':\n'arrested'}]\nACE\nThe candidate was chosen to lead the team after being\nselected by the majority of voters in the competitive election.\n[{'event': 'Personnel:Elect', 'trigger':\n'selected'}]\nACE\nFigure 10: Illustration of the interface for the human evaluation of the synthetically generated data. Short instructions\nare provided at the top. Each query comprises the sentence, annotation, and dataset. The human annotator is\nexpected to score 1-5 for each of the three metrics on the right.\n19\n\nDataset\nEvent\nMethod\nTrigger\nSentence\nACE\nAttack\nSTAR\nraid\nAs the rebels embarked on a daring trek across the desert, they launched a\nsurprise raid on the heavily guarded fortress, catching the enemy off guard.\nsiege\nThe government issued fines to the rebel group, while the city was under\nsiege, with gunfire erupting throughout the streets.\nslay\nThe wealthy businessman filed a complaint against his former partner, lead-\ning to a court proceeding, just as the rival gang leader slays his arch-nemesis\nin a brutal battle.\nFIG\nshooting\nAs the rival businessman signed the contract, a sudden shooting erupted\noutside, causing chaos in the midst of the transaction.\nbombing\nAs the city was rocked by a devastating bombing, thousands of protesters\ntook to the streets to demand justice and peace.\nfight\nThe rival gangs engaged in a brutal fight, sparking a wave of gunfire that left\nseveral people injured and buildings damaged.\nSPEED\nDeath\nSTAR\nasphyxiation\nThe hikerâ€™s life was tragically cut short as asphyxiation occurred after she\nbecame stuck in the narrow cave crevice.\noverdose\nA young musicianâ€™s overdose became the tragic finale of a life cut short in\nthe midst of chaos.\ndrowning\nAs she struggled to stay afloat, her heart stopped beating, and she succumbed\nto drowning in the icy waters.\nFIG\nkilled\nThe patientâ€™s feverish state was triggered when they tested positive for the\nvirus, which ultimately led to their being killed by the rapidly spreading\ninfection.\nlost\nAs the pandemic spread rapidly across the globe, thousands of people lost\ntheir lives due to the deadly virus.\ndied\nThe elderly man, who had been suffering from a severe case of tuberculosis,\ndied in his sleep.\nGENIA\nBinding\nSTAR\nmerge\nThe regulatory proteinâ€™s ability to activate a specific region of the DNA\nmolecule triggers the merge of two proteins, leading to the modification of\ngene expression.\nfuse\nWhen the proteins fuse together, the activity of the transcription factor is\ninhibited, preventing the gene expression from proceeding.\nsnap\nWhen the two proteins snap together, the binding of the complex inhibits the\nexpression of the target gene by deactivating a specific region of the DNA\nmolecule.\nFIG\nbound\nDuring the phosphorylation of the enzyme, it bound to the DNA sequence,\ninitiating the transcription process.\ntranslocation\nThe protein translocation to the nucleus triggers the induction of gene\nexpression.\nbinds\nWhen the enzyme binds to the substrate, it activates the addition of a phos-\nphate group to the target molecule, marking a crucial change in its function.\nTable 16: Comparison of generated triggers and sentences from STAR and FIG methods\n20\n",
  "metadata": {
    "source_path": "papers/arxiv/FIG_Forward-Inverse_Generation_for_Low-Resource_Domain-specific_Event\n__Detection_f94a017585be5c68.pdf",
    "content_hash": "f94a017585be5c68bd70b0b88436d6479334a9367a3e3b0b7c061f0087ab1d1f",
    "arxiv_id": null,
    "title": "FIG_Forward-Inverse_Generation_for_Low-Resource_Domain-specific_Event\n__Detection_f94a017585be5c68",
    "author": "",
    "creation_date": "D:20250225030659Z",
    "published": "2025-02-25T03:06:59",
    "pages": 20,
    "size": 933241,
    "file_mtime": 1740470087.7366374
  }
}