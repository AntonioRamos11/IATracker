{
  "text": "Capability Instruction Tuning: A New Paradigm for Dynamic LLM Routing\nYi-Kai Zhang, De-Chuan Zhan, Han-Jia Ye*\nSchool of Artificial Intelligence, Nanjing University\nNational Key Laboratory for Novel Software Technology, Nanjing University\n{zhangyk, zhandc, yehj}@lamda.nju.edu.cn\nAbstract\nLarge Language Models (LLMs) have demonstrated human-\nlike instruction-following abilities, particularly those exceed-\ning 100 billion parameters. The combined capability of some\nsmaller, resource-friendly LLMs can address most of the in-\nstructions that larger LLMs excel at. In this work, we ex-\nplore how to route the best-performing LLM for each instruc-\ntion to achieve better overall performance. We develop a new\nparadigm, constructing capability instructions with model\ncapability representation, user instruction, and performance\ninquiry prompts to assess the performance. To learn from capa-\nbility instructions, we introduce a new end-to-end framework\ncalled Model Selection with Aptitude Test (MODEL-SAT),\nwhich generates positive and negative samples based on what\ndifferent models perform well or struggle with. MODEL-SAT\nuses a model capability encoder that extends its model repre-\nsentation to a lightweight LLM. Our experiments show that\nMODEL-SAT understands the performance dimensions of can-\ndidate models and provides the probabilities of their capability\nto handle various instructions. Additionally, during deploy-\nment, a new model can quickly infer its aptitude test results\nacross 50 tasks, each with 20 shots. MODEL-SAT performs\nstate-of-the-art model routing without candidate inference and\nin real-world new model-released scenarios. The code is avail-\nable at https://github.com/Now-Join-Us/CIT-LLM-Routing.\nIntroduction\nLarge Language Models (LLMs) (OpenAI 2022; Du et al.\n2022; Touvron et al. 2023a; Chiang et al. 2023; Jiang et al.\n2023) rapidly evolve, demonstrating near-human general ca-\npabilities, especially in understanding, reasoning, and cre-\native tasks related to instruction-response scenarios. Recent\nadvancements have even enabled these LLMs to be trained\nin multilingual (Yang et al. 2024; Dubey et al. 2024), mul-\ntidomain (Zhou et al. 2024; Yang et al. 2024), and multi-\nmodal (Chen et al. 2015, 2023; Reid et al. 2024) environ-\nments, allowing them to tackle complex instructions such as\n“What is the relationship between Fourier series and Hilbert\nspace?” or to interpret images by identifying, “What are the\nbasis vectors of the Hilbert space?”\nThe rise of LLMs and their extensions has incredi-\nbly energized community applications. However, achiev-\n*Corresponding author.\nCopyright © 2025, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nPhi-1\n27%\n45%\n80%\n88%\n90%\n+ChatGLM2 +Zephyr\n+Yi\n+InternLM\n: GPT-4o’s incorrect\n: GPT-4o’s correct\n: Following 5 LLMs’\ncorrect instructions,\nrespectively\n: Test instruction\n(Phi-1+ChatGLM2+Zephyr) covers 80% of the GPT-4o’s correct instructions.\n2023/06\nRelease Time\n2023/06\n2023/08\n2023/11\n2024/01\nCoverage Ratio of GPT-4o Corrects\nFigure 1: Illustration of Coverage Observation: The com-\nbined capabilities of the earlier-released model zoo effec-\ntively address most of the instructions that GPT-4o excels at.\nThe union of samples managed accurately by Phi-1, Chat-\nGLM2, and Zephyr covers 80% of GPT-4o’s correct instruc-\ntions. The smaller-scale model zoo can enhance overall per-\nformance by selecting a suitable model for each instruction.\ning more comprehensive capabilities often requires LLMs\nof a larger scale. According to the Open LLM Leader-\nboard (Aidar Myrzakhan 2024), 60% of the top 50 LLMs\nhave around 70 billion (B) parameters or more, with only\nthree LLMs under 10B. Additionally, some closed-source\nLLMs consistently dominate performance rankings over ex-\ntended periods. Consequently, optimizing LLM applications\noften hinges on substantial computational resources or costly\ntoken purchases. A natural idea arises: Can we utilize multi-\nple smaller LLMs, which are more resource-friendly and have\nbelow one-tenth of the parameters of their larger counterparts,\nto achieve performance comparable to gigantic LLMs while\nmaintaining low inference costs?\nIn the experiments, we find that the combined capability of\narXiv:2502.17282v1  [cs.CL]  24 Feb 2025\n\nsome smaller-scale LLMs, despite their lower overall perfor-\nmance, can address most of the instructions that larger LLMs\nexcel at. As shown in Figure 1, on the Massive Multitask\nLanguage Understanding (MMLU) (Hendrycks et al. 2020)\nbenchmark, the Phi-1 LLM with 1.3B performs nearly 50%\nworse than GPT-4o. However, it exhibits similar effectiveness\nto GPT-4o in the high school mathematics category.\nMoreover, we create an early-access LLM zoo that includes\nPhi-1 (Gunasekar et al. 2023) and four 7B LLMs, which were\nreleased a year earlier than GPT-4o and exhibit an approxi-\nmately 30% performance gap compared to GPT-4o. However,\nthe combined accurate responses from this zoo cover 90%\nof which GPT-4o handles correctly and address nearly 80%\nwith which GPT-4o struggles. By strategically assigning in-\nstructions to the suitable LLM in the zoo, there is potential\nto exceed GPT-4o’s performance by 15%. From this phe-\nnomenon, the model routing for each instruction enhances\nperformance with seamless LLM transitions and minimal\ninference costs, all without user awareness.\nThe key to the proposed instruction-level model routing\nis to efficiently identify the optimal model from a vast pool\nof options, without prior access to the potential candidates’\ninference outputs (Tan et al. 2023; Xiao et al. 2023) or the\ntarget task’s ground truth (You et al. 2022; Pándy et al. 2022).\nIn this paper, we introduce MODEL-SAT: Model Selection\nwith Aptitude Test. Our approach leverages 50 core 20-shot\ntasks, where the model test result represents model capabil-\nity. By learning the generalization relationships between the\ncapability representations of the candidate models and the\ninstructions to be assigned, we can select the most suitable\nmodel across various repositories and target instructions.\nDriven by the model capability representation, the MODEL-\nSAT framework establishes a novel paradigm, denoted as ca-\npability instruction tuning. Capability instructions consist of\na capability representation, a user instruction, and a prompt to\nprobe whether the model can perform that instruction. Using\nextensive historical performance data, capability instruction\ntuning learns an implicit relationship between core capability\nrepresentations and unseen instructions. Moreover, it delves\ndeeper into understanding the mapping between the capabili-\nties’ performance and the instructions’ semantic distribution.\nThis intuition comes from the observation that individuals\nwho perform well in the mathematical sections of the college\nadmission SAT in the United States often pursue careers that\ninvolve logical reasoning. Capability instruction tuning aims\nto equip the model with a lightweight standardized guide to\nassess its effectiveness in handling future instructions.\nSpecifically, we combine model capability representation\nwith positive and negative training instructions regarding\ncurrent model performance, yielding statements like, “The\nmodel achieves accuracy 85% on the task of ’Mathematics,\nGeometry, ...’. Instruction: ..., Predict whether the model can\nhandle ...”. To align the performance distribution inherent in\nmodel representation to the instruction semantic, we are the\nfirst to incorporate a capability encoder and extend the input\nof a lightweight LLM to include capability representation.\nThe end-to-end MODEL-SAT functions as a model router\nthat outputs the probabilities indicating which models will\nlikely excel at specific instructions.\nAdditionally, we establish several comprehensive bench-\nmarks for model routing of LLMs and their extensions. Our\nbenchmarks cover a range of model zoos, such as (1) smaller-\nscale, weaker ones, (2) mixed-scale options, and (3) high-\nperformance larger-scale LLMs. Furthermore, we expand\nthe model routing to include multimodal LLM-instruction\nsettings. MODEL-SAT achieve significantly improved over-\nall performance across model zoos without incurring any\ninference overhead, comparable to the performance levels of\nlarger-scale LLMs. Notably, the capability instruction tuning\nmaintains the model representation generalization to unseen\ndata. The new LLM can quickly develop effective model rep-\nresentations after just a few inferences (only on 50 x 20-shot\ntasks). In light of practical routing scenarios with the emer-\ngence of new-version LLMs, we establish 60 incremental\nrouting scenarios that impose higher routing speed and over-\nhead requirements. Throughout these settings, MODEL-SAT\nconsistently demonstrates superior performance.\nIn summary, our contributions are:\n• A novel paradigm: capability instruction tuning,\nwhere model representation with efficient aptitude tests\nand instructions create capability instructions for high-\nperformance-driven instruction-level model routing.\n• MODEL-SAT framework, features a model capability\nencoder and a lightweight LLM to end-to-end learn the\nrouter via various model capability representations.\n• Comprehensive model routing benchmarks for LLMs\nand their extensions, covering five LLM zoo setups with\nmultimodal scenarios, as well as simulating 60 incremental-\nreleased model routers to ensure quick adaptation to unseen\ndata and new LLMs.\n• An open-source, deployable model routing toolkit that\napplies model routing techniques to any model zoo, en-\nhancing performance while remaining unaware of users\nwith acceptable routing delays.\nPreliminary\nWe begin by discussing the key elements and the pipeline of\nmodel selection, followed by the evolution of related works.\nInstruction, Output, and Answer\nConsider a test instruction dataset Dtest = {(xi, ai)}N\ni=1 with\nN labeled samples. The xi and ai represent the instruction\nand its corresponding answer, respectively. Given an LLM\nor its extension, represented as f, the output generated for\ninstruction xi is denoted as oi, i.e., f(xi) = oi. There are no\nrestrictions on the language, domain, or modality of xi; In\nthis paper, we focus on decoder-only text generation models,\nwhich means that ai is typically presented in text form. For\nthe model f to excel at instruction xi, it is equivalent to\nobtaining a high score on the evaluation eval (oi, ai).\nPipeline of Model Routing\nConsider a candidate model zoo composed of many trained\nLLMs, M = {f m}M\nm=1. Model routing involves selecting a\nmodel from the zoo for each instruction xi in the test dataset\nDtest. Specifically, the sequence of selected models is formal-\nized as f = (f1, f2, . . . , fN), where fi ∈M. We define\n\nIn a cage, there are a total of 10 heads and 28 legs.\nCalculate how many chickens        and rabbits        are?\n2 chickens and 8 rabbits.\n6 chickens and 4 rabbits.\n6 chickens and 8 rabbits.\nUser Instruction\nHow Do Re-ranking Based Methods Work\n•\nFirst Inference on All Models\n𝐱!\n𝐨!\n\"\n𝐨!\n#\n𝐨!\n$\nPredict Re-ranking Score\nCapability Representation\nfrom Aptitude Test\n×\n1\n=\n𝐱!\n75% on Psychology, 55% on …\n80% on Math, 35% on Histor…\n65% on Medicine, 65% on Bi…\n𝐨!\n\"\n𝐨!\n#\n𝐨!\n$\n0.61\n0.58\n0.52\nPrevious Method\nOur Implementation\n𝐜\"\n𝐜𝟐\n𝐜𝟑\n1\n=\n0.61\n0.76\n0.52\n𝐜\"\n𝐜𝟐\n𝐜𝟑\nFixed 𝐜!\n× 𝐱!\nOnline-generated 𝐨!\n\"\nSelect the\nPink one\nOnly let the selected model\nanswer the instruction!\n6 chickens and 4 rabbits.\nResponse\nPhi-1-1.3B\n+BLOOMZ-3B\n+Meta-Llama-3-8B\n+Phi-3-Medium-14B\n+Yi-1.5-Chat-34B\n+Meta-Llama-3-70B\nRe-ranking Based (BGE)\nMODEL-SAT\nOverhead of Selection (FLOPS)\nGrowth of the Candidate Model Library\n(b) Comparative Analysis of\nRe-ranking Based Method\nv.s. Our Deployment Cost:\nEfficiency Gains from\nCapability Representation.\n(a) Evaluation Comparison: Re-ranking Based Method v.s. Model Selection with Aptitude Test.\nFigure 2: Illustration of Model Routing with Capability Instructions: A Comparison with Re-ranking Based Methods. The\ngoal of model router is to select the optimal model for a given user instruction without access to ground truth and enhance overall\nperformance. Previous re-ranking methods require inference for each candidate. MODEL-SAT employs a lightweight aptitude\ntest to create capability representations. It learns the intrinsic relationship between model representations and the instructions to\nbe assigned, significantly speeding up model routing and streamlining deployment.\nthe optimal model ˆf for instruction xi as the model that\nmaximizes the score: eval\n\u0010\nˆf(xi), ai\n\u0011\n. The objective of the\ninstruction-level model routing is:\nˆf =\n \narg min\nf m∈M\nℓ(f m (xi) , ai)\n!N\ni=1\n,\n(1)\nwhere ℓ(·) represents the loss function associated with the\nmetrics between om\ni\n= f m(xi) and the ground truth ai.\nThe model routing bottleneck arises from the number of\ninstructions on which no model in the zoo performs well.\nRevisit from Requirement, Target, and Key Inputs\nRouting target of parameter initialization or models with\nzero-shot capabilities: Early model router (Tran, Nguyen,\nand Hassner 2019; Nguyen et al. 2020; Tan, Li, and Huang\n2021; Ding et al. 2022) efforts primarily focus on identify-\ning a good training initialization that facilitates fine-tuning\ndownstream tasks to achieve optimal performance. In this\ncontext, candidate models likely required additional training\nto adapt to the target task. Recently, guided by scaling laws,\nfoundational models like LLMs have experienced remark-\nable advancements in their zero-shot capabilities (Touvron\net al. 2023b; Wei et al. 2022; Team et al. 2023). Extended\nmodels have demonstrated considerable potential in mul-\ntilingual, multi-domain, and multimodal applications. For\ninstance, Llama 3.1 (Dubey et al. 2024) serves as a multilin-\ngual agent, Qwen2-Math (Yang et al. 2024) tackles several\nOlympiad-level problems, and GPT-4o (OpenAI 2023) pro-\ncesses information from multiple sources.\nRouting requirements with target instruction annota-\ntion, backpropagation delay, or candidate output: Some\nworks (Bao et al. 2019; Li et al. 2021; You et al. 2021; Desh-\npande et al. 2021; Pándy et al. 2022) design the proxy metric\nof transferability, which approximates the lower bound of\nfine-tuned performance. These works often rely on certain\nsource clues, labeled instructions, or backpropagation steps to\nassess the transferability from the source pre-trained model\nto the target dataset. Additionally, some re-ranking-based\nworks (Tan et al. 2023; Xiao et al. 2023; Zhang et al. 2024a)\ntrain an extra model to learn the contrastive relationships\nbetween the instruction and the candidate inference outputs\n{om\ni }M\nm=1, routing the optimal one linked to model f m. How-\never, obtaining all inferences may introduce significant delays\nwhen the number of models M in the repository becomes ex-\ncessively large (Shnitzer et al. 2023; Lu et al. 2023; Hu et al.\n2024). Our MODEL-SAT aims to route models without anno-\ntation or inference requirements, considering candidates as\nblack boxes. A central feature is constructing model represen-\ntations for each model and learning the adjusted relationship\nbetween it and the target instructions.\nKey input – model representation for model routing:\nWhen routing a model for instruction, the router requires\nthe key representation that captures the model’s character-\nistics. We followed the concept of learnware (Zhou 2016),\nleveraging a small amount of model-proficient data to con-\n\nCapability Instruction: c! + 𝒙\" + p\nCapability Instruction: The model achieve 75% on Psychology, 55% on Math, 35% on Medicine ……, “\n, How many\nand\n?”. Predict whether the model can handle 𝒙!\nUser Instruction 𝒙!: \"10 heads and 28 legs, How many\nand\n?\"\nCapability Representation 𝐜𝒎\n10 heads\n28 legs\nUser Instruction 𝒙𝒊\nPerformance Inquiry Prompt p\nFigure 3: One example of a Capability Instruction. It is an instruction for model routing that inquires whether a model can\nhandle a specific user instruction. It comprises three components: the capability representation cm based on the streamlined\naptitude test, the user instruction xi to be assigned, and a performance inquiry prompt p. This instruction is inputted into the\nMODEL-SAT Capability LLM, which outputs the probability that the model can perform the user instruction well.\nUser Instruction 𝒙𝒊Performance Inquiry Prompt p\nLLM 𝝋\nCapability\nEncoder 𝝍\nConnector\nCapability Representation 𝐜𝒎\nScore on ‘Yes’\nFigure 4: The Architecture of MODEL-SAT.\nstruct shared specifications (Zhou and Tan 2024; Tan et al.\n2024). Other relevant methods leverage forward behavior or\nresults on target as model representation, which inevitably\nintroduces inference delays. Recently, some approaches (Lu\net al. 2024; Srivatsa, Maurya, and Kochmar 2024; Ding et al.\n2024; Feng, Shen, and You 2024) have started to utilize\nlearnable parameters as model representations. For instance,\nsome introduce a surrogate scorer as the corresponding model\nrepresentation, learning the mapping from the task to the ac-\ncuracy of candidate model outputs. Model Spider (Zhang\net al. 2023b) takes this concept by encoding the model rep-\nresentation into a learnable vector, which acts as the input\ntoken for a Transformer-based router. However, learnable rep-\nresentation face challenges when new models are introduced,\nas they require extensive historical performance for costly\npre-training of the router. Our solution uses text-only descrip-\ntions of capabilities. New models can create representations\nby inferring 50 quick tasks, each with 20 shots.\nMODEL-SAT: Model Routing with Aptitude Test\nIn this section, we start by building the model representation\nand progress to the details of MODEL-SAT, training data,\nand optimization process. Finally, we outline an efficient\ndeployment framework for model routing.\nCapability Instructions\nThe capability instruction mainly comprises the capability\nrepresentation of the candidate model f m, user instruction xi,\nand performance inquiry prompt. Specifically, the model’s ca-\npability representation is formed from 50 distinct tasks across\nvarious categories from the MMLU dataset, with each task\nbeing 20-shot. We provide a concise description of five key-\nwords for each task. Next, we evaluate the candidate models\nacross these 50 tasks and describe the results in natural lan-\nguage, i.e., model representation. Furthermore, the advantage\nof representing in natural language is that it helps to include\nextra expert knowledge, such as mentioning which languages\na model supports. The easy-to-obtain representations serve\nas an aptitude test for the models, indicating their potential\ncapabilities across various dimensions.\nTo assess how well the candidate model can follow a single\nor a set of instructions xi, we introduce the training instruc-\ntions that were executed correctly versus those incorrectly.\nThese will be paired with the performance inquiry prompt\np to form the capability instruction, denoted as zi, which\ndrives the router to predict adaptation scores. As illustrated\nin Figure 3, it combines the capability representation cm for\ncandidate m, the instruction xi, and a inquiry prompt p.\nCore task sampling: We sample instructions of core tasks\nwith the highest distinguishability, avoiding those where most\nmodels perform correctly or incorrectly. In the model zoo of\ntraining, samples for which half of the models make mistakes\nwhile the other half are correct carry greater weight.\nArchitecture\nMotivation: Although LLMs demonstrate strong instruction-\nfollowing abilities, a gap exists between performance and the\nsemantic distribution in capability instructions, particularly\nin understanding combinations of performance dimensions.\nFor example, if a candidate model achieves 80% in mathe-\nmatics and 95% in legal principles, the model may possess\nlegal reasoning skills. To address this, we propose extending\na capability encoder E5-Large ~0.5B (ψ) before a Phi-3-Mini\n3.8B LLM (φ) to align the candidate performance with the\ninstructions. The architecture is illustrated in Figure 4.\nStructure: The capability instruction comprises the ca-\npability representation cm for model m, the instruction xi,\nand the query prompt p. We first align the model represen-\ntation, mapped by the capability encoder, into an embedded\nfeature of LLM inputs. This is achieved using a single-layer\nMLP, which acts as a connector to adapt the dimensions.\nConsequently, we derive the aligned model capability vector:\necm = W · ψ (cm) ,\n(2)\nwhere ecm is combined with the input embeddings of xi and\np to form the capability instruction zi, i.e.,\nzi = [ecm, exi, ep] = [e1, e2, . . . , es] ,\n(3)\nwhere s denotes the length of the concatenated capability\ninstruction sequence. Our alignment module operates at a nat-\nural language level, allowing for a streamlined design. In the\nfollowing Section , we also explore alternative approaches,\nincluding training without the alignment module.\nTuning Recipe\nForward Process of the Prediction Score: As shown in\nFigure 3, the query prompt p in the capability instruction\n\nincludes keywords related to positive terms that the model\nexcels at. For example, “Yes” serves as the key response\nin the prompt “predict whether the model can handle test\ninstruction by indicating ‘Yes’ or ‘No’.” In this context, the\nmodel routing prediction score is:\nPr (‘Yes’ | zi) =\ns\nY\nt=1\n1(|cm|,s] · φ (et | [e1, · · · , et−1]) ,\n(4)\nwhere we omit the input embedding layer for the LLM φ.\nPositive and Negative Instructions for Training: We ap-\nply Homogeneous In-Batch Negative Sampling (Karpukhin\net al. 2020; Zhang et al. 2023a) for each capability repre-\nsentation cm with its well-performed and poorly-performed\ninstructions to enhance the discriminative during training.\nTypically, a k-shot training batch Z = {zi}k\ni=0 contains 1\npositive instruction and k −1 negative ones.\nLoss Design: We denote the position of the positive instruc-\ntion in the training batch Z as ypos, and the remaining ones\nare k −1 negative instructions. Our objective is to enhance\nthe prediction score for the positive ones as the candidate per-\nforms better on this instruction. We employ the cross-entropy\nloss to optimize this in one batch Z:\nLCE = EZ∈Dtest [−log Pr (hφ,‘Yes’ (Z) = ypos | Z)] ,\n(5)\nwhere hφ,‘Yes’ (Z) = arg maxzi∈Z Pr (‘Yes’ | zi) is the\nLLM φ to identify which instruction zi ∈Z can be done\nwell (positive) and which cannot (negative).\nLearning Strategy: The model representation is derived\nfrom the capability distribution on MMLU. Similarly, we\ndevelop both in-domain and out-of-domain learning envi-\nronments for MODEL-SAT. In the first stage, we collect\nin-domain positive and negative training instructions, primar-\nily sourced from the same category as the MMLU dataset.\nWe only fine-tune the connector between the capability en-\ncoder ψ and the LLM φ, establishing an initial capability-\nto-instruction mapping. In the second stage, we fine-tune all\nmodel parameters. We apply a larger learning rate on the\nencoder and connector to enhance capability alignment with\ninstruction semantics.\nData Refinement: We further address noise in whether\nthe candidate model can accurately perform the instructions,\ninfluencing whether a capability instruction is a positive or\nnegative training sample. For those difficult instructions that\nonly a few models handle correctly, we implement a circle\ntest by rotating the sequence of options to prevent lucky\nguesses. Furthermore, we prioritize higher-ranked candidates\nin the training data by sampling with increased weight.\nEfficient Deployment\nMODEL-SAT provides the routing prediction for the candi-\ndate model applied to the target instruction. These scores are\ngenerated by the same model, rendering them comparable.\nIn this paper, we propose an open-source and comprehensive\nmodel routing toolkit, MODEL-SAT. This toolkit offers a\nviable solution for dynamic model routing within communi-\nties such as HuggingFace, harnessing the repository to boost\nperformance on target tasks.\nMODEL-SAT exhibits remarkable generalization capabili-\nties for unseen data, which can be directly concatenated into\nthe capability instruction. Similarly, the incremental exten-\nsion to new models proves highly efficient, requiring only\ninference on 50 core tasks for the model representation. As\nlater addressed in the experiments, MODEL-SAT exhibits\nzero-shot model routing abilities, facilitating the streamlined\ndevelopment of capability instructions in broader contexts.\nExperiments\nThis section begins by detailing the construction of training\nand test instructions in capability instructions tuning. It then\npresents different zoo setups for testing and concludes with\nan analysis of results and ablation studies.\nTraining and Test Instructions\nAs mentioned earlier, the capability instruction consists of\nmodel representation cm, instructions xi to assign, and per-\nformance inquiry prompts p.\nCandidate Model Representations cm for candidate m:\nWe introduced 66 open-source LLMs of varying scales. This\nincludes 60 models under 10B, 15 ones between 10B and\n20B, and 5 ones around 60B. We sample 50 categories from\nthe MMLU dataset, with 20 distinguishing instructions from\neach. Different candidate models share core tasks to ensure\nstability in capability demonstration.\nInstructions xi Pending to Assign: We consider more\nthan 20 datasets that include areas such as language, analysis,\nunderstanding, and reasoning in general evaluations, as well\nas specialized fields like mathematics and medicine. For each\ndataset, we sample sets of positive and negative instructions\nwhere the model performed well or poorly, with sampling\non stronger models assigned greater weight. Each dataset\ncontains about 100 instructions on average.\nPerformance Inquiry Prompts p: We explore different\napproaches for the probability of model routing. For capabil-\nity instructions, we design the performance inquiry prompt,\nsuch as “predict whether the model can handle test instruc-\ntion by indicating ‘Yes’ or ‘No’.” In this context, a response\nof ‘Yes’ signifies that the model is well-performed to the\ninstruction. We also experiment with integrating a regression\nlinear layer onto the next token embedding.\nThe capability instruction for the test zi similarly consists\nof the model representation cm, the target instruction xi to be\nassigned, and the performance inquiry prompt p. To ensure\ntest stability, we conduct a perturbation evaluation on model\nrepresentation. Specifically, we randomly alter the ranking of\nthe aptitude test results in capability representation twice and\nthen calculate the average routing scores Pr (‘Yes’ | zi). The\nresponse on this instruction xi is provided by the candidate\nmodel with the highest routing score.\nBenchmarks of LLM Routing\nIn this section, we outline benchmarks with various LLMs\nand their extension zoos, featuring detailed settings.\nSmaller-Scale LLM do Better: As demonstrated in the\nTable 1, the smaller-scale zoo contains InternLM2.5 (7.7B),\n\nMethod\n#Params\nIn-Domain\nOut-of-Domain\nMean\nMMLU\nWinoG.\nARC-C\nBoolQ\nTruthfulQAMRPC\nMNLI\nSmaller-scale LLMs (<10B)\nInternLM2.5 (Cai et al. 2024)\n7.7B\n69.88\n81.22\n60.75\n70.43\n54.56\n68.38\n60.68\n66.89\nMeta-Llama-3 Instruct (Touvron et al. 2023b)\n8.0B\n65.59\n75.45\n62.12\n76.76\n51.63\n68.38\n55.82\n65.62\nQwen2 Instruct (Yang et al. 2024)\n7.6B\n69.13\n74.11\n61.43\n82.57\n55.49\n78.92\n54.96\n68.19\nGLM-4 (Zeng et al. 2024)\n9.4B\n69.28\n80.82\n66.13\n84.77\n59.32\n78.92\n40.73\n68.65\nPhi-3 Small-128K (Abdin et al. 2024)\n7.4B\n75.90\n77.11\n71.08\n86.70\n64.62\n75.98\n46.82\n71.38\nBest-Performing among the five above\nxB\n75.90\n81.22\n71.08\n86.70\n64.62\n78.92\n60.68\n74.16\nLLM Selection on Smaller-scale LLMs (5 models)\nRandom Selection\nxB\n70.11\n77.66\n64.59\n79.94\n57.58\n72.79\n51.54\n67.74\nCappy (Tan et al. 2023)\nM·xB\n69.53\n78.06\n63.99\n81.10\n57.77\n74.75\n53.17\n68.34\nBGE Large (Xiao et al. 2023)\n(0.3+M)·xB\n71.68\n78.53\n66.38\n82.48\n61.44\n73.28\n55.39\n69.88\nGTE Large (Zhang et al. 2024a)\n(1.8+M)·xB\n72.02\n79.32\n68.09\n83.73\n59.61\n75.74\n56.16\n70.67\nMODEL-SAT (Ours)\nM·4.3B+xB\n79.86\n82.24\n72.53\n86.73\n65.12\n79.66\n60.83\n75.28\nLarger-Scale LLMs (10B~50B)\nPhi-3 Medium-128K (Abdin et al. 2024)\n14B\n76.63\n74.35\n66.49\n86.30\n54.54\n78.92\n59.42\n70.95\nYi-1.5 Chat (Young et al. 2024)\n34B\n77.15\n81.47\n70.62\n87.84\n62.02\n80.88\n61.56\n74.50\nMeta-Llama-3 Instruct (Touvron et al. 2023b)\n70B\n79.89\n82.62\n71.67\n93.61\n61.83\n83.58\n65.07\n76.90\nQwen2 Instruct (Yang et al. 2024)\n72B\n83.79\n84.41\n68.62\n94.90\n54.85\n84.31\n66.95\n76.83\nMixtral-8x22B Instruct-v0.1 (Jiang et al. 2024)\n140B\n77.63\n85.25\n72.68\n92.71\n68.19\n81.13\n67.70\n77.90\nCapability Instruction Tuning w/o Inference Overhead\nSmaller-Scale LLM Zoo\n79.86\n82.24\n72.53\n86.73\n65.12\n79.66\n60.83\n75.28\nSmaller-Mixed LLM Zoo\n78.60\n82.08\n72.01\n86.48\n64.50\n78.19\n60.72\n74.65\nOurs\nMiddle-Mixed LLM Zoo\nM·4.3B+xB\n79.97\n83.03\n72.69\n87.80\n64.87\n83.82\n61.80\n76.28\nLarger-Mixed LLM Zoo\n84.16\n86.27\n73.21\n93.94\n69.16\n85.54\n67.92\n80.03\nHigh-Performance LLM Zoo\n85.64\n87.85\n73.63\n95.02\n69.40\n88.24\n68.39\n81.17\nTable 1: A Comprehensive Performance Evaluation: Covering smaller-scale, high-performance giant LLMs, and a mixed\nLLM zoo of small, medium, and large levels. Model-SAT performs instruction-level model selection, consistently maintaining\nefficient and precise results that outperform the optimal one in the LLM zoo. Bold is the best, and underlined is the second-best.\nMethod\nMMLU\nARC-C\nTruthfulQA\nMean\nRandom Selection\n70.11\n64.59\n57.58\n64.09\nBest-Perfoming\n75.90\n71.08\n64.62\n70.53\nRoBERTa + MLP\n71.25\n64.33\n59.12\n64.90\n+ k Nearest Neighbors\n70.02\n65.02\n58.51\n64.52\n+ Random Forest\n73.75\n68.05\n61.44\n67.75\nPhi-3 Mini-128K\n74.71\n70.58\n61.70\n68.83\nMODEL-SAT\n79.86\n72.53\n65.12\n72.50\nTable 2: Performance Comparisons of Other Learning\nStrategies for Capability Instructions. The capability en-\ncoder of Model-SAT learns the mapping of performance to\nsemantics, demonstrating strong model selection abilities.\nMeta-Llama-3-Instruct (8.0B), Qwen2-Instruct (7.6B), GLM-\n4 (9.4B), and Phi-3-Small-128K (7.4B). The smaller-\nmixed zoo includes the smaller-scale zoo and Phi-1 (1.3B),\nBLOOMZ (3B), and Zephyr-Alpha (7.2B). These LLMs have\nfewer than 10B parameters and low deployment costs. In Fig-\nure 1, we show that the union of correct responses can cover\na set of instructions that only larger-scale ones can manage.\nGeneral LLM Zoo Settings. 1) Middle-Mixed and\nLarger-Mixed LLM Zoo: The middle-mixed zoo includes\nthe smaller-scale zoo and Phi-3-Medium-128K (14B), and\nYi-1.5-Chat (34B). The larger-mixed zoo consists of the\nmiddle-mixed ones, Meta-Llama-3-Instruct (70B), Qwen2-\nDataset\nPhi-3\nInternLM MiniCPM\nRandom\nMODEL-SAT\nVision\nXC2-VL\nLlama3-V\nSelection\n(Ours)\nMMMU VAL\n41.86\n41.06\n43.10\n42.19\n43.21\nAI2D TEST\n78.40\n79.44\n77.33\n78.24\n80.38\nCCBench\n37.60\n56.62\n57.16\n50.49\n56.96\nTable 3: Performance Comparisons in Selection of Multi-\nmodal LLM. Model-SAT maintains excellent average per-\nformance on the above three popular evaluation benchmarks.\nInstruct (72B), and Mixtral-8x22B-Instruct-v0.1 (140B). The\nmixed zoo can validate the routing method across different\ncapabilities. 2) High-Performance LLM Zoo: We select\nfrom larger-scale LLMs to boost performance further. The\nmodel zoo contains only three models above with over 70B\nparameters. 3) Multimodal LLM Zoo: To verify the gen-\nerality of capability instruction tuning, we construct a mul-\ntimodal LLM zoo that includes MiniCPM-Llama3-V 2.5,\nPhi-3-Vision-128k-Instruct, and InternLM-XComposer2-VL-\n7B.\nInstructions for Model Routing Evaluation: The test\ncapability instructions differ from the training ones of model\nrouters and consist of seven evaluation datasets. Datasets\nincluding MMLU (Hendrycks et al. 2021) (5-shot) and\nWinoGrande (Sakaguchi et al. 2020) (5-shot) cover a broad\nrange and are involved in the training part as the in-domain\n\n35\n40\n45\n50\n55\n60\n65\n70\n75\nBLOOM-3B\nBLOOM-7B1\nBLOOMZ-3B\nBLOOMZ-7B1\nChatGLM2-6B\nVicuna-7B-v1.5\nRedPajama-Chat-3B-v1\nBaichuan2-7B-Chat\nQwen-7B\nQwen-7B-Chat\nChatGLM3-6B\nChatGLM3-6B-Base\nPhi-1\nPhi-1_5\nMistral-7B-OpenOrca\nMistral-7B-sft-Alpha\nMistral-7B-sft-Beta\nZephyr-7B-Alpha\nYi-6B\nYi-6B-Chat\nZephyr-7B-Beta\nQwen-1.8B\nQwen-1.8B-Chat\nDeepSeek-LLM-7B-Base\nDeepSeek-LLM-7B-Chat\nInternLM-Chat-7B\nQwen1.5-0.5B\nQwen1.5-0.5B-Chat\nQwen1.5-1.8B\nQwen1.5-1.8B-Chat\nQwen1.5-4B\nQwen1.5-4B-Chat\nQwen1.5-7B\nQwen1.5-7B-Chat\nYi-9B\nInternLM2-1.8B\nInternLM2-7B\nInternLM2-Chat-1.8B\nInternLM2-Chat-7B\nInternLM2_5-7B\nInternLM2_5-7B-Chat\nZephyr-7B-Gemma-sft\nZephyr-7B-Gemma\nMeta-Llama-3-8B-Instruct\nPhi-3-Mini-128k-Instruct\nPhi-3-Mini-4k-Instruct\nPhi-3-Small-128k-Instruct\nPhi-3-Small-8k-Instruct\nYi-1.5-6B\nYi-1.5-6B-Chat\nYi-1.5-9B\nYi-1.5-9B-Chat\nQwen2-0.5B\nQwen2-0.5B-Instruct\nQwen2-1.5B\nQwen2-1.5B-Instruct\nQwen2-7B\nQwen2-7B-Instruct\nGLM-4-9B\nGLM-4-9B-Chat\nReal-world Model Selection with Latest LLM Release\nRandom Selection\nCurrent Best-Performing\nBGE-Reranker\nRoBERTa + MLP\nModel-SAT (Ours)\n62.12% Meta-Llama-3 (8B)\nAccuracy (in %)\nLLM (released over time)\n71.67% Meta-Llama-3 (70B)\n~74.57%\n71.08%\n~66.21%\n~63.99%\nFigure 5: Real-world Model Routing with Latest LLM Release on ARC-Challenge. MODEL-SAT (in red) quickly generalizes\nto unseen LLMs without extra training, maintaining robust performance despite dynamically adding diverse LLMs.\nevaluation. On the other hand, datasets such as ARC-\nChallenge (Bhakthavatsalam et al. 2021) (25-shot), Truth-\nfulQA (6-shot), and BoolQ (Clark et al. 2019) (1-shot) with\nMRPC (1-shot) and MNLI (1-shot) in GLUE (Wang et al.\n2019) benchmark focus on specific capabilities, serving as\nthe unseen out-of-domain evaluation of the model routers. We\nconsider the evaluation datasets MMMU-VAL (Hendrycks\net al. 2020), AI2D-TEST (Kembhavi et al. 2016), and\nCCBench (Liu et al. 2024) in the multimodal scenario.\nReal-world Model Routing with Unseen Datasets &\nLatest LLMs: 1) In Table 1, In-Domain and Out-of-Domain\nindicate whether the dataset is included in the training set\nfor LLM routing. 2) In Figure 5, We design a novel model\nrouting setting that, with the release of 60 LLMs, we update\nthe existing zoo after each new model with the top 5 histor-\nically best and the latest one. With the continual increment\nof unseen LLMs, this dynamic environment tests whether\nmethods can maintain a compelling performance.\nToward Comprehensive and Effective Routing\nPerformance Analysis in Various Model Zoos. Table 1\ndemonstrates that MODEL-SAT performs impressively\nacross five comprehensive LLM Zoos. (a) Smaller-Scale:\nrouting of LLMs under 10B achieve performance comparable\nto the ~70B LLMs. MODEL-SAT’s average score of 75.28%\nclosely matches Meta-Llama-3-Instruct-70B’s 76.90%, and\noutperforms it on the ARC-C and TruthfulQA benchmarks.\nFurthermore, MODEL-SAT selects the optimal model for\neach instruction, surpassing the best-performing models in\nthe Smaller-Scale Zoo. (b) Smaller-Mixed: We add three ear-\nlier released, weaker, and smaller LLMs. MODEL-SAT main-\ntained stable performance, with a slight decrease of about 1%\ncompared to row (a), while performance on ARC-C, BoolQ,\nand MNLI benchmarks remained nearly identical. (c) Middle-\nMixed: Row (c) includes two medium-scale models (10B to\n70B), resulting in improved performance for MODEL-SAT\ncompared to row (a). Its average performance now closely\nmatches that of the 70B model. (d) Larger-Mixed: Incorporat-\ning three 70B models in row (d) showed that MODEL-SAT\nremains robust despite significant performance variances in\nthe LLM zoo, with improvements of nearly 5% on MMLU,\nBoolQ, and so on. (e) High-Performance: Row (e) features\na routing of only three 70B models, revealing that the capa-\nbilities of gigantic LLMs are further unleashed, achieving a\nstate-of-the-art score of 85.64% on MMLU and 73.63% on\nthe ARC-Challenge.\nComparative Analysis of Routing Delay. In Table 1,\nthe selected model parameter-scale is denoted as xB. The\noverhead associated with the re-ranking method is related\nto the M candidates of xB. Figure 2 illustrates that the re-\nranking requires obtaining all inference results from the zoo\nfirst, while MODEL-SAT processes model representations\nall at once and utilizes them throughout its lifetime. As the\nscale of models in the zoo grows, MODEL-SAT’s routing\ncost remains unaffected by inference ones, ensuring efficient\nmodel routing.\nComparison Ranking-based Methods: We evaluate re-\nranking methods such as Cappy, BGE-Large, and GTE-Large.\nAlthough they have access to the outputs of each candidate,\nre-ranking often struggles to find optimal results, potentially\nbecause it primarily focuses on retrieving different semantics\nrather than optimizing performance across similar outputs.\nDetailed Comparative Analysis: Furthermore, we explore\nvarious learning strategies within the capability instruction\ntuning framework. Using features extracted from RoBERTa,\nwe train MLP (classification-based), k Nearest Neighbors\n(clustering-based), and Random Forest (tree-based). We ob-\nserved that, except for tree models that are suitable for han-\ndling capability representation as similar tabular data, other\nlearning strategies fail to capture performance distribution\nmappings. Additionally, we analyze MODEL-SAT without\nthe capability encoder. Since the model representation is ex-\npressed at the natural language level, Phi-3 in Table 2 can also\nlearn some LLM-Instruction mappings, but its performance\nremains inferior to that of capability-encoder-based ones.\nAblation Studies: Explore Generalization in Multi-\nmodal Scenarios. Most multimodal LLMs (MLLMs) are\nderived from input-extending LLMs. Multimodal MODEL-\nSAT is built on the Wings (Zhang et al. 2024b) training\narchitecture, integrating model representation embeddings\nwith visual ones. It maintains strong performance in mul-\ntimodal scenarios, achieving optimal average performance\nacross MMMU-VAL, AI2D-TEST, and CCBench datasets.\nConclusion\nThis paper proposes a novel model routing paradigm called\ncapability instruction tuning with instruction-level model\nrouting. It constructs a capability instruction comprising ca-\npabilities, instructions, and inquiry prompts to select the most\nsuitable one. We present MODEL-SAT, featuring a capabil-\nity encoder and lightweight LLM. It selects models without\ninference overhead of candidates and quickly adapts to new\nmodels. Model-SAT performs optimally across five proposed\nLLM routing settings and its multimodal extension.\n\nAcknowledgments\nThis work is partially supported by NSFC (62376118,\n62250069), Key Program of Jiangsu Science Foundation\n(BK20243012), Collaborative Innovation Center of Novel\nSoftware Technology and Industrialization.\nReferences\nAbdin, M. I.; Jacobs, S. A.; Awan, A. A.; et al. 2024. Phi-3\nTechnical Report: A Highly Capable Language Model Lo-\ncally on Your Phone. CoRR, abs/2404.14219.\nAidar Myrzakhan, Z. S., Sondos Mahmoud Bsharat. 2024.\nOpen-LLM-Leaderboard: From Multi-choice to Open-style\nQuestions for LLMs Evaluation, Benchmark, and Arena.\narXiv preprint arXiv:2406.07545.\nBao, Y.; Li, Y.; Huang, S.-L.; Zhang, L.; Zheng, L.; Zamir, A.;\nand Guibas, L. 2019. An Information-Theoretic Approach to\nTransferability in Task Transfer Learning. In ICIP.\nBhakthavatsalam, S.; Khashabi, D.; Khot, T.; et al. 2021.\nThink you have Solved Direct-Answer Question Answering?\nTry ARC-DA, the Direct-Answer AI2 Reasoning Challenge.\nCoRR, abs/2102.03315.\nCai, Z.; Cao, M.; Chen, H.; et al. 2024. Internlm2 technical\nreport. arXiv preprint arXiv:2403.17297.\nChen, L.; Li, J.; Dong, X.; Zhang, P.; He, C.; Wang, J.;\nZhao, F.; and Lin, D. 2023. Sharegpt4v: Improving large\nmulti-modal models with better captions. arXiv preprint\narXiv:2311.12793.\nChen, X.; Fang, H.; Lin, T.-Y.; Vedantam, R.; Gupta, S.; Dol-\nlar, P.; and Zitnick, C. L. 2015. Microsoft COCO Captions:\nData Collection and Evaluation Server. arXiv:1504.00325.\nChiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y.; Wu, Z.; Zhang, H.;\nZheng, L.; Zhuang, S.; Zhuang, Y.; Gonzalez, J. E.; Stoica,\nI.; and Xing, E. P. 2023. Vicuna: An Open-Source Chatbot\nImpressing GPT-4 with 90%* ChatGPT Quality.\nClark, C.; Lee, K.; Chang, M.; Kwiatkowski, T.; Collins, M.;\nand Toutanova, K. 2019. BoolQ: Exploring the Surprising\nDifficulty of Natural Yes/No Questions. In NAACL-HLT.\nDeshpande, A.; Achille, A.; Ravichandran, A.; Li, H.; Zan-\ncato, L.; Fowlkes, C.; Bhotika, R.; Soatto, S.; and Perona,\nP. 2021. A linearized framework and a new benchmark for\nmodel selection for fine-tuning. CoRR, abs/2102.00084.\nDing, D.; Mallick, A.; Wang, C.; Sim, R.; Mukherjee, S.;\nRuhle, V.; Lakshmanan, L. V.; and Awadallah, A. H. 2024.\nHybrid LLM: Cost-Efficient and Quality-Aware Query Rout-\ning. arXiv preprint arXiv:2404.14618.\nDing, N.; Chen, X.; Levinboim, T.; Changpinyo, S.; and Sori-\ncut, R. 2022. PACTran: PAC-Bayesian Metrics for Estimat-\ning the Transferability of Pretrained Models to Classification\nTasks. In ECCV.\nDu, Z.; Qian, Y.; Liu, X.; Ding, M.; Qiu, J.; Yang, Z.; and\nTang, J. 2022. GLM: General Language Model Pretraining\nwith Autoregressive Blank Infilling. In ACL, 320–335.\nDubey, A.; Jauhri, A.; Pandey, A.; et al. 2024. The Llama 3\nHerd of Models.\nFeng, T.; Shen, Y.; and You, J. 2024. GraphRouter: A Graph-\nbased Router for LLM Selections. CoRR, abs/2410.03834.\nGunasekar, S.; Zhang, Y.; Aneja, J.; et al. 2023. Textbooks\nAre All You Need. CoRR, abs/2306.11644.\nHendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika,\nM.; Song, D.; and Steinhardt, J. 2020.\nMeasuring mas-\nsive multitask language understanding.\narXiv preprint\narXiv:2009.03300.\nHendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.;\nSong, D.; and Steinhardt, J. 2021. Measuring Massive Multi-\ntask Language Understanding. In ICLR.\nHu, Q. J.; Bieker, J.; Li, X.; Jiang, N.; Keigwin, B.;\nRanganath, G.; Keutzer, K.; and Upadhyay, S. K. 2024.\nROUTERBENCH: A Benchmark for Multi-LLM Routing\nSystem. arXiv preprint arXiv:2403.12031.\nJiang, A. Q.; Sablayrolles, A.; Mensch, A.; Bamford, C.;\nChaplot, D. S.; de las Casas, D.; Bressand, F.; Lengyel, G.;\nLample, G.; Saulnier, L.; Lavaud, L. R.; Lachaux, M.-A.;\nStock, P.; Scao, T. L.; Lavril, T.; Wang, T.; Lacroix, T.; and\nSayed, W. E. 2023. Mistral 7B. arXiv:2310.06825.\nJiang, A. Q.; Sablayrolles, A.; Roux, A.; Mensch, A.; Savary,\nB.; Bamford, C.; Chaplot, D. S.; Casas, D. d. l.; Hanna, E. B.;\nBressand, F.; et al. 2024. Mixtral of experts. arXiv preprint\narXiv:2401.04088.\nKarpukhin, V.; Oguz, B.; Min, S.; Lewis, P. S. H.; Wu, L.;\nEdunov, S.; Chen, D.; and Yih, W. 2020. Dense Passage\nRetrieval for Open-Domain Question Answering. In EMNLP,\n6769–6781. Association for Computational Linguistics.\nKembhavi, A.; Salvato, M.; Kolve, E.; Seo, M.; Hajishirzi,\nH.; and Farhadi, A. 2016. A diagram is worth a dozen images.\nIn ECCV, 235–251.\nLi, Y.; Jia, X.; Sang, R.; Zhu, Y.; Green, B.; Wang, L.; and\nGong, B. 2021. Ranking neural checkpoints. In CVPR.\nLiu, Y.; Duan, H.; Zhang, Y.; Li, B.; Zhang, S.; Zhao, W.;\nYuan, Y.; Wang, J.; He, C.; Liu, Z.; Chen, K.; and Lin, D.\n2024. MMBench: Is Your Multi-modal Model an All-around\nPlayer?\nLu, K.; Yuan, H.; Lin, R.; Lin, J.; Yuan, Z.; Zhou, C.; and\nZhou, J. 2023.\nRouting to the expert: Efficient reward-\nguided ensemble of large language models. arXiv preprint\narXiv:2311.08692.\nLu, X.; Liusie, A.; Raina, V.; Zhang, Y.; and Beauchamp,\nW. 2024.\nBlending Is All You Need: Cheaper, Better\nAlternative to Trillion-Parameters LLM.\narXiv preprint\narXiv:2401.02994.\nNguyen, C. V.; Hassner, T.; Archambeau, C.; and Seeger, M.\n2020. LEEP: A New Measure to Evaluate Transferability of\nLearned Representations. In ICML.\nOpenAI. 2022. ChatGPT. https://openai.com/blog/chatgpt.\nOpenAI. 2023.\nGPT-4 Technical Report.\nCoRR,\nabs/2303.08774.\nPándy, M.; Agostinelli, A.; Uijlings, J. R. R.; Ferrari, V.; and\nMensink, T. 2022. Transferability Estimation using Bhat-\ntacharyya Class Separability. In CVPR.\nReid, M.; Savinov, N.; Teplyashin, D.; et al. 2024. Gemini\n1.5: Unlocking multimodal understanding across millions of\ntokens of context. arXiv preprint arXiv:2403.05530.\n\nSakaguchi, K.; Bras, R. L.; Bhagavatula, C.; and Choi, Y.\n2020. WinoGrande: An Adversarial Winograd Schema Chal-\nlenge at Scale. In AAAI, 8732–8740.\nShnitzer, T.; Ou, A.; Silva, M.; Soule, K.; Sun, Y.; Solomon,\nJ.; Thompson, N.; and Yurochkin, M. 2023. Large language\nmodel routing with benchmark datasets.\narXiv preprint\narXiv:2309.15789.\nSrivatsa, K.; Maurya, K. K.; and Kochmar, E. 2024. Har-\nnessing the Power of Multiple Minds: Lessons Learned from\nLLM Routing. arXiv preprint arXiv:2405.00467.\nTan, B.; Zhu, Y.; Liu, L.; Xing, E. P.; Hu, Z.; and Chen, J.\n2023. Cappy: Outperforming and Boosting Large Multi-Task\nLMs with a Small Scorer. CoRR, abs/2311.06720.\nTan, Y.; Li, Y.; and Huang, S.-L. 2021. OTCE: A Transfer-\nability Metric for Cross-Domain Cross-Task Representations.\nIn CVPR.\nTan, Z.; Liu, J.; Bi, X.; Tan, P.; Zheng, Q.; Liu, H.; Xie,\nY.; Zou, X.; Yu, Y.; and Zhou, Z. 2024.\nBeimingwu: A\nLearnware Dock System. In Baeza-Yates, R.; and Bonchi, F.,\neds., SIGKDD, 5773–5782. ACM.\nTeam, G.; Anil, R.; Borgeaud, S.; Wu, Y.; Alayrac, J.-B.; Yu,\nJ.; Soricut, R.; Schalkwyk, J.; Dai, A. M.; Hauth, A.; et al.\n2023. Gemini: a family of highly capable multimodal models.\narXiv preprint arXiv:2312.11805.\nTouvron,\nH.;\nMartin,\nL.;\nStone,\nK.;\net\nal.\n2023a.\nLlama 2: Open Foundation and Fine-Tuned Chat Models.\narXiv:2307.09288.\nTouvron, H.; Martin, L.; Stone, K.; et al. 2023b. Llama 2:\nOpen foundation and fine-tuned chat models. arXiv preprint\narXiv:2307.09288.\nTran, A. T.; Nguyen, C. V.; and Hassner, T. 2019. Transfer-\nability and hardness of supervised classification tasks. In\nICCV.\nWang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and\nBowman, S. R. 2019. GLUE: A Multi-Task Benchmark and\nAnalysis Platform for Natural Language Understanding. In\nICLR.\nWei, J.; Bosma, M.; Zhao, V.; Guu, K.; Yu, A. W.; Lester, B.;\nDu, N.; Dai, A. M.; and Le, Q. V. 2022. Finetuned Language\nModels are Zero-Shot Learners. In ICLR.\nXiao, S.; Liu, Z.; Zhang, P.; and Muennighoff, N. 2023. C-\nPack: Packaged Resources To Advance General Chinese Em-\nbedding. arXiv:2309.07597.\nYang, A.; Yang, B.; Hui, B.; et al. 2024. Qwen2 Technical\nReport. arXiv:2407.10671.\nYou, K.; Liu, Y.; Wang, J.; and Long, M. 2021. LogME: Prac-\ntical Assessment of Pre-trained Models for Transfer Learning.\nIn ICML.\nYou, K.; Liu, Y.; Zhang, Z.; Wang, J.; Jordan, M. I.; and Long,\nM. 2022. Ranking and Tuning Pre-trained Models: A New\nParadigm for Exploiting Model Hubs. Journal of Machine\nLearning Research, 23.\nYoung, A.; Chen, B.; Li, C.; et al. 2024. Yi: Open Foundation\nModels by 01.AI. CoRR, abs/2403.04652.\nZeng, A.; Xu, B.; Wang, B.; et al. 2024. ChatGLM: A Family\nof Large Language Models from GLM-130B to GLM-4 All\nTools. CoRR, abs/2406.12793.\nZhang, P.; Xiao, S.; Liu, Z.; Dou, Z.; and Nie, J. 2023a.\nRetrieve Anything To Augment Large Language Models.\nCoRR, abs/2310.07554.\nZhang, X.; Zhang, Y.; Long, D.; Xie, W.; Dai, Z.; Tang, J.;\nLin, H.; Yang, B.; Xie, P.; Huang, F.; et al. 2024a. mGTE:\nGeneralized Long-Context Text Representation and Rerank-\ning Models for Multilingual Text Retrieval. arXiv preprint\narXiv:2407.19669.\nZhang, Y.; Huang, T.; Ding, Y.; Zhan, D.; and Ye, H. 2023b.\nModel Spider: Learning to Rank Pre-Trained Models Effi-\nciently. In NeurIPS.\nZhang, Y.-K.; Lu, S.; Li, Y.; Ma, Y.; Chen, Q.-G.; Xu, Z.;\nLuo, W.; Zhang, K.; Zhan, D.-C.; and Ye, H.-J. 2024b. Wings:\nLearning Multimodal LLMs without Text-only Forgetting.\nZhou, Z. 2016. Learnware: on the future of machine learning.\nFrontiers Comput. Sci., 10(4): 589–590.\nZhou, Z.; Shi, J.; Song, P.; Yang, X.; Jin, Y.; Guo, L.; and Li,\nY. 2024. LawGPT: A Chinese Legal Knowledge-Enhanced\nLarge Language Model. CoRR, abs/2406.04614.\nZhou, Z.; and Tan, Z. 2024. Learnware: small models do big.\nSci. China Inf. Sci., 67(1).\n",
  "metadata": {
    "source_path": "papers/arxiv/Capability_Instruction_Tuning_A_New_Paradigm_for_Dynamic_LLM_Routing_7322f6c19279d2e2.pdf",
    "content_hash": "7322f6c19279d2e2736ecc65bfa9e6b35d864668822cffc8857dc8a4c7830772",
    "arxiv_id": null,
    "title": "Capability_Instruction_Tuning_A_New_Paradigm_for_Dynamic_LLM_Routing_7322f6c19279d2e2",
    "author": "",
    "creation_date": "D:20250225025815Z",
    "published": "2025-02-25T02:58:15",
    "pages": 9,
    "size": 1156410,
    "file_mtime": 1740470165.19779
  }
}