{
  "text": "arXiv:2502.16912v1  [cs.CC]  24 Feb 2025\nWhen Can We Solve the Weighted Low Rank Approximation\nProblem in Truly Subquadratic Time?\nChenyang Li∗\nYingyu Liang†\nZhenmei Shi‡\nZhao Song§\nAbstract\nThe weighted low-rank approximation problem is a fundamental numerical linear algebra\nproblem and has many applications in machine learning. Given a n × n weight matrix W and\na n × n matrix A, the goal is to ﬁnd two low-rank matrices U, V ∈Rn×k such that the cost\nof ∥W ◦(UV ⊤−A)∥2\nF is minimized. Previous work has to pay Ω(n2) time when matrices A\nand W are dense, e.g., having Ω(n2) non-zero entries. In this work, we show that there is a\ncertain regime, even if A and W are dense, we can still hope to solve the weighted low-rank\napproximation problem in almost linear n1+o(1) time.\n∗lchenyang550@gmail.com. Fuzhou University.\n† yingyul@hku.hk. The University of Hong Kong.\nyliang@cs.wisc.edu. University of Wisconsin-Madison.\n‡ zhmeishi@cs.wisc.edu. University of Wisconsin-Madison.\n§ magic.linuxkde@gmail.com. The Simons Institute for the Theory of Computing at the UC, Berkeley.\n\n1\nINTRODUCTION\nWeighted low-rank approximation problem [SJ03] is a fundamental numerical linear algebra prob-\nlem related to matrix completion [LLR16], faster SVD decomposition [BJS14, BKS16], and principal\ncomponent analysis [XXF+21]. It is a prevalent focus over recent years and has been widely applied\nin machine learning in broad practical applications, such as vision detection [CXT+22], represen-\ntation learning [WWZ+19], image classiﬁcation [PCP+23], language model [HHC+21], weather\nprediction [WTL18], and many more.\nWe can formulate the weighted low-rank approximation\nproblem as below.\nDeﬁnition 1.1. Given two n × n matrices A and W, the goal is to ﬁnd two n × k matrices U, V\nsuch that ∥W ◦(UV ⊤−A)∥2\nF ≤(1 + ǫ) min{A′|rank(A′)=k} ∥W ◦(A′ −A)∥2\nF . Here, W ◦B denotes\na matrix with entries given by Wi,jBi,j, where ◦is the Hadamard product operation.\nHere, we provide an example to show that attention computation can be formulated as a\nweighted low-rank approximation problem.\nLet Q, K, V ∈Rn×d be the query, key, and value\nmatrix. Let A := Activation(QK⊤) ∈Rn×n be the attention matrix, e.g, using Softmax as activa-\ntion. Let W ∈Rn×n be the attention mask. Then, we have the attention output as (A ◦W)V . We\ncan clearly see that A ◦W is the target of the weighted low-rank approximation problem.\nThere is some well-known hardness in the weighted low-rank approximation problem. [GG11,\nRSW16] showed that, when we want to ﬁnd an approximate solution with ǫ = 1/ poly(n), the\nweighted low-rank approximation problem is “equivalent ” to NP-hard problem.\nTo practically\nﬁx it, under certain assumptions (e.g., the weighted matrix W has r distinct columns/rows for\nsmall r), we may have a polynomial time complexity algorithm to solve the weighted low-rank\napproximation problem. Thus, there are many theoretical and empirical works studying how we\nare able to solve the weighted low-rank approximation problem eﬃciently (e.g., [EVDH12, ZQZ+19,\nSYYZ23, WY24] and many more).\nHowever, the best previous works still need to solve weighted low-rank approximation in Ω(n2)\nif A has Ω(n2) non-zero entries and W has Ω(n2) non-zero-entries. Therefore, in this work, we raise\na natural fundamental mathematical question:\nIs there some dense regime setting for A and W so that we can solve the weighted low-rank\napproximation problem in almost linear n time?\nThe answer is positive. When we have mild practical assumptions (see detail in Assumption 3.2),\nwe can solve the weighted low-rank approximation problem in almost linear complexity, i.e., n1+o(1).\nWe state our main results in the following, whose proof is in Section 7.\nTheorem 1.2 (Main result). Let A and W denote two n × n matrices. Assume each entry of A\nand W needs nγ bits1 to represent, where γ = o(1). Let r be the number of distinct columns and\nrows of W (Deﬁnition 3.1). Assume A ◦W has at most r · p distinct columns and at most r · p\ndistinct rows, where p = no(1). Assume that k2r = O(log n/ log log n). For every constant ǫ > 0,\nthere is an algorithm running in time, n1+o(1) time which outputs a factorization (into an n × k\nand a k × n matrix) of a rank-k matrix bA for which\n∥W ◦(A −bA)∥2\nF ≤(1 + ǫ) OPT,\nwith probability at least 0.99, where\nOPT :=\nmin\n{Ak|rank(Ak)=k} ∥W ◦(A −Ak)∥2\nF .\n1Each entry in a matrix can be represented by nγ bits. In a real dataset, if we use the ﬂoat32 format, then as\nlong as nγ ≥32, our assumption holds.\n1\n\nTheorem 1.2 shows that, with high probability, we can solve the weighted low-rank approxima-\ntion problem in almost linear complexity when there is a small number of distinct rows and columns\nin the target matrix. Furthermore, for any δ ∈(0, 0.1), if we conduct O(log(1/δ)) repetitions and\ntake the median of results as the ﬁnal output, our success probability can be boosted to 1 −δ.\nNote that it is a standard way to boost the success probability from constant 0.99 to 1 −δ for any\nδ ∈(0, 0.1). The high-level idea is a majority vote, which is equivalent to taking the median.\nComparison with Previous Works.\nOur results are beyond [RSW16] in the following sense.\nUnder the condition in Theorem 1.2, their algorithm requires (nnz(A) + nnz(W)) · no(1) + n1+o(1)\nrunning time complexity, where nnz(A) means number of non-zero entries in A. When A or W are\ndense matrix, their complexity is n2+o(1), while our complexity is n1+o(1). We use a similar high-\nlevel proof framework as [RSW16]. However, as our problem setup changes, all analyses need to be\nupdated accordingly. In particular, Theorem 6.1 in Section 6 is brand new. The high-level intuition\nis that we need to carefully handle the distinct patterns in the analysis to avoid the quadratic time\ncomplexity. Then, during the matrix product, we can rewrite the summation order. Then, by\nﬁne-grained analysis, we can group many operations together due to the few distinct patterns and\nbypass the quadratic complexity.\nRoadmap. In Section 2, we provide related work of weighted low-rank. In Section 3, we provide\nthe preliminary of our problem setup and some tools from previous works. Then, in Section 4, we\nprovide a warm up of our analysis to introduce our technique. In Section 5, we provide the lower\nbound of cost used in binary search. In Section 6, we introduce how to estimate the OPT value. In\nSection 7, we can successfully recover our solutions. Finally, we state our conclusion in Section 8.\n2\nRELATED WORK\n2.1\nWeighted Low Rank Applications\nWeighted low-rank techniques have been widely applied in broad machine learning applications\nsuch as grayscale-thermal detection [LWZ+16], object detection [TWZL16, CXT+22], fault de-\ntection [DCZ+17], defect detection [MWLZ20, JLD+20], background estimation [DL17b, DLR18],\nmulti-task learning [FLCT18], robust visual representation learning [KJS15, WZX+18, WWZ+19],\nadversarial learning [LBBM19], image restoration [PSDX14, CYZ+20], image clustering and clas-\nsiﬁcation [SJS+11, WL17, WLLZ18, FZCW21, FZC+22a, FZC+22b, KKK+23, PCP+23], robust\nprincipal component analysis [XXF+21], language models training [HHW+22], language model\ncompression [HHC+21], weather prediction [WTL18], tensor training [CWC+21, ZWH+22], do-\nmain generalization [SMF+24] and many more.\nWeighted low-rank techniques have also been\nwidely used in signal processing for ﬁlter design and noise removal [LPW97, LHZC10, JYCL15].\n2.2\nWeighted Low Rank in Attention Mechanism\nParticularly, a line of works shows that the attention matrix may have some low-rank prop-\nerty, even under softmax activation function by polynomial approximation methods, e.g., [AS23,\nKLL+25, LLS+25b, CHL+24, LLSS24b, LLS+24, LSS+24, LSSY24, LSSZ24, SWXL24, XSL24,\nAS24c, AS24a, AS24b, HSK+24, HWG+24, HWL+24a, HWL+24b]. Thus, under such conditions,\nwe may use weighted low-rank approximations for transformers’ attention acceleration. For a few\ndistinct columns or rows, empirically, we see that the attention matrix has some good patterns\n[JLZ+24, CLS+25, LLS+25a, CLL+25, SMN+24]. In this work, we focus on the theoretical analysis\nand leave its empirical justiﬁcation as future work.\n2\n\n2.3\nWeighted Low Rank Approximation and Acceleration\nMany previous works try to solve in an eﬃcient way empirically and theoretically [MMH03,\nSLVH04, WJ06, MVH07, Mar08, EVDH12, MXZZ13, MU14, RSW16, LLR16, Dut16, DL17b,\nDL17a, BWZ19, HLX+19, ZQZ+19, SWZ+20, TH21, YZLS22, SYYZ23, ZYLS24]. Particularly,\nrecently, [WY24] proposes an algorithm to output a slightly higher rank output as a proxy to solve\nthe weighted low-rank approximation problem eﬃciently. [LLR16] develops an eﬃcient framework\nfor alternating minimization to get the weighted low-rank approximation.\nSimilarly, [SYYZ23]\nproposes a more robust framework to get the solution.\n2.4\nSketching for Numerical Linear Algebra\nIn this work, we use the sketching technique to accelerate a submodular optimization problem.\nWe provide a brief overview of prior sketching work across various domains. Sketching has been\nutilized to enhance numerous continuous optimization challenges, including linear programming\n[CLS19, Son19, Bra20, JSWZ21, SY21, GS22], empirical risk minimization [LSZ19, QSZZ23], the\ncutting plane method [JLSW20], calculating the John Ellipsoid [CCLY19, SYYZ22], and many\nmore.\nBeyond continuous optimization, sketching has also been applied to several discrete op-\ntimization issues [DSW22, SXZ22, Zha22, JLSZ23]. Additionally, sketching concepts have been\nimplemented in addressing theoretical problems in large language models, such as exponential and\nsoftmax regression [LSZ23, GSY23, DLS23a, LLSS24a], and reducing the feature dimension of atten-\ntion matrices [DMS23]. Sketching techniques prove valuable in various machine learning tasks, in-\ncluding matrix completion [GSYZ23], adversarial training [GQSW22], training over-parameterized\nneural tangent kernel regression [BPSW21, SZZ21, Zha22, ALS+23, HSWZ22], matrix sensing\n[DLS23b, QSZ23], kernel density estimation [QRS+22], and federated learning [SWYZ23]. More-\nover, the application of sketching extends to the theory of relational databases [QJS+22].\n3\nPRELIMINARY\nWe deﬁne [n] := {1, 2, . . . , n}. Let R denote the real numbers, and R≥0 denote the nonnegative\nreal numbers.\nLet ∥A∥denote the spectral norm of matrix A.\nLet ∥A∥2\nF = P\ni,j A2\ni,j denote\nthe Frobenius norm of A. Let W ◦A denote the entry-wise product of matrices W and A. Let\n∥A∥2\nW = P\ni,j W 2\ni,jA2\ni,j denote the weighted Frobenius norm of A. Let nnz(A) denote the number of\nnonzero entries of A. Let det(A) denote the determinant of a square matrix A. Let A⊤denote the\ntranspose of A. Let A† denote the Moore-Penrose pseudo-inverse of A. Let A−1 denote the inverse\nof a full rank square matrix A.\nFor the weight matrix W, we always use W∗,j to denote the j-th column of W, and Wi,∗to\ndenote the i-th row of W. Let diag(W∗,j) denote the diagonal matrix with entries from the vector\nW∗,j. Let diag(Wi,∗) denote the diagonal matrix with entries from the vector Wi,∗.\nDeﬁnition 3.1 (Distinct rows and columns). Given a matrix R ∈m×n, we have R∗,1, R∗,2, . . . , R∗,n ∈\nRm as its column vectors, and R⊤\n1,∗, R⊤\n2,∗, . . . , R⊤\nm,∗∈Rn as its row vectors.\nWe deﬁne r :=\n|{R∗,1, R∗,2, . . . , R∗,n}| and p := |{R⊤\n1,∗, R⊤\n2,∗, . . . , R⊤\nm,∗}|, where | · | means the cardinality of a set,\ni.e., the number of elements in a set. Then, we say R has r distinct columns and p distinct rows.\nAssumption 3.2. Let r, p ∈N+. We assume n × n matrix W has r distinct columns and rows.\nWe also assume that W ◦A has rp distinct columns and rows.\nIn a real application, when the data has strong periodicity, e.g., signal processing, or uniformly\ndraws from a small set, e.g., classiﬁcation, or the data has a certain structure, e.g., block attention\n3\n\nmask in Transformers [VSP+17, DFG+24], the matrix may have a small number of distinct columns\nor rows.\nLet Z[x1, x2, · · · , xv] denote the set of multi-variable polynomial with v variables and the co-\neﬃcients are from Z. Furthermore, if all the coeﬃcients of a polynomial are from {0, 1, · · · , 2H},\nthen we say the bitsize of coeﬃcients of this polynomial is H.\nLemma 3.3 (Cramer’s rule). Let R be an n × n invertible matrix. Then, for each i ∈[n], j ∈[n],\n(R−1)i,j = det(R−i\n−j)/ det(R),\nwhere R−i\n−j is the matrix R with the i-th row and j-th column removed.\n3.1\nPolynomial System Decision Solver\nHere, we formally state the theorem of the decision problem. For a full discussion of algorithms in\nreal algebraic geometry, we refer the reader to [BPR05] and [Bas14].\nTheorem 3.4 (Decision Problem [Ren92a, Ren92b, BPR96]). Given a real polynomial system\nP(x1, x2, · · · , xv) having v variables and m polynomial constraints fi(x1, x2, · · · , xv)∆i0, ∀i ∈[m],\nwhere ∆i is any of the “standard relations”: {>, ≥, =, ̸=, ≤, <}, let d denote the maximum degree\nof all the polynomial constraints and let H denote the maximum bitsize of the coeﬃcients of all the\npolynomial constraints. Then in\n(md)O(v) poly(H)\ntime, one can determine whether a real solution exists for the polynomial system P.\n3.2\nLower Bound on Cost\nThe key result we used for proving the lower bound is the following bound on the minimum value\nattained by an integer polynomial restricted to a compact connected component of a basic closed\nsemi-algebraic subset of Rv (see details in Deﬁnition 3.9) deﬁned by polynomials with integer\ncoeﬃcients in terms of the degrees and the bitsizes of the coeﬃcients of the polynomials involved.\nTheorem 3.5 ([JPT13]). Let T := {x ∈Rv | f1(x) ≥0, · · · , fℓ(x) ≥0, fℓ+1(x) = 0, · · · , fm(x) = 0}\nbe deﬁned by polynomials f1, · · · , fm ∈Z[x1, · · · , xv] with v ≥2, degrees bounded by an even integer\nd and coeﬃcients of absolute value at most H bitsize, and let C be a compact connected component\nof T. Let g ∈Z[x1, · · · , xv] (here g can be viewed as an objective function) be a polynomial of degree\nat most d and coeﬃcients of absolute value bounded by H. Let eH := max{H, 2v + 2m}. Then, the\nminimum value that g takes over C satisﬁes that if it is not zero, then its absolute value is ≥\n2−23v log(d) log(eH).\n3.3\nUpper Bound on Cost\nNow, we provide the upper bound on the OPT, which is deﬁned below.\nDeﬁnition 3.6. Given A, W ∈Rn×n and k ∈[n], we deﬁne OPT as\nOPT :=\nmin\nU∈Rn×k,V ∈Rn×k ∥(UV ⊤−A) ◦W∥2\nF.\n4\n\nLemma 3.7 (folklore). Let A, W, OPT be deﬁned in Deﬁnition 3.6. Let each entry of A, W can be\nrepresented by nγ bits, with γ ∈(0, 1). Then, we have\nOPT ≤poly(n) · 2nγ.\nProof. Set U and V to be the zero matrices.\n3.4\nSketching Tool\nWe use a sketching tool from previous work.\nLemma 3.8 (Theorem 3.1 in [RSW16]). Let A1, . . . , Am ∈Rn×k be m matrices of size n × k.\nLet b1, . . . , bm ∈Rn×1 be m column vectors of dimension n.\nFor 1 ≤i ≤m denote: xi =\narg minx∈Rk ∥Aix −bi∥2\n2 the solution of the i-th regression problem. Let S ∈Rt×n be a random\nmatrix with i.i.d. Gaussian entries with zero mean and standard deviation 1/\n√\nt. For 1 ≤i ≤m\ndenote yi = arg miny∈Rk ∥SAiy −Sbi∥2\n2 the solution of the i-th regression problem in the sketch\nspace. We claim that for every 0 < ǫ < 1/2, with high probability, one can set t = O(k/ǫ) such\nthat:\nm\nX\ni=1\n∥Aiyi −bi∥2\n2 ≤(1 + ǫ) ·\nm\nX\ni=1\n∥Aixi −bi∥2\n2.\n3.5\nBackgrounds on Semi-Algebraic Sets\nThe following real algebraic geometry deﬁnitions are needed when proving a lower bound for the\nminimum nonzero cost of our problem. For a full discussion, we refer the reader to [BCR87]. Here,\nwe use the brief summary by [BPR05].\nDeﬁnition 3.9 ([BPR05]). Let R be a real closed ﬁeld. Given x = (x1, · · · , xv) ∈Rv, r ∈R, r > 0,\nwe denote\nBv(x, r) := {y ∈Rv|∥y −x∥2 < r2}\n(open ball),\n¯Bv(x, r) := {y ∈Rv|∥y −x∥2 ≤r2}\n(closed ball).\nA set S ⊂Rv is open if it is the union of open balls, i.e., if every point of U is contained in an\nopen ball contained in U.\nA set S ⊂Rv is closed if its complement is open. Clearly, the arbitrary union of open sets is\nopen, and the arbitrary intersection of closed sets is closed.\nSemi-algebraic sets are deﬁned by a ﬁnite number of polynomial inequalities and equalities.\nA semi-algebraic set has a ﬁnite number of connected components, each of which is semi-\nalgebraic. Here, we use the topological deﬁnition of a connected component, which is a maximal\nconnected subset (ordered by inclusion), where connected means it cannot be divided into two disjoint\nnonempty closed sets.\nA closed and bounded semi-algebraic set is compact.\nA semi-algebraic set S ⊂Rv is semi-algebraically connected if S is not the disjoint union of\ntwo non-empty semialgebraic sets that are both closed in S. Or, equivalently, S does not contain a\nnon-empty semi-algebraic strict subset which is both open and closed in S.\nA semi-algebraically connected component of a semi-algebraic set S is a maximal semi-algebraically\nconnected subset of S.\n5\n\n4\nWARMUP\nIn this section, to demonstrate the new technique, we prove the following theorem.\nLemma 4.1. Given two n × n size matrices A and W, 1 ≤k ≤n such that: Let each entry of\nA, W can be represented by nγ bits, with γ ∈(0, 1); Let OPT be deﬁned as Deﬁnition 3.6. Then,\nwe can show\n• Part 1. There is an algorithm that runs in 2O(nk log n) time, and outputs a number Λ such\nthat OPT ≤Λ ≤2 OPT.\n• Part 2.\nThere is an algorithm that runs in 2O(nk log n) time and returns U ∈Rn×k and\nV ∈Rn×k such that ∥(UV ⊤−A) ◦W∥2\nF ≤2 OPT.\nProof. Proof of Part 1.\nWe can create 2nk variables to explicitly represent each entry of U and V . Let g(x) = ∥W ◦\n(UV ⊤−A)∥2\nF . Let L = 2nγ. Then, we can write down a polynomial system (the decision problem\ndeﬁned in Theorem 3.4)\nmin g(x)\ns.t. Ui,j ∈[−L, L], ∀i, j\nVi,j ∈[−L, L], ∀i, j\nUsing Theorem 3.5, we know the above system has\nm = 2nk, v = 2nk, d = 4, H = nγ, eH = nγ + O(nk).\nThe lower bound on g(x) (if g(x) is not zero) is going to be\nclower = 2−23v log(d) log( e\nH)\n≥2−2O(nk) log(nγ+nk)\n≥2−2O(nk log n).\nBy Lemma 3.7, we know the upper bound is Cupper = poly(n) · 2nγ. After knowing the lower\nbound and upper bound on cost, the number of binary search iterations is upper-bounded by\nlog(Cupper\nclower\n) = log( poly(n)2nγ\n2−2O(nk log n) ) ≤2O(nk log n).\nIn each of the above iterations, we need to run Theorem 3.4 with the system\ns.t. g(x) ∈[Γt, 2Γt], Ui,j ∈[−L, L], Vi,j ∈[−L, L]\nwith parameters,\nm = 2nk + 1, v = 2nk, d = 4, H = nγ.\nThen, running time complexity is\n(md)O(v) · poly(H) = (10nk)O(nk) · poly(nγ)\n= 2O(nk log n).\n6\n\nThus, combining the number of iterations and time for each iteration, we can ﬁnd the number\nΓ ∈[OPT, 2 OPT].\nProof of Part 2.\nNext, similar to Part 1, we need to repeat the binary search for 2nk times for each variable in\nU and V , and each time, the number of total binary search steps is nγ. Thus, we can output the\nU, V in the same running time as ﬁnding Γ.\nIn the next few sections, we will explain how to reduce the number of variables and how to\nreduce the number of constraints.\n5\nLOWER BOUND ON OPT\nWe assume that W has r distinct rows and r distinct columns. Then, we get rid of the dependence\non n in the degree.\nTheorem 5.1 (Implicitly in [RSW16]). Assuming that W has r distinct rows and r distinct\ncolumns, each entry of A and W needs nγ bits to represent. Assume OPT > 0. Then we know\nthat, with high probability,\nOPT ≥2−nγ2 e\nO(rk2/ǫ).\nProof. We use Ai,∗∈Rn denote the i-th row of A. We use Wi,∗∈Rn to denote the i-th row of W.\nLet (U1)i,∗denote the i-th row of U1. For any n × k matrix U1 and for any k × n matrix Z1, we\nhave\n∥(U1Z1 −A) ◦W∥2\nF\n=\nn\nX\ni=1\n∥(U1)i,∗Z1 diag(Wi,∗) −Ai,∗diag(Wi,∗)∥2\n2.\nBased on the observation that W has r distinct rows, we use group g1,1, g1,2, · · · , g1,r to denote\nr disjoint sets such that\n∪r\ni=1g1,i = [n]\nFor any i ∈[r], for any j1, j2 ∈g1,i, we have Wj1,∗= Wj2,∗.\nThus, we can have\nn\nX\ni=1\n∥(U1)i,∗Z1 diag(Wi,∗) −Ai,∗diag(Wi,∗)∥2\n2\n=\nr\nX\ni=1\nX\nℓ∈g1,i\n∥(U1)ℓ,∗Z1 diag(Wℓ,∗) −Aℓ,∗diag(Wℓ,∗)∥2\n2.\nWe can sketch the objective function by choosing Gaussian matrices S1 ∈Rn×s1 with s1 = O(k/ǫ).\nn\nX\ni=1\n∥(U1)i,∗Z1 diag(Wi,∗)S1 −Ai,∗diag(Wi,∗)S1∥2\n2.\nLet bU1 denote the optimal solution of the sketch problem,\n7\n\nbU1 = arg\nmin\nU1∈Rn×k\nn\nX\ni=1\n∥(U1)i,∗Z1 diag(Wi,∗)S1 −Ai,∗diag(Wi,∗)S1∥2\n2.\nBy properties of S1, plugging bU1 into the original problem, we obtain\nr\nX\ni=1\nX\nℓ∈g1,i\n∥(bU1)ℓ,∗Z1 diag(Wℓ,∗) −Aℓ,∗diag(Wℓ,∗)∥2\n2\n≤(1 + ǫ) · OPT .\nLet R denote the set of all S(g1,i) (for all i ∈[r] and |R| = r)\nNote that bU1 also has the following form, for each ℓ∈L ⊂[n] (Note that |L| = rp.)\n(bU1)ℓ,∗= Aℓ,∗diag(Wℓ,∗)S1 · (Z1 diag(Wℓ,∗)S1)†\n= Aℓ,∗diag(Wℓ,∗)S1 · (Z1 diag(Wℓ,∗)S1)⊤\n· ((Z1 diag(Wℓ,∗)S1)(Z1 diag(Wℓ,∗)S1)⊤)−1.\nRecall the number of diﬀerent diag(Wℓ,∗) is at most r.\nFor each k × s1 matrix Z1 diag(Wℓ,∗)S1, we create k × s1 variables to represent it. Thus, we\ncreate r matrices,\n{Z1DWi,∗S1}i∈R.\nFor simplicity, let P1,i ∈Rk×s1 denote Z1 diag(Wi,∗)S1. Then we can rewrite bU i as follows\nbU i\n1 = Ai,∗diag(Wi,∗)S1 · P ⊤\n1,i(P1,iP ⊤\n1,i)−1.\nIf P1,iP ⊤\n1,i ∈Rk×k has rank-k, then we can use Cramer’s rule to write down the inverse of P1,iP ⊤\n1,i.\nFor the situation, it is not full rank. We can guess the rank. Let ti ≤k denote the rank of P1,i.\nThen, we need to ﬁgure out a maximal linearly independent subset of columns of P1,i. We can\nalso guess all the possibilities, which is at most 2O(k). Because we have r diﬀerent P1,i, the total\nnumber of guesses we have is at most 2O(rk). Thus, we can write down (P1,iP ⊤\n1,i)−1 according to\nCramer’s rule. Note that (P1,iP ⊤\n1,i)−1 can be view as Pa/Pb where Pa is a polynomial and Pb is\nanother polynomial which is essentially det(P1,iP ⊤\n1,i).\nAfter bU1 is obtained, we ﬁx bU1. We consider\nbU2 = arg\nmin\nU2∈Rn×k ∥(bU1U ⊤\n2 −A) ◦W∥2\nF .\nIn a similar way, we can get and write bU2.\nOverall, by creating l = O(rk2/ǫ) variables, we have rational polynomials bU1(x) and bU2(x).\nNote that bU1(x) only has rp diﬀerent rows, and same for bU2(x).\nIndeed, now we have only 2r distinct denominators (w.l.o.g., assume the ﬁrst r columns are\ndistinct and the ﬁrst r rows are distinct),\nh1,i(x) = det(P1,iP ⊤\n1,i), ∀i ∈[r]\n8\n\nh2,i(x) = det(P2,iP ⊤\n2,i), ∀i ∈[r].\nThen, we can write down the following optimization problem,\nmin\nx∈Rl p(x)/q(x)\ns.t. h2\n1,i(x) ̸= 0, h2\n2,i(x) ̸= 0, ∀i ∈[r],\nq(x) =\nr\nY\ni=1\nh2\n1,i(x)h2\n2,i(x),\nwhere q(x) has degree O(rk), the maximum coeﬃcient in absolute value is\n(2nγ)O(rk)\nand the number of variables O(rk2/ǫ). However, that formulation p(x)/q(x) is not a polynomial,\nto further make it a polynomial, we introduce variable y:\nmin\nx∈Rl p(x)y\ns.t. h2\n1,i(x) ̸= 0, h2\n2,i(x) ̸= 0, ∀i ∈[r],\nq(x) =\nr\nY\ni=1\nh2\n1,i(x)h2\n2,i(x)\nq(x)y −1 = 0.\nApplying Theorem 3.5, with parameters\nm = O(r), v = O(rk2/ǫ), d = O(r),\nH = 2O(nγrk), eH = O(H).\nwe can achieve the following minimum nonzero cost:\n≥2−23v log(d) log(eH) ≥2−nγ2 e\nO(rk2/ǫ).\nThus, we complete the proofs.\n6\nFEW DISTINCT COLUMNS\nIn this section, we try to estimate the OPT value.\nTheorem 6.1. Given a matrix A and W, each entry can be written using O(nγ) bits for γ > 0.\nGiven A ∈Rn×n, W ∈Rn×n, 1 ≤k ≤n. Assume that W has r distinct columns and rows.\nThen, with high probability, one can output a number Λ in time n1+γ · p · 2O(k2r/ǫ) such that\nOPT ≤Λ ≤(1 + ǫ) OPT .\nProof. Let U ∗\n1 , U ∗\n2 ∈Rn×k denote the matrices satisfying\n∥W ◦(U ∗\n1 (U ∗\n2 )⊤−A)∥2\nF = OPT .\n9\n\nWe use Ai,∗∈Rn denote the i-th row of A. We use Wi,∗∈Rn to denote the i-th row of W. Let\n(U1)i,∗denote the i-th row of U1. Let Z1 = (U ∗\n2 )⊤. For any n × k matrix U1,\n∥(U1Z1 −A) ◦W∥2\nF\n=\nn\nX\ni=1\n∥(U1)i,∗Z1 diag(Wi,∗) −Ai,∗diag(Wi,∗)∥2\n2.\nBased on the observation that W has r distinct rows, we use group g1,1, g1,2, · · · , g1,r to denote\nr disjoint sets such that\n∪r\ni=1g1,i = [n]\nFor any i ∈[r], for any j1, j2 ∈g1,i, we have Wj1,∗= Wj2,∗.\nNext, based on assumptions on W and A, we use g1,i,1, g1,i,2, g1,i,p to denote p groups such that\n∪p\nj=1g1,i,j = g1,i.\nFor any i ∈[r], for any j ∈[p], for any ℓ1, ℓ2 ∈g1,i,j, we have (Wℓ1,∗◦Aℓ1,∗) = (Wℓ2,∗◦Aℓ2,∗).\nLet S(g1,i,j) denote the smallest index from set g1,i,j\nThus, we can have\nn\nX\ni=1\n∥(U1)i,∗Z1 diag(Wi,∗) −Ai,∗diag(Wi,∗)∥2\n2\n=\nr\nX\ni=1\np\nX\nj=1\nX\nℓ∈g1,i,j\n∥(U1)ℓ,∗Z1 diag(Wℓ,∗)\n−Aℓ,∗diag(Wℓ,∗)∥2\n2\n=\nr\nX\ni=1\np\nX\nj=1\n|g1,i,j| · ∥(U1)S(g1,i,j),∗Z1 diag(WS(g1,i,j),∗)\n−AS(g1,i,j),∗diag(WS(g1,i,j),∗)∥2\n2.\nWe can sketch the objective function by choosing Gaussian matrices S1 ∈Rn×s1 with s1 = O(k/ǫ).\nr\nX\ni=1\np\nX\nj=1\n|g1,i,j| · ∥(U1)S(g1,i,j),∗Z1 diag(WS(g1,i,j),∗)S1\n−AS(g1,i,j),∗diag(WS(g1,i,j),∗)S1∥2\n2.\nLet bU1 denote the optimal solution of the sketch problem,\nbU1 = arg min\nU1\nr\nX\ni=1\np\nX\nj=1\n|g1,i,j|\n· ∥(U1)S(g1,i,j),∗Z1 diag(WS(g1,i,j),∗)S1\n−AS(g1,i,j),∗diag(WS(g1,i,j),∗)S1∥2\n2.\nBy properties of S1, plugging bU1 into the original problem, we obtain\nr\nX\ni=1\np\nX\nj=1\n|g1,i,j| · ∥(U1)S(g1,i,j),∗Z1 diag(WS(g1,i,j),∗)\n10\n\n−AS(g1,i,j),∗diag(WS(g1,i,j),∗)∥2\n2 ≤(1 + ǫ) · OPT .\nLet R denote the set of all S(g1,i) (for all i ∈[r] and |R| = r).\nLet L denote the set of all S(g1,i,j) (for all i ∈[r], j ∈[p] and |L| = rp).\nNote that bU1 also has the following form, for each ℓ∈L ⊂[n] (Note that |L| = rp.)\n(U1)ℓ,∗= Aℓ,∗diag(Wℓ,∗)S1 · (Z1 diag(Wℓ,∗)S1)†\n= Aℓ,∗diag(Wℓ,∗)S1 · (Z1 diag(Wℓ,∗)S1)⊤\n· ((Z1 diag(Wℓ,∗)S1)(Z1 diag(Wℓ,∗)S1)⊤)−1.\nRecall the number of diﬀerent Aℓ,∗diag(Wℓ,∗) is at most rp, and the number of diﬀerent\ndiag(Wℓ,∗) is at most r. For each k × s1 matrix Z1 diag(Wℓ,∗)S1, we create k × s1 variables to\nrepresent it. Thus, we create r matrices,\n{Z1 diag(Wi,∗)S1}i∈R.\nFor simplicity, let P1,i ∈Rk×s1 denote Z1 diag(Wi,∗)S1. Then we can rewrite (bU1)i,∗as follows\n(bU1)i,∗= Ai,∗diag(Wi,∗)S1 · P ⊤\n1,i(P1,iP ⊤\n1,i)−1.\nIf P1,iP ⊤\n1,i ∈Rk×k has rank-k, then we can use Cramer’s rule to write down the inverse of P1,iP ⊤\n1,i.\nFor the situation, it is not full rank. We can guess the rank. Let ti ≤k denote the rank of P1,i.\nThen, we need to ﬁgure out a maximal linearly independent subset of columns of P1,i. We can\nalso guess all the possibilities, which is at most 2O(k). Because we have r diﬀerent P1,i, the total\nnumber of guesses we have is at most 2O(rk). Thus, we can write down (P1,iP ⊤\n1,i)−1 according to\nCramer’s rule. Note that (P1,iP ⊤\n1,i)−1 can be view as Pa/Pb where Pa is a polynomial and Pb is\nanother polynomial which is essentially det(P1,iP1,i).\nAfter bU1 is obtained, we will ﬁx bU1 in the next round.\nFor any n × k matrix U2, we can rewrite\n∥(bU1U ⊤\n2 −A) ◦∥2\nF\n=\nn\nX\ni=1\n∥diag(W∗,i)bU1(U ⊤\n2 )∗,i −diag(W∗,i)A∗,i∥2\n2.\nBased on the observation that W has r columns rows, we use group g2,1, g2,2, · · · , g2,r to denote\nr disjoint sets such that\n∪r\ni=1g2,i = [n].\nFor any i ∈[r], for any j1, j2 ∈g2,i, we have W∗,j1 = W∗,j2.\nNext, based on assumptions on W and A, we use g2,i,1, g2,i,2, g2,i,p to denote p groups such that\n∪p\nj=1g2,i,j = g2,i.\nFor any i ∈[r], for any j ∈[p], for any ℓ1, ℓ2 ∈g2,i,j, we have (W∗,ℓ1 ◦A∗,ℓ1) = (W∗,ℓ2 ◦A∗,ℓ2).\nLet S(g2,i,j) denote the smallest index from set g2,i,j\nThus, we can have\nn\nX\ni=1\n∥diag(W∗,i)bU1(U ⊤\n2 )∗,i −diag(W∗,i)A∗,i∥2\n2\n11\n\n=\nr\nX\ni=1\np\nX\nj=1\nX\nℓ∈g1,i,j\n∥diag(W∗,ℓ)bU1(U ⊤\n2 )∗,ℓ\n−diag(W∗,ℓ)A∗,ℓ∥2\n2\n=\nr\nX\ni=1\np\nX\nj=1\n|g2,i,j| · ∥diag(W∗,S(g2,i,j))bU1(U ⊤\n2 )∗,S(g2,i,j)\n−diag(W∗,S(g1,i,j))A∗,S(g1,i,j)∥2\n2.\nWe can sketch the objective function by choosing Gaussian matrices S2 ∈Rn×s1 with s2 = O(k/ǫ).\nr\nX\ni=1\np\nX\nj=1\n|g2,i,j| · ∥S2 diag(WS(g2,i,j),∗)bU1(U ⊤\n2 )∗,S(g2,i,j)\n−S2 diag(W∗,S(g2,i,j))A∗,S(g2,i,j)∥2\n2.\nLet bU1 denote the optimal solution of the sketch problem,\nbU1 = arg min\nU1\nr\nX\ni=1\np\nX\nj=1\n|g2,i,j|\n· ∥S2 diag(WS(g2,i,j),∗)bU1(U ⊤\n2 )∗,S(g2,i,j)\n−S2 diag(W∗,S(g2,i,j))A∗,S(g2,i,j)∥2\n2.\nBy properties of S1, plugging bU2 into the original problem, we obtain\nr\nX\ni=1\np\nX\nj=1\n|g2,i,j| · ∥diag(WS(g2,i,j),∗)bU1(bU ⊤\n2 )∗,S(g2,i,j)\n−diag(W∗,S(g2,i,j))A∗,S(g2,i,j)∥2\n2 ≤(1 + ǫ) · OPT .\nLet R denote the set of all S(g1,i) (for all i ∈[r] and |R| = r).\nLet L denote the set of all S(g1,i,j) (for all i ∈[r], j ∈[p] and |L| = rp).\nNote that bU1 also has the following form, for each ℓ∈L ⊂[n] (Note that |L| = rp.)\n(U1)ℓ,∗= (S2 diag(W∗,ℓ)bU1)† · S2 diag(W∗,ℓ)A∗,ℓ\n= ((S2 diag(W∗,ℓ)bU1)(S2 diag(W∗,ℓ)bU1)⊤)−1\n· S2 diag(W∗,ℓ)bU1 · S2 diag(W∗,ℓ)A∗,ℓ.\nRecall the number of diﬀerent A∗,ℓdiag(W∗,ℓ) is at most rp, and the number of diﬀerent\ndiag(W∗,ℓ) is at most r.\nFor each s2 × k matrix S2 diag(W∗,i)bU1, we create s2 × k variables to represent it. Thus, we\ncreate r matrices,\n{S2 diag(W∗,i)bU1}i∈R.\nFor simplicity, let P2,i ∈Rk×s2 denote S2 diag(W∗,i)bU1. Then we can rewrite bU2 as follows\n(bU ⊤\n2 )∗,i = (P2,iP ⊤\n2,i)−1P2,iS2 diag(W∗,i)A∗,i.\n12\n\nIn a similar way, we can write bU2. Overall, by creating l = O(rk2/ǫ) variables, we have rational\npolynomials bU1(x) and bU2(x). Note that bU1(x) only has rp diﬀerent rows, and same for bU2(x).\nPutting it all together, we can write the objective function,\nmin\nx∈Rl ∥(bU1(x)bU2(x)⊤−A) ◦W∥2\nF\ns.t. h1,i(x) ̸= 0, ∀i ∈[r]\nh2,i(x) ̸= 0, ∀i ∈[r].\nNote that bU1(x)bU2(x) ◦W only has rp distinct rows. Also, A ◦W only has rp distinct rows.\nWriting down the objective function ∥(bU1(x)bU2(x)⊤−A) ◦W∥2\nF only requires n(rp) · poly(kr/ǫ)\ntime.\nCombining the binary search explained in Lemma 4.1 with the lower bound on cost (Theo-\nrem 5.1) we obtained, we can ﬁnd the solution for the original problem in time,\n(np · poly(kr/ǫ) + n2\ne\nO(rk2/ǫ)) · nγ = n1+γp · 2\ne\nO(rk2/ǫ).\n7\nRECOVER A SOLUTION\nWe state our results and proof of recovering a solution.\nTheorem 7.1. Given a matrix A and W, each entry can be written using O(nγ) bits for γ > 0.\nGiven A ∈Rn×n, W ∈Rn×n, 1 ≤k ≤n and ǫ ∈(0, 0.1). Assume W has r distinct columns and\nrows. Assume A ◦W has at most r · p distinct columns and at most r · p distinct rows. Let OPT\nbe deﬁned as Deﬁnition 3.6. Then, with high probability, one can output two matrices U, V ∈Rn×k\nin time n1+γ · 2O(k2r/ǫ) such that\n∥(UV ⊤−A) ◦W∥2\nF ≤(1 + ǫ) OPT .\nFurther, if we choose (1) k2r = O(log n/ log log n), (2) ǫ ∈(0, 0.1) to be a small constant, (3)\np = no(1) and (4) γ = o(1), then the running time becomes n1+o(1).\nProof. Here, we show how to recover an approximate solution, not only the value of OPT.\nThe idea is to recover the entries of U and V one by one and use the algorithm from the previous\nsection for the corresponding decision problem. We initialize the semialgebraic set to be\nS = {x ∈Rl | q(x) ̸= 0, p(x) ≤Λq(x)}.\nWe start by recovering the ﬁrst entry of U. We perform the binary search to localize the entry,\nwhich takes log(2nγ) invocations of the decision algorithm. For each step of binary search, we use\nTheorem 3.5 to determine whether the following semi-algebraic set S is empty or not,\nS ∩{U1,1(x) ≥bU −\n1,1, U1,1(x) ≤bU +\n1,1}.\nAfter that, we declare the ﬁrst entry of U to be any point in this interval. Then, we add an equality\nconstraint that ﬁxes the entry of bU to this value and add a new constraint into S permanently,\ne.g., S ←S ∩{U1,1(x) = bU1,1}. Next, we repeat the same with the second entry of U and so on.\nThis allows us to recover a solution of cost at most (1 + ǫ) OPT in time\nn1+γ · p · 2O(k2r/ǫ).\nIf we choose γ = o(1), ǫ = Θ(1), p = no(1) and k2r = O(log n/ log log n), then the running time\nbecomes n1+o(1). Thus, we complete the proof.\n13\n\n8\nCONCLUSION\nWe showed that the weighted low-rank approximation problem could be solved in almost linear\nn1+o(1) time when the weighted matrix W has few distinct columns and rows and W ◦A has few\ndistinct columns and rows. This demonstrates that truly subquadratic time is achievable for a\ndense regime not previously known to be tractable. Future work could generalize the assumptions\nand explore applications of the algorithm.\n9\nLIMITATIONS\nAlthough our algorithm can achieve a good approximation ratio in theory, it may not achieve\nthe ideal result in practical implementation due to various factors such as computational resource\nlimitation, data quality problems, or incomplete model speciﬁcation.\n10\nSOCIETAL IMPACT\nIn this paper, we introduce an algorithm that can solve weighted low-rank approximation prob-\nlems in near-linear time under certain conditions. Our paper is purely theoretical and empirical\nin nature (a mathematics problem), and thus, we foresee no immediate negative ethical impact.\nOur algorithms can improve the eﬃciency of data processing and model training, allowing complex\nalgorithms to be run in resource-limited environments and accelerating scientiﬁc research and tech-\nnological innovation. By reducing computing requirements, energy consumption can be reduced,\nand the environment can beneﬁt.\nReferences\n[ALS+23] Josh Alman, Jiehao Liang, Zhao Song, Ruizhe Zhang, and Danyang Zhuo. Bypass ex-\nponential time preprocessing: Fast neural network training via weight-data correlation\npreprocessing. In NeurIPS, 2023.\n[AS23] Josh Alman and Zhao Song. Fast attention requires bounded entries. arXiv preprint\narXiv:2302.13214, 2023.\n[AS24a] Josh Alman and Zhao Song. Fast rope attention: Combining the polynomial method\nand fast fourier transform. manuscript, 2024.\n[AS24b] Josh Alman and Zhao Song. The ﬁne-grained complexity of gradient computation for\ntraining large language models. In The Thirty-eighth Annual Conference on Neural\nInformation Processing Systems, 2024.\n[AS24c] Josh Alman and Zhao Song. How to capture higher-order correlations? generalizing\nmatrix softmax attention to kronecker computation.\nIn The Twelfth International\nConference on Learning Representations, 2024.\n[Bas14] Saugata Basu.\nAlgorithms in real algebraic geometry:\na survey.\narXiv preprint\narXiv:1409.1534, 2014.\n[BCR87] Jacek Bochnak, Michel Coste, and Marie-Fran¸coise Roy. G´eom´etrie alg´ebrique r´eelle,\nvolume 12. Springer Science & Business Media, 1987.\n14\n\n[BJS14] Srinadh Bhojanapalli, Prateek Jain, and Sujay Sanghavi. Tighter low-rank approxi-\nmation via sampling the leveraged element. In Proceedings of the twenty-sixth annual\nACM-SIAM symposium on Discrete algorithms, pages 902–920. SIAM, 2014.\n[BKS16] Srinadh Bhojanapalli, Anastasios Kyrillidis, and Sujay Sanghavi. Dropping convexity\nfor faster semi-deﬁnite optimization. In Conference on Learning Theory, pages 530–\n582. PMLR, 2016.\n[BPR96] Saugata Basu, Richard Pollack, and Marie-Fran¸coise Roy.\nOn the combinatorial\nand algebraic complexity of quantiﬁer elimination.\nJournal of the ACM (JACM),\n43(6):1002–1045, 1996.\n[BPR05] Saugata Basu, Richard Pollack, and Marie-Francoise Roy. Algorithms in real algebraic\ngeometry, vol. 20033, 2005.\n[BPSW21] Jan van den Brand, Binghui Peng, Zhao Song, and Omri Weinstein. Training (over-\nparametrized) neural networks in near-linear time. In ITCS, 2021.\n[Bra20] Jan van den Brand. A deterministic linear program solver in current matrix multi-\nplication time. In Proceedings of the Fourteenth Annual ACM-SIAM Symposium on\nDiscrete Algorithms (SODA), pages 259–278. SIAM, 2020.\n[BWZ19] Frank Ban, David Woodruﬀ, and Richard Zhang.\nRegularized weighted low rank\napproximation. Advances in neural information processing systems, 32, 2019.\n[CCLY19] Michael B Cohen, Ben Cousins, Yin Tat Lee, and Xin Yang. A near-optimal algorithm\nfor approximating the john ellipsoid. In Conference on Learning Theory, pages 849–\n873. PMLR, 2019.\n[CHL+24] Yifang Chen, Jiayan Huo, Xiaoyu Li, Yingyu Liang, Zhenmei Shi, and Zhao Song.\nFast gradient computation for rope attention in almost linear time. arXiv preprint\narXiv:2412.17316, 2024.\n[CLL+25] Bo Chen, Xiaoyu Li, Yingyu Liang, Zhenmei Shi, and Zhao Song. Bypassing the ex-\nponential dependency: Looped transformers eﬃciently learn in-context by multi-step\ngradient descent. In International Conference on Artiﬁcial Intelligence and Statistics,\n2025.\n[CLS19] Michael B Cohen, Yin Tat Lee, and Zhao Song. Solving linear programs in the current\nmatrix multiplication time. In STOC, 2019.\n[CLS+25] Bo Chen, Yingyu Liang, Zhizhou Sha, Zhenmei Shi, and Zhao Song. Hsr-enhanced\nsparse attention acceleration.\nIn Conference on Parsimony and Learning. PMLR,\n2025.\n[CWC+21] Chuan Chen, Zhe-Bin Wu, Zi-Tai Chen, Zi-Bin Zheng, and Xiong-Jun Zhang. Auto-\nweighted robust low-rank tensor completion via tensor-train. Information Sciences,\n567:100–115, 2021.\n[CXT+22] Xiaolong Chen, Wei Xu, Shuping Tao, Tan Gao, Qinping Feng, and Yongjie Piao.\nTotal variation weighted low-rank constraint for infrared dim small target detection.\nRemote Sensing, 14(18):4615, 2022.\n15\n\n[CYZ+20] Yi Chang, Luxin Yan, Xi-Le Zhao, Houzhang Fang, Zhijun Zhang, and Sheng Zhong.\nWeighted low-rank tensor recovery for hyperspectral image restoration. IEEE trans-\nactions on cybernetics, 50(11):4558–4572, 2020.\n[DCZ+17] Zhaohui Du, Xuefeng Chen, Han Zhang, Boyuan Yang, Zhi Zhai, and Ruqiang Yan.\nWeighted low-rank sparse model via nuclear norm minimization for bearing fault de-\ntection. Journal of Sound and Vibration, 400:270–287, 2017.\n[DFG+24] Juechu Dong, Boyuan Feng, Driss Guessous, Yanbo Liang, and Horace He.\nFlex\nattention: A programming model for generating optimized attention kernels. arXiv\npreprint arXiv:2412.05496, 2024.\n[DL17a] Aritra Dutta and Xin Li. A fast algorithm for a weighted low rank approximation.\nIn 2017 Fifteenth IAPR International Conference on Machine Vision Applications\n(MVA), pages 93–96. IEEE, 2017.\n[DL17b] Aritra Dutta and Xin Li. Weighted low rank approximation for background estimation\nproblems. In Proceedings of the IEEE International Conference on Computer Vision\nWorkshops, pages 1853–1861, 2017.\n[DLR18] Aritra Dutta, Xin Li, and Peter Richt´arik. Weighted low-rank approximation of ma-\ntrices and background modeling. arXiv preprint arXiv:1804.06252, 2018.\n[DLS23a] Yichuan Deng, Zhihang Li, and Zhao Song. Attention scheme inspired softmax re-\ngression. arXiv preprint arXiv:2304.10411, 2023.\n[DLS23b] Yichuan Deng, Zhihang Li, and Zhao Song. An improved sample complexity for rank-1\nmatrix sensing. arXiv preprint arXiv:2303.06895, 2023.\n[DMS23] Yichuan Deng, Sridhar Mahadevan, and Zhao Song. Randomized and deterministic\nattention sparsiﬁcation algorithms for over-parameterized feature dimension.\narxiv\npreprint: arxiv 2304.03426, 2023.\n[DSW22] Yichuan Deng, Zhao Song, and Omri Weinstein. Discrepancy minimization in input-\nsparsity time. arXiv preprint arXiv:2210.12468, 2022.\n[Dut16] Aritra Dutta.\nWeighted low-rank approximation of matrices: some analytical and\nnumerical aspects. 2016.\n[EVDH12] Anders Eriksson and Anton Van Den Hengel. Eﬃcient computation of robust weighted\nlow-rank matrix approximations using the l 1 norm. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 34(9):1681–1690, 2012.\n[FLCT18] Baojie Fan, Xiaomao Li, Yang Cong, and Yandong Tang. Structured and weighted\nmulti-task low rank tracker. Pattern Recognition, 81:528–544, 2018.\n[FZC+22a] Zhiqiang Fu, Yao Zhao, Dongxia Chang, Yiming Wang, and Jie Wen. Latent low-rank\nrepresentation with weighted distance penalty for clustering. IEEE Transactions on\nCybernetics, 2022.\n[FZC+22b] Zhiqiang Fu, Yao Zhao, Dongxia Chang, Xingxing Zhang, and Yiming Wang.\nAuto-weighted low-rank representation for clustering.\nKnowledge-Based Systems,\n251:109063, 2022.\n16\n\n[FZCW21] Zhiqiang Fu, Yao Zhao, Dongxia Chang, and Yiming Wang. A hierarchical weighted\nlow-rank representation for image clustering and classiﬁcation. Pattern Recognition,\n112:107736, 2021.\n[GG11] Nicolas Gillis and Fran¸cois Glineur.\nLow-rank matrix approximation with weights\nor missing data is np-hard.\nSIAM Journal on Matrix Analysis and Applications,\n32(4):1149–1165, 2011.\n[GQSW22] Yeqi Gao, Lianke Qin, Zhao Song, and Yitan Wang. A sublinear adversarial training\nalgorithm. arXiv preprint arXiv:2208.05395, 2022.\n[GS22] Yuzhou Gu and Zhao Song.\nA faster small treewidth sdp solver.\narXiv preprint\narXiv:2211.06033, 2022.\n[GSY23] Yeqi Gao, Zhao Song, and Junze Yin. An iterative algorithm for rescaled hyperbolic\nfunctions regression. arXiv preprint arXiv:2305.00660, 2023.\n[GSYZ23] Yuzhou Gu, Zhao Song, Junze Yin, and Lichen Zhang.\nLow rank matrix com-\npletion via robust alternating minimization in nearly linear time.\narXiv preprint\narXiv:2302.11068, 2023.\n[HHC+21] Yen-Chang Hsu, Ting Hua, Sungen Chang, Qian Lou, Yilin Shen, and Hongxia Jin.\nLanguage model compression with weighted low-rank factorization. In International\nConference on Learning Representations, 2021.\n[HHW+22] Ting Hua, Yen-Chang Hsu, Felicity Wang, Qian Lou, Yilin Shen, and Hongxia Jin.\nNumerical optimizations for weighted low-rank estimation on language models. In\nProceedings of the 2022 Conference on Empirical Methods in Natural Language Pro-\ncessing, pages 1404–1416, 2022.\n[HLX+19] Yan Huang, Guisheng Liao, Yijian Xiang, Lei Zhang, Jie Li, and Arye Nehorai. Low-\nrank approximation via generalized reweighted iterative nuclear and frobenius norms.\nIEEE Transactions on Image Processing, 29:2244–2257, 2019.\n[HSK+24] Jerry Yao-Chieh Hu, Maojiang Su, En-Jui Kuo, Zhao Song, and Han Liu. Computa-\ntional limits of low-rank adaptation (lora) ﬁne-tuning for transformer models. In The\nThirteenth International Conference on Learning Representations, 2024.\n[HSWZ22] Hang Hu, Zhao Song, Omri Weinstein, and Danyang Zhuo. Training overparametrized\nneural networks in sublinear time. arXiv preprint arXiv:2208.04508, 2022.\n[HWG+24] Jerry Yao-Chieh Hu, Wei-Po Wang, Ammar Gilani, Chenyang Li, Zhao Song, and\nHan Liu. Fundamental limits of prompt tuning transformers: Universality, capacity\nand eﬃciency. arXiv preprint arXiv:2411.16525, 2024.\n[HWL+24a] Jerry Yao-Chieh Hu, Weimin Wu, Yi-Chen Lee, Yu-Chao Huang, Minshuo Chen, and\nHan Liu. On statistical rates of conditional diﬀusion transformers: Approximation,\nestimation and minimax optimality. arXiv preprint arXiv:2411.17522, 2024.\n[HWL+24b] Jerry Yao-Chieh Hu, Weimin Wu, Zhuoru Li, Sophia Pi, , Zhao Song, and Han Liu. On\nstatistical rates and provably eﬃcient criteria of latent diﬀusion transformers (dits).\nAdvances in Neural Information Processing Systems, 38, 2024.\n17\n\n[JLD+20] Xuan Ji, Jiuzhen Liang, Lan Di, Yunfei Xia, Zhenjie Hou, Zhan Huan, and Yuxi\nHuan.\nFabric defect fetection via weighted low-rank decomposition and laplacian\nregularization. Journal of Engineered Fibers and Fabrics, 15:1558925020957654, 2020.\n[JLSW20] Haotian Jiang, Yin Tat Lee, Zhao Song, and Sam Chiu-wai Wong. An improved cutting\nplane method for convex optimization, convex-concave games and its applications. In\nSTOC, 2020.\n[JLSZ23] Haotian Jiang, Yin Tat Lee, Zhao Song, and Lichen Zhang. Convex minimization with\ninteger minima in eO(n4) time. arXiv preprint arXiv:2304.03426, 2023.\n[JLZ+24] Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin\nAhn, Zhenhua Han, Amir H Abdi, Dongsheng Li, Chin-Yew Lin, et al. Minference\n1.0: Accelerating pre-ﬁlling for long-context llms via dynamic sparse attention. In The\nThirty-eighth Annual Conference on Neural Information Processing Systems, 2024.\n[JPT13] Gabriela Jeronimo, Daniel Perrucci, and Elias Tsigaridas. On the minimum of a poly-\nnomial function on a basic closed semialgebraic set and applications. SIAM Journal\non Optimization, 23(1):241–255, 2013.\n[JSWZ21] Shunhua Jiang, Zhao Song, Omri Weinstein, and Hengjie Zhang.\nFaster dynamic\nmatrix inverse for faster lps. In STOC, 2021.\n[JYCL15] Jielin Jiang, Jian Yang, Yan Cui, and Lei Luo. Mixed noise removal by weighted low\nrank model. Neurocomputing, 151:817–826, 2015.\n[KJS15] Alex Kulesza, Nan Jiang, and Satinder Singh.\nLow-rank spectral learning with\nweighted loss functions. In Artiﬁcial Intelligence and Statistics, pages 517–525. PMLR,\n2015.\n[KKK+23] Mohammad Ahmar Khan, Ghufran Ahmad Khan, Jalaluddin Khan, Taushif An-\nwar, Zubair Ashraf, Ibrahim Atoum, Naved Ahmad, Mohammad Shahid, Mohammad\nIshrat, and Abdulrahman Abdullah Alghamdi. Adaptive weighted low-rank sparse\nrepresentation for multi-view clustering. IEEE Access, 2023.\n[KLL+25] Yekun Ke, Xiaoyu Li, Yingyu Liang, Zhizhou Sha, Zhenmei Shi, and Zhao Song. On\ncomputational limits and provably eﬃcient criteria of visual autoregressive models: A\nﬁne-grained complexity analysis. arXiv preprint arXiv:2501.04377, 2025.\n[LBBM19] Peter Langenberg, Emilio Rafael Balda, Arash Behboodi, and Rudolf Mathar. On the\neﬀect of low-rank weights on adversarial robustness of neural networks. arXiv preprint\narXiv:1901.10371, 2019.\n[LHZC10] Yanen Li, Jia Hu, ChengXiang Zhai, and Ye Chen. Improving one-class collaborative\nﬁltering by incorporating rich user information.\nIn Proceedings of the 19th ACM\ninternational conference on Information and knowledge management, pages 959–968,\n2010.\n[LLR16] Yuanzhi Li, Yingyu Liang, and Andrej Risteski.\nRecovery guarantee of weighted\nlow-rank approximation via alternating minimization. In International Conference on\nMachine Learning, pages 2358–2367. PMLR, 2016.\n18\n\n[LLS+24] Yingyu Liang, Heshan Liu, Zhenmei Shi, Zhao Song, Zhuoyan Xu, and Junze Yin.\nConv-basis: A new paradigm for eﬃcient attention inference and gradient computation\nin transformers. arXiv preprint arXiv:2405.05219, 2024.\n[LLS+25a] Chenyang Li, Yingyu Liang, Zhenmei Shi, Zhao Song, and Tianyi Zhou.\nFourier\ncircuits in neural networks and transformers: A case study of modular arithmetic with\nmultiple inputs. In International Conference on Artiﬁcial Intelligence and Statistics,\n2025.\n[LLS+25b] Yingyu Liang, Jiangxuan Long, Zhenmei Shi, Zhao Song, and Yufa Zhou. Beyond lin-\near approximations: A novel pruning approach for attention matrix. In International\nConference on Learning Representations, 2025.\n[LLSS24a] Chenyang Li, Yingyu Liang, Zhenmei Shi, and Zhao Song. Exploring the frontiers of\nsoftmax: Provable optimization, applications in diﬀusion model, and beyond. arXiv\npreprint arXiv:2405.03251, 2024.\n[LLSS24b] Xiaoyu Li, Yingyu Liang, Zhenmei Shi, and Zhao Song. A tighter complexity analysis\nof sparseGPT. In Workshop on Machine Learning and Compression, NeurIPS 2024,\n2024.\n[LPW97] W-S Lu, S-C Pei, and P-H Wang. Weighted low-rank approximation of general com-\nplex matrices and its application in the design of 2-d digital ﬁlters. IEEE Transac-\ntions on Circuits and Systems I: Fundamental Theory and Applications, 44(7):650–655,\n1997.\n[LSS+24] Yingyu Liang, Zhizhou Sha, Zhenmei Shi, Zhao Song, and Yufa Zhou. Multi-layer\ntransformers gradient can be approximated in almost linear time.\narXiv preprint\narXiv:2408.13233, 2024.\n[LSSY24] Yingyu Liang, Zhenmei Shi, Zhao Song, and Chiwun Yang. Toward inﬁnite-long preﬁx\nin transformer. arXiv preprint arXiv:2406.14036, 2024.\n[LSSZ24] Yingyu Liang, Zhenmei Shi, Zhao Song, and Yufa Zhou.\nTensor attention train-\ning:\nProvably eﬃcient learning of higher-order transformers.\narXiv preprint\narXiv:2405.16411, 2024.\n[LSZ19] Yin Tat Lee, Zhao Song, and Qiuyi Zhang. Solving empirical risk minimization in the\ncurrent matrix multiplication time. In Conference on Learning Theory (COLT), pages\n2140–2157. PMLR, 2019.\n[LSZ23] Zhihang Li, Zhao Song, and Tianyi Zhou.\nSolving regularized exp, cosh and sinh\nregression problems. arXiv preprint, 2303.15725, 2023.\n[LWZ+16] Chenglong Li, Xiao Wang, Lei Zhang, Jin Tang, Hejun Wu, and Liang Lin. Weighted\nlow-rank decomposition for robust grayscale-thermal foreground detection.\nIEEE\nTransactions on Circuits and Systems for Video Technology, 27(4):725–738, 2016.\n[Mar08] Ivan Markovsky. Structured low-rank approximation and its applications. Automatica,\n44(4):891–909, 2008.\n19\n\n[MMH03] Jonathan H Manton, Robert Mahony, and Yingbo Hua. The geometry of weighted\nlow-rank approximations.\nIEEE Transactions on Signal Processing, 51(2):500–514,\n2003.\n[MU14] Ivan Markovsky and Konstantin Usevich. Software for weighted structured low-rank\napproximation.\nJournal of Computational and Applied Mathematics, 256:278–292,\n2014.\n[MVH07] Ivan Markovsky and Sabine Van Huﬀel.\nLeft vs right representations for solving\nweighted low-rank approximation problems. Linear algebra and its applications, 422(2-\n3):540–552, 2007.\n[MWLZ20] Dongmei Mo, Wai Keung Wong, Zhihui Lai, and Jie Zhou. Weighted double-low-\nrank decomposition with application to fabric defect detection. IEEE Transactions on\nAutomation Science and Engineering, 18(3):1170–1190, 2020.\n[MXZZ13] Deyu Meng, Zongben Xu, Lei Zhang, and Ji Zhao. A cyclic weighted median method\nfor l1 low-rank matrix factorization with missing entries. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence, pages 704–710, 2013.\n[PCP+23] Xinyu Pu, Hangjun Che, Baicheng Pan, Man-Fai Leung, and Shiping Wen. Robust\nweighted low-rank tensor approximation for multiview clustering with mixed noise.\nIEEE Transactions on Computational Social Systems, 2023.\n[PSDX14] Yigang Peng, Jinli Suo, Qionghai Dai, and Wenli Xu. Reweighted low-rank matrix\nrecovery and its application in image restoration. IEEE transactions on cybernetics,\n44(12):2418–2430, 2014.\n[QJS+22] Lianke Qin, Rajesh Jayaram, Elaine Shi, Zhao Song, Danyang Zhuo, and Shumo Chu.\nAdore: Diﬀerentially oblivious relational database operators. In VLDB, 2022.\n[QRS+22] Lianke Qin, Aravind Reddy, Zhao Song, Zhaozhuo Xu, and Danyang Zhuo. Adaptive\nand dynamic multi-resolution hashing for pairwise summations. In BigData, 2022.\n[QSZ23] Lianke Qin, Zhao Song, and Ruizhe Zhang. A general algorithm for solving rank-one\nmatrix sensing. arXiv preprint arXiv:2303.12298, 2023.\n[QSZZ23] Lianke Qin, Zhao Song, Lichen Zhang, and Danyang Zhuo. An online and uniﬁed\nalgorithm for projection matrix vector multiplication with application to empirical\nrisk minimization. In AISTATS, 2023.\n[Ren92a] James Renegar. On the computational complexity and geometry of the ﬁrst-order\ntheory of the reals. part i: Introduction. preliminaries. the geometry of semi-algebraic\nsets. the decision problem for the existential theory of the reals. Journal of symbolic\ncomputation, 13(3):255–299, 1992.\n[Ren92b] James Renegar. On the computational complexity and geometry of the ﬁrst-order\ntheory of the reals. part ii: The general decision problem. preliminaries for quantiﬁer\nelimination. Journal of Symbolic Computation, 13(3):301–327, 1992.\n[RSW16] Ilya Razenshteyn, Zhao Song, and David P Woodruﬀ. ’weighted low rank approx-\nimations with provable guarantees’. In Proceedings of the forty-eighth annual ACM\nsymposium on Theory of Computing, pages 250–263, 2016.\n20\n\n[SJ03] Nathan Srebro and Tommi Jaakkola.\nWeighted low-rank approximations. In Pro-\nceedings of the 20th international conference on machine learning (ICML-03), pages\n720–727, 2003.\n[SJS+11] Fanhua Shang, LC Jiao, Jiarong Shi, Maoguo Gong, and RH Shang. Fast density-\nweighted low-rank approximation spectral clustering. Data Mining and Knowledge\nDiscovery, 23:345–378, 2011.\n[SLVH04] Mieke Schuermans, Philippe Lemmerling, and Sabine Van Huﬀel. Structured weighted\nlow rank approximation. Numerical linear algebra with applications, 11(5-6):609–618,\n2004.\n[SMF+24] Zhenmei Shi, Yifei Ming, Ying Fan, Frederic Sala, and Yingyu Liang. Domain gener-\nalization via nuclear norm regularization. In Conference on Parsimony and Learning,\npages 179–201. PMLR, 2024.\n[SMN+24] Zhenmei Shi, Yifei Ming, Xuan-Phi Nguyen, Yingyu Liang, and Shaﬁq Joty. Discov-\nering the gems in early layers: Accelerating long-context llms with 1000x input token\nreduction. arXiv preprint arXiv:2409.17422, 2024.\n[Son19] Zhao Song. Matrix theory: optimization, concentration, and algorithms. The Univer-\nsity of Texas at Austin, 2019.\n[SWXL24] Zhenmei Shi, Junyi Wei, Zhuoyan Xu, and Yingyu Liang. Why larger language models\ndo in-context learning diﬀerently? arXiv preprint arXiv:2405.19592, 2024.\n[SWYZ23] Zhao Song, Yitan Wang, Zheng Yu, and Lichen Zhang.\nSketching for ﬁrst order\nmethod: Eﬃcient algorithm for low-bandwidth channel and vulnerability. In ICML,\n2023.\n[SWZ+20] Le Sun, Feiyang Wu, Tianming Zhan, Wei Liu, Jin Wang, and Byeungwoo Jeon.\nWeighted nonlocal low-rank tensor decomposition method for sparse unmixing of hy-\nperspectral images. IEEE Journal of Selected Topics in Applied Earth Observations\nand Remote Sensing, 13:1174–1188, 2020.\n[SXZ22] Zhao Song, Zhaozhuo Xu, and Lichen Zhang. Speeding up sparsiﬁcation using inner\nproduct search data structures. arXiv preprint arXiv:2204.03209, 2022.\n[SY21] Zhao Song and Zheng Yu. Oblivious sketching-based central path method for linear\nprogramming. In International Conference on Machine Learning, pages 9835–9847.\nPMLR, 2021.\n[SYYZ22] Zhao Song, Xin Yang, Yuanyuan Yang, and Tianyi Zhou. Faster algorithm for struc-\ntured john ellipsoid computation. arXiv preprint arXiv:2211.14407, 2022.\n[SYYZ23] Zhao Song, Mingquan Ye, Junze Yin, and Lichen Zhang. Eﬃcient alternating min-\nimization with applications to weighted low rank approximation.\narXiv preprint\narXiv:2306.04169, 2023.\n[SZZ21] Zhao Song, Lichen Zhang, and Ruizhe Zhang. Training multi-layer over-parametrized\nneural network in subquadratic time. arXiv preprint arXiv:2112.07628, 2021.\n21\n\n[TH21] Elena Tuzhilina and Trevor Hastie. Weighted low rank matrix approximation and\nacceleration. arXiv preprint arXiv:2109.11057, 2021.\n[TWZL16] Chang Tang, Pichao Wang, Changqing Zhang, and Wanqing Li. Salient object detec-\ntion via weighted low rank matrix recovery. IEEE Signal Processing Letters, 24(4):490–\n494, 2016.\n[VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez,  Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in\nneural information processing systems, 30, 2017.\n[WJ06] Karl Werner and Magnus Jansson. Reduced rank linear regression and weighted low\nrank approximations. IEEE transactions on signal processing, 54(6):2063–2075, 2006.\n[WL17] Xiaotao Wang and Fang Liu. Weighted low-rank representation-based dimension re-\nduction for hyperspectral image classiﬁcation. IEEE Geoscience and Remote Sensing\nLetters, 14(11):1938–1942, 2017.\n[WLLZ18] Qidi Wu, Yibing Li, Yun Lin, and Ruolin Zhou. Weighted sparse image classiﬁcation\nbased on low rank representation. Computers, Materials & Continua, 56(1), 2018.\n[WTL18] Tyler Wilson, Pang-Ning Tan, and Lifeng Luo. A low rank weighted graph convo-\nlutional approach to weather prediction. In 2018 IEEE International Conference on\nData Mining (ICDM), pages 627–636. IEEE, 2018.\n[WWZ+19] Lei Wang, Bangjun Wang, Zhao Zhang, Qiaolin Ye, Liyong Fu, Guangcan Liu, and\nMeng Wang. Robust auto-weighted projective low-rank and sparse recovery for visual\nrepresentation. Neural Networks, 117:201–215, 2019.\n[WY24] David Woodruﬀand Taisuke Yasuda. Reweighted solutions for weighted low rank\napproximation. In International Conference on Machine Learning. PMLR, 2024.\n[WZX+18] Jie Wen, Bob Zhang, Yong Xu, Jian Yang, and Na Han. Adaptive weighted nonneg-\native low-rank representation. Pattern Recognition, 81:326–340, 2018.\n[XSL24] Zhuoyan Xu, Zhenmei Shi, and Yingyu Liang. Do large language models have com-\npositional ability? an investigation into limitations and scalability. In Conference on\nLanguage Modeling, 2024.\n[XXF+21] Zhengqin Xu, Huasong Xing, Shun Fang, Shiqian Wu, and Shoulie Xie.\nDouble-\nweighted low-rank matrix recovery based on rank estimation. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision, pages 172–180, 2021.\n[YZLS22] Baturalp Yalcin, Haixiang Zhang, Javad Lavaei, and Somayeh Sojoudi. Factorization\napproach for low-complexity matrix completion problems: Exponential number of\nspurious solutions and failure of gradient methods. In International Conference on\nArtiﬁcial Intelligence and Statistics, pages 319–341. PMLR, 2022.\n[Zha22] Lichen Zhang. Speeding up optimizations via data structures: Faster search, sample\nand maintenance. Master’s thesis, Carnegie Mellon University, 2022.\n[ZQZ+19] Jianwei Zheng, Mengjie Qin, Xiaolong Zhou, Jiafa Mao, and Hongchuan Yu. Eﬃ-\ncient implementation of truncated reweighting low-rank matrix approximation. IEEE\nTransactions on Industrial Informatics, 16(1):488–500, 2019.\n22\n\n[ZWH+22] Yang Zhang, Yao Wang, Zhi Han, Yandong Tang, et al. Eﬀective tensor completion\nvia element-wise weighted low-rank tensor train with overlapping ket augmentation.\nIEEE Transactions on circuits and systems for video technology, 32(11):7286–7300,\n2022.\n[ZYLS24] Haixiang Zhang, Baturalp Yalcin, Javad Lavaei, and Somayeh Sojoudi. A new com-\nplexity metric for nonconvex rank-one generalized matrix completion. Mathematical\nProgramming, 207(1):227–268, 2024.\n23\n",
  "metadata": {
    "source_path": "papers/arxiv/When_Can_We_Solve_the_Weighted_Low_Rank_Approximation_Problem_in_Truly\n__Subquadratic_Time_3b98a933d0462ec9.pdf",
    "content_hash": "3b98a933d0462ec97ab33486143d211e5b7e8558bb84769aaef75eb9861a304a",
    "arxiv_id": null,
    "title": "When_Can_We_Solve_the_Weighted_Low_Rank_Approximation_Problem_in_Truly\n__Subquadratic_Time_3b98a933d0462ec9",
    "author": "",
    "creation_date": "D:20250224213330-05'00'",
    "published": "20250224213330-05'00'",
    "pages": 24,
    "size": 283992,
    "file_mtime": 1740470205.7884974
  }
}