{
  "text": "BigMac: A Communication-Efficient Mixture-of-Experts Model Structure for Fast\nTraining and Inference\nZewen Jin1 2*, Shengnan Wang2*, Jiaan Zhu1 3, Hongrui Zhan1,\nYouhui Bai2, Lin Zhang2, Zhenyu Ming2, Cheng Li1 3\n1University of Science and Technology of China\n2Huawei Technologies\n3Institute of Artificial Intelligence, Hefei Comprehensive National Science Center\nzevin@mail.ustc.edu.cn, wangshengnan12@huawei.com, andyzhu@mail.ustc.edu.cn, zhr2001@mail.ustc.edu.cn,\nbaiyouhui@huawei.com, zhang.lin4@huawei.com, mingzhenyu1@huawei.com, chengli7@ustc.edu.cn\nAbstract\nThe\nMixture-of-Experts\n(MoE)\nstructure\nscales\nthe\nTransformer-based large language models (LLMs) and\nimproves their performance with only the sub-linear in-\ncrease in computation resources. Recently, a fine-grained\nDeepSeekMoE structure is proposed, which can further\nimprove the computing efficiency of MoE without perfor-\nmance degradation. However, the All-to-All communication\nintroduced by MoE has become a bottleneck, especially\nfor the fine-grained structure, which typically involves\nand activates more experts, hence contributing to heavier\ncommunication overhead.\nIn this paper, we propose a novel MoE structure named Big-\nMac, which is also fine-grained but with high communication\nefficiency. The innovation of BigMac is mainly due to that\nwe abandon the communicate-descend-ascend-communicate\n(CDAC) manner used by fine-grained MoE, which leads\nto the All-to-All communication always taking place at\nthe highest dimension. Instead, BigMac designs an efficient\ndescend-communicate-communicate-ascend (DCCA) man-\nner. Specifically, we add a descending and ascending pro-\njection at the entrance and exit of the expert, respectively,\nwhich enables the communication to perform at a very low\ndimension. Furthermore, to adapt to DCCA, we re-design the\nstructure of small experts, ensuring that the expert in BigMac\nhas enough complexity to address tokens. Experimental re-\nsults show that BigMac achieves comparable or even better\nmodel quality than fine-grained MoEs with the same number\nof experts and a similar number of total parameters. Equally\nimportantly, BigMac reduces the end-to-end latency by up\nto 3.09× for training and increases the throughput by up to\n3.11× for inference on state-of-the-art AI computing frame-\nworks including Megatron, Tutel, and DeepSpeed-Inference.\nIntroduction\nIncreasing the size of Transformer-based large language\nmodels (LLMs) can continuously improve downstream ap-\nplication performance. Such a phenomenon, known as the\nscaling law, has been demonstrated by the auto-regressive\ndense models such as GPT-series (OpenAI et al. 2024) and\nLlama-series (Dubey et al. 2024). However, this comes at\nthe price of higher computing complexity and more resource\nconsumption. Fortunately, the Mixture-of-Experts (MoE)\n*Zewen and Shengnan equally contributed to this work.\nFigure 1: Convergence result comparison of MoE models\nwith three structures. GPT-Fine-Grained takes 38.9 hours\nto reach the target perplexity of 13.69, while GPT-BigMac\nspends only 22.8 hours (1.7× faster). GPT-Vanilla fails to\nconverge to the target perplexity under time budget.\ntechnique, capable of expanding the model size tens or even\nhundreds of times without significantly increasing the com-\nputation, is widely used in various emerging huge mod-\nels, such as GShard (Lepikhin et al. 2020), GLaM (Du\net al. 2022), Switch Transformer (Fedus, Zoph, and Shazeer\n2022), and Mixtral (Jiang et al. 2024), each of which con-\nsists of hundreds of billion parameters or even beyond.\nRecently, DeepSeekMoE (Dai et al. 2024), a fine-grained\nand more parameter-efficient MoE structure, has been pro-\nposed. Compared to conventional MoE models, for the same\nmodel size, DeepSeekMoE has significantly more experts\nper MoE layer and fewer parameters per expert. It was\ndemonstrated that such a new structure can achieve com-\nparable or even better results than conventional MoE mod-\nels with much less time complexity and hence it is adopted\nby many later released models, such as Qwen2 (Yang et al.\n2024) and DeepSeek-v2 (DeepSeek-AI et al. 2024).\nHowever, the MoE faces a serious All-to-All communi-\ncation bottleneck during both training and inference. This\nis mainly because of the underlying expert parallelism (EP)\nstrategy, which assigns experts to several hardware ac-\ncelerators to avoid out-of-memory errors or improve effi-\nciency (Fedus, Zoph, and Shazeer 2022). As a result, for\neach MoE layer, two All-to-All communication steps are\nintroduced for dispatching tokens to their best-fit experts,\nwhich may be stored in the other accelerators and then gath-\nering the results back before proceeding to the next layer.\narXiv:2502.16927v1  [cs.LG]  24 Feb 2025\n\nFigure 2: The MoE layers of different structures. Here, N represents the number of experts in the Vanilla MoE model and\nACT represents the activation function like ReLU. W↓and W↑represent the descending and ascending projection matrix of an\nexpert, respectively. W ′\n↓and W ′\n↑represent the projection matrices introduced in BigMac.\nRecent studies have already found that the All-to-All com-\nmunication is a key issue leading to the low efficiency of\nMoE model training and inference (Hu et al. 2024). Even\nworse, this bottleneck will be more pronounced in the fine-\ngrained MoE structure, which generally requires to activate\nmore experts to ensure performance.\nIn this paper, we propose a novel efficient MoE structure,\nnamed BigMac, which is also fine-grained but can greatly\nreduce the All-to-All communication overhead. Note that\nexisting fine-grained MoE models adopt the communicate-\ndescend-ascend-communicate (CDAC) manner, performing\nthe All-to-All at a high dimension, leading to a heavy com-\nmunication overhead. In contrast, BigMac designs an effi-\ncient descend-communicate-communicate-ascend (DCCA)\nmanner, which is capable of performing the All-to-All com-\nmunication at a very low dimension. Furthermore, to adapt\nto DCCA, we further re-design the structure of the small ex-\npert in BigMac, ensuring the complexity of each single ex-\npert for the overall performance of the whole model. Specif-\nically, each expert in BigMac is composed of an ascending\nprojection and a descending projection with an activation in\nbetween, which is the opposite of fine-grained MoE models.\nBriefly, we make the following contributions:\n• We propose a novel MoE structure named BigMac,\nwhich can greatly improve the efficiency of MoE models\nfor both training and inference. In addition, BigMac no\nlonger suffers from problems such as the limited expert\ncapacity and limited top k, which are common restric-\ntions in the existing MoE structures.\n• We\ndesign\na\ndescend-communicate-communicate-\nascend (DCCA) strategy to ensure that the communi-\ncation is always executed at the very low dimension,\nhence the communication overhead is greatly reduced.\nTo guarantee the computing efficiency, BigMac adopts\nthe idea of fine-grained MoE models, namely, each MoE\nlayer is composed of a large number of small experts,\nwhile the structure of each small expert is re-designed to\nadapt to the DCCA strategy.\n• We pre-train the MoE models with different MoE struc-\ntures and show that BigMac converges faster than the\nother structures (shown in Figure 1). The results on mul-\ntiple downstream tasks show that BigMac can achieve\ncomparable or better performance against other base-\nlines using the same amount of resources. Moreover,\nevaluations on state-of-the-art distributed training / in-\nference frameworks, including Megatron, Tutel,\nand\nDeepSpeed-Inference, show that BigMac can signifi-\ncantly mitigate the communication overhead, reducing\nthe end-to-end latency by up to 3.09× for training and\nincreasing the throughput by up to 3.11× for inference.\nRelated Work and Motivation\nFine-grained MoE. Starting from GShard (Lepikhin et al.\n2020), the Mixture-of-Experts (MoE) technology has been\napplied to the Transformer architecture, allowing for a sig-\nnificant increase in the number of parameters with only\na sub-linear increase in computational resources. As illus-\ntrated in Figure 2a, the dense Feed-Forward Network (FFN)\nmodules in Transformer are replaced with MoE sub-layers,\neach consisting of multiple parallel experts. The number of\nexperts activated by each token is defined as top k. How-\never, it is challenging for these conventional MoE models to\nexploit expert specialization, since they are based on the top-\n1 or top-2 routing strategies with the coarse-grained expert\nactivation. To address this issue, a new fine-grained MoE ar-\nchitecture is proposed in DeepSeekMoE (Dai et al. 2024). To\nimprove expert specialization, DeepSeekMoE maintains the\nsame number of parameters as the conventional MoE mod-\nels, while splitting experts into finer granularity and choos-\ning a higher top k for token distribution. Such an architec-\nture with a large number of smaller experts has been adopted\nby DeepSeek-V2 (DeepSeek-AI et al. 2024) and Qwen2-\n57B-A14B (Yang et al. 2024), demonstrating better model\nquality and computation efficiency than the conventional\nones with a small number of large experts.\nNevertheless, this fine-grained MoE architecture faces a\nsevere communication problem for its training and inference\ntasks, due to the following reasons. First of all, to improve\ncomputation efficiency and cope with the single hardware\naccelerator’s memory limit, the common practice is to lever-\nage Expert Parallelism (EP) for assigning experts to differ-\nent accelerators (Fedus, Zoph, and Shazeer 2022). Second,\n\nExpert Config\nTraining (ms)\nInference (ms)\ntop k/#experts\nA2A\nRatio\nA2A\nRatio\n1 / 64\n336.9\n59.9%\n94.9\n51.2%\n2 / 64\n535.6\n71.3%\n132.9\n65.2%\n4 / 64\n1,089.5\n84.1%\n268.7\n79.2%\n6 / 64\n1,692.8\n89.1%\n457.0\n86.5%\n8 / 64\n2,383.4\n91.8%\n696.5\n90.6%\nTable 1: The All-to-All latency and its ratio in training and\ninference task of MoE models with small experts across var-\nious top k. The evaluation is conducted under the expert par-\nallelism degree as 32 on 32 devices. The All-to-All duration\nincreases by 7.1x and 7.3x when top k is 8, corresponding\nto proportions of 91.8% and 90.6%.\nEP requires injecting two costly All-to-All communication\noperations per MoE layer for distributing tokens to various\nexperts and also gathering results for proceeding the compu-\ntation to the next layer (green boxes in Figure 2b). Third, the\nAll-to-All communication already accounts for a large por-\ntion of the overall training or inference time, while its over-\nhead as well as time ratio increases with the top k value.\nTable 1 shows the All-to-All time costs and time ratios of\na fine-grained MoE model with 64 experts per layer and\nvarious top k choices for training and inference jobs. When\ntop k is 1, the All-to-All overhead respectively contributes\nto 59.9% and 51.2% of the end-to-end time in training and\ninference. However, when top k is 8, the All-to-All duration\nincreases by 7.1× and 7.3×, almost dominating the entire\ntraining and inference tasks, with proportions increasing to\nan astonishing 91.8% and 90.6%, respectively. In conclu-\nsion, considering that in the future new MoE models, it is\nvery likely that their experts will become smaller and more\nnumerous and the value of top k will be larger, the optimiza-\ntion of All-to-All communication becomes urgent.\nSystem-wise Optimizations. Fortunately, there have been\na few initial attempts in the systems community to improve\nthe scheduling of EP-enabled parallel training or inference.\nFor instance, Lina (Li et al. 2023) leverages a fine-grained\nscheduling strategy to avoid bandwidth contention between\nAll-to-All communication and All-Reduce communication.\nTutel (Hwang et al. 2023) schedules transmission jobs in\na network topology-aware fashion to make full use of the\nintra-node and inter-node network bandwidth. Furthermore,\nFasterMoE (He et al. 2022) and Tutel partition the input to-\nkens into small chunks and overlap All-to-All communica-\ntion with FFN computation in each expert. However, these\nefforts are designed for conventional MoE models, and when\nacting on fine-grained ones, their effect will be quite limited.\nThis is mainly due to the fact that the amount of computa-\ntion per expert is drastically reduced in the fine-grained MoE\nmodel, yet the amount of communication dominates the en-\ntire pipeline, and thus the space for bandwidth optimization\nand overlap optimization becomes very small.\nCommunication Volume Reduction. To address the com-\nmunication bottleneck that is difficult to resolve at the sys-\ntem level, some have begun advocating data compression\nNotation\nDescription\nb\nglobal batch size\ns\nsequence length\nh\nhidden dimension\nh f\nFFN intermediate hidden dimension\ne\nnumber of experts\ntop k\nnumber of experts to route to\nf\nexpert capacity factor\nep\nexpert parallelism degree\ntp\ntensor parallelism degree\nr\ndownscaling factor\nTable 2: Description of the notations used in this paper.\ntechniques. For instance, ScheMoE (Shi et al. 2024) applies\nthe ZFP compression algorithm (Lindstrom 2014) to tokens\nbefore transmission and indicates that such a compression\ntechnique can significantly reduce the All-to-All communi-\ncation overhead and accelerate MoE training. However, such\nlossy MoE structure-agnostic compression schemes can lead\nto a decline in model quality, making them unsuitable for\ndownstream tasks with high precision requirements. Fur-\nthermore, the extra compression and decompression steps\ncan bring non-negligible computational overhead. There-\nfore, there is an urgent need for a new fine-grained MoE ar-\nchitecture from an algorithmic perspective with the follow-\ning advantages: 1) significantly reducing data volumes trans-\nferred in All-to-All communication; 2) maintaining the same\nmodel quality; and 3) avoiding extra computation overhead,\ncomparing to the state-of-the-art fine-grained MoE struc-\ntures, as well as some of the above optimizations.\nBigMac: Communication-Efficient MoE\nStructure\nIn this paper, we propose BigMac, a novel MoE structure\nthat eliminates the well-known All-to-All communication\nbottleneck. Note that BigMac builds atop the success of\nfine-grained MoE models such as DeepSeekMoE and Qwen,\nwhere it also assigns a large number of small experts for\neach MoE layer, as shown in Figure 2c. However, beyond\nthis similarity, BigMac has the following two main differ-\nences in structure that reflect its design rationales, compared\nto fine-grained ones.\n• Low-dimensional communication: we scale down the\ninput/output tokens of experts to decrease the hidden di-\nmension of the tokens to transfer, which greatly reduces\nthe All-to-All communication overhead.\n• Performance assurance: to adapt to the decreased di-\nmension of input/output tokens, we have to re-design\nthe structure of each expert, using reversed projections\nto avoid the expert parameter count decreasing syn-\nchronously with the dimension and to align with the fine-\ngrained MoE in terms of the total parameter count, to\navoid diminishing the model quality.\nBelow, we will detail the BigMac’s design with necessary\nnotations, summarized in Table 2.\n\nDCCA: Low-dimensional Communication Strategy\nBigMac’s efficient communication strategy is motivated by\nthe estimation of the All-to-All communication overhead in\neach MoE layer of the fine-grained MoE models. This over-\nhead can be described as\nC = 2 × top k × ep −1\nep\nbsh,\n(1)\nwhich is proportional to the standard hidden dimension\nh. For the fine-grained MoE model, as shown in Fig-\nure 2b, the model follows a communicate-descend-ascend-\ncommunicate (CDAC) manner, namely, the dimension of\nthe tokens will be scaled down by a descending projec-\ntion after the first All-to-All communication, and further\nbe scaled up before the second All-to-All communication.\nTherefore, actually the fine-grained MoE model always\ntransmits the token at the highest dimension, contributing\nto the serious overhead analyzed previously. Inspired by this\nfact, we ask a key question: is it possible for models like fine-\ngrained MoEs communicate at low-dimensional level while\nmaintaining the overall performance without degradation?\nTo this end, as shown in Figure 2c, at each MoE layer,\nBigMac moves the descending and ascending projections\noutside of every small expert and places the descending\nprojection before the first All-to-All operation for remark-\nably scaling down tokens sent to their best-fit experts. This\nchange allows the communication to happen at the lowest\ndimension. Following this, we place the ascending projec-\ntion after the second All-to-All operation to scale up the to-\nkens to their standard sizes. In contrast to the above CDAC\nmanner used in fine-grained MoE models, BigMac follows a\ndescend-communicate-communicate-ascend (DCCA) man-\nner. Within the DCCA execution, the whole process of the\nMoE module is described by the following equation:\nx′ = xW ′\n↓;\ny′ =\nX\ni∈T\npi(x)Ei(x′);\ny = y′W ′\n↑.\n(2)\nHere, x and y represent the output and input of two consec-\nutive attention layers, W ′\n↓and W ′\n↑are the descending and\nascending projection matrices, T refers to the set of top k\nexperts for token distribution, Ei refers to the expert com-\nputation in BigMac, and pi refers to the gate-value of acti-\nvating the ith expert. Note that we can choose to use either\nx or x′ as the input of the gating function for token routing.\nHere, we choose x, the vector before downscaling for rout-\ning, since the routing function is computationally efficient\nand a high-dimensional input vector generally leads to more\naccurate routing. In conclusion, DCCA reduces C in Equa-\ntion 1 into a much smaller C′ by changing h to rh, where\nr is the downscaling factor. Later, we will explain the value\nassignment to r and overall communication savings.\nBigMac Expert Design\nBased on the DCCA strategy, following the expert struc-\nture of the fine-grained MoE models is impractical. Other-\nwise, the expert will have much fewer parameters and con-\nsequently hurt model quality. Recall that expert computation\ncan be described as E(x) = σ(xWh×h f)Wh f×h, where\nσ is an activation function, h is the dimension of the in-\nput/output tokens and h f refers to the intermediate dimen-\nsion. Compared to CDAC, DCCA significantly reduces the\ninput/output dimension h, resulting in a smaller E(x) with\nthe same intermediate dimension h f.\nAs a result, to align BigMac’s model size to that of fine-\ngrained MoE models, we should increase the dimension h f.\nThe specific structure of the expert designed for adapting the\nDCCA strategy is shown in Figure 2c. From the appearance,\nit is closer to the conventional MoE structure in Figure 2a,\nand it can be seen as swaping the two projection matrices\nof fine-grained MoE experts in Figure 2b. In this way, the\nexpert in the fine-grained MoE in Equation 3 can be replaced\nwith the one in BigMac as shown in Equation 4:\nEi(x) = σ(xWi,↓)Wi,↑,\n(3)\nEi(x) = σ(xWi,↑)Wi,↓.\n(4)\nIt can be verified that the BigMac expert involves the same\nsize and same computational complexity compared with the\nexpert in fine-grained MoE.\nAdvantages Beyond Efficient Communication\nExcept communication efficiency, BigMac further possesses\nmany beneficial characteristics.\nEnabling Dropless Token Routing.\nIn both training and\ninference phases of MoE, the token routing imbalance prob-\nlem occurs frequently, and it results in a severe straggler\nproblem. To reduce the overhead brought by the imbalanced\nrouting, most of the existing MoE models will set a threshold\nfor the expert capacity (Fedus, Zoph, and Shazeer 2022), de-\ntermined by the expert capacity factor f, which is often set\nin a range from 1 to 1.25. Each expert will drop the tokens\nexceeding the expert capacity. It was demonstrated in (San-\nseviero et al. 2023) that the quality of the MoE model can\nbe continuously improved by increasing the capacity factor,\nwhich implies that token dropping is harmful for model’s\ngeneration. To ensure the performance, the recently pro-\nposed DeepSeekMoE and Mixtral remove the expert capac-\nity limit, at the cost of high communication overhead (Dai\net al. 2024; Jiang et al. 2024; Xue et al. 2024). Fortunately,\nthe communication overhead has been greatly mitigated in\nBigMac. The increased token transmission brought by re-\nmoving expert capacity limit will not significantly affect the\noverall training or inference efficiency.\nEnabling Flexible Selection of top k.\nThe number of ac-\ntivated experts, top k, is another key factor affecting model\nquality and overall latency. To some extent, a larger top k\ncontributes to better model performance (Dai et al. 2024).\nHowever, a larger top k corresponds to a heavier communi-\ncation overhead, leading to lower efficiency for training and\ninference. Taking this into account, the existing MoE mod-\nels generally select a relatively small top k. Considering the\nhigh efficiency of BigMac in both computation and commu-\nnication, BigMac is able to withstand a much larger top k to\nenhance the performance. Hence, BigMac provides a more\nflexible choice for practitioners.\n\nMetrics\nGPT-Fine-Grained\nGPT-BigMac\n#Param\n(4h2 + 8h + (2rh2 + 2rh)e)l + (v + e + 2)h\n(4h2 + 8h + (2rh2 + 2rh)e)l + (v + e + 2)h + 2rlh2\n#FLOPs\n12bslh2(2 + s\nh +\nv\n2lh + rtop k)\n12bslh2(2 + s\nh +\nv\n2lh + rtop k) + 12rbslh2\n#A2A\n8bslhtop k ep−1\nep\n8bslhtop k ep−1\nep r\nTable 3: Statistics of two MoE models. #Param refers to the number of parameters, #FLOPs refers to the number of floating-\npoint operations of an iteration for different MoE structures, and #A2A refers to the transfer size of All-to-All communication.\nMetrics\nGPT-Fine-Grained\nGPT-BigMac\n#Param\n3.73B\n3.78B (+1.35%)\n#FLOPs\n3,490.67 T\n3,649.00 T (+4.54%)\n#A2A\n1,488.00 GB\n372.00 GB (-75.00%)\nTable 4: Statistics for two MoE models with BF16 precision\nand an expert parallelism degree of 32 on 32 devices.\nHyper-Params\nValues\n#Layers l\n24\n#Heads a\n16\nHidden Dimension h\n2,048\nSequence Length s\n2,048\nVocabulary Size v\n50,257\nGlobal Batch Size b\n0.5 M\nDropout Rate\n0.1\nExpert Capacity Factor f\n1.2\nLoad Balance Type\naux loss\nBalance Coefficient α\n0.001\nOptimizer\nAdam\nAdam ϵ, β\n1e-8, (0.9,0.95)\nWeight Decay\n0.1\nLearning Rate\n3.0e-4\nMinimum Learning Rate\n3.0e-5\nLearning Decay Steps\n5,200\nLearning Rate Decay Style\ncosine\nWarmup Steps\n1,200\nGradient Clipping\n1.0\nRandom Seed\n1,234\nTable 5: Hyper-parameters of pre-training to compare the\nvalidation perplexity curves in Figure 1.\nAnalysis of Different MoE Structures\nTo understand how BigMac differs from the existing fine-\ngrained MoE structure, we analyze the parameter size and\nthe number of FLOPs as well as the communication over-\nhead for different MoE structures in Table 3. It indicates\nthat the two additional projection matrices in BigMac can\nsignificantly reduce the All-to-All transmission size by a ra-\ntio of (1 −r) while involving negligible overhead. For a\nmore intuitive elaboration, we show the concrete numbers\nwith GPT3-XL as the base model in Table 4, where we acti-\nvate 8 experts out of 64 experts and set the downscaling fac-\ntor r as 0.25, considering a similar setting of DeepSeek-V2,\ni.e., scaling down from 5,120 to 1,536. Table 4 shows that\nthe additional scaling matrices introduce only 4.54% FLOPs\nwhile reducing up to 75% communication overhead.\nPre-Training and Downstream Evaluation\nPre-Training Tasks\nTo show the acceleration of training convergence with con-\nstant model quality, we first pre-train three MoE models\nwith different MoE structures, namely GPT-Vanilla, GPT-\nFine-Grained, and GPT-BigMac, all of which use GPT3-\nXL as the base model. Vanilla represents the conventional\nMoE with large experts, Fine-Grained refers to the MoE\nmodel with small experts, while BigMac is our design. For\na fair comparison, we keep the same parameter size of\nMoE layers across the three models. We use the Wikipedia\ndataset (Wikimedia 2024) containing 3.6 B tokens to train\nthese models on Megatron (NVIDIA 2019), one of the state-\nof-the-art LLM training frameworks.\nFigure 1 shows the curve of validation perplexity of pre-\ntraining, indicating that GPT-BigMac converges much faster\nthan others and achieves the lowest validation perplexity\nwithin the same time. For example, to achieve the same val-\nidation perplexity of 13.69, GPT-Fine-Grained requires 38.9\nhours while GPT-BigMac only needs 22.8 hours, which is\n1.71× faster. In addition, among the three model structures,\nGPT-Vanilla fails to converge to the same validation perplex-\nity under the time budget, indicating that with the same pa-\nrameter size, the MoE structure with small experts outper-\nforms the conventional MoE. Further, with the evaluation\non WikiText2 (Merity et al. 2016), GPT-BigMac achieves\nthe perplexity score of 17.4, while GPT-Vanilla and GPT-\nFine-Grained get 27.4 and 17.9, respectively. The hyper-\nparameters for pre-training are shown in Table 5 and the\ndegree of Tensor Parallelism, Expert Parallelism, and Data\nParallelism is set as 4, 4, and 2, respectively.\nDownstream Tasks\nTo demonstrate how BigMac impacts the model quality on\ndownstream tasks, we utilized a larger dataset named Open-\nWebText2 dataset (EleutherAI 2020) with 14.8 B tokens.\nFirst, we compare the performance after training for the\nsame duration (8 days) based on the hyper-parameters in Ta-\nble 5. We evaluate the fine-grained and BigMac variants,\nwhich are based on GPT3-XL, on eight popular zero-shot\ntasks, including four long-term dependence prediction tasks\n(LAMBADA (Paperno et al. 2016), PTB (Marcus, San-\ntorini, and Marcinkiewicz 1993), WikiText103 and Wiki-\nText2 (Merity et al. 2016)) and four question answering\n\nMoE\nStructure\nPTB\n(PPL↓)\nWikiText-\n103 (PPL↓)\nWikiText2\n(PPL↓)\nLAMBADA\n(ACC↑)\nHellaSwag\n(ACC↑)\nWinoGrande\n(ACC↑)\nPIQA\n(ACC↑)\nRACE-H\n(ACC↑)\nFine-Grained\n51.0\n18.2\n16.8\n39.9\n31.6\n50.7\n65.1\n30.5\nBigMac\n34.9\n16.8\n15.8\n40.8\n33.2\n51.1\n65.2\n31.3\nTable 6: Downstream results for different MoE models (based on GPT3-XL) after training with the same time.\nMoE\nStructure\nPTB\n(PPL↓)\nWikiText-\n103 (PPL↓)\nWikiText2\n(PPL↓)\nLAMBADA\n(ACC↑)\nHellaSwag\n(ACC↑)\nWinoGrande\n(ACC↑)\nPIQA\n(ACC↑)\nRACE-H\n(ACC↑)\nVanilla\n57.6\n22.3\n20.1\n33.8\n28.7\n50.3\n61.1\n29.5\nFine-Grained\n67.7\n19.1\n17.9\n38.3\n31.0\n49.5\n65.0\n29.8\nBigMac\n52.3\n19.1\n17.7\n37.6\n30.8\n51.3\n64.2\n30.7\nTable 7: Downstream results for different MoE models (based on GPT3-Medium) after training with the same number of tokens.\nDepth\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\nFine-Grained\n99.1\n99.3\n99.0\n98.6\n98.4\n98.3\n98.3\n98.2\n97.9\nBigMac\n100.0\n99.9\n99.4\n99.0\n98.8\n98.6\n98.5\n98.3\n98.1\nTable 8: Recall scores of NeedleInAHaystack for different MoE models after training with the same number of tokens.\ntasks (PIQA (Bisk et al. 2020), HellaSwag (Zellers et al.\n2019) and WinoGrande (Sakaguchi et al. 2019), and RACE-\nH (Lai et al. 2017)). Table 6 shows the results of the eight\ndownstream tasks in terms of accuracy (ACC) and perplex-\nity (PPL). It shows that after training with the same time and\nGPU resources, GPT-BigMac gives a better model quality.\nNext, we further compare the performance after train-\ning for the same number of steps and tokens (3 epochs\nfor all models). For efficiency, we use GPT3-Medium as\nthe base model and use the same hyper-parameters in Ta-\nble 5, except that the values of Hidden Dimension, Learn-\ning Decay Steps, and Warmup Steps are 1,024, 28,000,\nand 5,000, respectively. Table 7 shows the results of the\neight downstream tasks. GPT-BigMac delivers comparable\nor better results against GPT-Fine-Grained, achieving the\nbest performance for 5 out of 8 tasks. For example, GPT-\nBigMac surpasses GPT-Fine-Grained by a score of 0.9 on\nRACE-H. Both GPT-BigMac and GPT-Fine-Grained out-\nperform GPT-Vanilla, which shows the superiority of fine-\ngrained MoE models.\nIn addition, we also evaluate two\ntasks, including GovReport (Huang et al. 2021) for sum-\nmarization and NeedleInAHaystack (Kamradt 2023) for re-\ntrieval. GPT-BigMac achieves the score of 19.5 for GovRe-\nport, which is better than 17.7 achieved by GPT-DeepSeek.\nFor NeedleInAHaystack, GPT-BigMac delivers comparable\nrecall scores across different depths (Table 8).\nTraining and Inference Speedups\nIn the last section, we have shown that compared with the\ntraditional MoE structure, MoE structures with small experts\nare more powerful. In this section, we further compare the\ncommunication efficiency of the fine-grained MoE structure\nand BigMac in more depth.\nExperimental Setup\nWe intensively profile the time ratios of training and infer-\nence for GPT-Fine-Grained and GPT-BigMac, based on the\nstate-of-the-art frameworks Megatron (Shoeybi et al. 2020),\nTutel (Hwang et al. 2023), and DeepSpeed-Inference (Mi-\ncrosoft 2024). Megatron supports various parallelism strate-\ngies including data parallelism (DP), tensor parallelism\n(TP), and expert parallelism (EP). Tutel is a specialized\nframework to optimize the All-to-All communication for\nMoE models.\nDeepSpeed-Inference supports techniques\nspecialized for LLM inference including KV cache manage-\nment to efficiently serve the models. All the experiments\nare conducted on a cluster of 4 machines connected with\n100 Gbps InfiniBand. Each machine has the same configu-\nration and is equipped with eight GPUs. Each GPU is con-\nnected with PCIe 4.0 x 16 and has 48 GB HBM, delivering\nup to 149.7 TFLOPS (FP16) with 96 cores. For all the ex-\nperiments, the input sequence length is 2,048 and the global\nbatch size is 64. We mainly compare the two structures in\nterms of training step latency, the corresponding All-to-All\nlatency, and the inference throughput.\nComparing Training Latency via Megatron\nWe first compare the training step time of fine-grained and\nBigMac models under the Megatron framework. Here, we\nadopt four base models including GPT3-Medium, GPT3-\nXL, GPT3-2.7B, and GPT3-6.7B.\nFigure 3 shows that GPT-BigMac achieves the speedups\nof 1.53-2.41× and 2.45-3.07× than GPT-Fine-Grained for\nTop4 and Top8 routing settings, respectively. Note that\nlarger top k generally indicates the heavier communica-\ntion, hence GPT-BigMac enjoys greater advantages in the\nTop8 setting. For the MoE models with small experts, larger\ntop k implies better performance to some extent. Due to\n\n(a) Top8\n(b) Top4\nFigure 3: Per-iteration training time comparison between the\nfine-grained structure and BigMac on Megatron. The mod-\nels are constructed from four base models, namely GPT3-\nMedium, GPT3-XL, GPT3-2.7B, and GPT3-6.7B, ordered\nby the size of parameters.\n(a) Top8\n(b) Top4\nFigure 4: Training time breakdown under different paral-\nlelism settings on Megatron. The labels (ep, tp) represent\nexpert parallelism degree and tensor parallelism degree, re-\nspectively. For each group, the left bar is the result of\nGPT-Fine-Grained, and the right bar corresponds to GPT-\nBigMac. The numbers displayed on the right bar indicate\nthe speedup in end-to-end latency.\nthe high communication efficiency, BigMac can choose a\nlarger top k than GPT-Fine-Grained. Surprisingly, GPT-\nBigMac using the Top8 routing can still outperform GPT-\nFine-Grained using the Top4 routing by 27.7-55.4% in terms\nof the end-to-end latency.\nBreakdown Analysis. To understand the above speedups in\ndepth, we report the breakdown results for training with an\nemphasis on the All-to-All communication cost. In Figure 4,\nthe (32, 1) groups refer to the setup with only the expert par-\nallelism and its degree ep setting to 32. In this setting, Big-\nMac achieves an end-to-end speedup of 2.37× and 2.95×\nunder the Top4 and Top8 routing, respectively, compared to\nthe fine-grained baseline, where the speedup w.r.t. the All-\nto-All communication is 3.48× and 3.72×, respectively. In\naddition to the above pure expert parallelism setting, we also\nconsider the combinations of various parallelism modes. We\nadopt tensor parallelism with the following settings. Specifi-\ncally, we set the tensor parallelism degree tp from 1 to 8, and\nthen adjust expert parallelism degree ep by ep = 32/tp. In\nthis situation, BigMac can still reduce the All-to-All com-\nmunication by 2.47-3.73× and the end-to-end latency by\n1.55-2.77×. In Megatron, the TP-SP communication in the\nMoE layer involves the operations of All-to-All, All-Gather,\nand Reduce-Scatter within each TP group. All these oper-\n(a) ep=32, Top8\n(b) ep=32, Top4\n(c) ep=16, Top8\n(d) ep=16, Top4\nFigure 5: Inference throughput comparison between GPT-\nFine-Grained and GPT-BigMac on Megatron. We conduct\nexperiments with different numbers of GPUs with expert\nparallelism degree ep and top k values. The numbers un-\nder x-axis represents different prompt lengths.\nations happen at the higher dimension in the original fine-\ngrained structure and the lower dimension with the design of\nBigMac. In this way, BigMac also reduces the TP-SP com-\nmunication by 1.42-2.34× for different setups. Finally, ac-\ncording to the results of the four parallelism settings shown\nin the figure, for the sake of efficiency, expert parallelism\nis preferred over tensor parallelism in our setting, as tensor\nparallelism involves more expensive all-reduce communica-\ntion.\nInference Throughput Comparison with Megatron\nFor inference, we measure the throughput of the forward\npass under the Megatron framework. We keep the number\nof the tokens per batch to be 128k, but with varying prompt\nlengths, ranging from 128 to 1,024. We use 16 and 32 GPUs\nfor evaluation and we set the expert parallelism degree ep to\n16 and 32, respectively. Here we do not adopt tensor paral-\nlelism since it is less efficient.\nFigure 5 shows that GPT-BigMac consistently outper-\nforms GPT-Fine-Grained and achieves 1.72-2.45× speedups\nacross all the settings. First, BigMac can obtain higher\nspeedups with larger top k,. Second, the amplitude of\nspeedup decreases slightly as the prompt length increases.\nNote that the larger prompt length brings heavier computa-\ntion overhead in the attention layer, and then the proportion\nof All-to-All communication decreases correspondingly, es-\npecially for BigMac, which explains its slight decline in the\ninference throughput.\nComparison on All-to-All Optimized System\nFinally, we investigate if BigMac’s model structure can\nbring benefits further on systems which have already opti-\nmized the All-to-All bottleneck of MoE from systems per-\nspectives.\nFor training, we evaluate on Tutel and for in-\n\n(a) Top8\n(b) Top4\nFigure 6: Training time breakdown on Tutel. For each group,\nlabels (ep, f) refer to the corresponding EP degree and the\nexpert capacity factor f, where f=D refers to the dynamic\ncapacity factor adaption. For each group, the left bar is the\nresult of GPT-Fine-Grained, and the right bar corresponds to\nGPT-BigMac.\n(a) ep=32, Top8\n(b) ep=32, Top4\n(c) ep=16, Top8\n(d) ep=16, Top4\nFigure 7: Inference throughput comparison between GPT-\nFine-Grained and GPT-BigMac on Tutel with f=1.2. The\nnumbers under x-axis represents different prompt lengths.\nference, we evaluate on Tutel and DeepSpeed-Inference.\nWe evaluate GPT-Fine-Grained and GPT-BigMac, using the\nGPT3-Medium as the base model, with different expert par-\nallelism degrees and top k values. In Tutel, we adopt the\n2DH All-to-All communication technique and set the over-\nlapping degree as 4 to hide communications with expert\ncomputations. In addition, Tutel supports dynamic capacity\nfactor adaption, which avoids token dropping while reducing\ntoken padding. We measure with a fixed factor (f=1.2) and\nthe dynamic capacity factor adaption (f=∞), respectively.\nTraining Latency on Tutel. Figure 6 shows the train-\ning speedups of GPT-BigMac, compared with GPT-Fine-\nGrained under Top8/Top4 routing, and we show the results\nwith fixed capacity factor (f=1.2) and dynamic capacity fac-\ntor (f=∞), respectively. We can see that BigMac has signif-\nicant speedups ranging from 1.71× to 3.09× in all the cases,\nand BigMac shows greater advantages in Top8 routing and\ndynamic capacity setting, since both larger top k and larger\ncapacity indicate more data transmission.\nGeneration Length\n1\n2\n5\n10\nep=16,Top8\n3.11×\n2.89×\n2.41×\n1.99×\nep=16,Top4\n2.81×\n2.50×\n2.03×\n1.62×\nTable 9: Inference throughput speedup of GPT-BigMac on\nDeepSpeed-Inference under different generation lengths.\nInference Throughput on Tutel. We summarize the infer-\nence throughput of GPT-Fine-Grained and GPT-BigMac on\nTutel for different prompt lengths in Figure 7. GPT-BigMac\nconsistently outperforms GPT-Fine-Grained by 1.67-1.87×,\nunder different top k value and expert parallelism degrees.\nThis implies that with system optimizations enabled by Tu-\ntel, BigMac can still maintain a high throughput over differ-\nent prompt lengths.\nInference Throughput on DeepSpeed-Inference. We next\ncompare the inference throughput of GPT-Fine-Grained and\nGPT-BigMac on DeepSpeed-Inference for different gen-\neration lengths. Table 9 shows the speedup of inference\nthroughput under the prompt length of 128. The results show\nthat on DeepSeepd-Inference, which involves techniques in-\ncluding KV cache management, GPT-BigMac consistently\noutperforms GPT-Fine-Grained by 1.62-3.11× over differ-\nent generation lengths.\nDiscussion\nIn Figure 2c, BigMac introduces two additional scaling\nprojections. However, the computation brought by the two\nprojections is negligible compared with the benefits from\nthe All-to-All communication reduction. For small models\nwithout the necessity of expert parallelism, BigMac indeed\nslightly increases the overall latency since no All-to-All\ncommunication is required in this case. Therefore, BigMac\nis more suitable for large models which are the current trend\nof novel models. In our structure, the downscaling factor r\naffects both the All-to-All communication overhead and the\nmodel quality. In this paper, for a fair comparison, we set the\nfactor r as 0.25 to ensure that the MoE models with three dif-\nferent structures involve similar number of parameters. One\ncan adjust the ratio in real applications, according to the ac-\ntual demand.\nConclusion\nWe proposed a novel MoE structure named BigMac\nwhich uses a descend-communicate-communicate-ascend\n(DCCA) strategy to reduce the communication overhead by\nperforming All-to-All operations at the lowest dimension.\nResults demonstrate that BigMac achieves comparable or\nsuperior model quality to the existing MoE structures, with\nsignificant speedups in training and inference across differ-\nent platforms, making it a strong contender among MoE-\nbased large language models.\nAcknowledgements\nWe thank the anonymous reviewers for their insightful com-\nments. This work is supported by the Strategic Priority Re-\n\nsearch Program of the Chinese Academy of Sciences, Grant\nNo. XDB0660101, XDB0660000, and XDB0660100. We\nthank the technical support from Huawei and computing re-\nsources from Institute of Artificial Intelligence, Hefei Com-\nprehensive National Science Center. Cheng Li is the corre-\nsponding author.\nReferences\nBisk, Y.; Zellers, R.; Bras, R. L.; Gao, J.; and Choi, Y. 2020.\nPIQA: Reasoning about Physical Commonsense in Natural\nLanguage. In Thirty-Fourth AAAI Conference on Artificial\nIntelligence.\nDai, D.; Deng, C.; Zhao, C.; Xu, R. X.; Gao, H.; Chen, D.;\nLi, J.; Zeng, W.; Yu, X.; Wu, Y.; et al. 2024. DeepSeek-\nMoE: Towards Ultimate Expert Specialization in Mixture-\nof-Experts Language Models. arXiv:2401.06066.\nDeepSeek-AI; Liu, A.; Feng, B.; Wang, B.; Wang, B.;\nLiu, B.; Zhao, C.; Dengr, C.; Ruan, C.; Dai, D.; et al.\n2024. DeepSeek-V2: A Strong, Economical, and Efficient\nMixture-of-Experts Language Model. arXiv:2405.04434.\nDu, N.; Huang, Y.; Dai, A. M.; Tong, S.; Lepikhin, D.; Xu,\nY.; Krikun, M.; Zhou, Y.; Yu, A. W.; Firat, O.; et al. 2022.\nGlam: Efficient scaling of language models with mixture-of-\nexperts. In International Conference on Machine Learning,\n5547–5569. PMLR.\nDubey, A.; Jauhri, A.; Pandey, A.; Kadian, A.; Al-Dahle,\nA.; Letman, A.; Mathur, A.; Schelten, A.; Yang, A.;\nFan, A.; et al. 2024.\nThe Llama 3 Herd of Models.\narXiv:2407.21783.\nEleutherAI. 2020.\nOpenWebText2.\nhttps://github.com/\nEleutherAI/openwebtext2. [last access: August 15, 2024].\nFedus, W.; Zoph, B.; and Shazeer, N. 2022. Switch trans-\nformers: Scaling to trillion parameter models with simple\nand efficient sparsity. Journal of Machine Learning Re-\nsearch, 23(120), 1-39.\nJournal of Machine Learning Re-\nsearch, 23(120): 1–39.\nHe, J.; Zhai, J.; Antunes, T.; Wang, H.; Luo, F.; Shi, S.; and\nLi, Q. 2022. Fastermoe: modeling and optimizing training\nof large-scale dynamic pre-trained models. In Proceedings\nof the 27th ACM SIGPLAN Symposium on Principles and\nPractice of Parallel Programming, 120–134.\nHu, Q.; Ye, Z.; Wang, Z.; Wang, G.; Zhang, M.; Chen, Q.;\nSun, P.; Lin, D.; Wang, X.; Luo, Y.; et al. 2024. Characteriza-\ntion of large language model development in the datacenter.\nIn 21st USENIX Symposium on Networked Systems Design\nand Implementation (NSDI 24), 709–729.\nHuang, L.; Cao, S.; Parulian, N.; Ji, H.; and Wang, L. 2021.\nEfficient Attentions for Long Document Summarization.\narXiv:2104.02112.\nHwang, C.; Cui, W.; Xiong, Y.; Yang, Z.; Liu, Z.; Hu, H.;\nWang, Z.; Salas, R.; Jose, J.; Ram, P.; et al. 2023. Tutel:\nAdaptive mixture-of-experts at scale. Proceedings of Ma-\nchine Learning and Systems, 5: 269–287.\nJiang, A. Q.; Sablayrolles, A.; Roux, A.; Mensch, A.;\nSavary, B.; Bamford, C.; Chaplot, D. S.; Casas, D. d. l.;\nHanna, E. B.; Bressand, F.; et al. 2024. Mixtral of experts.\narXiv:2401.04088.\nKamradt, G. 2023.\nNeedle In A Haystack - Pres-\nsure Testing LLMs. https://github.com/gkamradt/LLMTest\nNeedleInAHaystack. [last access: December 14, 2024].\nLai, G.; Xie, Q.; Liu, H.; Yang, Y.; and Hovy, E. 2017.\nRACE: Large-scale ReAding Comprehension Dataset From\nExaminations. In Palmer, M.; Hwa, R.; and Riedel, S., eds.,\nProceedings of the 2017 Conference on Empirical Methods\nin Natural Language Processing, 785–794. Copenhagen,\nDenmark: Association for Computational Linguistics.\nLepikhin, D.; Lee, H.; Xu, Y.; Chen, D.; Firat, O.; Huang,\nY.; Krikun, M.; Shazeer, N.; and Chen, Z. 2020. Gshard:\nScaling giant models with conditional computation and au-\ntomatic sharding. arXiv:2006.16668.\nLi, J.; Jiang, Y.; Zhu, Y.; Wang, C.; and Xu, H. 2023. Accel-\nerating distributed {MoE} training and inference with lina.\nIn 2023 USENIX Annual Technical Conference (USENIX\nATC 23), 945–959.\nLindstrom, P. 2014. Fixed-Rate Compressed Floating-Point\nArrays. IEEE Transactions on Visualization and Computer\nGraphics, 20(12): 2674–2683.\nMarcus, M.; Santorini, B.; and Marcinkiewicz, M. A. 1993.\nBuilding a large annotated corpus of English: The Penn\nTreebank. Computational linguistics, 19(2): 313–330.\nMerity, S.; Xiong, C.; Bradbury, J.; and Socher, R. 2016.\nPointer Sentinel Mixture Models. arXiv:1609.07843.\nMicrosoft. 2024.\nDeepSpeed Inference.\nhttps://www.\ndeepspeed.ai/inference/. [last access: December 14, 2024].\nNVIDIA. 2019. NVIDIA/Megatron-LM: Ongoing research\ntraining transformer models at scale.\nhttps://github.com/\nNVIDIA/Megatron-LM. [last access: August 12, 2024].\nOpenAI; Achiam, J.; Adler, S.; Agarwal, S.; Ahmad, L.;\nAkkaya, I.; Aleman, F. L.; Almeida, D.; Altenschmidt,\nJ.; Altman, S.; et al. 2024.\nGPT-4 Technical Report.\narXiv:2303.08774.\nPaperno, D.; Kruszewski, G.; Lazaridou, A.; Pham, Q. N.;\nBernardi, R.; Pezzelle, S.; Baroni, M.; Boleda, G.; and\nFern´andez, R. 2016. The LAMBADA dataset.\nSakaguchi, K.; Bras, R. L.; Bhagavatula, C.; and Choi,\nY. 2019. WinoGrande: An Adversarial Winograd Schema\nChallenge at Scale. arXiv:1907.10641.\nSanseviero, O.; Tunstall, L.; Schmid, P.; Mangrulkar, S.;\nBelkada, Y.; and Cuenca, P. 2023. Mixture of Experts Ex-\nplained. [last access: August 15, 2024].\nShi, S.; Pan, X.; Wang, Q.; Liu, C.; Ren, X.; Hu, Z.; Yang, Y.;\nLi, B.; and Chu, X. 2024. ScheMoE: An Extensible Mixture-\nof-Experts Distributed Training System with Tasks Schedul-\ning. In Proceedings of the Nineteenth European Conference\non Computer Systems, 236–249.\nShoeybi, M.; Patwary, M.; Puri, R.; LeGresley, P.; Casper,\nJ.; and Catanzaro, B. 2020. Megatron-LM: Training Multi-\nBillion Parameter Language Models Using Model Paral-\nlelism. arXiv:1909.08053.\nWikimedia. 2024. Wikimedia Downloads. https://dumps.\nwikimedia.org. [last access: August 15, 2024].\n\nXue, F.; Zheng, Z.; Fu, Y.; Ni, J.; Zheng, Z.; Zhou, W.; and\nYou, Y. 2024. OpenMoE: An Early Effort on Open Mixture-\nof-Experts Language Models. arXiv:2402.01739.\nYang, A.; Yang, B.; Hui, B.; Zheng, B.; Yu, B.; Zhou, C.; Li,\nC.; Li, C.; Liu, D.; Huang, F.; et al. 2024. Qwen2 technical\nreport. arXiv:2407.10671.\nZellers, R.; Holtzman, A.; Bisk, Y.; Farhadi, A.; and Choi,\nY. 2019.\nHellaSwag: Can a Machine Really Finish Your\nSentence? In Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics.\n",
  "metadata": {
    "source_path": "papers/arxiv/BigMac_A_Communication-Efficient_Mixture-of-Experts_Model_Structure_for\n__Fast_Training_and_Inference_59fa192afb9286dd.pdf",
    "content_hash": "59fa192afb9286dd5c6653a07e44222a1d389250419309d3ead1a21b054a4478",
    "arxiv_id": null,
    "title": "BigMac_A_Communication-Efficient_Mixture-of-Experts_Model_Structure_for\n__Fast_Training_and_Inference_59fa192afb9286dd",
    "author": "",
    "creation_date": "D:20250225023038Z",
    "published": "2025-02-25T02:30:38",
    "pages": 10,
    "size": 907838,
    "file_mtime": 1740470204.5444143
  }
}