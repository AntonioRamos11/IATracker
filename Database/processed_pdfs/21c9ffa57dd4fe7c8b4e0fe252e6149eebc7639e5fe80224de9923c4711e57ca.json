{
  "text": "DIS-CO: Discovering Copyrighted Content in VLMs Training Data\nAndr´e V. Duarte 1 2 Xuandong Zhao 3 Arlindo L. Oliveira 2 Lei Li 1\nAbstract\nHow can we verify whether copyrighted con-\ntent was used to train a large vision-language\nmodel (VLM) without direct access to its training\ndata? Motivated by the hypothesis that a VLM\nis able to recognize images from its training cor-\npus, we propose DIS-CO, a novel approach to\ninfer the inclusion of copyrighted content during\nthe model’s development. By repeatedly query-\ning a VLM with specific frames from targeted\ncopyrighted material, DIS-CO extracts the con-\ntent’s identity through free-form text completions.\nTo assess its effectiveness, we introduce Movie-\nTection, a benchmark comprising 14,000 frames\npaired with detailed captions, drawn from films\nreleased both before and after a model’s training\ncutoff. Our results show that DIS-CO significantly\nimproves detection performance, nearly doubling\nthe average AUC of the best prior method on mod-\nels with logits available. Our findings also high-\nlight a broader concern: all tested models appear\nto have been exposed to some extent to copy-\nrighted content. Our code and data are available\nat https://github.com/avduarte333/\nDIS-CO\n1. Introduction\nThe rapid evolution of large-scale models has driven a\nparadigm shift toward multimodality, with recent large\nvision-language models (VLMs) gaining prominence for\ntheir ability to process both visual and textual information\n(Alayrac et al., 2022; Liu et al., 2023; OpenAI, 2023; Wang\net al., 2024). While these models showcase remarkable per-\nformance across a variety of tasks, their reliance on vast, di-\nverse datasets introduces challenges in ensuring compliance\nwith ethical and legal standards. Without strict safeguards\n1Carnegie Mellon University\n2INESC-ID / Instituto Su-\nperior T´ecnico, ULisboa 3UC Berkeley.\nCorrespondence to:\nAndr´e V. Duarte <andre.v.duarte@tecnico.ulisboa.pt>, Xuandong\nZhao <xuandongzhao@berkeley.edu>, Arlindo L. Oliveira <ar-\nlindo.oliveira@tecnico.ulisboa.pt>, Lei Li <leili@cs.cmu.edu>.\nQ: “What movie is \nthis frame from?”\n…\nFrozen\nVLMs accurately\nmatch Suspect\nmovie titles\nto their frames!\nClean\nMovies\nSuspect\nMovies\nVisual-LLM\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAquaman-2\nFigure 1. Our DIS-CO reveals that VLMs successfully map frames\nof suspect movies to their titles, even when the frames are highly\nchallenging. For example, the GPT-4o model accurately identifies\nthe movie “Frozen” (blue circle), despite the varying complexity\nof its frames. In contrast, for newly released films, the models are\nunable to perform similar frame-to-title predictions.\nduring the data collection step, proprietary content could be\nincorporated into the models’ knowledge, opening the door\nto intellectual property infringements and potential legal\nconflicts (Carlini et al., 2022a; Nasr et al., 2023; Duan et al.,\n2024). In fact, in the United States alone, more than 24\ncopyright lawsuits were filed against the AI industry since\n2023 (Knibbs, 2024), reflecting growing concerns about the\nuse of protected material in training (Kadrey, 2023; Daily\nNews, 2024).\nDiscovering training data is, therefore, essential for effec-\ntively addressing the ethical and legal challenges of model\ntraining. However, the lack of transparency in data col-\nlection (often justified by competitive concerns) makes it\nparticularly difficult to determine whether specific materials\nhave been used.\nTo tackle these challenges, Membership Inference At-\ntacks (MIAs) serve as a tool to identify whether specific\ndata samples were part of a model’s training set. While\nMIA techniques are well-studied for text-based models,\ntheir adaptation to multimodal settings, particularly VLMs,\nremains less explored - a gap that our work aims to address.\n1\narXiv:2502.17358v1  [cs.CV]  24 Feb 2025\n\nDIS-CO: Discovering Copyrighted Content in VLMs Training Data\nRecent advancements have been made in this field, but\ncertain challenges and limitations continue to exist. The\nMaxR´enyi-K% method (Li et al., 2024b) is based on the in-\ntuition that data encountered during training leads to greater\nmodel confidence when generating outputs, reflected by a\nsmaller R´enyi entropy in the next-token probability distri-\nbution for image or text tokens. While this method proved\neffective in settings with unrestricted access to output logits,\nits applicability is limited in the context of black-box mod-\nels like Gemini (Reid et al., 2024), which, at most, allow\nfor inspection of the top-5 logits for each predicted token.\nThe work of Pinto et al. (2024) addresses the challenge\nof discovering training data in a fully black-box fashion\nby introducing a method tailored to document-based VQA\ntasks. By removing key textual content from input images,\nthe authors demonstrate the models’ ability to recall mem-\norized training data, including sensitive information. This\napproach is, nonetheless, limited to VQA datasets, which\nconstitute only a small fraction of the diverse data types\nused to train VLMs.\nIn this paper we propose DIS-CO, a novel method for detect-\ning models’ training data that overcomes the limitations of\nprevious approaches while being applicable to both white-\nand black-box VLMs. The core idea of DIS-CO is to prompt\nmodels to map a set of images from a target media docu-\nment to its corresponding identity (e.g., movie titles) in an\nunconstrained, free-form text-generation manner, enabling\nthem to produce answers freely instead of selecting from\npredefined options. As a result, under the null hypothesis:\nthe target content was not in the training dataset, the model\nis much less likely to correctly identify the data, reducing\nfalse positives. Consequently, correct outputs become a\nmore reliable indication that the target content was part of\nthe training dataset (Zhang et al., 2024a).\nThis idea is well illustrated in Figure 1, which demonstrates\nthat models solve the task correctly far more often for con-\ntent that was very likely included in their training data com-\npared to content that was definitely excluded, such as movies\nreleased after the model’s cutoff knowledge date. For in-\nstance, DIS-CO maps frames from the movie Frozen to the\ncorrect title in nearly 90% of the test cases, while a movie\nlike Aquaman and the Lost Kingdom is correctly identified\nin less than 2% of the time.\nWe conduct experiments on two benchmarks, MovieTec-\ntion (our newly introduced dataset) and VL-MIA/Flickr (Li\net al., 2024b). MovieTection contains 14,000 diverse movie\nframes paired with descriptive captions, split chronologi-\ncally based on films released before/after the models’ train-\ning cutoff (October 2023). VL-MIA/Flickr, derived from\nCOCO (Lin et al., 2014) (member data) and recent Flickr\nimages (non-member data), serves as a proof-of-validity\ndataset for DIS-CO.\nOur main contributions are as follows:\n• We introduce DIS-CO, an innovative method applicable to\nboth white-box and black-box VLMs, designed to detect\nwhether copyrighted content was included in training. To\nthe best of our knowledge, this is the first work to detect\ncopyrighted movies in the context of VLMs.\n• We introduce MovieTection, a new benchmark designed\nfor detecting training data of VLMs. The dataset in-\ncludes content from 100 movie titles, unfolded into 14,000\nframes, with a mix of easily identifiable and challenging\nexamples to test model capabilities comprehensively.\n• Experiments show that DIS-CO effectively detects copy-\nrighted movies across the six tested model families, with\nGPT-4o achieving an average accuracy of 34% on the\n“hard to guess” frames of the suspect movies.\n• We show that fine-tuning a model to avoid disclosing\nmemorized content for a particular movie is an effective\ndefense strategy, capable of generalization to other titles.\n2. Preliminary and Related Work\nMembership Inference Attacks (MIAs) are designed to de-\ntermine whether a specific data instance was included in the\ntraining set of a machine learning model (Shokri et al., 2017;\nHu et al., 2022). This area of research has seen growing\ninterest with the increasing use of LLMs, which are known\nto memorize and occasionally reproduce training data (Nasr\net al., 2023; Carlini et al., 2022b; Hans et al., 2024).\nClassical MIAs are typically divided into two main ap-\nproaches: reference-based and reference-free. Reference-\nbased methods involve training a set of “shadow models” to\nreplicate the target model’s behavior (Carlini et al., 2022a;\nLong et al., 2018; Mireshghallah et al., 2022; Watson et al.,\n2022).\nIn contrast, reference-free methods rely on cal-\nculating specific metrics, such as the perplexity of a sen-\ntence, to identify patterns indicative of training set member-\nship (Yeom et al., 2018; Salem et al., 2018; Carlini et al.,\n2020; Song & Mittal, 2021). Among these, the Min-K%-\nProb method stands out as a more refined approach. It\nhypothesizes that the average log-likelihood of the top-k%\nleast probable tokens in an example is higher if the exam-\nple was part of the training data compared to if it was not\n(Shi et al., 2023). Building on this foundation, recent ex-\ntensions such as Min-K%++ (Zhang et al., 2024b) and DC-\nPDD (Zhang et al., 2024c) have introduced further improve-\nments. However, a key limitation of most reference-free\nmethods is their dependence on access to token probabil-\nity distributions, which restricts their interoperability with\nblack-box models such as Gemini (Reid et al., 2024).\nWith recent research shifting focus from text-only models to\nmulti-modal architectures, the task of detecting training data\nand evaluating model memorization has begun to emerge\n2\n\nDIS-CO: Discovering Copyrighted Content in VLMs Training Data\nin this domain as well (Kokhlikyan et al., 2024; Jayaraman\net al., 2024; Pinto et al., 2024). Building on techniques\noriginally developed for text-only models, Li et al. (2024b)\npropose a novel image-based MIA pipeline that adapts meth-\nods like Min-K%-Prob (Shi et al., 2023) to VLMs. The\nwork presents the MaxR´enyi-K% metric, which enables\nimage membership inference by analyzing the output logits\ncorresponding to the model’s image-specific slice.\nDetecting training data is especially significant when it in-\nvolves copyrighted content, as the reproduction of such ma-\nterial by large models raises legal and ethical concerns (Li\net al., 2024a; Meeus et al., 2024a). In the light of coun-\nterfactual memorization studies, the methods proposed by\nDuarte et al. (2024) and Golchin & Surdeanu (2024) perform\nmembership inference through a multiple-choice question-\nanswering (MCQA) setting. These approaches demonstrate\nsolid results and have the advantage of being applicable\nto both white-box and black-box models, as they do not\ndepend on access to token probabilities. However, it is\nknown that multiple-choice scenarios may induce a selec-\ntion bias (Zheng et al., 2024a) on the models, which intro-\nduces some uncertainty about whether chance played a role\nin the results. In contrast, Karamolegkou et al. (2023) adopt\na prompting approach with free-form text generation, aim-\ning to elicit verbatim reproduction of copyrighted material.\nThis provides stronger evidence of memorization because,\nin an unconstrained, free-form setting, the model is much\nless likely to produce correct outputs by chance. While this\nmethod may fail to detect cases where models are trained\non copyrighted data without memorizing it (Meeus et al.,\n2024b), it also avoids the issues raised by Das et al. (2024)\nand Maini et al. (2024), who warn that many membership\ninference methods risk overstating results by exploiting data\ndistribution shifts, such as temporal patterns, rather than\nidentifying genuine memorization. For these reasons, we\nalso focus on free-form text generation in this work, as it\nprovides a more robust and unbiased indication of whether\nthe target content is part of the model’s training data.\n3. Benchmark: MovieTection\nOur proposed benchmark, MovieTection, distinguishes\nmember and non-member data based on a clearly defined\ntemporal constraint. Movies released in 2024 or later are\nconsidered non-member data, as they fall outside the knowl-\nedge cutoff dates of all tested models. Movies from January\nto September 2023 are excluded due to uncertainty regarding\nmodels’ exposure to content from that period. For instance,\nQwen2-VL (Wang et al., 2024) reports a knowledge cutoff\nin June 2023. Movies released on or before 2022 are treated\nas potential member data, as they are more likely to have\nbeen included in the training datasets of such models.\nMovieTection currently comprises frames from 100 movies,\nwith plans for future expansion. The selection of movies\nincorporated into the benchmark is guided by their status as\nbox office hits, based on the assumption that highly popular\nmovies, due to their widespread availability, are more likely\nto appear in training datasets. For the suspect data, we pri-\nmarily select titles randomly from the Box Office Mojo’s1\nlist of the all-time highest-grossing films, with some excep-\ntions to accommodate specific experiments, such as analyz-\ning the impact of IMDb2 ratings (Section 5). For the clean\ndata, we sample most titles from the Box Office Mojo’s list\nof the highest-grossing films of 2024.\nFor each movie, we extract frames categorized into two\ntypes: main frames and neutral frames. This categorization\nis designed to introduce varying levels of difficulty for as-\nsessing a model’s knowledge about a movie. Main frames\ntypically feature key characters to the movie’s plot. These\nframes are intended to be easily recognizable by viewers\nfamiliar with the movie. In contrast, neutral frames focus\non ordinary visuals, such as landscapes, objects, or minor\ncharacters, that are not strongly tied to the movie’s narrative.\nNeutral frames are designed to present a significantly higher\nchallenge, as they rely on subtle contextual cues that are\nalmost impossible to associate with the correct title without\nprior knowledge of the movie. Figure 2 illustrates the two\nframe types.\nEach extracted frame is accompanied by a detailed caption,\ngenerated using the 7B version of the Qwen2-VL model.\nThe prompt used for caption generation, along with an ex-\nample, is provided in Appendix B. In total, 140 frames are\nextracted per movie, comprising 100 main frames and 40\nneutral ones.\n4. DIS-CO\nOur proposed method, DIS-CO, determines whether exam-\nples are memorized by evaluating the model’s performance\non a question-answering task with free-form text responses.\nWe argue that eliciting free-form completions is preferable\nto a multiple-choice question-answering (MCQA) format\nbecause it significantly reduces the influence of “luck” as-\nsociated with guessing. In multiple-choice settings, models\ncan achieve high accuracy even when responding randomly,\nas the limited set of predefined options increases the likeli-\nhood of selecting the correct answer. In contrast, free-form\nresponses require the model to generate answers without\nsuch constraints, making it far less likely for correct re-\nsponses to arise by chance, even if this comes at the expense\nof a slightly lower accuracy.\nThe task we propose involves models performing accurate\nidentification of the content’s identity which, in the case of\n1https://www.boxofficemojo.com/\n2https://www.imdb.com/\n3\n\nDIS-CO: Discovering Copyrighted Content in VLMs Training Data\nSuspect Book\nData Collection: Frames can be of 2 different types.\n0%\n20%\n40%\n60%\n80%\n100%\nSuspect\nMovie\nBaseline\nPerformance\nAccuracy\n1.1 Query Model with IMGs\n1.2 Query Model with Text Caption\n2. Remove Intersection\n3. Evaluate\nSuspect Movie\nLarge Δ?\nSuspect Movie was \nIn Pretraining Data\nΔ\nMain Frames\nContains Main Characters\nFrames featuring key characters from the plot.\nEasily Recognizable\nIdentifiable to viewers who saw the movie.\nNeutral Frames\nBackground / Neutral Elements\nNeutral visuals like landscapes or objects.\nHardly Recognizable\nFrames not easily tied to the movie's narrative.\nBuild MovieTection Dataset\nClean\nSuspect\nSept.2023\n𝑡\nBox Office Hits\n50 Suspect | 50 Clean\nAt least $200M Worldwide B.O.\nData Properties\n100 Main ∧40 Neutral Frames/Movie\nDetailed VLM Caption / Frame\n$\nQuery\nQ: “What movie is this \nframe / description from?”\nIMG 𝑖\nChatGPT\nVision-LLM\nRepeat for \nAll Movies\nBaseline Performance Range\nAnswer\nA: The movie is _______\nFree-Form\nGeneration\nCaption 𝑖\nor\nFigure 2. The pipeline begins with the construction of the MovieTection benchmark, where we categorize movie frames into main and\nneutral types to introduce varying levels of difficulty. Models are then queried with image frames and their corresponding captions,\nproducing predictions for both modalities. Predictions from image frames that overlap with caption-based predictions are excluded,\nisolating cases where image-based memorization is inferred. Performance on the suspect movie is compared against the expected baseline\nperformance, with discrepancies indicating potential training exposure.\ncopyrighted movie identification, corresponds to correctly\nidentifying the movie title. We operate under the premise\nthat models map a frame to the appropriate title far more\nreliably when that movie is included in their training data\ncompared to when it is not. The specific prompts used\nfor evaluating models on the MovieTection benchmark are\nprovided in Appendix C.\nThe overall pipeline of DIS-CO is illustrated in Figure 2.\nAfter constructing the MovieTection dataset, we first query\nthe models with clean data to establish a baseline for its\nexpected performance on this set. While one might expect\nthe models to fail completely on all these examples, given\nthat these movies were unreleased at the time of the training\ncutoff, this is not always the case. Some of the movies in\nthis set, though unreleased, were already announced and\nacknowledged by the models, leading to correct predictions\nfor certain examples (See Table 3 - Section 6.2 and Ap-\npendix D). Capturing this baseline performance is crucial to\navoid incorrectly classifying a movie as part of the training\ndata simply because some frames were accurately identified.\nAnother important factor to consider is the time effect. In\ngeneral terms, the older a movie is, the greater the likelihood\nthat a model has residual knowledge about it. This knowl-\nedge can come from publicly available online content, such\nas movie posters, trailers, forum discussions, or datasets\nlike OpenSubtitles (Lison & Tiedemann, 2016), which typ-\nically do not raise copyright infringement concerns. To\nestimate this baseline knowledge accumulated over time,\nwe query the models using the detailed captions from the\nolder movies, as making correct predictions based solely on\ntextual descriptions is unlikely to be problematic.\nFinally, to determine whether a specific movie was likely\nincluded in the model’s training data, we query the model\nseparately with image frames and then with their correspond-\ning caption information from the suspect movie. After both\nqueries are completed, we compare the predictions from\nthese two inputs. If there is an overlap of correct predic-\ntions between the frame-based and caption-based queries,\nwe disregard those results, as they suggest the model did\nnot had to rely on the image content to make accurate pre-\ndictions. By examining the remaining correct predictions,\nwhich rely solely on image content, we infer whether the\nmodel is utilizing memorized visual information. Ideally, af-\nter removing the intersection, the performance of the suspect\nmovie should fall within this range defined by the baseline\nperformance on the recent movies and the clean baseline\nperformance on older movies with accumulated knowledge\nover time. However, if performance remains significantly\nhigher than this range, even after removing the intersection,\nit suggests that the model relied on memorized visual infor-\nmation specific to the movie frames, indicating the movie\nwas likely included in its training data.\n4.1. Upper-Bound Estimation of Memorization\nWhile our proposed approach of removing the intersec-\ntion between frame-based and caption-based correct pre-\ndictions provides a more precise set of potentially mem-\norized movies, we cannot rule out that those frames are\n4\n\nDIS-CO: Discovering Copyrighted Content in VLMs Training Data\nnot part of the training data. Consequently, we also con-\nsider an upper-bound estimation of memorization, where all\ncorrectly identified frames, regardless of their intersection\nwith captions, contribute to the possible classification of the\nmovie as part of the training data. For clarity, throughout the\nremainder of this paper, we use two notations: ⌊DIS-CO⌋\nrepresents the smaller set obtained after removing the inter-\nsection, while DIS-CO denotes the upper-bound estimation,\nincluding all correctly identified frames.\n4.2. Mitigating the Disclosure of Memorization\nWhile training on copyrighted data may occasionally be\nunavoidable, the associated risks can be mitigated by en-\nsuring that the model does not disclose memorization. For\na movie that is identified as likely included in the training\ndata, we propose fine-tuning the model on a subset of its\nframes while replacing the movie label with a neutral desig-\nnation such as ‘Copyrighted Content’. More details on the\nfine-tuning in Appendix E.\n5. Experiments\nWe assess the effectiveness of DIS-CO through a range of\ndifferent experiments, which are guided by the following\nquestions:\n• Does a longer context reveal more memorization? As\nLLMs often perform better with more context in their\nqueries, we hypothesize that VLMs behave similarly. Us-\ning the MovieTection dataset, we examine the effect of\nvarying the number of frames in the prompt (N ∈[1, 4]).\n• Are factors like movie popularity or quality good prox-\nies for memorization? To test whether popularity (e.g.,\nbox-office revenue) or quality (e.g., IMDb ratings) are\nproxies for memorization, we collect movies where one\nfactor varies while the other is controlled. For instance,\nin the box-office experiment, movies with similar IMDb\nratings are chosen to isolate the impact of popularity.\n• How susceptible are models to memorization when\nexposed to new data? We investigate the model’s ability\nto memorize new content by fine-tuning it on a movie\nguaranteed to be outside the training data.\n• How to prevent a model to disclose memorization?\nSimilarly to the previous experiment, we fine-tune the\nmodel (this time on a suspect movie), with a modified\nlabeling objective. This experiment investigates whether\nthis defense mechanism can mitigate memorization dis-\nclosure for the suspect movie and whether its effects gen-\neralize to other movies.\n• To what extent does generalization influence the\nmodel’s performance? Humans are capable of general-\nizing from partial information, often identifying movies\nthey haven’t fully seen by relying on related content such\nas posters or trailers. To assess how closely models align\nwith humans on this movie detection task, we compare\nthe performance of the models with that of 10 human par-\nticipants who were selected to identify 200 images from\nMovieTection.\n5.1. Experiment Setup\nTo evaluate DIS-CO, we follow the procedure outlined\nas follows.\nLet the “Suspect” group be represented as\nS = {s1, s2, . . . , sNS} and the “Clean” group as C =\n{c1, c2, . . . , cNC}, where NS and NC denote the number\nof movies in each group, respectively. For each movie,\nwe calculate its accuracy: A(si) for si ∈S and A(cj) for\ncj ∈C. The accuracy is calculated as the proportion of pre-\ndictions aligning with the expected outcomes. By default, a\nweighted average is then applied to account for the unequal\nproportions of main and neutral frames and the total value\nis reported. Nonetheless, some results for main and neutral\nframes are reported individually to provide further insights\non the performance across frame types.\nWe then perform a random sampling process with replace-\nment, repeated 10 times. In each iteration, M elements\nare sampled from each group, where M corresponds to NS\nor NC, depending on the group being sampled. For each\niteration, a threshold θ is optimized to achieve maximum\nseparation between the two groups, and the Area Under the\nCurve (AUC) is computed.\nTo complete the analysis, we calculate the mean and stan-\ndard deviation of the AUC or the average accuracy for the\n“Suspect” and “Clean” groups over these iterations. Detec-\ntion is consistently conducted at the movie level, rather than\non individual frames.\n5.1.1. BENCHMARKS AND BASELINES\nWe begin by evaluating DIS-CO using the proof-of-concept\ndataset VL-MIA/Flickr introduced by Li et al. (2024b). This\ndataset consists of 600 images, evenly divided into training\nand non-training based on a temporal split. Member images\nare sourced from a subset of COCO (Lin et al., 2014), while\nnon-member images are obtained from Flickr, restricted\nto content published after January 1, 2024. This temporal\nseparation aligns with the knowledge cutoff dates of the\nmodels used in our evaluation.\nFor the fine-tuning experiments, we use two movies: IF\n(2024) and Moana (2016), which have nearly identical dura-\ntions (1h48min and 1h47min, respectively), allowing us to\nsample frames at an equal rate, resulting in 6000 frames per\nmovie. The remaining experiments utilize the MovieTection\ndataset, as detailed in Section 3.\nWe evaluate DIS-CO against three baselines: (i) Captions,\n(ii) MCQA, and (iii) R´enyi (α = 0.5). The Captions base-\n5\n\nDIS-CO: Discovering Copyrighted Content in VLMs Training Data\nline involves prompting the models using only the textual\ninformation available in MovieTection. MCQA (Multiple-\nChoice Question Answering) presents the models with four\npossible answers per query, designed to be slightly challeng-\ning by including similar movies as distractors (e.g., animated\nmovies are paired with other animated ones). The R´enyi\nbaseline applies the Max-K% method (α = 0.5) proposed\nby Li et al. (2024b). We report the results for the value of K\nthat achieves the best detection performance.\n6. Results\n6.1. Proof-of-Concept\n6.1.1. VL-MIA/FLICKR\nAs introduced in Section 5.1.1, VL-MIA/Flickr is an MIA\ndataset where the ‘suspect’ images are sourced from COCO\n(Lin et al., 2014). This proof-of-concept is essential because,\nwhile the inclusion of movies in the training data of VLMs\nremains uncertain, the presence of COCO images is well-\ndocumented in the training data of various models (Radford\net al., 2021; Liu et al., 2023), making it an ideal validation\ndataset for DIS-CO. Demonstrating DIS-CO’s ability to\ndetect COCO data supports its effectiveness and underscores\nits potential applicability to similar scenarios.\nTable 1. Accuracy of DIS-CO and MCQA on the suspect split of\nVL-MIA/Flickr.\nMethod\nGPT-4o\nGemini-1.5 Pro\nQwen2-VL 72B\nDIS-CO\n0.4130.01\n0.2430.00\n0.1830.00\nMCQA\n0.020.01\n0.2500.00\n0.4830.00\nAs shown in Table 1, DIS-CO enables the models to achieve\ncompetitive accuracy, particularly with GPT-4o, which\nscores 0.413 despite the inherent difficulty of the task. This\nresult underscores the models’ capacity to identify their\ntraining data, aligning with the high probability that these\nimages are part of the datasets used during pretraining.\nBy contrast, GPT-4o faces difficulties when performing the\ntask in a MCQA setting, achieving an accuracy of only 0.02.\nFurther analysis reveals that this gap is due to selection bias,\nwhich, as illustrated in this example, can significantly affect\na VLM’s performance (extra details in Appendix G).\n6.1.2. LEARNING CLEAN MOVIE\nTo pivot towards our primary goal of detecting copyrighted\nmovies, this experiment investigates the memorization capa-\nbilities of a VLM by intentionally fine-tuning it on a movie\nit has definitively never encountered before.\nFrom Figure 3, we draw three key observations. First, it is\nhighly unlikely for the model to accurately predict a clean\nmovie without prior exposure. Second, training on randomly\n0\n1000\n2000\n3000\n4000\n5000\n6000\n# Unique Fine-Tuning Frames\n0.00\n0.25\n0.50\n0.75\n1.00\nAccuracy\nSequential Frames\nRandom Frames\nFigure 3. Accuracy of Qwen2-VL 7B in identifying a clean movie\nas a function of the number of unique fine-tuning frames.\nordered frames accelerates generalization compared to se-\nquential ordering. Third, the model begins to accurately\ndetect the movie after seeing as few as 1500 frames.\nThese findings underscore the significant capacity of even\nrelatively small models, like Qwen2-VL 7B, to memorize\nvisual content with minimal exposure. If a model of this size\ncan achieve such memorization under targeted fine-tuning,\nit is highly likely that larger, more expressive models, such\nas GPT-4o or LLaMA-3.2 90B, would demonstrate similar\nor greater tendencies, even with a different training strategy,\nlike pretraining, as previous studies show that memorization\nscales with model size and capacity (Carlini et al., 2022b;\nDuarte et al., 2024).\n6.2. Main Results\nInitially, we evaluate DIS-CO and ⌊DIS-CO⌋in compari-\nson to baseline methods, focusing on their performance in\ndistinguishing between training and non-training data, as\nshown in Table 2. For instance, for neutral frames, DIS-CO\nachieves an average AUC of 0.930, with ⌊DIS-CO⌋closely\nfollowing at 0.928, indicating that removing predictions\noverlapping with captions has minimal impact on detection\nperformance. This finding underscores the robustness of\nboth DIS-CO variants, with ⌊DIS-CO⌋offering an added\nadvantage by reducing potential biases. Notably, both vari-\nants surpass other baselines across AUC metrics, with the\nR´enyi method underperforming significantly, yielding an\naverage AUC closer to 0.5.\nNext, we assess the performance of DIS-CO and ⌊DIS-CO⌋\nin terms of accuracy. While captions achieve relatively\nstrong AUC values (e.g., 0.858 for neutral frames), their\noverall accuracy on suspect movies is less compelling. As\npresented in Table 3, DIS-CO and ⌊DIS-CO⌋achieve con-\nsistently higher average accuracy scores for suspect movies,\neffectively identifying memorized content with greater re-\nliability. Although MCQA achieves the highest accuracy\nfor suspect movies, it also incorrectly classifies much of the\nclean data as suspect. This behavior inflates its accuracy\nwhich consequently results in a large number of false pos-\nitives, ultimately lowering its AUC performance, as seen\n6\n\nDIS-CO: Discovering Copyrighted Content in VLMs Training Data\nTable 2. AUC Scores for detecting copyrighted movies present in models training data for MovieTection. The best score in each column is\nhighlighted in bold.\nGPT-4o\nGemini-1.5 Pro\nLLaMA-3.2 90B\nQwen2-VL 72B\nAvg.\nNeutral Frames\nCaptions\n0.8880.027\n0.9080.031\n0.8260.021\n0.8110.027\n0.858\nMCQA\n0.7580.048\n0.7220.037\n0.7370.052\n0.8980.015\n0.778\nR´enyi (α = 0.5)\n-\n-\n0.3630.052\n0.5980.050\n0.481\n⌊DIS-CO⌋\n0.9870.010\n0.9360.024\n0.8920.021\n0.8970.023\n0.928\nDIS-CO\n0.9890.010\n0.9420.025\n0.8970.020\n0.8930.025\n0.930\nMain Frames\nCaptions\n1.0000.000\n0.9630.029\n0.9120.028\n0.9240.022\n0.949\nMCQA\n0.7690.048\n0.7040.040\n0.7610.040\n0.8990.014\n0.783\nR´enyi (α = 0.5)\n-\n-\n0.5140.050\n0.5900.061\n0.552\n⌊DIS-CO⌋\n1.0000.000\n0.9780.024\n0.9780.010\n0.9790.014\n0.983\nDIS-CO\n1.0000.000\n0.9810.022\n0.9860.006\n0.9860.016\n0.988\nTable 3. Average accuracy scores in for GPT-4o on the MovieTec-\ntion dataset. Scores are produced only based on the neutral frames.\nGPT-4o Accuracy\nSuspect Movies\nClean Movies\nCaptions\n0.1280.01\n0.0010.00\nMCQA\n0.7210.02\n0.4100.05\n⌊DIS-CO⌋\n0.2260.02\n0.0020.00\nDIS-CO\n0.3380.03\n0.0020.00\nin Table 2. By contrast, DIS-CO variants maintain a more\nbalanced approach, avoiding such pitfalls and achieving su-\nperior performance across both suspect and clean datasets.\n6.3. Longer Context\nN=1\nN=2\nN=3\nN=4\nNumber of Frames (N) in Prompt Context\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nGPT-4o\nGemini-1.5 Pro\nQwen2-VL 72B\nLlama-3.2 90B\nFigure 4. DIS-CO’s accuracy on the MovieTection suspect split\nwith varying numbers of frames in the prompt. Accuracy improves\nas the number of frames increases, suggesting that longer contexts\nenable models to perform better predictions. Scores are produced\nwith the weighted combination of the main and neutral frames.\nWe evaluate the effect of increasing the number of frames\nin the prompt on DIS-CO’s detection performance. As\nshown in Figure 4, there is a positive correlation between the\nnumber of frames and performance, with the trend closely\napproximating a linear pattern. Moreover, GPT-4o demon-\nstrates a clear performance advantage, consistently outper-\nforming Gemini and the two other white-box models. Fur-\nther results and analysis can be found in Appendix I.\n6.4. Popularity and Quality\nWe investigate the relationship between memorization and\ntwo key factors: movie popularity (box office revenue) and\nquality (IMDb ratings). As shown in Figures 5 and 6, both\nfactors exhibit a positive correlation with detection perfor-\nmance, albeit with slightly different patterns. Higher box\noffice revenue leads to a consistent improvement across mod-\nels, with GPT-4o showing the strongest gains. For IMDb\nratings, performance generally improves as ratings increase,\nwith a minor U-shaped trend observed at the lower end for\nGPT-4o and Gemini-1.5 Pro. From a rating of 6 onward,\nthe positive trend becomes more pronounced and consistent\nacross models. These results suggest that both popularity\nand quality serve as useful proxies for memorization, with\neach exhibiting unique dynamics that may vary depending\non the specific range of the factor being analyzed.\n6.5. Preventing Disclosure of Memorization\nThe results in Figure 7 validate our premise that fine-tuning\na model with an alternate target label can effectively prevent\nit from revealing its knowledge of a suspect movie.\nThe results from this experiment align closely with those\npresented in Section 6.1.2. The key insight, however, is\nthat the model learns the task significantly faster, requiring\nonly 500 frames compared to the 1500 frames needed for\nlearning a new movie - a 3x reduction in the number of\nframes needed.\nTo evaluate the generalization capabilities of our approach,\nwe analyze the model’s performance on a subset of the Movi-\neTection subset, focusing on the neutral frames. As shown in\n7\n\nDIS-CO: Discovering Copyrighted Content in VLMs Training Data\n200M 400M 600M 800M\n1B\n1.2B\n1.4B\n1.6B\n1.8B\n2B\n2.2B\nGrossing ($)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nGPT-4o\nGemini-1.5 Pro\nQwen2-VL 72B\nLlama-3.2 90B\nFigure 5. Box-Office effect of suspect movies on DIS-CO’s perfor-\nmance. Higher box-office revenue leads to improved performance\nacross models. This suggests that popular movies are more likely\nto be memorized by models, likely due to their increased presence\nin training datasets. Scores are produced with the weighted combi-\nnation of the main and neutral frames.\n4\n5\n6\n7\n8\n9\nRating\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nGPT-4o\nGemini-1.5 Pro\nQwen2-VL 72B\nLlama-3.2 90B\nFigure 6. IMDb movie rating effect of suspect movies on DIS-\nCO’s performance. Detection improves with higher IMDb ratings,\nwith a notable trend across models starting from a rating of 6.\nHigher-quality movies might have a stronger presence in datasets\nand are therefore more likely to be memorized. Scores are pro-\nduced with the weighted combination of main and neutral frames.\nTable 4, fine-tuning the model to label the Moana movie as\n‘Copyrighted Content’ improved its ability to classify other\nanimated movies (Lion King and Frozen) as copyrighted,\nwith accuracies of 0.625 and 0.450, respectively. This sug-\ngests that the model successfully associates similar visual\nstyles or content characteristics with the ‘Copyrighted Con-\ntent’ label. In contrast, non-animated movies (La La Land\nand Baywatch) exhibited much lower accuracies of 0.050\nand 0.020, respectively. This highlights the model’s capacity\nto generalize within a specific content domain while avoid-\ning overgeneralization across dissimilar genres. Further\nresults are presented in Appendix E.\n0\n1000\n2000\n3000\n4000\n5000\n6000\n# Unique Fine-Tuning Frames\n0.00\n0.25\n0.50\n0.75\n1.00\nAccuracy\nSequential Frames\nRandom Frames\nFigure 7. Accuracy of Qwen2-VL 7B in preventing memorization\ndisclosure of a previously learned movie, as a function of the\nnumber of unique fine-tuning frames.\nTable 4. Accuracy for neutral frames of MovieTection subset be-\nfore and after fine-tuning to prevent disclosing memorization.\nFine-Tuning\nLion King\nFrozen\nLa La Land\nBaywatch\nBefore\n0.000\n0.000\n0.000\n0.000\nAfter\n0.625\n0.450\n0.050\n0.020\n6.6. Human Experiment\nIn this final experiment, our goal is to assess whether\nsome of the performance displayed by DIS-CO could be at-\ntributed to generalization capabilities rather than memoriza-\nU1\nU2\nU3\nU4\nU5\nU6\nU7\nU8\nU9\nU10\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nAccuracy\nUser has NOT SEEN the movie\nUser has SEEN the movie\nFigure 8. Human evaluators’ performance on MovieTection subset.\nScores are produced only based on the neutral frames.\ntion. From Figure 8, two key insights emerge. First, humans\ndemonstrate the ability to recognize certain neutral frames\nwhen they have previously seen the movie, achieving an\naverage detection accuracy of 0.190. This is closely aligned\nwith the performance of ⌊DIS-CO⌋on suspect movies in\nTable 3, though slightly lower (0.190 vs. 0.226).\nThe second insight concerns human accuracy when iden-\ntifying movies they have not seen but may be aware of: a\npure generalization result. In this case, the average accuracy\ndrops significantly to 0.023, highlighting the difficulty of\nrecognizing movies without prior exposure. Even if the\nscores of both DIS-CO variants in Table 3 were adjusted to\naccount for a similar generalization effect, their detection\naccuracy would still surpass text-only detection methods.\nThis supports our hypothesis that the superior performance\nof DIS-CO is not merely a result of generalization or resid-\nual knowledge from publicly available content. Instead, it\nstrongly suggests that the models were exposed to some\ncopyrighted content from MovieTection during training.\n7. Conclusions\nIn this study, we introduce DIS-CO to analyze the potential\ninclusion of copyrighted content in VLMs training data, by\ntesting whether models can map movie frames to their titles\n8\n\nDIS-CO: Discovering Copyrighted Content in VLMs Training Data\nusing free-form text generation. The key intuition is that\nmodels trained on specific content are more likely to identify\nit, even when prompted with less distinctive frames.\nWe validate DIS-CO on recognizing COCO images, a stan-\ndard inclusion on VLM training, and then expand its use to\ndetecting copyrighted movies. The results show that DIS-\nCO consistently outperforms existing approaches while be-\ning compatible with both white-box and black-box models.\nThe limited ability of human evaluators to correctly identify\nmovies they have not seen suggests that the models’ accurate\npredictions are more likely a result of being trained on this\ncontent, rather than generalization or publicly available data.\nImpact Statement\nThis research advances the field of Machine Learning by\nintroducing a method for detecting data used in training\nvision-language models. Our work primarily serves as an\nacademic reference, contributing to a broader understand-\ning of the presence of copyrighted materials within training\ndatasets, for which our findings may help inform discussions\non compliance, attribution, and compensation for content\nowners. However, while our approach offers a new perspec-\ntive, we emphasize that its real-world applications should\nbe considered with caution, given the academic nature and\nlimitations of our methodology.\nWe also recognize that the release of the MovieTection\ndataset may raise ethical considerations related to copyright.\nHowever, we argue that the dataset falls within the scope of\nfair use due to the following reasons:\nFirst, we limit our dataset to 140 frames per title, a small\nfraction of any full-length film, ensuring minimal redistribu-\ntion of copyrighted content.\nSecond, the purpose and scope of MovieTection is strictly\nacademic. The dataset is intended solely for research and\nserves no commercial purpose that could conflict with the\ninterests of copyright holders.\nFinally, we believe that our dataset does not impact the mar-\nket value of the original films. Since the dataset consists of\na sparse collection of individual frames, it does not substi-\ntute for watching the films, nor does it reduce demand for\nlegitimate viewings.\nReferences\nAgrawal, P., Antoniak, S., Hanna, E. B., Bout, B., Chap-\nlot, D., Chudnovsky, J., Costa, D., De Monicault, B.,\nGarg, S., Gervet, T., et al. Pixtral 12B. arXiv preprint\narXiv:2410.07073, 2024.\nAlayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Has-\nson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds,\nM., Ring, R., Rutherford, Simonyan, K., et al. Flamingo:\na Visual Language Model for Few-Shot Learning. In\nAdvances in Neural Information Processing Systems, vol-\nume 35, pp. 23716–23736, 2022.\nCarlini, N., Tram`er, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D. X.,\nErlingsson, ´U., Oprea, A., and Raffel, C. Extracting\nTraining Data from Large Language Models. In USENIX\nSecurity Symposium, 2020.\nCarlini, N., Chien, S., Nasr, M., Song, S., Terzis, A.,\nand Tramer, F. Membership Inference Attacks From\nFirst Principles.\nIn 2022 IEEE Symposium on Secu-\nrity and Privacy (SP), pp. 1897–1914, Los Alamitos,\nCA, USA, may 2022a. IEEE Computer Society. doi:\n10.1109/SP46214.2022.9833649.\nCarlini, N., Ippolito, D., Jagielski, M., Lee, K., Tram`er, F.,\nand Zhang, C. Quantifying Memorization Across Neural\nLanguage Models. ArXiv, abs/2202.07646, 2022b.\nDaily News. Daily News LP v. Microsoft Corporation, April\n2024. URL https://www.courtlistener.com/\ndocket / 68484432 / daily - news - lp - v -\nmicrosoft-corporation/. United States District\nCourt, S.D. of New York, Case No. 1:24-cv-03285, filed\nApril 30, 2024.\nDas, D., Zhang, J., and Tram`er, F. Blind baselines beat mem-\nbership inference attacks for foundation models. arXiv\npreprint arXiv:2406.16201, 2024.\nDuan, M., Suri, A., Mireshghallah, N., Min, S., Shi, W.,\nZettlemoyer, L., Tsvetkov, Y., Choi, Y., Evans, D., and\nHajishirzi, H. Do membership inference attacks work on\nlarge language models? arXiv preprint arXiv:2402.07841,\n2024.\nDuarte, A. V., Zhao, X., Oliveira, A. L., and Li, L. DE-\nCOP: Detecting Copyrighted Content in Language Mod-\nels Training Data. In Proceedings of the 41st Interna-\ntional Conference on Machine Learning, volume 235 of\nProceedings of Machine Learning Research, pp. 11940–\n11956. PMLR, 21–27 Jul 2024.\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle,\nA., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan,\nA., et al. The llama 3 Herd of Models. arXiv preprint\narXiv:2407.21783, 2024.\nGolchin, S. and Surdeanu, M. Data Contamination Quiz:\nA Tool to Detect and Estimate Contamination in Large\nLanguage Models, 2024.\n9\n\nDIS-CO: Discovering Copyrighted Content in VLMs Training Data\nHans, A., Kirchenbauer, J., Wen, Y., Jain, N., Kazemi,\nH., Singhania, P., Singh, S., Somepalli, G., Geiping,\nJ., Bhatele, A., and Goldstein, T. Be like a Goldfish,\nDon’t Memorize! Mitigating Memorization in Genera-\ntive LLMs. In The Thirty-eighth Annual Conference on\nNeural Information Processing Systems, 2024.\nHu, H., Salcic, Z., Sun, L., Dobbie, G., Yu, P. S., and Zhang,\nX. Membership inference attacks on machine learning: A\nsurvey. ACM Computing Surveys (CSUR), 54(11s):1–37,\n2022.\nJayaraman, B., Guo, C., and Chaudhuri, K.\nD´ej`a Vu\nMemorization in Vision–Language Models.\nIn The\nThirty-eighth Annual Conference on Neural Informa-\ntion Processing Systems, 2024.\nURL https://\nopenreview.net/forum?id=SFCZdXDyNs.\nKadrey. Kadrey v. Meta Platforms, Inc., July 2023. URL\nhttps : / / www.courtlistener.com/ docket/\n67569326/kadrey-v-meta-platforms-inc/.\nUnited States District Court, N. D. of California, Case\nNo. 3:23-cv-03417, filed July 7, 2023.\nKaramolegkou, A., Li, J., Zhou, L., and Søgaard, A. Copy-\nright Violations and Large Language Models. In Pro-\nceedings of the 2023 Conference on Empirical Methods\nin Natural Language Processing, pp. 7403–7412, Sin-\ngapore, December 2023. Association for Computational\nLinguistics. doi: 10.18653/v1/2023.emnlp-main.458.\nKnibbs, K. Every AI Copyright Lawsuit in the US, Visual-\nized. Wired, 2024. URL https://www.wired.com/\nstory/ai-copyright-case-tracker/.\nKokhlikyan, N., Jayaraman, B., Bordes, F., Guo, C., and\nChaudhuri, K. Measuring Dejavu Memorization Effi-\nciently. In The Thirty-eighth Annual Conference on Neu-\nral Information Processing Systems, 2024.\nLi, H., Deng, G., Liu, Y., Wang, K., Li, Y., Zhang, T., Liu,\nY., Xu, G., Xu, G., and Wang, H. Digger: Detecting copy-\nright content mis-usage in large language model training.\narXiv preprint arXiv:2401.00676, 2024a.\nLi, Z., Wu, Y., Chen, Y., Tonin, F., Rocamora, E. A., and\nCevher, V. Membership Inference Attacks against Large\nVision-Language Models. In The Thirty-eighth Annual\nConference on Neural Information Processing Systems,\n2024b.\nLin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ra-\nmanan, D., Doll´ar, P., and Zitnick, C. L. Microsoft coco:\nCommon objects in context. In Computer Vision–ECCV\n2014: 13th European Conference, Zurich, Switzerland,\nSeptember 6-12, 2014, Proceedings, Part V 13, pp. 740–\n755. Springer, 2014.\nLison, P. and Tiedemann, J. OpenSubtitles2016: Extracting\nLarge Parallel Corpora from Movie and TV Subtitles. In\nProceedings of the Tenth International Conference on\nLanguage Resources and Evaluation (LREC‘16), pp. 923–\n929, Portoroˇz, Slovenia, May 2016. European Language\nResources Association (ELRA).\nLiu, H., Li, C., Wu, Q., and Lee, Y. J. Visual Instruction\nTuning, 2023.\nLong, Y., Bindschaedler, V., Wang, L., Bu, D., Wang, X.,\nTang, H., Gunter, C. A., and Chen, K. Understanding\nMembership Inferences on Well-Generalized Learning\nModels. arXiv preprint arXiv:1802.04889, 2018.\nMaini, P., Jia, H., Papernot, N., and Dziedzic, A. LLM\nDataset Inference: Did you train on my dataset? arXiv\npreprint arXiv:2406.06443, 2024.\nMeeus, M., Jain, S., Rei, M., and de Montjoye, Y.-A. Did\nthe neurons read your book? document-level membership\ninference for large language models. In 33rd USENIX\nSecurity Symposium (USENIX Security 24), pp. 2369–\n2385, 2024a.\nMeeus, M., Shilov, I., Faysse, M., and Montjoye, Y.-A. D.\nCopyright Traps for Large Language Models. In Pro-\nceedings of the 41st International Conference on Ma-\nchine Learning, volume 235 of Proceedings of Machine\nLearning Research, pp. 35296–35309. PMLR, 21–27 Jul\n2024b.\nMireshghallah, F., Goyal, K., Uniyal, A., Berg-Kirkpatrick,\nT., and Shokri, R. Quantifying Privacy Risks of Masked\nLanguage Models Using Membership Inference Attacks.\nIn Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, pp. 8332–\n8347, Abu Dhabi, United Arab Emirates, December\n2022. Association for Computational Linguistics. doi:\n10.18653/v1/2022.emnlp-main.570.\nNasr, M., Carlini, N., Hayase, J., Jagielski, M., Cooper,\nA. F., Ippolito, D., Choquette-Choo, C. A., Wallace, E.,\nTram`er, F., and Lee, K. Scalable Extraction of Training\nData from (Production) Language Models. arXiv preprint\narXiv:2311.17035, 2023.\nOpenAI.\nGPT-4 Technical Report.\narXiv preprint\narXiv:2303.08774, 2023.\nOpenAI.\nGPT-4o System Card.\narXiv preprint\narXiv:2410.21276, 2024.\nPinto, F., Rauschmayr, N., Tram`er, F., Torr, P., and Tombari,\nF. Extracting Training Data From Document-Based VQA\nModels. In Proceedings of the 41st International Confer-\nence on Machine Learning, volume 235 of Proceedings\nof Machine Learning Research, pp. 40813–40826. PMLR,\n21–27 Jul 2024.\n10\n\nDIS-CO: Discovering Copyrighted Content in VLMs Training Data\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark,\nJ., Krueger, G., and Sutskever, I. Learning Transferable\nVisual Models From Natural Language Supervision. In\nProceedings of the 38th International Conference on Ma-\nchine Learning, volume 139 of Proceedings of Machine\nLearning Research, pp. 8748–8763. PMLR, 18–24 Jul\n2021.\nReid, M., Savinov, N., Teplyashin, D., Lepikhin, D., Lilli-\ncrap, T., Alayrac, J.-b., Soricut, R., Lazaridou, A., Firat,\nO., Schrittwieser, J., et al. Gemini 1.5: Unlocking multi-\nmodal understanding across millions of tokens of context,\n2024.\nSalem, A., Zhang, Y., Humbert, M., Berrang, P., Fritz, M.,\nand Backes, M. Ml-leaks: Model and data independent\nmembership inference attacks and defenses on machine\nlearning models. arXiv preprint arXiv:1806.01246, 2018.\nShi, W., Ajith, A., Xia, M., Huang, Y., Liu, D., Blevins,\nT., Chen, D., and Zettlemoyer, L. Detecting Pretrain-\ning Data from Large Language Models. arXiv preprint\narXiv:2310.16789, 2023.\nShokri, R., Stronati, M., Song, C., and Shmatikov, V. Mem-\nbership Inference Attacks Against Machine Learning\nModels. In 2017 IEEE Symposium on Security and Pri-\nvacy (SP), pp. 3–18, Los Alamitos, CA, USA, may 2017.\nIEEE Computer Society. doi: 10.1109/SP.2017.41.\nSong, L. and Mittal, P. Systematic evaluation of privacy\nrisks of machine learning models. In 30th USENIX Secu-\nrity Symposium (USENIX Security 21), pp. 2615–2632,\n2021.\nWang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen,\nK., Liu, X., Wang, J., Ge, W., et al. Qwen2-VL: Enhanc-\ning vision-language model’s perception of the world at\nany resolution. arXiv preprint arXiv:2409.12191, 2024.\nWatson, L., Guo, C., Cormode, G., and Sablayrolles, A. On\nthe Importance of Difficulty Calibration in Membership\nInference Attacks. In The Tenth International Conference\non Learning Representations, ICLR 2022, Virtual Event,\nApril 25-29, 2022. OpenReview.net, 2022.\nYeom, S., Giacomelli, I., Fredrikson, M., and Jha, S. Privacy\nrisk in machine learning: Analyzing the connection to\noverfitting. In 2018 IEEE 31st computer security founda-\ntions symposium (CSF), pp. 268–282. IEEE, 2018.\nZhang, J., Das, D., Kamath, G., and Tram`er, F. Membership\nInference Attacks Cannot Prove that a Model Was Trained\nOn Your Data. arXiv preprint arXiv:2409.19798, 2024a.\nZhang, J., Sun, J., Yeats, E., Ouyang, Y., Kuo, M., Zhang, J.,\nYang, H. F., and Li, H. Min-k%++: Improved baseline for\ndetecting pre-training data from large language models.\narXiv preprint arXiv:2404.02936, 2024b.\nZhang, W., Zhang, R., Guo, J., de Rijke, M., Fan, Y., and\nCheng, X. Pretraining Data Detection for Large Language\nModels: A Divergence-based Calibration Method. In Pro-\nceedings of the 2024 Conference on Empirical Methods\nin Natural Language Processing, pp. 5263–5274, Mi-\nami, Florida, USA, November 2024c. Association for\nComputational Linguistics.\nZheng, C., Zhou, H., Meng, F., Zhou, J., and Huang, M.\nLarge Language Models Are Not Robust Multiple Choice\nSelectors. In The Twelfth International Conference on\nLearning Representations, 2024a.\nZheng, Y., Zhang, R., Zhang, J., Ye, Y., and Luo, Z. Lla-\nmaFactory: Unified Efficient Fine-Tuning of 100+ Lan-\nguage Models. In Proceedings of the 62nd Annual Meet-\ning of the Association for Computational Linguistics (Vol-\nume 3: System Demonstrations), pp. 400–410, Bangkok,\nThailand, August 2024b. Association for Computational\nLinguistics.\n11\n\nDIS-CO: Discovering Copyrighted Content in VLMs Training Data\nA. Neutral Frames - Examples of Model Predictions\nFigure 9. Movie: Notting Hill\nFigure 10. Movie: 1917\nFigure 11. Movie: Baywatch\nFigure 12. Movie: Jurassic Park\nFigure 13. Movie: La La Land\nFigure 14. Movie: Life of Pi\nFigure 15. Movie: Moana\nFigure 16. Movie: The Greatest Showman\nNeutral frame examples from MovieTection. The tested models are GPT-4o, Qwen2-VL 72B, Gemini-1.5 Pro, and\nLLaMA-3.2 90B (from top to bottom). Green indicates correct predictions, while red denotes incorrect predictions.\n12\n\nDIS-CO: Discovering Copyrighted Content in VLMs Training Data\nB. Captions Generation Prompt - Example\nTable 5. Qwen2-VL 7B Prompt (along with two examples), to generate a detailed image caption. We specifically instruct the model to not\ndisclose specific knowledge of the image to avoid biasing the QA task.\nPrompt: Your task is to generate a detailed caption of this image. You should not reveal if you know who are the\ncharacters / what movie this is from. You need to make an unbiased caption generation, writing at most a single\nparagraph. Start the answer with ‘The image depicts’ ...\nExample 1:\nOutput: The image depicts a grand, ancient tower with intricate architectural details, standing prominently in\nthe foreground of a bustling, sprawling cityscape. The tower features multiple levels, each adorned with ornate\nbalconies and arched windows, some of which are draped with red curtains. The city below is a vibrant mix of\nbuildings, with a variety of architectural styles, including domes and spires, suggesting a rich cultural heritage. The\nscene is bathed in warm sunlight, casting soft shadows and highlighting the textures of the stone and brickwork.\nExample 2:\nOutput: The image depicts a vintage car, likely from the early 20th century, parked indoors in a dimly lit room.\nThe car has a classic design with a red body and gold accents, featuring large, spoked wheels and a prominent front\ngrille. In the background, two individuals are standing near the car, one dressed in a white dress and the other in a\ndark coat, suggesting a scene from a historical or period drama. The setting appears to be a museum or a collection,\ngiven the presence of other vintage vehicles and maritime artifacts in the background.\n13\n\nDIS-CO: Discovering Copyrighted Content in VLMs Training Data\nC. MovieTection Evaluation Prompts\nC.1. DIS-CO: Image Input\nTable 6. Generic prompt example for a MovieTection movie. Image Input with Free-Form Text Generation.\nPrompt: The following image is a frame from a certain scene from a certain movie.\nCan you name the movie?\nHere you should make your guess for the movie in the image. Your guess must be only the movie name.\n<Image Here>\nAnswer:\nC.2. MCQA Image Input\nTable 7. Generic prompt example for a MovieTection movie. Image Input with MCQA.\nUser Prompt: Question: The following image is a frame from a certain scene from a certain movie.\nWhich of the following 4 options is the movie of this frame?\nOptions:\nA. Movie 1\nB. Movie 2\nC. Movie 3\nD. Movie 4\n<Image Here>\nAnswer:\nC.3. Text Captions Only Input\nTable 8. Generic prompt example for a MovieTection movie. Caption Input with Free-Form Text Generation.\nPrompt: The following caption describes a scene from a certain movie.\nCan you name the movie?\nHere you should make your guess for the movie in the image based on the caption. Your guess must be only the\nmovie name.\n<Caption Here>\nAnswer:\n14\n\nDIS-CO: Discovering Copyrighted Content in VLMs Training Data\nD. Models Acknowledge Upcoming Movies\nTable 9. GPT-4o, without searching the web, correctly acknowledges the existence of an upcoming Aquaman movie.\nPrompt: Without looking online, what can you tell me about the Aquaman 2 movie?\nAnswer: As of my latest knowledge update (October 2023), Aquaman and the Lost Kingdom (commonly referred\nto as Aquaman 2) is the sequel to the 2018 film Aquaman. Directed by James Wan, it is part of the DC Extended\nUniverse (DCEU) and stars Jason Momoa as Arthur Curry/Aquaman. The movie was expected to continue\nexploring Aquaman’s dual responsibilities as the King of Atlantis and a superhero defending the surface world.\nKey Details:\n• Plot: The movie was rumored to delve into darker themes compared to its predecessor and would explore\nArthur’s responsibilities as a leader of Atlantis. Black Manta (played by Yahya Abdul-Mateen II) was confirmed\nto return as a major antagonist, seeking revenge against Aquaman for their conflict in the first movie.\n• Returning Cast: In addition to Jason Momoa, other returning cast members include Amber Heard as Mera,\nPatrick Wilson as Orm (Ocean Master), and Temuera Morrison as Thomas Curry (Aquaman’s father).\n• New Additions: The movie was reported to feature new characters, with Pilou Asbæk (known from Game of\nThrones) joining the cast in an unspecified role.\n• Creative Team: James Wan, who directed the first film, returned to helm the sequel. The script was written by\nDavid Leslie Johnson-McGoldrick, who co-wrote the original Aquaman.\n• Visual Style: Like the first film, Aquaman 2 was expected to have visually stunning underwater scenes and\ninnovative effects to depict Atlantis and other aquatic realms.\nTable 10. Complete List of Clean MovieTection Movies that GPT-4o acknowledges the existence of.\nMovie Title\nRelease Date (Month-Year)\nTotal Accuracy\nA Quiet Place: Day One\nJune-2024\n0.00\nAlien: Romulus\nAugust-2024\n0.00\nAquaman and the Lost Kingdom\nDecember-2023\n0.02\nArgylle\nFebruary-2024\n0.06\nBack to Black\nApril-2024\n0.00\nBad Boys: Ride or Die\nJune-2024\n0.00\nBlink Twice\nAugust-2024\n0.00\nBob Marley: One Love\nFebruary-2024\n0.02\nDeadpool & Wolverine\nJuly-2024\n0.00\nDespicable Me 4\nJuly-2024\n0.00\nExhuma\nFebruary-2024\n0.00\nFuriosa: A Mad Max Saga\nMay-2024\n0.00\nGhostbusters: Frozen Empire\nMarch-2024\n0.00\nGodzilla Minus One\nDecember-2023\n0.06\nGodzilla x Kong: The New Empire\nMarch-2024\n0.00\nInside Out 2\nJune-2024\n0.00\nJoker: Folie `a Deux\nOctober-2024\n0.00\nKingdom of the Planet of the Apes\nMay-2024\n0.00\nKung Fu Panda 4\nMarch-2024\n0.00\nTrolls Band Together\nNovember-2023\n0.00\nFrom Table 10, we observe that GPT-4o acknowledges 20 out of the 50 movies listed in MovieTection. Nonetheless, for the\nvast majority of these movies, this acknowledgment does not correspond to a positive accuracy.\n15\n\nDIS-CO: Discovering Copyrighted Content in VLMs Training Data\nE. Fine-Tuning Experiments\nTo conduct the fine-tuning experiments, we select two distinct movies: IF and Moana, representing the clean and suspect\ntitles, respectively. For each movie, we create a supervised fine-tuning (sft) dataset consisting of 6,000 frames. Each frame\nis paired with a task-specific prompt that instructs the model to identify the movie’s title. To avoid overfitting, the prompts\nare randomly sampled from a pool of 30 paraphrased versions, generated using GPT-4o. Figure 17 illustrates one example\nof the created sft data for the selected movies.\nWe explore the trade-off between the quantity of training data and the model’s ability to memorize content by varying the\nproportion of frames used for fine-tuning. Specifically, we test seven configurations, using 1%, 5%, 10%, 25%, 50%, 75%,\nand 100% of the movie’s frames. Additionally, we evaluate two strategies for sampling frames: randomly selecting frames\nfrom the entire movie or selecting frames sequentially in their original order\nFine-tuning is performed using the Qwen2-VL 7B model, leveraging Low-Rank Adaptation (LoRA) as implemented in the\nLlamaFactory framework (Zheng et al., 2024b). The number of training epochs is adjusted proportionally to the percentage\nof frames used, ensuring consistent exposure to the dataset. For instance, when training with the entire dataset (100%), we\nperform one epoch, whereas using half the dataset (50%) involves training for two epochs, effectively maintaining equivalent\nframe coverage across configurations.\n\"content\": The following frame belongs to a movie. Can\nyou guess which one it is? Answer with only the title.\n\"role\": user\n\"content\": IF\n\"role\": assistant\n\"content\": Copyrighted Content\n\"role\": assistant\nFigure 17. Examples from the supervised fine-tuning datasets used in the experiments.\nE.1. Preventing Disclosing Memorization - Additional Results\n1%\n5%\n10%\n25%\n50%\n75%\n100%\nFrame Proportion\n0.00\n0.05\n0.10\n0.15\n0.20\nAccuracy\nMain Frames\nNeutral Frames\nFigure 18. Accuracy for MovieTection suspect split on preventing disclosing memorization, as a function of the number of fine-tuning\nframes. Randomly selected frames\nFigure 18 expands upon the results presented in Table 4, summarizing the average accuracy for main and neutral frames\nwithin the suspect split of MovieTection. The key takeaways are that the model generalizes more effectively to neutral frames\nthan to main ones and that longer fine-tuning on the new content leads to greater prevention of memorization disclosure.\nThese observations align with expectations. Main frames typically contain highly distinctive visual elements, making\nmovies easily recognizable. As a result, fine-tuning on a single movie (Moana) may not be sufficient for the model to fully\ngeneralize the ‘Copyrighted Content’ label to other titles.\nIn contrast, neutral frames are less distinctive. Since the model is already less confident in its responses on these frames, it\nbecomes more susceptible to the influence of fine-tuning. Therefore, when encountering other animated movies, which\nshare stylistic similarities with Moana, the model is more likely to generalize the ‘Copyrighted Content’ label, as we see in\nTable 4.\n16\n\nDIS-CO: Discovering Copyrighted Content in VLMs Training Data\nF. Implementation Details\nWe utilize a diverse set of models, including GPT-4o (OpenAI, 2024), Gemini-1.5 Pro (Reid et al., 2024), LLaMA-3.2\n(Dubey et al., 2024), Qwen2-VL (Wang et al., 2024), LLaVA-v1.5 (Liu et al., 2023), and Pixtral (Agrawal et al., 2024).\nWhen generating detailed captions for the frames, our model requires a certain level of creativity while staying truthful\nto the image content, therefore, we set the temperature=0.1 to achieve this. For evaluation, we aim for complete\ndeterminism, so the temperature parameter is fixed at 0.\nWhen performing inference with GPT-4o and Gemini, we leverage their API functionalities to output responses in JSON\nformat, which ensures better adherence to the task instruction. However, some models, particularly LLaMA and Pixtral,\ntend to struggle with strictly outputting just the movie name, which complicates the automatic evaluation of the task. To\nmitigate this, whenever we observe such inconsistencies, we perform a second model iteration where we feed the outputs to\nGPT-4o Mini, specifically instructed to extract only the movie name.\nMost experiments with white-box models are conducted on a computing cluster equipped with four NVIDIA A100 80GB\nGPUs, allowing their efficient execution without requiring model quantization.\nF.1. Time Analysis - DIS-CO and Baselines\nWe perform an analysis of the expected time that each method\nneeds to evaluate a movie, which we present in Figure 19.\nFirst, we observe that R´enyi is the most time-consuming approach,\nrequiring 306 seconds to complete. MCQA also has a relatively\nlong completion time (105 seconds), which we attribute to the\nneed for a second model iteration, in order to extract the correct\nlabel from the answer. While DIS-CO effectively leads the model\nproduce the expected outputs, MCQA does not exhibit the same\nlevel of reliability. As a result, we must perform an additional\nstep using GPT-4o Mini to extract the correct label, which explains\nwhy MCQA takes longer than DIS-CO. Notably, DIS-CO achieves\nthe fastest completion time at just 41 seconds. Finally, ⌊DIS-CO⌋\ntakes 95 seconds, which aligns with expectations, as it combines\nthe steps of evaluating both on captions and images.\nCaptions\nMCQA\nRényi\nDIS-CO\nDIS-CO\n0\n50\n100\n150\n200\n250\n300\nTime (s)\nFigure 19. The time required to complete an evaluation on\na random MovieTection Movie using Qwen2-VL 7B.\nG. Selection Bias\nOn Figure 20, we present the accuracies of GPT-4o on a MCQA\nsetting for our proof-of-concept experiment of detecting images\nfrom the COCO dataset.\nIn the first experiment (left bar), the correct answer is randomly\nplaced in a different position for each iteration. In the second\nexperiment (right bar), the correct answer is always positioned\nat the same fixed location. Ideally, a model should be robust to\nvariations in answer order, provided it has sufficient knowledge to\nanswer the question accurately.\nThe DIS-CO results in Table 1 suggest that the model possesses the\nknowledge to perform the task accurately. However, the MCQA\nresults reveal that certain answer positions pose significant chal-\nlenges for the model, hindering its ability to correctly select the\nappropriate option. This is therefore a consequence of selection\nbias. To emphasize this limitation, we report the accuracy from the\nsecond experiment (right bar) in Section 6.1.1.\nRandom Position\nFixed Position\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nAccuracy\nFigure 20. Impact of option position on GPT-4o accuracy\nfor VL-MIA/Flickr dataset proof-of-concept experiment.\n17\n\nDIS-CO: Discovering Copyrighted Content in VLMs Training Data\nH. Additional Main Results\nH.1. MovieTection Accuracy on Clean and Suspect Movies - Large Models\nTable 11. Accuracy scores for MovieTection movies included in the training data of VLMs - Suspect\nGPT-4o\nGemini-1.5 Pro\nLLaMA-3.2 90B\nQwen2-VL 72B\nNeutral Frames\nCaptions\n0.1280.011\n0.0790.012\n0.0780.015\n0.0750.010\nMCQA\n0.7210.024\n0.5500.019\n0.5400.028\n0.6170.031\n⌊DIS-CO⌋\n0.2260.021\n0.1520.023\n0.1340.017\n0.1220.012\nDIS-CO\n0.3380.030\n0.2090.031\n0.1760.023\n0.1760.018\nMain Frames\nCaptions\n0.1970.018\n0.1260.020\n0.1220.024\n0.1210.020\nMCQA\n0.7700.025\n0.6380.020\n0.6510.036\n0.6920.034\n⌊DIS-CO⌋\n0.5120.017\n0.3280.024\n0.3000.032\n0.2740.014\nDIS-CO\n0.7040.023\n0.4290.033\n0.4040.039\n0.3770.028\nTable 12. Accuracy scores for MovieTection movies included in the training data of VLMs - Clean\nGPT-4o\nGemini-1.5 Pro\nLLaMA-3.2 90B\nQwen2-VL 72B\nNeutral Frames\nCaptions\n0.0010.001\n0.0000.000\n0.0000.000\n0.0010.001\nMCQA\n0.4100.057\n0.2950.038\n0.2950.052\n0.1490.024\n⌊DIS-CO⌋\n0.0020.001\n0.0040.001\n0.0050.002\n0.0000.000\nDIS-CO\n0.0020.001\n0.0040.001\n0.0050.002\n0.0010.001\nMain Frames\nCaptions\n0.0000.00\n0.0000.00\n0.0000.00\n0.0000.00\nMCQA\n0.4450.046\n0.3800.039\n0.3650.051\n0.1880.025\n⌊DIS-CO⌋\n0.0100.003\n0.0100.005\n0.0130.005\n0.0000.000\nDIS-CO\n0.0100.003\n0.0100.005\n0.0130.005\n0.0000.000\nThe additional accuracy results in Table 11 and Table 12 reinforce the trends observed in Tables 2 and 3 from the main\ntext. While GPT-4o consistently achieves the highest performance, the relative ranking of methods remains stable across all\nmodels.\n(i) MCQA, once again, demonstrates relatively high accuracy for suspect movies across all models; however, this comes at\nthe cost of a high false positive rate on clean movies. This tradeoff undermines its overall reliability, as it leads to misclassify\nnon-memorized content as suspect.\n(ii) Captions, despite occasionally achieving moderate AUC scores (Table 2), exhibit poor accuracy performance, even\nin detecting suspect movies. This limitation is most pronounced in models like Qwen2-VL 72B, where caption-based\nclassification of neutral frames results in an accuracy below 10%. Such results suggest that captions alone are insufficient\nindicators of memorization.\nBy contrast, DIS-CO and ⌊DIS-CO⌋continue to outperform alternative baselines, demonstrating stronger detection capabil-\nities for suspect movies while maintaining low false positive rates for clean movies. Their consistent superiority across\nmodels further underscores their robustness and reliability in identifying memorized content.\n18\n\nDIS-CO: Discovering Copyrighted Content in VLMs Training Data\nH.2. MovieTection Accuracy on Clean and Suspect Movies - Small Open Source Models\nTable 13. Accuracy scores for MovieTection movies included in the training data of Smaller Open-Source VLMs - Suspect\nQwen2-VL 7B\nLLaVA-v1.5 7B\nLLaMA-3.2 11B\nPixtral-12B\nNeutral Frames\nCaptions\n0.0350.010\n0.0290.009\n0.0470.008\n0.0440.009\nMCQA\n0.4850.047\n0.3970.069\n0.4200.014\n-\n⌊DIS-CO⌋\n0.0750.015\n0.0190.006\n0.0890.016\n0.0430.013\nDIS-CO\n0.0990.023\n0.0300.010\n0.1100.020\n0.0580.017\nMain Frames\nCaptions\n0.0660.0016\n0.0700.019\n0.0870.017\n0.0760.015\nMCQA\n0.5580.040\n0.4250.074\n0.5070.036\n-\n⌊DIS-CO⌋\n0.2010.023\n0.0440.016\n0.2150.028\n0.1110.025\nDIS-CO\n0.2600.0034\n0.0720.027\n0.2730.036\n0.1600.0035\nTable 14. Accuracy scores for MovieTection movies included in the training data of Smaller Open-Source VLMs - Clean\nQwen2-VL 7B\nLLaVA-v1.5 7B\nLLaMA-3.2 11B\nPixtral-12B\nNeutral Frames\nCaptions\n0.0010.010\n0.0000.000\n0.0000.000\n0.0010.001\nMCQA\n0.1150.023\n0.0920.037\n0.2770.065\n-\n⌊DIS-CO⌋\n0.0000.000\n0.0000.000\n0.0030.003\n0.000\nDIS-CO\n0.0000.000\n0.0000.000\n0.0030.003\n0.0000.000\nMain Frames\nCaptions\n0.0010.001\n0.0000.000\n0.0000.000\n0.0010.001\nMCQA\n0.1160.020\n0.0920.037\n0.2770.065\n-\n⌊DIS-CO⌋\n0.0000.000\n0.0000.000\n0.0200.017\n0.0000.000\nDIS-CO\n0.0000.000\n0.0000.000\n0.0200.017\n0.0000.000\nThe accuracy results in Tables 13 and 14 extend our analysis to smaller open-source VLMs. DIS-CO and ⌊DIS-CO⌋, while\nexhibiting reduced accuracy in absolute terms compared to the larger models (Tables 11 and 12), maintain their advantage\nover the alternative baselines. These methods consistently demonstrate stronger detection capabilities for suspect movies\nwhile keeping false positives on clean movies to a minimum. LLaVA-v1.5 7B seems to be the only outlier in this trend, as\nboth DIS-CO variants perform closer to captions rather than showing a clear advantage.\n19\n\nDIS-CO: Discovering Copyrighted Content in VLMs Training Data\nI. Long Context - Additional Results\nN=1\nN=2\nN=3\nN=4\nNumber of Frames (N) in Prompt Context\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nGPT-4o\nGemini-1.5 Pro\nQwen2-VL 72B\nLlama-3.2 90B\nFigure 21. DIS-CO’s accuracy on the MovieTection suspect split\nwith varying numbers of frames in the prompt. Scores are pro-\nduced with the main frames and using the large models.\nN=1\nN=2\nN=3\nN=4\nNumber of Frames (N) in Prompt Context\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nGPT-4o\nGemini-1.5 Pro\nQwen2-VL 72B\nLlama-3.2 90B\nFigure 22. DIS-CO’s accuracy on the MovieTection suspect split\nwith varying numbers of frames in the prompt. Scores are pro-\nduced with the neutral frames and using the large models.\nN=1\nN=2\nN=3\nN=4\nNumber of Frames (N) in Prompt Context\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nQwen2-VL 7B\nLlava-v1.5 7B\nLlama-3.2 11B\nPixtral 12B\nFigure 23. DIS-CO’s accuracy on the MovieTection suspect split\nwith varying numbers of frames in the prompt. Scores are pro-\nduced with the main frames and using the smaller models.\nN=1\nN=2\nN=3\nN=4\nNumber of Frames (N) in Prompt Context\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nQwen2-VL 7B\nLlava-v1.5 7B\nLlama-3.2 11B\nPixtral 12B\nFigure 24. DIS-CO’s accuracy on the MovieTection suspect split\nwith varying numbers of frames in the prompt. Scores are pro-\nduced with the neutral frames and using the smaller models.\nIn the main text, we observed in Section 6.3 a general trend where increasing the number of frames in the prompt led to\nimproved detection performance. Here, we extend this analysis by separately evaluating the impact of the two frame types\nalong the multiple models.\nLarge-Scale Models: From Figure 21 and Figure 22 we observe that, regardless of the frame type, the trend remains: more\nframes in the prompt consistently lead to better performance. The only key distinction between the two types is that the\nneutral frames yield lower absolute accuracies. Nonetheless, this is expected given the increased difficulty of detection when\nusing frames that are less informative.\nInterestingly, despite Meta’s official recommendation that LLaMA performs best with a single image during inference3, our\nresults suggest that while the model may not have been explicitly optimized for multi-image inputs, it can still benefit from\nthe extended context in this setting.\nSmaller-Scale Models: These models follow the same pattern observed in Figures 21 and 22. However, their overall\naccuracy remains lower, which is expected given their smaller size and capacity. Only LLaVA appears to be an exception, as\nit does not seem to effectively leverage multiple-image inputs, showing limited improvement compared to the other models.\n3https : / / huggingface.co / meta - llama / Llama - 3.2 - 11B - Vision - Instruct / discussions /\n43#66f98f742094ed9e5f5107d4\n20\n\nDIS-CO: Discovering Copyrighted Content in VLMs Training Data\nJ. Popularity - Additional Results\n200M 400M 600M 800M\n1B\n1.2B\n1.4B\n1.6B\n1.8B\n2B\n2.2B\nGrossing ($)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nGPT-4o\nGemini-1.5 Pro\nQwen2-VL 72B\nLlama-3.2 90B\nFigure 25. Box-Office effect of suspect movies on DIS-CO’s per-\nformance. Scores are produced with the main frames and using\nthe large models.\n200M 400M 600M 800M\n1B\n1.2B\n1.4B\n1.6B\n1.8B\n2B\n2.2B\nGrossing ($)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nGPT-4o\nGemini-1.5 Pro\nQwen2-VL 72B\nLlama-3.2 90B\nFigure 26. Box-Office effect of suspect movies on DIS-CO’s per-\nformance. Scores are produced with the neutral frames and using\nthe large models.\n200M 400M 600M 800M\n1B\n1.2B\n1.4B\n1.6B\n1.8B\n2B\n2.2B\nGrossing ($)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nQwen2-VL 7B\nLlava-v1.5 7B\nLlama-3.2 11B\nPixtral 12B\nFigure 27. Box-Office effect of suspect movies on DIS-CO’s per-\nformance. Scores are produced with the main frames and using\nthe smaller models.\n200M 400M 600M 800M\n1B\n1.2B\n1.4B\n1.6B\n1.8B\n2B\n2.2B\nGrossing ($)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nQwen2-VL 7B\nLlava-v1.5 7B\nLlama-3.2 11B\nPixtral 12B\nFigure 28. Box-Office effect of suspect movies on DIS-CO’s per-\nformance. Scores are produced with the neutral frames and using\nthe smaller models.\nIn the main text, we observed a general trend where higher box-office revenue correlates with improved detection performance\nacross models (Figure 5). Here, we extend this analysis by separately evaluating the impact of the two frame types along the\nmultiple models.\nLarge-Scale Models: Figures 25 and 26 show that higher box-office revenue consistently improves detection performance,\nremaining agnostic to the frame type used. Both main and neutral frames follow similar patterns, with the key distinction\nbeing that neutral frames yield slightly lower absolute accuracies due to their inherent difficulty. This consistency across\nframe types confirms that Figure 5 accurately captures the overall trend of the models, despite presenting results based on\nthe grouping of both frame types.\nSmall-Scale Models: Figures 27 and 28 show a much more inconsistent relationship between box-office revenue and\ndetection accuracy compared to larger models. While LLaMA-3.2 11B, shows a noticeable improvement with higher-\ngrossing films, other models, like LLaVA, display erratic fluctuations with less clear trends.\n21\n\nDIS-CO: Discovering Copyrighted Content in VLMs Training Data\nK. Quality - Additional Results\n4\n5\n6\n7\n8\n9\nRating\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nGPT-4o\nGemini-1.5 Pro\nQwen2-VL 72B\nLlama-3.2 90B\nFigure 29. IMDb movie rating effect of suspect movies on DIS-\nCO’s performance. Scores are produced with the main frames\nand using the large models.\n4\n5\n6\n7\n8\n9\nRating\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nGPT-4o\nGemini-1.5 Pro\nQwen2-VL 72B\nLlama-3.2 90B\nFigure 30. IMDb movie rating effect of suspect movies on DIS-\nCO’s performance. Scores are produced with the neutral frames\nand using the large models.\n4\n5\n6\n7\n8\n9\nRating\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nQwen2-VL 7B\nLlava-v1.5 7B\nLlama-3.2 11B\nPixtral 12B\nFigure 31. IMDb movie rating effect of suspect movies on DIS-\nCO’s performance. Scores are produced with the main frames\nand using the smaller models.\n4\n5\n6\n7\n8\n9\nRating\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nQwen2-VL 7B\nLlava-v1.5 7B\nLlama-3.2 11B\nPixtral 12B\nFigure 32. IMDb movie rating effect of suspect movies on DIS-\nCO’s performance. Scores are produced with the neutral frames\nand using the smaller models.\nIn the main text, we observed that higher IMDb ratings generally led to improved detection performance across models\n(Figure 6). Here, we extend this analysis by separating the main and neutral frame types and evaluating performance across\nboth large-scale and smaller models.\nLarge-Scale Models: Figures 29 and 30 reveal an overall upward trend in detection performance as IMDb ratings increase.\nHowever, an interesting U-shaped pattern is noticeable, particularly in main frames, where detection accuracy initially drops\nfor lower-rated movies (around Rating∈[4,5]) before rising sharply from Rating=6 onward. In contrast, neutral frames\ndisplay a more gradual improvement without the same dip at low ratings. Only Gemini-1.5 Pro, unexpectedly, shows a sharp\ndrop at Rating=9, deviating from the otherwise consistent trend.\nSmall-Scale Models: Figures 31 and 32, on the other hand, show that overall performance remains weak across most rating\nlevels, with a notable exception in Rating=8, where most models exhibit a sudden increase in accuracy, though the reason\nfor this improvement is unclear.\n22\n\nDIS-CO: Discovering Copyrighted Content in VLMs Training Data\nL. Time Effect on MovieTection\nThe proposed temporal split of MovieTection was well suited for the\ntested models, but as new models emerge, the current suspect/clean\nsplit assumption may no longer hold. To explore this, we tested a\nnewer model (Gemini-2.0 Flash) on the clean MovieTection data to\nassess whether it has started acquiring knowledge of these movies.\nFrom Figure 33, we see that while Gemini-1.5 Pro struggles with\nidentifying clean movies, achieving an accuracy of only 0.01,\nGemini-2.0 Flash shows a nearly 10× increase, reaching 0.078.\nAlthough these values remain low and do not suggest that most\nmovies in the split were seen by the new model, individual inspec-\ntion of the results indicates that some titles might raise suspicion.\nIn fact, with Gemini-1.5 Pro, Bob Marley: One Love scores 0.1,\nbut with Gemini-2.0 Flash, the same movie reaches 0.69.\nNov 2023\nSet 2024\nModels Knowledge Cut-off Dates\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\nAccuracy\nGemini-1.5-Pro\nGemini-2.0-Flash\nFigure 33. Effect of knowledge cut-off date on MovieTec-\ntion clean split performance across similar models.\n23\n",
  "metadata": {
    "source_path": "papers/arxiv/DIS-CO_Discovering_Copyrighted_Content_in_VLMs_Training_Data_21c9ffa57dd4fe7c.pdf",
    "content_hash": "21c9ffa57dd4fe7c8b4e0fe252e6149eebc7639e5fe80224de9923c4711e57ca",
    "arxiv_id": null,
    "title": "DIS-CO: Discovering Copyrighted Content in VLMs Training Data",
    "author": "André V. Duarte, Xuandong Zhao, Arlindo L. Oliveira, Lei Li",
    "creation_date": "D:20250225030355Z",
    "published": "2025-02-25T03:03:55",
    "pages": 23,
    "size": 5630669,
    "file_mtime": 1740470157.513278
  }
}