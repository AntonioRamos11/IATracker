{
  "text": "Time series forecasting based on optimized LLM for fault prediction\nin distribution power grid insulators\nJoão P. Matos-Carvalhoa,b,c, Stefano Frizzo Stefenond, Valderi Reis Quietinho Leithardte,\nKin-Choong Yowd\naLASIGE, Departamento de Informática, Faculdade de Ciências, Universidade de Lisboa, 1749–016 Lisboa,\nPortugal,\nbCenter of Technology and Systems (UNINOVA-CTS) and Associated Lab of Intelligent Systems\n(LASI), Caparica, 2829-516, Portugal\ncCOPELABS, Lusófona University, Campo Grande, 376, Lisboa, 1749-024, Portugal\ndFaculty of Engineering and Applied Sciences, University of Regina, Saskatchewan, S4S 0A2, Canada\neInstituto Universitário de Lisboa (ISCTE-IUL), ISTAR, Lisboa, Portugal\nAbstract\nSurface contamination on electrical grid insulators leads to an increase in leakage current until\nan electrical discharge occurs, which can result in a power system shutdown.\nTo mitigate\nthe possibility of disruptive faults resulting in a power outage, monitoring contamination and\nleakage current can help predict the progression of faults. Given this need, this paper proposes\na hybrid deep learning (DL) model for predicting the increase in leakage current in high-\nvoltage insulators.\nThe hybrid structure considers a multi-criteria optimization using tree-\nstructured Parzen estimation, an input stage filter for signal noise attenuation combined with a\nlarge language model (LLM) applied for time series forecasting. The proposed optimized LLM\noutperforms state-of-the-art DL models with a root-mean-square error equal to 2.24×10−4 for\na short-term horizon and 1.21×10−3 for a medium-term horizon.\nKeywords:\nDeep learning, LLM; multi-criteria optimization, power grid insulator, time series forecasting\n1. Introduction\nInsulators in the conventional electrical power distribution and transmission system are\ninstalled outdoors and are susceptible to environmental conditions such as the presence of\ncontamination, solar radiation, atmospheric discharges, and thermal variation [1]. These envi-\nronmental stresses can result in a power outage if preventive or predictive maintenance is not\nhandled [2].\nA way of identifying where the power grid needs more attention is to perform inspections\non the electrical system, either through imaging or using specific equipment to assess the con-\ndition of the system [3]. Besides monitoring using specific equipment such as ultrasound, radio\ninterference, ultraviolet, and infrared cameras, an analysis can be done in terms of power grid\ncontamination. The contamination of insulators is a subject that has been extensively stud-\nied, because over time the contamination increases the surface conductivity of the insulators,\nresulting in flashover [4].\nSpecific techniques for measuring the contamination of insulators, whether this is saline\ncontamination or based on kaolin (artificial contamination) are explored. Measuring leakage\ncurrent is one way of determining how the contamination is influencing the insulator’s degra-\ndation [5]. The higher the surface conductivity of the insulator, the greater the chance of this\n1\narXiv:2502.17341v1  [cs.LG]  24 Feb 2025\n\ncomponent experiencing leakage current, which leads to a flashover voltage in the power system\nthat can result in a complete shutdown of the grid [6].\nConsidering the advances made in predicting time series using machine learning (ML)-based\nmodels, especially involving deep learning (DL) [7], using them to predict the increase in leakage\ncurrent could be an alternative for improving the monitoring performance of electrical systems.\nBased on this premise, this paper proposes a hybrid model for predicting leakage current in\nelectric power distribution insulators. Given the vast nomenclature of the field, the acronyms\nof this paper are standardized according to Table 1.\nThe proposed hybrid model employs noise attenuation considering an input filter stage. The\nChristiano Fitzgerald (CF) asymmetric random walk [8], Hodrick-Prescott (HP) [9], season-\ntrend decomposition using LOESS (STL) [10], multiple STL (MSTL) [11], empirical wavelet\ntransform (EWT) [12], Butterworth [13], and empirical mode decomposition (EDM) [14] filters\nare analyzed, and the best filter is employed in the model input. For the STL and MSTL, the\nLOESS stands for locally estimated scatterplot smoothing.\nBased on the filtered signal, the LLM is used to perform the time series prediction, also\nknown as timeLLM. To ensure that the best structure is used, the Optuna framework using\nthe tree-structured Parzen estimator (TPE) is considered for hyperparameter tuning [15]. Con-\nsidering the proposed optimized LLM model applied for fault prediction, this paper has the\nfollowing contributions:\n• It presents an innovative way of analyzing time series based on LLM models, and is a\nstrategy that can be applied in the future considering the advances in this field.\n• By using a filter on the input signal, unrepresentative noises are disregarded, making the\nprediction model more assertive and promising in chaotic time series analysis.\n• Based on a strategy using TPE, the structure is hyper-adjusted ensuring that the optimal\nhyperparameters are used in the proposed model.\nThe remainder of this paper is as follows: In section 2, related works about fault prediction in\ninsulators are presented. Section 3 explains the proposed optimized LLM model. In section 4\nthe results of the application of the proposed model are discussed, firstly the evaluation of\nthe filter is presented, followed by the tuning strategy. Considering a hypertuned structure,\na statistical analysis and benchmarking are presented. Section 5 presents final remarks and\ndirections for future research.\n2. Related Works\nFault prediction in distribution grid insulators is critical for maintaining the reliability and\nsafety of power systems [16]. Recent studies have explored various methodologies to predict in-\nsulator failures, focusing on analyzing leakage currents, employing ML techniques, and utilizing\nadvanced signal processing methods. A way to identify faults in this context involves monitoring\nthe time series data of the leakage current of insulators under contaminated conditions.\nSeveral authors have applied computer vision-based methods considering convolutional neu-\nral networks (CNNs) for insulator fault identification [17]. Prominent in this area is the you\nonly look once (YOLO) model, which was applied in its third generation by Yang et al. [18], in\nthe fourth generation by Li et al. [19], in the fifth generation by Zhou et al. [20], considering a\nhypertuned version by Stefenon et al. [21], or using a hybrid version in [22].\nDeng et al. [23] proposed a modified YOLO that can be computed on edge devices. The\nmodification to make the algorithm more efficient was in the backbone of the YOLO. Instead of\nusing a CSPDarknet53 (standard for the YOLO version that they considered), they applied a\n2\n\nTable 1: Acronyms considered in this paper.\nAcronym\nName\nANFIS\nAdaptive neuro-fuzzy inference system\nAR\nAuto-regressive\nA-LSTM\nAttention-based long short-term memory\nCF\nChristiano Fitzgerald\nCNN\nConvolutional neural networks\nDL\nDeep learning\nDeepNPTS\nDeep non-parametric time series\nDeepTCN\nDeep temporal convolutional network\nEI\nExpected improvement\nEMD\nEmpirical mode decomposition\nEWT\nEmpirical wavelet transform\nGMDH\nGroup method of data handling\nGRU\nGated recurrent unit\nHP\nHodrick-Prescott\nHFCM\nHigh order fuzzy cognitive maps\nIMF\nIntrinsic mode function\nKDE\nKernel density estimation\nLLM\nLarge language model\nLOESS\nLocally estimated scatterplot smoothing\nLSTM\nLong short-term memory\nMAE\nMean absolute error\nMAPE\nMean absolute percentage error\nML\nMachine learning\nMSTL\nMultiple season-trend decomposition using LOESS\nN-BEATS\nNeural basis expansion analysis\nNHINTS\nNeural hierarchical interpolation for time series\nRMSE\nRoot-mean-square error\nRNN\nRecurrent neural network\nSMAPE\nSymmetric mean absolute percentage error\nSTGCN\nSpatiotemporal graph convolutional network\nSTL\nSeason-trend decomposition using LOESS\nSVM\nSupport vector machine\nTCN\nTemporal convolutional networks\nTFT\nTemporal fusion transformer\nTime-LLM\nTime large language model\nTPE\nTree-structured Parzen estimator\nYOLO\nYou only look once\n3\n\nlightweight network MobilieNetv3. Based on this modification, the proposed algorithm achieved\nan accuracy of 0.945 with a speed of 58.5 frames per second. He et al. [24] also applied a modified\nversion of YOLO, considering a model called multi-fault insulator detection they achieved an\naccuracy of 0.939.\nZhao et al. [25] presented an improved faster region CNN for insulator fault detection.\nThey considered preprocessing techniques for image segmentation, reducing the image noise,\nand having the focus on the insulators, even when complex backgrounds are considered. Based\non their model, it was possible to achieve a mean average precision of 0.908 considering glass\ninsulators and 0.917 when composite insulators were evaluated. Based on CNNs, Lin et al.\n[26] considered infrared images for insulator defect diagnosis. Applications based on CNNs are\npromising, as they can be handled indirectly (without contact with the electrical network) [27],\nwhile in the case of leakage current it is necessary to measure the insulators directly, making it\na more challenging approach.\nSopelsa Neto et al. [28] subjected insulators to saline environments to simulate contami-\nnation and analyzed the progression of leakage currents leading up to disruptive discharges.\nThe researchers evaluated several time series forecasting models, including group method of\ndata handling (GMDH), long short-term memory (LSTM), adaptive neuro-fuzzy inference sys-\ntem (ANFIS), and various ensemble learning models.\nThey found that integrating wavelet\ntransforms with these models improved prediction accuracy, with the wavelet-ANFIS model\nachieving the best performance. In [29] the Christiano-Fitzgerald filter was combined with the\nGMDH to predict faults in contaminated insulators.\nStudies of the contamination process that leads to the development of flashover, such as\nthe one presented by Jin et al. [30] are rarer. Especially when it comes to applications of ML\nmodels, as presented by Zhang et al. [31], or hybrid methods for time series forecasting. In\nother applications, some authors have researched how to better identify faults based on time\nseries analysis, as presented in Table 2.\nSeveral authors (see Table 2) presented hybrid methods for predicting time series, such\nas CNN-LSTM, which uses CNN for feature extraction and LSTM for time series forecast-\ning, showing that combining techniques with different objectives can improve the architecture,\nresulting in a hybrid approach that outperforms the lasted architectures for time series fore-\ncasting. A more detailed discussion of which state-of-the-art models are used for time series\nforecasting is presented in the next subsection.\n2.1. State-of-the-Art in Time Series Forecasting\nIn recent years, advances in ML and DL models have driven the development of more robust\nand accurate methods to predict time series. In this regard, DL-based approaches including\nrecurrent neural networks (RNN) [54] and transformers [55], have shown promising performance.\nIn [56] the authors considered the use of the temporal fusion transformer (TFT) model for time\nseries. This model is an attention-based deep neural network architecture designed for multi-\nhorizon time series forecasting, combining high performance with interpretability. The model\nincorporates static covariate encoders, gating mechanisms, variable selection networks, and\nhybrid temporal processing, which uses LSTMs for local patterns and self-attention to capture\nlong-term dependencies.\nOreshkin et al. [57] proposed the neural basis expansion analysis for time series forecasting\n(N-BEATS), which is a deep neural network architecture designed for univariate time series\nforecasting based on residual connections and multiple fully connected layers. The deep tempo-\nral convolutional network (DeepTCN) was proposed in [58] and is a CNN architecture developed\nfor probabilistic forecasting of multiple related time series. The model employs dilated causal\nconvolutions, which guarantee dependence only on past inputs and capture long-range patterns\n4\n\nTable 2: Fault prediction and anomaly detection using ML approaches considering time series.\nWork\nMethod\nApplication\n[32]\nParallel time series modeling with\nLightNet and DarkNet.\nFault detection on intelligent\nvehicles.\n[33]\nError fusion of multiple sparse auto-\nencoder LSTM.\nMechanical fault prediction.\n[34]\nAttention-based-LSTM,\nrandom\nforest, and extra-tree.\nMachinery fault prediction.\n[35]\nCNN, gated recurrent unit (GRU),\nattention, and knowledge graph.\nMachinery fault diagnosis.\n[36]\nMultiple time-series CNN.\nFault in semiconductor.\n[37]\nMasked spatial graph attention net-\nwork with GRU.\nFault detection for unmanned\naerial vehicles.\n[38]\nTime series transformer.\nMachinery fault diagnosis.\n[39]\nUnsupervised\ndeep\nautoencoder\nwith dimension fusion function.\nFault\ndetection\nin\naero-\nengines.\n[40]\nGaussian-linearized\ntransformer\nwith decomposition.\nFault diagnosis in methane\ngas sensors.\n[41]\nCNN-LSTM with attention.\nWind turbine fault prediction.\n[42]\nCNN-LSTM with one-class support\nvector machine (SVM).\nFault detection in multivari-\nate complex process systems.\n[43]\nStandard LSTM and an LSTM au-\ntoencoder with a one-class SVM.\nAnomaly detection in supply\nchain management.\n[44]\nSpatial\nand\ntemporal\nattention-\nbased GRU with seasonal-trend de-\ncomposition.\nFault\ndiagnosis\nof\nelectro-\nmechanical actuators.\n[45]\nAutoformer enhanced by Dilated\nloss module.\nPotential bushing and trans-\nformer faults.\n[46]\nMulti-scale CNN and LSTM.\nBearing fault diagnosis.\n[47]\nWavelet tranform with LSTM.\nFault in power grids.\n[48]\nLinear regression,\nsupport vector\nregression,\nmultilayer Perceptron,\ndeep neural network, and RNNs.\nFailure prediction in contami-\nnated insulators.\n[49]\nEnsemble random subspace with\nHodrick–Prescott filter.\nFault forecasting in pin-type\ninsulators.\n[50]\nEWT-sequence-to-sequence-LSTM\nwith attention mechanism.\nInsulators fault prediction.\n[51]\nStacking ensemble learning model\nwith wavelet transform.\nInsulators contamination fore-\ncasting.\n[52]\nBootstrap aggregation with Chris-\ntiano–Fitzgerald random walk filter.\nFault\nprediction\nbased\non\nleakage current.\n[53]\nAdaptive neuro-fuzzy inference sys-\ntem with wavelet packets transform.\nInsulator fault forecasting.\n5\n\nwith computational efficiency. DeepTCN demonstrates robustness outperforming models such\nas seasonal autoregressive integrated moving average, light gradient boosting machine [59], and\nthe probabilistic forecasting with autoregressive recurrent network [60].\nIn [61] the authors presented the multivariate time series forecasting with a graph neural\nnetworks model. This is a graph neural network designed-based model to forecast multivari-\nate time series by learning the underlying graph structure. Its architecture combines three\nmain components: a graph learning layer, which extracts dynamic relationships between series\nwithout the need for a predefined graph structure; graph convolution modules, which model\nspatial dependencies between variables; and temporal convolution modules, which capture long-\nterm patterns through dilated convolutions, making it a robust approach for spatiotemporal\nforecasting in different datasets types.\nYu et al. [62] proposed the spatiotemporal graph convolutional network (STGCN), which is\na DL architecture designed for traffic prediction, combining graph convolutions and temporal\nconvolutions with gated linear units to efficiently model spatial and temporal dependencies.\nThe architecture is composed of spatiotemporal convolutional blocks, where causal temporal\nconvolutions extract sequential patterns and graph convolutions capture spatial relationships\nwithout relying on a fixed grid structure. STGCN is computationally efficient, enabling fast and\nscalable training for large networks. The model outperformed other approaches such as full-\nconnected LSTM and graph convolutional GRU in several error metrics such as mean absolute\nerror (MAE), mean absolute percentage error (MAPE), and root-mean-square error (RMSE),\nconsuming up to 14 times less training time compared to state-of-the-art models.\nIn [63] the authors claim that using hybrid models can improve time series forecasting. The\nhybrid model proposed by the authors combines CNNs, attention-based LSTM (A-LSTM), and\nan auto-regressive model (AR) to forecast energy generation from multiple renewable sources.\nCNN captures spatial correlations between energy sources, A-LSTM models non-linear temporal\npatterns, and AR extracts linear trends.\nIts main advantages include the ability to model\ncapture complex temporal patterns and predictive superiority over traditional models such\nas artificial neural networks and decision trees. The model also demonstrated a significant\nreduction in prediction errors compared to previous state-of-the-art approaches, reducing MAE\nby up to 27.1% and MAPE by 53.6% for photovoltaic solar energy. The high R2 values (>0.945)\nconfirm its good fit with the observed data, making it a robust and effective solution for\nforecasting renewable energy.\nMohammadi et al. [64] employed the EWT high-order fuzzy cognitive map that is also a\nhybrid model for forecasting time series that combines the EWT, high-order fuzzy cognitive\nmaps (HFCM), and ridge regression. The model’s architecture decomposes the time series data\nwith EWT, which adapts filters to the signal spectrum, followed by modeling with HFCM,\nwhich captures long-term temporal dependencies, and hyperparameter optimization with ridge\nregression, avoiding overfitting.\nThe forecast is reconstructed using inverse EWT, allowing\ncomplex patterns in non-stationary series to be captured. Evaluated with RMSE on 15 real data\nsets, the model outperformed 11 state-of-the-art algorithms, including LSTM, RNN, ANFIS,\ntemporal convolutional network (TCN), and CNN-fuzzy cognitive maps.\nJin et al. [65] proposed a time series forecasting by reprogramming large language mod-\nels (time-LLM) framework to predict time series, without modifying the backbone language\nmodel. The approach transforms time series into prototypical textual representations and uses\nthe prompt-as-prefix technique to improve the input context. Its main advantages include gen-\neralization to multiple domains, data efficiency, advanced reasoning capabilities, and no need\nfor fine-tuning, allowing robust predictions even with few examples or in zero-shot learning\nscenarios. Evaluated on different datasets, time-LLM outperformed conventional models in\nmetrics such as mean-square error, MAE, symmetric MAPE (SMAPE), and overall weighted\n6\n\naverage, demonstrating high accuracy in short and long-term forecasts.\nIn [66] proposes the use of CNN-LSTM for wind energy forecasting, with advanced hyper-\nparameter optimization to improve accuracy and efficiency. CNN is used to extract spatial\npatterns from the data, while LSTM models short- and long-term temporal dependencies. Dif-\nferent hyperparameter optimization algorithms are evaluated, with Bayesian optimization via\nTPE being the most common approach. The results show that the advanced selection of hy-\nperparameters significantly improves the effectiveness of wind energy forecasting, making the\nmodels more reliable.\nThese studies underscore the importance of integrating advanced analytical methods, such as\nML and signal processing, with traditional monitoring techniques to improve fault prediction\nin distribution grid insulators. Continued research in this area is essential to develop more\naccurate and reliable predictive models, ultimately contributing to the improved stability and\nefficiency of power distribution grids.\n3. Methodology\nThis paper considers the use of a filter input stage for noise attenuation and a hypertuned\nLLM model for time series forecasting. The time series is denoised to remove high frequencies\nand to have the focus of the analysis on the variation trend. To ensure that an optimal structure\nis assumed, the hyperparameters of the LLM are tuned via multi-criteria optimization based\non TPE using the Optuna framework [67]. In this section, all considered techniques employed\nin the proposed optimized LLM model are explained.\n3.1. Input Stage Filter\nFilters are applied in this paper to reduce the high frequencies in the signal, thus focusing\non predicting the trend, which represents the most significant variation in the temporal analysis\nthat leads to failure after a certain time in conditions of high contamination. The decomposition\nis expressed as:\nyt = τt + st + ϵt,\n(1)\nwhere τt is the trend component, st is the seasonal (cyclical) component, and ϵt is the remainder\n(residual) component [68].\nThe focus of the prediction here is the τt because its variation\nrepresents the increase in the leakage current until it reaches the limit accepted by the insulator.\nThe CF, HP, STL, MSTL, EWT, Butterworth, and EDM filters are analyzed for signal\ndenoising, and the best filter is employed in the model input stage. The symbols used in the\nequation of these filters are presented in Table 3. The symbols that are used in both the filters\nand the model are presented in the section that explains the model architecture.\n3.1.1. Christiano Fitzgerald Asymmetric Random Walk (CF) Filter\nThe CF filter is designed for the decomposition of time series data into seasonal and trend\ncomponents. Unlike symmetric filters, the CF filter can operate asymmetrically, making it\nuseful for real-time forecasting. The CF filter is based on the concept of band-pass filtering,\nwhere the goal is to isolate cycles within a specified frequency range [69]. The mathematical\nfoundation of the filter involves approximating the ideal band-pass filter, which has a transfer\nfunction defined as:\nH(ω) =\n(\n1\nif κlower ≤|ω| ≤κupper,\n0\notherwise,\n(2)\n7\n\nSymbol\nDefinition\na, b\nCoefficients of the filter\nc\nIntrinsic mode function\ne\nEnvelop of the IMFs\nh\nCandidate IMF\nm\nEnvelopes of envelopes\nn\nOrder of the filter\np\nLower filter window\nq\nUpper filter window\ns\nSeasonal component\nw\nFilter weights\nF\nFourier transform\nH\nTransfer function\nM\nNumber of seasonal components\nN\nNumber of modes of decomposition\nP\nDegree of the polynomial\nS\nLaplace transform variable\nT\nSampling period\nW\nEmpirical wavelet component\nZ\nComplex variable in the z-domain\nβ\nCoefficient of the polynomial\nδ\nWidth of the transition band\nϵ\nResidual component\nκ\nCutoff frequency\nλ\nSmoothing hyperparameter\nτ\nTrend component\nϕ\nScaling function\nψ\nWavelet function\nω\nAngular frequency\nTable 3: Symbols used in filter equations.\nwhere κlower and κupper are the lower and upper cutoff frequencies, respectively. The filter is\nimplemented as:\nˆst =\nq\nX\nj=−p\nwjyt−j,\n(3)\nwhere yt is the observed time series, ˆst is the estimated seasonal component, wj are the filter\nweights, and p and q define the range of the filter window. The weights wj are determined by\nminimizing the error between the filtered signal and the ideal band-pass filter. Mathematically,\nthis involves solving:\nmin\nwj\nZ π\n−π\n\f\f\f\f\fH(ω) −\nq\nX\nj=−p\nwje−iωj\n\f\f\f\f\f\n2\ndω.\n(4)\nWhen future observations are not available, necessitating the use of an asymmetric filter,\nthe CF filter solves this by adjusting the weights such that:\nˆst =\nq\nX\nj=0\nwjyt−j,\n(5)\n8\n\nwhere only past and present observations are used.\nThe computation of these asymmetric\nweights involves a recursive approach to approximate the frequency response of the ideal band-\npass filter. The CF filter provides a robust method for decomposing and forecasting time series\ndata in real-time applications [29].\n3.1.2. Hodrick-Prescott (HP) Filter\nThe HP filter is used in time series analysis to decompose a series into its trend and seasonal\ncomponents. It is particularly for extracting the smooth long-term trend from noisy data [70].\nThe HP filter decomposes a time series yt into trend τt and seasonal st components, previously\ndefined in Eq. (1). The estimation of τt is formulated as a minimization problem. Specifically,\nthe objective function combines a goodness-of-fit term and a smoothness penalty:\nmin\n{τt}\nT\nX\nt=1\n(yt −τt)2 + λ\nT−1\nX\nt=2\n[(τt+1 −τt) −(τt −τt−1)]2 .\n(6)\nwhere the first term, PT\nt=1(yt −τt)2, ensures that the estimated trend τt fits the data well.\nThe second term, PT−1\nt=2 [(τt+1 −τt) −(τt −τt−1)]2, penalizes changes in the slope of the trend,\neffectively ensuring that τt is smooth over time. The smoothing hyperparameter λ controls the\ntrade-off between these two objectives.\nThe choice of λ is subjective and may affect the decomposition. This issue can be solved\nwith a λ adjustment. When λ →0, the trend τt closely follows the original series yt, allowing\nfor greater flexibility, on the other hand, when λ →∞, the trend becomes a linear function, as\nthe penalty on deviations from linearity dominates [71].\n3.1.3. Season-Trend Decomposition using LOESS (STL) Filter\nThe operation in STL decomposition is based on the application of LOESS, a regression\nmethod used to estimate a smooth function by fitting weighted local polynomials [72]. The\nLOESS procedure at any time point t involves: Initially, it defines the neighborhood around t\nby selecting points within a specified window determined by the smoothing hyperparameter λ,\nwhich controls the fraction of data used in the local fit. Thus, the weights are assigned to the\nobservations within the neighborhood based on their distances from t using a kernel function,\ncommonly the tricube kernel:\nw(x) = (1 −|x|3)3,\nfor |x| ≤1,\nw(x) = 0, otherwise.\n(7)\nFitting a weighted polynomial (typically linear or quadratic) to the data within the neigh-\nborhood, minimizing the locally weighted sum of squared residuals, given by\nmin\nβ0,β1,...,βP\nX\ni\nw(xi)\n \nyi −\nP\nX\nj=0\nβjxj\ni\n!2\n,\n(8)\nwhere β0, β1, . . . , βP are the coefficients of the polynomial, and P is the degree of the polynomial\n(commonly P = 1 or P = 2). Once the decomposition is complete, the trend component τt\ncan be used for forecasting. For example, future values of yt may be predicted by evaluating\nthe trend τt using prediction models. The remainders are often modeled as a stochastic noise,\nassumed as white noise [73].\n9\n\n3.1.4. Multiple Season-Trend Decomposition using LOESS (MSTL) Filter\nThe MSTL is a method used for analyzing time series data, particularly when multiple\nseasonal components and trends are present.\nThe primary goal of MSTL is to decompose\nthe time series into trend, seasonality, and remainder components according to Equation 1.\nMathematically, for a univariate time series yt, the method can be represented as follows:\nyt = τt +\nM\nX\nj=1\nsj,t + ϵt,\n(9)\nwhere sj,t is the seasonal component for the j-th seasonal frequency, where j = 1, . . . , M, and\nM is the number of seasonal components [74].\nLike the STL, the MSTL employs the concept of LOESS to estimate each component iter-\natively. LOESS operates by fitting a polynomial regression model locally for a subset of data\npoints around each time index t, weighted by a kernel function [75]. The kernel assigns higher\nweights to points closer to t and lower weights to points farther away.\nIn the MSTL decomposition process, the seasonal components sj,t are estimated first. Each\nseasonal component corresponds to a specific periodicity, and LOESS smoothing is applied after\naggregating data for that periodicity. MSTL is robust to outliers through the use of LOESS,\nwhere weights are iteratively adjusted based on residuals to reduce the influence of extreme\nvalues [76].\n3.1.5. Empirical Wavelet Transform (EWT) Filter\nThe EWT is a signal decomposition method that is designed to extract meaningful frequency\ncomponents from a signal. Unlike traditional wavelet transforms, which rely on predefined\nmother wavelets and fixed frequency partitions, the EWT constructs wavelet filters based on\nthe spectral characteristics of the input signal. This adaptability makes it a promising method\nfor time series forecasting, especially in cases where signals exhibit non-stationary behavior [77].\nFor each segment [ωk−1, ωk], a scaling function ϕk(ω) and a wavelet function ψk(ω) are con-\nstructed. These functions are designed to satisfy orthogonality and completeness conditions\nover the defined frequency intervals. The ϕk(ω) is used to capture the low-frequency compo-\nnents, while the ψk(ω) isolates the band-limited frequency components [78]. The empirical\nscaling function and wavelet function in the Fourier domain can be defined as:\nϕk(ω) =\n\n\n\n\n\n\n\n1,\nω ∈[0, ωk −δk],\ncos\n\u0010\nπ\n2δk (ω −(ωk −δk))\n\u0011\n,\nω ∈[ωk −δk, ωk],\n0,\notherwise,\n(10)\nψk(ω) =\n\n\n\n\n\n\n\n\n\n\n\n\n\n1,\nω ∈[ωk−1 + δk, ωk −δk],\nsin\n\u0010\nπ\n2δk (ω −(ωk−1 + δk))\n\u0011\n,\nω ∈[ωk−1, ωk−1 + δk],\ncos\n\u0010\nπ\n2δk (ω −(ωk −δk))\n\u0011\n,\nω ∈[ωk −δk, ωk],\n0,\notherwise,\n(11)\nwhere δk is the width of the transition band.\nAfter constructing the wavelet and scaling functions, the signal x(t) is decomposed into\nempirical wavelet coefficients using the inverse Fourier transform. The kth empirical wavelet\ncomponent is given by:\n10\n\nWk(t) = F −1 [ˆx(ω)ψk(ω)] ,\n(12)\nwhere F −1 denotes the inverse Fourier transform. Similarly, the residual low-frequency compo-\nnent is captured by:\nsN(t) = F −1 [ˆx(ω)ϕN(ω)] .\n(13)\nThus, the original signal can be reconstructed as the sum of the decomposed components:\nx(t) = sN(t) +\nN\nX\nk=1\nWk(t).\n(14)\nThe EWT is particularly advantageous for time series forecasting as it effectively isolates\nthe dominant modes of variability, allowing for independent modeling of each component. This\nmodularity facilitates the application of predictive models, such as prediction models, neural\nnetworks, or hybrid approaches, on each extracted component [79].\n3.1.6. Butterworth Filter\nThe Butterworth filter is a signal processing technique used in time series analysis and\nforecasting due to its smooth frequency response and minimal distortion characteristics. It\neffectively separates high-frequency noise from low-frequency trends in time series data. The\nButterworth filter is applied to have a maximally flat magnitude response in the passband,\navoiding ripples in the frequency response [13]. The general transfer function for an nth-order\nButterworth filter is given by:\nH(ω) =\n1\nq\n1 +\n\u0000 ω\nκ\n\u00012n,\n(15)\nwhere ω is the angular frequency, κ is the cutoff frequency, and n is the order of the filter. The\ncutoff frequency κ defines the boundary between the passband and the stopband. The filter’s\norder, n, defines the steepness of the transition between the passband and the stopband. Higher-\norder filters yield sharper transitions but introduce higher computational complexity [80].\nIn time series forecasting, the Butterworth filter is often used as a low-pass filter to extract\nthe low-frequency trend component of a time series. This is accomplished by suppressing high-\nfrequency variations, which are typically associated with noise or short-term fluctuations while\nretaining the underlying trend [81]. To apply the Butterworth filter to discrete time series data,\nthe transfer function is transformed into the Z-domain using the bilinear transformation:\nS =\n\u0012 2\nT\n\u0013 1 −Z−1\n1 + Z−1,\n(16)\nwhere S is the Laplace transform variable, T is the sampling period, and Z is the complex\nvariable in the Z-domain. Substituting this transformation into the continuous-time transfer\nfunction yields the discrete-time transfer function:\n11\n\nH(Z) =\nPn\nk=0 bkZ−k\n1 + Pn\nk=1 akZ−k ,\n(17)\nwhere the coefficients ak and bk are determined by the order of the filter and the cutoff frequency.\nThe recursive form of this difference equation enables efficient implementation of the filter in\ntime series forecasting applications [82].\nThe Butterworth filter minimizes distortion in the trend component by ensuring that the\nmagnitude response in the passband is as flat as possible. This flexibility in adjusting the\ncutoff frequency and filter order allows it to be tailored to the specific characteristics of the\ntime series under study, making it a powerful method for preprocessing and smoothing data\nbefore modeling and prediction [83].\n3.1.7. Empirical Mode Decomposition (EMD) Filter\nThe EMD is an adaptive, data-driven technique for analyzing non-linear and non-stationary\ntime series data. It decomposes a signal into a finite set of components called intrinsic mode\nfunctions (IMFs) and a residual. The decomposition process begins by identifying the IMFs,\nwhich are defined by two conditions: the number of extrema and zero crossings in an IMF must\neither be equal or differ at most by one, and the mean value of the upper envelope and the\nlower envelope must be zero at every point [84].\nTo formalize the process, consider a time series x(t). The first step involves constructing\nenvelopes for the signal using cubic splines to interpolate the local maxima and minima. De-\nnoting the upper and lower envelopes as eupper(t) and elower(t), respectively, their mean m(t) is\ncalculated as\nm(t) = eupper(t) + elower(t)\n2\n.\n(18)\nThe mean is then subtracted from the original signal to produce a candidate IMF:\nh(t) = x(t) −m(t).\n(19)\nThis process, known as sifting, is iterated until h(t) satisfies the IMF criteria. The first IMF\n(c1(t)), captures the highest frequency oscillations in the signal. To extract subsequent IMFs,\nc1(t) is subtracted from the original signal:\nϵ1(t) = x(t) −c1(t),\n(20)\nwhere ϵ1(t) becomes the new input signal for further decomposition. Repeating this procedure\nyields a set of IMFs {c1(t), c2(t), . . . , cn(t)} and a final residual ϵn(t), such that\nx(t) =\nn\nX\ni=1\nci(t) + ϵn(t).\n(21)\nEach IMF isolates oscillatory modes of different scales, making EMD particularly suitable\nfor analyzing signals where distinct temporal scales are present. For time series forecasting,\nconsider that the extracted IMFs often exhibit simpler patterns than the original signal. These\ncomponents can then be modeled independently using ML methods. Forecasting can proceed\nby predicting each IMF ci(t) individually by ML models [85].\n12\n\n3.2. Prediction Model Architecture\nLLM applied for time series, or timeLLM, is built upon the foundational structure of large\nlanguage models by incorporating temporal reasoning directly into its architecture. The in-\nnovation of this model lies in the integration of temporal embeddings, dynamic contextual\nadjustments, and attention mechanisms for time-dependent data. These features enhance the\nmodel’s ability to handle sequential and time-sensitive information [86]. The symbols used in\nthe equation of the model architecture and its hypertuning are presented in Table 4.\nSymbol\nDefinition\nd\nEmbedding dimension\ni, k\nIteration\nl, g\nConditional densities\np\nProbability\nt\nTimestamps or step\nx\nInput data\ny\nObserved time series (objective)\nγ\nQuantile of the observed objective\nη\nEvent in a sequence\nG\nGating function\nK\nKernel function\nL\nLoss function\nN\nNormalization constant\nX\nHyperparameter space\nh\nModified input representation\nv\nVector representation\nE\nStandard token embedding\nK\nKey matrix\nQ\nQuery matrix\nT\nTemporal embedding\nV\nValue matrix\nW\nTime-aware weighting matrix\nTable 4: Symbols used in model equations.\nTemporal embeddings encode time-related information, such as timestamps or relative du-\nrations, by mapping these into a latent space. Mathematically, given a sequence of events\n{η1, η2, . . . , ηn} occurring at corresponding timestamps {t1, t2, . . . , tn}, temporal embeddings\nT(t) map each timestamp ti to a vector representation vi ∈Rd, where d is the embedding\ndimension. These embeddings are learned jointly with the model hyperparameters, ensuring\nthat temporal context is captured alongside linguistic features [87].\nThe temporal embeddings are integrated into the model’s transformer layers. For each input\ntoken xi, the modified input representation is given by\nhi = E(xi) + T(ti),\n(22)\nwhere E(xi) is the standard token embedding and T(ti) is the temporal embedding.\nThis\naddition ensures that the model incorporates temporal information at the input stage.\nThe attention mechanism in timeLLM is adapted to prioritize temporal dependencies. The\nscaled dot-product attention is modified to include a temporal weighting term. For query Q,\nkey K, and value V matrices, the attention weights are computed as\nAttention(Q, K, V) = softmax\n\u0012QKT\n√dk\n+ WT\n\u0013\nV,\n(23)\n13\n\nwhere WT is a time-aware weighting matrix that adjusts the attention based on temporal prox-\nimity [88]. Incorporating dynamic contextual awareness, timeLLM leverages time-dependent\npositional encodings and gating mechanisms. The model introduces a gating function G(t)\nthat modulates the influence of temporal embeddings based on the recency or relevance of\ninformation. The gated representation is computed as\nh′\ni = G(ti) · hi,\n(24)\nwhere G(ti) is a learnable function of ti.\nTimeLLM has practical applications in event forecasting, where it analyzes historical data\nto predict future outcomes. For example, given a time series {(ti, xi)} representing observed\ndata points, the model learns to predict xn+1 by optimizing the loss function\nL =\nn\nX\ni=1\n(xi −ˆxi)2 + λ∥T(t)∥2,\n(25)\nwhere the second term regularizes the temporal embeddings to prevent overfitting. By analyzing\ncross-attention patterns between temporally ordered inputs, timeLLM identifies dependencies\nthat inform predictions and interpretations [89]. The incorporation of these mechanisms makes\ntimeLLM a robust solution for time-sensitive applications, such as time series forecasting pre-\nsented in this paper.\n3.3. Model Hypertuning\nFor hyperparameter model tuning, the TPE is applied. TPE is a model-based optimiza-\ntion algorithm that builds on the Bayesian optimization, tailored for hyperparameter tuning\n[90].\nThe TPE algorithm uses probabilistic modeling to construct surrogate models of the\nobjective function and guides the search for optimal hyperparameter configurations. In this\npaper, the TPE is applied for hypertuning the proposed optimized LLM model, the considered\nhyperparameters are the batch size, dropout, learning rate, and number of heads.\nThe goal of hyperparameter optimization is to minimize or maximize an objective function\nf : X →R, where X is the hyperparameter space [91]. The TPE idea relies on modeling the\nconditional probability p(y | x), where x ∈X is a vector of hyperparameters and y = f(x) is\nthe corresponding objective value. The TPE replace p(y | x) with two conditional densities,\nl(x) and g(x), based on a threshold γ such that:\nl(x) = p(x | y < γ),\ng(x) = p(x | y ≥γ),\n(26)\nwhere γ is a quantile of the observed objective values {y1, y2, . . . , yn}, often chosen as a fixed\npercentile. The TPE algorithm thus rewrites the marginal likelihood p(x | y) using Bayes’\ntheorem:\np(x | y) =\n(\nl(x)p(y < γ)/N\nif y < γ,\ng(x)p(y ≥γ)/N\nif y ≥γ,\n(27)\nwhere N = p(y) is the normalization constant.\nTPE focuses on maximizing the expected improvement (EI) by choosing hyperparameter\nconfigurations that are likely to improve upon the current best observations. The EI criterion\nis reformulated in TPE by using the ratio of densities l(x)/g(x). Intuitively, TPE selects x\nto maximize this ratio, favoring regions of the search space that are more probable under\nl(x) (regions associated with good performance) while being less probable under g(x) (regions\nassociated with worse performance) [92].\n14\n\nThe algorithm constructs l(x) and g(x) using kernel density estimation (KDE). For a set\nof observed hyperparameter configurations {x1, x2, . . . , xn} and their corresponding objective\nvalues {y1, y2, . . . , yn}, the densities are modeled as:\nl(x) =\nX\ni∈Il\nwiK(x, xi),\ng(x) =\nX\ni∈Ig\nwiK(x, xi),\n(28)\nwhere Il = {i | yi < γ}, Ig = {i | yi ≥γ}, wi are the weights associated with each sample, and\nK(x, xi) is a kernel function. The threshold γ is updated dynamically as new observations are\nadded to the dataset.\nBy modeling l(x) and g(x) using KDE, TPE can efficiently handle complex, multimodal\ndistributions over the hyperparameter space. Moreover, it exploits the tree-structured depen-\ndencies within x by breaking the optimization problem into sub-problems, each model with a\nseparate density estimator. The hyperparameter configuration for the next evaluation is sam-\npled by optimizing the acquisition function, which in TPE is the maximization of the density\nratio:\nx∗= arg max\nx∈X\nl(x)\ng(x).\n(29)\nThis process is repeated iteratively, with each new observation refining the density estima-\ntors l(x) and g(x), guiding the search toward better regions of the hyperparameter space. TPE\nis particularly effective for problems with hierarchical or conditional hyperparameter structures,\nmaking it well-suited for modern ML models [93].\nIn this paper, the Optuna framework is considered for the application of the TPE algorithm.\nThe Monte Carlo approach is considered for analyzing different forecast horizons, this method\nrelies on generating random numbers to explore a wide range of possible scenarios within\na model, ensuring adequate random sampling [94].\nThe complete pipeline of the proposed\noptimized LLM method is presented in Figure 1.\nInstance Norm\nPatching\nPatch Reprogram\nPre-trained LLM\nOutput Projection\nOutput Patch \nEmbeddings\nForecasts\nOutput Token \nEmbeddings\nFlatten & Linear\nInput Text\nTokenization\nToken Embedder\nOutput Embeddings\nInput Embeddings\nAdd & Layer Norm\nFeed Forward\nAdd & Layer Norm\nMulti Head Attention\nReprogrammed \nPatch Embeddings\nMulti Head Attention\nLinear\nLinear\nText Proteotypes\nPre-trained  \nWord Embedding\nTraining \nData\nTime Series \nPatching\nPatch \nEmbedder\nData Split\nOriginal Signal\nPre-trained LLM \n(Embedder)\nValidation \nData\nModel \nTraining\n<dataset description>                          \n### Domain: <domain knowledge>                 \n### Instruction: <task information>    \n### Input statistics:                            \n<time series statistic 1>                     \n<time series statistic 2> ?  \nParameter \nTuner\nTraining \nEvaluation \nResults\nTime-LLM \nModel \nModel and \nHyperparameter \nselection during \nprototyping\nStatistical \nValidation\nFiltered \nSignal \nTime-LLM Model\nCF, HP, STL, \nMSTL, EWT, \nEDM, \nButterworth\nValidation \nEvaluation \nResults\nHyperturning\nDenoising\nForward\nBackward\nFrozen\nTraining\nPrompt \nEmbeddings\nPatch \nEmbeddings\nFigure 1: Proposed optimized LLM model.\n15\n\n4. Results and Discussion\nThis section presents the dataset used for the experiments, the analysis setup, the results\nof applying the proposed optimized LLM, and a benchmarking with other well-established DL\nmethods. After defining the dataset and comparison settings used in this paper, the results of\napplying filters to reduce noise are discussed. From the filtered signal, the model is optimized\nusing hypertuning. Once optimized, an analysis is handled concerning the variation in the\nforecast horizon, after which comparative results of the proposed model to other models are\npresented.\n4.1. Dataset\nThe data set considered in this paper refers to leakage current measurements in an experi-\nment on insulators subjected to artificial contamination. The experiment consists of increasing\nthe level of contamination in insulators until a disruptive discharge occurs. The increase in\ncontamination results in an increase in leakage current, which is the main indicator that a fault\nmay occur [95]. Six insulators were evaluated in the experiment, four of which were discharged\nbefore the end of the experiment and were disregarded. Of the two insulators that withstood the\nincrease in contamination without being discharged, only one had a linear increase in leakage\ncurrent, and this was the insulator considered in this study.\nTo reduce the complexity of the analysis, a downsample is handled in the pre-preprocessing\nstage, which means that instead of 96,800 recorded records being considered, corresponding to\n26.9 hours of evaluation, 968 are the focus of the analysis. Electrical discharges occurred in\nmany insulators after the leakage current exceeded 200mA, which is an appropriate threshold\nfor predicting that a fault will occur.\nThe considered signal of the leakage current in the\ninsulator in question is shown in Figure 2.\n0\n200\n400\n600\n800\nSample Index\n0.05\n0.10\n0.15\n0.20\n0.25\nLeakage Current (A)\nFigure 2: Leakage current in an insulator with artificial contamination.\nThe artificial contamination considered was applied in a high-voltage laboratory, in which a\nvoltage of 8.66 kV RMS 60Hz was applied. The increase in contamination was handled following\nthe IEC 60507 [95] standard, which is specific to this evaluation. LabVIEW software was used\nto monitor and save the leakage current values from the experiment. The insulators were fixed\naccording to the power utility’s standards and were grounded to measure the current flowing\nthrough the ground.\n4.2. Experiment Setup\nThe experiments utilized an NVIDIA RTX 3060 TI graphics processing unit with 120 GB\nof random-access memory. The models were implemented in Python. Processing time encom-\npasses the total duration required for both model training and testing. The evaluation metrics\ninclude RMSE, MAE, MAPE, and SMAPE, defined as follows:\n16\n\nRMSE =\nv\nu\nu\nt1\nn\nn\nX\ni=1\n(yi −ˆyi)2,\n(30)\nMAE = 1\nn\nn\nX\ni=1\n|yi −ˆyi|,\n(31)\nMAPE = 100%\nn\nn\nX\nt=1\n\f\f\f\f\nyi −ˆyi\nyi\n\f\f\f\f ,\n(32)\nSMAPE = 100%\nn\nn\nX\ni=1\n|yi −ˆyi|\n(|yi + ˆyi|)/2,\n(33)\nwhere yi is the actual value, ˆy is the forecasted value, and n is the number of observations.\nTo have a comparative assessment of the proposed optimized LLM, the standard RNN,\ndilated RNN, LSTM, GRU, TFT, TCN, informer, DeepNPTS, N-BEATS, and NHITS models\nare evaluated. In the analysis of different forecast horizons, considering multiple LLM models,\nthe Monte Carlo approach was applied with a step equal to 5, from the initial horizon to the\nlast horizon evaluated.\nThe comparative analysis considered multi-horizon prediction, taking into account a horizon\nof 5 to 60 steps ahead (short-term and medium-term horizons). Each sample in the experiment\nconsiders the signal recorded at a time step of one second.\nThe experiment, which lasted\na few hours, represents the accumulation of contaminants that would occur in an external\nenvironment over approximately thirty years. The time it takes for a fault to develop depends\nheavily on the characteristics of the location where the system is installed.\n4.3. Filtering Analysis\nIn the pre-processing stage, to ensure a fair analysis, all the filters considered in this paper\nare applied using their default settings. The results of this evaluation in relation to the original\nsignal are shown in Figure 3. This figure shows the sample index as the horizontal axis, since\nthe downsample is applied to reduce the complexity of the analysis.\n400\n500\n600\n700\n800\n900\nSample Index\n0.100\n0.125\n0.150\n0.175\n0.200\n0.225\n0.250\n0.275\nLeakage Current (A)\nOriginal\nCF\nHP\nSTL\nMSTL\nEWT\nEMD\nButterworth\nFigure 3: Comparison of application of filters to the original signal.\nBased on these results, the timeLLM model is applied to the original signal and the denoised\nsignals by all the filters, as shown in Table 5. In this analysis, the default setup is used in the\nmodel considering a horizon equal to 60 steps ahead. The time needed to process the model\nwith each of the filters was equivalent since the greatest computational effort is regarding the\ntraining stage of the DL model, not the usage of the filter.\n17\n\nTable 5: Results of timeLLM using different filters.\nFilter\nRMSE\nMAE\nMAPE\nSMAPE\nTime (s)\nOriginal\n6.06×10−3\n4.63×10−3\n3.33×10−2\n3.42\n1.24×101\nCF\n5.52×10−3\n4.32×10−3\n3.12×10−2\n3.20\n1.27×101\nHP\n5.38×10−3\n4.23×10−3\n3.05×10−2\n3.13\n1.28×101\nSTL\n5.15×10−3\n3.95×10−3\n2.85×10−2\n2.92\n1.23×101\nMSTL\n5.63×10−3\n4.56×10−3\n3.29×10−2\n3.38\n1.25×101\nEWT\n3.15×10−3\n2.99×10−3\n2.25×10−2\n2.28\n1.25 ×101\nEMD\n5.27×10−3\n3.69×10−3\n2.64×10−2\n2.71\n1.23×101\nButterworth\n5.04×10−3\n3.86×10−3\n2.79×10−2\n2.86\n1.28×101\nBest results in bold\nConsidering the pre-defined setup of the filtering techniques, the best results were found\nusing EWT, as the RMSE is equal to 3.15×10−3, MAE equal to 2.99×10−3, MAPE equal to\n2.25%, and a SMAPE equal to 2.28. It is important to note that all these filters could be\napplied at this stage, but fine-tuning is required for each specific filter.\nThe results of using all the filters were superior to the results obtained by the base model\nwithout using the filter, showing that their application is promising where the model using the\nEWT filter showed, in general, lower errors than the other filters and the original data. In the\nfollowing subsection, the model hypertuning evaluation is presented.\n4.4. Hypertuning Analysis\nAn important definition to be made in the model’s tuning phase is the variability space\n(gap) of each hyperparameter. If the space is not properly defined, the model may struggle to\nfind the optimum. A large search space makes it difficult to optimize the hyperparameters, and\na small space can limit the search, causing the TPE to find values near the extreme of variation\nfor each hyperparameter.\nTo ensure that the model has been properly optimized, hypertuning with a large search\nspace has been done beforehand, so the search space presented here takes into account the gap\nof best values found in the first experiment. Considering the gap of the first experiment, the\nhypertuning is done with the following values of batch size [10 to 20], number of heads [1 to 8],\nlearning rate [0.001 to 0.01], and dropout [0.0 to 0.7]. The results of this analysis are shown\nin Figure 4. This analysis considered a time-LLM model with the EWT filter, using a horizon\nand input size equal to 20. Considering that all the optimal results of the hyperparameters\nevaluated were within the variation gap analyzed, the analysis was carried out properly.\nIn Figure 4, the black dots represent the local minima of the gradient of each combination,\nconsidering Eq. (30) as the loss function. The lighter the gradient (white being the lightest\ncolor), the greater the loss function result and, in turn, the worse the result obtained. Con-\nversely, the darker the color of the gradient (in this case dark blue), the lower the loss function\nresult and, in turn, the better the result, since in both cases the optimization function is to\nreduce the RMSE.\nIn this optimization, the batch size hyperparameter had an importance of 13% in achieving\nthe goal of reducing the RMSE, the learning rate had an importance of 17%, the number of\nheads in the model had an importance of 19%, and the dropout had an importance of 51%,\nmaking it the most important hyperparameter to optimize.\nThe rank of the hyperparameter fitting attempts is shown in Figure 5. The values in the\ncenter of the plots are linear, as they only show the variation of one hyperparameter at a time.\nIt should be noted that, for this case, the redder it gets, the higher the loss function (RMSE\nin this case), and, in turn, the worse the approximation of the output to the ground truth.\n18\n\nFigure 4: Results of multi-objective optimization.\nSimilarly, the lower the value of the loss function, the color will tend toward dark blue, and the\ncloser the output model will be to the ground truth.\nFigure 5: Trial rank of hyperparâmeters.\nMulti-criteria optimization tries interactively to find the best hyperparameter setting to\nminimize the objective function, reducing the RMSE in this paper, by finding an optimum that\n19\n\ncombines the setting of several hyperparameters simultaneously. The optimum values found\nin this experiment were a batch size of 17, using 3 heads, a learning rate of 9.77×10−3, and\na dropout equal to 1.43×10−1, this hyperparameter setup was used for the following analyses,\nthis being the optimized model.\nFigure 6 presents the different combinations between different variables and the lighter the\ncolor of the combination, the higher the loss function value and, consequently, the worse the\nmodel result. The darker the color of the combination between different variables (dark blue\nin this case), the lower the loss function value and therefore the better the model output.\nFigure 6: Model trails in hypertuning.\nAn interesting observation in this presentation (Figure 6) is that many values are close in\nthe trials. This shows an advantage in the application of TPE since when the value of the\nhyperparameter is getting close to the optimum, the algorithm creates new, more assertive\ntrials for multi-criteria optimization. Other hypertuning techniques, such as random sampling,\ncan have a larger search space, but because they are random trials, fewer values close to the\noptimum are evaluated.\n4.5. Multi-horizon Analysis\nThe purpose of the evaluation of horizons is to analyze the impact using a longer forecast\nhorizon has on the model’s performance. To this end, Table 6 presents a comparative analysis\nof various forecast horizons using the proposed optimized LLM model using the EWT as the\ninput stage filter. A forecast of 5 steps ahead is considered a short-term horizon, and 60 steps\nahead is considered a medium-term horizon.\nThe results of this comparison showed that the longer the forecast horizon, the more difficult\nit is to produce a forecast with lower error. The time needed to train the model and carry out\nthe test did not vary considerably when changing the forecast horizon, but the error had a big\nimpact, showing how challenging it is to carry out forecasts with long forecast horizons.\nThe MAPE values show that the model struggles to make predictions for horizons greater\nthan 20 steps ahead since the values were higher than 30%. This challenge is related to the\nvariability of the data, where non-linear time series are more difficult to predict in horizons\nthat consider many steps ahead. This result shows that the proposed model is better suited to\nshort-term than medium-term horizons.\nA visualization of the variation in the model’s performance as the forecast horizon increases\ncan be seen in Figure 7. For random sampling, a Monte Carlo approach is considered to train\nthe model. The model shows a considerably promising result up to a Horizon equal to 20 steps\nahead, after which greater difficulties are encountered in handling the forecast, especially after\n20\n\nTable 6: Time step horizon analysis.\nHorizon\nRMSE\nMAE\nMAPE\nSMAPE\nTime (s)\n05\n2.24×10−4\n1.84×10−4\n1.41×10−3\n1.41×10−1\n1.67×101\n10\n3.58×10−4\n2.95×10−4\n2.25×10−3\n2.25×10−1\n1.68×101\n15\n4.50×10−4\n3.84×10−4\n2.90×10−3\n2.91×10−1\n1.67×101\n20\n4.62×10−4\n3.77×10−4\n2.84×10−3\n2.85×10−1\n1.68×101\n25\n5.79×10−4\n4.98×10−4\n3.76×10−3\n3.77×10−1\n1.71×101\n30\n2.82×10−3\n2.61×10−3\n1.95×10−2\n1.97×10−1\n1.67×101\n35\n5.87×10−4\n4.40×10−4\n3.27×10−3\n3.26×10−1\n1.70×101\n40\n1.32×10−3\n1.07×10−3\n7.86×10−3\n7.82×10−1\n1.68×101\n45\n1.41×10−3\n1.21×10−3\n8.90×10−3\n8.85×10−1\n1.67×101\n50\n1.02×10−3\n8.54×10−4\n6.27×10−3\n6.25×10−1\n1.68×101\n55\n1.54×10−3\n1.19×10−3\n8.66×10−3\n8.60×10−1\n1.70×101\n60\n1.21×10−3\n9.30×10−4\n6.77×10−3\n6.73×10−1\n1.68×101\nBest results in bold\n35 steps ahead, this result is expected because the longer the horizon, the more difficult it\nbecomes to predict the variation.\nFigure 7: Forecast results versus the original signal for different prediction horizons.\n4.6. Statistical Analysis\nTo perform a statistical analysis, the proposed optimized LLM model was computed for\n50 runs with different initialization weights (seed). The result of this evaluation is shown in\nTable 7. The mean, median, mode, range, standard deviation, 25th, 50th, and 75th percentile\n(%ile), interquartile range (IQR), skewness, and kurtosis for each performance metric (RMSE,\nMAE, MAPE and SMAPE) are presented. In general, the results of the statistical analysis\nshow that the model has promising average values, making it a suitable model for the task\ndiscussed here.\nTo give a visual presentation of the variability of the results in relation to the 50 runs,\nFigure 8 shows a box plot of the errors evaluated in this paper using the proposed optimized\nLLM model. This presentation shows some outliers with greater error than the average values\nobtained by the model.\nConsidering that the initialization of the weights is random and that the vast majority\nof the results are close to the average (observing a logarithmic presentation), the results are\npromising. These results show that the model is stable, and based on that, a final comparative\nanalysis is presented in the following.\n21\n\nTable 7: Statistical performance of the hypertuned EWT-LLM considering 50 runs.\nMetrics\nRMSE\nMAE\nMAPE\nSMAPE\nMean\n1.61×10−3\n1.38×10−3\n1.00×10−2\n1.01\nMedian\n1.06×10−3\n8.50×10−4\n6.30×10−3\n6.32×10−1\nMode\n4.40×10−4\n3.60×10−4\n2.66×10−3\n2.65×10−1\nRange\n5.25×10−3\n4.63×10−3\n3.37×10−2\n3.46\nStd. Dev.\n1.40×10−3\n1.25×10−3\n9.07×10−3\n9.24×10−1\n25th %ile\n6.70×10−4\n5.30×10−4\n3.91×10−3\n3.91×10−1\n50th %ile\n1.06×10−3\n8.50×10−4\n6.30×10−3\n6.32×10−1\n75th %ile\n1.82×10−3\n1.56×10−3\n1.13×10−2\n1.14\nIQR\n1.15×10−3\n1.02×10−3\n7.42×10−3\n7.46×10−1\nSkewness\n1.65\n1.66\n1.66\n1.70\nKurtosis\n1.81\n1.88\n1.89\n2.05\nRMSE\nMAE\nMAPE\nSMAPE\n10\n3\n10\n2\n10\n1\n100\nFigure 8: Results of RMSE, MAE, MAPE, and SMAPE considering 50 runs.\n4.7. Benchmarking\nThe comparative analysis presented here focuses on comparing our proposed method to\nother well-established DL architectures. This evaluation considers two different horizons: A\nshort-term horizon of 5-steps ahead and a medium-term horizon of 60-steps ahead. The results\nof the short-term horizon are presented in Table 9, and the results of the medium-term horizon\nare presented are presented in Table 8.\nTable 8: Comparative analysis of DL models for a short-term horizon.\nModel\nRMSE\nMAE\nMAPE\nSMAPE\nTime (s)\nStandard RNN\n5.41×10−3\n5.09×10−3\n3.87×10−2\n3.96\n3.27\nDilated RNN\n3.48×10−3\n3.11×10−3\n2.37×10−2\n2.40\n9.69\nLSTM\n2.06×10−3\n1.95×10−3\n1.48×10−2\n1.49\n3.65\nGRU\n1.87×10−3\n1.71×10−3\n1.30×10−2\n1.29\n4.12\nTFT\n7.20×10−4\n5.81×10−4\n4.43×10−3\n4.42×10−1\n1.08×102\nTCN\n5.87×10−3\n4.85×10−3\n3.69×10−2\n3.59\n4.03\nInformer\n1.22×10−2\n9.33×10−3\n7.09×10−2\n7.30\n7.88×101\nDeepNPTS\n6.67×10−4\n6.02×10−4\n4.59×10−3\n4.58×10−1\n3.34\nN-BEATS\n8.97×10−4\n7.48×10−4\n5.71×10−3\n5.69×10−1\n3.36×101\nNHITS\n8.74×10−4\n7.34×10−4\n5.60×10−3\n5.58×10−1\n4.89×101\nOur\n2.24×10−4\n1.84×10−4\n1.41×10−3\n1.41×10−1\n1.67×101\nBest results in bold\nFor a short-term horizon, the most promising results were considering the TFT and the\n22\n\nDeepNPTS that had an RMSE of 7.20×10−4 and 6.67×10−4 respectively. In this evaluation,\nthe TFT needed a longer time to be computed, which is not an issue here, considering that the\nanalysis is performed offline, and the time required for training is just for computation com-\nparison. Our proposed model (optimized LLM) outperforms all these state-of-the-art models,\nhaving an RMSE of 2.24×10−4 for a short-term horizon.\nTable 9: Comparative analysis of DL models for a medium-term horizon.\nModel\nRMSE\nMAE\nMAPE\nSMAPE\nTime (s)\nStandard RNN\n1.42×10−2\n1.34×10−2\n9.77×10−2\n1.03×101\n6.07\nDilated RNN\n2.69×10−2\n2.64×10−2\n1.94×10−1\n2.15×101\n1.40×101\nLSTM\n2.19×10−2\n2.11×10−2\n1.54×10−1\n1.68×101\n5.81\nGRU\n1.21×10−2\n9.32×10−3\n6.71×10−2\n7.10\n1.15×101\nTFT\n1.97×10−2\n1.68×10−2\n1.23×10−1\n1.13×101\n1.31×103\nTCN\n3.59×10−2\n3.41×10−2\n2.53×10−1\n2.22×101\n5.96\nInformer\n1.05×10−1\n5.10×10−2\n3.73×10−1\n3.79×101\n8.02×102\nDeepNPTS\n7.40×10−3\n6.01×10−3\n4.34×10−2\n4.48\n8.81\nN-BEATS\n5.22×10−3\n3.73×10−3\n2.68×10−2\n2.73\n4.00×101\nNHITS\n6.94×10−3\n4.95×10−3\n3.55×10−2\n3.67\n5.11×101\nOur\n1.21×10−3\n9.30×10−4\n6.77×10−3\n6.73×10−1\n1.68×101\nBest results in bold\nFor a medium-term horizon (horizon = 60), as shown in Table 9, the state-of-the-art models\nthat showed the best results were DeepNPTS, N-BEATS, and NHITS with an RMSE below than\n×10−2. The algorithm proposed in this paper had better results with an RMSE of 1.21 × 10−3,\nwhich clearly shows better performance in predicting future samples.\n5. Conclusion\nIn this paper, a hybrid model integrating a filtering input stage and an LLM applied for\ntime series, optimized via the Optuna framework, was proposed for insulator fault prediction.\nThe time series of the leakage current of the insulators is based on a high-voltage experiment\nwith artificial contamination. Considering an evaluation of CF, HP, STL, MSTL, EWT, But-\nterworth, and EDM filters, the EWT showed more promising results with its default setup.\nTherefore, the EWT decomposed time series signals, mitigating noise and non-stationary ef-\nfects. These decomposed signals were subsequently forecasted by an optimized LLM, which\ndemonstrated robust capabilities in modeling complex temporal dependencies inherent in insu-\nlator degradation patterns.\nThe experimental results underscore the superiority of the optimized LLM framework in\ncomparison to the state-of-the-art DL models, achieving an RMSE equal to 2.24×10−4 for a\nshort-term horizon (5 steps ahead) and 1.21×10−3 for a medium-term horizon (60 steps ahead)\nin insulator fault detection. The proposed methodology enhances the reliability of power grid\nmaintenance and provides a generalizable framework for predictive maintenance in industrial\nsystems.\nFuture work could explore the integration of real-time adaptive decomposition techniques,\nexpand the model’s interpretability for operational decision-making, and validate the framework\non larger cross-domain datasets. Deploying the monitoring model on-site can promote practical\nimplementation in smart grid applications.\n23\n\nAcknowledgements\nThis work was partially supported by the Natural Sciences and Engineering Research Council of\nCanada (NSERC), funding reference number DDG-2024-00035. Cette recherche a été financée\npar le Conseil de recherches en sciences naturelles et en génie du Canada (CRSNG), numéro\nde référence DDG-2024-00035. This research was also partially funded by the Fundação para a\nCiência e a Tecnologia (FCT, https://ror.org/00snfqn58) under Grants UIDB/04111/2020,\nUIDB/00066/2020, UIDB/00408/2020 and the LASIGE Research Unit, ref. UID/000408/2025,\nas well as by the Instituto Lusófono de Investigação e Desenvolvimento (ILIND), Portugal,\nunder Project COFAC/ILIND/COPELABS/1/2024.\nDeclaration of competing interest\nThe authors declare that they have no known competing financial interests or personal\nrelationships that could have appeared to influence the work reported in this paper.\nCRediT authorship contribution statement\nJoão Pedro Matos-Carvalho and Stefano Frizzo Stefenon: Conceptualization, soft-\nware, formal analysis, writing - original draft. Valderi Reis Quietinho Leithard and Kin-\nChoong Yow: Supervision, writing - review & editing.\nData availability\nFor future analysis, the original data is available at: https://github.com/SFStefenon/\nLeakageCurrent.\nReferences\n[1] Mohammad Akbari and Amir Abbas Shayegani-Akmal. Experimental investigation on\nthe accelerated aging of silicone rubber insulators based on thermal stress. International\nJournal of Electrical Power & Energy Systems, 149:109049, 2023. doi: 10.1016/j.ijepes.\n2023.109049.\n[2] Rafael Castillo-Sierra, Oscar Oviedo-Trespalacios, John E Candelo-Becerra, Jose D Soto,\nand Maria Calle. A novel method for prediction of washing cycles of electrical insulators in\nhigh pollution environments. International Journal of Electrical Power & Energy Systems,\n130:107026, 2021. doi: 10.1016/j.ijepes.2021.107026.\n[3] Yanpeng Hao, Yingying Zhang, Zikui Shen, Lin Liu, Congwei Yao, Shenglong Huang,\nZhimin Zhang, Yao Zheng, and Lin Yang. Ultrasonic longitudinal wave reflection prop-\nagation model and defect detection method for the cone of 126-kV three-phase basin in-\nsulators. IEEE Transactions on Instrumentation and Measurement, 73:1–8, 2024. doi:\n10.1109/TIM.2024.3412189.\n[4] Xinhan Qiao, Zhijin Zhang, Raji Sundararajan, Xingliang Jiang, Jianping Hu, and Zhen\nFang. The failure arc paths of the novel device combining an arrester and an insulator under\ndifferent pollution levels. International Journal of Electrical Power & Energy Systems, 125:\n106549, 2021. doi: 10.1016/j.ijepes.2020.106549.\n24\n\n[5] Yong Liu, Hongbao Zong, Sheng Gao, and BX Du. Contamination deposition and dis-\ncharge characteristics of outdoor insulators in fog-haze conditions. International Journal of\nElectrical Power & Energy Systems, 121:106176, 2020. doi: 10.1016/j.ijepes.2020.106176.\n[6] L Cui and M Ramesh. Prediction of flashover voltage using electric field measurement on\nclean and polluted insulators. International Journal of Electrical Power & Energy Systems,\n116:105574, 2020. doi: 10.1016/j.ijepes.2019.105574.\n[7] Hemant Yadav and Amit Thakkar.\nNOA-LSTM: An efficient LSTM cell architecture\nfor time series forecasting.\nExpert Systems with Applications, 238:122333, 2024.\ndoi:\n10.1016/j.eswa.2023.122333.\n[8] Tiago Mota Dutra, José Carlos Dias, and João CA Teixeira. Measuring financial cycles:\nEmpirical evidence for Germany, United Kingdom and United States of America. Interna-\ntional Review of Economics & Finance, 79:599–630, 2022. doi: 10.1016/j.iref.2022.02.039.\n[9] Yves S Schüler. Filtering economic time series: On the cyclical properties of Hamilton’s\nregression filter and the Hodrick-Prescott filter. Review of Economic Dynamics, 54:101237,\n2024. doi: 10.1016/j.red.2024.101237.\n[10] Houtian He, Shangce Gao, Ting Jin, Syuhei Sato, and Xingyi Zhang. A seasonal-trend\ndecomposition-based dendritic neuron model for financial time series prediction. Applied\nSoft Computing, 108:107488, 2021. doi: 10.1016/j.asoc.2021.107488.\n[11] Kasun Bandara, Rob J Hyndman, and Christoph Bergmeir.\nMSTL: A seasonal-trend\ndecomposition algorithm for time series with multiple seasonal patterns. International\nJournal of Operational Research, 52(1):79–98, 2025. doi: 10.1504/IJOR.2025.143957.\n[12] Xianshuang Yao, Huiyu Wang, Yanning Shao, Zhanjun Huang, Shengxian Cao, and\nQingchuan Ma.\nMulti-state delayed echo state network with empirical wavelet trans-\nform for time series prediction.\nApplied Intelligence, 54(6):4646–4667, 2024.\ndoi:\n10.1007/s10489-024-05386-1.\n[13] Xueyan Hu, Wei Liu, and Hua Huo.\nAn intelligent network traffic prediction method\nbased on Butterworth filter and CNN-LSTM. Computer Networks, 240:110172, 2024. doi:\n10.1016/j.comnet.2024.110172.\n[14] Anne Carolina Rodrigues Klaar, Laio Oriel Seman, Viviana Cocco Mariani, and Leandro\ndos Santos Coelho. Random convolutional kernel transform with empirical mode decom-\nposition for classification of insulators from power grid. Sensors, 24(4):1113, 2024. doi:\n10.3390/s24041113.\n[15] Stefano Frizzo Stefenon, Laio Oriel Seman, Luiza Scapinello Aquino da Silva, Vi-\nviana Cocco Mariani, and Leandro dos Santos Coelho.\nHypertuned temporal fusion\ntransformer for multi-horizon time series forecasting of dam level in hydroelectric power\nplants. International Journal of Electrical Power & Energy Systems, 157:109876, 2024.\ndoi: 10.1016/j.ijepes.2024.109876.\n[16] PHV Rocha, EG Costa, AR Serres, GVR Xavier, JEB Peixoto, and RL Lins. Inspection\nin overhead insulators through the analysis of the irradiated RF spectrum. International\nJournal of Electrical Power & Energy Systems, 113:355–361, 2019. doi: 10.1016/j.ijepes.\n2019.05.060.\n25\n\n[17] Xinyu Liu, Xiren Miao, Hao Jiang, and Jing Chen. Box-point detector: A diagnosis method\nfor insulator faults in power lines using aerial images and convolutional neural networks.\nIEEE Transactions on Power Delivery, 36(6):3765–3773, 2021. doi: 10.1109/TPWRD.\n2020.3048935.\n[18] Zhanshe Yang, Zheng Xu, and Yunhao Wang. Bidirection-Fusion-YOLOv3: An improved\nmethod for insulator defect detection using UAV image. IEEE Transactions on Instru-\nmentation and Measurement, 71:1–8, 2022. doi: 10.1109/TIM.2022.3201499.\n[19] Jianqi Li, Yaqian Xu, Keheng Nie, Binfang Cao, Sinuo Zuo, and Jiang Zhu. PEDNet: A\nlightweight detection network of power equipment in infrared image based on YOLOv4-\nTiny. IEEE Transactions on Instrumentation and Measurement, 72:1–12, 2023. doi: 10.\n1109/TIM.2023.3235416.\n[20] Ming Zhou, Bo Li, Jue Wang, and Shi He.\nFault detection method of glass insulator\naerial image based on the improved YOLOv5. IEEE Transactions on Instrumentation and\nMeasurement, 72:1–10, 2023. doi: 10.1109/TIM.2023.3269099.\n[21] Stefano Frizzo Stefenon, Laio Oriel Seman, Anne Carolina Rodrigues Klaar, Raúl García\nOvejero, and Valderi Reis Quietinho Leithardt. Hypertuned-YOLO for interpretable dis-\ntribution power grid fault location based on EigenCAM. Ain Shams Engineering Journal,\n15(6):102722, 2024. doi: 10.1016/j.asej.2024.102722.\n[22] Stefano Frizzo Stefenon, Gurmail Singh, Bruno José Souza, Roberto Zanetti Freire, and\nKin-Choong Yow. Optimized hybrid YOLOu-Quasi-ProtoPNet for insulators classification.\nIET Generation, Transmission & Distribution, 17(15):3501–3511, 2023. doi: 10.1049/gtd2.\n12886.\n[23] Fangming Deng, Zhongxin Xie, Wei Mao, Bing Li, Yun Shan, Baoquan Wei, and Han Zeng.\nResearch on edge intelligent recognition method oriented to transmission line insulator fault\ndetection. International Journal of Electrical Power & Energy Systems, 139:108054, 2022.\ndoi: 10.1016/j.ijepes.2022.108054.\n[24] Min He, Liang Qin, Xinlan Deng, and Kaipei Liu.\nMFI-YOLO: Multi-fault insulator\ndetection based on an improved YOLOv8. IEEE Transactions on Power Delivery, 39(1):\n168–179, 2024. doi: 10.1109/TPWRD.2023.3328178.\n[25] Wenqing Zhao, Minfu Xu, Xingfu Cheng, and Zhenbing Zhao. An insulator in transmis-\nsion lines recognition and fault detection model based on improved faster RCNN. IEEE\nTransactions on Instrumentation and Measurement, 70:1–8, 2021. doi: 10.1109/TIM.2021.\n3112227.\n[26] Yu Lin, Lianfang Tian, and Qiliang Du. Automatic overheating defect diagnosis based\non rotated detector for insulator in infrared image. IEEE Sensors Journal, 23(21):26245–\n26258, 2023. doi: 10.1109/JSEN.2023.3315280.\n[27] Gurmail Singh, Stefano Frizzo Stefenon, and Kin-Choong Yow. Interpretable visual trans-\nmission lines inspections using pseudo-prototypical part network. Machine Vision and\nApplications, 34(3):41, 2023. doi: 10.1007/s00138-023-01390-6.\n[28] N. F. Sopelsa Neto, S. F. Stefenon, L. H. Meyer, R. G. Ovejero, and V. R. Q. Leithardt.\nFault prediction based on leakage current in contaminated insulators using enhanced time\nseries forecasting models. Sensors, 22(16):6121, 2022. doi: 10.3390/s22166121.\n26\n\n[29] Stefano Frizzo Stefenon, Laio Oriel Seman, Nemesio Fava Sopelsa Neto, Luiz Henrique\nMeyer, Viviana Cocco Mariani, and Leandro dos Santos Coelho. Group method of data\nhandling using Christiano-Fitzgerald random walk filter for insulator fault prediction. Sen-\nsors, 23(13):6118, 2023. doi: 10.3390/s23136118.\n[30] Lijun Jin, Gangjie Zhou, Zhikang Yuan, Haifeng Jin, and Xiaokun Man. Study on initial\ndischarge process of insulator pollution flashover considering quantum transition. IEEE\nTransactions on Dielectrics and Electrical Insulation, 29(5):1818–1827, 2022. doi: 10.1109/\nTDEI.2022.3190346.\n[31] Liping Zhang, Xiao Li, Junmei Zhao, Yewei Zhang, and Qiang Zhang. Flashover detection\nand anomaly prediction in aerial images of insulator strings in complex environments.\nIEEE Access, 12:94926–94935, 2024. doi: 10.1109/ACCESS.2024.3424406.\n[32] Yang Xing, Zhongxu Hu, Peng Hang, and Chen Lv.\nLearning from the dark side: A\nparallel time series modelling framework for forecasting and fault detection on intelligent\nvehicles. IEEE Transactions on Intelligent Vehicles, 9(2):3205–3219, 2024. doi: 10.1109/\nTIV.2023.3342648.\n[33] Jianwen Guo, Zhenpeng Lao, Ming Hou, Chuan Li, and Shaohui Zhang. Mechanical fault\ntime series prediction by using EFMSAE-LSTM neural network. Measurement, 173:108566,\n2021. doi: 10.1016/j.measurement.2020.108566.\n[34] Hongling Xu, Ruizhe Ma, Li Yan, and Zongmin Ma. Two-stage prediction of machinery\nfault trend based on deep learning for time series analysis. Digital Signal Processing, 117:\n103150, 2021. doi: 10.1016/j.dsp.2021.103150.\n[35] Haiying Liu, Ruizhe Ma, Daiyi Li, Li Yan, and Zongmin Ma. Machinery fault diagnosis\nbased on deep learning for time series analysis and knowledge graphs. journal of signal\nprocessing systems, 93:1433–1455, 2021. doi: 10.1007/s11265-021-01718-3.\n[36] Chia-Yu Hsu and Wei-Chen Liu. Multiple time-series convolutional neural network for fault\ndetection and diagnosis and empirical study in semiconductor manufacturing. Journal of\nIntelligent Manufacturing, 32(3):823–836, 2021. doi: 10.1007/s10845-020-01591-0.\n[37] Kai He, Daojie Yu, Dong Wang, Mengjuan Chai, Shuntian Lei, and Changlin Zhou. Graph\nattention network-based fault detection for uavs with multivariant time series flight data.\nIEEE Transactions on Instrumentation and Measurement, 71:1–13, 2022. doi: 10.1109/\nTIM.2022.3219489.\n[38] Yuhong Jin, Lei Hou, and Yushu Chen. A time series transformer based method for the\nrotating machinery fault diagnosis. Neurocomputing, 494:379–395, 2022. doi: 10.1016/j.\nneucom.2022.04.111.\n[39] Mengyu Chen, Zechen Li, Xiang Lei, Shan Liang, Shuangxin Zhao, and Yiting Su. Un-\nsupervised fault detection driven by multivariate time series for aeroengines. Journal of\nAerospace Engineering, 36(2):04022129, 2023. doi: 10.1061/JAEEEZ.ASENG-4576.\n[40] Kai Zhang, Wangze Ning, Yudi Zhu, Zhuoheng Li, Tao Wang, Wenkai Jiang, Min Zeng,\nand Zhi Yang. Gaussian-linearized transformer with tranquilized time-series decomposition\nmethods for fault diagnosis and forecasting of methane gas sensor arrays. Applied Sciences,\n14(1):218, 2023. doi: 10.3390/app14010218.\n27\n\n[41] Yuan Xie, Jisheng Zhao, Baohua Qiang, Luzhong Mi, Chenghua Tang, and Longge Li.\nAttention mechanism-based cnn-lstm model for wind turbine fault prediction using SSN\nontology annotation. Wireless Communications and Mobile Computing, 2021(1):6627588,\n2021. doi: 10.1155/2021/6627588.\n[42] Rajeevan Arunthavanathan, Faisal Khan, Salim Ahmed, and Syed Imtiaz. A deep learning\nmodel for process fault prognosis. Process Safety and Environmental Protection, 154:467–\n479, 2021. doi: 10.1016/j.psep.2021.08.022.\n[43] H Du Nguyen, Kim Phuc Tran, Sébastien Thomassey, and Moez Hamad. Forecasting and\nanomaly detection approaches using lstm and lstm autoencoder techniques with the appli-\ncations in supply chain management. International Journal of Information Management,\n57:102282, 2021. doi: 10.1016/j.ijinfomgt.2020.102282.\n[44] Xiaoyu Zhang, Liwei Tang, and Jiusheng Chen. Fault diagnosis for electro-mechanical\nactuators based on STL-HSTA-GRU and SM. IEEE Transactions on Instrumentation and\nMeasurement, 70:1–16, 2021. doi: 10.1109/TIM.2021.3127641.\n[45] Xiangjie Huang, Xun Lang, Songhua Liu, Peng Li, Tao Guo, and Li Yu. Dilated aut-\noformer: An improved model for predicting the state of transformer bushing. In 2024\n43rd Chinese Control Conference (CCC), pages 8858–8863, Kunming, China, 2024. doi:\n10.23919/CCC63176.2024.10662411.\n[46] Xiaohan Chen, Beike Zhang, and Dong Gao. Bearing fault diagnosis base on multi-scale\ncnn and lstm model.\nJournal of Intelligent Manufacturing, 32(4):971–987, 2021.\ndoi:\n10.1007/s10845-020-01600-2.\n[47] Nathielle Waldrigues Branco, Mariana Santos Matos Cavalca, Stefano Frizzo Stefenon, and\nValderi Reis Quietinho Leithardt. Wavelet LSTM for fault forecasting in electrical power\ngrids. Sensors, 22(21):8323, 2022. doi: 10.3390/s22218323.\n[48] A. Medeiros, A. Sartori, S. F. Stefenon, L. H. Meyer, and A. Nied. Comparison of artificial\nintelligence techniques to failure prediction in contaminated insulators based on leakage\ncurrent. Journal of Intelligent & Fuzzy Systems, 42(4):3285–3298, 2022. doi: 10.3233/\nJIFS-211126.\n[49] Laio Oriel Seman, Stefano Frizzo Stefenon, Viviana Cocco Mariani, and Leandro Santos\nCoelho. Ensemble learning methods using the Hodrick–Prescott filter for fault forecasting\nin insulators of the electrical power grids. International Journal of Electrical Power &\nEnergy Systems, 152:109269, 2023. ISSN 0142-0615. doi: 10.1016/j.ijepes.2023.109269.\n[50] Anne Carolina Rodrigues Klaar, Stefano Frizzo Stefenon, Laio Oriel Seman, Viviana Cocco\nMariani, and Leandro Santos Coelho.\nOptimized EWT-Seq2Seq-LSTM with attention\nmechanism to insulators fault prediction.\nSensors, 23(6):3202, 2023.\ndoi:\n10.3390/\ns23063202.\n[51] S. F. Stefenon, M. H. D. M. Ribeiro, A. Nied, V. C. Mariani, L. S. Coelho, V. R. Q.\nLeithardt, L. A. Silva, and L. O. Seman. Hybrid wavelet stacking ensemble model for\ninsulators contamination forecasting. IEEE Access, 9:66387–66397, 2021. doi: 10.1109/\nACCESS.2021.3076410.\n28\n\n[52] Nathielle Waldrigues Branco, Mariana Santos Matos Cavalca, and Raúl García Ove-\njero. Bootstrap aggregation with christiano–fitzgerald random walk filter for fault pre-\ndiction in power systems. Electrical Engineering, 106(3):3657–3670, 2024. doi: 10.1007/\ns00202-023-02146-1.\n[53] Stéfano Frizzo Stefenon, Roberto Zanetti Freire, Leandro Santos Coelho, Luiz Henrique\nMeyer, Rafael Bartnik Grebogi, William Gouvêa Buratto, and Ademir Nied. Electrical\ninsulator fault forecasting based on a wavelet neuro-fuzzy system. Energies, 13(2):484,\n2020. doi: 10.3390/en13020484.\n[54] Hansika Hewamalage, Christoph Bergmeir, and Kasun Bandara. Recurrent neural net-\nworks for time series forecasting: Current status and future directions.\nInternational\nJournal of Forecasting, 37(1):388–427, 2021. ISSN 0169-2070. doi: 10.1016/j.ijforecast.\n2020.06.008.\n[55] Junlong Tong, Liping Xie, Wankou Yang, Kanjian Zhang, and Junsheng Zhao.\nEn-\nhancing time series forecasting: A hierarchical transformer with probabilistic decompo-\nsition representation.\nInformation Sciences, 647:119410, 2023.\nISSN 0020-0255.\ndoi:\n10.1016/j.ins.2023.119410.\n[56] Bryan Lim, Sercan O. Arık, Nicolas Loeff, and Tomas Pfister. Temporal fusion transformers\nfor interpretable multi-horizon time series forecasting. International Journal of Forecasting,\n37(4):1748–1764, 2021. ISSN 0169-2070. doi: 10.1016/j.ijforecast.2021.03.012.\n[57] Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-BEATS:\nNeural basis expansion analysis for interpretable time series forecasting. In International\nConference on Learning Representations, pages 1–31, Addis Ababa, Ethiopia, 2020. ICLR.\n[58] Yitian Chen, Yanfei Kang, Yixiong Chen, and Zizhuo Wang. Probabilistic forecasting\nwith temporal convolutional neural network. Neurocomputing, 399:491–501, 2020. ISSN\n0925-2312. doi: 10.1016/j.neucom.2020.03.011.\n[59] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and\nTie-Yan Liu. LightGBM: A highly efficient gradient boosting decision tree. In Conference\non Neural Information Processing Systems, volume 31, pages 1–9, Long Beach, USA, 2017.\nNIPS.\n[60] David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. Deepar: Proba-\nbilistic forecasting with autoregressive recurrent networks. International Journal of Fore-\ncasting, 36(3):1181–1191, 2020. ISSN 0169-2070. doi: 10.1016/j.ijforecast.2019.07.001.\n[61] Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun Chang, and Chengqi Zhang.\nConnecting the dots: Multivariate time series forecasting with graph neural networks.\nIn International Conference on Knowledge Discovery & Data Mining, volume 26, page\n753–763, New York, USA, 2020. ACM. doi: 10.1145/3394486.3403118.\n[62] Bing Yu, Haoteng Yin, and Zhanxing Zhu. Spatio-temporal graph convolutional networks:\nA deep learning framework for traffic forecasting. In International Joint Conference on\nArtificial Intelligence, volume 27, pages 3634–3640, Stockholm, Sweden, 2018. IJCAI. doi:\n10.24963/ijcai.2018/505.\n29\n\n[63] Jianqin Zheng, Jian Du, Bohong Wang, Jiří Jaromír Klemeš, Qi Liao, and Yongtu Liang.\nA hybrid framework for forecasting power generation of multiple renewable energy sources.\nRenewable and Sustainable Energy Reviews, 172:113046, 2023. ISSN 1364-0321. doi: 10.\n1016/j.rser.2022.113046.\n[64] Hossein Abbasian Mohammadi, Sedigheh Ghofrani, and Ali Nikseresht. Using empirical\nwavelet transform and high-order fuzzy cognitive maps for time series forecasting. Applied\nSoft Computing, 135:109990, 2023. ISSN 1568-4946. doi: 10.1016/j.asoc.2023.109990.\n[65] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang, Xiaoming Shi, Pin-Yu\nChen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, and Qingsong Wen. Time-LLM: Time\nseries forecasting by reprogramming large language models. In International Conference\non Learning Representations, volume 12, pages 1–24, Vienna, Austria, 2024. ICLR.\n[66] Shahram Hanifi, Andrea Cammarono, and Hossein Zare-Behtash. Advanced hyperparam-\neter optimization of deep learning models for wind power prediction. Renewable Energy,\n221:119700, 2024. ISSN 0960-1481. doi: 10.1016/j.renene.2023.119700.\n[67] Polipireddy Srinivas and Rahul Katarya. hyOPTXg: OPTUNA hyper-parameter optimiza-\ntion framework for predicting cardiovascular disease using XGBoost. Biomedical Signal\nProcessing and Control, 73:103456, 2022. doi: 10.1016/j.bspc.2021.103456.\n[68] Zuokun Ouyang, Philippe Ravier, and Meryem Jabloun. STL decomposition of time series\ncan benefit forecasting done by statistical methods but not by machine learning ones.\nEngineering Proceedings, 5(1):42, 2021. doi: 10.3390/engproc2021005042.\n[69] Debesh Bhowmik, Sandeep Poddar, et al. Cyclical and seasonal patterns of india’s gdp\ngrowth rate through the eyes of hamilton and hodrick prescott filter models. Advancement\nin Management and Technology (AMT), 1(3):7–17, 2021. doi: 10.46977/apjmt.2021v01i03.\n002.\n[70] Ziqiang Li, Yun Liu, and Gouhei Tanaka. Multi-reservoir echo state networks with hodrick–\nprescott filter for nonlinear time-series prediction. Applied Soft Computing, 135:110021,\n2023. doi: 10.1016/j.asoc.2023.110021.\n[71] Neslihan Sakarya and Robert M de Jong. The spectral analysis of the hodrick–prescott\nfilter. Journal of Time Series Analysis, 43(3):479–489, 2022. doi: 10.1111/jtsa.12622.\n[72] Li Xu, Yanxia Ou, Jingjing Cai, Jin Wang, Yang Fu, and Xiaoyan Bian. Offshore wind\nspeed assessment with statistical and attention-based neural network methods based on stl\ndecomposition. Renewable Energy, 216:119097, 2023. doi: 10.1016/j.renene.2023.119097.\n[73] Anne Carolina Rodrigues Klaar, Stefano Frizzo Stefenon, Laio Oriel Seman, Viviana Cocco\nMariani, and Leandro Santos Coelho. Structure optimization of ensemble learning methods\nand seasonal decomposition approaches to energy price forecasting in Latin America: A\ncase study about Mexico. Energies, 16(7):3184, 2023. doi: 10.3390/en16073184.\n[74] Amirhossein Sohrabbeig, Omid Ardakanian, and Petr Musilek. Decompose and conquer:\nTime series forecasting with multiseasonal trend decomposition using loess. Forecasting, 5\n(4):684–696, 2023. doi: 10.3390/forecast5040037.\n[75] Manel Rhif, Ali Ben Abbes, Beatriz Martinez, Rogier de Jong, Yanfang Sang, and Imed Ri-\nadh Farah. Detection of trend and seasonal changes in non-stationary remote sensing data:\n30\n\nCase study of tunisia vegetation dynamics. Ecological Informatics, 69:101596, 2022. doi:\n10.1016/j.ecoinf.2022.101596.\n[76] Mohammed Elseidi. MSTL-NNAR: A new hybrid model of machine learning and time\nseries decomposition for wind speed forecasting. Stochastic Environmental Research and\nRisk Assessment, 38:2613–2632, 2024. doi: 10.1007/s00477-024-02701-7.\n[77] Hossein Abbasian Mohammadi, Sedigheh Ghofrani, and Ali Nikseresht. Using empirical\nwavelet transform and high-order fuzzy cognitive maps for time series forecasting. Applied\nSoft Computing, 135:109990, 2023. doi: 10.1016/j.asoc.2023.109990.\n[78] Lu Peng, Lin Wang, De Xia, and Qinglu Gao. Effective energy consumption forecasting\nusing empirical wavelet transform and long short-term memory. Energy, 238:121756, 2022.\ndoi: 10.1016/j.energy.2021.121756.\n[79] Wei Liu and Wei Chen.\nRecent advancements in empirical wavelet transform and its\napplications. IEEE Access, 7:103770–103780, 2019. doi: 10.1109/ACCESS.2019.2930529.\n[80] Shibendu Mahata, Norbert Herencsar, and David Kubanek. Optimal approximation of\nfractional-order butterworth filter based on weighted sum of classical butterworth filters.\nIEEE Access, 9:81097–81114, 2021. doi: 10.1109/ACCESS.2021.3085515.\n[81] Pushpendra Singh, Amit Singhal, Binish Fatimah, and Anubha Gupta. A novel PRFB\ndecomposition for non-stationary time-series and image analysis. Signal Processing, 207:\n108961, 2023. doi: 10.1016/j.sigpro.2023.108961.\n[82] Qin Guodong and Chen Zhongxian.\nA comparison of ocean wave height forecasting\nmethods for ocean wave energy conversion systems.\nWater, 15(18):3256, 2023.\ndoi:\n10.3390/w15183256.\n[83] Rathachai Chawuthai, Nachaphat Ainthong, Surasee Intarawart, Niracha Boonyanaet,\nand Agachai Sumalee. Travel time prediction on long-distance road segments in thailand.\nApplied Sciences, 12(11):5681, 2022. doi: 10.3390/app12115681.\n[84] Wanming Ying, Jinde Zheng, Haiyang Pan, and Qingyun Liu. Permutation entropy-based\nimproved uniform phase empirical mode decomposition for mechanical fault diagnosis.\nDigital Signal Processing, 117:103167, 2021. doi: 10.1016/j.dsp.2021.103167.\n[85] Ming-De Liu, Lin Ding, and Yu-Long Bai. Application of hybrid model based on empirical\nmode decomposition, novel recurrent neural networks and the arima to wind speed predic-\ntion. Energy Conversion and Management, 233:113917, 2021. doi: 10.1016/j.enconman.\n2021.113917.\n[86] Shamsu Abdullahi, Kamaluddeen Usman Danyaro, Abubakar Zakari, Izzatdin Abdul Aziz,\nNoor Amila Wan Abdullah Zawawi, and Shamsuddeen Adamu. Time-series large language\nmodels: A systematic review of state-of-the-art. IEEE Access, pages 1–28, 2025. doi:\n10.1109/ACCESS.2025.3535782.\n[87] Yongqian Sun, Yang Guo, Minghan Liang, Xidao Wen, Junhua Kuang, Shenglin Zhang,\nHongbo Li, Kaixu Xia, and Dan Pei. Multivariate time series anomaly detection based\non pre-trained models with dual-attention mechanism. In International Symposium on\nSoftware Reliability Engineering Workshops (ISSREW), volume 35, pages 73–78, Tsukuba,\nJapan, 2024. IEEE. doi: 10.1109/ISSREW63542.2024.00050.\n31\n\n[88] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y Zhang, Xiaoming Shi, Pin-Yu\nChen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, et al. Time-llm: Time series forecasting\nby reprogramming large language models. arXiv:2310.01728, 2:1–24, 2024. doi: 10.48550/\narXiv.2310.01728.\n[89] Georgios Samaras, Marinela Mertiri, Maria-Evgenia Xezonaki, Vasileios Theodorou, Pan-\nteleimon Konstantinos Chartsias, and Theodoros Bozios.\nUnlocking the path towards\nAI-native networks with optimized lightweight large language models. In International\nMediterranean Conference on Communications and Networking (MeditCom), volume 1,\npages 108–113, Madrid, Spain, 2024. IEEE. doi: 10.1109/MeditCom61057.2024.10621076.\n[90] Zeyu Zhang, Xiaoqian Liu, Xiling Zhang, Zhishan Yang, and Jian Yao. Carbon price\nforecasting using optimized sliding window empirical wavelet transform and gated re-\ncurrent unit network to mitigate data leakage.\nEnergies, 17(17):4358, 2024.\ndoi:\n10.3390/en17174358.\n[91] Stefano Frizzo Stefenon, Laio Oriel Seman, Evandro Cardozo da Silva, Erlon Cristian\nFinardi, Leandro dos Santos Coelho, and Viviana Cocco Mariani. Hypertuned wavelet\nconvolutional neural network with long short-term memory for time series forecasting in\nhydroelectric power plants. Energy, 313:133918, 2024. ISSN 0360-5442. doi: 10.1016/j.\nenergy.2024.133918.\n[92] Bernardo Luis Tuleski, Cristina Keiko Yamaguchi, Stefano Frizzo Stefenon, Leandro dos\nSantos Coelho, and Viviana Cocco Mariani.\nAudio-based engine fault diagnosis with\nwavelet, Markov blanket, ROCKET, and optimized machine learning classifiers. Sensors\n(Basel, Switzerland), 24(22):7316, 2024. doi: 10.3390/s24227316.\n[93] Evandro Cardozo da Silva, Erlon Cristian Finardi, and Stefano Frizzo Stefenon. Enhancing\nhydroelectric inflow prediction in the Brazilian power system: A comparative analysis of\nmachine learning models and hyperparameter optimization for decision support. Electric\nPower Systems Research, 230:110275, 2024. doi: 10.1016/j.epsr.2024.110275.\n[94] Adolphus Lye, Alice Cicirello, and Edoardo Patelli. Sampling methods for solving bayesian\nmodel updating problems: A tutorial. Mechanical Systems and Signal Processing, 159:\n107760, 2021. doi: 10.1016/j.ymssp.2021.107760.\n[95] IEC 60507. Artificial pollution tests on high-voltage ceramic and glass insulators to be\nused on A.C. systems. International standard, International Electrotechnical Commission\n(IEC), Geneva, Switzerland, March 2013.\n32\n",
  "metadata": {
    "source_path": "papers/arxiv/Time_series_forecasting_based_on_optimized_LLM_for_fault_prediction_in\n__distribution_power_grid_insulators_334c0327d23739bd.pdf",
    "content_hash": "334c0327d23739bd8b37104f8e6b1036c49627486f39d749f80a4346f918d9c4",
    "arxiv_id": null,
    "title": "Time series forecasting based on optimized LLM for fault prediction in distribution power grid insulators",
    "author": "João P. Matos-Carvalho; Stefano Frizzo Stefenon; Valderi Reis Quietinho Leithardt; Kin-Choong Yow; ",
    "creation_date": "D:20250225030248Z",
    "published": "2025-02-25T03:02:48",
    "pages": 32,
    "size": 2973919,
    "file_mtime": 1740470159.1013837
  }
}