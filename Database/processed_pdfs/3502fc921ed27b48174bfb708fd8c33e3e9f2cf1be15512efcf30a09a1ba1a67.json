{
  "text": "Distributional Vision-Language Alignment by Cauchy-Schwarz Divergence\nWenzhe Yin * 1 Zehao Xiao * 1 Pan Zhou 2 Shujian Yu 3 4 Jiayi Shen 1 Jan-Jakob Sonke 5 Efstratios Gavves 1\nAbstract\nMultimodal alignment is crucial for various down-\nstream tasks such as cross-modal generation and\nretrieval. Previous multimodal approaches like\nCLIP maximize the mutual information mainly\nby aligning pairwise samples across modalities\nwhile overlooking the distributional differences,\nleading to suboptimal alignment with modality\ngaps. In this paper, to overcome the limitation,\nwe propose CS-Aligner, a novel and straight-\nforward framework that performs distributional\nvision-language alignment by integrating Cauchy-\nSchwarz (CS) divergence with mutual informa-\ntion. In the proposed framework, we find that the\nCS divergence and mutual information serve com-\nplementary roles in multimodal alignment, cap-\nturing both the global distribution information of\neach modality and the pairwise semantic relation-\nships, yielding tighter and more precise alignment.\nMoreover, CS-Aligher enables incorporating addi-\ntional information from unpaired data and token-\nlevel representations, enhancing flexible and fine-\ngrained alignment in practice. Experiments on\ntext-to-image generation and cross-modality re-\ntrieval tasks demonstrate the effectiveness of our\nmethod on vision-language alignment.\n1. Introduction\nModality alignment is a cornerstone of multimodal repre-\nsentation learning, enabling success across diverse applica-\ntions such as image-text retrieval (Huang et al., 2024; Kouk-\nounas et al., 2024), text-to-image (T2I) generation (Ramesh\net al., 2022; Razzhigaev et al., 2023), and multimodal chat-\nbots (Zhu et al., 2023). As a pioneering work in this field,\nCLIP (Radford et al., 2021) leverages contrastive loss to\nmaximize the mutual information between paired text and\nimage representations, effectively capturing pairwise and se-\n*Equal contribution\n1University of Amsterdam 2Singapore\nManagement University 3Vrije Universiteit Amsterdam 4The Arc-\ntic University of Norway 5The Netherlands Cancer Institute. Cor-\nrespondence to: Pan Zhou <panzhou@smu.edu.sg>.\n(a) Without CS-Aligner\n(b) With CS-Aligner\nFigure 1: TSNE visualizations of CLIP text and image\nfeatures without (a) and with (b) CS-Aligner. The orig-\ninal CLIP feature distributions reveal a clear domain gap\n(a). Adapting the model with our CS-Aligner effectively\neliminates the modality gap, leading to tighter alignment.\nmantic relationships. Its versatility has made it a foundation\nfor many multimodal tasks.\nAlthough widely adopted, CLIP suffers from a persistent\nmodality gap between text and image representations in its\nlatent space. As shown in Fig. 1a, text and image embed-\ndings often fail to align precisely and may remain separated\nfrom each other. This modality gap has been observed and\nexplored in prior studies (Zhou et al., 2023; Liang et al.,\n2022; Shi et al., 2023), which attribute the issue to factors\nsuch as cone effects (Liang et al., 2022) or suboptimal la-\ntent space structures (Shi et al., 2023). Intriguingly, Liang\net al. (2022) observed a phenomenon that CLIP’s contrastive\nlearning objective may inadvertently exacerbate this gap, im-\nplying mutual information alone is insufficient for aligning\ntext and image representation distributions.\nSeveral strategies have been proposed to address the modal-\nity gap, such as projection modules with cosine similar-\nity (Zhou et al., 2023; Gao et al., 2024; Huang et al., 2024)\nand geodesic multimodal mixup (Oh et al., 2024). UnCLIP-\nbased models like DALL-E 2 (Ramesh et al., 2022) and\nKandinsky (Razzhigaev et al., 2023) employ text-to-image\nprior modules (e.g., diffusion models) to map text embed-\ndings to image feature space. A more recent alternative\nEclipse (Patel et al., 2024) uses ℓ2 loss to train a prior\nadapter.\nThese works aim to transform representations\nacross modalities for alignment. However, they remain\nexploring alignment sample-wisely and heavily rely on pair-\nwise data. Although sample-wise alignment effectively cap-\n1\narXiv:2502.17028v1  [cs.LG]  24 Feb 2025\n\nDistributional Vision Language Alignment by Cauchy-Schwarz Divergence\ntures semantic information, it falls short in aligning entire\ndata distributions. Similar to the InfoNCE in CLIP, the\nmethods struggle to match the representation spaces across\nmodalities, ultimately limiting the overall alignment. More-\nover, the reliance on carefully curated text-image pairs limits\nscalability and applicability to real-world scenarios with un-\npaired and noisy datasets (Lin et al., 2014; Li et al., 2023).\nTo address these challenges, we propose CS-Aligner, a novel\ndistributional approach that incoporates Cauchy-Schwarz\n(CS) divergence (Principe et al., 2000b) for modality align-\nment. As a symmetric measure, CS divergence robustly and\nefficiently estimates the distance between any representation\ndistributions without parametric distributional assumptions,\nmaking it highly suitable for multimodal distribution align-\nment. Furthermore, we also analyze the complementary\nroles of CS divergence and mutual information in multi-\nmodal alignment and propose integrating these two metrics\nwithin CS-Aligner. This enables CS-Aligner to align vi-\nsion and language representations in both distributional and\nsample-wise levels, considering both the global modality\nand local semantic information, leading to more comprehen-\nsive and tighter alignment as shown in Fig. 1b.\nMoreover, based on the distributional approach, CS-Aligner\nenables alignment with additional unpaired data, such as\n(a) single images with multiple captions or (b) entirely un-\npaired vision-language data, introducing more distributional\ninformation from richer, unstructured multimodal data for\nalignment robustness and flexibility in real-world scenarios.\nBeyond the unpaired alignment, we also introduce a novel\ntoken-level alignment scheme for vision-language models,\nwhich integrates more detailed information in diverse tokens\nto enhance multimodal alignment. Extensive experiments\non downstream tasks, including T2I generation and image-\ntext retrieval, demonstrate the effectiveness of our approach.\n2. Related work\nVision-language alignment and applications. CLIP (Rad-\nford et al., 2021) serves as a foundational model for vision-\nlanguage alignment in multimodal tasks. Several works\nhave enhanced CLIP through techniques such as momentum\ndistillation (Li et al., 2021) and noisy text supervision (Jia\net al., 2021). Despite its success, CLIP suffers from a per-\nsistent modality gap between text and image representa-\ntions. Prior studies (Zhou et al., 2023; Liang et al., 2022;\nShi et al., 2023) attribute this gap to factors such as cone\neffects (Liang et al., 2022) and suboptimal latent space struc-\ntures (Shi et al., 2023). To address this, various strategies\nhave been proposed, including projection adapters (Zhou\net al., 2023; Gao et al., 2024; Huang et al., 2024), geodesic\nmultimodal mixup (Oh et al., 2024), and parameter-efficient\nfine-tuning (Zanella & Ben Ayed, 2024). Recent works\nalso improve CLIP by large language models (LLMs) (Jang\net al., 2024; Koukounas et al., 2024; Huang et al., 2024) for\ndownstream tasks such as image-text retrieval.\nIn addition to image-text retrieval, text-to-image (T2I)\ngeneration is another application that reflects the vision-\nlanguage alignment capability. T2I has advanced signif-\nicantly over the past decades, driven by both diffusion-\nbased (Ramesh et al., 2021; Rombach et al., 2022; Saharia\net al., 2022; Nichol et al., 2021) and GAN-based mod-\nels (Zhang et al., 2017; Tao et al., 2023). Among diffusion-\nbased methods, the unCLIP framework (Ramesh et al., 2021;\n2022) employs a two-stage architecture with a CLIP-guided\ndiffusion prior and a decoder (e.g., DALL-E-2 (Ramesh\net al., 2022) or Karlo (Donghoon et al., 2022)). Its prior\nmodule gϕ maps text representations y to image ones x by\na diffusion model. Recently, Eclipse (Patel et al., 2024)\nemploys an ℓ2 loss to simplify the prior loss by eliminating\ndiffusion time and intruding a noise ϵ term:\nLprior = Eϵ∼N(0,I)\nh\n∥x −gϕ(ϵ, y)∥2\n2\ni\n.\n(1)\nHowever, these methods still rely on pairwise loss (e.g., ℓ2).\nIn contrast, our approach introduces distributional alignment\nfor a more holistic modality alignment.\nCauchy-Schwarz divergence. The Cauchy-Schwarz (CS)\ndivergence (Principe et al., 2000a;b) is derived from the\nCauchy-Schwarz inequality for square-integrable functions.\nIt serves as a symmetric distribution distance metric with\nnotable properties, such as the ability to measure condi-\ntional distributions (Yu et al., 2023) and the closed-form\nexpression for mixtures of Gaussians (Kampa et al., 2011).\nCS divergence has been successfully applied across various\ndomains, including deep clustering (Trosten et al., 2021),\ndisentangled representation learning (Tran et al., 2022), and\ndeep regression (Yu et al., 2024). Moreover, due to its advan-\ntage of estimating discrepancy between conditional distribu-\ntions, it has demonstrated success in the domain adaption\narea (Yin et al., 2024) and time series clustering (Yu et al.,\n2023). However, the utility of CS divergence in foundation\nmodels remains unclear and unexplored.\n3. Methodology\nIn this section, we first review the mutual information used\nin previous multimodal methods and analyze its limitations\nfor alignment. Then we introduce the novel CS-Aligner\nframework for distributional multimodal alignment and de-\ntail the estimations of its terms. After that, we extend the\nmethod by incorporating additional distribution information.\nFinally, we provide parameter-efficient implementations.\n3.1. Mutual Information is insufficient for alignment\nPrevious multimodal methods like CLIP (Radford et al.,\n2021) learn text and image representations in a shared space\n2\n\nDistributional Vision Language Alignment by Cauchy-Schwarz Divergence\n(a) High MI & Low Divergence.\n(b) High MI & High Divergence.\n(c) Low MI & Low Divergence.\nFigure 2: Toy examples: mutual information and distribution divergence between two data distributions. Distributions\nwith the same high mutual information value can exhibit either small (a) or large (b) distributional distances, demonstrating\nthat mutual information alone is insufficient for multimodal alignment. In addition, distribution divergence measures the\ncloseness between distributions but does not guarantee that the underlying random variables are statistically correlated (c).\nby maximizing lower bounds (e.g., InfoNCE (Oord et al.,\n2018)) of mutual information between modalities:\nI(x; y) =\nZ Z\np(x, y) log p(x, y)\np(x)p(y) dx dy,\n(2)\nwhere p(x) and p(y) are the distributions of image and text\nfeatures. p(x, y) denotes the joint probability. This objec-\ntive is optimized via the InfoNCE bound (Oord et al., 2018),\nwhich approximates I(x; y) using paired data samples\n{(xi, yi)}N\ni=1. We denote image-text pairs as {(xi, yi)}N\ni=1.\nThe multimodal InfoNCE loss combines symmetric image-\ntext and text-image alignment terms:\nLInfoNCE = 1\n2 (LI2T + LT2I) ,\nLI2T = −1\nN\nN\nX\ni=1\nlog\nexp (sim(xi, yi)/τ)\nPN\nj=1 exp (sim(xi, yj)/τ)\n,\nLT2I = −1\nN\nN\nX\ni=1\nlog\nexp (sim(yi, xi)/τ)\nPN\nj=1 exp (sim(yi, xj)/τ)\n,\n(3)\nwhere sim(·, ·) denotes cosine similarity. τ is temperature.\nCritically, this formulation requires paired data {(xi, yi)}.\nAlthough widely adopted, mutual information alone is insuf-\nficient for effective modality alignment (Liang et al., 2022).\nThe reason is that mutual information quantifies the sta-\ntistical dependence between two random variables (Cover,\n1999), ensuring that the distribution p(x) is related to p(y).\nHowever, it does not guarantee that the distributions p(x)\nand p(y) are statistically similar or close to each other. In\nother words, two distributions can differ significantly or be\nfar apart, yet exhibit strong dependence. We illustrate this\nissue with the following toy example.\nExample 3.1. Consider two Gaussian distributions, p(x) ∼\nN(µx, σ2\nx) and p(y) ∼N(µy, σ2\ny), with a joint distribu-\ntion p(x, y) ∼N\n\u0012\u0012µx\nµy\n\u0013\n,\n\u0012\nσ2\nx\nρσxσy\nρσxσy\nσ2\ny\n\u0013\u0013\n. Here,\nµx and µy are the means of x and y, σ2\nx and σ2\ny are their\nvariances, and ρ is the correlation coefficient and controls\ntheir linear dependency. When ρ = 0.99, the two modal-\nities are highly dependent, with high mutual information\n(I = 1.959; see Fig. 2a and 2b). When ρ = 0, the modali-\nties are independent, resulting in zero mutual information\n(Fig. 2c). Interestingly, two distributions with the same\nmutual information value can either exhibit minimal statisti-\ncal distance and nearly identical shapes, including similar\nlocations, widths, and higher-order moments, as shown in\nFig. 2a, or have completely different shapes with distinct\nmeans (0 for p(x) and 2 for p(y)) and variances (100 for\np(x) and 0.01 for p(y)), as illustrated in Fig. 2b. Quantita-\ntively, the former case shows a minimal KL divergence of\n0, while the latter exhibits a KL divergence of nearly 5, 194.\nSee details in Appendix A.\nExample 3.1 shows that despite strong dependence and high\nmutual information, the representation distributions of two\nmodalities can remain misaligned and be far from each other.\nThis issue is also observed in the CLIP model pretrained\nwith InfoNCE, where the vision and language representa-\ntions exhibit a noticeable distributional gap, as shown in\nFig. 1a. This gap results in inconsistently aligned multi-\nmodal features, hindering the clear representation of shared\nsemantics and disrupting effective mapping between modal-\nities. Ultimately, this misalignment degrades performance\nin downstream tasks, including cross-modality generation.\nNotably, although directly minimizing the divergence be-\ntween distributions may reduce the distributional gap, it\nrisks creating independent multimodal distributions with-\nout common semantic information (Fig. 2c). Therefore,\nmaximizing mutual information and minimizing divergence\ncomplement each other to achieve effective multimodal rep-\nresentation alignment.\n3.2. Distributional multimodal alignment\nTo overcome the limitations of mutual information term\nalone, we propose a distributional alignment framework.\n3\n\nDistributional Vision Language Alignment by Cauchy-Schwarz Divergence\nSpecifically, we introduce a distribution divergence mini-\nmization regularization into the optimization of multimodal\nalignment, defining the overall problem as maximizing mu-\ntual information while ensuring a small divergence between\nmultimodal distributions:\nmax I(x; y), s.t. D(p(x), p(y)) ≤ϵ,\n(4)\nwhere ϵ > 0 is a small constant representing the permissi-\nble divergence threshold. To address this constrained op-\ntimization, we introduce a Lagrangian multiplier λ ≥0,\nreformulating it into an unconstrained problem:\nmin −I(x; y) + λD(p(x), p(y)).\n(5)\nThis formulation optimizes for high mutual information\nbetween paired data (x, y) while reducing the divergence\nbetween the distributions p(x) and p(y).\nUnlike parametric distributions, distributions of different\nreal-world modalities exhibit unpredictable variability and\ninconsistent overlaps, meaning that p(x) and p(y) may fol-\nlow arbitrary distributions with a small intersection.\nTherefore, it is crucial to overcome these challenges to\nmeasure and optimize multimodal distribution divergence\nrobustly. Below, we outline several key properties that an\neffective metric should satisfy for multimodal alignment.\nRemark 3.2. Key properties for distribution align metrics:\n• Symmetry: Both distributions are treated equally, en-\nsuring consistent and unbiased multimodal alignment,\nformulated by D(p(x), p(y)) = D(p(y), p(x)).\n• Differentiable and Efficient Estimation:\nEnable\ndifferentiable estimation without distribution as-\nsumptions to facilitate optimization, formulated as\n∂D(p(x; θ), p(y; ϕ)) ̸= ∅, ∀p(x), p(y). Achieve the\nestimation non-parametrically or efficiently.\n• Robustness to Small Distribution Overlap: Provide\nreliable measurements even when distributions have\nminimal overlap of supports, which may often occur in\nmultimodal scenarios. The property is formulated as\n0 ≤D(p(x), p(y)) ≤∞when 0 < µ\n\u0000supp(p(x)) ∩\nsupp(p(y))\n\u0001\n< ϵ. µ\n\u0000supp(p(x)) ∩supp(p(y))\n\u0001\nde-\nnotes the overlap of p(x) and p(y). ϵ is a small value.\nThese properties enable the divergence term in (5) to align\narbitrary distributions with small support overlap, which is\nwell-suited for large-scale multimodal applications involv-\ning deep learning.\nWhile KL divergence and Wasserstein distance are widely\nused, they fail to meet these requirements. KL divergence\nis asymmetric, inefficient for non-Gaussian data, and unre-\nliable for distributions with small overlap (Yu et al., 2024).\nSimilarly, Wasserstein distance is inefficient to estimate, re-\nquiring additional learnable module (Arjovsky et al., 2017)\nor Sinkhorn iterations (Cuturi, 2013). As a result, these met-\nrics are suboptimal for large-scale multimodal alignment\ntasks (details in Appendix B).\nTo satisfy these properties, we introduce CS divergence\n(Principe et al., 2000a;b), a symmetric and robust metric\nto quantify the distance between two probability density\nfunctions:\nDCS(p(x); p(y)) = −log\n \n(\nR\np(x)p(y)dxdy)2\nR\np(x)2dx\nR\np(y)2dy\n!\n. (6)\nThe CS divergence satisfies all the desired properties. It\nis a symmetric distance metric between any two proba-\nbility density functions p(x) and p(y), satisfying 0 ≤\nDCS < ∞, with the minimum achieved if and only if\np(x) = p(y). Furthermore, CS divergence can be esti-\nmated non-parametrically using a kernel density estimator\n(KDE) (Parzen, 1962), eliminating the need for explicit para-\nmetric assumptions about the underlying distributions. It\nalso offers an elegant closed-form expression for mixtures\nof Gaussians (MoG) (Kampa et al., 2011) and infinite MoG.\nThis provides significant flexibility in measuring distribu-\ntional distance.\nBy incorporating the CS divergence into Eq. (5), we propose\nCS-Aligner, with the objective function:\nmin −I(x; y) + λDCS(p(x), p(y)).\n(7)\nThis formulation ensures both semantic and distributional\nalignment, enabling robust and efficient multimodal learning\nacross diverse real-world tasks.\n3.3. CS-Aligner\nWe use CS-Aligner to align the pretrained multimodal mod-\nels, as illustrated in Fig. 3. To enable the alignment, we\ndetail the estimation methods for both CS divergence and\nmutual information in Eq. (7), providing the complete ob-\njective of CS-Aligner.\nCS divergence estimation.\nWe use KDE to estimate\nDCS(p(x); p(y)) nonparametrically. Given i.i.d. samples\n{xi}M\ni=1 ∼p(x) and {yi}N\ni=1 ∼p(y), the empirical CS\ndivergence estimator is given by (Jenssen et al., 2006):\nbDCS(p(x); p(y)) = log\n\u0010 1\nM 2\nM\nX\ni,j=1\nκ(xi, xj)\n\u0011\n+\nlog\n\u0010 1\nN 2\nN\nX\ni,j=1\nκ(yi, yj)\n\u0011\n−2 log\n\u0010\n1\nMN\nM\nX\ni=1\nN\nX\nj=1\nκ(xi, yj)\n\u0011\n.\n(8)\nwhere κ is a kernel function such as Gaussian κσ(x, y) =\nexp(−∥x −y∥2\n2/2σ2). This estimator is symmetric, dif-\nferentiable, and computationally efficient, making it suit-\nable for multimodal alignment. Moreover, the third term\n4\n\nDistributional Vision Language Alignment by Cauchy-Schwarz Divergence\nText \nEncoder\n(Bert/LLMs)\nTexts\nAdapter\nImage \nEncoder\n(ViT)\nImages\nAdapter\nText feature\nImage feature\nCross-modality tasks\nCS-Aligner\nText space\nImage space\nCS Divergence\nInfoNCE\nVision-language retrieval\nA medium outdoor view of the\nengine of a train\nA high-angle view of a black and\nbrown Yorkshire\n......\nA small Mercedes coupe is seen in\na parking garage\nText-to-image generation\nA train traveling\non a snow\ncovered track\naligned text\nfeature\nImage\ngeneration\nmodel\n（unCLIP）\n. . .\nFigure 3: Illustration of CS-Aligner. We achieve vision-language alignment by freezing the pretrained text and image\nencoders and applying parameter-efficient fine-tuning methods (e.g., adapter) with our CS-Aligner. CS-Aligner optimizes\nthe adapters using the aggregated CS divergence and InfoNCE, as formulated in Eq. (5). Once aligned, the adapters are\nutilized for various cross-modality tasks: the aligned text adapter facilitates text-to-image generation without additional\nmodifications, while the aligned multimodal adapters are used for vision-language retrieval.\nin Eq. (8) ensures that bDCS(p(x); p(y)) →∞only when\nκ(x, y) →0 (i.e., when the distributions do not overlap).\nHowever, as long as there is nonzero overlap between the\ndistributions, the estimator remains well-defined and valid.\nRemark 3.3. Connection to the prior loss (ℓ2 loss) (Pa-\ntel et al., 2024).\nConsider the third term in Eq.\n(8),\nwhich involves κ(xi, yj) defined by the Gaussian kernel\nκσ(x, y) = exp\n\u0000−∥x −y∥2\n2/2σ2\u0001\n. A second-order Taylor\nexpansion yields\nκ(xi, yj) = exp\n\u0012\n−(xi −yj)2\n2σ2\n\u0013\n≈1−(xi −yj)2\n2σ2\n. (9)\nWhen i = j (i.e., diagonal of κ(x, y)), this approximation\nreduces to a weighted ℓ2 loss by 1/2σ2, analogous to the\nEq. 1. Consequently, the ℓ2 loss emerges as a special case of\nour divergence, focusing solely on paired sample reconstruc-\ntion and omitting broader distribution alignment, including\noff-diagonal (cross-sample) contributions.\nMutual information estimation. The mutual information\nand the CS divergence serve complementary roles for align-\nment: mutual information captures semantic relationships\nby focusing on pairwise samples, while CS divergence\naligns modalities at the distributional level (overall data\ndistribution), leveraging global information.\nInspired by CLIP-based methods (Radford et al., 2021), we\nestimate the mutual information term I(x, y) in Eq. (5)using\nits lower bound, optimized via InfoNCE (Eq. (3)). We\nspecifically choose InfoNCE not only for the optimization\nefficiency but also for its compatibility with CS divergence\nin the cosine similarity space.\nRemark 3.4. The connection between CS divergence and\nInfoNCE becomes evident when analyzing both terms from\na cosine similarity perspective. For a characteristic ker-\nnel κ(x, y) = ⟨ϕ(x), ϕ(y)⟩H, where ϕ maps samples\nto a Reproducing Kernel Hilbert Space (RKHS) H, the\nmean embeddings are: µx = 1\nm\nPm\ni=1 ϕ(xi)\nand\nµy =\n1\nn\nPn\ni=1 ϕ(yi), The CS divergence can then be expressed\nas:\nˆDCS(p(x); p(y)) = −2 log\n\u0012\n⟨µx, µy⟩H\n∥µx∥H∥µy∥H\n\u0013\n= −2 log sim(µx, µy),\n(10)\nwhich evaluates the cosine similarity between distributions.\nSimilarly, InfoNCE evaluates cosine similarity between\npaired samples (Eq. 3). This dual-level similarity assess-\nment underscores the synergy between CS divergence and\nmutual information, offering a unified and robust framework\nfor multimodal alignment.\nFinal objective function. With the exact estimation of the\nCS divergence in Eq. (8) and InfoNCE in Eq. (3), the final\nobjective function of our method is formulated as:\nLCS-Aligner = bDCS(p(x); p(y)) + λLInfoNCE.\n(11)\n3.4. Extended alignment with unpaired data\nBenefits from the distributional alignment, we further pro-\npose some novel extensions of CS-Aligner, which leverage\nadditional information in unpaired data. While mutual infor-\nmation estimation requires pairwise data, the CS divergence\nestimator (Eq. (8)) can operate seamlessly on unpaired data\nwithout introducing additional computation. This unique\ncapability enables CS-Aligner to extend beyond traditional\npairwise multimodal alignment by incorporating additional\ndistributional information from unpaired data or tokens. We\n5\n\nDistributional Vision Language Alignment by Cauchy-Schwarz Divergence\nintroduce two novel directions for this extended alignment:\nunpaired data alignment and token alignment.\nUnpaired vision-language alignment. Our method lever-\nages two forms of unpaired alignments: (1) images with\nmultiple captions, and (2) independently sampled unpaired\nimages and texts. The unpaired alignments are achieved\nusing Eq. (8), where {xi}M\ni=1 and {yj}N\nj=1 can be indepen-\ndent with M ̸= N. In both scenarios, our method leverages\nmore uncurated unpaired data for distributional multimodal\nalignment, providing greater flexibility and robustness.\nVision-language token alignment. We also propose a novel\nintra-sample distribution alignment approach between vi-\nsion and language tokens. Unlike CLIP-based models (Rad-\nford et al., 2021), which align only the “CLS” tokens of\nvision and text representations, the method considers all\nvision and text tokens for a more fine-grained alignment.\nSpecifically, each vision feature xi ∈RV ×D is modeled\nas a token distribution p(xi) containing V vision tokens,\nwhile each text feature yi ∈RL×D is represented as a token\ndistribution p(yi) consisting of L text tokens. D denotes\nthe feature dimension. We compute the CS divergence be-\ntween the vision and text token distributions. The internal\ntoken-wise alignment loss Ltoken is formulated as:\nLtoken = 1\nB\nB\nX\ni=1\nbDCS(p(xi); p(yi)),\n(12)\nwhere B is the batch size. In general, V ̸= L, and vision\nand language tokens do not have a direct pairing, making\nInfoNCE inapplicable for estimation. Through our distribu-\ntional alignment, Eq. (12) enables comprehensive alignment\nacross all tokens, capturing more details and potentially\nenhancing fine-grained alignment.\n3.5. Parameter-efficient multimodal alignment\nWe demonstrate the effectiveness of our CS-Aligner by per-\nforming vision-language alignment in a parameter-efficient\nmanner using pretrained vision and language models, such\nas CLIP and large language models (LLMs) (Dubey et al.,\n2024). To adapt these pretrained models, we employ two\nwidely used frameworks: adapter (Gao et al., 2024) and\nLoRA (Low-Rank Adaptation) (Hu et al., 2021).\nAdapter alignment. We add a lightweight transformer\n(Vaswani, 2017) on top of the pretrained model as an adapter.\nThe adapter projects text embeddings or image embeddings\ninto a shared representation space and distribution.\nLoRA alignment. We also explore LoRA to insert trainable\nlow-rank matrices into the pretrained weights of the text\nencoder. It enables fine-grained adjustments to the represen-\ntations, aligning them with the other modality distribution.\nThe adapter and LoRA enable efficient alignment of the\nmultimodal large-scale pretrained models, without requiring\nextensive computational resources.\n4. Experiments\nWe evaluate our method on two tasks to illustrate its vision-\nlanguage alignment ability: text-to-image (T2I) generation\nin Section 4.1 and image-text retrieval in Section 4.2.\n4.1. Text to image generation\nDatasets. Following a previous T2I approach (Patel et al.,\n2024), we train our method on four datasets: MSCOCO\n(Lin et al., 2014), CC3M (Sharma et al., 2018), CC12M\n(Changpinyo et al., 2021), and LAION-HighResolution-\n5M (Schuhmann et al., 2022). MSCOCO contains 80K\nimages paired with multiple captions. CC3M and CC12M\ninclude about 2.5M and 10M image-text pairs, respectively.\nLAION-HighResolution comprises 175M high-resolution\npairs, from which we select 5M for training. We evaluate\nthe aligned model on the MSCOCO 30K validation set.\nExperimental setup.\nWe build our method based on\nunCLIP-style approaches (e.g., DALL-E-2 (Ramesh et al.,\n2022), Karlo (Donghoon et al., 2022), Kandinsky (Razzhi-\ngaev et al., 2023)). These methods typically train a diffusion\nprior module on large-scale datasets (more than hundreds\nof millions data) to map text representations into the image\nrepresentation space. The diffused text representations are\nfed into a separate decoder for image generation.\nDifferently, CS-Aligner trains an adapter to align text rep-\nresentations to image feature space on small-scale datasets,\ne.g., MSCOCO (0.08M), CC3M (3M), and CC12M (12M),\nand LAION-HighRes subset (5M). After alignment, we di-\nrectly process the aligned text features using the pretrained\ndecoder of the large-scale methods (e.g., Karlo and Kandin-\nsky) to generate images, without additional prior modules\nor multiple diffusion steps. We evaluate generation quality\nwith the FID score (Heusel et al., 2017), which measures\nhow closely generated images match the real image distri-\nbution. This metric is particularly well-suited for evaluating\nmodality alignment, as it directly reflects the distribution\ndistance. Additional details can be found in Appendix C.\nBaselines. Our baselines consists of both large-scale meth-\nods Karlo (Donghoon et al., 2022), Kandinsky (Razzhi-\ngaev et al., 2023) and recent small-scale alignment method\nEclipse (Patel et al., 2024). Eclipse streamlines the prior\nmodule in Karlo and Kandinsky by employing an L2 loss\nfor T2I. For a fair comparison, we adopt the same Trans-\nformer adapter as Eclipse and only align the “CLS” tokens,\nhighlighting the advantages of our distributional alignment.\nComparisons. We compare our method with both the large-\nscale diffusion-based methods and the small-scale alignment\n6\n\nDistributional Vision Language Alignment by Cauchy-Schwarz Divergence\nTable 1: Comparisons with T2I methods. Our method out-\nperforms both large-scale diffusion-based methods (Karlo,\nKandinsky) and the recent small-scale method Eclipse.\nMethods\nDatasize (M)\nFID\nLarge-scale methods\nDALL-E2\n250\n10.65\nKandinsky\n177\n20.48\nKarlo\n115\n20.64\nSmall-scale alignment\nEclipse + Kandinsky decoder\n0.08(COCO)\n16.53\nOurs + Kandinsky decoder\n0.08(COCO)\n12.62\nEclipse + Karlo decoder\n0.08(COCO)\n23.67\nOurs + Karlo decoder\n0.08(COCO)\n11.27\nTable 2: Comparisons with Eclipse on various training\ndata. Our method consistently performs better.\nMethod\nCC3M\nCC12M\nLAION-HighRes 5M\nEclipse\n26.73\n26.98\n19.16\nOurs\n22.88\n22.72\n14.79\nmethods. The results are provided in Table 1. By aligning\ntext representations to image representations on the small\nMSCOCO data, our method achieves superior T2I gener-\nation than the large-scale methods Karlo and Kandinsky,\nwithout any diffusion steps. CS-Aligner also outperforms\nEclipse by an obvious margin using either Karlo or Kandin-\nsky decoders. The results demonstrate the effective vision-\nlanguage alignment capability of our method. Moreover, we\ncompare CS-Aligner with Eclipse across different training\ndatasets. As shown in Table 2, our method performs better\nacross diverse training data (CC3M, CC12M, and LAION-\nHighRes-5M), underscoring the importance of the modality\ndistribution information for robust alignment.\nQualitative Visualization. To further evaluate our method,\nwe present qualitative visualizations of generated images\nusing the Karlo decoder. As shown in Fig. 4, our aligned text\nrepresentations result in more realistic images with stronger\nsemantic consistency with the input sentence, highlighting\nthe effectiveness of CS-Aligner in enhancing alignment.\nCS-Aligner with different adaptation approaches. To\ndemonstrate the robustness of our method across different\nmodels, we perform alignments for T2I using both adapter\nand LoRA. Specifically, we apply LoRA with a low-rank\ndimension of 8 to every transformer layer in the CLIP text\nencoder. As shown in Table 3, based on either Karlo or\nKandinsky, CS-Aligner with LoRA introduces fewer param-\neters, while still achieving comparable results compared\nwith the adapter-based one, demonstrating the effectiveness\nand adaptability of CS-Aligner across different models.\nCS-Aligner with multiple captions. It is common in real-\nCS-Aligner\nEclipse\n“A cat sitting \nbeside a \nlaptop on a \ndesk”\nNo Alignment\n“A couple of \nmen standing \non a boat next \nto a small \ndog”\nFigure 4: Qualitative Visualization. We present visual-\nizations for no alignment (left), Eclipse (middle), and CS-\nAligner (right). CS-Aligner achieves more realistic genera-\ntions with stronger semantic consistency.\nTable 3:\nCS-Aligner with different adaptation ap-\nproaches. Our method achieves good alignment using both\nadapter and LoRA.\nBase Model\nAdaptation\n#Parameters\nFID\nKandinsky\nAdapter\n34M\n12.62\nLoRA\n6M\n13.52\nKarlo\nAdapter\n33M\n11.27\nLoRA\n1.3M\n15.63\nworld datasets for a single image to correspond to multiple\ncaptions (e.g., 5 captions per image in MSCOCO). Due to\ntheir pairwise alignment nature, previous methods such as\nInfoNCE and ℓ2-based approaches (Radford et al., 2021; Pa-\ntel et al., 2024) struggle to simultaneously leverage multiple\ncaptions. In contrast, by incorporating CS divergence, our\nCS-Aligner enables training for alignment with single image\nand multiple captions. To demonstrate the benefits of multi-\nple captions for CS-Aligner, we conducted experiments on\nthe MSCOCO dataset by estimating the CS divergence term\nbDCS in Eq. (11) using both single and multiple captions. As\nshown in Fig. 5a, CS-Aligner effectively leverages the infor-\nmation provided by multiple captions, leading to improved\nvision-language alignment.\nCS-Aligner with additional unpaired data. Collecting\nand accurately annotating paired vision-language data is\nboth challenging and costly. Enhancing alignment with ad-\nditional unpaired data offers a more flexible and scalable\nsolution for real-world applications. However, similar to the\ncase of multiple captions, previous methods (Radford et al.,\n2021; Patel et al., 2024) struggle to fully utilize unpaired\ndata due to their reliance on pairwise alignment, whereas\nCS-Aligner naturally incorporates the unpaired data infor-\nmation by CS divergence. To demonstrate this capability,\nwe conduct experiments on the MSCOCO dataset using the\nKandinsky decoder with (1) 80K paired training samples,\n(2) 40K paired training samples, and (3) 40K paired training\nsamples supplemented with 80K unpaired samples, where\n7\n\nDistributional Vision Language Alignment by Cauchy-Schwarz Divergence\n(a) Align with multiple captions. (b) Align with unpaired data.\nFigure 5: CS-Aligner with additional information. Our\nmethod benefits from the additional information from the\nmultiple captions (a) and unpaired data (b).\na black and white\ncat looks at the \ncamera and there \nis a TV in the \nbackground\nw/ token alignment\nOverall FID: 12.14\nw/o token alignment\nOverall FID: 12.62\nan old photo of a \nlittle girl sitting on \nher dad’s lap\nFigure 6: CS-Aligner with token alignment. Token align-\nment enhance more fine-grained vision-language alignment.\nthe unpaired samples are used to estimate the CS divergence.\nAs shown in Fig. 5b, the result with 40K paired training\ndata is lower than 80K. However, introducing additional\nunpaired data obviously improves the performance, even\nsurpassing the model trained with 80K paired samples. This\ndemonstrates CS-Aligner’s ability to effectively leverage\nthe distributional information of modalities for alignment.\nCS-Aligner with token alignment. Beyond the unpaired\ndata, CS-Aligner also enables token-level alignment by treat-\ning the tokens of each sample as a distribution. We evaluated\nthe token-level extension of CS-Aligner using the Kandinsky\ndecoder on the MSCOCO dataset. As shown in Fig. 6, in-\ncorporating token alignment further improves performance.\nMoreover, qualitative results indicate that token alignment\nenhances fine-grained details in generated images, suggest-\ning an improved ability to capture fine-grained relationships\nbetween modalities. Additional visualizations are provided\nin Fig. 8 in Appendix D.2.\n4.2. Image-Text Retrieval\nExperimental Setup. Effective multimodal alignment also\nbenefits cross-model retrieval. To demonstrate the align-\nTable 4: Comparisons of image-to-text (I2T) and text-\nto-image (T2I) retrieval. Our method outperforms the\nbaselines on diverse datasets.\nFlickr30k\nUrban-1k\nDOCCI\nAverage\nMethods\nI2T\nT2I\nI2T\nT2I\nI2T\nT2I\nI2T\nT2I\nLong-CLIP\n90.0\n76.2\n82.5\n86.1\n66.5\n78.6\n79.7\n80.3\nCLIP\n85.2\n65.0\n68.3\n55.6\n63.1\n65.8\n72.2\n62.1\nLLM2CLIP-3M\n89.6\n77.3\n87.1\n91.1\n84.9\n87.8\n87.2\n85.4\nOurs-3M\n91.8\n81.0\n87.6\n92.2\n86.6\n89.1\n88.7\n87.4\nment ability of our method on retrieval tasks, we conduct\nexperiments aligning LLMs (Dubey et al., 2024) text repre-\nsentations with CLIP vision representations on both image-\nto-text and text-to-image retrieval. We use the Flickr 1K\ntest set (Young et al., 2014) for short-text retrieval, while\nUrban1K (Zhang et al., 2025) and DOCCI (Onoe et al.,\n2025) are employed for long-text retrieval. We compare\nCS-Aligner against pure InfoNCE-based methods, such as\nLong-CLIP (Zhang et al., 2025) and LLM2CLIP (Huang\net al., 2024), as the baselines. To ensure a fair compari-\nson, we adopt the setup from LLM2CLIP, aligning CLIP\nViT-L/14 image representations with Llama 3 (8B) text rep-\nresentations. Both the vision and text representations are\naligned by adapters trained on the CC3M dataset.\nComparisons. The results in Table 4 show that our method\nconsistently and significantly outperforms the baselines\nacross various datasets for both image-to-text (I2T) and\ntext-to-image (T2I) retrieval. This demonstrates the effec-\ntiveness of our method for aligning the two modalities into\na shared space. Moreover, the ability to align a different text\nencoder (LLM) with the successful alignment of an LLM-\nbased text encoder with the CLIP image encoder highlights\nthe flexibility and generalizability of our approach.\n5. Conclusion\nIn this paper, we propose CS-Aligner, a novel distributional\nalignment framework that integrates Cauchy–Schwarz (CS)\ndivergence with mutual information for multimodal align-\nment. By combining global distributional alignment with\nInfoNCE, CS-Aligner achieves tighter and more comprehen-\nsive alignment. By considering the modality distributional\ninformation, our method enables to leverage additional and\ndetailed information from unpaired samples and tokens,\nleading to more flexible and fine-grained information for\nalignment. We demonstrate the effectiveness of our align-\nment on text-to-image generation and cross-modal retrieval.\nLimitation & future work. Although our method has\nnumerous applications, it is currently evaluated only on\nunCLIP-type models for generation. In the future, we will\nexplore its integration with stable-diffusion-based models.\nBroader modalities (e.g., audio and video) and diverse tasks\n(e.g., image-to-text generation) could further strengthen\nalignment and broaden the applicability of CS-Aligner.\n8\n\nDistributional Vision Language Alignment by Cauchy-Schwarz Divergence\nImpact Statement\nThis paper contributes to the advancement of Machine\nLearning. While our work may have various societal impli-\ncations, none require specific emphasis at this stage.\nReferences\nArjovsky, M., Chintala, S., and Bottou, L. Wasserstein gen-\nerative adversarial networks. In International conference\non machine learning, pp. 214–223. PMLR, 2017.\nChangpinyo, S., Sharma, P., Ding, N., and Soricut, R. Con-\nceptual 12m: Pushing web-scale image-text pre-training\nto recognize long-tail visual concepts. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pp. 3558–3568, 2021.\nCover, T. M. Elements of information theory. John Wiley &\nSons, 1999.\nCuturi, M. Sinkhorn distances: Lightspeed computation\nof optimal transport. Advances in neural information\nprocessing systems, 26, 2013.\nDonghoon, L., Jiseob, K., Jisu, C., Jongmin, K., Minwoo,\nB., Woonhyuk, B., and Saehoon, K. Karlo-v1.0.alpha\non coyo-100m and cc15m. https://github.com/\nkakaobrain/karlo, 2022.\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle,\nA., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan,\nA., et al. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783, 2024.\nGao, P., Geng, S., Zhang, R., Ma, T., Fang, R., Zhang, Y.,\nLi, H., and Qiao, Y. Clip-adapter: Better vision-language\nmodels with feature adapters. International Journal of\nComputer Vision, 132(2):581–595, 2024.\nHeusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and\nHochreiter, S. Gans trained by a two time-scale update\nrule converge to a local nash equilibrium. Advances in\nneural information processing systems, 30, 2017.\nHo, J. and Salimans, T. Classifier-free diffusion guidance.\narXiv preprint arXiv:2207.12598, 2022.\nHo, J., Jain, A., and Abbeel, P. Denoising diffusion proba-\nbilistic models. Advances in neural information process-\ning systems, 33:6840–6851, 2020.\nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,\nS., Wang, L., and Chen, W. Lora: Low-rank adaptation of\nlarge language models. arXiv preprint arXiv:2106.09685,\n2021.\nHuang, W., Wu, A., Yang, Y., Luo, X., Yang, Y., Hu, L., Dai,\nQ., Dai, X., Chen, D., Luo, C., et al. Llm2clip: Powerful\nlanguage model unlock richer visual representation. arXiv\npreprint arXiv:2411.04997, 2024.\nJang, Y. K., Kang, J., Lee, Y. J., and Kim, D. Mate: Meet at\nthe embedding–connecting images with long texts. arXiv\npreprint arXiv:2407.09541, 2024.\nJenssen, R., Principe, J. C., Erdogmus, D., and Eltoft, T.\nThe cauchy–schwarz divergence and parzen windowing:\nConnections to graph theory and mercer kernels. Journal\nof the Franklin Institute, 343(6):614–629, 2006.\nJia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham, H.,\nLe, Q., Sung, Y.-H., Li, Z., and Duerig, T. Scaling up\nvisual and vision-language representation learning with\nnoisy text supervision. In International conference on\nmachine learning, pp. 4904–4916. PMLR, 2021.\nKampa, K., Hasanbelliu, E., and Principe, J. C. Closed-form\ncauchy-schwarz pdf divergence for mixture of gaussians.\nIn The 2011 International Joint Conference on Neural\nNetworks, pp. 2578–2585. IEEE, 2011.\nKorotin, A., Selikhanovych, D., and Burnaev, E. Neural op-\ntimal transport. arXiv preprint arXiv:2201.12220, 2022.\nKoukounas, A., Mastrapas, G., G¨unther, M., Wang, B.,\nMartens, S., Mohr, I., Sturua, S., Akram, M. K., Mart´ınez,\nJ. F., Ognawala, S., et al. Jina clip: Your clip model is\nalso your text retriever. arXiv preprint arXiv:2405.20204,\n2024.\nLi, J., Selvaraju, R., Gotmare, A., Joty, S., Xiong, C., and\nHoi, S. C. H. Align before fuse: Vision and language\nrepresentation learning with momentum distillation. Ad-\nvances in neural information processing systems, 34:\n9694–9705, 2021.\nLi, T., Bhardwaj, S., Tian, Y., Zhang, H., Barber, J., Katabi,\nD., Lajoie, G., Chang, H., and Krishnan, D. Leverag-\ning unpaired data for vision-language generative models\nvia cycle consistency. arXiv preprint arXiv:2310.03734,\n2023.\nLiang, V. W., Zhang, Y., Kwon, Y., Yeung, S., and Zou,\nJ. Y. Mind the gap: Understanding the modality gap\nin multi-modal contrastive representation learning. Ad-\nvances in Neural Information Processing Systems, 35:\n17612–17625, 2022.\nLin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ra-\nmanan, D., Doll´ar, P., and Zitnick, C. L. Microsoft coco:\nCommon objects in context. In Computer Vision–ECCV\n2014: 13th European Conference, Zurich, Switzerland,\nSeptember 6-12, 2014, Proceedings, Part V 13, pp. 740–\n755. Springer, 2014.\n9\n\nDistributional Vision Language Alignment by Cauchy-Schwarz Divergence\nNichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin,\nP., McGrew, B., Sutskever, I., and Chen, M.\nGlide:\nTowards photorealistic image generation and editing\nwith text-guided diffusion models.\narXiv preprint\narXiv:2112.10741, 2021.\nOh, C., So, J., Byun, H., Lim, Y., Shin, M., Jeon, J.-J.,\nand Song, K. Geodesic multi-modal mixup for robust\nfine-tuning. Advances in Neural Information Processing\nSystems, 36, 2024.\nOnoe, Y., Rane, S., Berger, Z., Bitton, Y., Cho, J., Garg,\nR., Ku, A., Parekh, Z., Pont-Tuset, J., Tanzer, G., et al.\nDocci: Descriptions of connected and contrasting images.\nIn European Conference on Computer Vision, pp. 291–\n309. Springer, 2025.\nOord, A. v. d., Li, Y., and Vinyals, O. Representation learn-\ning with contrastive predictive coding. arXiv preprint\narXiv:1807.03748, 2018.\nParzen, E. On estimation of a probability density function\nand mode. The annals of mathematical statistics, 33(3):\n1065–1076, 1962.\nPatel, M., Kim, C., Cheng, S., Baral, C., and Yang, Y.\nEclipse: A resource-efficient text-to-image prior for im-\nage generations. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pp.\n9069–9078, 2024.\nPrincipe, J. C., Xu, D., Fisher, J., and Haykin, S. Informa-\ntion theoretic learning. Unsupervised adaptive filtering,\n1:265–319, 2000a.\nPrincipe, J. C., Xu, D., Zhao, Q., and Fisher, J. W. Learning\nfrom examples with information theoretic criteria. Jour-\nnal of VLSI signal processing systems for signal, image\nand video technology, 26:61–77, 2000b.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,\net al. Learning transferable visual models from natural\nlanguage supervision. In International conference on\nmachine learning, pp. 8748–8763. PMLR, 2021.\nRamesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Rad-\nford, A., Chen, M., and Sutskever, I. Zero-shot text-to-\nimage generation. In International conference on ma-\nchine learning, pp. 8821–8831. Pmlr, 2021.\nRamesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M.\nHierarchical text-conditional image generation with clip\nlatents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.\nRazzhigaev, A., Shakhmatov, A., Maltseva, A., Arkhip-\nkin, V., Pavlov, I., Ryabov, I., Kuts, A., Panchenko, A.,\nKuznetsov, A., and Dimitrov, D. Kandinsky: an improved\ntext-to-image synthesis with image prior and latent diffu-\nsion. arXiv preprint arXiv:2310.03502, 2023.\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and\nOmmer, B. High-resolution image synthesis with latent\ndiffusion models. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pp.\n10684–10695, 2022.\nSaharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton,\nE. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan,\nB., Salimans, T., et al. Photorealistic text-to-image dif-\nfusion models with deep language understanding. Ad-\nvances in neural information processing systems, 35:\n36479–36494, 2022.\nSchuhmann, C., Beaumont, R., Vencu, R., Gordon, C.,\nWightman, R., Cherti, M., Coombes, T., Katta, A., Mullis,\nC., Wortsman, M., et al. Laion-5b: An open large-scale\ndataset for training next generation image-text models.\nAdvances in Neural Information Processing Systems, 35:\n25278–25294, 2022.\nSharma, P., Ding, N., Goodman, S., and Soricut, R. Con-\nceptual captions: A cleaned, hypernymed, image alt-text\ndataset for automatic image captioning. In Proceedings\nof the 56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pp. 2556–\n2565, 2018.\nShi, P., Welle, M. C., Bj¨orkman, M., and Kragic, D. Towards\nunderstanding the modality gap in clip. In ICLR 2023\nWorkshop on Multimodal Representation Learning: Perks\nand Pitfalls, 2023.\nTao, M., Bao, B.-K., Tang, H., and Xu, C. Galip: Generative\nadversarial clips for text-to-image synthesis. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp. 14214–14223, 2023.\nTran, L., Pantic, M., and Deisenroth, M. P. Cauchy-schwarz\nregularized autoencoder. Journal of Machine Learning\nResearch, 23, 2022.\nTrosten, D. J., Lokse, S., Jenssen, R., and Kampffmeyer, M.\nReconsidering representation alignment for multi-view\nclustering. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pp. 1255–\n1265, 2021.\nVaswani, A. Attention is all you need. Advances in Neural\nInformation Processing Systems, 2017.\nWang, Q., Kulkarni, S. R., and Verd´u, S. Divergence es-\ntimation for multidimensional densities via k-nearest-\nneighbor distances. IEEE Transactions on Information\nTheory, 55(5):2392–2405, 2009.\n10\n\nDistributional Vision Language Alignment by Cauchy-Schwarz Divergence\nYin, W., Yu, S., Lin, Y., Liu, J., Sonke, J.-J., and Gavves, S.\nDomain adaptation with cauchy-schwarz divergence. In\nThe 40th Conference on Uncertainty in Artificial Intelli-\ngence, 2024.\nYoung, P., Lai, A., Hodosh, M., and Hockenmaier, J. From\nimage descriptions to visual denotations: New similarity\nmetrics for semantic inference over event descriptions.\nTransactions of the Association for Computational Lin-\nguistics, 2:67–78, 2014.\nYu, S., Li, H., Løkse, S., Jenssen, R., and Pr´ıncipe, J. C. The\nconditional cauchy-schwarz divergence with applications\nto time-series data and sequential decision making. arXiv\npreprint arXiv:2301.08970, 2023.\nYu, S., Yu, X., Løkse, S., Jenssen, R., and Principe, J. C.\nCauchy-schwarz divergence information bottleneck for\nregression. arXiv preprint arXiv:2404.17951, 2024.\nZanella, M. and Ben Ayed, I. Low-rank few-shot adapta-\ntion of vision-language models. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 1593–1603, 2024.\nZhang, B., Zhang, P., Dong, X., Zang, Y., and Wang, J.\nLong-clip: Unlocking the long-text capability of clip. In\nEuropean Conference on Computer Vision, pp. 310–325.\nSpringer, 2025.\nZhang, H., Xu, T., Li, H., Zhang, S., Wang, X., Huang, X.,\nand Metaxas, D. N. Stackgan: Text to photo-realistic\nimage synthesis with stacked generative adversarial net-\nworks. In Proceedings of the IEEE international confer-\nence on computer vision, pp. 5907–5915, 2017.\nZheng, C., Vuong, T.-L., Cai, J., and Phung, D. Movq: Mod-\nulating quantized vectors for high-fidelity image genera-\ntion. Advances in Neural Information Processing Systems,\n35:23412–23425, 2022.\nZhou, C., Zhong, F., and ¨Oztireli, C. Clip-pae: projection-\naugmentation embedding to extract relevant features for\na disentangled, interpretable and controllable text-guided\nface manipulation. In ACM SIGGRAPH 2023 Conference\nProceedings, pp. 1–9, 2023.\nZhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M.\nMinigpt-4: Enhancing vision-language understanding\nwith advanced large language models. arXiv preprint\narXiv:2304.10592, 2023.\n11\n\nDistributional Vision Language Alignment by Cauchy-Schwarz Divergence\nA. Details of the toy examples\nMutual information.\nFor two continuous random variables x and y, the mutual information is defined as:\nI(x; y) =\nZ Z\np(x, y) log\n\u0010\np(x,y)\np(x) p(y)\n\u0011\ndx dy.\n(13)\nFor a bivariate Gaussian distribution\np(x, y) ∼N\n\u0010\u0012µx\nµy\n\u0013\n,\n\u0012\nσ2\nx\nρσxσy\nρσxσy\nσ2\ny\n\u0013\u0011\n,\nthe mutual information admits the closed-form solution:\nI(x; y) = −1\n2 ln\n\u00001 −ρ2\u0001\n.\n(14)\nIn particular, for correlation ρ = 0.99, we have I(x, y) ≈1.959, while for ρ = 0, the variables are independent and\nI(x, y) = 0.\nDivergence.\nFor univariate Gaussian distributions p(x) = N\n\u0000µx, σ2\nx\n\u0001\nand p(y) = N\n\u0000µy, σ2\ny\n\u0001\n, the KL divergence is given\nby:\nDKL\n\u0000p(x) ∥p(y)\n\u0001\n= ln\n\u0010\nσy\nσx\n\u0011\n+ σ2\nx + (µx −µy)2\n2 σ2y\n−\n1\n2.\n(15)\nFor Fig. 2a and Fig. 2c, we set σx = σy = 1. Hence, when µx = µy = 0, DKL\n\u0000p(x) ∥p(y)\n\u0001\n= 0.\nFor Fig. 2b, we use σx = 10 and σy = 0.1. When µx = 0 and µy = 2, the DKL\n\u0000p(x) ∥p(y)\n\u0001\n= 5194.89, which is very\nlarge.\nB. Comparison bwtween CS divergence and other metrics\nIn the following, we analyze why CS divergence is a better choice than well-known KL divergence and Wasserstein distance.\nComparison with KL divergence. KL divergence, a widely used metric in deep learning, is defined as:\nDKL(p(x)∥p(y)) =\nZ\np(x) log p(x)\np(y)dx,\n(16)\nwhere p(x) and p(y) are the probability distributions being compared. However, KL divergence is a non-symmetric metric,\nmaking it less suitable for learning a shared representation space (bi-directional), as commonly needed in multi-modal\nlearning.\nMinimal distribution overlap requirement. Furthermore, KL divergence can diverge to infinity when the distributions p(x)\nand p(y) have minimal overlap, making it invalid. In contrast, CS divergence remains well-defined under such conditions\nand provides a stable estimation (Yu et al., 2024; Yin et al., 2024). This issue is particularly critical in multi-modality\nsettings, where the distributions from different modalities may have limited overlap.\nNonparametric estimation. Additionally, when the distributions are not assumed to be Gaussian, a nonparametric estimator\nis required for KL divergence. A common choice, the k-NN estimator (Wang et al., 2009), is non-differentiable, which poses\nchallenges for optimization in gradient-based learning frameworks. In contrast, the CS divergence demonstrates greater\nstability and differentiability when paired with KDE, making it a more robust choice.\nComparison with wasserstein distance. Wasserstein Distance is also widely used for distribution discrepancy (e.g.\nGAN (Arjovsky et al., 2017)). However, Wasserstein distance is be computed either by using an additional learnable module\n(e.g., a neural network for estimating a transport map (Korotin et al., 2022)) or by solving an optimization problem, often\napproximated via multiple Sinkhorn (Cuturi, 2013) iterations for computational efficiency, leading to efficiency problem in\nlarge-scale training. In contrast, CS divergence can be efficiently estimated by a nonparametric estimator.\n12\n\nDistributional Vision Language Alignment by Cauchy-Schwarz Divergence\nC. Implementation details\nImplementation details\nOur models were trained on 4 NVIDIA RTX A100 GPUs with a global batch size of 1,024 (256\nper GPU). We optimized parameters using AdamW with a cosine annealing learning rate schedule, spanning a total of 100\nGPU hours. Mixed-precision training (FP16) was employed to enhance computational efficiency while maintaining stability.\nWe use the learning rate of 5e −5. We use hyperparameter λ as 0.01 to keep the same number scale as the divergence.\nKernel density estimator.\nA proper kernel size is critical in KDE for accurate estimation of Eq. (8). In this paper, we\nfollow (Yin et al., 2024) to normalize the features from two modalities and use a kernel size 1. In general, this is sufficient to\nensure stable learning.\nC.1. T2I details\nKandinsky details.\nWe use Kandinsky v2.2, an unCLIP-type model that utilizes CLIP ViT-bigG-14-laion2B-39B-b160k\nwith 1280 projection dimensions for text and image encoders. Kandinsky v2.2 employs a latent diffusion model and\nMOVQ (Zheng et al., 2022) as the decoder to generate images of size 512 × 512 from the given image representation. When\nusing the Kandinsky decoder, we apply 50 denoising steps (Ho et al., 2020) with a classifier-free guidance scale of 7.5 (Ho\n& Salimans, 2022).\nKarlo details.\nKarlo uses CLIP-ViT-L/14 with 768 projection dimensions for image and text encoders. It employs a\ndiffusion model to decode the image representation into a low-resolution image, followed by a super-resolution diffusion\nmodule that upsamples it to 512 × 512. When using the Karlo decoder, we apply 25 denoising steps with a classifier-free\nguidance scale of 7.5, followed by an additional 7 super-resolution steps.\nAdapter details.\nTo ensure a fair comparison, our adapter module has the same architecture as Eclipse (Patel et al.,\n2024), which is based on the standard PriorTransformer model (Ramesh et al., 2022) but modified to be time-independent.\nSpecifically, it consists of 10 layers with 16 attention heads, each having a head dimension of 32. The embedding dimension\nis 768/1280, with three additional embeddings. The model does not use time embeddings and has a dropout rate of 0.0.\nLoRA\nWe configure LoRA (Low-Rank Adaptation) for CLIP with a rank of r = 8 and a scaling factor of α = 16, enabling\nefficient adaptation while maintaining a low computational footprint. The targeted modules include the self-attention\nprojections, the fully connected layers, and the text projection layer, ensuring adaptation across both vision and text\nprocessing components. A dropout rate of 0.1 is applied to enhance regularization. For the CLIP encoder in Kandinsky,\nViT-bigG-14-laion2B-39B-b160k, the number of LoRA parameters is 6 million. As for CLIP-ViT-L/14 in Karlo, the CLIP\nmodel size is smaller, resulting in 1.3 million LoRA parameters.\nLAION-HighResolution-5M selection.\nWe use a subset of 5 million image-text pairs from the LAION-HighResolution\ndataset, which contains 175 million pairs. Due to computational constraints, we download only a portion of the dataset and\nselect pairs with English captions.\nD. More experimental results\nAlignment with InfoNCE is not enough for the generation task.\nWe train the adapter solely with InfoNCE and use\nthe Kandinsky decoder to generate the corresponding images. Table 5 shows that InfoNCE alone struggles to align the\nmultimodal distributions, resulting in a high FID score.\nTable 5: Ablation study of CS-Aligner. Alignment with CS-Aligner significantly outperforms the alignment results with\nonly InfoNCE.\nLoss\nFID\nInfoNCE\n151.35\nCS-Aligner\n12.62\n13\n\nDistributional Vision Language Alignment by Cauchy-Schwarz Divergence\nD.1. More visualization\nWe illustrate more high-resolution images generated by the Kandinsky decoder with our aligned text representation in Fig. 7.\nThe adapter is trained on LAION-HighRes 5M.\nD.2. More visualization for token alignment\nWe provide more visualizations with and without the token alignment Fig. 8, demonstrating its ability to generate more\nfine-grained images with CS-Aligner.\n14\n\nDistributional Vision Language Alignment by Cauchy-Schwarz Divergence\nA  vase of flowers displayed at \nthe front windows of a store\nA bald man with a beard wearing \na polka dot bow tie\nA basket fill with different \ntypes of fruit\nA bedroom decorated with \na blue underwater theme\nA wooden cutting board filled \nwith chopped vegetables\n\"Head shot\" of a zebra against a \nstark background\nA backpack and a line of \nsupplies laying out\nA bathroom is very colorful with \nblue yellow and red\nA bear made out of gummy \nbears sitting on a counter\nA bed in a bedroom next to a \nslide glass door\nA bed with a colorful blanket \nhas an iron bed frame\na bench surrounded by \ndifferent types of plant life\nA big black dog sitting next to a \nlaptop computer\nA big brown horse near a fence \nin the snow\na big sail boat in the sea with \nother boats\na black and white cat hugging a \nhandbag\nFigure 7: Qualitative visualization. The adapter is trained on LAION-HighRes 5M. The aligned text representation is then\ndecoded by the Kandinsky decoder.\n15\n\nDistributional Vision Language Alignment by Cauchy-Schwarz Divergence\nw/ token \nAlignment\nOverall FID:\n12.14\nw/o token \nalignment\nOverall FID:\n12.62\nA black pan filled with \nmushrooms and vegetables\nLiving room setting with \nfurniture, fireplace and lamp\nA neat yellow bed in a room \nwith blue walls\na black red and green train \nengine on a track\nFigure 8: Token alignment is effective for fine-grained generations with more details and stronger semantic corre-\nspondence with the text inputs.\n16\n",
  "metadata": {
    "source_path": "papers/arxiv/Distributional_Vision-Language_Alignment_by_Cauchy-Schwarz_Divergence_3502fc921ed27b48.pdf",
    "content_hash": "3502fc921ed27b48174bfb708fd8c33e3e9f2cf1be15512efcf30a09a1ba1a67",
    "arxiv_id": null,
    "title": "Distributional Vision-Language Alignment by Cauchy-Schwarz Divergence",
    "author": "Wenzhe Yin, Zehao Xiao, Pan Zhou, Shujian Yu, Jiayi Shen, Jan-Jakob Sonke, Efstratios Gavves",
    "creation_date": "D:20250225023927Z",
    "published": "2025-02-25T02:39:27",
    "pages": 16,
    "size": 9955188,
    "file_mtime": 1740470192.8596344
  }
}