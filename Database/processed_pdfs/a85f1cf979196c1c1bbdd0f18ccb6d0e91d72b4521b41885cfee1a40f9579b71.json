{
  "text": "Supervised Contrastive Learning from Weakly-Labeled\nAudio Segments for Musical Version Matching\nJoan Serr`a 1 R. Oguz Araz 2 Dmitry Bogdanov 2 Yuki Mitsufuji 1 3\nAbstract\nDetecting musical versions (different renditions\nof the same piece) is a challenging task with im-\nportant applications. Because of the ground truth\nnature, existing approaches match musical ver-\nsions at the track level (e.g., whole song). How-\never, most applications require to match them at\nthe segment level (e.g., 20 s chunks). In addition,\nexisting approaches resort to classification and\ntriplet losses, disregarding more recent losses that\ncould bring meaningful improvements. In this pa-\nper, we propose a method to learn from weakly an-\nnotated segments, together with a contrastive loss\nvariant that outperforms well-studied alternatives.\nThe former is based on pairwise segment distance\nreductions, while the latter modifies an existing\nloss following decoupling, hyper-parameter, and\ngeometric considerations. With these two ele-\nments, we do not only achieve state-of-the-art\nresults in the standard track-level evaluation, but\nwe also obtain a breakthrough performance in a\nsegment-level evaluation. We believe that, due to\nthe generality of the challenges addressed here,\nthe proposed methods may find utility in domains\nbeyond audio or musical version matching.\n1. Introduction\nWhen two audio tracks contain different renditions of the\nsame musical piece, they are considered musical versions1.\nMusical versions are inherent in human culture and predate\nrecorded music and notation, as ancient music was transmit-\nted solely through playing and listening (Ball, 2010), which\nnaturally led to variations in tunes, rhythms, structures, etc.\nLearning representations of musical versions is a challeng-\ning task due to the degree and amount of variations that\ncan be present between versions, which go beyond typical\n1Sony AI 2Music Technology Group, Universitat Pompeu\nFabra 3Sony Group Corporation. Correspondence to: Joan Serr`a\n<joan.serra@sony.com>.\nPreprint. Work in progress.\naugmentations used by the machine learning community.\nTwo musical versions may feature different instrumentation\nor timbre, together with tonality and chord modifications,\naltered melodies, substantial changes to rhythm and tempo,\nan alternate temporal development or structure, and many\nmore (Yesiler et al., 2021). Yet, musical versions retain their\nessence, to the point that we can generally agree whether\ntwo of them correspond to the same piece or not2. Therefore,\nlearnt version representations need to encapsulate multiple\ncharacteristics shared between versions that, at the same\ntime, can discriminate them from other pieces.\nMusical version matching has several relevant applica-\ntions (Serr`a, 2011; Yesiler et al., 2021), including specific\napplications to plagiarism and near-duplicate detection3.\nBeyond business impact (Page, 2023) and cultural/artistic\nappreciation, some applications have become even more\nrelevant today, given the sustained rise and improvement\nof music generative models (e.g., Copet et al., 2023; Evans\net al., 2024; Liu et al., 2024). Indeed, the recent efforts on\nassessing music data replication, memorization, and attri-\nbution in such models exploit some form of music similar-\nity (Barnett et al., 2024; Bralios et al., 2024) or, for improved\nresults, musical version matching (Batlle-Roca et al., 2024).\nA fundamental limitation of version matching approaches is\nthat they operate at the full-track level, learning and extract-\ning individual representations from relatively long record-\nings (for instance, a few-minute song). This is due to ground\ntruth version annotations being only available per track.\nHowever, the segments of interest, for both classical and\nmodern applications, are much shorter than the track length\n(for instance, around 10â€“20 s). This mismatch between the\nlearning and inference stages, as we will see, causes a dra-\nmatic performance degradation (Sec. 5). Another challenge\nis that, in contrast to standard supervised learning tasks,\nmusical version data sets contain only a few items per class.\nFor instance, up to 56% of a recent realistic large-scale\n1A related but more restrictive and biased term in the literature\nis â€œcover songsâ€. To better understand this restriction and bias, see\nthe discussion found in, for example, Yesiler et al. (2021).\n2A well-known source collecting this information is https:\n//secondhandsongs.com.\n3Note that musical version matching may expand and subsume\ntraditional music fingerprinting (Cano et al., 2005).\n1\narXiv:2502.16936v1  [cs.SD]  24 Feb 2025\n\nSupervised Contrastive Learning from Weakly-Labeled Audio Segments for Musical Version Matching\nTable 1. Comparison of characteristics for a number of existing approaches and the proposed method CLEWS. We exclude multi-feature\nand/or multi-modal approaches (for example fusing CQT and melody estimations or leveraging audio and lyrics information). For further\ndetails and approaches we refer to the survey by Yesiler et al. (2021).\nNAME(S)\nMAIN\nINPUT\nARCH.\nSEGMENT\nPARTIAL\nLOSS / TRAIN\nRETRIEVAL\nREFERENCE\nLEARNING\nMATCH\nCONCEPT\nDISTANCE\nCQTNET\nYU ET AL. (2020)\nCQT\nCONVNET\nâœ—\nâœ—\nCLASSIF.\nCOSINE\nDORAS&PEETERS\nDORAS & PEETERS (2020)\nHCQT\nCONVNET\nâœ—\nâœ—\nTRIPLET\nCOSINE\nMOVE/RE-MOVE\nYESILER ET AL. (2020A)\nCREMA\nCONVNET\nâœ—\nâœ—\nTRIPLET\nEUCLIDEAN\nPICKINET\nOâ€™HANLON ET AL. (2021)\nCQT\nCONVNET\nâœ—\nâœ—\nCLASSIF.+CENTER\nCOSINE\nLYRACNET\nHU ET AL. (2022)\nCQT\nWIDERESNET\nâœ—\nâœ—\nCLASSIF.\nCOSINE\nBYTECOVER1/2\nDU ET AL. (2022)\nCQT\nRESNET\nâœ—\nâœ—\nCLASSIF.+TRIPLET\nCOSINE\nCOVERHUNTER\nLIU ET AL. (2023)\nCQT\nCONFORMER\nâœ—\nâœ“\nCLASSIF.+FOCAL+CENTER\nCOSINE\nBYTECOVER3/3.5\nDU ET AL. (2023)\nCQT\nRESNET\nâœ“\nâœ—\nCLASSIF.+TRIPLET\nCOSINE\nDVINET/DVINET+\nARAZ ET AL. (2024A)\nCQT\nCONVNET\nâœ—\nâœ—\nTRIPLET\nCOSINE\nCLEWS (PROPOSED)\nTHIS PAPER\nCQT\nRESNET\nâœ“\nâœ“\nCONTRASTIVE\nEUCLIDEAN\ndata set of around 500 k tracks is formed by only 2-item\nclasses, with an average of 5 items per class (Araz et al.,\n2024a). This characteristic suggests that, besides the tradi-\ntional focus on classification and triplet losses, a supervised\ncontrastive learning approach (Sec. 2) could also work well.\nIn this paper, we consider a full music track as a succession\nof weakly-labeled audio segments, and learn a contrastive\nrepresentation using such weak supervision. To do so, we\nintroduce two main methods. First, we develop a number\nof pairwise distance selection strategies, which reduce a\nsegment-level distance matrix into a track-level distance\nmatrix. This enables the direct utilization of track-level\nannotations without statically assigning them to some or\nall of the segments. Second, we reformulate the alignment\nand uniformity (A&U) loss of Wang & Isola (2020), origi-\nnally introduced for self-supervised learning, to operate on a\n(weakly) supervised learning task. Motivated by decoupling,\nhyper-parameter, and geometric considerations, we intro-\nduce several changes that convert A&U into a new loss func-\ntion: the strict decoupling of positives and negatives (Yeh\net al., 2022), the simplification of hyper-parameters, the\nnative operation in Euclidean geometry (cf. Koishekenov\net al., 2023), and a smoothing constant for negative pairs.\nWith both distance reduction and contrastive learning strate-\ngies, we do not only outperform existing approaches in the\nsegment-level evaluation by a large margin, but we also\nachieve state-of-the-art results in the standard track-level\nevaluation. We also perform an extensive ablation study\nto empirically compare the proposed methods with several\nalternatives, including additional reduction strategies and\ncommon contrastive losses. We believe that, due to the\ngenerality of the challenges addressed here, the proposed\nmethods may find utility in further domains beyond musical\nversion matching. To facilitate understanding and reproduc-\ntion, we share our code and model checkpoints in http:\n//anonymous-to-be-released-github-link.\n2. Background\nAfter a history of rule-, feature-, and model-based ap-\nproaches (Serr`a, 2011; Yesiler et al., 2021), musical ver-\nsion matching is currently tackled as a supervised learning\nproblem, focusing on full-track pairwise matching (Table 1).\nHowever, two versions do not necessarily need to match for\ntheir entire duration, and actually several applications rely\non few-second partial matches. Only a couple of approaches\nbase their learning or retrieval stages on segments or par-\ntial matches, respectively. ByteCover3 (Du et al., 2023)\npioneered learning from segments with their â€œmaxmeanâ€\noperator. However, such operator still does not allow for\npartial matches, as it forces all segments of a track to match\nsome segment from another track. CoverHunter (Liu et al.,\n2023) is able to detect partial matches of around 45 s. How-\never, the learning strategy to do so is based on a two-stage\nbrute-force approach. First, it trains a coarse detector model\non 15-second segments using classification, focal, and cen-\nter losses. Then, it resorts to this first-stage model and a\nrule-based approach to (weakly) label 45-second segments,\nwhich are finally used to train the second-stage model with\nthe same losses. In both stages, CoverHunter treats seg-\nments as full tracks. To our knowledge, we are the first\nto consider an entirely segment-based approach for both\nlearning and retrieval stages.\nThe literature on musical version matching has tradition-\nally considered a number of classification (Sun et al., 2014)\nand triplet (Schroff et al., 2015) loss variants, and their\ncombination (Table 1). However, given the same ground\ntruth, another approach to learning version representations\nwould be to consider a supervised contrastive loss like N-\npairs (Sohn, 2016) or SupCon (Khosla et al., 2021). In addi-\ntion, a number of well-established losses for self-supervised\nlearning like InfoNCE/NT-Xent (Van den Oord et al., 2018;\nChen et al., 2020), alignment and uniformity (Wang & Isola,\n2020), or SigLIP (Zhai et al., 2023) could also be adapted.\nAn analysis of the relations between many of such losses is\n2\n\nSupervised Contrastive Learning from Weakly-Labeled Audio Segments for Musical Version Matching\ncarried out by Koromilas et al. (2024). Apart from the loss\nfunction, other considerations such as positive/negative de-\ncoupling (Yeh et al., 2022) and the correspondence between\ndistance and space geometry (Koishekenov et al., 2023) are\npotentially relevant in a practical case. To our knowledge,\nwe are the first to consider a supervised contrastive loss for\nmusical version matching.\n3. Contrastive Learning from Weakly-Labeled\nAudio Segments\nWe now detail our approach to perform contrastive learning\nfrom weakly-labeled audio segments (CLEWS). The first\npart deals with track-level labels and their allocation to\nsegment distances (we base our development on distances,\nbut it can be easily reformulated using similarities). The\nsecond part details the contrastive loss function we use. The\nthird part explains our architecture and training procedure.\n3.1. Segment Distance Reduction\nFramework â€” Given the k-th waveform segment of the\ni-th music track, xk\ni , we compute latent representations\nzk\ni = F(xk\ni ), where F represents a neural network that\npools the time-varying information of the segment into a\nsingle vector (architecture details can be found in Sec. 3.3\nand Appendix A). Then, for every possible pair of segments\nk and l of every possible pair of tracks i and j, we com-\npute their distance Ëœdkl\nij and obtain the distance matrix ËœD\n(Fig. 1). At this point, if there are n query tracks and m\ncandidate tracks with u and v segments4, respectively, we\nhave ËœD âˆˆRnuÃ—mv\nâ‰¥0\n. However, since labels are only pro-\nvided at the track level, our binary ground truth assignments\n(1 for version/positive and 0 for non-version/negative) are\nA âˆˆZnÃ—m\n2\n. Therefore, we need some strategy to (weakly)\nallocate n Ã— m labels to nu Ã— mv segment distances.\nA naÂ¨Ä±ve strategy to do such allocation would be to propa-\ngate all positive/negative track assignments to all segment\ncomparisons in the sub-rectangle ËœDij defined by a pair of\ntracks i and j. This is the approach implicitly followed by\nCoverHunter (Liu et al., 2023). However, besides its poor\nperformance (Sec. 5), this strategy incurs a fundamental\nerror, in the sense that it is teaching the model that all pos-\nitive segments are â€˜similarâ€™ to all other positive segments,\nwhich in the case of musical versions is false (two segments\ncould reproduce two different motives of the same song;\neven though it is the same song, the segments are usually\nnot the same, unless it is an extremely repetitive song). In-\nstead of trying to allocate n Ã— m positive/negative distances\nto nu Ã— mv segment distances, we take the opposite view\n4Notice that, similar to cross-attention in Transformers, we can\ndeal with different track lengths by taking a maximum length u, v\nand masking. See Fig. 1 (right) for a couple of examples.\nz!\n\" z!\n#\nz!\n$\nz!%\"\n\"\nz!%\"\n$\nz&\n\"\nz&\n#\nz&\n'\nz&%\"\n\"\nz&%\"\n#\nz&%\"\n'\nz!%\"\n(\nz&%\"\n)\n\"ğ‘‘&!\n'$\n$ğƒ&!\n$ğƒ\nFigure 1. Illustration of four reduction functions R over pair-\nwise segment distances Ëœdkl\nij. They are depicted on different sub-\nrectangles ËœDij, where tracks i, j, and j + 1 are versions (green\nsquares) and track i + 1 is not (orange squares). The four func-\ntions correspond to: Rmeanmin (top left), Rbpwr-3 (top right), Rbest-10\n(bottom left), and Rmin (bottom right). The Rbpwr-3 strategy de-\npicts its minimum/masking recursion in increasingly dark levels\n(green/purple cells). The sub-rectangles for Rbpwr-3 and Rmin also\nexemplify dealing with different lengths by masking (gray cells).\nand reduce nu Ã— mv segment distances to n Ã— m track dis-\ntances, matching the dimensions of positive/negative assign-\nments A. More specifically, we consider several reduction\nfunctions R, producing D = R(ËœD), where D âˆˆRnÃ—m\nâ‰¥0\n.\nIn addition, we employ different reduction functions for\npositive and negative pairs, R+ and Râˆ’, respectively.\nReduction Functions â€” The naÂ¨Ä±ve strategy outlined above\nwould correspond to a mean reduction over the entire sub-\nrectangle determined by ËœDij:\ndij = Rmean\n\u0000ËœDij\n\u0001\n= 1\nuv\nX\n1â‰¤kâ‰¤u\n1â‰¤lâ‰¤v\nËœdkl\nij.\nInstead, if we just consider the r best matches across seg-\nments, we have\ndij = Rbest-r\n\u0000ËœDij\n\u0001\n= 1\nr\nX\n1â‰¤tâ‰¤r\ntopr\n\u0000ËœDij\n\u0001\nt\nfor r â‰¤uv, where topr(D) is a function that returns the\nlowest r distances in D (Fig. 1, bottom left). Another possi-\nbility is to consider just the single best correspondence in\nthe entire sub-rectangle (Fig. 1, bottom right), yielding\ndij = Rmin\n\u0000ËœDij\n\u0001\n= min\n1â‰¤kâ‰¤u\n1â‰¤lâ‰¤v\nËœdkl\nij.\n3\n\nSupervised Contrastive Learning from Weakly-Labeled Audio Segments for Musical Version Matching\nA further alternative is to use an operator like the one used\nin ByteCover3 (Du et al., 2023), which first searches for the\ncandidate best match per query and then averages across all\nquery segments. Reformulating it for distances (Fig. 1, top\nleft), we obtain\ndij = Rmeanmin\n\u0000ËœDij\n\u0001\n= 1\nu\nX\n1â‰¤kâ‰¤u\nmin\n1â‰¤lâ‰¤v\nËœdkl\nij.\nNotice that the Rmean, Rbest-r, and Rmeanmin strategies above\ndo not explicitly prevent multiple consecutive segments of\ntrack i being assigned to the same segment of track j (ver-\ntical or horizontal traces in Fig. 1). As mentioned, this is\nunrealistic for the majority of music tracks (and in general\nfor any signal or sequence presenting a minimal variability\nwith time). Notice also that Rmean and Rmeanmin force a\nfull-track match, that is, they are teaching the model that all\nsegments in track i should find a match in track j. Again,\nthis is an unrealistic assumption for musical versions where\nthe structure changes (and in general for any signal or se-\nquence featuring only partial matches).\nBest-pair Reduction â€” Motivated by the issues of con-\nsecutive and global segment matching above, we decide to\ndesign an additional reduction strategy that explicitly deals\nwith both. We term it bpwr-r, for â€˜best pair without replace-\nmentâ€™ with a threshold r. In a nutshell, Rbpwr-r operates by\nsorting all distances in the ËœDij sub-rectangle in increasing\norder, then taking the first one (say the one involving seg-\nments k and l), removing all distances computed by using\neither one or the other segment (either k or l), and iterating r\ntimes. It then takes the average among those r best pairwise\ndistances. More formally, we can express it as\ndij = Rbpwr-r\n\u0000ËœDij\n\u0001\n= 1\nr\nX\n1â‰¤qâ‰¤r\nRmin\n\u0010\nËœD\n(q)\nij\n\u0011\n(1)\nfor r â‰¤min(u, v), with the recursion\nËœD\n(q)\nij =\n(ËœDij\nfor q = 1,\nmaskmin\n\u0010\nËœD\n(qâˆ’1)\nij\n\u0011\nfor q > 1,\nwhere maskmin(D) is a function that masks the row and the\ncolumn corresponding to the minimum element in D, such\nthat those elements are not eligible by the Rmin of Eq. 1 in\niterations q > 1. A schema of Rbpwr-3 is illustrated in Fig. 1\n(top right; recursion is depicted by progressively darker\ncolors). Notice that masking rows and columns avoids the\nissue of consecutive segment matching5, and that a thresh-\nold r < min(u, v) avoids the issue of full-track matching,\nwhich only happens when r = min(u, v).\n5There are some specific cases where a pattern could occupy\ntwo consecutive segments (e.g., when it is split between them or\nwhen it is reproduced at half the speed). We claim that, in such\ncases, learning from just one of the two consecutive segments is\nenough (our results in Sec. 5 support this claim).\nPositives and Negatives Reduction â€” We note that the\nprevious distance reduction strategies have some concep-\ntual parallelism with the negative/positive mining strategies\nused with triplet losses (Schroff et al., 2015) or in some\ncontrastive approaches (cf. Kalantidis et al., 2020). In our\ncase, for instance, Rmin could correspond to a hard min-\ning strategy, while Rbest-r or Rbpwr-r could be regarded as\nsemi-hard mining of segment pairs. Thus, inspired by those\nstrategies, we decide to study if applying different reduc-\ntion strategies for positives and negatives has some effect in\nour setup. To obtain a track-based pairwise distance matrix\nthat combines different reductions for positive and negative\npairs, we calculate\nD = A âŠ™R+\u0000ËœD\n\u0001\n+ (1 âˆ’A) âŠ™Râˆ’\u0000ËœD\n\u0001\n,\n(2)\nwhere âŠ™denotes element-wise multiplication and 1 is the all-\nones matrix (recall that the elements in A are 1 for positives\nand 0 otherwise). The reductions R+ and Râˆ’can be chosen\namong the ones presented above.\n3.2. Contrastive Loss\nMotivation â€” After computing pairwise track-level dis-\ntances D, we need a contrastive loss that can exploit them\nand that, ideally, can outperform the existing losses in the\nconsidered task. For that, one can consider any supervised\ncontrastive loss function that operates on distances, or adapt\nan existing self-supervised loss to the supervised frame-\nwork (Sec. 2). In our case, we opt for the latter and choose\nthe A&U loss of Wang & Isola (2020) due to its appeal-\ning properties and intuitive derivation. One of the practical\nproperties we value is that, by using expectations, we have\na similar behavior for different batch sizes (Wang & Isola\n2020; see also Koromilas et al. 2024). In our analysis, we\nwill use the concept of â€œpotentialâ€ as introduced by Wang &\nIsola (2020), but nonetheless will depart from the concept\nof uniformity in the hypersphere.\nChanges to Alignment and Uniformity â€” The A&U loss\nwas designed for self-supervised contrastive learning, and\nthus expects one positive for each item in the batch (obtained\nthrough some augmentation), and negatives correspond to\nall other elements in the batch (uniformly sampled from all\navailable data). To adapt A&U to supervised contrastive\nlearning with multiple positives per anchor, we need to\ncarefully define both positive and negative sets. In particular,\nwe want to preserve the decoupling of the alignment and\nuniformity terms as, apart from respecting the original idea\nof A&U, it typically yields improved performance (Yeh\net al., 2022). Therefore, from all pairwise assignments A in\nthe batch, we need to gather positive A+ and negative Aâˆ’\nassignment sets such that A+ âˆ©Aâˆ’= âˆ…. This also implies\ndiscarding comparisons of one track against itself (that is,\nthe diagonals of D and A, and potentially other spurious\ncells corresponding to sampling the same track more than\n4\n\nSupervised Contrastive Learning from Weakly-Labeled Audio Segments for Musical Version Matching\nonce in the same batch).\nGiven the sets A+ and Aâˆ’, we can write a decoupled, su-\npervised version of A&U over a batch as\nËœL =\n1\n|A+|\nX\n(i,j)âˆˆA+\ndÎ±\nij + Î» log\nï£«\nï£­\n1\n|Aâˆ’|\nX\n(i,j)âˆˆAâˆ’\neâˆ’Î³d2\nij\nï£¶\nï£¸,\n(3)\nwhere | | denotes set cardinality and Î±, Î», and Î³ are hyper-\nparameters. Wang & Isola (2020) do not report strong per-\nformance differences by changing Î±, Î», and Î³ within a\ncertain range, and generally set Î± = 2, Î» = 1, and Î³ = 3\nfor their experiments. In preliminary experiments, and for\nour task, we find similar conclusions for Î± and Î», but not for\nÎ³. In addition, we are motivated to use Î± = 2 and Î» = 1, as\nthat makes alignment and uniformity terms more compara-\nble (same distance and same weight; see also the gradient\nanalysis below).\nThe original A&U loss employs Euclidean distances on\nthe hypersphere, using L2-normalized z vectors. This, in\nour view, presents a potential issue, in the sense that the\nemployed distance function does not match with the geo-\nmetric structure of the space (Euclidean vs. hypersphere\nsurface, respectively). In our approach, instead of consid-\nering the negative arc length (which corresponds to the\ngeodesic distance in the hypersphere) to improve perfor-\nmance like Koishekenov et al. (2023), we opt for the plain\nEuclidean space (of which the Euclidean distance is its\ngeodesic distance). Thus, we do not constrain z to have\na unit norm. Notice that, with this change, the uniformity\nconcept does not apply, as we are not in an hypersphere\nanymore. Nonetheless, we can still reason and base our in-\ntuitions on the kernel and potential concepts used to derive\nthe uniformity term in Wang & Isola (2020).\nWith the decoupling, hyper-parameter, and geometric con-\nsiderations above, we formulate the CLEWS loss as\nL =\n1\n|A+|\nX\n(i,j)âˆˆA+\nd2\nij + log\nï£«\nï£­Îµ +\n1\n|Aâˆ’|\nX\n(i,j)âˆˆAâˆ’\neâˆ’Î³d2\nij\nï£¶\nï£¸,\nwhere dij are distances after reduction (Eq. 2) and Î³, Îµ > 0\nare hyper-parameters. We use dimension-normalized Eu-\nclidean distances (root mean squared differences), as this\ndoes not affect our geometric considerations (it just adds\na constant) and facilitates maintaining the same hyper-\nparameters when changing the dimensionality of z. The\nÎµ hyper-parameter is initially introduced for numerical sta-\nbility. However, we note that it also has a soft thresholding\nor smoothing effect for the potential between negative pairs.\nRole of the Hyper-parameters â€” We now briefly and\nintuitively study the role of Î³ and Îµ in L (a full analysis\nis beyond the scope of the present paper). To do so, we\nconsider the gradient of L for a specific distance pair dij.\nDepending if the pair is in A+ or Aâˆ’, we have\nâˆ‡+ â‰œâˆ‚L\nâˆ‚dij\n\f\f\f\f\n(i,j)âˆˆA+ = 2dij\n|A+|\nor\nâˆ‡âˆ’â‰œâˆ‚L\nâˆ‚dij\n\f\f\f\f\n(i,j)âˆˆAâˆ’=\nâˆ’2Î³dijeâˆ’Î³d2\nij\n|Aâˆ’|Îµ + c + eâˆ’Î³d2\nij ,\n(4)\nwhere eâˆ’Î³d2\nij corresponds to the negative potential for the\npair i, j (Wang & Isola, 2020) and the constant c is the sum\nof all potentials that do not feature (i, j). To facilitate our\nanalysis, we view Îµ as a reference potential and redefine\nË†Îµ = |Aâˆ’|Îµ+c. We can then consider three cases with regard\nto the relation of Ë†Îµ and the potential for the negative pair\ni, j. If Ë†Îµ â‰ªeâˆ’Î³d2\nij (case 1), we have âˆ‡âˆ’â‰ˆâˆ’2Î³dij and, if\nË†Îµ â‰ˆeâˆ’Î³d2\nij (case 2), we have âˆ‡âˆ’â‰ˆâˆ’Î³dij. In both cases,\nand thanks to having set Î± = 2 after Eq. 3, the terms in âˆ‡âˆ’\nare similar to the ones in âˆ‡+ (thus positive and negative\npairs have a comparable influence). Moreover, we see that\nÎ³ is also acting as a weight for the negative pairsâ€™ gradient\n(thus taking a similar role as the original Î» in Eq. 3). Finally,\nif Ë†Îµ â‰«eâˆ’Î³d2\nij (case 3), we have\nâˆ‡âˆ’â‰ˆâˆ’2Î³dijeâˆ’Î³d2\nij\nË†Îµ\n,\nwhich implies a progressively vanishing gradient with in-\ncreasing dij (eâˆ’Î³d2\nij decreases much faster than Î³dij in-\ncreases). Thus, we obtain a smooth transition to zero gra-\ndient after the potential eâˆ’Î³d2\nij crosses a threshold that is a\nfunction of Îµ. A visualization of the effect of Î³ and Îµ on\nâˆ‡âˆ’(Eq. 4) is given in Appendix A.\n3.3. Architecture and Training\nWe now overview CLEWSâ€™ network architecture F and\nits training procedure (further details are available in Ap-\npendix A and in our code). To obtain segment embedding\nvectors z on which to compute distances, we start from\nthe full-track audio waveform and uniformly randomly cut\na 2.5 min block x from it. We further cut x into 8 non-\noverlapping 20-second segments xk (we repeat-pad the last\nsegment). We then compute its constant-Q spectrogram,\ndownsample it in time by a factor of 5, and normalize it\nbetween 0 and 1, all following similar procedures as com-\nmon version matching approaches (Yesiler et al., 2021).\nAfter that, we pass it to a learnable frontend, formed by two\n2D strided convolutions, batch normalization, and a ReLU\nactivation. Next, we employ a pre-activation ResNet50\nbackbone (He et al., 2016) with ReZero (Bachlechner et al.,\n2021) and instance-batch normalization (Pan et al., 2018).\nWe pool the remaining spectro-temporal information with\n5\n\nSupervised Contrastive Learning from Weakly-Labeled Audio Segments for Musical Version Matching\ngeneralized mean pooling (RadenoviÂ´c et al., 2019), and\nproject to a 1024-dimensional representation z using batch\nnormalization and a linear layer. We train CLEWS with\nR+ = Rbpwr-5, Râˆ’= Rmin, Î³ = 5, and Îµ = 10âˆ’6 as de-\nfaults, and study the effect of such choices in Sec. 5. Since\ntest sets also contain tracks longer than the 2.5 min used for\ntraining, in CLEWS we use our proposed Rbpwr-10 for track\nmatching, together with a segment hop size of 5 s.\nWe train all models with Adam using a learning rate of\n2Â·10âˆ’4, following a reduce-on-plateau schedule with a 10-\nepoch patience and an annealing factor of 0.2. The only\nexception is in ablation experiments, where we train for\n20 epochs featuring a final 5-epoch polynomial learning\nrate annealing. In every epoch, we group all tracks into\nbatches of 25 anchors and, for each of them, we uniformly\nsample with replacement 3 positives from the corresponding\nversion group (excluding the anchor). Thus, we get an\ninitial (track-based) batch size of 100. For every track in\nthe batch, we uniformly sample 2.5 min from the full-length\nmusic track and create the aforementioned 8 segments per\ntrack. Thus, we get a final (segment-based) batch size of\n800. We only use time stretch, pitch roll, and SpecAugment\naugmentations (Liu et al., 2023).\n4. Evaluation Methodology\nData â€” We train and evaluate all models on the publicly-\navailable data sets DiscogsVI-YT (DVI; Araz et al., 2024a)\nand SHS100k-v2 (SHS; Yu et al., 2020), using the prede-\nfined partitions. SHS is a well-established reference data set.\nHowever, since it is based on YouTube links, it is almost\nimpossible to gather it entirely nowadays (we managed to\ngather 82% of it). In addition, one could consider it slightly\nbiased, as the version group sizes are unrealistically large (cf.\nDoras & Peeters, 2020; Araz et al., 2024a). Instead, the re-\ncently proposed DVI data set is 5 times larger and better\nrepresents the real-world distribution of version group sizes.\nFor both data sets, we use 16 kHz mono audio and cap the\nmaximum length to the first 10 min.\nBaselines â€” To compare the performance of the proposed\napproach with the state of the art, we consider several base-\nlines: CQTNet (Yu et al., 2020), MOVE (Yesiler et al.,\n2020a), LyraC-Net (Hu et al., 2022), CoverHunter (Liu et al.,\n2023), DVINet+ (Araz et al., 2024b), Bytecover2 (Du et al.,\n2022), ByteCover3 (Du et al., 2023), and ByteCover3.5 (Du\net al., 2024). For CoverHunter, we just consider the first\nâ€œcoarseâ€ stage, as that is the part dealing with segments.\nCoverHunter, CQTNet, and DVINet have convenient source\ncode available, and thus we can produce results by using it in\nour own pipeline (this way we can compare those baselines\nwith our model rigorously under the same setting). Due to\nGPU memory restrictions, we train with randomly-sampled\naudio blocks of 2.5 min. For the other baselines, we can only\nuse already reported results as reference. The ByteCover se-\nries of models are known to be non-reproducible (Oâ€™Hanlon\net al., 2021; Hu et al., 2022), with all attempts to date sub-\nstantially under-performing6 the reported results. We also\nimplement our version of the ByteCover models and, for the\nfirst time, are able to obtain results that come close to the\nones reported in the original papers (Sec. 5). In the follow-\ning, we denote our approximations to ByteCover with a â€ \nsymbol. Having retrained/replicated baselines provides us\nan estimation of their performance in scenarios that have not\nyet been considered in the literature, such as with the DVI\ndata set or the segment-level evaluation proposed below.\nEvaluation â€” During testing, to compute candidate em-\nbeddings, we treat all models as if they were segment-based\nand extract overlapping blocks or segments using the same\nlength as in training and a hop size of 5 s (this yielded a\nmarginal improvement for full-track baselines trained on\n2.5 min blocks). With these candidate embeddings, we per-\nform both track- and segment-level evaluations. The former\nis equivalent to the usual evaluation setup in musical ver-\nsion matching, while the latter focuses on the retrieval of\nbest-matching segments. For the track-level evaluation, we\nuse the same segment length (the training segment length)\nand hop size for both queries and candidates. Then, to mea-\nsure the performance of the system working at the full-track\nlevel, we use Rmeanmin to compute the final query-candidate\ndistance. For the segment-level evaluation, we keep the\nsame segment configuration as in the track-level case, but\nwe vary the query segment length Ï„. This way we assess\na modelâ€™s performance on different query lengths found in\nreal-world scenarios. Then, to measure the performance\nof the system working at the segment level, we use Rmin\nto compute a best-match query-candidate distance. With\nthis best-match approach, we simulate the performance of\nan equivalent segment-based retrieval system using all raw\nsegments as candidates (see Appendix B). As evaluation\nmeasures, we compute the usual mean average precision\n(MAP) plus an enhanced version of the normalized average\nrank (NAR; see Appendix B). MAP focuses on the precision\nin the top candidates while NAR focuses more on the overall\nrecall, which we think is a better option for musical version\nmatching, especially for segment-based applications.\n5. Results\nComparison with the State of the Art â€” First of all, we\nfocus on the track-level evaluation and compare with the\nstate of the art. We observe that CLEWS outperforms all\nconsidered approaches, many of them by a large margin\n(Table 2). CLEWS obtains a NAR of 2.70 on DVI-Test and\na MAP of 0.876 on SHS-Test, setting a new state-of-the-art\n6See\nfor\ninstance\nhttps://github.com/Orfium/\nbytecover/issues/2.\n6\n\nSupervised Contrastive Learning from Weakly-Labeled Audio Segments for Musical Version Matching\nTable 2. Track-level evaluation and comparison with the state of the art. The symbol â€  denotes that it is our implementation.\nAPPROACH\nDVI-TEST\nSHS-TEST\nNAR â†“\nMAP â†‘\nNAR â†“\nMAP â†‘\nCOVERHUNTER-COARSE (LIU ET AL., 2023)\n10.36 Â± 0.07\n0.157 Â± 0.001\n4.09 Â± 0.17\n0.491 Â± 0.007\nMOVE (YESILER ET AL., 2020A)\nN/A\nN/A\nN/A\n0.519\nCQTNET (YU ET AL., 2020)\n6.68 Â± 0.07\n0.493 Â± 0.002\n2.67 Â± 0.16\n0.677 Â± 0.007\nDVINET+ (ARAZ ET AL., 2024B)\n3.69 Â± 0.06\n0.643 Â± 0.002\n2.39 Â± 0.16\n0.720 Â± 0.007\nLYRAC-NET (HU ET AL., 2022)\nN/A\nN/A\nN/A\n0.765\nBYTECOVER3â€  (BASED ON DU ET AL., 2023)\n5.64 Â± 0.05\n0.513 Â± 0.002\n1.91 Â± 0.14\n0.783 Â± 0.006\nBYTECOVER1/2â€  (BASED ON DU ET AL., 2022)\n4.98 Â± 0.06\n0.595 Â± 0.002\n1.95 Â± 0.14\n0.813 Â± 0.006\nBYTECOVER3 (DU ET AL., 2023)\nN/A\nN/A\nN/A\n0.824\nBYTECOVER3.5 (DU ET AL., 2024)\nN/A\nN/A\nN/A\n0.857\nBYTECOVER2 (DU ET AL., 2022)\nN/A\nN/A\nN/A\n0.863\nCLEWS (PROPOSED)\n2.70 Â± 0.05\n0.774 Â± 0.002\n1.27 Â± 0.12\n0.876 Â± 0.005\n5 10\n20\n30\n40\n60\n90\nÏ„ [s]\n1\n2\n5\n10\n20\n50\nNAR â†“\n5 10\n20\n30\n40\n60\n90\nÏ„ [s]\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMAP â†‘\nCoverHunter-Coarse\nCQTNet\nDVINet+\nByteCover1/2 â€ \nByteCover3 â€ \nCLEWS (proposed)\nFigure 2. Segment-level evaluation with DVI-Test. NAR (left) and MAP (right) for different query segment lengths Ï„ (notice the\nlogarithmic axis for NAR). The shaded regions correspond to 95% confidence intervals (barely visible due to the size of DVI-Test).\nComparatively similar results for SHS-Test and also for an alternative evaluation protocol are available in Appendix C.\nresult on both data sets. Besides CLEWS, an interesting\nobservation to make is that the rank of an existing approach\nconsiderably varies between SHS-Test and DVI-Test. That\nis the case of DVINet+, which obtains a modest performance\non SHS-Test, but achieves the best performance among the\nexisting approaches on DVI-Test. We hypothesize that this\nis thanks to being the only considered approach (apart from\nCLEWS) that does not learn from a classification loss: be-\ncause SHS-Train has more items per class than the more real-\nistic DVI-Train (an average of 12 vs. 2, respectively), a clas-\nsification loss is able to learn a useful representation from\nSHS-Train, but not from DVI-Train. Both DVINet+ and\nCLEWS, utilizing triplet and contrastive losses, respectively,\ndo not suffer from this issue and maintain a comparatively\nsimilar performance across the two data sets. Another obser-\nvation to make with regard to the consideration of segments\nis that treating them independently (CoverHunter-Coarse)\nyields worse results than developing some specific strategies\n(ByteCover3, ByteCover3â€ , and CLEWS), and that learning\nfrom a global match (ByteCover3 and ByteCover3â€ ) is not\nas optimal as learning from a partial match (CLEWS). The\nlatter is further supported by our ablations below.\nSegment-based Version Matching â€” We now focus on\nthe segment-level evaluation and study the performance as\na function of the query segment length Ï„. We observe that\nCLEWS again outperforms all considered models both in\nDVI-Test (Fig. 2) and SHS-Test (Appendix C), and for both\nNAR and MAP measures. Importantly, CLEWS maintains\na high performance for all considered lengths (Fig. 2). The\nonly exception is with Ï„ = 5 where, according to our listen-\ning experience, it is sometimes difficult even for a human\nto establish if two audio segments are versions or not. Ac-\ncording to our segment-level evaluation, ByteCover3â€  fea-\ntures some noticeable performance degradation with large\nÏ„, perhaps due to the global match approach. CQTNet\nand DVINet+, both based on the same plain convolutional\narchitecture, show an early performance decline for Ï„ < 60.\nAblations and Hyper-parameters â€” Finally, we focus our\nattention on possible variations to the default CLEWS. We\nstart by studying the effect of positive R+ and negative Râˆ’\nsegment distance reductions (Table 3). For R+, if we keep\nRâˆ’=Rmin, we observe that, depending on the evaluation\nmeasure, we have two options that are better than the default:\n7\n\nSupervised Contrastive Learning from Weakly-Labeled Audio Segments for Musical Version Matching\nTable 3. Results on DVI-Valid for different positive R+ and nega-\ntive Râˆ’distance reductions. The default CLEWS reductions are\nR+ = Rbpwr-5 and Râˆ’= Rmin.\nR+\nRâˆ’\nNAR â†“\nMAP â†‘\nCLEWS (PROPOSED)\n2.57 Â± 0.09\n0.804 Â± 0.003\nRbpwr-3\nRmin\n2.60 Â± 0.09\n0.809 Â± 0.003\nRbpwr-8\nRmin\n2.51 Â± 0.09\n0.789 Â± 0.003\nRmeanmin\nRmin\n2.58 Â± 0.09\n0.798 Â± 0.003\nRbest-10\nRmin\n2.63 Â± 0.09\n0.795 Â± 0.003\nRmin\nRmin\n2.79 Â± 0.09\n0.799 Â± 0.003\nRbpwr-5\nRbest-10\n2.82 Â± 0.10\n0.779 Â± 0.003\nRbpwr-5\nRbpwr-5\n2.88 Â± 0.10\n0.778 Â± 0.003\nRbpwr-5\nRmeanmin\n4.95 Â± 0.12\n0.488 Â± 0.004\nTable 4. Results on DVI-Valid for different loss functions using\nthe default CLEWS reductions of R+ = Rbpwr-5 and Râˆ’= Rmin.\nLOSS FUNCTION\nNAR â†“\nMAP â†‘\nCLEWS (PROPOSED)\n2.57 Â± 0.09\n0.804 Â± 0.003\nSUPCON\n2.69 Â± 0.09\n0.676 Â± 0.004\nSIGLIP\n2.79 Â± 0.09\n0.684 Â± 0.004\nTRIPLET\n3.08 Â± 0.11\n0.717 Â± 0.004\nSUPCON-DECOUPLED\n3.14 Â± 0.11\n0.739 Â± 0.004\nA&U-DECOUPLED\n3.25 Â± 0.11\n0.620 Â± 0.004\nCLASSIFICATION XENT\n8.91 Â± 0.14\n0.205 Â± 0.003\nRbpwr-8 for NAR and Rbpwr-3 for MAP. Nonetheless, we\ndecide to keep the default one as a compromise between the\ntwo. With such compromise in mind, we observe that, for\npositive reductions, global matches (Rbpwr-8 and Rmeanmin)\nunder-perform a partial matches (Rbpwr-r), and that learning\nfrom consecutive segments (Rbest-10 and Rmeanmin) is not as\ncompetitive as avoiding them (Rbpwr-r). For Râˆ’, if we keep\nR+ =Rbpwr-5, we see that all considered negative reductions\nunder-perform the default Rmin. We hypothesize that, as\nwith triplet losses, a hard negative mining or worst-case\nstrategy is beneficial (cf. Schroff et al., 2015; Kalantidis\net al., 2020). Reduction strategies based on Rmean did not\nlearn well, both for R+ and Râˆ’(not shown).\nThe next aspect we study is the effect of the loss function\ngiven the default reduction strategies for R+ and Râˆ’(Ta-\nble 4). We observe that the proposed CLEWS loss performs\nbetter in both NAR and MAP, with a significant difference\nin the latter measure. Standard losses like SupCon, SigLIP,\nand Triplet (Sec. 2) come next, not being able to take as\nmuch profit from R+ and Râˆ’as CLEWS in the task we\nstudy. As already mentioned, a standard classification loss\nbased on cross-entropy does not perform well, especially\nwhen training with very few instances per class.\nThe last aspect we study is the effect of hyper-parameters Î³\nand Îµ. If we zoom in the resolution of NAR and MAP, we\nobserve an opposite trend for the two evaluation measures\n1\n2\n5\n8\n12\nÎ³\n(using Îµ = 10âˆ’6)\n2.25\n2.50\n2.75\n3.00\n3.25\nNAR â†“\n0.725\n0.750\n0.775\n0.800\n0.825\nMAP â†‘\n10âˆ’3\n10âˆ’4\n10âˆ’5\n10âˆ’6\n10âˆ’7\n10âˆ’8\nÎµ\n(using Î³ = 5)\n2.25\n2.50\n2.75\n3.00\n3.25\nNAR â†“\n0.725\n0.750\n0.775\n0.800\n0.825\nMAP â†‘\nFigure 3. Effect of hyper-parameters Î³ (top) and Îµ (bottom) on\nDVI-Valid. Shaded regions correspond to 95% confidence inter-\nvals, and the default value is highlighted with a square marker.\n(Fig. 3): with progressively decreasing NAR (better perfor-\nmance), we obtain a progressively decreasing MAP (worse\nperformance). This indicates that hyper-parameters Î³ and Îµ\ncan be deliberately tuned to benefit one or the other measure\n(we did not extensively tune them for the results reported\npreviously). Moreover, we actually see that setting Î³ = 2\ncould have provided a better NAR and a slighlty increased\nMAP. Overall, however, if we zoom ourselves out from the\nresolution shown in Fig. 3, we essentially observe a plateau\nof performance between Î³ âˆˆ[2, 8] and Îµ âˆˆ[10âˆ’8, 10âˆ’5].\n6. Conclusion\nIn this paper, we tackle the task of segment-based musi-\ncal version matching, and propose both a strategy to deal\nwith weakly-labeled segments and a contrastive loss that\noutperforms well-studied alternatives. Through a series of\nextensive experiments, we show that our approach not only\nachieves state-of-the-art results in two different datasets and\ntwo different metrics, but also that it significantly outper-\nforms existing approaches in a best-match, segment-level\nevaluation. We also study the effect of different reduction\nstrategies, compare against existing losses, and analyze the\neffect of the hyper-parameters in our ablation studies. As\nweakly labeled segment information is ubiquitous in many\nresearch areas, and since the concepts exploited here are gen-\neral to a wide range of contrastive learning tasks, we believe\nour methods could serve as inspiration or find usefulness in\ndomains beyond audio and musical version matching.\n8\n\nSupervised Contrastive Learning from Weakly-Labeled Audio Segments for Musical Version Matching\nAcknowledgements\nR. Oguz Araz is supported by the pre-doctoral program\nAGAUR-FI ajuts (2024 FI-3 00065) Joan OrÂ´o, and the\nCÂ´atedras ENIA program â€œIA y MÂ´usica: CÂ´atedra en Inteligen-\ncia Artificial y MÂ´usicaâ€ (TSI-100929-2023-1).\nImpact Statement\nThis paper presents work whose goal is to advance the fields\nof machine learning and music information retrieval. Musi-\ncal version matching can be used to enhance music discov-\nery, preserve cultural heritage, and support fair copyright\nmanagement. By connecting versions across styles and per-\nformances, musical version matching also fosters creativity,\npromotes artistic appreciation, and paves the way for more\nequitable solutions in the music industry, benefiting society\nat large. As with any machine learning tool, however, there\nalways exists the possibility of some potential misuses of\nitself or of some of its components, none of which we feel\nmust be specifically highlighted here.\nReferences\nAraz, R. O., Serra, X., and Bogdanov, D. Discogs-VI: a\nmusical version identification dataset based on public\neditorial metadata. In Proc. of the Int. Soc. for Music\nInformation Retrieval Conf. (ISMIR), pp. in press, 2024a.\nAraz, R. O., Serr`a, J., Serra, X., Mitsufuji, Y., and Bog-\ndanov, D. Discogs-VINet-MIREX. In Music Information\nRetrieval Evaluation eXchange (MIREX), 2024b.\nBachlechner, T., Majumder, B., Mao, H., Cottrell, G., and\nMcAuley, J. ReZero is all you need: fast convergence\nat large depth. In Proc. of the Conf. on Uncertainty in\nArtificial Intelligence (UAI), pp. 1352â€“1361, 2021.\nBall, P. The music instinct. Random House, London, UK,\n2010.\nBarnett, J., Flores Garcia, H., and Pardo, B. Exploring\nmusical roots: applying audio embeddings to empower\ninfluence attribution for a generative music model. ArXiv,\n2401.14542, 2024.\nBatlle-Roca, R., Liao, W.-H., Serra, X., Mitsufuji, Y., and\nGÂ´omez, E. Towards assessing data replication in music\ngeneration with music similarity metrics on raw audio.\nIn Proc. of the Int. Soc. for Music Information Retrieval\nConf. (ISMIR), pp. in press, 2024.\nBosteels, K. and Kerre, E. E. Fuzzy audio similarity mea-\nsures based on spectrum histograms and fluctuation pat-\nterns. In Proc. of the Int. Conf. on Multimedia and Ubiq-\nuitous Engineering (MUE), pp. 361â€“365, 2007.\nBralios, D., Wichern, G., Germain, F. G., Pan, Z., Khurana,\nS., Hori, C., and Le Roux, J. Generation or replication:\nauscultating audio latent diffusion models. In Proc. of\nthe IEEE Int. Conf. on Acoustics, Speech and Signal\nProcessing (ICASSP), pp. 1156â€“1160, 2024.\nCano, P., Batlle, E., Kalker, T., and Haitsma, J. A review of\naudio fingerprinting. Journal of VLSI Signal Processing\nSystems for Signal, Image and Video Technology, 41:271â€“\n284, 2005.\nChen, T., Kornblith, S., Norouzi, M., and Hinton, G. A\nsimple framework for contrastive learning of visual repre-\nsentations. In Proc. of the Int. Conf. on Machine Learning\n(ICML), pp. 1597â€“1607, 2020.\nCopet, J., Kreuk, F., Gat, I., Remez, T., Kant, D., Synnaeve,\nG., Adi, Y., and DÂ´efossez, A. Simple and controllable\nmusic generation. In Advances in Neural Information\nProcessing Systems (NeurIPS), volume 36, pp. 47704â€“\n47720. 2023.\nDoras, G. and Peeters, G. A prototypical triplet loss for\ncover detection. In Proc. of the IEEE Int. Conf. on Acous-\ntics, Speech and Signal Processing (ICASSP), pp. 3797â€“\n3801, 2020.\nDu, X., Chen, K., Wang, Z., Zhu, B., and Ma, Z.\nByteCover2: towards dimensionality reduction of latent\nembedding for efficient cover song identification. In Proc.\nof the IEEE Int. Conf. on Acoustics, Speech and Signal\nProcessing (ICASSP), pp. 616â€“620, 2022.\nDu, X., Wang, Z., Liang, X., Liang, H., Zhu, B., and Ma, Z.\nByteCover3: accurate cover song identification on short\nqueries. In Proc. of the IEEE Int. Conf. on Acoustics,\nSpeech and Signal Processing (ICASSP), 2023.\nDu, X., Liu, M., and Zou, P. X-Cover: better music ver-\nsion identification system by integrating pretrained ASR\nmodel. In Proc. of the Int. Soc. for Music Information\nRetrieval Conf. (ISMIR), pp. in press, 2024.\nEvans, Z., Carr, C. J., Taylor, J., Hawley, S. H., and Pons, J.\nFast timing-conditioned latent audio diffusion. In Proc.\nof the Int. Conf. on Machine Learning (ICML), volume\n235, pp. 12652â€“12665, 2024.\nHe, K., Zhang, X., Ren, S., and Sun, J. Identity mappings\nin deep residual networks. In Proc. of the European Conf.\non Computer Vision (ECCV), pp. 630â€“645, 2016.\nHu, S., Zhang, B., Lu, J., Jiang, Y., Wang, W., Kong, L.,\nZhao, W., and Jiang, T. WideResNet with joint repre-\nsentation learning and data augmentation for cover song\nidentification. In Proc. of the Conf. of the Int. Speech\nAssociation (INTERSPEECH), pp. 4187â€“4191, 2022.\n9\n\nSupervised Contrastive Learning from Weakly-Labeled Audio Segments for Musical Version Matching\nKalantidis, Y., Sariyildiz, M. B., Pion, N., Weinzaepfel,\nP., and Larlus, D. Hard negative mixing for contrastive\nlearning. In Advances in Neural Information Processing\nSystems (NeurIPS), volume 33, pp. 21798â€“21809. 2020.\nKhosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola,\nP., Maschinot, A., Liu, C., and Krishnan, D. Supervised\ncontrastive learning. In Advances in Neural Information\nProcessing Systems (NeurIPS), pp. 18661â€“18673. 2021.\nKoishekenov, Y., Vadgama, S., Valperga, R., and Bekkers,\nE. J. Geometric contrastive learning. In Proc. of the\nIEEE/CVF Int. Conf. on Computer Vision Workshops\n(ICCVW), pp. 206â€“215, 2023.\nKoromilas, P., Bouritsas, G., Giannakopoulos, T., Nicolaou,\nM., and Panagakis, Y. Bridging mini-batch and asymp-\ntotic analysis in contrastive learning: from InfoNCE to\nkernel-based losses. In Proc. of the Int. Conf. on Machine\nLearning (ICML), pp. 25276â€“25301, 2024.\nLiu, F., Tuo, D., Xu, Y., and Han, X. CoverHunter: cover\nsong identification with refined attention and alignments.\nIn Proc. of the IEEE Int. Conf. on Multimedia and Expo\n(ICME), pp. 1080â€“1085, 2023.\nLiu, H., Tian, Q., Yuan, Y., Liu, X., Mei, X., Kong, Q.,\nWang, Y., Wang, W., Wang, Y., and Plumbley, M. D.\nAudioLDM 2: learning holistic audio generation with\nself-supervised pretraining. IEEE/ACM Trans. on Audio,\nSpeech, and Language Processing, 32:2871â€“2883, 2024.\nMÂ¨uller, H., MÂ¨uller, W., Squire, D. M., Marchand-Maillet,\nS., and Pun, T. Performance evaluation in content-based\nimage retrieval: overview and proposals. Pattern Recog-\nnition Letters, 22(5):593â€“601, 2001.\nOâ€™Hanlon, K., Benetos, E., and Dixon, S. Detecting cover\nsongs with pitch class key-invariant networks. In Proc. of\nthe IEEE Int. Workshop on Machine Learning for Signal\nProcessing (MLSP), 2021.\nPage,\nW.\nMusic\nsmashes\nbox\noffice\nrecords:\nglobal\nvalue\nof\nmusic\ncopyright\nsoars\nto\n$45.5 bn,\nnow\nworth\nmore\nthan\ncinema,\n2023.\nURL\nhttps://pivotaleconomics.com/\nundercurrents/music-copyright-2023.\nPan, X., Luo, P., Shi, J., and Tang, X. Two at once: enhanc-\ning learning and generalization capacities via IBN-Net. In\nProc. of the European Conf. on Computer Vision (ECCV),\npp. 484â€“500, 2018.\nRadenoviÂ´c, F., Tolias, G., and Chum, O. Fine-tuning CNN\nimage retrieval with no human annotation. IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence, 41\n(7):1655â€“1668, 2019.\nSchroff, F., Kalenichenko, D., and Philbin, J. FaceNet: a\nunified embedding for face recognition and clustering. In\nProc. of the IEEE Conf. on Computer Vision and Pattern\nRecognition (CVPR), pp. 815â€“823, 2015.\nSerr`a, J. Identification of versions of the same musical com-\nposition by processing audio descriptions. PhD Thesis,\nUniversitat Pompeu Fabra, 2011.\nSohn, K. Improved deep metric learning with multi-class\nN-pair loss objective. In Advances in Neural Information\nProcessing Systems (NIPS), volume 29, pp. 1857â€“1865.\n2016.\nSun, Y., Wang, X., and Tang, X. Deep learning face repre-\nsentation from predicting 10,000 classes. In Proc. of the\nIEEE Conf. on Computer Vision and Pattern Recognition\n(CVPR), pp. 1891â€“1898, 2014.\nVan den Oord, A., Li, Y., and Vinyals, O. Representa-\ntion learning with contrastive predictive coding. ArXiv,\n1807.03748, 2018.\nWang, T. and Isola, P. Understanding contrastive repre-\nsentation learning through alignment and uniformity on\nthe hypersphere. In Proc. of the Int. Conf. on Machine\nLearning (ICML), volume 119, pp. 9929â€“9939, 2020.\nYeh, C.-H., Hong, C.-Y., Hsu, Y.-C., Liu, T.-L., Chen, Y.,\nand LeCun, Y. Decoupled contrastive learning. In Proc.\nof the European Conf. on Computer Vision (ECCV), pp.\n668â€“684, 2022.\nYesiler, F., Serr`a, J., and GÂ´omez, E. Accurate and scal-\nable version identification using musically-motivated em-\nbeddings.\nIn Proc. of the IEEE Int. Conf. on Acous-\ntics, Speech and Signal Processing (ICASSP), pp. 21â€“25,\n2020a.\nYesiler, F., Serr`a, J., and GÂ´omez, E. Less is more: faster\nand better music version identification with embedding\ndistillation. In Proc. of the Int. Soc. for Music Information\nRetrieval Conf. (ISMIR), pp. 884â€“802, 2020b.\nYesiler, F., Doras, G., Bittner, R. M., Tralie, C. J., and Serr`a,\nJ. Audio-based musical version identification: elements\nand challenges. IEEE Signal Processing Magazine, 38\n(6):115â€“136, 2021.\nYu, Z., Xu, X., Chen, X., and Yang, D. Learning a rep-\nresentation for cover song identification using convolu-\ntional neural network. In Proc. of the IEEE Int. Conf. on\nAcoustics, Speech and Signal Processing (ICASSP), pp.\n541â€“545, 2020.\nZhai, X., Mustafa, B., Kolesnikov, A., and Beyer, L. Sig-\nmoid loss for language image pre-training. In Proc. of\nthe IEEE/CVF Int. Conf. on Computer Vision (ICCV), pp.\n11975â€“11986, 2023.\n10\n\nSupervised Contrastive Learning from Weakly-Labeled Audio Segments for Musical Version Matching\nAPPENDIX\nIn this supplementary part of the paper we provide further information on the proposed method (Appendix A). We also\nexplain with detail our evaluation methodology (Appendix B). Finally, we show additional results that could not fit in the\nmain manuscript (Appendix C).\nA. Method Details\nA.1. Loss Gradient Visualization\nIn Sec. 3.2 of the main manuscript, we study the effect of hyper-parameters Î³ and Îµ on the gradient of negative pairs âˆ‡âˆ’.\nHere, to further facilitate understanding, we plot the result of âˆ‡âˆ’(Eq. 4) for a range of potentials eâˆ’Î³d2\nij under different\nvalues of Î³ and Îµ in Fig. 4. We do so using |Aâˆ’| = 128 and c = (|Aâˆ’| âˆ’1)eâˆ’Î³d2\nij. With the latter, we approximate the case\nwhere the average negative potential is not far from the potential of the i, j pair.\n10âˆ’12\n10âˆ’9\n10âˆ’6\n10âˆ’3\n100\neâˆ’Î³d2\nij\nâˆ’0.15\nâˆ’0.10\nâˆ’0.05\n0.00\nâˆ‡âˆ’\nÎ³ = 2\n10âˆ’12\n10âˆ’9\n10âˆ’6\n10âˆ’3\n100\neâˆ’Î³d2\nij\nâˆ’0.15\nâˆ’0.10\nâˆ’0.05\n0.00\nÎ³ = 5\n10âˆ’12\n10âˆ’9\n10âˆ’6\n10âˆ’3\n100\neâˆ’Î³d2\nij\nâˆ’0.15\nâˆ’0.10\nâˆ’0.05\n0.00\nÎ³ = 10\nFigure 4. Plot of âˆ‡âˆ’as a function of the negative pair potential eâˆ’Î³d2\nij for different values of Î³ and Îµ. From left to right, we show\nÎ³ = {2, 5, 10}. From darker to lighter, colors correspond to Îµ = {10âˆ’8, 10âˆ’7, 10âˆ’6, 10âˆ’5, 10âˆ’4, 10âˆ’3}. Dash-dotted lines indicate\neach Îµ value (notice that, in L, Îµ is compared to an average negative pair potential, hence placing Îµ as a reference in the potential axis\nmakes sense).\nA.2. A More Numerically-Friendly Version of L\nFor conducting all our experiments, we found no issue in the use of L with regard to numerical stability with 32-bit precision.\nHowever, we should note that L, as written in the main manuscript, may have some numerical instability, especially when\nemploying abnormally small/large values of Îµ/Î³, or potentially when using a numerical precision below 32 bits. In such\ncases, we recommend switching to the formulation below.\nFirst of all, we multiply the terms inside the logarithm by 1/Îµ:\nL =\n1\n|A+|\nX\n(i,j)âˆˆA+\nd2\nij + log\nï£«\nï£­Îµ +\n1\n|Aâˆ’|\nX\n(i,j)âˆˆAâˆ’\neâˆ’Î³d2\nij\nï£¶\nï£¸=\n1\n|A+|\nX\n(i,j)âˆˆA+\nd2\nij + log\nï£«\nï£­1 +\n1\nÎµ|Aâˆ’|\nX\n(i,j)âˆˆAâˆ’\neâˆ’Î³d2\nij\nï£¶\nï£¸+ log(Îµ).\nWith this, we obtain the term log(Îµ), which is just a constant that does not affect the gradient and can thus be dropped. Next,\nwe perform the change of variable 1/(Îµ|Aâˆ’|) = Î²eb, where b â‰¥0 is a constant we will set for the upper numerical limit we\nallow to the exponential. With this change and a few simple operations, we arrive to\nL =\n1\n|A+|\nX\n(i,j)âˆˆA+\nd2\nij + log\nï£«\nï£­1 + Î²\nX\n(i,j)âˆˆAâˆ’\nebâˆ’Î³d2\nij\nï£¶\nï£¸,\nwhere Î² = 1/(Îµ|Aâˆ’|eb). We can now choose b as a compromise between the overflow of eb when dij = 0 and the underflow\nof ebâˆ’Î³d2\nij when dij is large. For normalized Euclidean distances d and the ranges of Îµ and Î³ we consider, we choose b = 10.\nNote that, in addition, log(1 + x) can be implemented with log1p(x) in most scientific programming languages.\n11\n\nSupervised Contrastive Learning from Weakly-Labeled Audio Segments for Musical Version Matching\nA.3. Model\nWe here provide further specification of our network architecture (for full detail we refer the interested reader to the published\ncode). As mentioned, we use 16 kHz mono audio with a maximum length of 10 min for both training and evaluation. For\ntraining, we cut 2.5 min blocks uniformly at random. For CLEWS, we divide such blocks into 8 non-overlapping 20-second\nsegments. As the last segment is only 10 s, we take the opportunity to repeat-pad such segment and also consider it in our\ntraining, with the hope that this will facilitate retrieval with query lengths shorter than 20 s (which we also repeat-pad as they\nwould not be long enough to accommodate the total striding factor of our architecture).\nAfter obtaining segments, we apply a constant-Q transform (CQT) with 20 ms hop size, spanning 7 octaves (from a minimum\nfrequency of 32.7 Hz), and with 12 bins per octave (we use the nnAudio library7 in non-trainable mode, with the rest of\nthe parameters set as default). We then take the CQT magnitude and average in time every 5 consecutive frames without\noverlap. This CQT representation is sent to three data augmentation functions (explained in the next subsection).\nThe neural network architecture starts by taking the square root of the CQT magnitude, normalizing every segmentâ€™s\nrepresentation between 0 and 1, and applying a learnable affine transformation. Next, we apply a 128-channel 2D\nconvolution with a frequency-time kernel size of 12Ã—3 and a frequency-time stride of (1,2). This is followed by batch\nnormalization (BN), a ReLU activation, and a 256-channel 2D convolution with a kernel size of 12Ã—3 and a stride of (2,2).\nThis constitutes our frontend. Unless stated otherwise, we use the default PyTorch8 parameters from version 2.3.1.\nAs mentioned in the main manuscript, our backbone is formed by pre-activation ResNet modules with ReZero and instance-\nbatch normalization (IBN). We use 3, 4, 6, and 3 residual blocks with 256, 512, 1024, and 2048 channels, respectively. The\nstrides are (1,1), (2,2), (2,2), and (1,1) for each block. The residual blocks have an IBNâ€“ReLUâ€“convâ€“BNâ€“ReLUâ€“conv\nstructure, with a kernel of 3Ã—3 in the convolution layers. To reduce GPU memory consumption, we employ half the channel\ndimension inside the residual block. If there is some channel or stride change, the skip connection features a BN-ReLU-conv\nblock also with a 3Ã—3 kernel.\nThe output of the backbone is time- and frequency-pooled by a generalized mean pooling operation with a single learnable\nexponent. Finally, the result is processed with BN and projected to 1024 dimensions by a linear layer. None of our linear\nor convolutional layers feature bias terms. As mentioned in the main manuscript, we use normalized squared Euclidean\ndistances (mean squared differences) between embedding vectors.\nA.4. Training\nWe train all models with Adam using the default PyTorch parameters, a learning rate of 2Â·10âˆ’4, and a batch size of\n800 segments chosen from 100 tracks featuring 3 positives per anchor. In the main experiments, we follow a reduce-on-\nplateau strategy for the learning rate, monitoring an average between MAP and NAR measures on the validation set. We\ndefine an epoch as using all training tracks as anchor once, and set a 10-epoch patience period and an annealing factor of 0.2.\nIn the ablation experiments, to reduce the computational burden, we only train for 20 epochs and, during the last 5 epochs,\nwe apply a polynomial learning rate annealing with an exponent of 2.\nDuring training, we employ three CQT data augmentation functions: SpecAugment, time stretch, and pitch roll. For\nSpecAugment, we mask a maximum of 15% of the time/frequency tiles. For time stretch, we resample by a uniformly\nsampled factor between 0.6 and 1.8. For pitch roll, we choose a uniform value between âˆ’12 and +12. In the DVI data set,\nwe use a probability of 0.1 independently for each augmentation. However, since the SHS data set is considerably smaller\nthan DVI and potentially features less variability, we find some benefit in increasing such probability for SHS. In that case,\nwe set the augmentation probabilities to 0.4, 0.3, and 0.5 for SpecAugment, time stretch, and pitch roll, respectively.\nB. Evaluation Methodology Details\nB.1. Track- and Segment-level Evaluations\nFor the track-level evaluation, we cut the entire raw waveform (up to the first 10 min) into overlapping blocks or segments\nusing a hop size of 5 s. For both the queries and the candidates, the length of such blocks/segments corresponds to the same\nlength we used to train each model (that is, 2.5 min for CQTNet, DVINet+, and ByteCover1/2â€  and 20 s for CoverHunter,\n7https://github.com/KinWaiCheuk/nnAudio\n8https://pytorch.org/docs/2.3/\n12\n\nSupervised Contrastive Learning from Weakly-Labeled Audio Segments for Musical Version Matching\nByteCover3â€ , and CLEWS). Next, we compute pairwise distances for each query-candidate block/segment and apply a\ndistance reduction function. We use Rmeanmin for all models except CLEWS, which exploits the newly proposed Rbpwr-10.\nAfter reduction we obtain a track-based distance matrix that we can use to sort candidates per query and compute common\nevaluation measures.\nFor the segment-level evaluation, we also cut the entire raw waveform into overlapping blocks/segments with a hop\nsize of 5 s. For candidates, we also use the same length that we used to train each model (same as in the track-level\nevaluation). However, for the queries, we extract multiple-length segments with a hop size of 5 s (we consider segment\nlengths Ï„ = {5, 10, 20, 30, 40, 60, 90} s). Then, given a segment length, we compute pairwise distances for each query-\ncandidate block/segment, and apply the Rmin distance reduction to obtain a track-based distance matrix. After that, the\nevaluation proceeds as with the track-level evaluation (and any common evaluation protocol in musical version matching).\nThe usage of Rmin puts the focus on the best-matching segment per track, and is equivalent to performing version matching\non an index formed by all possible segments, treating them independently, and removing duplicate track names after sorting.\nB.2. Normalized Average Rank\nTo evaluate retrieval performance and complement mean average precision (MAP), we employ an enhanced version of the\nnormalized average rank (NAR), originally proposed by MÂ¨uller et al. (2001). Given a list of retrieved items R, sorted in\ndescending order of predicted relevance to a query q, and containing a set of target matches M = {m1, . . . m|M|}, M âŠ‚R,\nBosteels & Kerre (2007) redefined NAR as\n]\nNARq =\n1\n|M||R|\n|M|\nX\ni=1\n\u0010\nrank(mi, R) âˆ’i\n\u0011\n,\nwhere the function rank(m, R) âˆˆ[1, |R|] returns the rank of m in R. This definition, as well as the one of MÂ¨uller et al.\n(2001), yields 0 for perfect retrieval, 0.5 for random retrieval, and approaches 1 as performance worsens. However, a value\nequal to one is never obtained. Not only that, but the maximum bound inversely depends on |M| and, therefore, can be\ndifferent for each query q. To avoid that, one should replace the number of retrieved items |R| in the denominator by the\nnumber of non-relevant retrieved items |R| âˆ’|M|. Hence, we correct the definition of Bosteels & Kerre (2007) and employ\nNARq =\n100\n|M| (|R| âˆ’|M|)\n|M|\nX\ni=1\n\u0010\nrank(mi, R) âˆ’i\n\u0011\n,\nwhich additionally yields a convenient % value, now between 0 and 100 for all sizes of M. Our final number is the average\nover all queries Q:\nNAR =\n1\n|Q|\nX\nqâˆˆQ\nNARq.\nNote that, in research evaluation scenarios, one must compute both MAP and NAR measures excluding the query from the\ncandidate list.\nC. Additional Results\nC.1. Segment-level Evaluation with the Best Match Protocol\nIn the main manuscript, we present the results for the segment-level evaluation on DVI-Test (Fig. 2). The exact numbers for\nsuch plots can be found here in Table 5. For SHS-Test, we obtain comparable results, which can be found below in Fig. 5\nand Table 6.\nC.2. Segment-level Evaluation with the Random Segment Protocol\nIn our segment-level evaluation, we adopt a best match protocol as specified in Sec. 4. However, Du et al. (2023) introduced\nwhat could be termed as the â€˜random segmentâ€™ protocol: â€œFor each query, we constructed a query set consisting of the\noriginal full-track recording, and 9 music clips randomly cut from it, with the duration being 6, 10, 15, 20, 25, 30, 40, 50 and\n60 seconds respectivelyâ€ (Du et al., 2023). Apart from lacking further specification, we claim that using random segments\nbiases the evaluation, as we can never reach a perfect accuracy (a random segment from a song does not necessarily need to\n13\n\nSupervised Contrastive Learning from Weakly-Labeled Audio Segments for Musical Version Matching\nhave a match in a version song). Furthermore, if the objective is to match tracks by their segments, we believe using random\nsegments for evaluation may implicitly favor approaches exploiting more generic or global track characteristics than the\nspecific matching-segment information. These are the reasons why we introduce our segment-based protocol. However, in\nthe spirit of comparing with existing reported values, and to avoid any doubt on the performance of the proposed approach,\nwe replicate such protocol (to our best) and compute again results for all methods considered here. They are shown in\nFig. 6 and Table 7 below, together with the MAP values of ByteCover2, ByteCover3, and Re-MOVE (Yesiler et al., 2020b)\nreported by Du et al. (2023).\nTable 5. Segment-level evaluation with DVI-Test. NAR (top) and MAP (bottom) results for different lengths of query segments Ï„. The Â±\nsymbol marks 95% confidence intervals.\nAPPROACH\nÏ„ [S]\n5\n10\n20\n30\n40\n60\n90\nCOVERHUNTER-COARSE\n14.97 Â± 0.07\n12.01 Â± 0.07\n11.01 Â± 0.07\n10.89 Â± 0.07\n10.88 Â± 0.07\n10.95 Â± 0.07\n11.07 Â± 0.07\nCQTNET\n49.96 Â± 0.10\n48.48 Â± 0.10\n16.35 Â± 0.08\n8.67 Â± 0.07\n7.20 Â± 0.07\n6.65 Â± 0.07\n6.60 Â± 0.07\nDVINET+\n49.80 Â± 0.13\n42.11 Â± 0.12\n10.42 Â± 0.07\n5.23 Â± 0.06\n4.20 Â± 0.06\n3.84 Â± 0.06\n3.76 Â± 0.06\nBYTECOVER1/2 â€ \n29.91 Â± 0.10\n13.50 Â± 0.08\n6.77 Â± 0.06\n5.75 Â± 0.06\n5.42 Â± 0.06\n5.19 Â± 0.06\n5.09 Â± 0.06\nBYTECOVER3 â€ \n30.11 Â± 0.10\n18.45 Â± 0.08\n8.66 Â± 0.06\n6.62 Â± 0.06\n6.18 Â± 0.06\n6.42 Â± 0.06\n7.06 Â± 0.06\nCLEWS (OURS)\n5.10 Â± 0.06\n3.02 Â± 0.05\n2.86 Â± 0.05\n2.84 Â± 0.05\n2.85 Â± 0.05\n2.87 Â± 0.05\n2.89 Â± 0.05\nCOVERHUNTER-COARSE\n0.060 Â± 0.001\n0.106 Â± 0.001\n0.132 Â± 0.001\n0.133 Â± 0.001\n0.132 Â± 0.001\n0.129 Â± 0.001\n0.127 Â± 0.001\nCQTNET\n0.001 Â± 0.000\n0.001 Â± 0.000\n0.011 Â± 0.000\n0.078 Â± 0.001\n0.282 Â± 0.002\n0.426 Â± 0.002\n0.475 Â± 0.002\nDVINET+\n0.001 Â± 0.000\n0.002 Â± 0.000\n0.026 Â± 0.000\n0.171 Â± 0.001\n0.421 Â± 0.002\n0.561 Â± 0.002\n0.616 Â± 0.002\nBYTECOVER1/2 â€ \n0.008 Â± 0.000\n0.083 Â± 0.001\n0.363 Â± 0.002\n0.483 Â± 0.002\n0.529 Â± 0.002\n0.564 Â± 0.002\n0.582 Â± 0.002\nBYTECOVER3 â€ \n0.001 Â± 0.000\n0.062 Â± 0.001\n0.358 Â± 0.002\n0.452 Â± 0.002\n0.508 Â± 0.002\n0.503 Â± 0.002\n0.473 Â± 0.002\nCLEWS (OURS)\n0.271 Â± 0.002\n0.670 Â± 0.002\n0.754 Â± 0.002\n0.756 Â± 0.002\n0.755 Â± 0.002\n0.747 Â± 0.002\n0.738 Â± 0.002\n5 10\n20\n30\n40\n60\n90\nÏ„ [s]\n1\n2\n5\n10\n20\n50\nNAR â†“\n5 10\n20\n30\n40\n60\n90\nÏ„ [s]\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMAP â†‘\nCoverHunter-Coarse\nCQTNet\nDVINet+\nByteCover1/2 â€ \nByteCover3 â€ \nCLEWS (proposed)\nFigure 5. Segment-level evaluation with SHS-Test. NAR (left) and MAP (right) for different lengths of query segments Ï„ (notice the\nlogarithmic axis for NAR). The shaded regions correspond to 95% confidence intervals.\n14\n\nSupervised Contrastive Learning from Weakly-Labeled Audio Segments for Musical Version Matching\nTable 6. Segment-level evaluation with SHS-Test. NAR (top) and MAP (bottom) results for different lengths of query segments Ï„. The Â±\nsymbol marks 95% confidence intervals.\nAPPROACH\nÏ„ [S]\n5\n10\n20\n30\n40\n60\n90\nCOVERHUNTER-COARSE\n10.78 Â± 0.23\n6.61 Â± 0.20\n4.90 Â± 0.18\n4.62 Â± 0.18\n4.54 Â± 0.18\n4.52 Â± 0.18\n4.56 Â± 0.18\nCQTNET\n49.80 Â± 0.48\n44.98 Â± 0.47\n9.45 Â± 0.25\n4.32 Â± 0.18\n3.21 Â± 0.16\n2.74 Â± 0.16\n2.67 Â± 0.16\nDVINET+\n49.44 Â± 0.52\n43.12 Â± 0.52\n16.05 Â± 0.36\n5.45 Â± 0.22\n3.29 Â± 0.18\n2.62 Â± 0.17\n2.52 Â± 0.17\nBYTECOVER1/2 â€ \n22.68 Â± 0.42\n6.53 Â± 0.22\n2.71 Â± 0.16\n2.28 Â± 0.15\n2.12 Â± 0.15\n2.07 Â± 0.15\n2.03 Â± 0.14\nBYTECOVER3 â€ \n15.78 Â± 0.32\n10.73 Â± 0.20\n3.84 Â± 0.16\n2.97 Â± 0.15\n2.71 Â± 0.15\n2.68 Â± 0.15\n4.53 Â± 0.19\nCLEWS (OURS)\n3.39 Â± 0.17\n1.49 Â± 0.13\n1.33 Â± 0.12\n1.35 Â± 0.12\n1.37 Â± 0.12\n1.42 Â± 0.12\n1.47 Â± 0.13\nCOVERHUNTER-COARSE\n0.099 Â± 0.004\n0.274 Â± 0.007\n0.414 Â± 0.007\n0.435 Â± 0.007\n0.440 Â± 0.007\n0.439 Â± 0.007\n0.435 Â± 0.007\nCQTNET\n0.003 Â± 0.000\n0.003 Â± 0.000\n0.038 Â± 0.001\n0.095 Â± 0.003\n0.361 Â± 0.007\n0.603 Â± 0.007\n0.652 Â± 0.007\nDVINET+\n0.003 Â± 0.000\n0.005 Â± 0.000\n0.028 Â± 0.001\n0.093 Â± 0.003\n0.365 Â± 0.007\n0.630 Â± 0.007\n0.691 Â± 0.007\nBYTECOVER1/2 â€ \n0.033 Â± 0.002\n0.250 Â± 0.006\n0.640 Â± 0.007\n0.738 Â± 0.007\n0.770 Â± 0.006\n0.790 Â± 0.006\n0.800 Â± 0.006\nBYTECOVER3 â€ \n0.098 Â± 0.004\n0.237 Â± 0.007\n0.628 Â± 0.008\n0.687 Â± 0.008\n0.690 Â± 0.007\n0.674 Â± 0.007\n0.576 Â± 0.008\nCLEWS (OURS)\n0.394 Â± 0.007\n0.806 Â± 0.006\n0.859 Â± 0.005\n0.861 Â± 0.005\n0.859 Â± 0.005\n0.852 Â± 0.006\n0.847 Â± 0.006\n5 10\n20\n30\n40\n60\n90\nÏ„ [s]\n1\n2\n5\n10\n20\n50\nNAR â†“\n5 10\n20\n30\n40\n60\n90\nÏ„ [s]\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMAP â†‘\nCoverHunter-Coarse\nCQTNet\nDVINet+\nByteCover1/2 â€ \nByteCover3 â€ \nRe-MOVE\nByteCover3\nByteCover2\nCLEWS (proposed)\nFigure 6. Segment-level evaluation with SHS-Test using the random segment protocol of Du et al. (2023). NAR (left) and MAP (right) for\ndifferent lengths of random query segments Ï„ (notice the logarithmic axis for NAR). The shaded regions correspond to 95% confidence\nintervals. The dotted lines correspond to values reported by Du et al. (2023).\nTable 7. Segment-level evaluation with SHS-Test using the random segment protocol of Du et al. (2023). NAR (top) and MAP (bottom)\nresults for different lengths of random query segments Ï„. The Â± symbol marks 95% confidence intervals. Du et al. (2023) did not report\nany confidence interval.\nAPPROACH\nÏ„ [S]\n5\n10\n20\n30\n40\n60\n90\nCOVERHUNTER-COARSE\n15.15 Â± 0.30\n9.97 Â± 0.25\n7.42 Â± 0.22\n6.56 Â± 0.21\n6.03 Â± 0.21\n5.62 Â± 0.20\n5.27 Â± 0.19\nCQTNET\n49.73 Â± 0.48\n46.67 Â± 0.47\n18.55 Â± 0.32\n10.21 Â± 0.25\n7.18 Â± 0.23\n4.88 Â± 0.20\n3.78 Â± 0.18\nDVINET+\n49.50 Â± 0.52\n44.13 Â± 0.51\n22.65 Â± 0.38\n11.41 Â± 0.28\n7.09 Â± 0.24\n4.57 Â± 0.20\n3.50 Â± 0.19\nBYTECOVER1/2 â€ \n31.96 Â± 0.40\n19.16 Â± 0.34\n9.65 Â± 0.26\n6.55 Â± 0.22\n5.04 Â± 0.21\n3.77 Â± 0.19\n3.02 Â± 0.17\nBYTECOVER3 â€ \n23.41 Â± 0.37\n10.95 Â± 0.28\n6.64 Â± 0.23\n5.23 Â± 0.21\n4.68 Â± 0.20\n4.03 Â± 0.19\n5.52 Â± 0.12\nCLEWS (OURS)\n15.27 Â± 0.30\n8.09 Â± 0.25\n4.46 Â± 0.19\n3.30 Â± 0.17\n2.81 Â± 0.16\n2.32 Â± 0.15\n2.07 Â± 0.15\nCOVERHUNTER-COARSE\n0.068 Â± 0.003\n0.193 Â± 0.005\n0.314 Â± 0.006\n0.354 Â± 0.007\n0.370 Â± 0.007\n0.385 Â± 0.007\n0.399 Â± 0.007\nCQTNET\n0.003 Â± 0.000\n0.003 Â± 0.000\n0.019 Â± 0.001\n0.088 Â± 0.003\n0.276 Â± 0.006\n0.474 Â± 0.007\n0.586 Â± 0.007\nDVINET+\n0.003 Â± 0.000\n0.004 Â± 0.000\n0.016 Â± 0.001\n0.089 Â± 0.003\n0.276 Â± 0.006\n0.498 Â± 0.007\n0.623 Â± 0.007\nBYTECOVER1/2 â€ \n0.022 Â± 0.001\n0.110 Â± 0.004\n0.357 Â± 0.006\n0.516 Â± 0.007\n0.607 Â± 0.007\n0.691 Â± 0.007\n0.746 Â± 0.007\nBYTECOVER3 â€ \n0.044 Â± 0.002\n0.133 Â± 0.004\n0.432 Â± 0.007\n0.511 Â± 0.007\n0.528 Â± 0.007\n0.539 Â± 0.007\n0.469 Â± 0.008\nRE-MOVE\n0.023\n0.069\n0.196\n0.308\n0.407\n0.505\nN/A\nBYTECOVER3\n0.084\n0.257\n0.496\n0.600\n0.666\n0.732\nN/A\nBYTECOVER2\n0.016\n0.074\n0.282\n0.442\n0.564\n0.684\nN/A\nCLEWS (OURS)\n0.140 Â± 0.004\n0.455 Â± 0.006\n0.652 Â± 0.007\n0.714 Â± 0.006\n0.746 Â± 0.006\n0.780 Â± 0.006\n0.802 Â± 0.006\n15\n",
  "metadata": {
    "source_path": "papers/arxiv/Supervised_contrastive_learning_from_weakly-labeled_audio_segments_for\n__musical_version_matching_a85f1cf979196c1c.pdf",
    "content_hash": "a85f1cf979196c1c1bbdd0f18ccb6d0e91d72b4521b41885cfee1a40f9579b71",
    "arxiv_id": null,
    "title": "Supervised Contrastive Learning from Weakly-Labeled Audio Segments for Musical Version Matching",
    "author": "Joan SerrÃ , R. Oguz Araz, Dmitry Bogdanov, Yuki Mitsufuji",
    "creation_date": "D:20250225023126Z",
    "published": "2025-02-25T02:31:26",
    "pages": 15,
    "size": 874142,
    "file_mtime": 1740470204.0243795
  }
}