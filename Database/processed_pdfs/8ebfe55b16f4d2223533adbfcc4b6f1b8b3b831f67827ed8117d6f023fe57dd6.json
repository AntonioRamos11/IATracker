{
  "text": "Mutual Reinforcement of LLM Dialogue Synthesis and Summarization\nCapabilities for Few-Shot Dialogue Summarization\nYen-Ju Lu†* , Ting-Yao Hu∗, Hema Swetha Koppula, Hadi Pouransari,\nJen-Hao Rick Chang, Yin Xia, Xiang Kong, Qi Zhu,\nSimon Wang, Oncel Tuzel, Raviteja Vemulapalli\n†Johns Hopkins University\nApple\nAbstract\nIn this work, we propose Mutual Reinforcing\nData Synthesis (MRDS) within LLMs to im-\nprove few-shot dialogue summarization task.\nUnlike prior methods that require external\nknowledge, we mutually reinforce the LLM’s\ndialogue synthesis and summarization capa-\nbilities, allowing them to complement each\nother during training and enhance overall per-\nformances. The dialogue synthesis capability\nis enhanced by directed preference optimiza-\ntion with preference scoring from summariza-\ntion capability. The summarization capabil-\nity is enhanced by the additional high quality\ndialogue-summary paired data produced by the\ndialogue synthesis capability. By leveraging\nthe proposed MRDS mechanism, we elicit the\ninternal knowledge of LLM in the format of\nsynthetic data, and use it to augment the few-\nshot real training dataset. Empirical results\ndemonstrate that our method improves dialogue\nsummarization, achieving a 1.5% increase in\nROUGE scores and a 0.3% improvement in\nBERT scores in few-shot settings.\nFurther-\nmore, our method attains the highest average\nscores in human evaluations, surpassing both\nthe pre-trained models and the baselines fine-\ntuned solely for summarization tasks.\n1\nIntroduction\nDialogue summarization focuses on producing con-\ncise and coherent summaries of conversations in\nvarious domains such as customer service (Feigen-\nblat et al., 2021; Zhao et al., 2021), medical consul-\ntations (Chintagunta et al., 2021; Jain et al., 2022;\nZeng et al., 2020), and casual interactions (Chen\net al., 2021; Gliwa et al., 2019). While state-of-the-\nart large language models (LLMs) such as Llama3\n(Dubey et al., 2024) have been shown to work well\nfor a wide variety of natural language processing\ntasks, their dialogue summarization performance is\n*Research conducted during internship at Apple. Corre-\nsponding authors: ylu125@jhu.edu, tingyao_hu@apple.com\nDialogue Synthesis\nSummarization\nPreference scoring for DPO\nData synthesis for SFT\nLLM\nFigure 1: Mutual Reinforcement via Data Synthesis -\nWe first leverage the summarization capability of the\npretrained LLM for scoring dialogue preference pairs\nthat are used to improve the dialogue synthesis capa-\nbility with DPO, and then use the improved synthesis\ncapability to generate SFT data for improving the sum-\nmarization capability.\nunsatisfactory in many target domains of interest.\nThis could be because, these models, despite being\ntrained on massive datasets, may have seen limited\ndata related to dialogue summarization in specific\ntarget domains.\nCollecting a large-scale, real-world dialogue\ndataset and manually annotating it with concise\nsummaries is not only time-consuming and expen-\nsive, but it also raises privacy concerns in many\ntarget domains, as conversational data is often sen-\nsitive in nature. Consequently, there is a growing\ninterest in developing few-shot learning approaches\nthat improve the dialogue summarization capabili-\nties of pretrained LLMs using limited real dialogue-\nsummary pairs from the target domain.\nExisting works that address data scarcity for dia-\nlogue summarization assume access to either addi-\ntional dialogue-summary pairs from other domains\n(Li et al., 2023; Park et al., 2024; Yu et al., 2021;\nZou et al., 2021; Zhong et al., 2022) or additional\nunlabeled dialogues from the target domain (Chen\nand Yang, 2021; He et al., 2024) or teacher models\nthat are larger and more powerful than the target\nmodels (Ouyang et al., 2023; Pham et al., 2023).\nWhile these approaches have shown some perfor-\narXiv:2502.17328v1  [cs.CL]  24 Feb 2025\n\nmance improvements, they need access to external\nresources that may not be available in all scenarios.\nDifferent from these works, we focus on improving\nthe dialogue summarization capability of a pre-\ntrained LLM using limited real dialogue-summary\npairs from the target domain without relying on any\nadditional data sources or external models.\nLLMs possess a vast amount of implicit knowl-\nedge acquired during pre-training on large-scale\ntext corpora, enabling them to generate contex-\ntually relevant text. Motivated by this, we intro-\nduce the framework of Mutual Reinforcement via\nData Synthesis (MRDS) that harnesses the internal\nknowledge embedded within these models and their\ninherent data synthesis capability to address the\ndata scarcity problem in dialogue summarization.\nSpecifically, given a pretrained LLM, our method\nincorporates dialogue synthesis and summarization\ninto a mutually reinforcing cycle to enhance both\ncapabilities simultaneously (see Fig. 1).\nTo improve the dialogue synthesis capability, we\ncreate a dataset of synthetic dialogue preference\npairs scored by leveraging the summarization ca-\npability of the pretrained LLM, and train a LoRA\n(Hu et al., 2022) adapter for dialogue synthesis us-\ning direct preference optimization (DPO) with the\nsynthetic dialogue preference pairs in addition to\nsupervised finetuning (SFT) with the limited real\ndialogue-summary pairs. This ensures that the gen-\nerated dialogues are coherent and closely aligned\nwith their corresponding summaries. To improve\nthe summarization capability, we utilize the trained\ndialogue synthesis adapter to generate synthetic\ndialogue-summary pairs, and train a LoRA adapter\nfor summarization by performing SFT with lim-\nited real data and generated synthetic data. Effec-\ntively, the initial summarization capability of the\npretrained LLM helps improve the dialogue synthe-\nsis capability which in turn helps further improve\nthe summarization capability.\nMajor contributions:\n• Mutual reinforcement framework: We in-\ntroduce a framework that incorporates dia-\nlogue synthesis and summarization capabil-\nities of an LLM into a mutually reinforcing\ncycle, and enhances both capabilities using\nlimited target domain data.\n• DPO-Enhanced dialogue synthesis: We pro-\npose to use direct preference optimization to\nimprove the dialogue synthesis capability of a\npretrained LLM, ensuring that the generated\ndialogues are correctly formatted, coherent,\nand closely aligned with their summaries.\n• Experimental validation: We demonstrate\nconsistent improvements over various alter-\nnative approaches in terms of BERT and\nROUGE scores on two widely-used bench-\nmark datasets, namely SAMSum and Dialog-\nSum. The proposed approach also achieves\nthe best average score in human evaluations.\nWe also present several ablation results that\ndemonstrate the effectiveness of individual\ncomponents of the proposed approach.\n2\nRelated Work\nLow-resource/Few-shot Dialogue Summariza-\ntion.\nMultiple lines of methods have been pro-\nposed to address the data sparsity problem of di-\nalogue summarization. Yu et al. (2021) and Zou\net al. (2021) employ datasets from multiple source\ndomains to conduct pre-training. He et al. (2024)\nrely on semi-supervised learning techniques such\nas pseudo labeling to incorporate the additional\ndialogue data without summary annotation. Xie\net al. (2024) and Zhao et al. (2022) design sophis-\nticated prompt tuning strategies, enabling cross-\ntask knowledge transfer. Recently, several meth-\nods have leveraged synthetic data generation us-\ning external knowledge or unlabeled datasets. For\ninstance, GENDEX (Park et al., 2024) generates\nsynthetic dialogues by utilizing external knowledge\nbases, thereby enriching the training data and im-\nproving model performance. Similarly, composi-\ntional data augmentation proposed by Ouyang et al.\n(2023) creates new training samples by recombin-\ning existing data, enhancing diversity without addi-\ntional manual annotations. Additionally, Tian et al.\n(2024) employed a mixture-of-experts framework\nthat integrates external knowledge to enhance sum-\nmarization capabilities. While these approaches\nhave demonstrated performance gains, they often\ndepend on domain-specific resources, large unla-\nbeled datasets, or external stronger models, which\nmay not be available or practical in few-shot set-\ntings. This reliance on external data limits their\napplicability in scenarios where access to such re-\nsources is constrained.\nSynthetic Data from LLMs\nMany previous\nworks have shown that LLMs are capable of syn-\nthesizing high quality training data for machine\n\nlearning models. One line of methods primarily\nfocus on zero-shot learning scenario (Ye et al.,\n2022a,b; Gao et al., 2023; Meng et al., 2022; Gupta\net al., 2024), where they sample data from LLMs\nbased on task related prompts, and use the syn-\nthetic data to train small, task-specific models from\nscratch. Other works also demonstrate the effec-\ntiveness of synthetic data from LLM in different do-\nmains such as speech recognition (Su et al., 2024),\ninformation extraction (Bonifacio et al., 2022; Josi-\nfoski et al., 2023), text-to-SQL (Yang et al., 2024),\nand dialogue state tracking (Kulkarni et al., 2024;\nMehri et al., 2022). Recently, several works also\ninvestigate the idea of LLM self-improvement, sug-\ngesting that the synthetic data from LLMs can\nimprove their own instruction following abilities.\nSelf-instruct (Wang et al., 2023) samples from an\nLLM to create a synthetic prompt-response paired\ndataset, which can be used to finetune the orig-\ninal LLM. Li et al. (2024) introduce instruction\nbacktranslation, which obtains synthetic instruc-\ntion prompts from back-translating a web scale\ncorpus with the same LLM. Gulcehre et al. (2023),\nYuan et al. (2024), and Chen et al. (2024) pay at-\ntention to the generation of responses, but utilize\nthem in different manners. Gulcehre et al. (2023)\nrely on an external scoring function to obtain the re-\nward of synthetic responses. Yuan et al. (2024) pro-\npose a self-rewarding framework, using the LLM\nto score the response generated from itself. Chen\net al. (2024) design a self-play mechanism, finetun-\ning the LLM to distinguish the responses generated\nby the itself and human responses.\n3\nMethodology\nNotations\nLet Dp denote the limited real dataset\nof dialogue-summary pairs, and S denote the set\nof summaries in Dp.\nGiven a pretrained LLM, we characterize its di-\nalogue synthesis and summarization capabilities\nusing two separate LoRA (Hu et al., 2022) adapters.\nFirst, we leverage the summarization capability of\nthe pretrained LLM to score synthetic dialogue\npreference pairs, and use this data to train the dia-\nlogue synthesis adapter using DPO in addition to\nSFT on real data Dp. Then, we leverage the trained\ndialogue synthesis adapter to generate synthetic\ndialogue-summary pairs that are used (in addition\nto real data Dp) to train the summarization adapter.\nEffectively, the synthesis and summarization capa-\nbilities help improve each other without using any\nSummaries\n \nDialogue \nSynthesizer\nSynthetic\ndialogues\nIterative \nsynthesis\nSummarizer\nContent-based\npreference pairs\n \nFormat-based\npreference pairs\nDirect Preference \nOptimization\nPreference\nscoring\nLLM\nFigure 2: Preference pairs for DPO - Given a summary\nset S, the LLM generates synthetic dialogues which are\nevaluated based on content alignment and format cor-\nrectness. Content-based preferences leverage the LLM’s\nown summarization capability to assess how well the\nsynthesized dialogues align with the input summaries.\nFormat-based preferences ensure that the dialogues fol-\nlow proper formatting.\nexternal models or additional real data.\n3.1\nDialogue Synthesizer Training\nThe dialogue synthesizer (pretrained LLM with dia-\nlogue synthesis adapter) takes a summary s as input\nand generates a dialogue ˆd. A straightforward way\nto train this model is to perform SFT with the real\ndialogue-summary pairs Dp. However, when the\namount of such training data is limited, the result-\ning dialogue synthesizer MSFT\ndlg\noften generates\npoor quality dialogues in terms of both dialogue\nformat and content (see Table 8).\nTo encourage the dialogue synthesizer to gener-\nate higher quality dialogues, we construct a syn-\nthetic dataset of preference pairs {s, ˆd1, ˆd2 | s ∈\nS, P( ˆd1) > P( ˆd2)}, where P denotes a dialogue\nquality scoring function, and train the synthesizer\nusing DPO. Specifically, we generate two types of\npreference pairs using two different quality scor-\ning functions, one focusing on the dialogue format\nand the other focusing on the dialogue content (see\nFig. 2). Both the preferred and rejected dialogues in\nthese pairs are synthetic dialogues generated using\nthe SFT-trained dialogue synthesizer MSFT\ndlg .\nFormat-based Preference Pairs\nSince the SFT-\ntrained synthesizer MSFT\ndlg\noften generates dia-\nlogues with several formatting errors, we develop\n\nan iterative dialogue synthesis (IDS) method that\ngenerates correctly-formatted dialogues. First, we\ngenerate a dialogue conditioned on the input sum-\nmary and check it for format errors. We discard the\nportion of the dialogue after the first detected error,\nconcatenate the remaining correctly-formatted par-\ntial dialogue to the initial input prompt and give it\nas input to the dialogue synthesizer to complete the\ndialogue. This process is repeated until we get a\ndialogue without formatting errors. Table 11 shows\nan example run of this IDS process.\nFor a given summary, we generate multiple dia-\nlogues with format errors by directly sampling from\nthe SFT-trained synthesizer MSFT\ndlg\nand multiple\nclean dialogues by repeating the above IDS process\nseveral times. We use these samples to form prefer-\nence pairs {s, ˆd1, ˆd2 | s ∈S, F( ˆd1) = 1, F( ˆd2) =\n0} , where F denotes the format check-based bi-\nnary scoring function (1 for clean dialogues, 0 for\nthose with errors). When trained with these prefer-\nence pairs, the dialogue synthesizer learns to gen-\nerate dialogues in correct format. Table 9 shows an\nexample of format-based preference pair.\nContent-based Preference Pairs\nA dialogue ˆd\ngenerated by a well-trained dialogue synthesizer\nshould have high content alignment with the sum-\nmary s used to generate ˆd. We leverage the summa-\nrization capability of the pretrained LLM to mea-\nsure this alignment. The main motivation is that if\nwe summarize the synthetic dialogue ˆd, the corre-\nsponding summary s should have high probability\nas the summarization output. Based on this, we\nuse the likelihood Msum(s| ˆd) of the summary s\nconditioned on the dialogue ˆd measured by the pre-\ntrained LLM as the alignment score.\nTo encourage the dialogue synthesizer to gen-\nerate dialogues that have high content alignment\nwith the input summaries, we construct a dataset\nof preference pairs using the content alignment\nscore Msum(s| ˆd).\nSpecifically, for each sum-\nmary s, we first generate multiple clean dialogues\nfrom the SFT-trained dialogue synthesizer MSFT\ndlg\nfollowing the IDS process described above, and\nthen pick the dialogues with the best and least\ncontent alignment scores to form preference pairs\n{s, ˆd1, ˆd2 | s ∈S, Msum(s| ˆd1) > Msum(s| ˆd2)}.\nTable 10 shows an example of content-based pref-\nerence pair.\nInstead of using the pretrained LLM, we also\nexplored training the summarization adapter on the\nlimited real data Dp and using it for alignment scor-\ning to generate DPO training data. However, we\ndid not observe significant improvements in the\nfinal summarization results (after training the dia-\nlogue synthesizer with DPO, generating synthetic\ndialogues, and using them to train the final summa-\nrization adapter).\nTraining with DPO and SFT\nIn addition to\nDPO with synthetic preference pairs, we also use\nSFT with the limited real data Dp to train the dia-\nlogue synthesizer. We accumulate gradients from\nboth losses in each optimization step. While DPO\nwith synthetic data encourages the model to gener-\nate contextually accurate dialogues in correct for-\nmat, it does not explicitly encourage the model\nto generate dialogues that mimic the target distri-\nbution represented by real data Dp. By combin-\ning DPO with SFT on real data, we encourage the\nmodel to generate dialogues closer to the target\ndistribution while being contextually accurate and\nbetter-formatted.\n3.2\nSummarizer Training\nThe summarization model (pretrained LLM with\nsummarization adapter) is trained using SFT with\nlimited real data Dp and additional synthetic\ndialogue-summary pairs.\nSynthetic Summary Generation\nTo generate\nsynthetic summaries that mimic the distribution\nrepresented by the real summary set S, we first ex-\ntract a (2-3 words) topic for each summary s ∈S\nusing the pretrained LLM. Then, for each topic,\nwe generate multiple new synthetic summaries by\nusing a topic-based summary synthesizer. This syn-\nthesizer is obtained by supervised LoRA finetuning\nof the pretrained LLM using the real summaries in\nS and the corresponding extracted topics.\nSynthetic Dialogue Generation\nFor each syn-\nthetic summary, we generate a dialogue by directly\nsampling from the dialogue synthesizer that has\nbeen trained with both DPO and SFT, as explained\nin the previous section. Since this improved synthe-\nsizer already generates dialogues with high qual-\nity, we do not use the costly iterative synthesis\napproach in this step.\nTraining with Synthetic and Real Data\nMod-\nels trained only on limited real data tend to overfit\nquickly. While combining real and synthetic data\nsamples in each minibatch could address this issue\nto some extent, finding the perfect ratio between the\n\ntwo data sources is challenging and highly depen-\ndent on the quality of the synthetic data. Moreover,\nusing a fixed ratio of real and synthetic data sam-\nples throughout the training may not be optimal. If\nthe minibatch is dominated by synthetic data, then\nthe model may inherit the artifacts present in the\nsynthetic data, and if the minibatch is dominated\nby real data, the model may start to overfit before\ntaking full advantage of the synthetic data.\nTo address these issues, we follow a two-stage\ntraining strategy, where we train only on synthetic\ndata in the first stage and only on real data in the\nsecond stage. The synthetic-only first stage allows\nthe model to learn general dialogue summariza-\ntion skills without the risk of quickly overfitting\non limited real data. The real-only second stage\nallows the model to adapt to the distribution of the\nreal data mitigating the artifacts learned from syn-\nthetic data. Following this two-stage approach, we\nmake effective use of both synthetic and real data,\nresulting in a more accurate summarization model.\n4\nExperiments\nDatasets\nWe experiment with two widely-used\nbenchmark datasets, namely SAMSum (Gliwa\net al., 2019) and DialogSum (Chen et al., 2021).\nThe SAMSum dataset contains over 16,000 casual\nconversations mimicking everyday chats among\nfriends and family. The DialogSum dataset in-\ncludes about 13,460 face-to-face spoken dialogues\nbetween friends, colleagues, and between service\nproviders and customers, covering various daily-\nlife topics. These datasets also provide a human-\nwritten summary for each dialogue. Since this\nwork focuses on few-shot settings, for each dataset,\nwe experiment with either 100 or 300 dialogue-\nsummary pairs as the few-shot training dataset Dp.\n4.1\nAlternative Methods\nWe compare our MRDS method with several di-\nalogue summarization alternative approaches, di-\nvided into two categories: methods using the pre-\ntrained Llama3 model without fine-tuning and\nmethods fine-tuned on real or synthetic data.\nPre-trained Methods\n- Zero-shot: Zero-shot summarization perfor-\nmance of Llama3.\n- ICL: Summarization performance of Llama3\nusing in-context learning with k = 7 exam-\nples. 1\n1We experimented with different number of in-context\nFine-tuned Methods\n- Real only: Fine-tuning with real data only.\n- SFT: Two-stage training using synthetic dia-\nlogues generated by the SFT dialogue synthe-\nsizer.\n- SFT + Post-processing: Two-stage training\nusing synthetic dialogues from the SFT dia-\nlogue synthesizer, enhanced with Iterative Di-\nalogue Synthesis (IDS) and content alignment\nfiltering.\n4.2\nImplementation Details\nWe use Llama3-8B-Instruct (Dubey et al., 2024)\nas the pretrained base LLM. We use a rank of 16\nand an alpha of 32 for all LoRA adapters, and keep\nthe base model parameters frozen while training\nthe LoRA adaptors. Table 6 shows all the prompts\nused for topic extraction, topic-based summary syn-\nthesis, summary-based dialogue synthesis, and di-\nalogue summarization tasks. All presented results\nare averaged over three runs.\nDialogue Summarization\nFor the baseline\nmodel trained exclusively on real data, we opti-\nmized the hyperparameters and applied the same\nsettings to all subsequent experiments for consis-\ntency. Our training strategy includes a batch size\nof 10 and a maximum learning rate of 2.0 × 10−4\nwith a warmup over the first 50 batches. We use the\nReduceLROnPlateau scheduler with a patience of\n5 and a reduction factor of 0.7. Training is stopped\nif the loss does not improve for 100 steps. We se-\nlect the best checkpoint based on the validation loss\nobtained during the real data training phase.\nIn synthetic data experiments, we employ a two-\nstage training approach using the same hyperpa-\nrameters. In the first phase, we train exclusively\non synthetic data until the learning rate reduces to\n2.0 × 10−5, effectively serving as a pre-training\nphase. In the second phase, we apply the same\ntraining strategy as in the real-only experiments to\nensure a fair comparison.\nDialogue Synthesis\nFor the dialogue synthesizer\ntrained with SFT only, we use a learning rate of\n2.0 × 10−4 along with the ReduceLROnPlateau\nscheduler. The batch size and other hyperparame-\nters are the same as those used for dialogue summa-\nrization. When training the synthesizer using both\nSFT and DPO, we start from the SFT checkpoint.\nIn this combined training, we use a batch size of\nexamples and k = 7 worked best.\n\nfour for DPO and one for SFT, jointly updating the\ndialogue synthesizer by combining the losses from\nboth objectives. A fixed learning rate of 1 × 10−5\nis used during this phase. We validate the synthe-\nsizer checkpoints on the official validation set of\nthe dataset, evaluating both format correctness and\nsummarization cross-entropy loss. We select the\ncheckpoint with the lowest summarization CE loss,\nensuring at least 85% format correctness. Detailed\ntraining hyperparameters are provided in Table 13.\n5\nResults\n5.1\nDialogue Summarization\nWe conducted experiments on the SAMSum and\nDialogSum datasets to evaluate the effectiveness\nof our proposed mutual reinforcing data synthe-\nsis (MRDS) method. The results are presented\nin Table 1, comparing various summarization ap-\nproaches under 100-shot and 300-shot settings us-\ning metrics such as ROUGE-1 (R-1), ROUGE-2 (R-\n2), ROUGE-L (R-L) (Lin, 2004), and BERTScore\n(Zhang et al., 2020).\nIn the zero-shot setting, the pre-trained LLM\n(Zero shot) achieves R-1 scores of 31.3 on SAM-\nSum and 28.2 on DialogSum. In-context learning\napproach (ICL) improves the R-1 score on SAM-\nSum to 39.5 and DialogSum to 31.4, demonstrating\nefficiency of ICL in low-resource scenarios.\nWhen fine-tuning with 100 real shots, the real-\nonly method significantly improves performance\nover zero-shot methods, achieving R-1 scores of\n50.9 on SAMSum and 44.0 on DialogSum. Incor-\nporating dialogues from the SFT model maintains\nsimilar performance, while post-processing tech-\nniques (SFT + Post-processing) further enhance\nresults, increasing R-1 scores to 51.8 on SAMSum\nand 44.7 on DialogSum, indicating the effective-\nness of IDS and content alignment filtering. In\nthe 300-shot setting, all methods benefit from addi-\ntional training data. The real-only method reaches\nR-1 scores of 51.1 on SAMSum and 45.2 on Di-\nalogSum, with SFT showing incremental gains,\nand SFT + Post-processing achieving R-1 scores\nof 52.7 on SAMSum and 46.1 on DialogSum.\nOur proposed MRDS method outperforms all\napproaches in both 100-shot and 300-shot settings.\nIn the 100-shot setting, MRDS achieves the high-\nest R-1 scores of 52.1 on SAMSum and 45.5 on\nDialogSum, along with improvements in R-2, R-L,\nand BERTScore metrics, highlighting its ability to\nleverage synthesized data effectively. In the 300-\nshot setting, MRDS continues to deliver the best\nperformance, matching the top R-1 score of 52.7 on\nSAMSum and setting a new high of 47.0 on Dialog-\nSum. The method consistently delivers superior\nROUGE and BERTScore values, highlighting its\nrobustness and scalability with increased data. This\nshows that MRDS not only improves efficiency by\neliminating post-processing steps but also signifi-\ncantly boosts summarization performance.\n5.2\nHuman Evaluation\nTable 2 presents the results of human evaluations,\ncomparing four groups of summaries: two from\nthe pretrained instructed Llama3 model—zero-shot\nresults (Zero) and in-context learning (ICL)—and\ntwo from the fine-tuned Llama3-based summariza-\ntion model: trained on real data only (Real) and\ntrained with the proposed MRDS approach, which\ncombines real and synthetic summaries and dia-\nlogues. Five human evaluators assessed the sum-\nmaries based on informativeness, faithfulness, flu-\nency, and redundancy, using a scale from zero to\ntwo, following the evaluation protocol from (Xie\net al., 2024). The average scores across the four\nmetrics are also reported.\nFor the pre-trained models, while the zero-shot\nmodel (Zero) achieves the highest score in infor-\nmativeness (1.95), it scores poorly in redundancy\n(0.83), indicating a tendency to produce overly long\nsummaries by including too much information –\nundesirable in summarization tasks. The in-context\nlearning model (ICL) shows slight improvements in\nfaithfulness (1.69 vs. 1.50) and redundancy (1.23\nvs. 0.83) compared to zero-shot, indicating that\nit generates more concise and faithful summaries\nbut still inherits some limitations of the pre-trained\nmodel. For the fine-tuned models, the Real ap-\nproach outperforms the pre-trained models in over-\nall average score (1.72 vs. 1.56 for Zero and 1.64\nfor ICL), demonstrating the benefit of fine-tuning\nwith real data in improving summary quality. How-\never, our proposed MRDS method achieves the\nhighest average score (1.84), outperforming both\nthe real-only and pre-trained models, particularly\nin faithfulness and redundancy. This suggests that\nincorporating synthesized data helps the model pro-\nduce more precise, concise summaries. The results\nhighlight that our MRDS approach significantly en-\nhances summarization quality from the perspective\nof human evaluators. Example summaries from\ndifferent approaches are shown in Table 7.\n\nTable 1: Comparison of Summarization Methods on 100 and 300 shots.\nApproach\nSAMSum\nDialogSum\nR-1\nR-2\nR-L\nBERTScore\nR-1\nR-2\nR-L\nBERTScore\nZero shot\n31.3\n12.3\n23.9\n81.2\n28.2\n10.0\n21.4\n81.6\nICL (k=7)\n39.5\n17.6\n30.9\n83.2\n31.4\n11.9\n24.5\n83.1\n100 Real shots\nReal only\n50.9\n26.5\n42.6\n86.6\n44.0\n18.2\n36.0\n86.8\nSFT\n50.9\n26.5\n42.6\n86.5\n45.1\n19.0\n36.9\n86.9\nSFT + Post-processing\n51.8\n27.3\n43.5\n86.7\n44.7\n18.8\n36.5\n87.1\nMRDS (ours)\n52.1\n27.5\n43.4\n86.8\n45.5\n19.3\n37.2\n87.2\n300 Real shots\nReal only\n51.1\n26.9\n42.8\n86.5\n45.2\n19.5\n37.3\n87.2\nSFT\n52.1\n27.6\n43.7\n86.8\n45.9\n19.8\n37.7\n87.2\nSFT + Post-processing\n52.7\n28.1\n44.1\n87.0\n46.1\n20.0\n37.6\n87.3\nMRDS (ours)\n52.7\n28.3\n44.4\n87.0\n47.0\n20.4\n38.6\n87.5\nTable 2: Human evaluation results on SAMSum with\n300-shot data, covering informativeness (inf.), faithful-\nness (fai.), fluency (flu.), redundancy (red.), and the\naverage (ave.) of the four scores.\nInf.\nFai.\nFlu.\nRed.\nAve.\nZero\n1.95\n1.50\n1.94\n0.83\n1.56\nICL\n1.65\n1.69\n2.00\n1.23\n1.64\nReal\n1.32\n1.61\n2.00\n1.94\n1.72\nMRDS\n1.55\n1.88\n1.94\n2.00\n1.84\n5.3\nDialogue Synthesis Efficiency\nWe compared the efficiency of the post-processing\napproach—which includes iterative synthesis and\ncontent alignment filtering—with the DPO-based\nMRDS approach (Table 3). In the 100-shot scenar-\nios, although MRDS still required iterative synthe-\nsis due to less consistent format correctness with\nlimited data, it achieved 550 dialogues per hour\non SAMSum compared to 63 dialogues per hour\nwith post-processing—an 8.7-fold improvement.\nIn the 300-shot scenarios, MRDS significantly in-\ncreased throughput, generating 2,900 dialogues per\nhour on SAMSum versus 37.5 dialogues per hour\nwith post-processing—a 77-fold improvement—by\neliminating the need for IDS or content alignment\nfiltering due to enhanced format correctness and\nsummarization alignment.\nTable 3: Comparison of efficiency for dialogue synthesis\nwith (Dialogues/hour).\nApproach\n100 shots\n300 shots\nSAMSum\nPost-Processing\n63\n37.5\nMRDS\n550\n2900\nDialogSum\nPost-Processing\n85\n66.7\nMRDS\n315\n2500\n5.4\nModel Analysis\nSummaries Effectiveness.\nTo evaluate the im-\npact of different summary types on dialogue synthe-\nsis, we experimented with two training strategies:\n(1) a fixed 1:1 ratio of real to synthesized data and\n(2) a two-stage training approach, training first on\nsynthesized data and then switching to real data.\nWe tested these strategies using three types of sum-\nmaries: the same summaries (identical to those\nin the real data), synthetic summaries generated\nfrom our synthesis process in Sec. 3.2, and unseen\nsummaries drawn from additional training data.\nTable 4 presents the results. Under the fixed\nratio strategy, using the same summaries did not\nyield any improvement over the real-only approach,\nlikely due to overfitting to the limited real sum-\nmaries. Synthetic summaries provided some im-\nprovement in the 100-shot setting but introduced\n\nTable 4: Comparison of different summaries with synthesis data on DialogSum 100 and 300 real shots. Synthesis\ndialogues are generated by SFT-trained dialogue synthesizer with IDS and content alignment filtering.\nApproach\n100 Real shots\n300 Real shots\nR-1\nR-2\nR-L\nBERTScore\nR-1\nR-2\nR-L\nBERTScore\nReal only\n44.0\n18.2\n36.0\n86.8\n45.2\n19.5\n37.3\n87.2\nFixed Ratio Training\nSame Summ\n42.8\n17.4\n35.3\n86.4\n44.6\n19.0\n36.7\n86.7\nSynthetic Summ\n44.1\n18.2\n36.1\n86.7\n44.0\n18.2\n35.5\n86.6\nUnseen Summ\n44.5\n18.8\n36.3\n87.0\n46.6\n20.5\n38.1\n87.5\nTwo-Stage Training\nSame Summ\n42.5\n16.8\n34.6\n86.3\n45.8\n19.9\n37.6\n87.2\nSynthetic Summ\n44.7\n18.8\n36.5\n87.1\n46.1\n20.0\n37.6\n87.3\nUnseen Summ\n45.3\n19.7\n37.4\n87.0\n46.8\n20.8\n38.8\n87.6\nartifacts that degraded performance in the 300-shot\nsetting. This suggests that mixing synthetic and\nreal data in a fixed ratio can lead to the model learn-\ning undesirable patterns from the synthetic data as\nthe amount of real data increases.\nIn contrast, the two-stage training approach pro-\nduced better results. Training with synthetic sum-\nmaries first allowed the model to learn general pat-\nterns from the synthesized data before fine-tuning\non real data, which improved performance in both\nthe 100-shot and 300-shot settings. Specifically, us-\ning synthetic summaries in two-stage training sig-\nnificantly outperformed using the same summaries,\nmitigating overfitting and enhancing generalization.\nThis highlights the value of synthetic summaries\nin the more effective two-stage training framework.\nAdditionally, incorporating unseen summaries im-\nproved both training strategies, but the two-stage\ntraining still provided superior results.\nDialogue Generation and Filtering.\nTo evaluate\nthe benefits of generating and filtering dialogues,\nwe experimented with unseen summaries using the\ntwo-stage training strategy. In the first stage, we\ntested three types of synthetic data: summaries\nonly, summaries with unfiltered dialogues, and ap-\nproaches involving content alignment filtering or\nDPO training (MRDS), as shown in Table 5. We\nfound that training with summaries alone improved\nupon the real-only approach; however, adding di-\nalogues to the training data further enhanced the\nresults. Finally, the models utilizing filtering or\nDPO outperformed all other methods, demonstrat-\ning that including dialogues with filtering and DPO\nTable 5: Ablation study for dialogue generation, filter-\ning, and DPO on unseen summaries. The training is\nconducted in two stages training.\nApproach\nDialogSum 300 shots\nR-1\nR-2\nR-L\nB-S\nReal only\n45.2\n19.5\n37.3\n87.2\nUnseen Summaries\nw/o dialogue\n45.8\n19.9\n37.3\n87.2\nw/o filtering\n46.3\n20.1\n37.9\n87.3\nw/ filtering\n46.8\n20.8\n38.8\n87.6\nMRDS\n47.0\n20.7\n38.7\n87.5\neffectively improves the final outcomes.\n6\nConclusion\nWe introduce a novel approach that mutually rein-\nforces dialogue synthesis and summarization ca-\npabilities of a large language model (LLM) to im-\nprove few-shot dialogue summarization without\nrelying on external data. By leveraging the dia-\nlogue synthesis capability enhanced by DPO, we\nsynthesize well-formatted, coherent dialogues to\naugment the few-shot real dataset. Furthermore,\nthe two-stage training strategy effectively incorpo-\nrated synthesized dialogues without introducing ar-\ntifacts, improving summarization accuracy. Empir-\nical results demonstrated significant improvements:\na 1.5% increase in ROUGE scores, a 0.3% improve-\nment in BERT scores. Human evaluations con-\nfirmed that our method outperforms the real-only\n\nbaseline and, in certain aspects, surpasses human-\nannotated ground truth summaries. Our approach\noffers a practical solution for real-world applica-\ntions with limited data, utilizing the model’s inher-\nent capabilities without external resources. Future\nwork could extend this self-alignment framework\nto other NLP tasks affected by data scarcity and\nexplore its integration with larger or more diverse\nLLM architectures.\n7\nLimitations\nComparing to the baseline method, training sum-\nmarization adapters on few shot real data only, our\nmethod requires additional computation cost for\ndata synthesis adapter training and sampling. Also,\nour method makes an assumption that part of the\ninternal knowledge of LLM is useful for the tar-\nget domain, which might be incorrect for a highly\nspecialized target domain.\n8\nEthics Statement\nOur research introduces Mutual Reinforcing Data\nSynthesis (MRDS) within LLMs to enhance few-\nshot dialogue summarization tasks. While advanc-\ning natural language processing, we acknowledge\nethical considerations associated with our method-\nology. By leveraging synthetic data generated by\nLLMs, we reduce reliance on large-scale real-world\ndatasets that may contain sensitive or personally\nidentifiable information (PII). We strive to protect\nuser privacy and adhere to data protection regu-\nlations by using publicly available datasets (CC\nBY-NC-ND 4.0 and CC BY-NC-SA 4.0) and im-\nplementing data anonymization techniques.\nOur method utilizes LLMs pre-trained on vast\ncorpora that might contain biases and stereotypes.\nAlthough we train our synthesis and summariza-\ntion models on public datasets with daily conver-\nsations, we recognize that biases may persist. We\nencourage future work to identify and reduce bi-\nases in synthetic data generation and in models\ntrained on such data. We are committed to trans-\nparency. All experimental details—including data\npreprocessing, model configurations, and evalua-\ntion metrics—are thoroughly documented to ensure\nreproducibility and allow critical assessment by the\nresearch community. By openly sharing our meth-\nods and findings, we aim to foster collaboration\nand uphold ethical standards in AI development.\nReferences\nLuiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and\nRodrigo Nogueira. 2022.\nInpars: Unsupervised\ndataset generation for information retrieval. In Pro-\nceedings of the 45th International ACM SIGIR Con-\nference on Research and Development in Information\nRetrieval, SIGIR ’22, page 2387–2392, New York,\nNY, USA. Association for Computing Machinery.\nJiaao Chen and Diyi Yang. 2021. Simple conversational\ndata augmentation for semi-supervised abstractive\ndialogue summarization. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 6605–6616.\nYulong Chen, Yang Liu, Liang Chen, and Yue\nZhang. 2021.\nDialogsum:\nA real-life scenario\ndialogue summarization dataset.\narXiv preprint\narXiv:2105.06762.\nZixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan\nJi, and Quanquan Gu. 2024. Self-play fine-tuning\nconverts weak language models to strong language\nmodels. In Proceedings of the 41st International\nConference on Machine Learning, volume 235 of\nProceedings of Machine Learning Research, pages\n6621–6642. PMLR.\nBharath Chintagunta, Namit Katariya, Xavier Amatri-\nain, and Anitha Kannan. 2021.\nMedically aware\nGPT-3 as a data generator for medical dialogue sum-\nmarization. In Proceedings of the Second Workshop\non Natural Language Processing for Medical Conver-\nsations, pages 66–76, Online. Association for Com-\nputational Linguistics.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, et al. 2024. The llama 3 herd of models. arXiv\npreprint arXiv:2407.21783.\nGuy Feigenblat, Chulaka Gunasekara, Benjamin Szna-\njder, Sachindra Joshi, David Konopnicki, and Ranit\nAharonov. 2021. TWEETSUMM - a dialog sum-\nmarization dataset for customer service. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2021, pages 245–260, Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nJiahui Gao, Renjie Pi, LIN Yong, Hang Xu, Jiacheng\nYe, Zhiyong Wu, WEIZHONG ZHANG, Xiaodan\nLiang, Zhenguo Li, and Lingpeng Kong. 2023. Self-\nguided noise-free data generation for efficient zero-\nshot learning. In The Eleventh International Confer-\nence on Learning Representations.\nBogdan Gliwa, Iwona Mochol, Maciej Biesek, and Alek-\nsander Wawer. 2019. SAMSum corpus: A human-\nannotated dialogue dataset for abstractive summa-\nrization. In Proceedings of the 2nd Workshop on\nNew Frontiers in Summarization, pages 70–79, Hong\nKong, China. Association for Computational Linguis-\ntics.\n\nCaglar Gulcehre, Tom Le Paine, Srivatsan Srini-\nvasan, Ksenia Konyushkova, Lotte Weerts, Abhishek\nSharma, Aditya Siddhant, Alex Ahern, Miaosen\nWang, Chenjie Gu, et al. 2023.\nReinforced self-\ntraining (rest) for language modeling. arXiv preprint\narXiv:2308.08998.\nHimanshu Gupta,\nKevin Scaria,\nUjjwala Anan-\ntheswaran,\nShreyas\nVerma,\nMihir\nParmar,\nSaurabh Arjun Sawant, Chitta Baral, and Swa-\nroop Mishra. 2024.\nTarGEN: Targeted data\ngeneration with large language models.\nIn First\nConference on Language Modeling.\nJianfeng He, Hang Su, Jason Cai, Igor Shalyminov,\nHwanjun Song, and Saab Mansour. 2024.\nSemi-\nsupervised dialogue abstractive summarization via\nhigh-quality pseudolabel selection. In Proceedings\nof the 2024 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies (Volume 1: Long\nPapers), pages 5976–5996, Mexico City, Mexico. As-\nsociation for Computational Linguistics.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2022. LoRA: Low-rank adaptation of\nlarge language models. In International Conference\non Learning Representations.\nRaghav Jain, Anubhav Jangra, Sriparna Saha, and Adam\nJatowt. 2022. A survey on medical document sum-\nmarization. arXiv preprint arXiv:2212.01669.\nMartin Josifoski, Marija Sakota, Maxime Peyrard, and\nRobert West. 2023. Exploiting asymmetry for syn-\nthetic training data generation: SynthIE and the case\nof information extraction. In Proceedings of the 2023\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1555–1574, Singapore. As-\nsociation for Computational Linguistics.\nAtharva\nKulkarni,\nBo-Hsiang\nTseng,\nJoel\nRuben Antony Moniz,\nDhivya Piraviperumal,\nHong Yu, and Shruti Bhargava. 2024. SynthDST:\nSynthetic data is all you need for few-shot dialog\nstate tracking. In Proceedings of the 18th Conference\nof the European Chapter of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 1988–2001, St. Julian’s, Malta. Association\nfor Computational Linguistics.\nXian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer\nLevy, Luke Zettlemoyer, Jason E Weston, and Mike\nLewis. 2024. Self-alignment with instruction back-\ntranslation. In The Twelfth International Conference\non Learning Representations.\nYu Li, Baolin Peng, Pengcheng He, Michel Galley, Zhou\nYu, and Jianfeng Gao. 2023. DIONYSUS: A pre-\ntrained model for low-resource dialogue summariza-\ntion. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1368–1386, Toronto,\nCanada. Association for Computational Linguistics.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nShikib Mehri, Yasemin Altun, and Maxine Eskenazi.\n2022. LAD: Language models as data for zero-shot\ndialog. In Proceedings of the 23rd Annual Meeting\nof the Special Interest Group on Discourse and Dia-\nlogue, pages 595–604, Edinburgh, UK. Association\nfor Computational Linguistics.\nYu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han.\n2022. Generating training data with language mod-\nels: Towards zero-shot language understanding. In\nNeurIPS.\nSiru Ouyang, Jiaao Chen, Jiawei Han, and Diyi Yang.\n2023. Compositional data augmentation for abstrac-\ntive conversation summarization. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 1471–1488.\nSangwon Park, Hongseok Choi, Dongha Choi, and\nHyunju Lee. 2024. Gendex: Generative data aug-\nmentation strategy leveraging external data for ab-\nstractive dialogue summarization. In Findings of the\nAssociation for Computational Linguistics ACL 2024,\npages 3171–3185.\nMinh-Quang Pham, Sathish Indurthi, Shamil Chollam-\npatt, and Marco Turchi. 2023. Select, prompt, filter:\nDistilling large language models for summarizing\nconversations. In Proceedings of the 2023 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 12257–12265, Singapore. Association\nfor Computational Linguistics.\nHsuan Su, Ting-Yao Hu, Hema Swetha Koppula,\nRaviteja Vemulapalli, Jen-Hao Rick Chang, Karren\nYang, Gautam Varma Mantena, and Oncel Tuzel.\n2024. Corpus synthesis for zero-shot asr domain\nadaptation using large language models. In ICASSP.\nYuanhe Tian, Fei Xia, and Yan Song. 2024. Dialogue\nsummarization with mixture of experts based on large\nlanguage models. In Proceedings of the 62nd Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 7143–7155.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A. Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2023. Self-instruct: Aligning language\nmodels with self-generated instructions. In Proceed-\nings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 13484–13508, Toronto, Canada. Association\nfor Computational Linguistics.\nKaige Xie, Tong Yu, Haoliang Wang, Junda Wu, Han-\ndong Zhao, Ruiyi Zhang, Kanak Mahadik, Ani\nNenkova, and Mark Riedl. 2024. Few-shot dialogue\nsummarization via skeleton-assisted prompt transfer\nin prompt tuning. In Proceedings of the 18th Confer-\nence of the European Chapter of the Association for\n\nComputational Linguistics (Volume 1: Long Papers),\npages 2408–2421, St. Julian’s, Malta. Association\nfor Computational Linguistics.\nJiaxi Yang, Binyuan Hui, Min Yang, Jian Yang, Junyang\nLin, and Chang Zhou. 2024. Synthesizing text-to-\nSQL data from weak and strong LLMs. In Proceed-\nings of the 62nd Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 7864–7875, Bangkok, Thailand. Associ-\nation for Computational Linguistics.\nJiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiang-\ntao Feng, Zhiyong Wu, Tao Yu, and Lingpeng Kong.\n2022a. ZeroGen: Efficient zero-shot learning via\ndataset generation. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 11653–11669, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nJiacheng Ye, Jiahui Gao, Zhiyong Wu, Jiangtao Feng,\nTao Yu, and Lingpeng Kong. 2022b. ProGen: Pro-\ngressive zero-shot dataset generation via in-context\nfeedback. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2022, pages 3671–\n3683, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nTiezheng Yu, Zihan Liu, and Pascale Fung. 2021.\nAdaptSum: Towards low-resource domain adapta-\ntion for abstractive summarization. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 5892–5904,\nOnline. Association for Computational Linguistics.\nWeizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho,\nXian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason E\nWeston. 2024. Self-rewarding language models. In\nProceedings of the 41st International Conference on\nMachine Learning, volume 235 of Proceedings of\nMachine Learning Research, pages 57905–57923.\nPMLR.\nGuangtao Zeng, Wenmian Yang, Zeqian Ju, Yue Yang,\nSicheng Wang, Ruisi Zhang, Meng Zhou, Jiaqi\nZeng, Xiangyu Dong, Ruoyu Zhang, Hongchao Fang,\nPenghui Zhu, Shu Chen, and Pengtao Xie. 2020.\nMedDialog: Large-scale medical dialogue datasets.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9241–9250, Online. Association for Computa-\ntional Linguistics.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Eval-\nuating text generation with bert. In International\nConference on Learning Representations.\nLulu Zhao, Fujia Zheng, Keqing He, Weihao Zeng, Yue-\njie Lei, Huixing Jiang, Wei Wu, Weiran Xu, Jun\nGuo, and Fanyu Meng. 2021. Todsum: Task-oriented\ndialogue summarization with state tracking. arXiv\npreprint arXiv:2110.12680.\nLulu Zhao, Fujia Zheng, Weihao Zeng, Keqing He,\nWeiran Xu, Huixing Jiang, Wei Wu, and Yanan Wu.\n2022. Domain-oriented prefix-tuning: Towards ef-\nficient and generalizable fine-tuning for zero-shot\ndialogue summarization. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 4848–4862, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nMing Zhong, Yang Liu, Yichong Xu, Chenguang Zhu,\nand Michael Zeng. 2022.\nDialoglm: Pre-trained\nmodel for long dialogue understanding and summa-\nrization. In Proceedings of the AAAI Conference\non Artificial Intelligence, volume 36, pages 11765–\n11773.\nYicheng Zou, Bolin Zhu, Xingwu Hu, Tao Gui, and\nQi Zhang. 2021. Low-resource dialogue summariza-\ntion with domain-agnostic multi-source pretraining.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n80–91, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nA\nAppendix\nA.1\nPrompt Templates\nTable 6 shows the prompt templates we use in this\nwork for different purposes.\nA.2\nExamples of Summarization Result\nTable 7 presents an example dialogue along with\nsummaries produced by various methods, includ-\ning the human-labeled ground truth summary, the\noutput from the zero-shot pre-trained model, sum-\nmaries generated using in-context learning, and\nthose fine-tuned with real data only as well as with\nthe MRDS approach.\nA.3\nSynthetic Dialogue from SFT Trained\nSynthesizer\nTo show the limited capability of dialogue synthe-\nsizer trained on few shot real data, we provide an\nexample of low quality synthetic dialogue in Table\n8.\nA.4\nPreference Pair Examples\nTables 10 and 9 provide examples of content-based\nand format-based preference pairs, respectively. In\nTable 10, the summarization model scores the con-\ntent alignment of the dialogues using Msum(s| ˆd),\nselecting the best and worst ones as the preferred\nand rejected dialogues. In Table 9, the preferred dia-\nlogues are synthesized using IDS, where F( ˆd) = 1,\nwhile the rejected dialogues are generated through\n\nTopic extraction\nPlease determine the main topic of the provided summary.\nThe topic should be a brief phrase that captures the essence of the summary.\nInstructions:\n- Ensure the topic is around 2 words in length.\n- It should clearly reflect the core idea of the summary.\n- Avoid specific details and names; the topic should be general enough to apply to various summaries.\n- Use precise and specific language.\nSummary: [summary]\nTopic:\nSummary synthesis given topic\nPlease write a summary for a document based on the provided topic.\nInstructions:\n- The summary should be approximately [word count] words in length.\n- Ensure the summary captures the main idea related to the topic.\nTopic: [topic]\nSummary:\nDialogue synthesis given summary\nPlease write a dialogue based on the given summary.\nEach line should start with the anonymous speaker´s index followed by a colon.\nInstruction:\n- Use the [num of speaker] speakers in the dialogue:\n#1, #2 ...\n- Ensure each line starts with the speaker’s index followed by a colon.\n- Write the dialogue clearly and naturally.\n- The dialogue should be around [num of turns] turns and [num of words in dialogue] words in length.\nSummary: [summary]\nDialogue summarization\nDocument: [Dialogue]\nSummarize the provided document\nDialogue Summarization (zero shot)\nDialogue: [Dialogue]\nSummarize the provided dialogue.\nDialogue Summarization (in-context learning)\nYour task is to summarize the provided dialogue. The following are some examples:\nExample 1:\nDialogue: [Example dialogue 1]\nSummary: [Example summary 1]\nExample 2:\nDialogue: [Example dialogue 2]\nSummary: [Example summary 2]\n. . .\nNow, here is the target dialogue:\n[Dialogue]\nSummarize the target dialogue.\nDialogue Summarization (zero shot + length control)\nDialogue: [Dialogue]\nSummarize the provided dialogue.\nThe summary should be around [average word count of summaries in training set] words in length.\nTable 6: Prompt templates.\n\nInput dialogue\nAvery: You went to Ethan’s house?\nDavid: yeah I had to babysit\nAvery: Aww, how do you babysit, just curious\nDavid: I had to go through a lot :/\nAvery: Was his sister naughty\nDavid: Tooo much\nAvery: Lol\nDavid: I will just refuse net time :/\nAvery: As you wish\nDavid: :/\nAvery: I just got his text\nDavid: What is he saying\nAvery: He is asking me to say thanks to you\nDavid: yeah whatever<3\nAvery: He was saying that your phone was switched off\nDavid: Yeah i have just turned it on\nAvery: I have told him about that\nDavid: k\nAvery: Gotta go now\nGround Truth Summary\nDavid was looking after Ethan’s sister. Ethan is grateful. David won’t do it again.\nGenerated Summary (Zero Shot)\nThe dialogue is between Avery and David, discussing David’s experience babysitting Ethan’s sister.\nDavid mentions that the sister was \"too much\" and that he will refuse to babysit again.\nAvery shares that Ethan sent a text asking Avery to thank David,\nand also mentions that David’s phone was switched off.\nDavid confirms that he just turned it on and the conversation ends with Avery saying they have to go.\nGenerated Summary (In-context Learning)\nDavid babysat at Ethan’s house and had a difficult time.\nAvery asks how he managed it and David says it was a lot of work.\nAvery asks if Ethan’s sister was naughty and David confirms that she was.\nDavid jokes that he will refuse to babysit again.\nAvery mentions Ethan sent a text asking Avery to thank David, and David agrees to let Avery do so.\nAvery also mentions that Ethan’s phone was switched off, which David had just turned back on.\nGenerated Summary (Real only)\nDavid had to babysit Ethan’s sister.\nGenerated Summary (MRDS, Ours)\nDavid had to babysit Ethan’s sister. Avery got a text from Ethan saying thanks to David.\nTable 7: Summarization results from our method and baselines.\n\nSummary s:\n#2 can’t join #1 and Mike for a game because\nhe is studying.\nGenerated dialogue ˆd:\n#1: So will you come at 2?\n#2: sure bro, what time you gonna be with Mike?\n#1: about 20 min.00, Mike will call you\n2: he is going\n3 no way, he’s still sleeping hehehe\n2 he’s studying\n1: and?\nnot coming\nTable 8: Low quality dialogue generated by SFT-trained dialogue synthesizer. #1 and #2 are anonymized speaker\nnames. The text highlighted in red shows content that is not aligned with the summary (#2 can’t join and #2 is\nstudying instead of Mike). The lines highlighted in blue are format errors, since they do not start with a proper\nspeaker name (#1, #2, ...).\nthe SFT synthesizer’s one-shot synthesis, where\nF( ˆd) = 0.\nA.5\nData Formatting and Anonymization\nTo simplify the training of the data synthesizer, we\nperform anonymization preprocessing on both sum-\nmaries and dialogues. Specifically, we extract the\nnumber of speakers and their names from the dia-\nlogues as metadata and replace the names in both\ndialogues and summaries with uniform identifiers\n(e.g., \"#1\", \"#2\").\nA.5.1\nData Formatting\nTo ensure that the synthetic data adheres to the cor-\nrect format, we have established a set of formatting\nrules:\n- Speaker Identity: each sentence in the dia-\nlogues must begin with a speaker identifier\nfollowed by a colon (e.g., \"#1:\", \"#2:\").\n- Consistency of Names: Synthetic dialogues\nand summaries should not contain incorrect or\nextra names—for example, \"#4\" in dialogue-\nsummary pairs with fewer than four speakers,\nor a \"#\" symbol without a number.\n- Inclusion of Speaker Names: Synthetic sum-\nmaries must contain at least one anonymized\nspeaker name.\nFor summary synthesis, we discard any sum-\nmaries that do not comply with these formatting\nrules. For dialogue synthesis, we apply Iterative Di-\nalogue Synthesis (IDS) to ensure that the synthetic\ndialogues conform to the correct format.\nA.5.2\nIterative Dialogue Synthesis\nTable 11 illustrates an example of iterative dialogue\nsynthesis, showing partial inputs given to the syn-\nthesizer and the corresponding synthesized outputs.\nDuring the synthesis process, we check for format-\nting issues as described in Sec. A.5.1. If an error\nis found, the flawed sentence, along with all sub-\nsequent sentences, is discarded, and the correctly\nformatted segment (e.g., \"#1:\", \"#2:\") is used to\nre-synthesize the dialogue.\nA.5.3\nSynthetic Dialogue Recovery\nAfter data synthesis, we restore the anonymized\ndialogues and summaries before training the sum-\nmarization model by replacing placeholders with\nreal names using metadata extracted during the\nanonymization process. For synthetic summaries\nthat lack original metadata, we seed the synthesis\nprocess with metadata from the real data in the\nfew-shot training set, then replace the placeholders\naccordingly. Alternatively, we can generate ran-\ndom names and speaker numbers to populate the\nplaceholders.\nA.6\nHuman Evaluation Details\nWe hired five machine learning researchers, and\nreceived their consent to report the results from\ntheir annotation work. We adopt the human evalu-\nation method proposed in the previous work (Xie\net al., 2024). For completeness, we describe the 4\nmetrics (informativeness, faithfulness, fluency and\nredundancy) and the corresponding instructions in\nTable 12.\n\nSummary:\nOn Sunday #1 and #2 are meeting with Dominica and James who got married recently.\nPreferred dialogue:\n#1: Hahaha we must meet Dominica tomorrow with James.\n#2: yep\n#1: I dont think she will be very available after the marriage.\n#2: ye ye, she’s very busy\n#1: I asked :) :) <file_gifile>\n#2: so 8 o’clock?\n#1: sounds super\nRejected dialogue:\n#1: are u free next Sunday?\n#1 file_gifiles : yeah\n#2: ok\n#1: wanna meet Domin?\n#2: ok\n# fileifile\n# gileile\n# : ok: <file :)\nfile :) <file :) : :file :) <file < :)\n# Ok, ok :) <file :)\n:) it sounds cool\nTable 9: Example of a format-based preference pair. #1 and #2 are anonymized speaker names. The lines highlighted\nin blue are format errors, since they do not start with a proper speaker name (#1, #2).\nSummary:\n#1 is outside waiting for #2, who is late.\nPreferred dialogue:\n#1: Hey, it’s #1. I’m sitting outside where are you?.\n#2: <file_giffile>\n#1: You’re late! And I was waiting for you there.\nRejected dialogue:\n#1: I can’t come to the meeting\n#2: what did?\n#1: I have not enough money for cab. I must wait for #1\n#2: #1 is waiting for us!\n#1: how can I get there?\n#2: use bus, it’s the fastest, come back me\nTable 10: Example of a content-based preference pair. #1 and #2 are anonymized speaker names.\n\nInput summary\nBoth #1 and #2 forgot about Monica’s birthday today.\nAs they do not know what present to buy for her, #2 will phone Monica’s boyfriend and ask his advice.\nFirst partial response\n#1:\nFirst synthesis result\n#1: I forgot about Monicas birthday today haha\n#2: Me to, me too\n#1: what now?\n#2: we should make something?\n#1: what do you propose to buy?\n#1: I’m asking my brother’s opinion, he knows her a bit more\n#2: same here, I will phone her boyfried\n#1: you must have more info\n2: ya, I will get some advice\n1:\nI asked and we decided to buy her something\n2.: 1 can not see.\n5. This 2 1 3’s got 1 6.75.\nSecond partial response\n#1: I forgot about Monicas birthday today haha\n#2: Me to, me too\n#1: what now?\n#2: we should make something?\n#1: what do you propose to buy?\n#1: I’m asking my brother’s opinion, he knows her a bit more\n#2: same here, I will phone her boyfried\n#1: you must have more info\n#2:\nSecond synthesis result\n#1: I forgot about Monicas birthday today haha\n#2: Me to, me too\n#1: what now?\n#2: we should make something?\n#1: what do you propose to buy?\n#1: I’m asking my brother’s opinion, he knows her a bit more\n#2: same here, I will phone her boyfried\n#1: you must have more info\n#2: sure\n#1: I am already looking for some ideas\n#2: it looks hopeless\n#2: I will call and he phone\n#1: yes\nTable 11: An example of iterative dialogue synthesis (IDS), condensed for brevity. In each step, the correctly-\nformatted partial response is concatenated to the initial dialogue synthesis prompt and given as input for generation.\nNewly generated text appears in blue, while discarded text is italicized. In each step, we also add an anonymized\nspeaker name (shown in yellow) at the end of the partial response before using it for generation. We always use #1\nas the first partial response, and randomly choose an anonymized speaker in the subsequent steps.\n\nInformativeness\nWhether the critical information in the dialogue is missed in the summary:\n*0: lots of the critical information in the dialogue is missed;\n*1: a small amount of the critical information in the dialogue is missed;\n*2: no critical information in the dialogue is missed.\nFaithfulness\nWhether the information presented in the summary is factually incorrect or unmentioned\naccording to the dialogue:\n*0: lots of the information presented in the summary is factually incorrect or unmentioned;\n*1: a small amount of the information presented in the summary is factually incorrect or unmentioned;\n*2: no information presented in the summary is factually incorrect or unmentioned.\nFluency\nWhether the sentences in the summary are ungrammatical or ill-formed:\n*0: lots of the sentences in the summary are ungrammatical or ill-formed;\n*1: a small amount of the sentences in the summary are ungrammatical or ill-formed;\n*2: no sentence in the summary is ungrammatical or ill-formed.\nRedundancy\nWhether the expressions of the summary can be simplified:\n*0: lots of the expressions of the summary can be simplified;\n*1: a small amount of the expressions of the summary can be simplified;\n*2: no expression of the summary can be simplified.\nTable 12: Human evaluation metrics and their corresponding instructions\nA.7\nTraining Hyperparameters\nTable 13 presents the hyperparameters used for the\nsummarization and dialogue synthesis models. We\nconducted an extensive search for hyperparameters\non the summarization model trained exclusively\nwith real data. All other SFT training follows the\nsame set of hyperparameters. The LR threshold\nfor the two-stage summarization approach refers\nto the minimum learning rate reached during the\nfirst stage of training with synthetic data. Once this\nthreshold is met, the second stage begins using real\ndata, applying the same hyperparameters as those\nused for the real-only summarization model.\nThe DPO synthesis model is initialized from\nthe checkpoints of the SFT synthesis model and\nis trained using both DPO and SFT loss.\nThe\nbatch size for DPO training is four, randomly sam-\npled from two preference sets: format-based and\ncontent-based preference pairs. Additionally, SFT\ntraining data with a batch size of one is included\nin the process, and the combined losses are used to\nupdate the synthesis model.\nA.8\nDialogue Synthesis Quality\nWe evaluated the performance of different dialogue\nsynthesis methods, including direct synthesis (One-\nshot), iterative dialogue synthesis (Iterative), and\nvarious configurations of DPO, as presented in Ta-\nble 14. In our DPO training, we constructed differ-\nent sets of preference pairs based on the definitions\nprovided earlier. For the single joint preference\nset, we used dialogues from the post-processing\nstep as the preferred data ( ˆd1) and the raw dia-\nlogues without iterative synthesis as the rejected\ndata ( ˆd2), forming preference pairs: {s, ˆd1, ˆd2 |\ns ∈S, F( ˆd1) = 1, F( ˆd2) = 0, Msum(s | ˆd1) >\nMsum(s |\nˆd2)}.\nFor the separated preference\nsets, we included both formatting preference pairs:\n{s, ˆd1, ˆd2 | s ∈S, F( ˆd1) = 1, F( ˆd2) = 0} and\nsummarization preference pairs: {s, ˆd1, ˆd2 | s ∈\nS, Msum(s | ˆd1) > Msum(s | ˆd2)}. We also incor-\nporated supervised fine-tuning data into the DPO\ntraining, indicated as \"+SFT.\"\nFrom Table 14, the SFT model’s one-shot syn-\nthesis had low format correctness (24%) and a\nhigh summarization cross-entropy (CE) loss of\n4.74. While Iterative synthesis ensured 100% for-\nmat correctness, the CE loss remained similar at\n4.69, showing little improvement in content align-\nment. Applying DPO with the joint preference set\nimproves format correctness to 78% and reduces\nthe summarization CE loss to 4.62. However, train-\ning with DPO loss alone was unstable, so adding\nSFT loss (Joint Preference Set + SFT), stabilized\n\nTable 13: Hyper-parameters used for training dialogue summarization, two-stage dialogue summarization, dia-\nlogue/summary synthesis, and DPO-based dialogue synthesis models.\nSummarization\nTwo-stages Summ.\nSynthesis\nDPO Syn.\nLoss\nCE\nCE\nCE\nDPO and CE\nBatch Size\n10\n10\n10\n4 DPO & 1 SFT\nLearning Rate\n2.0e-4\n2.0e-4\n2.0e-4\n1.0e-5\nOptimizer\nReduceLROnPlateau\nReduceLROnPlateau\nReduceLROnPlateau\nFixed LR\nValidation Steps\n2\n2\n2\n20\nPatience\n5\n5\n5\n-\nFactor\n0.7\n0.7\n0.7\n-\nWarmup Steps\n50\n50\n50\n-\nEarly Stopping\n50\n50\n50\n-\nLR Threshold\n-\n2.0e-5\n-\n-\nLoRA Rank\n16\nLoRA Alpha\n32\nDropout Rate\n0.4\nSynthesis Model\nSynthesis Method\nFormat Corr.\nSumm. CE\nSFT\nOne-shot\n24%\n4.74\nSFT\nIterative\n100%*\n4.69\nJoint Preference Set\nOne-shot\n78%\n4.62\nJoint Preference Set + SFT\nOne-shot\n96%\n4.45\nSeparate Preference Sets + SFT (MRDS)\nOne-shot\n92%\n4.13\nTable 14: Dialouge Synthesis with different training strategy on SAMSum 300 shots experiments.\n\ntraining, achieving 96% format correctness and\na CE loss of 4.45. Yet, this approach prioritized\nformatting over content quality, limiting further\nimprovement in CE loss.\nBy separating the preference sets and including\nboth formatting and summarization preferences in\nDPO training (Separate Preference Sets + SFT),\nour MRDS approach effectively balanced format\ncorrectness (92%) and significantly lowered the\nCE loss to 4.13, indicating better alignment with\nsummaries. This demonstrates that separating pref-\nerence sets allows concurrent optimization of for-\nmatting and content, leading to the best overall\nperformance.\nA.9\nStatistical Analysis of Dialogue\nSummarization Experiments\nTo further validate the improvements introduced by\nthe MRDS method, we conduct each experiment\nthree times under the 100-shot setting on both the\nSAMSum and DialogSum datasets. For each run,\nwe compute the mean and sample standard devia-\ntion (using n−1 in the denominator) for ROUGE-1,\nROUGE-2, ROUGE-L, and BERTScore. In ad-\ndition, independent two-sample t-tests (with de-\ngrees of freedom df = 4) were performed to as-\nsess the statistical significance of the differences\nbetween the baseline (summarization adapter fine-\ntuned with real data only) and MRDS results.\nTables 15 and 16 summarize the results for SAM-\nSum and DialogSum, respectively. For instance, on\nSAMSum the ROUGE-1 score improved from an\naverage of 50.90% (std = 0.18%) to 52.10% (std\n= 0.25%), a gain of 1.20 percentage points (t =\n6.80, p ≈0.003). Similar statistically significant\nimprovements are observed across all metrics and\non both datasets.\nThe t-test results confirm that the improvements\nachieved by MRDS over the baseline are statisti-\ncally significant (p-values < 0.05) across all metrics\non both datasets.\nA.10\nComparison with State-of-the-art\nMethods\nIn Table 17, we compare the proposed MRDS\nwith three existing state-of-the-art methods that\nuse data augmentation and pseudo labeling to ad-\ndress the data scarcity problem for dialogue sum-\nmarization task. The results show that our MRDS\nmethod achieves competitive or better performance\nin Rouge-1, Rouge-2 and Rouge-L. In the table,\nwe also compare the different backbone models,\nthe amount of few-shot paired data and the need\nof unlabelled data among existing methods and\nMRDS.\n\nTable 15: Statistical analysis on SAMSum 100-shot experiments (values in percentages). The baseline method\nadopts summarization adapter finetuned with real data only.\nMetric\nBaseline (mean ± std)\nMRDS (Ours) (mean ± std)\nDifference\nt-value\np-value\nROUGE-1\n50.90 ± 0.18\n52.10 ± 0.25\n+1.20\n6.80\n0.003\nROUGE-2\n26.54 ± 0.04\n27.54 ± 0.31\n+1.00\n5.52\n0.005\nROUGE-L\n42.62 ± 0.09\n43.42 ± 0.35\n+0.80\n3.87\n0.018\nBERTScore\n86.59 ± 0.04\n86.81 ± 0.07\n+0.22\n4.84\n0.011\nTable 16: Statistical analysis on DialogSum 100-shot experiments (values in percentages). The baseline method\nadopts summarization adapter finetuned with real data only.\nMetric\nBaseline (mean ± std)\nMRDS (Ours) (mean ± std)\nDifference\nt-value\np-value\nROUGE-1\n44.04 ± 0.55\n45.47 ± 0.15\n+1.43\n4.35\n0.015\nROUGE-2\n18.23 ± 0.35\n19.26 ± 0.27\n+1.03\n4.06\n0.017\nROUGE-L\n35.97 ± 0.49\n37.25 ± 0.13\n+1.28\n4.32\n0.016\nBERTScore\n86.81 ± 0.11\n87.19 ± 0.02\n+0.38\n5.86\n0.008\nTable 17: Comparison among the proposed MRDS and three existing state-of-the-art methods (Semi-CODA (Chen\nand Yang, 2021), SiCF (He et al., 2024) and COMPO (Ouyang et al., 2023)) on SAMSum dataset. ∗Using unlabled\ndata.\nMethods\nbackbone\nnum. of paired data\nRouge-1\nRouge-2\nRouge-L\nSemi-CODA∗\nBART-large\n147\n42.16\n17.82\n38.89\nSiCF∗\nDialogLM\n147\n45.85\n19.90\n35.96\nCOMPO\nBART-large\n147\n49.78\n24.65\n45.41\nMRDS (ours)\nLlama3-8B-Instruct\n100\n52.1\n27.5\n43.4\n",
  "metadata": {
    "source_path": "papers/arxiv/Mutual_Reinforcement_of_LLM_Dialogue_Synthesis_and_Summarization\n__Capabilities_for_Few-Shot_Dialogue_Summarization_8ebfe55b16f4d222.pdf",
    "content_hash": "8ebfe55b16f4d2223533adbfcc4b6f1b8b3b831f67827ed8117d6f023fe57dd6",
    "arxiv_id": null,
    "title": "Mutual_Reinforcement_of_LLM_Dialogue_Synthesis_and_Summarization\n__Capabilities_for_Few-Shot_Dialogue_Summarization_8ebfe55b16f4d222",
    "author": "",
    "creation_date": "D:20250225030138Z",
    "published": "2025-02-25T03:01:38",
    "pages": 20,
    "size": 556835,
    "file_mtime": 1740470159.4694083
  }
}