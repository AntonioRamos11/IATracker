{
  "text": "Interpretable Text Embeddings and Text Similarity Explanation: A Primer\nJuri Opitz1\nLucas Möller2\nAndrianos Michail1\nSimon Clematide1\n1University of Zurich, Switzerland\n2IMS at University of Stuttgart, Germany\n1{jurialexander.opitz,andrianos.michail,simon.clematide}@uzh.ch\n2lucas.moeller@ims.uni-stuttgart.de\nAbstract\nText embeddings and text embedding models\nare a backbone of many AI and NLP systems,\nparticularly those involving search. However,\ninterpretability challenges persist, especially in\nexplaining obtained similarity scores, which is\ncrucial for applications requiring transparency.\nIn this paper, we give a structured overview\nof interpretability methods specializing in ex-\nplaining those similarity scores, an emerging re-\nsearch area. We study the methods’ individual\nideas and techniques, evaluating their potential\nfor improving interpretability of text embed-\ndings and explaining predicted similarities.\n1\nIntroduction\nEmbedding models (Reimers and Gurevych, 2019;\nGao et al., 2021) are indispensable across numer-\nous NLP tasks in both academia and industry.\nApplications range from semantic search and in-\nformation retrieval (Ye et al., 2016; Guo et al.,\n2020; Muennighoff, 2022; Hambarde and Proenca,\n2023; Alatrash et al., 2024) to text classification\n(Schopf et al., 2022), topic modeling (Grooten-\ndorst, 2022), NLG evaluation (Celikyilmaz et al.,\n2020; Uhrig et al., 2021; Sai et al., 2022; Lari-\nonov et al., 2023; Chollampatt et al., 2025), knowl-\nedge graph construction (Plenz et al., 2023), and\nretrieval-augmented generation (RAG)—a specific\nform of information retrieval leveraging embed-\nding similarity to identify evidence from a large\ncorpus and summarize it using generative Large\nLanguage Models (LLMs, Lewis et al., 2020; Gao\net al., 2023). Furthermore, advances in base models\n(Günther et al., 2023; Wang et al., 2024a), context\nsize (Li et al., 2023), and scalable training infras-\ntructure (Wang et al., 2022) have steadily enhanced\nthe capabilities of embeddings.\nHowever, an urgent challenge persists: the prob-\nlem of interpretability. For example, when a set of\nmost similar documents is retrieved in response to\na query, we would like to articulate why these doc-\numents were selected as the most similar, or why a\nparticular document was omitted. Similarity vari-\nables are also highly conflated: Were the obtained\nsimilarity ratings based on semantic similarity, re-\nlatedness, paraphrasticity, relevance—subtly dis-\ntinct concepts (Budanitsky and Hirst, 2006; Kolb,\n2009; Michail et al., 2025)—or were they influ-\nenced mainly by superficial characteristics such as\ntoken overlap? And such questions are not just\ntheoretical; they have significant practical implica-\ntions. For instance, embedding systems deployed\nin sensitive domains may need to justify outputs,\nperhaps even in a legal context.\nFortunately, recent research has begun address-\ning this interpretability gap. Our paper aims to\nserve as a primer for researchers and practitioners\nthat seek to understand embedding-based similarity\nmodels and measurements. By presenting a struc-\ntured overview of interpretability approaches, we\nhope to ease entry into this area and inspire fur-\nther innovations. Understanding how similarity is\ncomputed—and how it can be explained—not only\nenhances transparency but may also pave the way\nfor improved methods and applications.\n2\nSetting the Stage\nWe study explainability in neural text embed-\ndings and their induced similarity. We distinguish\nthis from common approaches to classification-\nexplanation like ‘LIME’ (Ribeiro et al., 2016), or\n‘Shapley values’, see also Calderon and Reichart\n(2024)’s survey. Similarity is not based on a single\ninput but rather the interaction of two inputs, hence\nthe need for specialized methods.\nNotation.\nAssume a (tokenized) input text =\n[t1, ..., tn], and two neural networks F, G consist-\ning of L layers, each representing a function, e.g.,\nin the case of F: F = fL◦...◦f1. Typically F = G,\ni.e., the weights of the two networks are shared,\n1\narXiv:2502.14862v1  [cs.CL]  20 Feb 2025\n\nlayer f1\n layer f...\n  layer fL  \nlayer g1\n layer g...\nlayer gL\n \n∑\n \n= sim\nsim\nreduce\n \n \n∑≈sim\n∈Rd x 1\n∈Rd x 1\nSet-based\nAttributions\nInput: n tokens of Text X\nInput: m tokens of Text Y\nreduce\n∈Rd x m\n∈Rd x n\nFeat 1\nFeat 2\n...\nSpace-\nshaping\nToken\nalignment\nbox\nFigure 1: Three explanation perspectives.\nalso called Siamese network (Koch et al., 2015);\nif not mentioned otherwise, we thus only speak of\nF. The first layer maps tokens to real valued em-\nbeddings: E1 = f1(text) ∈Rd×m, whereas the\nconsecutive neural layers perform non-linear oper-\nations to transform and refine the representation.\nOftentimes, there is a last (optional) layer L + 1\nthat has a special goal: producing a vectorized\nfixed-size representation independent of document\nlength, i.e., eL+1 = reduce(EL) ∈Rd×1. This\nlayer would perform averaging, or max-pooling\nacross the individual token embedding dimensions.\nFinally, we can efficiently match two texts x, y\nthrough their embeddings ex\n=\nF(x), ey\n=\nF(y) by calculating a similarity function sim:\nsim(ex, ey) ∈R; in the simplest case, this can\nbe the dot product sim(ex, ey) := eT\nx ey, possibly\nnormalized by length lx,y = |ex|2 · |ey|2 to achieve\nthe (practically strongly correlated) ‘cosine sim-\nilarity’. Importantly, such a value quantifies the\nsimilarity relationship between texts, and thus we\ncan rank texts according to their similarity.\nApproach categorization and paper structure.\nOur definition of text embedding and similarity\nallows us to distinguish different types of explain-\nability approaches (Figure 1).\nFirst we will visit space shaping approaches\n(§3). They aim at shaping the projected embedding\nspaces, infusing some useful structure. E.g., the\nembedding space could be made up of different\nfeatures, which would equate to a sim that we can\ndecompose and better understand (Figure 1: Feat 1,\nFeat 2...). Alternatively, we can shape the space to\nbe more expressive. E.g., we observe approaches\nthat represent text as a high-dimensional box, or a\nrandom variable (Figure 1, box, Nd(µ, Σ)).\nAnother class of approaches are set-based ap-\nproaches (§4). They do not base their sim on two\nembeddings but on two sets of embeddings. These\nembeddings typically relate to human interpretable\nunits (i.e., tokens), often it’s the last layer’s output\nfL(x), fL(y). Set-theoretic operators can then be\napplied (e.g., intersection). We can also retrieve an\nalignment between the embeddings, adding another\nlayer of transparency as to what is sim made up\nfrom (Figure 1, Token alignment).\nThe third category of approaches we denote as\nattribution-based approaches (§5). These aim at\nattributing sim directly to the inputs, or pairs of\ninputs, given the representations that are consumed\nby certain neural network layers (Figure 1, Attribu-\ntions: For a particular layer, a pairwise similarity\nmatrix is built that approximates the sim).\nWe conclude the presentation of the three classes\nof approaches with a discussion (§6), outlining\npertinent challenges. Finally, we give an overview\nof related studies (§7), as well as datasets that\nelicit human similarity explanations.\n3\nShaping Interpretable Spaces\n3.1\nIdea\nThese approaches aim at structuring the embedding\nspace such that it becomes more interpretable. E.g.,\nthe space can be shaped to express aspects, inter-\npretable geometries or probabilistic distributions.\n3.2\nApproaches\n3.2.1\nFeature Decomposition\nTraditional methods for text similarity often relied\non explicit “bag-of-words” feature representation.\nWhile this provides great transparency in repre-\nsentation and similarity calculation, it lacks the\nrepresentational power of neural embeddings, can-\nnot match paraphrases, and thus result in relatively\npoor performance on standard benchmarks. Re-\ncent efforts aim to combine the interpretability of\nfeatures with the power of neural embeddings.\nQ/A features.\nThis approach involves framing\nembedding generation as answering a set of pre-\ndefined questions about a text and encoding the\nanswers as features, enabling interpretability. For\nthis, we first need to find a suitable set of questions\nabout texts, and create training data that elicits an-\nswers to these questions. Afterwards, we can distill\nan efficient and interpretable text embedding model\nusing this training data. Specifically, Benara et al.\n(2024) let an LLM answer “Yes”/“No” questions\n2\n\n0.00\n0.25\n0.50\n0.75\n1.00\nOVERALL Concepts\nNegations\nFocus\nCoreference Quantifier\nFigure 2: How the overall sim emerges from different\naspectual similarites, via (S3BERT) space decomposi-\ntion. The example is shortened to a selection of features.\nabout a text (e.g., is the text about sports?, Does\nthe text express a command?), building prompts\nbased on dataset description. For predicting fMRI\nresponses to language stimuli their method outper-\nforms several baselines. On the other hand, Sun\net al. (2024) first build a concept space from a\ndataset by clustering word embeddings, and intro-\nduce two constraints, namely that the Q/A prompt\nbe based on focal concepts. Also, for positive text\npairs, all questions should be answered with “Yes”,\nwhile for negative text pairs all questions be an-\nswered with “No”, to sharpen the boundary be-\ntween similar and non-similar texts.\nSub-embeddings.\nAn embedding space can be\ndecomposed into multi-dimensional subspaces,\neach isolating a certain semantic aspect. This en-\nables the overall similarity between texts be broken\ndown into similarity scores for different aspects.\nAn example for this is the approach by Opitz and\nFrank (2022, S3BERT: Semantically Structured\nSBERT). The method requires a user to define a\nset of metrics that measure interpretable similarity\naspects of two text (e.g., Is the focus of the texts the\nsame?). Since such aspects often are implicit in the\ntexts, they leverage abstract meaning representation\ngraphs (Banarescu et al., 2013) that encode aspects\nsuch as number, focus, semantic roles, negation;\nand use graph matching metrics (Opitz, 2023) on\naspectual subgraphs. They fine-tune a reference\nembedding model such that the similarity of as-\npectual sub-embeddings regresses to the aspectual\ngraph metrics. A consistency loss and residual sub-\nembedding helps tie the overall similarities to the\noriginal reference. Lastly, an explanation can look\nas follows (Figure 2): Two men are singing is simi-\nlar to Three men are singing by a value of 0.76. The\nsimilarity of concepts increases the value, while the\ndissimilarity of quantificational structure lowers it.\nSimilar approaches do not leverage a consis-\ntency loss and wish to induce entirely new decom-\nposed spaces: We can learn “multi-facet” embed-\ndings (Risch et al., 2021, with graph metric ground\ntruth) or “specialized-aspect” embeddings (Osten-\ndorff et al., 2022; Schopf et al., 2023, with aspect-\nspecific transformer encoders).\nA more coarse decomposition is induced by Pon-\nwitayarat et al. (2024), who construct two spaces,\none for texts that are only vaguely similar (lower\nrange), and the other to capture finer text similarity\nbetween already highly similar texts (upper range).\nThis idea was based on their linguistic analysis of\nthe Semantic Textual Similarity dataset (STS, Cer\net al., 2017), finding that one continuous similarity\nrange is not expressive enough, motivating their de-\ncomposition into two continua. To decompose, they\nuse a classification loss (high-range vs. low-range),\nand learn the representation of positive examples\nonly in the “upper range part” of the space.\n3.2.2\nNon-Euclidean Geometry\nCertain text relationships are inherently asymmet-\nric. For instance, a natural relation between texts\nis entailment: A given hypothesis follows from a\npremise. Some embedding geometries offer a way\nto model these relationships.\nAn interesting example are box-embeddings:\nConsider all two-dimensional boxes centered at\nzero with their left bottom corner. For two such\nboxes a and b we have their size sa = a1 · a2,\nsb = b1 · b2, and their overlap oa,b = min(a1, b1) ·\nmin(a2, b2).\nWe arrive, e.g., at a similarity\noa,b/(sa + sb −oa,b), and interesting asymmetric\nrelationships like the containment or entailment of,\ne.g., a in b: oa,b/sb — it’s exactly 1 if a is fully con-\ntained/entailed in/by b. The challenge is to learn\nsuch objects in high dimensionality: To see a ma-\njor bottleneck, consider that box size and overlap\napproach zero in high dimensionality, since they\ninvolve a large product. To alleviate such learning\nproblems, Chheda et al. (2021) propose to adopt a\nprobabilistic soft box overlap formulation based on\nGumbel random variables (Dasgupta et al., 2020).\nOn the other hand,\nHuang et al. (2023)\nlearn interpretable composition operators, such as\nunion/fusion, or difference, by modeling the oper-\nators with neural networks, and retraining the em-\nbedding models such that their space is shaped for\noperator allowance. Evaluation shows little loss on\n3\n\nstandard similarity accuracy, but greatly improved\nperformance for compositional generation tasks.\nAnother line of research investigates probabilis-\ntic text embeddings, viewing a text as a random vari-\nable (RV). Intuitively, this provides us with a model\nof multiple interpretation, which seems appealing\ndue to natural language ambiguity: A text can have\nmultiple interpretations, and only some of these\ninterpretation can map to those of another similar\ntext. But how to build such a probabilistic space?\nShen et al. (2023) model a text as a Gaussian RV\nNd(µ, Σ) by estimating “Model uncertainty” via\nMonte Carlo Dropout (Gal and Ghahramani, 2016),\nand data uncertainty via smaller linguistic pertur-\nbations (e.g., dropping a word). The covariance\nmatrix (ˆΣ) is then efficiently approximated through\nbanding estimator (Jacob Bien and Xiao, 2016).\nFor increased efficiency, Yoda et al. (2024) learn to\ndirectly predict mean (ˆµ) and covariance (ˆΣ).\n3.2.3\nCombining Token Embeddings\nCombination-based approaches build a new em-\nbedding space by aggregating token-level repre-\nsentations with explicit weights that reflect their\nimportance. E.g., Wang and Kuo (2020) estimate\ntoken importance and novelty using variance across\ntransformer layers, constructing weighted embed-\ndings. On the other hand, Seo et al. (2022) train\nmodels to learn token weights directly, using a re-\nconstruction loss to prevent catastrophic forgetting.\nAlternatively, we can create static embeddings for\nall tokens in the vocabulary, using one transformer\nforward pass (for each token), and then calculate\na simple average that is informed by Zipf token\nstatistics (Tulkens and van Dongen, 2024).\n3.3\nChallenges and Opportunities\nQ/A approaches can outperform certain baselines,\nbut they do not (yet) fully seem to match the per-\nformance of reference embedding models with dis-\ntributed features, probably since it is difficult to\nfind a generalizable set of questions.1\nSimilarly, the sub-embedding decomposition ap-\nproaches requires the definition of custom aspects,\nand the features are not directly interpretable on\ntheir own —only their similarity value is.\nOn one hand, crafting the right features (through\nquestions or interpretable metrics) can be seen as\na drawback of the feature based approaches. How-\n1In experiments, they are compared mainly against base-\nlines like bag-of-words (Sun et al., 2024), or they are built for\na specific domain (Benara et al., 2024).\never, it is also an interesting opportunity, since it\nallows for exploring custom spaces.\nFinally, non-euclidean geometry based methods\nand combination-based ones leave ample space for\nexploration. Modeling embeddings as, e.g., boxes,\nallows application of interpretable operators align-\ning with semantic relationships (e.g., entailment).\n4\nSet-based Interpretability\n4.1\nIdea\nSet-based approaches to similarity explainability\nrely on matching two sets, rather than two points.\nThese sets typically consist of human-interpretable\nitems, e.g., tokens. Aligning these, we may be\nprovided with insight into how different text parts\nrelate to each other. Sets also offer inherent in-\nterpretability through set-theoretic operations that\nmay align naturally with some interesting semantic\ntext relationships (e.g., entailment as subset).\n4.2\nApproaches\n4.2.1\nEmbedding Set Interpretability\nAlignment-based methods derive similarity by\naligning token embeddings from one text with\nthose of another. These approaches typically use\nembeddings from the last layer of a model. Two\nfocal techniques in this category are “ColBERT”\nand “BERTscore”. The ColBERT approach (Khat-\ntab and Zaharia, 2020; Santhanam et al., 2022,\nFigure 3) computes an asymmetric alignment by\nviewing x as the query and y as the candidate\n(aka “passage”) using two encoders; in our no-\ntation, F := Q and G := C.\nFor each indi-\nvidual token embedding in the embedded query\nEx = Q(x), we search in the embedded candi-\ndate tokens Ey = C(y) for the best match and\nsum over those. For computing a similarity score,\nboth methods rely on a greedy max-matching, for-\nmally, sim(x, y) = P\nt∈x max([Q(x)T C(y)]t).\nThe BERTscore approach, which is used in the eval-\nuation of machine translation (Zhang et al., 2020),\nfurther computes a symmetric harmonic mean (F1\nscore) as HM( 1\n|x|sim(x, y), 1\n|y|sim(y, x)).\nWhile the asymmetry in ColBERT is also\nachieved through the different encoders, the asym-\nmetry in BERTscore is achieved by the calculation\nof precision and recall, hence their different appli-\ncation cases (IR vs. evaluation). In their potential\nfor explainability, both approaches appear similar:\nTheir similarity score can be seen as constructed\nfrom an interpretable alignment from one docu-\n4\n\nFigure 3: An example of a late-interaction matrix be-\ntween query and passage token embeddings in the Col-\nBERTv2.0 model. The overall sim is 0.965. Red boxes\nindicate the sum of row-wise maxima (alignment).\nment to another, and the contributions of single\ntoken embedding pairs can be clearly highlighted.2\n4.2.2\nExplicit Multi-Interpretation\nMost set-based approaches use token embeddings,\nbut some extend the concept to text embeddings.\nThe first class of methods generates sets of text\nembeddings by either hypothesizing about a text or\ndecomposing it into smaller parts. In particular, we\ncan use a generative model to construct hypotheses\nabout a text (Hoyle et al., 2023), or decomposing it\ninto smaller statements or descriptions (Ravfogel\net al., 2024). Having deconstructed a text x into\nsmaller parts {x1, ...xn}, we call our text embed-\nding model exactly n times, and thus construct a set\nof n respective text embeddings {e1, ...en} that can\nbe matched to explain similarity of facts contained\nin a text, also with different abstractness levels.3\nAn interesting variation of such a multi-text set-\nbased approach is proposed by Liu et al. (2024).\nTo compute the similarity of two texts, they sample\nsets of possible continuation from an LLM, and\ncalculate the average (i.e., expected) difference in\nlog-likelihood between the two input texts that are\ncontinued with a randomly sampled continuation.\n2For refined alignment, optimal transport can replace\ngreedy max-matching (Kusner et al., 2015; Lee et al., 2022).\n3An example for different abstractness levels from Ravfo-\ngel et al. (2024): Given “On July 2, concurrent with the Battle\nof Gettysburg in neighboring Adams County, Captain Ulric\nDahlgren’s Federal cavalry patrol galloped into Greencastle’s\ntown square, where they surprised and captured several Con-\nfederate cavalrymen carrying vital correspondence from Rich-\nmond.”, the description 1 is: “Military personnel thwarting an\nenemy’s attempt to convey vital documents.” The description\n2 is: “The disruption of a communication exchange in a rural\narea.” The description 3 is: “A dramatic, unexpected event\noccurring in a town square during a battle.”\nLiu and Soatto (2024) multi-modally calculate\nsimilarity values between texts through the respec-\ntive imagery they evoke, using denoising through\nStochastic Differential Equations (Song et al.,\n2021). Essentially, the idea is that two texts are\nmore similar if they evoke more similar imagery,\nallowing for visual interpretation of the score.\n4.3\nChallenges and Opportunities\nSet-based approaches allow an interpretable align-\nment of token-level embeddings. This alone has\nuseful applications; for instance, to elicit token-\nlevel semantic differences between related docu-\nments (Vamvas and Sennrich, 2023). Sets also\nallow an intuitive view on asymmetric text relation-\nships, increasing their explanation appeal in asym-\nmetric tasks like NLI. However, it is crucial to note\nthat token embedding alignment does not equate\nto input token alignment, as the contextualization\nsteps may obscure the actual contributions of input\ntokens and any thereupon based explanation.\nWe also saw that we can abstract from sets of to-\nken embeddings to sets of text embeddings, e.g., by\ndecomposing a text into smaller statements, before\ngenerating embeddings. At the cost of a greater\nnumber of inferences, the interpretability potential\nof this class of explicit multi-interpretation based\napproaches is that they can deliver evidence about\nwhich statements conveyed by the texts are actually\nmatching and contributing to the overall similarity.\n5\nAttribution-based Interpretabilty\n5.1\nIdea\nAttribution-based approaches aim at attributing a\nmodel prediction onto input or intermeditate fea-\nture representations. In other words: they assign\nimportance values to features for how much they\ncontribute to a given prediction. However, a special\ncharacteristic of similarity models is that their pre-\ndictions do not depend on individual features, due\nto the multiplicative interaction between the two in-\nputs’ embeddings in sim. Thus first-order methods\ndo not suffice (Sundararajan et al., 2020; Janizek\net al., 2021). Instead, second-order methods are re-\nquired to attribute predictions of similarity models.\n5.2\nApproaches\nTwo lines of work have addressed this issue in\ntext similarity models: integrated Jacobians, an\nextension of the theory behind integrated gradients\nto Siamese models (Moeller et al., 2023, 2024) and\n5\n\nFigure 4: Interaction-attributions between two sentences\ncomputed with the IG method. The sim is 0.618 and\nthe attribution error is 0.001 for N=50 integration steps.\nBiLRP that uses layer-wise relevance propagation\nfor this model class (Vasileiou and Eberle, 2024).\n5.2.1\nIntegrated Jacobians\nIntegrated gradients (IG) attributes a scalar model\nprediction back onto individual input features by in-\ntegrating over a number of interpolations between\nthe actual input and an uninformative reference in-\nput (Sundararajan et al., 2017). The method can\nprovide a closed-form solution to explaining the\ndifference in the model prediction between the ref-\nerence and the actual input. Its output takes the\nform of a vector with importance values for all in-\nput features. Moeller et al. (2023) have applied\nthe underlying theory of IG to Siamese encoders,\nenabling the attribution of similarity predictions\nonto feature-interactions between the two inputs.\nDifferent from IG, the output takes the form of a\nfeature-pair attribution matrix. For text encoder\nmodels it can be reduced to a token-token matrix,\nshowing the contribution of token interactions to\nthe sim (Figure 4). The authors provide an exact\nversion of their attributions, which guarantees that\nthe sum over the attribution matrix must exactly\nequal the predicted similarity score. This version\nalso enables the quantification of an attribution er-\nror, however, it requires an adjustment of the mod-\nels through fine-tuning. Alternatively, approximate\nattributions can be calculated for any off-the-shelf\nmodel without a need to adjust it (Moeller et al.,\n2024). The approximation has been shown to cor-\nrelate sufficiently with the exact variant.\n5.2.2\nBiLRP\nLayer-wise relevance propagation (LRP) is a frame-\nwork to propagate feature-importance values for a\nmodel prediction back through the model in a layer-\nwise fashion (Bach et al., 2015; Montavon et al.,\n2019). Propagation rules are derived for individ-\nual layers based on first-order Taylor expansion of\nthe underlying function. Thus, the approach es-\nsentially linearly approximates all computations\nin a model’s graph around a given reference input.\nBiLRP extends the LRP framework to Siamese sim-\nilarity models by computing LRP values for each\nembedding dimension of the two encoders sepa-\nrately and subsequently taking their matrix product.\nThus, the computation also takes the form of a prod-\nuct between two Jacobian-like matrices. Whereas,\nintegrated Jacobians constructs these matrices by\nintegrating over interpolated inputs, in BiLRP they\noriginate from the layer-wise propagation rules.\nThe method was originally proposed in the com-\nputer vision domain (Eberle et al., 2020) and has\nrecently also been applied to Siamese text encoder\nmodels (Vasileiou and Eberle, 2024).\n5.3\nChallenges and Opportunities\nAttribution approaches need to build Jacobian ma-\ntrices, coming at a temporal complexity of 2×D\nindependent backward passes, D being the model’s\nembedding dimensionality. The resulting Jacobians\nhave a quadratic spatial complexity of D ×Din.\nWith Din being a sequential representation, the re-\nquired memory can grow quickly for higher D or\nlong inputs, requiring large GPUs to compute the\nassociated matrix multiplications efficiently.\nInspite of these computational costs, attribution\nbased methods have the advantage of being model\nagnostic, not requiring additional design choices\non model architectures or training objectives. In\ncontrast to (last-layer-embedding-)set based ap-\nproaches like ColBERT, they also relate more di-\nrectly to the actual input tokens. Different from\nspace shaping approaches they also do not pose\nany constraints on embeddings during training.\n6\nDiscussion\n6.1\nSummary and Overview of Approaches\nWe identify general features that allow for sum-\nmarizing and comparing similarity interpretability\napproaches from a broader perspective. We pro-\npose the following key features for categorization:\n6\n\nPaper\nType\nSubtype\nTrain\nApprox.\nInf. Cost\ncode\nSun et al. (2024)\nspace-shaping\nQA-feature\nyes\nno\nO(n)\ngithub\nBenara et al. (2024)\nspace-shaping\nQA-feature\nyes\nno\nO(n)\ngithub\nOpitz and Frank (2022)\nspace-shaping\nsub-embedding\nyes\nyes\nO(n)\ngithub\nRisch et al. (2021)\nspace shaping\nsub-embedding\nyes\nno\nO(n)\ngithub\nSchopf et al. (2023)\nspace shaping\nsub-embedding\nyes\nno\nO(nk)\nNA\nPonwitayarat et al. (2024)\nspace-shaping\nsub-embedding\nyes\nno\nO(n)\ngithub\nHuang et al. (2023)\nspace-shaping\nnon-Euclidean space\nyes\nyes\nO(n2)\ngithub\nChheda et al. (2021)\nspace-shaping\nnon-Euclidean space\nyes\nno\nO(n)\ngithub\nShen et al. (2023)\nspace-shaping\nnon-Euclidean space\nno\nyes\nO(nk)\nNA\nYoda et al. (2024)\nspace-shaping\nnon-Euclidean space\nyes\nno\nO(n)\ngithub\nWang and Kuo (2020)\nspace-shaping\ncombination\nno\nno\nO(n)\ngithub\nSeo et al. (2022)\nspace-shaping\ncombination\nyes\nno\nO(n)\nNA\nMoeller et al. (2023, 2024)\ntoken-attribution\nintegrated gradients\nno\nyes\nO(n2)\ngithub\nVasileiou and Eberle (2024)\ntoken-attribution\nrelevance propagation\nno\nyes\nO(n2)\ngithub\nKhattab and Zaharia (2020); Santhanam et al. (2022)\nset-based\ntoken-set\nno\nyes\nO(nk)\ngithub\nHoyle et al. (2023)\nset-based\ntext-set\nno\nno\nO(nk)\ngithub\nRavfogel et al. (2024)\nset-based\ntext-set\nno\nno\nO(nk)\ngithub\nLiu et al. (2024)\nset-based\ntext set\nno\nno\nO(nk)\ngithub\nLiu and Soatto (2024)\nset-based\nimage-set\nyes\nno\nO(nk)\nNA\nTable 1: Overview and broader differentiation of approaches.\n• Type: The three overarching categories that we\nused to structure the interpretability landscape.\n• Subtype: Our finer sub-classification.\n• Train: If the method requires training.\n• Approx(imative): Refers to whether a method\napproximates the similarity score of a reference\nembedding model. This ensures the interpretabil-\nity method’s similarities are predictable.\n• Inf(erence) Cost: An estimate of computational\ncost in terms for inferring pairwise explanations\nin a data set of size n. We use k to denote other\npotential relevant parameters, e.g., the maximum\nsize of encountered token sets, the number of\nencoder calls (if not one).\nA classification of the visited approaches according\nto this taxonomy is shown in Table 1.\n6.2\nPertinent Challenges\nMitigate tradeoffs.\nMethods differ in their con-\nceptualization of interpretability, computational\ncost, fidelity to input tokens, and eventual depen-\ndencies to a specific model as their basis. Space\nshaping is highly adaptive and allows to express\ndifferent semantic aspects of interest, or model in-\ntuitive relations with non-Euclidean spaces —but\ntends to require custom training and definitions\nthat may not generalize. Then, methods that com-\npute the similarity from two sets of embeddings\n(token embeddings, or text embeddings) can pro-\nduce a visually inspectable alingment and allow\nfor interpretable asymmetric matching with set-\noperators —but the last layer’s embeddings are\nhighly contextualized and thus are technically no\nlonger bound to the input tokens at the given posi-\ntion, limiting the interpretability and even bearing\nthe risk for potential deceptions from alignment\ninspection. Given the above limitations, space-\nshaping and set-based approaches, however, result\nin inherently interpretable models as opposed to re-\nquiring post-hoc explainability to gain insights into\ntheir prediction mechanisms (Rudin, 2019). In con-\ntrast, attribution-based methods can mitigate the\ncontextualization issue as they attribute predictions\nto earlier representations. They also generalize to\narbitrary models as long as they are differentiable\nand do not require modification and training of\nmodels. Different from other explainability meth-\nods, they can provide a set of theoretical guarantees\n(Sundararajan et al., 2017; Janizek et al., 2021), e.g.\nthat attributions must sum to the prediction score\n(Moeller et al., 2023). However, it has been shown\nthat nevertheless there remain fundamental limi-\ntations in their faithfulness (Bilodeau et al., 2024)\ncoming at a trade-off between higher computational\nexpenses but general applicability to any model.\nWhat’s the “right” explanation?\nUnfortunately\nthere may be no straightforward answer to this\nquestion. Given the above trade-offs, no method\ncan be seen to be guaranteed faithful (Murdoch\net al., 2019). Therefore, none of these approaches\nshould be interpreted as true and unique explana-\ntions for model predictions. At the same time all of\nthem provide insights into similarity models going\nbeyond a single scalar similarity score. We argue,\nthat while we cannot conclude individual methods\n7\n\nto be unambiguous, we can still use them to gain\ndeeper insights into model mechanisms which may\nlead to hypotheses about where these models fail\nand how they may be improved (Wiegreffe and Pin-\nter, 2019). Rather than competing for the best ex-\nplanation, which may not exist, we suggest to take\nindividual methods as independent pieces of evi-\ndence for a given phenomenon. By now, we have,\nfor instance, multiple pieces of evidence for the\nhypothesis that text similarity models do not suf-\nficiently account for negation (Weller et al., 2024;\nMoeller et al., 2024; Nikolaev and Padó, 2023).\nWhich method to consult will also depend on\nthe context in which a model is used. Higher-level\nhuman interpretable aspects of similarity may be\nbest explained by the abstract meaning represen-\ntations in an “S3BERT” model (Opitz and Frank,\n2022). Quickly accessible information about which\nparts of a query and a document are matched in an\nIR model without requiring back propagation, may\nbe best achieved by a set-based ColBERT model\n(Santhanam et al., 2022). However, if we require\ndense representations to build a memory-efficient\nvector-index that enables approximate search and\nwe don’t want to constrain the model otherwise, an\nattribution-based approach like BiLRP may be the\nright choice (Vasileiou and Eberle, 2024). If, addi-\ntionally, we require a statement of how trustworthy\nthe explanations are the exact variant of IJ would\nbe an apt choice (Moeller et al., 2023).\nOther challenges.\nAs models become capable of\ningesting longer context (Zhang et al., 2024; Xiong\net al., 2024), we may wonder if interpretability ap-\nproaches transfer to explaining the similarity of\nlong documents. We speculate that some patterns\nmay indeed generalize, especially more abstract\nones, like the topics that occur in them. Other axes\nof similarity may be style-related (“both books are\nwritten in 18th century British English”), or stance\nrelated (“both arguments are in favor of green en-\nergy”). Fine-grained explanations like token attri-\nbutions may necessitate a meta explanation step.\nThe embedding research landscape has also\nfound another recent focus in multi-linguality\n(Wang et al., 2024b). Since many facets of text\nsemantics appear universal, from coarser attributes\nlike entities or topics, to finer ones, like semantic\nroles and polarity, we see a fruitful venue in study-\ning language phenomena cross-lingually through\nthe eyes of interpretable embedding models, or test-\ning hypotheses about universal semantics.\n7\nMore Explanations and Related Work\nIn this last section, we study related work on simi-\nlarity interpretability. We want to focus on evalua-\ntion studies and datasets that elicit explanations.\nDatasets.\nLopez-Gazpio et al. (2017) release\nthe i(nterpretable)STS data set that elicits rela-\ntions and similarities between individual segments\nof texts.\nDeshpande et al. (2023) propose the\nC(onditional)STS dataset that elicits similarity val-\nues for specific aspect of interest. The theory that\nunderlies iSTS aligns with attribution or set-based\napproaches, while CSTS is motivated by a more\nabstract multi-aspect view akin to what is sought\nby feature-based explainability methods.\nThe STS3k data set (Fodor et al., 2024) con-\nsists of systematically controlled pairs of sentences\nrated for semantic similarity by human participants.\nThrough experiments on STS3k they find that\n“state-of-the-art transformers poorly capture the pat-\ntern of human semantic similarity judgments.”\nDatasets might also be repurposed for inter-\npretability studies. E.g., evaluation data that high-\nlight error spans (Freitag et al., 2021; Leiter et al.,\n2023), or explained interactions between text spans\nin NLI (Ray Choudhury et al., 2023).\nSimilarity interpretability studies.\nNikolaev\nand Padó (2023) construct sets of sentences with\npre-defined lexical and syntactic structures. Their\nstudy reveals that model-assigned similarities are\nmore strongly determined by the overlap in the set\nof their noun participants than by having the same\npredicates, lengthy nominal modifiers, or adjuncts.\nWeller et al. (2024) ask similarity models to\nrank documents that differ only by negation. They\nfind that most current information retrieval models\ndo not consider negation, performing similarly or\nworse than randomly ranking.\nNastase and Merlo (2024) track linguistic in-\nformation in embedding models via specialized\ndatasets that test for grammatical and semantic\nagreement, finding that aspectual semantic knowl-\nedge can be localized in certain embedding regions.\n8\nConclusion\nWhat makes two texts similar in the eyes of a\nmodel? We gave an introduction and overview\nof an emerging branch of text embedding models:\nThe challenge of similarity interpretability and ex-\nplanation. We hope that our work can be a handy\nresource and entrance point for future research.\n8\n\nLimitations\nCapturing the full breadth of the emerging area\nof interpretable text embeddings and their similar-\nity cost us some depth and exactness. Particularly\nin §6 where we summarized the methods over all\nthree general interpretability approaches, we intro-\nduced some fuzzy and coarse concepts, e.g., “token-\nset” (rather: token-embedding set [from the last\nlayer]); k in inference cost (sometimes the size of\nencountered token or embedding sets, sometimes\nit’s model encoder calls), etc. Nevertheless, the\nslight fuzziness also helped us to discuss and com-\npare the diverse methods that have varying degrees\nof complexity—simple set matching to second-\norder attribution methods based on Jacobians of\nencoders—from a thousand foot view. There is\nalso a chance that we missed some papers, hence\nwe suggest viewing our primer as a guide and a sur-\nvey that is representative of the area, but possibly\nnot fully exhaustive.\nAbout the three examples shown in our paper:\nThey are selected to outline the idea behind three\ndifferent types of approaches. We selected different\ntext pairs to highlight these ideas, in order to avoid\nany possible potential for invoking example read-\nings that favor one method over the other, which\nis not possible based on a single example, anyway.\nWe leave deeper qualitative comparison of provided\nexplanations to future work, and discussed the need\nfor this in §6.2.\nAcknowledgments\nThree authors received funding from the Swiss Na-\ntional Science Foundation (SNSF 213585) and the\nLuxembourg National Research Fund (17498891)\nthrough the Impresso research project.\nReferences\nRawaa Alatrash, Mohamed Amine Chatti, Qurat Ul Ain,\nYipeng Fang, Shoeb Joarder, and Clara Siepmann.\n2024. Conceptgcn: Knowledge concept recommen-\ndation in moocs based on knowledge graph convolu-\ntional networks and sbert. Computers and Education:\nArtificial Intelligence, 6:100193.\nSebastian Bach, Alexander Binder, Grégoire Montavon,\nFrederick Klauschen, Klaus-Robert Müller, and Wo-\njciech Samek. 2015.\nOn pixel-wise explanations\nfor non-linear classifier decisions by layer-wise rele-\nvance propagation. PloS one, 10(7):e0130140.\nLaura Banarescu, Claire Bonial, Shu Cai, Madalina\nGeorgescu, Kira Griffitt, Ulf Hermjakob, Kevin\nKnight, Philipp Koehn, Martha Palmer, and Nathan\nSchneider. 2013. Abstract Meaning Representation\nfor sembanking. In Proceedings of the 7th Linguistic\nAnnotation Workshop and Interoperability with Dis-\ncourse, pages 178–186, Sofia, Bulgaria. Association\nfor Computational Linguistics.\nVinamra Benara, Chandan Singh, John X Morris,\nRichard Antonello, Ion Stoica, Alexander G Huth,\nand Jianfeng Gao. 2024. Crafting interpretable em-\nbeddings by asking llms questions. arXiv preprint\narXiv:2405.16714.\nBlair Bilodeau, Natasha Jaques, Pang Wei Koh, and\nBeen Kim. 2024. Impossibility theorems for feature\nattribution. Proceedings of the National Academy of\nSciences, 121(2):e2304406120.\nAlexander Budanitsky and Graeme Hirst. 2006. Eval-\nuating wordnet-based measures of lexical semantic\nrelatedness. Computational linguistics, 32(1):13–47.\nNitay Calderon and Roi Reichart. 2024. On behalf of\nthe stakeholders: Trends in nlp model interpretability\nin the era of llms. arXiv preprint arXiv:2407.19200.\nAsli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao.\n2020. Evaluation of text generation: A survey. arXiv\npreprint arXiv:2006.14799.\nDaniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-\nGazpio, and Lucia Specia. 2017.\nSemEval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation.\nIn Proceedings\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1–14, Vancouver,\nCanada. Association for Computational Linguistics.\nTejas Chheda, Purujit Goyal, Trang Tran, Dhruvesh\nPatel, Michael Boratko, Shib Sankar Dasgupta, and\nAndrew McCallum. 2021. Box embeddings: An\nopen-source library for representation learning using\ngeometric structures. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing: System Demonstrations, pages\n203–211, Online and Punta Cana, Dominican Repub-\nlic. Association for Computational Linguistics.\nShamil Chollampatt, Minh Quang Pham, Sathish Reddy\nIndurthi, and Marco Turchi. 2025. Cross-lingual eval-\nuation of multilingual text generation. In Proceed-\nings of the 31st International Conference on Compu-\ntational Linguistics, pages 7766–7777, Abu Dhabi,\nUAE. Association for Computational Linguistics.\nShib Dasgupta, Michael Boratko, Dongxu Zhang, Luke\nVilnis, Xiang Li, and Andrew McCallum. 2020. Im-\nproving local identifiability in probabilistic box em-\nbeddings. Advances in Neural Information Process-\ning Systems, 33:182–192.\nAmeet Deshpande, Carlos Jimenez, Howard Chen,\nVishvak Murahari, Victoria Graf, Tanmay Rajpuro-\nhit, Ashwin Kalyan, Danqi Chen, and Karthik\nNarasimhan. 2023. C-STS: Conditional semantic\n9\n\ntextual similarity. In Proceedings of the 2023 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 5669–5690, Singapore. Associa-\ntion for Computational Linguistics.\nOliver Eberle, Jochen Büttner, Florian Kräutli, Klaus-\nRobert Müller, Matteo Valleriani, and Grégoire Mon-\ntavon. 2020. Building and interpreting deep similar-\nity models. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 44(3):1149–1161.\nJames Fodor, Simon De Deyne, and Shinsuke Suzuki.\n2024. Compositionality and sentence meaning: Com-\nparing semantic parsing and transformers on a chal-\nlenging sentence similarity dataset. Computational\nLinguistics, pages 1–52.\nMarkus Freitag, George Foster, David Grangier, Viresh\nRatnakar, Qijun Tan, and Wolfgang Macherey. 2021.\nExperts, errors, and context: A large-scale study of\nhuman evaluation for machine translation. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:1460–1474.\nYarin Gal and Zoubin Ghahramani. 2016. Dropout as\na bayesian approximation: Representing model un-\ncertainty in deep learning. In Proceedings of The\n33rd International Conference on Machine Learn-\ning, volume 48 of Proceedings of Machine Learning\nResearch, pages 1050–1059, New York, New York,\nUSA. PMLR.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimCSE: Simple contrastive learning of sentence em-\nbeddings. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 6894–6910, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,\nJinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen\nWang. 2023. Retrieval-augmented generation for\nlarge language models: A survey. arXiv preprint\narXiv:2312.10997.\nMaarten Grootendorst. 2022. Bertopic: Neural topic\nmodeling with a class-based tf-idf procedure. arXiv\npreprint arXiv:2203.05794.\nMichael Günther, Louis Milliken, Jonathan Geuter,\nGeorgios Mastrapas, Bo Wang, and Han Xiao. 2023.\nJina embeddings: A novel set of high-performance\nsentence embedding models. In Proceedings of the\n3rd Workshop for Natural Language Processing Open\nSource Software (NLP-OSS 2023), pages 8–18, Sin-\ngapore. Association for Computational Linguistics.\nJiafeng Guo, Yixing Fan, Liang Pang, Liu Yang,\nQingyao Ai, Hamed Zamani, Chen Wu, W. Bruce\nCroft, and Xueqi Cheng. 2020. A deep look into\nneural ranking models for information retrieval. In-\nformation Processing & Management, 57(6):102067.\nKailash A Hambarde and Hugo Proenca. 2023. Infor-\nmation retrieval: recent advances and beyond. IEEE\nAccess.\nAlexander Hoyle, Rupak Sarkar, Pranav Goel, and\nPhilip Resnik. 2023. Natural language decompo-\nsitions of implicit content enable better text repre-\nsentations. In Proceedings of the 2023 Conference\non Empirical Methods in Natural Language Process-\ning, pages 13188–13214, Singapore. Association for\nComputational Linguistics.\nJames Y. Huang, Wenlin Yao, Kaiqiang Song, Hong-\nming Zhang, Muhao Chen, and Dong Yu. 2023.\nBridging continuous and discrete spaces:\nInter-\npretable sentence representation learning via com-\npositional operations. In Proceedings of the 2023\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 14584–14595, Singapore.\nAssociation for Computational Linguistics.\nFlorentina Bunea Jacob Bien and Luo Xiao. 2016. Con-\nvex banding of the covariance matrix. Journal of the\nAmerican Statistical Association, 111(514):834–845.\nPMID: 28042189.\nJoseph D Janizek, Pascal Sturmfels, and Su-In Lee.\n2021. Explaining explanations: Axiomatic feature\ninteractions for deep networks. Journal of Machine\nLearning Research, 22(104):1–54.\nOmar Khattab and Matei Zaharia. 2020. Colbert: Effi-\ncient and effective passage search via contextualized\nlate interaction over bert. In Proceedings of the 43rd\nInternational ACM SIGIR Conference on Research\nand Development in Information Retrieval, SIGIR\n’20, page 39–48, New York, NY, USA. Association\nfor Computing Machinery.\nGregory Koch, Richard Zemel, Ruslan Salakhutdinov,\net al. 2015. Siamese neural networks for one-shot\nimage recognition. In ICML deep learning workshop,\n1, pages 1–30. Lille.\nPeter Kolb. 2009. Experiments on the difference be-\ntween semantic similarity and relatedness. In Pro-\nceedings of the 17th Nordic conference of computa-\ntional linguistics (NODALIDA 2009), pages 81–88.\nMatt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Wein-\nberger. 2015. From word embeddings to document\ndistances. In International conference on machine\nlearning, pages 957–966. PMLR.\nDaniil Larionov, Jens Grünwald, Christoph Leiter, and\nSteffen Eger. 2023. EffEval: A comprehensive eval-\nuation of efficiency for MT evaluation metrics. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2023, pages 78–96, Singapore. As-\nsociation for Computational Linguistics.\nSeonghyeon Lee, Dongha Lee, Seongbo Jang, and\nHwanjo Yu. 2022. Toward interpretable semantic tex-\ntual similarity via optimal transport-based contrastive\nsentence learning. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 5969–5979,\nDublin, Ireland. Association for Computational Lin-\nguistics.\n10\n\nChristoph Leiter, Juri Opitz, Daniel Deutsch, Yang Gao,\nRotem Dror, and Steffen Eger. 2023. The eval4nlp\n2023 shared task on prompting large language models\nas explainable metrics. In Proceedings of the 4th\nWorkshop on Evaluation and Comparison of NLP\nSystems, pages 117–138.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, Sebastian Riedel, and Douwe Kiela. 2020.\nRetrieval-augmented generation for knowledge-\nintensive nlp tasks. In Advances in Neural Infor-\nmation Processing Systems, volume 33, pages 9459–\n9474. Curran Associates, Inc.\nZehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long,\nPengjun Xie, and Meishan Zhang. 2023. Towards\ngeneral text embeddings with multi-stage contrastive\nlearning. arXiv preprint arXiv:2308.03281.\nTian Yu Liu and Stefano Soatto. 2024. Conjuring se-\nmantic similarity. arXiv preprint arXiv:2410.16431.\nTian Yu Liu, Matthew Trager, Alessandro Achille, Pra-\nmuditha Perera, Luca Zancato, and Stefano Soatto.\n2024. Meaning representations from trajectories in\nautoregressive models. In The Twelfth International\nConference on Learning Representations.\nI. Lopez-Gazpio, M. Maritxalar, A. Gonzalez-Agirre,\nG. Rigau, L. Uria, and E. Agirre. 2017. Interpretable\nsemantic textual similarity: Finding and explaining\ndifferences between sentences. Knowledge-Based\nSystems, 119:186–199.\nAndrianos Michail, Simon Clematide, and Juri Opitz.\n2025. PARAPHRASUS: A comprehensive bench-\nmark for evaluating paraphrase detection models. In\nProceedings of the 31st International Conference on\nComputational Linguistics, pages 8749–8762, Abu\nDhabi, UAE. Association for Computational Linguis-\ntics.\nLucas Moeller, Dmitry Nikolaev, and Sebastian Padó.\n2023. An attribution method for Siamese encoders.\nIn Proceedings of the 2023 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n15818–15827, Singapore. Association for Computa-\ntional Linguistics.\nLucas Moeller, Dmitry Nikolaev, and Sebastian Padó.\n2024.\nApproximate attributions for off-the-shelf\nSiamese transformers. In Proceedings of the 18th\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 2059–2071, St. Julian’s, Malta. Asso-\nciation for Computational Linguistics.\nGrégoire Montavon, Alexander Binder, Sebastian\nLapuschkin, Wojciech Samek, and Klaus-Robert\nMüller. 2019. Layer-wise relevance propagation: an\noverview. Explainable AI: interpreting, explaining\nand visualizing deep learning, pages 193–209.\nNiklas Muennighoff. 2022.\nSgpt:\nGpt sentence\nembeddings for semantic search.\narXiv preprint\narXiv:2202.08904.\nW James Murdoch, Chandan Singh, Karl Kumbier,\nReza Abbasi-Asl, and Bin Yu. 2019. Definitions,\nmethods, and applications in interpretable machine\nlearning. Proceedings of the National Academy of\nSciences, 116(44):22071–22080.\nVivi Nastase and Paola Merlo. 2024. Tracking linguis-\ntic information in transformer-based sentence embed-\ndings through targeted sparsification. In Proceedings\nof the 9th Workshop on Representation Learning for\nNLP (RepL4NLP-2024), pages 203–214, Bangkok,\nThailand. Association for Computational Linguistics.\nDmitry Nikolaev and Sebastian Padó. 2023. Represen-\ntation biases in sentence transformers. In Proceed-\nings of the 17th Conference of the European Chap-\nter of the Association for Computational Linguistics,\npages 3701–3716, Dubrovnik, Croatia. Association\nfor Computational Linguistics.\nJuri Opitz. 2023. SMATCH++: Standardized and ex-\ntended evaluation of semantic graphs. In Findings\nof the Association for Computational Linguistics:\nEACL 2023, pages 1595–1607, Dubrovnik, Croatia.\nAssociation for Computational Linguistics.\nJuri Opitz and Anette Frank. 2022. SBERT studies\nmeaning representations: Decomposing sentence em-\nbeddings into explainable semantic features. In Pro-\nceedings of the 2nd Conference of the Asia-Pacific\nChapter of the Association for Computational Lin-\nguistics and the 12th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 625–638, Online only. Association for\nComputational Linguistics.\nMalte Ostendorff, Till Blume, Terry Ruas, Bela Gipp,\nand Georg Rehm. 2022. Specialized document em-\nbeddings for aspect-based similarity of research pa-\npers. In Proceedings of the 22nd ACM/IEEE Joint\nConference on Digital Libraries, JCDL ’22, New\nYork, NY, USA. Association for Computing Machin-\nery.\nMoritz Plenz, Juri Opitz, Philipp Heinisch, Philipp Cimi-\nano, and Anette Frank. 2023. Similarity-weighted\nconstruction of contextualized commonsense knowl-\nedge graphs for knowledge-intense argumentation\ntasks. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 6130–6158, Toronto,\nCanada. Association for Computational Linguistics.\nWuttikorn\nPonwitayarat,\nPeerat\nLimkonchotiwat,\nEkapol Chuangsuwanich, and Sarana Nutanong.\n2024. Space decomposition for sentence embedding.\nIn Findings of the Association for Computational Lin-\nguistics: ACL 2024, pages 11227–11239, Bangkok,\nThailand. Association for Computational Linguistics.\nShauli Ravfogel, Valentina Pyatkin, Amir David Nissan\nCohen, Avshalom Manevich, and Yoav Goldberg.\n11\n\n2024. Description-based text similarity. In First\nConference on Language Modeling.\nSagnik Ray Choudhury, Pepa Atanasova, and Isabelle\nAugenstein. 2023. Explaining interactions between\ntext spans. In Proceedings of the 2023 Conference\non Empirical Methods in Natural Language Process-\ning, pages 12709–12730, Singapore. Association for\nComputational Linguistics.\nNils Reimers and Iryna Gurevych. 2019.\nSentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics.\nMarco Tulio Ribeiro, Sameer Singh, and Carlos\nGuestrin. 2016. \" why should i trust you?\" explaining\nthe predictions of any classifier. In Proceedings of\nthe 22nd ACM SIGKDD international conference on\nknowledge discovery and data mining, pages 1135–\n1144.\nJulian Risch, Philipp Hager, and Ralf Krestel. 2021.\nMultifaceted domain-specific document embeddings.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies:\nDemonstrations, pages 78–83, Online. Association\nfor Computational Linguistics.\nCynthia Rudin. 2019. Stop explaining black box ma-\nchine learning models for high stakes decisions and\nuse interpretable models instead. Nature machine\nintelligence, 1(5):206–215.\nAnanya B. Sai, Akash Kumar Mohankumar, and\nMitesh M. Khapra. 2022. A survey of evaluation\nmetrics used for nlg systems. ACM Comput. Surv.,\n55(2).\nKeshav Santhanam, Omar Khattab, Jon Saad-Falcon,\nChristopher Potts, and Matei Zaharia. 2022. Col-\nBERTv2:\nEffective and efficient retrieval via\nlightweight late interaction. In Proceedings of the\n2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 3715–3734, Seat-\ntle, United States. Association for Computational\nLinguistics.\nTim Schopf, Daniel Braun, and Florian Matthes. 2022.\nEvaluating unsupervised text classification: zero-shot\nand similarity-based approaches.\nIn Proceedings\nof the 2022 6th International Conference on Natu-\nral Language Processing and Information Retrieval,\npages 6–15.\nTim Schopf, Emanuel Gerber, Malte Ostendorff, and\nFlorian Matthes. 2023. AspectCSE: Sentence em-\nbeddings for aspect-based semantic textual similarity\nusing contrastive learning and structured knowledge.\nIn Proceedings of the 14th International Conference\non Recent Advances in Natural Language Processing,\npages 1054–1065, Varna, Bulgaria. INCOMA Ltd.,\nShoumen, Bulgaria.\nJaejin Seo, Sangwon Lee, Ling Liu, and Wonik Choi.\n2022. Ta-sbert: Token attention sentence-bert for\nimproving sentence representation. IEEE Access,\n10:39119–39128.\nLingfeng Shen, Haiyun Jiang, Lemao Liu, and Shum-\ning Shi. 2023. Sen2Pro: A probabilistic perspective\nto sentence embedding from pre-trained language\nmodel. In Proceedings of the 8th Workshop on Repre-\nsentation Learning for NLP (RepL4NLP 2023), pages\n315–333, Toronto, Canada. Association for Compu-\ntational Linguistics.\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma,\nAbhishek Kumar, Stefano Ermon, and Ben Poole.\n2021.\nScore-based generative modeling through\nstochastic differential equations. In International\nConference on Learning Representations.\nYiqun Sun, Qiang Huang, Yixuan Tang, Anthony KH\nTung, and Jun Yu. 2024. A general framework for\nproducing interpretable semantic text embeddings.\narXiv preprint arXiv:2410.03435.\nMukund Sundararajan, Kedar Dhamdhere, and Ashish\nAgarwal. 2020. The shapley taylor interaction in-\ndex. In International conference on machine learn-\ning, pages 9259–9268. PMLR.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.\nAxiomatic attribution for deep networks. In Interna-\ntional conference on machine learning, pages 3319–\n3328. PMLR.\nStephan Tulkens and Thomas van Dongen. 2024.\nModel2vec: The fastest state-of-the-art static em-\nbeddings in the world. GitHub Repositories.\nSarah Uhrig, Yoalli Garcia, Juri Opitz, and Anette Frank.\n2021. Translate, then parse! a strong baseline for\ncross-lingual AMR parsing. In Proceedings of the\n17th International Conference on Parsing Technolo-\ngies and the IWPT 2021 Shared Task on Parsing\ninto Enhanced Universal Dependencies (IWPT 2021),\npages 58–64, Online. Association for Computational\nLinguistics.\nJannis Vamvas and Rico Sennrich. 2023. Towards un-\nsupervised recognition of token-level semantic dif-\nferences in related documents. In Proceedings of the\n2023 Conference on Empirical Methods in Natural\nLanguage Processing, pages 13543–13552, Singa-\npore. Association for Computational Linguistics.\nAlexandros Vasileiou and Oliver Eberle. 2024. Explain-\ning text similarity in transformer models. In Proceed-\nings of the 2024 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies (Volume\n1: Long Papers), pages 7859–7873, Mexico City,\nMexico. Association for Computational Linguistics.\n12\n\nBin Wang and C.-C. Jay Kuo. 2020. Sbert-wk: A sen-\ntence embedding method by dissecting bert-based\nword models. IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing, 28:2146–2157.\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing\nJiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\nand Furu Wei. 2022. Text embeddings by weakly-\nsupervised contrastive pre-training. arXiv preprint\narXiv:2212.03533.\nLiang Wang, Nan Yang, Xiaolong Huang, Linjun Yang,\nRangan Majumder, and Furu Wei. 2024a. Improv-\ning text embeddings with large language models. In\nProceedings of the 62nd Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 11897–11916, Bangkok, Thai-\nland. Association for Computational Linguistics.\nLiang Wang, Nan Yang, Xiaolong Huang, Linjun Yang,\nRangan Majumder, and Furu Wei. 2024b. Multilin-\ngual e5 text embeddings: A technical report. arXiv\npreprint arXiv:2402.05672.\nOrion Weller, Dawn Lawrie, and Benjamin Van Durme.\n2024. NevIR: Negation in neural information re-\ntrieval. In Proceedings of the 18th Conference of the\nEuropean Chapter of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), pages\n2274–2287, St. Julian’s, Malta. Association for Com-\nputational Linguistics.\nSarah Wiegreffe and Yuval Pinter. 2019. Attention is not\nnot explanation. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 11–20, Hong Kong, China. Association for\nComputational Linguistics.\nWenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang,\nPrajjwal Bhargava, Rui Hou, Louis Martin, Rashi\nRungta, Karthik Abinav Sankararaman, Barlas Oguz,\nMadian Khabsa, Han Fang, Yashar Mehdad, Sharan\nNarang, Kshitiz Malik, Angela Fan, Shruti Bhosale,\nSergey Edunov, Mike Lewis, Sinong Wang, and Hao\nMa. 2024. Effective long-context scaling of founda-\ntion models. In Proceedings of the 2024 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies (Volume 1: Long Papers), pages 4643–4663,\nMexico City, Mexico. Association for Computational\nLinguistics.\nXin Ye, Hui Shen, Xiao Ma, Razvan Bunescu, and\nChang Liu. 2016. From word embeddings to docu-\nment similarities for improved information retrieval\nin software engineering. In Proceedings of the 38th\ninternational conference on software engineering,\npages 404–415.\nShohei Yoda, Hayato Tsukagoshi, Ryohei Sasano, and\nKoichi Takeda. 2024. Sentence representations via\nGaussian embedding. In Proceedings of the 18th\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics (Volume 2: Short\nPapers), pages 418–425, St. Julian’s, Malta. Associa-\ntion for Computational Linguistics.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Eval-\nuating text generation with bert. In International\nConference on Learning Representations.\nXin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie,\nZiqi Dai, Jialong Tang, Huan Lin, Baosong Yang,\nPengjun Xie, Fei Huang, Meishan Zhang, Wenjie\nLi, and Min Zhang. 2024. mGTE: Generalized long-\ncontext text representation and reranking models for\nmultilingual text retrieval. In Proceedings of the 2024\nConference on Empirical Methods in Natural Lan-\nguage Processing: Industry Track, pages 1393–1412,\nMiami, Florida, US. Association for Computational\nLinguistics.\n13\n",
  "metadata": {
    "source_path": "papers/arxiv/Interpretable_Text_Embeddings_and_Text_Similarity_Explanation_A_Primer_d2dee1b5e801cf81.pdf",
    "content_hash": "d2dee1b5e801cf8161efa11b69a7a52ae8759891e6d2289dbd5b687833dcec28",
    "arxiv_id": null,
    "title": "Interpretable_Text_Embeddings_and_Text_Similarity_Explanation_A_Primer_d2dee1b5e801cf81",
    "author": "",
    "creation_date": "D:20250221020523Z",
    "published": "2025-02-21T02:05:23",
    "pages": 13,
    "size": 628017,
    "file_mtime": 1740346981.1856952
  }
}