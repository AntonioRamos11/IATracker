{
  "text": "FR-Spec: Accelerating Large-Vocabulary Language Models\nvia Frequency-Ranked Speculative Sampling\nWeilin Zhao1*, Tengyu Pan1∗, Xu Han1†, Yudi Zhang2, Ao Sun3, Yuxiang Huang1\nKaihuo Zhang4, Weilun Zhao4, Yuxuan Li1, Jianyong Wang1, Zhiyuan Liu1†, Maosong Sun1\n1Tsinghua University, Beijing, China. 2Harbin Institute of Technology, Harbin, China.\n3Beijing University of Posts and Telecommunications, Beijing, China. 4OpenBMB\n{zwl23,pty23}@mails.tsinghua.edu.cn, {han-xu,liuzy}@tsinghua.edu.cn\nAbstract\nSpeculative sampling has emerged as an im-\nportant technique for accelerating the auto-\nregressive generation process of large language\nmodels (LLMs) by utilizing a draft-then-verify\nmechanism to produce multiple tokens per for-\nward pass. While state-of-the-art speculative\nsampling methods use only a single layer and\na language modeling (LM) head as the draft\nmodel to achieve impressive layer compression,\ntheir efficiency gains are substantially reduced\nfor large-vocabulary LLMs, such as Llama-3-\n8B with a vocabulary of 128k tokens. To ad-\ndress this, we present FR-Spec, a frequency-\nranked speculative sampling framework that\noptimizes draft candidate selection through vo-\ncabulary space compression. By constraining\nthe draft search to a frequency-prioritized to-\nken subset, our method reduces LM Head com-\nputation overhead by 75% while ensuring the\nequivalence of the final output distribution. Ex-\nperiments across multiple datasets demonstrate\nan average of 1.12× speedup over the state-of-\nthe-art speculative sampling method EAGLE-2.\n1\nIntroduction\nLarge language models (LLMs) have revolution-\nized the field of artificial intelligence (AI), en-\nabling a wide range of applications from conver-\nsational AI to complex reasoning tasks (Brown\net al., 2020; OpenAI, 2022; Guo et al., 2025). Over\ntime, driven by the need to improve tokenization\nefficiency and support multilingual capabilities and\ndomain-specific terminologies, the standard vocab-\nulary size of LLMs has grown significantly, from\na vocabulary of 32k tokens used in Llama-2 (Tou-\nvron et al., 2023) to the much larger vocabularies\nadopted by recent mainstream models. Notable\nexamples include Llama-3 (Dubey et al., 2024)\nwith 128k vocabulary tokens, Qwen-2.5 (Yang\n* indicates equal contribution.\n† indicates corresponding authors.\nHuggingface\nSGLang\nOur Impl\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\n25.0\nTime (ms)\n3.0×\nLlama-2-7B (32k vocab)\nHuggingface\nSGLang\nOur Impl\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\n25.0\nTime (ms)\n1.8×\nLlama-3-8B (128k vocab)\nVerification Time\nDrafting Time\nFigure 1: Comparison of the drafting and verification\ntimes of EAGLE-2 implemented by three frameworks\n(Huggingface, SGLang, and our optimized implementa-\ntion) for two vocabulary sizes: 32k (Llama-2-7B) and\n128k (Llama-3-8B).\net al., 2024b) with 152k vocabulary tokens, and\nDeepSeek-V3 (Liu et al., 2024) with 129k vocab-\nulary tokens. While larger vocabularies enhance\nmodel capabilities (Takase et al., 2024; Tao et al.,\n2024), the side effect of a large vocabulary on the\ngeneration speed of LLMs remains unstudied.\nTo meet the demand for faster generation speed,\nspeculative sampling (Leviathan et al., 2023; Chen\net al., 2023) has emerged as a leading technique,\nparticularly for deploying LLMs on resource-\nrestricted devices such as PCs, laptops, and mo-\nbile phones. These methods, such as Medusa (Cai\net al., 2024) and EAGLE-2 (Li et al., 2024b), em-\nploy a two-stage draft-then-verify mechanism. In\neach iteration, a lightweight draft model first pre-\ndicts several draft sequences. Subsequently, the\ntarget LLM verifies the drafted tokens in parallel\nand accepts the longest correct subsequence match-\ning the LLM’s own predictions. This approach\nallows the LLM to validate multiple tokens in one\nforward pass. The recent state-of-the-art specu-\nlative sampling method, EAGLE-2, has made re-\n1\narXiv:2502.14856v1  [cs.CL]  20 Feb 2025\n\nmarkable progress in reducing the time required for\nthe drafting process, by employing an extremely\nlightweight architecture — the drafting process re-\nlies solely on a single-layer transformer. Despite\nits simplicity, EAGLE-2 achieves impressive draft-\ning quality, enabling accurate and efficient token\npredictions that significantly accelerate the overall\ngeneration process.\nAlthough speculative sampling has shown\npromising results, its research highly relies on the\nHuggingface framework for experimental speedup\nevaluation. As a result, the negative effects of large\nvocabularies are obscured due to Python overhead,\nCPU processing, and suboptimal operator imple-\nmentations. By implementing EAGLE-2 in native\nC and CUDA, we observed a substantial increase\nin drafting time when transitioning from small to\nlarge vocabulary models, as illustrated in Figure 1.\nTo tackle this challenge and achieve further\nspeedup, we introduce FR-Spec, a frequency-\nranked speculative sampling framework that opti-\nmizes draft candidate selection through vocabulary\nspace compression. Our key inspiration is drawn\nfrom the long-tailed distribution (Zipf, 1950) of\ntoken frequencies in natural languages, as depicted\nin Figure 2. This distribution indicates that a signif-\nicant portion of tokens in the vocabulary of LLMs\nare rarely used. By restricting the draft search to a\nfrequency-prioritized subset of high-probability to-\nkens, we reduce the computational overhead of the\nlanguage modeling (LM) Head by 75%. While this\nresults in a slight reduction in drafting accuracy, it\nsignificantly improves the overall generation speed.\nImportantly, FR-Spec preserves the mathematical\nequivalence of the verification process, ensuring\nthat the final output distribution remains unaltered\ncompared with the original sampling methods.\nOur contributions are summarized as follows:\n1. A Systematic Time Breakdown of Specu-\nlative Sampling. To address the current limita-\ntions where the bottleneck analyses of speculative\nsampling are either insufficiently explored or com-\nmonly rely on sub-optimized implementations (e.g.\nHuggingface Transformers), we develop a highly\noptimized implementation and conduct detailed\nprofiling. Surprisingly, our analysis reveals that the\nLM Head, rather than the transformer layers, is\nthe primary bottleneck in the drafting process.\n2. Frequency-Ranked Speculative Sampling.\nTo mitigate the computational cost of the LM Head,\nwe propose a frequency-prioritized subset of the\nvocabulary for the drafting process, while retaining\n0\n20000\n40000\n60000\n80000\n100000\n120000\nVocabulary (Sorted by token frequency)\n0\n1e-05\n2e-05\nToken Probability\n75% of the vocabulary\n95.3%\n4.7%\nFigure 2: Token frequency distribution, statistically an-\nalyzed using the tokenizer of Llama-3-8B on a subset\nof 1B tokens randomly sampled from the SlimPajama-\n627B dataset (Soboleva et al., 2023). As shown in the\nfigure, 75% of the vocabulary tokens account for less\nthan 5% of all token occurrences in the dataset, present-\ning a “Long Tail” effect.\nthe full vocabulary for verification. Our method,\nFR-Spec, is designed as a plug-and-play solution,\ncompatible with existing speculative sampling tech-\nniques and requiring no retraining. Our approach\nachieves an extra 1.12× speedup when integrated\nwith the current state-of-the-art method EAGLE-2\nand 1.08× speedup when integrated with Medusa.\n2\nPreliminary\nIn this section, we introduce the concept of specula-\ntive sampling by taking the state-of-the-art method\nEAGLE-2 (Li et al., 2024b) as an example. The fun-\ndamental principles and operations of EAGLE-2\ncan serve as a representative model; other specula-\ntive sampling methods follow similar logic and can\nrefer to the related work section (Section 5).\nAn LLM T with the vocabulary V consists of an\nembedding layer E, L layers of transformer blocks\nT (1)\nlayer, T (2)\nlayer, · · · , T (L)\nlayer, and an LM Head with the\nweight WLM ∈R|V|×d. The embedding layer is\nresponsible for mapping tokens x ∈Rn into a\nd-dimensional latent space. After using the trans-\nformer blocks to encode token embeddings, the\nLM Head projects the output representations back\ninto the vocabulary space. Finally, a softmax func-\ntion is applied to the vocabulary space to get output\ntoken probabilities. Overall, the model T can be\nrepresented as first calculating the last hidden state\nHT (x) ∈Rn×d, followed by the LM Head projec-\ntion and softmax computation to obtain the final\n2\n\nIt (1.0)\nis (0.6)\nhas (0.2)\na (0.48)\nthe (0.06)\nto (0.14)\na (0.02)\ngood (0.34)\nnice (0.05)\nbe (0.08)\ndo (0.03)\nLM Head\nDraft Layer 1\nEmbed\nLM Head\nDraft Layer 1\nEmbed\nLM Head\nDraft Layer 1\nEmbed\nDrafting Process of EAGLE-2\nDrafting Process of EAGLE-2 + FR\nDraft Layer 1\nEmbed\nDraft Layer 1\nEmbed\nDraft Layer 1\nEmbed\nLM\nLM\nLM\nFigure 3: (Left) The drafting process of EAGLE-2 when prompt P =“It”, beam width = 2 and search depth = 3.\nIt picks out the top K = 8 probability tokens (purple) as the draft tree. (Right) The drafting process of FR-Spec,\nwhere the LM Head is cropped during the drafting process while the beam search procedure remains the same.\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\nIt\nis\nhas\na\nthe\nto\ngood\nbe\nIt\nis\nhas\na\nthe\nto\ngood\nbe\nTarget Layer 1\nEmbed\nLM Head\nTarget Layer L\nTarget Layer 2\n⋯\nVerification Process of EAGLE-2 or FR-Spec using Attention Mask\nFigure 4: The illustration of the verification process\nfor EAGLE-2 and FR-Spec, given the draft in Figure 3.\nFR-Spec solely modifies the drafting process while the\nverification process remains consistent with EAGLE-2.\noutput token probabilities:\nHT (x) = T (L)\nlayer ◦· · · ◦T (2)\nlayer ◦T (1)\nlayer\n\u0000E(x)\n\u0001\n,\nT (x) = Softmax(HT (x) WT\nLM).\n(1)\nFor the LLM T , EAGLE-2 trains a lightweight\ndraft model D to approximate T ’s behavior while\ndrastically reducing computational overhead. The\ndraft model D is structured as a single-layer trans-\nformer, with its latent dimension d being identical\nto that of the target LLM. For the draft model, the\nparameters of its embedding layer and LM head\nare directly sourced from the target LLM and are\nfrozen during the training process. The transformer\nlayer of the draft model is then trained on some\ntraining data to make the draft model mimic the\ngeneration results of the target LLM. To summa-\nrize, D can be represented as calculating the hidden\nstate HD(x) ∈Rn×d, and conducting LM Head\nprojection:\nHD(x) = D(1)\nlayer(E(x)),\nD(x) = Softmax(HD(x) WT\nLM).\n(2)\nEAGLE-2 actually combines HT (x) from the tar-\nget LLM with E(x) on the draft input, but this\ndoes not affect the presentation of our paper, so the\nformula is simplified as Eq.(2) for clarity.\nDuring inference, given a specific prompt P,\nEAGLE-2 adopts a beam-search algorithm based\non the softmax output of the draft model to com-\nplete a drafting process. Given a beam width and\na search depth, EAGLE-2 uses the draft model D\nto forward depth times and then select the top K\nprobability tokens from the beam search history as\nthe draft. As illustrated in Figure 3 (left), EAGLE-2\nfinally generates a draft tree consisting of multiple\ndraft sequences, and the draft tree is then verified\nby the target LLM T using a tree attention mask\ndemonstrated in Figure 4. The special tree attention\nmask is created based on the draft tree, where each\ntoken can only see tokens in its ancestral path and\nin the prompt prefix P.\n3\nMethodology\n3.1\nIdentifying Key Bottlenecks for\nSpeculative Sampling\nTo gain deeper insights into the time breakdown\nof speculative sampling and quantify the contri-\nbution of each component, we first implement an\noptimized speculative sampling framework and em-\nploy profiling tools to analyze the key bottlenecks\nof EAGLE-2 under our optimized framework.\nFiltering out Non-Algorithmic Overheads. Be-\nfore conducting the analysis, it is crucial to rule\nout the analysis errors caused by sub-optimized\nframework implementations.\nFor instance, de-\nspite its widespread use and convenience, Python’s\ndynamic typing and interpreted nature can intro-\nduce inefficiencies that are not directly related to\nthe analyzed algorithms. For example, the beam\nsearch algorithm in EAGLE-2, characterized by\na large number of short-duration computational\ntasks, leads to significant latency issues in the orig-\ninal PyTorch (Paszke et al., 2019) implementation,\nas illustrated in Figure 5. Specifically, executing\nthese tasks requires frequent waiting for Python’s\nlaunch commands, making them one of the bottle-\nnecks. To mitigate this, we reimplement EAGLE-2\n3\n\nPython\nX\nPython\nY\nPython\nZ\nC\nX\nC\nY\nC\nZ\ntime\nCPU:\nGPU:\nCPU:\nGPU:\nFigure 5: Comparison of Python-based implementation\nand C-based implementation. X, Y, and Z represent\nthree different short-duration computational tasks.\nusing native C and CUDA and preallocate all re-\nquired memory in advance. This eliminates the\noverhead associated with Python’s interpreter. As\ndemonstrated in Figure 5, this optimization can\nsignificantly reduce latency and make the overall\nexecution more streamlined.\nAdditionally, suboptimal operator implemen-\ntations can introduce significant implementation-\nlevel overheads.\nWe thus modify FlashAtten-\ntion (Dao, 2023) to support complex tree attention\nmasks as in Figure 4. To minimize the performance\nimpact of memory access for attention masks, we\noptimize the process by transmitting only the por-\ntion of the mask corresponding to the draft tokens,\ngiven that the prompt prefix P is entirely causal.\nMoreover, since EAGLE-2 (and other speculative\nsampling methods) typically involves no more than\n64 draft tokens, we employ bitmask compression\nusing “uint64” to ensure more contiguous and com-\npact memory access patterns, thereby enhancing\noverall efficiency.\nWall Time Breakdown. Based on our optimized\nimplementation framework, we observe a substan-\ntial increase in drafting time when shifting from\nsmall vocabulary LLMs to large vocabulary LLMs,\nas in Figure 1. To investigate the underlying rea-\nsons for this, we conduct a comprehensive pro-\nfiling analysis on our proposed framework. As\nshown in Figure 6, the computational bottleneck\nin the drafting process has shifted from the trans-\nformer layer, which is traditionally considered time-\nconsuming, to the LM Head. The vocabulary size\ndirectly causes such a significant disparity associ-\nated with the LM Head component. Additionally,\nthe softmax function, which operates across the\ndimension of the vocabulary size, also exhibits a\nnotable increase in wall time.\nSpecifically, the profiling data indicates that the\nLM Head component accounts for a substantial\n49% of the total computational time in the drafting\nprocess, nearly half of the entire processing time.\nWhen accounting for the combined computation\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\ns\nLlama-3-8B\nLlama-2-7B\n33%\n49%\n13%\n5%\n62%\n24%\n7%6%\nEmbedding + Transformer Layer\nWeight Projection of the LM Head\nSoftmax on the LM Head Projection\nOthers\nFigure 6: Time breakdown of the drafting process of\nEAGLE-2. We profile the EAGLE-2 trained on Llama-\n2-7B (32k vocabulary) and the EAGLE-2 trained on\nLlama-3-8B (128k vocabulary).\ntime of the LM Head and the softmax operation,\nboth directly proportional to the vocabulary size,\nthe proportion increases to 62%. In contrast, the\ntransformer layer accounts for only 33% of the\ncomputation time. This indicates that vocabulary-\nrelated computations require nearly twice the time\nof the transformer layer’s operations.\nThese findings indicate that while a large vocab-\nulary has a relatively moderate impact on the speed\nof the LLM itself, the scenario shifts significantly\nwithin the speculative sampling framework. This\nis due to the highly lightweight architecture of the\ndrafting model, which follows a 1:1 ratio of one\ntransformer layer to one LM Head. This under-\nscores the importance of optimizing vocabulary-\nrelated operations to enhance the efficiency of spec-\nulative sampling in large vocabulary settings.\n3.2\nAddressing the Bottleneck Caused by\nLarge Vocabulary\nTo optimize for large-vocabulary scenarios, we\nconducted a corpus-level token-frequency analy-\nsis, which revealed that the vast majority of tokens\nhardly appear in the corpus, demonstrating a sparse\npattern across the vocabulary. We then utilize the\nsparse pattern to let the draft model focus exclu-\nsively on drafting high-probability tokens, while\ntokens with extremely low probabilities of occur-\nrence are left to be handled by the LLM.\nCorpus-Level Token Statistics. We begin by ana-\nlyzing the token frequency distribution across a pre-\ntraining corpus SlimPajama-627B (Soboleva et al.,\n2023). The data in the pre-training corpus encom-\npasses a vast amount of information from diverse\nfields. It is highly suitable for token-frequency\nanalysis. As illustrated in Figure 2, we use a 1\nbillion token subset of the pretraining corpus to\nget the corpus-level token statistics. Our statisti-\ncal study reveals a pronounced long-tail pattern: a\nsmall subset (25%) of tokens (e.g., common words,\n4\n\npunctuation, and general-domain terms) accounts\nfor the majority of occurrences (95%), while the\nremaining (75%) tokens exhibit sparse frequencies\n(5%). This observation motivates our core design:\nrestricting the draft model’s generation scope to\nthe small subset of high-frequency tokens can sig-\nnificantly accelerate the drafting process without\nsacrificing much draft quality.\nFR-Spec. We propose a frequency-ranked draft-\ning mechanism. Let V denote the full vocabulary\nof the language model, and Vhigh ⊂V represent the\nsubset of high-frequency tokens identified through\npreviously mentioned corpus-level statistics. At\neach generation step, instead of computing proba-\nbilities over the entire vocabulary, we restrict the\ndrafting model’s output distribution D(x) to Vhigh,\nas shown in Figure 3 (right). We only limit the\nvocabulary of the drafting process while keeping\nthe verification process untouched.\nTo this end, we first create a sub matrix ˜\nWLM ∈\nR|Vhigh|×d from WLM ∈R|V|×d, by letting\n˜\nWLM[i, :] = WLM[Vhigh[i], :], i = 1 . . . |Vhigh|.\n(3)\nThen we change the draft equation from Eq.(2) to\nDFR(x) = Softmax(HD(x) ˜\nWT\nLM)\n(4)\nAs can be seen, from changing Eq.(2) to Eq.(4),\nthe computational complexity of the LM Head pro-\njection is reduced from the original O(nd|V|) to\nO(nd|Vhigh|), achieving a reduction by a factor of\n|V|\n|Vhigh|. Additionally, the input dimension of Soft-\nmax is reduced from HD(x)WT\nLM ∈Rn×|V| to\nHD(x) ˜\nWT\nLM ∈Rn×|Vhigh|. The operation time of\nthe softmax function, proportional to the input size,\nis also decreased by a factor of\n|V|\n|Vhigh| when using a\nreduced vocabulary subset.\nBy using a small subset of the original vocab-\nulary, FR-Spec indicates a context-related accel-\neration paradigm: sequences dominated by high-\nfrequency tokens benefit from reduced computa-\ntional overheads. While those regions requiring\nlow-frequency tokens (e.g., rare named entities or\ntechnical terms) inherently bypass acceleration. We\nwill balance this tradeoff in the subsequent experi-\nment section and demonstrate that the benefits of\nour approach outweigh its drawbacks.\n4\nExperiments\nThis section focuses on evaluating FR-Spec on\nvarious tasks when applying to various large-\nvocabulary LLMs to demonstrate the efficiency and\neffectiveness of FR-Spec.\n4.1\nExperimental Settings\nDatasets. To comprehensively assess the speed\nperformance of various speculative sampling meth-\nods, we evaluate FR-Spec across seven represen-\ntative text generation tasks: machine translation\n(MT.), multi-turn conversation (Conv.), retrieval-\naugmented generation (RAG), arithmetic reasoning\n(Math), question answering (QA), document sum-\nmarization (Summ.), and code generation (Code).\nSpecifically, we adopt Spec-Bench (Xia et al.,\n2024) benchmark, a widely used benchmark for\nspeculative sampling, which covers the first six\nsubtasks, with datasets drawn from the following\nsources: Translation from WMT14 DE-EN (Bojar\net al., 2014), Multi-turn Conversation from MT-\nbench (Zheng et al., 2023), RAG and QA from Nat-\nural Questions (Kwiatkowski et al., 2019), Math\nfrom GSM8K (Cobbe et al., 2021), and Summariza-\ntion from CNN/Daily Mail (Nallapati et al., 2016),\nwith 80 entries per subtask. In addition, we include\nthe HumanEval (Chen et al., 2021) benchmark,\nwhich focuses on code generation tasks and con-\ntains 164 entries. Following Xia et al. (2024), we\nset the maximum generation lengths to 1024 for all\nsubtasks in Spec-Bench and 512 for HumanEval.\nModels. We select Llama-3-8B-Instruct (128k\nvocabulary) (Dubey et al., 2024), Llama-3.2-\n1B-Instruct (128k vocabulary) and Qwen-2-7B-\nInstruct (152k vocabulary) (Yang et al., 2024a) as\nthe language models for experiments. These mod-\nels are recently representative and popular LLMs.\nEvaluation Methods. We select vanilla autore-\ngressive decoding and EAGLE-2 as our baselines.\nWe integrate FR-Spec with EAGLE-2, which we\ncalled “EAGLE-2 (+FR)” later. We report the mean\nacceptance length and decoding speed (token/s).\nFollowing the settings in Spec-Bench (Xia et al.,\n2024), we set the search depth of EAGLE-2 to 6\nand the total amount of draft tokens to 60.\nHardware Settings. Experiments in this section\nare performed on 1 × NVIDIA 80GB A800 GPU.\nThe CPU used is the Intel(R) Xeon(R) Platinum\n8470. Experiments on other platforms can refer to\nAppendix A.2.\n4.2\nAccept Length\nTo thoroughly investigate the impact of the\nfrequency-ranked drafting mechanism on existing\nspeculative sampling frameworks, we conducted\n5\n\nConfiguration\nMT.\nConv.\nRAG\nMath\nQA\nSumm.\nCode\nAverage\nFull Vocab (128k)\n3.66\n4.12\n4.05\n4.29\n3.49\n3.68\n3.92\n3.89 (100%)\n+FR 64k (ShareGPT)\n3.45\n4.08\n3.89\n4.20\n3.40\n3.56\n3.83\n3.77 (96.9%)\n+FR 32k (ShareGPT)\n3.23\n3.95\n3.59\n4.04\n3.25\n3.31\n3.62\n3.57 (91.8%)\n+FR 16k (ShareGPT)\n3.03\n3.71\n3.30\n3.74\n3.04\n3.02\n3.40\n3.32 (85.3%)\n+FR 8k (ShareGPT)\n2.82\n3.42\n2.95\n3.45\n2.82\n2.77\n3.19\n3.06 (78.7%)\n+FR 64k (SlimPajama)\n3.59\n4.07\n3.98\n4.26\n3.42\n3.65\n3.62\n3.80 (97.7%)\n+FR 32k (SlimPajama)\n3.39\n3.89\n3.85\n4.15\n3.34\n3.51\n3.29\n3.63 (93.3%)\n+FR 16k (SlimPajama)\n3.20\n3.63\n3.56\n3.84\n3.19\n3.28\n3.10\n3.40 (87.4%)\n+FR 8k (SlimPajama)\n2.98\n3.33\n3.25\n3.52\n2.97\n2.98\n2.86\n3.13 (80.5%)\nTable 1: Average accepted length for Llama-3-8B under different FR-Spec configurations. The numbers in\nparentheses (97.7%) indicate the ratio achieved compared to the full vocabulary baseline.\nMethod\nMT.\nConv.\nRAG\nMath\nQA\nSumm.\nCode\nAverage\nVanilla\n90.94\n90.43\n83.43\n91.16\n91.05\n86.63\n90.10\n89.11 (1.00×)\nEAGLE-2\n176.79\n203.41\n168.05\n209.88\n166.60\n167.12\n175.11\n180.99 (2.03×)\n+FR 64k\n192.85\n224.52\n178.53\n231.99\n183.17\n183.86\n183.11\n196.86 (2.21×)\n+FR 32k\n195.60\n227.68\n184.85\n243.36\n190.27\n188.14\n183.19\n201.87 (2.27×)\n+FR 16k\n194.02\n223.32\n178.22\n233.69\n188.60\n182.26\n176.70\n196.69 (2.21×)\n+FR 8k\n185.78\n210.66\n167.64\n218.57\n180.40\n170.97\n167.85\n185.98 (2.09×)\nTable 2: Decoding speed (token/s) of FR-Spec and baselines on Llama-3-8B under our implementation framework\nusing temperature=0. The numbers in parentheses (2.27×) indicate the speedup compared to the baseline (Vanilla).\nexperiments across seven subtasks, measuring the\naverage acceptance length under different vocabu-\nlary truncation strategies. The average acceptance\nlength is an important metric in speculative sam-\npling. It quantifies the number of draft tokens that\nare verified as correct in each iteration. It serves as\nan effective assessment of drafting quality and is\none important factor that affects the final speedup\naside from the drafting time.\nSpecifically, we tried two datasets for token fre-\nquency statistics: (1) SlimPajama-627B (Sobol-\neva et al., 2023).\nWe sample a 1 billion to-\nken subset from it. Conducting tokenization on\nthis subset requires less than 30 minutes.\n(2)\nShareGPT (ShareGPT, 2023). ShareGPT is the\ntraining data for EAGLE-2, and we use the whole\ndataset, which comprises 134 million tokens.\nBased on the token-frequency statistics, we se-\nlect four different vocabulary sizes (|Vhigh| =\n{8k, 16k, 32k, 64k}) to serve as the new LM Head\nconfigurations for the draft model. Table 1 reports\nthe average acceptance length of the Llama-3-8B\nmodel across different FR-Spec configurations. As\nshown in the results, when the vocabulary size of\nthe draft model was halved from 128k to 64k, the\naverage acceptance length only decreased slightly\n(2.3% for SlimPajama and 3.1% for ShareGPT).\nThis result is consistent with the “long-tail” charac-\nteristic of token frequency analyzed in Section 3.2.\nWhen the vocabulary size was reduced to 8k, a sig-\nnificant shortening of the acceptance length was\nobserved. This finding underscores the need to\nstrike a balance between the draft accuracy and\ndrafting time of the draft model. In Section 4.3\nbelow, we will conduct an in-depth analysis of this\ntrade-off, taking into account the drafting time.\nNotably,\nfrequency statistics derived from\nSlimPajama outperform those from ShareGPT in\nterms of average accept length. The observation\nremains consistent when applied to Qwen-2-7B\nand Llama-3.2-1B, as detailed in Appendix A.1\nand A.2. We attribute this difference to the higher\nquality and the larger volume of the SlimPajama\ndata. Therefore, we adopted SlimPajama-based\nstatistics for subsequent experiments.\n4.3\nDecoding Speed\nBased on our native C and CUDA implementa-\ntion, we evaluate the speed of the proposed FR-\nSpec method and baselines on the Llama-3-8B\nmodel, as detailed in Table 2. As can be seen,\nFR-Spec surpasses the original EAGLE-2 in all\nvocabulary configurations. Comparing different\nvocabulary sizes, setting |Vhigh| = 32k consis-\ntently outperforms other vocabulary configurations.\nSpecifically, this configuration achieves an average\n6\n\nTranslation\nConversation\nRAG\nMath\nQA\nSummarization\nCode\n50\n100\n150\n200\n250\nHuggingface EAGLE-2\nSGLang EAGLE-2\nOur Impl EAGLE-2\nOur Impl EAGLE-2(+FR)\nFigure 7: Decoding speed (token/s) of FR-Spec and\nEAGLE-2 for Llama-3-8B under different frameworks.\nspeedup improvement of 11.8% over EAGLE-2,\nachieving the best trade-off between draft accuracy\nand drafting time. Experiments on Llama-3-1B can\nrefer to Appendix A.2, where FR-Spec achieves\n24.2% extra speedup over EAGLE-2.\nFurthermore, we conducted speed analyses be-\ntween our implementation and mainstream frame-\nworks, namely Huggingface and SGLang. As the\nexperimental results demonstrated in Figure 7, our\noptimized EAGLE-2 achieves average speedups of\n1.63× and 1.28× compared to the original Hug-\ngingFace and SGLang versions, respectively. The\nFR-Spec further improves these performance gains,\nwith speedups of 1.82× and 1.42× over the Hug-\ngingFace and SGLang implemented EAGLE-2, re-\nspectively.\nFR-Spec supports both greedy decoding and\nrandom sampling. As illustrated in Table 3, FR-\nSpec can achieve a speedup ratio of 1.13× com-\npared to EAGLE-2 at a temperature of 1. This\nperformance is comparable to the acceleration ob-\nserved at the temperature of 0, showing that FR-\nSpec is effective at different generation settings.\n4.4\nModel Performance\nTo validate the correctness of our FR-Spec, we as-\nsessed the generation quality of the Llama-3-8B\nmodel across two tasks: code generation using the\nHumanEval benchmark and mathematical reason-\ning with the GSM8K benchmark. We compare the\nmodel’s performance between the HuggingFace im-\nplementation and our optimized implementation in\nTable 4, in both greedy decoding (temperature=0)\nand random sampling (temperature=1) scenarios.\nBenchmark Vanilla\nEAGLE-2\nEAGLE-2(+FR 32k)\ntoken/s token/s Speedup token/s\nSpeedup\nMT.\n90.32\n171.03\n1.89×\n188.69\n2.09×\nConv.\n89.85\n187.95\n2.09×\n212.08\n2.36×\nRAG\n83.18\n159.37\n1.92×\n178.64\n2.15×\nMath\n89.75\n196.34\n2.19×\n237.96\n2.65×\nQA\n90.58\n155.10\n1.71×\n182.59\n2.02×\nSumm.\n87.41\n158.72\n1.82×\n182.70\n2.09×\nCode\n89.77\n180.67\n2.01×\n183.54\n2.04×\nAverage\n88.69\n172.74\n1.95×\n195.17\n2.20×\nTable 3: Decoding speed (token/s) of Llama-3-8B using\ntemperature=1 under our implementation.\nHuggingface\nOur Implementation\nBenchmark\nTemp Vanilla\nEAGLE-2 Vanilla\nFR-Spec\nHumanEval\n0\n54.9\n54.9\n57.3\n58.5\n1\n51.0±1.4 50.6±3.1\n51.1±1.2 51.2±1.2\nGSM8K\n0\n76.8\n77.0\n76.3\n76.1\n1\n70.8±2.0 66.5±2.9\n65.6±1.8 67.1±0.8\nTable 4: Performance of the Llama-3-8B model on math\nreasoning and code generation tasks across two imple-\nmentation frameworks. Due to variability in results with\ntemperature=1, we report the average scores and vari-\nance across five different random seeds.\nThe experimental results indicate that the perfor-\nmance across both implementations is comparable,\nwith only minor discrepancies. These differences\nare expected, as different implementations use dif-\nferent computational orders, resulting in floating-\npoint numerical errors that accumulate within the\nmodel layers.\n4.5\nIntegration to Other Speculative Methods\nAs a plug-and-play acceleration solution that is\ncompatible with various speculative sampling meth-\nods, we further assess FR-Spec by integrating FR-\nSpec to Medusa (Cai et al., 2024), another repre-\nsentative speculative sampling method. Table 5\npresents the performance of FR-Spec in our op-\ntimized implementation of Medusa, where FR-\nSpec achieve 1.08× extra speedup. The experimen-\ntal results demonstrate that while our previous anal-\nysis primarily focused on EAGLE-2, our method\nalso shows effectiveness when applied to other rep-\nresentative speculative sampling approaches, ex-\nhibiting strong compatibility and user-friendliness\nacross different implementations.\n5\nRelated Work\nThis section mainly introduces model acceleration\nmethods related to large vocabulary and speculative\nsampling. More details on how LLMs work can\n7\n\nBenchmark Vanilla\nMedusa\nMedusa (+FR 32k)\ntoken/s token/s Speedup token/s\nSpeedup\nMT.\n90.94\n146.42\n1.61×\n157.54\n1.73×\nConv.\n90.43\n157.99\n1.75×\n169.26\n1.87×\nRAG\n83.43\n130.56\n1.56×\n139.62\n1.67×\nMath\n91.16\n160.95\n1.77×\n174.56\n1.91×\nQA\n91.05\n138.92\n1.53×\n151.07\n1.66×\nSumm.\n86.63\n130.08\n1.50×\n141.39\n1.63×\nCode\n90.10\n152.57\n1.69×\n161.28\n1.79×\nAverage\n89.11\n145.36\n1.63×\n156.39\n1.76×\nTable 5: Decoding speed (token/s) of Llama-3-8B using\ntemperature=0 under our implemented Medusa.\nrefer to surveys (Qiu et al., 2020; Han et al., 2021;\nBommasani et al., 2021; Zhao et al., 2023). Other\nacceleration methods such as quantization and dis-\ntillation can refer to suverys (Xu and McAuley,\n2023; Li et al., 2024a).\n5.1\nAcceration on Large Vocabulary\nRecent advancements in large language models\n(LLMs) have prompted a growing interest in ad-\ndressing the challenges associated with large vo-\ncabularies. While several optimization efforts have\nbeen proposed to tackle these issues, the majority\nfocus primarily on the training phase. Computing\nthe LM Head and the loss function over large vocab-\nularies requires storing a huge intermediate state\nbefore gradient computation. Therefore, MST (Luo\net al., 2024) and CCE (Wijmans et al., 2024) tried to\nmitigate the memory overhead caused by comput-\ning loss functions over large vocabularies. These\napproaches address the issue by using input parti-\ntioning or weight partitioning, and conduct activa-\ntion recomputation (Chen et al., 2016) during the\nbackward propagation. In addition to the aforemen-\ntioned works that require no modifications to the\nmodel architecture, Joulin et al. (2017) proposes a\nhierarchical vocabulary structure to eliminate the\ncomputation of irrelevant vocabulary adaptively.\nConstrained Decoding (Hokamp and Liu, 2017;\nDong et al., 2024) restricts the vocabulary space\nto generate highly structured outputs, particularly\nin the context of LLM agents, where the generated\ncontent must adhere to specific formats, such as\nproducing parsable code or invocable functions.\n5.2\nSpeculative Sampling\nTraditional autoregressive generation in LLMs suf-\nfers from low generation speed due to the se-\nquential nature of token prediction. To address\nthis limitation, speculative sampling has emerged\nas a promising approach, leveraging draft-then-\nverification paradigms to accelerate decoding (Xia\net al., 2023; Leviathan et al., 2023; Chen et al.,\n2023). Existing speculative sampling methods can\nbe categorized into two branches: (1) retrieval-\nbased drafting approaches like PLD (Saxena,\n2023), LLMA (Yang et al., 2023), and REST (He\net al., 2024) retrieve relevant context from the\nprompt, gaining significant speedups in context-\ndependent tasks (e.g., summarization) by reusing\nretrieved text spans from the prompt. (2) model-\nbased drafting methods exemplified by SpecIn-\nfer (Miao et al., 2024), DistillSpec (Zhou et al.),\nMedusa (Cai et al., 2024) and EAGLE (Li et al.,\n2024b), which employ a draft model for general-\npurpose acceleration. Our work focuses on the\nlatter category due to its broader applicability. The\ndraft models’ structures also differ. For example,\nMedusa generates draft tokens based solely on the\nmodel’s last hidden state, using a “MLP+LM Head”\nstructure, while EAGLE incorporates both the last\nhidden state and preceding tokens, using a trans-\nformer structure. Among these model-based draft-\ning methods, EAGLE-2 (Li et al., 2024b) achieves\nthe current state-of-the-art speed.\nTo further accelerate existing speculative sam-\npling methods, recent advancements have been\nmade at both the algorithmic and implementa-\ntion levels. At the algorithm level, HASS (Zhang\net al., 2025) has enhanced the training tasks for\ndraft models, AdaEAGLE (Zhang et al., 2024) and\nOPT-Tree (Wang et al., 2024) introduced adap-\ntive draft tree structures at inference time. Ad-\nditionally, TriForce (Sun et al., 2024) employs\nKV-Cache compression on draft models to acceler-\nate the drafting process in long-context scenarios,\nOuroboros (Zhao et al., 2024) utilize Lookahead\nDecoding (Fu et al., 2024) to accelerates the draft\nmodels when the draft model is not lightweight\nenough. From an implementation perspective, ef-\nficient LLM frameworks like vLLM (Kwon et al.,\n2023) and SGLang (Zheng et al., 2024) have in-\ntegrated speculative sampling. DeFT (Yao et al.,\n2025) leverages FlashAttention (Dao, 2023) to en-\nhance the efficiency of speculative sampling.\n6\nConclusion\nIn this paper, we systematically analyze the over-\nlooked issue of LM Head in speculative sampling.\nBased on our frequency statistics, we propose a\nfrequency-ranked optimization strategy to optimize\n8\n\nthe drafting process. We restrict the drafting space\nto a high-frequency subset of the vocabulary to\nmake draft models faster. Experiments demonstrate\nthat by building on top of EAGLE-2 and Medusa,\nwe can further achieve speedup ratios of 1.12× and\n1.08×, respectively. FR-Spec can be applied to\nmost existing speculative sampling methods with\none-click modification and requires no retraining.\nLimitations\nOur current approach relies on static frequency\nanalysis of the vocabulary, which, while effective,\nlacks adaptive mechanisms. Despite this limitation,\nthe proposed solution has demonstrated promising\ncompatibility. In the future, we will explore better\ndynamic mechanisms for further speedup.\nReferences\nOndˇrej Bojar, Christian Buck, Christian Federmann,\nBarry Haddow, Philipp Koehn, Johannes Leveling,\nChristof Monz, Pavel Pecina, Matt Post, Herve Saint-\nAmand, Radu Soricut, Lucia Specia, and Aleš Tam-\nchyna. 2014. Findings of the 2014 workshop on\nstatistical machine translation. In Proceedings of the\nNinth Workshop on Statistical Machine Translation,\npages 12–58.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli,\nRuss Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine Bosse-\nlut, Emma Brunskill, et al. 2021. On the opportuni-\nties and risks of foundation models. arXiv preprint\narXiv:2108.07258.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In Proceedings of NeurIPS, pages 1877–\n1901.\nTianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng,\nJason D. Lee, Deming Chen, and Tri Dao. 2024.\nMedusa: Simple LLM inference acceleration frame-\nwork with multiple decoding heads. In Proceedings\nof ICML.\nCharlie Chen, Sebastian Borgeaud, Geoffrey Irving,\nJean-Baptiste Lespiau, Laurent Sifre, and John\nJumper. 2023. Accelerating large language model\ndecoding with speculative sampling. arXiv preprint\narXiv:2302.01318.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. 2021.\nEvaluating large\nlanguage models trained on code. arXiv preprint\narXiv:2107.03374.\nTianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos\nGuestrin. 2016. Training deep nets with sublinear\nmemory cost. arXiv preprint arXiv:1604.06174.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, Christopher Hesse, and John Schulman.\n2021. Training verifiers to solve math word prob-\nlems. arXiv preprint arXiv:2110.14168.\nTri Dao. 2023. Flashattention-2: Faster attention with\nbetter parallelism and work partitioning.\narXiv\npreprint arXiv:2307.08691.\nYixin Dong, Charlie F Ruan, Yaxing Cai, Ruihang\nLai, Ziyi Xu, Yilong Zhao, and Tianqi Chen. 2024.\nXgrammar: Flexible and efficient structured genera-\ntion engine for large language models. arXiv preprint\narXiv:2411.15100.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, et al. 2024. The llama 3 herd of models. arXiv\npreprint arXiv:2407.21783.\nYichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang.\n2024.\nBreak the sequential dependency of LLM\ninference using lookahead decoding. In Proceedings\nof ICML.\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song,\nRuoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma,\nPeiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: In-\ncentivizing reasoning capability in llms via reinforce-\nment learning. arXiv preprint arXiv:2501.12948.\nXu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao\nLiu, Yuqi Huo, Jiezhong Qiu, Yuan Yao, Ao Zhang,\nLiang Zhang, et al. 2021. Pre-trained models: Past,\npresent and future. AI Open, 2:225–250.\nZhenyu He, Zexuan Zhong, Tianle Cai, Jason Lee, and\nDi He. 2024.\nREST: Retrieval-based speculative\ndecoding. In Proceedings of NAACL, pages 1582–\n1595.\nChris Hokamp and Qun Liu. 2017.\nLexically con-\nstrained decoding for sequence generation using grid\nbeam search. In Proceedings of ACL, pages 1535–\n1546.\nArmand Joulin, Moustapha Cissé, David Grangier,\nHervé Jégou, et al. 2017. Efficient softmax approx-\nimation for gpus. In Proceedings of ICML, pages\n1302–1310. PMLR.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: A benchmark for question answering\nresearch. TACL, 7:453–466.\n9\n\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph Gon-\nzalez, Hao Zhang, and Ion Stoica. 2023. Efficient\nmemory management for large language model serv-\ning with pagedattention. In Proceedings of SOSP,\npages 611–626.\nYaniv Leviathan, Matan Kalman, and Yossi Matias.\n2023. Fast inference from transformers via spec-\nulative decoding. In Proceedings of ICML, pages\n19274–19286.\nJinhao Li, Jiaming Xu, Shan Huang, Yonghua Chen,\nWen Li, Jun Liu, Yaoxiu Lian, Jiayi Pan, Li Ding,\nHao Zhou, et al. 2024a. Large language model infer-\nence acceleration: A comprehensive hardware per-\nspective. arXiv preprint arXiv:2410.04466.\nYuhui Li, Fangyun Wei, Chao Zhang, and Hongyang\nZhang. 2024b. Eagle-2: Faster inference of language\nmodels with dynamic draft trees. In Proceedings of\nEMNLP, pages 7421–7432.\nAixin Liu, Bei Feng, Bing Xue, Bingxuan Wang,\nBochao Wu, Chengda Lu, Chenggang Zhao, Chengqi\nDeng, Chenyu Zhang, Chong Ruan, et al. 2024.\nDeepseek-v3 technical report.\narXiv preprint\narXiv:2412.19437.\nCheng Luo, Jiawei Zhao, Zhuoming Chen, Beidi Chen,\nand Anima Anandkumar. 2024. Mini-sequence trans-\nformer: Optimizing intermediate memory for long se-\nquences training. arXiv preprint arXiv:2407.15892.\nXupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao\nCheng, Zeyu Wang, Zhengxin Zhang, Rae Ying Yee\nWong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, et al.\n2024. Specinfer: Accelerating large language model\nserving with tree-based speculative inference and\nverification. In Proceedings of ASPLOS, pages 932–\n949.\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos,\nÇa˘glar Gu˙lçehre, and Bing Xiang. 2016. Abstrac-\ntive text summarization using sequence-to-sequence\nRNNs and beyond. In Proceedings of CoNLL, pages\n280–290.\nTB OpenAI. 2022. Chatgpt: Optimizing language mod-\nels for dialogue. OpenAI.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. 2019. Pytorch: An imperative style,\nhigh-performance deep learning library. Proceedings\nof NeurIPS, 32.\nXipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao,\nNing Dai, and Xuanjing Huang. 2020. Pre-trained\nmodels for natural language processing: A survey.\nScience China Technological Sciences, 63(10):1872–\n1897.\nApoorv Saxena. 2023. Prompt lookup decoding.\nShareGPT. 2023. Sharegpt.\nDaria Soboleva, Faisal Al-Khateeb, Robert Myers, Ja-\ncob R Steeves, Joel Hestness, and Nolan Dey. 2023.\nSlimPajama: A 627B token cleaned and deduplicated\nversion of RedPajama.\nHanshi Sun, Zhuoming Chen, Xinyu Yang, Yuandong\nTian, and Beidi Chen. 2024. Triforce: Lossless accel-\neration of long sequence generation with hierarchical\nspeculative decoding. In Proceedings of COLM.\nSho Takase, Ryokan Ri, Shun Kiyono, and Takuya Kato.\n2024. Large vocabulary size improves large language\nmodels. arXiv preprint arXiv:2406.16508.\nChaofan Tao, Qian Liu, Longxu Dou, Niklas Muen-\nnighoff, Zhongwei Wan, Ping Luo, Min Lin, and\nNgai Wong. 2024. Scaling laws with vocabulary:\nLarger models deserve larger vocabularies. In Pro-\nceedings of NeurIPS, volume 37, pages 114147–\n114179.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023.\nLlama 2:\nOpen founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nJikai Wang, Yi Su, Juntao Li, Qingrong Xia, Zi Ye,\nXinyu Duan, Zhefeng Wang, and Min Zhang. 2024.\nOpt-tree: Speculative decoding with adaptive draft\ntree structure. arXiv preprint arXiv:2406.17276.\nErik Wijmans, Brody Huval, Alexander Hertzberg,\nVladlen Koltun, and Philipp Krähenbühl. 2024. Cut\nyour losses in large-vocabulary language models.\narXiv preprint arXiv:2411.09009.\nHeming Xia, Tao Ge, Peiyi Wang, Si-Qing Chen, Furu\nWei, and Zhifang Sui. 2023.\nSpeculative decod-\ning: Exploiting speculative execution for accelerat-\ning seq2seq generation. In Proceedings of EMNLP,\npages 3909–3925.\nHeming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang,\nYongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and Zhi-\nfang Sui. 2024. Unlocking efficiency in large lan-\nguage model inference: A comprehensive survey of\nspeculative decoding. In Findings of the ACL, pages\n7655–7671.\nCanwen Xu and Julian McAuley. 2023. A survey on\nmodel compression and acceleration for pretrained\nlanguage models.\nIn Proceedings of AAAI, vol-\nume 37, pages 10566–10575.\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng,\nBowen Yu, Chang Zhou, Chengpeng Li, Chengyuan\nLi, Dayiheng Liu, Fei Huang, Guanting Dong, et al.\n2024a. Qwen2 technical report.\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui,\nBo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu,\nFei Huang, Haoran Wei, et al. 2024b. Qwen2.5 tech-\nnical report. arXiv preprint arXiv:2412.15115.\n10\n\nNan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin\nJiang, Linjun Yang, Rangan Majumder, and Furu\nWei. 2023. Inference with reference: Lossless ac-\nceleration of large language models. arXiv preprint\narXiv:2304.04487.\nJinwei Yao, Kaiqi Chen, Kexun Zhang, Jiaxuan You,\nBinhang Yuan, Zeke Wang, and Tao Lin. 2025.\nDeFT: Decoding with flash tree-attention for effi-\ncient tree-structured LLM inference. In Proceedings\nof ICLR.\nLefan Zhang, Xiaodan Wang, Yanhua Huang, and Rui-\nwen Xu. 2025. Learning harmonized representations\nfor speculative sampling. In Proceedings of ICLR.\nSituo Zhang, Hankun Wang, Da Ma, Zichen Zhu,\nLu Chen, Kunyao Lan, and Kai Yu. 2024. Adaea-\ngle: Optimizing speculative decoding via explicit\nmodeling of adaptive draft structures. arXiv preprint\narXiv:2412.18910.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, et al. 2023. A\nsurvey of large language models.\narXiv preprint\narXiv:2303.18223.\nWeilin Zhao, Yuxiang Huang, Xu Han, Wang Xu,\nChaojun Xiao, Xinrong Zhang, Yewei Fang, Kai-\nhuo Zhang, Zhiyuan Liu, and Maosong Sun. 2024.\nOuroboros:\nGenerating longer drafts phrase by\nphrase for faster speculative decoding. In Proceed-\nings of EMNLP, pages 13378–13393.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric Xing, Hao Zhang,\nJoseph E Gonzalez, and Ion Stoica. 2023. Judging\nllm-as-a-judge with mt-bench and chatbot arena. In\nProceedings of CoNLL, volume 36, pages 46595–\n46623.\nLianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue\nSun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos\nKozyrakis, Ion Stoica, Joseph E Gonzalez, et al. 2024.\nSglang: Efficient execution of structured language\nmodel programs. arXiv preprint arXiv:2312.07104.\nYongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat,\nAditya Krishna Menon, Afshin Rostamizadeh, San-\njiv Kumar, Jean-François Kagy, and Rishabh Agar-\nwal. Distillspec: Improving speculative decoding via\nknowledge distillation. In Proceedings of ICLR.\nGeorge Kingsley Zipf. 1950. Human behavior and the\nprinciple of least effort: An introduction to human\necology. Language, 26:394.\nA\nAdditional Results\nA.1\nQwen-2-7B Performance\nFollowing the settings in Section 4.2, we inves-\ntigated the impact of FR-Spec on draft model’s\naccepted length in the Qwen-2-7B model, which\nhas a different vocabulary. The results in Table 6\nshow that the decrease ratio in acceptance length\nacross various configuration settings in Qwen-2-\n7B is similar to or even less than that observed\nin Llama-3-8B, indicating the effectiveness of our\nmethod on various LLMs.\nA.2\nLlama-3.2-1B Performance\nFollowing the settings in Section 4.2 and Sec-\ntion 4.3, we conducted accept length and speed\nexperiments on the Llama-3.2-1B model using a\nsingle 3090 GPU. Given the smaller size of the\nmodel, we adjusted the drafting depth of Eagle-2\nto 3 and set the total number of draft tokens to 30.\nThe average acceptance length obtained from\nthe experiments is presented in Table 7, while\nthe speedup ratio in our implemented framework\nis shown in Table 8.\nResults show that FR-\nSpec achieves an extra 1.24× speedup over the\nstate-of-the-art EAGLE-2. The speedup is even\nhigher than the experimental results for Llama-\n3-8B. This demonstrates the effectiveness of FR-\nSpec, particularly on less powerful hardware and\nsmaller LLMs.\nSpeed comparison with other frameworks is il-\nlustrated in Figure 8. The overall speedup ratio\nof FR-Spec was 5.24× and 2.61× compared with\nHuggingface and SGLang, respectively.\nTranslation\nConversation\nRAG\nMath\nQA\nSummarization\nCode\n100\n200\n300\n400\n500\nHuggingface EAGLE-2\nSGLang EAGLE-2\nOur Impl EAGLE-2\nOur Impl EAGLE-2(+FR)\nFigure 8: Decoding speed (token/s) of FR-Spec and\nEAGLE-2 for Llama-3.2-1B under different implemen-\ntation framework.\n11\n\nConfiguration\nMT.\nConv.\nRAG\nMath\nQA\nSumm.\nCode\nAverage\nFull Vocab (152k)\n2.90\n4.06\n3.65\n4.31\n3.27\n3.74\n4.22\n3.74 (100%)\n+FR 64k (ShareGPT)\n2.86\n3.98\n3.65\n4.22\n3.23\n3.67\n4.17\n3.68 (98.6%)\n+FR 32k (ShareGPT)\n2.76\n3.90\n3.42\n4.10\n3.24\n3.39\n3.98\n3.54 (94.8%)\n+FR 16k (ShareGPT)\n2.62\n3.64\n3.20\n3.85\n2.99\n3.08\n3.71\n3.30 (88.3%)\n+FR 8k (ShareGPT)\n2.45\n3.39\n3.01\n3.60\n2.48\n2.81\n3.41\n3.02 (80.9%)\n+FR 64k (SlimPajama)\n2.90\n3.97\n3.64\n4.29\n3.28\n3.73\n3.98\n3.69 (98.6%)\n+FR 32k (SlimPajama)\n2.83\n3.73\n3.53\n4.20\n3.39\n3.58\n3.71\n3.57 (95.4%)\n+FR 16k (SlimPajama)\n2.67\n3.50\n3.33\n3.95\n3.25\n3.35\n3.40\n3.35 (89.7%)\n+FR 8k (SlimPajama)\n2.60\n3.28\n3.12\n3.65\n2.91\n3.04\n3.10\n3.10 (83.0%)\nTable 6: Average accepted length for Qwen-2-7B on under different FR-Spec configurations.\nConfiguration\nMT.\nConv.\nRAG\nMath\nQA\nSumm.\nCode\nAverage\nFull Vocab (128k)\n2.49\n2.96\n2.80\n3.08\n2.69\n2.62\n3.04\n2.81 (100%)\n+FR 64k (ShareGPT)\n2.43\n2.93\n2.75\n3.05\n2.67\n2.58\n2.98\n2.77 (98.6%)\n+FR 32k (ShareGPT)\n2.39\n2.90\n2.65\n2.98\n2.54\n2.51\n2.85\n2.69 (95.7%)\n+FR 16k (ShareGPT)\n2.34\n2.78\n2.56\n2.88\n2.42\n2.42\n2.75\n2.59 (92.3%)\n+FR 8k (ShareGPT)\n2.25\n2.66\n2.44\n2.76\n2.35\n2.31\n2.65\n2.49 (88.6%)\n+FR 64k (SlimPajama)\n2.47\n2.92\n2.78\n3.07\n2.68\n2.61\n2.88\n2.77 (98.7%)\n+FR 32k (SlimPajama)\n2.43\n2.82\n2.69\n3.04\n2.58\n2.57\n2.70\n2.69 (95.8%)\n+FR 16k (SlimPajama)\n2.38\n2.72\n2.62\n2.91\n2.51\n2.50\n2.58\n2.60 (92.6%)\n+FR 8k (SlimPajama)\n2.30\n2.58\n2.50\n2.80\n2.40\n2.39\n2.43\n2.49 (88.5%)\nTable 7: Average accepted length for Llama-3.2-1B on under different FR-Spec configurations.\nMethod\nMT.\nConv.\nRAG\nMath\nQA\nSumm.\nCode\nAverage\nVanilla\n259.83\n255.89\n220.25\n263.34\n260.13\n248.15\n256.64\n252.03 (1.00×)\nEAGLE-2\n306.04\n358.37\n266.84\n372.37\n305.52\n294.82\n360.60\n323.51 (1.28×)\n+FR 64k\n349.12\n406.14\n297.62\n427.14\n350.08\n338.81\n390.78\n365.67 (1.45×)\n+FR 32k\n378.90\n428.75\n317.68\n467.53\n378.39\n363.70\n395.95\n390.13 (1.55×)\n+FR 16k\n394.81\n443.00\n326.75\n476.47\n394.47\n375.70\n402.07\n401.90 (1.59×)\n+FR 8k\n386.97\n428.94\n319.83\n462.98\n382.75\n363.50\n392.13\n391.01 (1.55×)\nTable 8: Decoding speed (token/s) of FR-Spec and other baselines on Llama-3.2-1B under our implementation\nusing temperature=0 and SlimPajama token-frequency statistics. The numbers in parentheses (1.59×) indicate the\nspeedup compared to the baseline (Vanilla).\n12\n",
  "metadata": {
    "source_path": "papers/arxiv/FR-Spec_Accelerating_Large-Vocabulary_Language_Models_via\n__Frequency-Ranked_Speculative_Sampling_0dce77990a2583a3.pdf",
    "content_hash": "0dce77990a2583a3e42e2f4b1a4d64149f5819c534e44ea1b7274d6f516015b3",
    "arxiv_id": null,
    "title": "FR-Spec_Accelerating_Large-Vocabulary_Language_Models_via\n__Frequency-Ranked_Speculative_Sampling_0dce77990a2583a3",
    "author": "",
    "creation_date": "D:20250221020510Z",
    "published": "2025-02-21T02:05:10",
    "pages": 12,
    "size": 1764246,
    "file_mtime": 1740346981.4497135
  }
}