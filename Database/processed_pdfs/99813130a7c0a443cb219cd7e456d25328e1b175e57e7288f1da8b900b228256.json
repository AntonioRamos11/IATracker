{
  "text": "JOURNAL OF LATEX CLASS FILES, JANUARY 2025\n1\nFrom System 1 to System 2: A Survey of\nReasoning Large Language Models\nZhong-Zhi Li∗, Duzhen Zhang∗, Ming-Liang Zhang§, Jiaxin Zhang§, Zengyan Liu§, Yuxuan Yao§,\nHaotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, Yingying Zhang, Fei Yin, Jiahua Dong,\nZhijiang Guo†, Le Song†, Cheng-Lin Liu†\nID, Fellow, IEEE\nAbstract—Achieving human-level intelligence requires refining the transition from the fast, intuitive System 1 to the slower, more\ndeliberate System 2 reasoning. While System 1 excels in quick, heuristic decisions, System 2 relies on logical reasoning for more\naccurate judgments and reduced biases. Foundational Large Language Models (LLMs) excel at fast decision-making but lack the depth\nfor complex reasoning, as they have not yet fully embraced the step-by-step analysis characteristic of true System 2 thinking. Recently,\nreasoning LLMs like OpenAI’s o1/o3 and DeepSeek’s R1 have demonstrated expert-level performance in fields such as mathematics\nand coding, closely mimicking the deliberate reasoning of System 2 and showcasing human-like cognitive abilities. This survey begins\nwith a brief overview of the progress in foundational LLMs and the early development of System 2 technologies, exploring how their\ncombination has paved the way for reasoning LLMs. Next, we discuss how to construct reasoning LLMs, analyzing their features, the\ncore methods enabling advanced reasoning, and the evolution of various reasoning LLMs. Additionally, we provide an overview of\nreasoning benchmarks, offering an in-depth comparison of the performance of representative reasoning LLMs. Finally, we explore\npromising directions for advancing reasoning LLMs and maintain a real-time GitHub Repository to track the latest developments. We\nhope this survey will serve as a valuable resource to inspire innovation and drive progress in this rapidly evolving field.\nIndex Terms—Slow-thinking, Large Language Models, Human-like Reasoning, Decision Making in AI, AGI\n✦\n1\nINTRODUCTION\n“Don’t teach. Incentivize.”\n—Hyung Won Chung, OpenAI\nA\nCHIEVING human-level intelligence requires refining\nthe transition from System 1 to System 2 reasoning\n[1]–[5]. Dual-system theory suggests that human cognition\noperates through two modes: System 1, which is fast, auto-\nmatic, and intuitive, enabling quick decisions with minimal\neffort, and System 2, which is slower, more analytical, and\ndeliberate [6], [7]. While System 1 is efficient for routine\nVersion: v1 (major update on February 23, 2025)\n∗Core contribution. §Significant contribution. †Corresponding author.\nDuzhen\nZhang,\nJiahua\nDong,\nand\nLe\nSong\nare\nwith\nthe\nMo-\nhamed\nbin\nZayed\nUniversity\nof\nArtificial\nIntelligence,\nAbu\nDhabi,\nUAE\n(E-mail:\nbladedancer957@gmail.com;\ndongjiahua1995@gmail.com;\nle.song@mbzuai.ac.ae).\nZhong-Zhi Li, Pei-Jie Wang, Xiuyi Chen, Fei Yin, and Cheng-Lin Liu\nare with the Institute of Automation, Chinese Academy of Sciences,\nBeijing, China (E-mail: lizhongzhi2022@ia.ac.cn; wangpeijie2023@ia.ac.cn,\nhugheren.chan@gmail.com; fyin@nlpr.ia.ac.cn; liucl@nlpr.ia.ac.cn).\nMing-Liang Zhang is with the AiShiWeiLai AI Research, Beijing, China (E-\nmail: zhangmingliang@yuaiweiwu.com).\nZengyan Liu, Yuxuan Yao, and Zhijiang Guo is with City University of\nHong Kong and the Hong Kong University of Science and Technology\n(Guangzhou), Guangzhou, China (E-mail: zengyaliu2-c@my.cityu.edu.hk;\nyuxuanyao3-c@my.cityu.edu.hk; zhijiangguo@hkust-gz.edu.cn).\nJiaxin Zhang is with the University of Strathclyde, Glasgow, UK (E-mail:\njiaxin.zhang@strath.ac.uk).\nHaotian Xu is with the Xiaohongshu Inc, Beijing, China (E-mail: xuhao-\ntian@xiaohongshu.com).\nYingying Zhang is with the East China Normal University, Shanghai, China\n(E-mail: yyzhang@fem.ecnu.edu.cn).\nJunhao Zheng is with the South China University of Technology, Guangzhou,\nChina (E-mail: junhaozheng47@outlook.com).\ntasks, it is prone to cognitive biases, especially in complex or\nuncertain situations, leading to judgment errors. In contrast,\nSystem 2 relies on logical reasoning and systematic thinking,\nresulting in more accurate and rational decisions [8]–[11]. By\nmitigating the biases of System 1, System 2 provides a more\nrefined approach to problem-solving [12]–[15].\nThe development of foundational Large Language Mod-\nels (LLMs)1 has marked a major milestone in Artificial\nIntelligence (AI). Models such as GPT-4o [16] and DeepSeek-\nv3 [17] have demonstrated impressive capabilities in text\ngeneration, language translation, and a variety of perception\ntasks [18]–[28]. These models, trained on extensive datasets\nand utilizing advanced algorithms, excel in understanding\nand generating human-like responses. However, despite\ntheir impressive achievements, foundational LLMs operate\nin a manner similar to System 1 reasoning, relying on fast,\nheuristic-driven decision-making. While they perform ex-\nceptionally well in providing rapid responses, they often fall\nshort in scenarios requiring deep, logical analysis and preci-\nsion in complex reasoning tasks. This limitation becomes\nespecially clear in situations involving intricate problem-\nsolving, logical analysis, or nuanced understanding, where\nthese models do not yet match human cognitive abilities.\nIn contrast, reasoning LLMs represent a significant ad-\nvancement in the evolution of language models. Models\n1. In this paper, “reasoning” refers to answering questions involving\ncomplex, multi-step processes with intermediate steps. Foundational\nLLMs: LLMs with basic reasoning abilities, handling simple or single-\nstep tasks. Reasoning LLMs: LLMs that excel in complex tasks like cod-\ning and mathematical proofs, incorporating a “thinking” process–tasks\nthat foundational LLMs struggle with.\narXiv:2502.17419v1  [cs.AI]  24 Feb 2025\n\nJOURNAL OF LATEX CLASS FILES, JANUARY 2025\n2\nFig. 1. The recent timeline of reasoning LLMs, covering core methods and the release of open-source and closed-source reproduction projects.\nlike OpenAI’s o1/o3 [29], [30] and DeepSeek’s R1 [31] are\ndesigned to emulate the slower, more deliberate reason-\ning associated with System 2 thinking. Unlike foundational\nLLMs, reasoning LLMs are equipped with mechanisms for\nprocessing information step-by-step, allowing them to make\nmore accurate and rational decisions. This shift from fast-\nthinking, intuitive processes to more methodical, reasoning-\ndriven models enables reasoning LLMs to tackle complex\ntasks, such as advanced mathematics [32]–[37], logical rea-\nsoning [38]–[44], and multimodal reasoning [45]–[47], with\nexpert-level performance, exhibiting human-like cognitive\nabilities. As a result, reasoning LLMs are increasingly seen\nas capable of achieving the deep, logical thinking needed\nfor tasks that were once considered beyond AI’s reach. The\nrecent timeline of reasoning LLMs is presented in Figure 1.\n1.1\nStructure of the Survey\nThis survey offers a comprehensive overview of the key con-\ncepts, methods, and challenges involved in the development\nof reasoning LLMs. As illustrated in Figure 2, this survey is\norganized as follows:\n1) Section 2 offers a concise overview of the progress in\nfoundational LLMs (Section 2.1) and the early develop-\nment of key System 2 technologies, including symbolic\nlogic systems (Section 2.2), Monte Carlo Tree Search\n(MCTS) (Section 2.3), and Reinforcement Learning (RL)\n(Section 2.4), highlighting how their combination has\npaved the way for reasoning LLMs.\n2) Section 3 introduces reasoning LLMs and outlines their\nconstruction process. Specifically, Section 3.1 presents\nthe characteristics of reasoning LLMs from two per-\nspectives: output behavior (Section 3.1.1) and training\ndynamics (Section 3.1.2), emphasizing their differences\nfrom foundational LLMs. Section 3.2 identifies the core\nmethods necessary for achieving advanced reasoning\ncapabilities, focusing on five aspects: Structure Search\n(Section 3.2.1), Reward Modeling (Section 3.2.2), Self\nImprovement (Section 3.2.3), Macro Action (Section\n\nJOURNAL OF LATEX CLASS FILES, JANUARY 2025\n3\nOrganizational\nStructure\n§2 Foundations of Reasoning LLMs\n§2.1 Foundational LLMs\n§2.2 Symbolic Logic Systems\n§2.3 Monte Carlo Tree Search\n§2.4 Reinforcement Learning\n§3 Blueprinting Reasoning LLMs\n§3.1 Feature Analysis\n§3.1.1 Output Behaviour\n§3.1.2 Training Dynamics\n§3.2 Core Method\n§3.2.1 Structure Search\n§3.2.2 Reward Modeling\n§3.2.3 Self Improvement\n§3.2.4 Macro Action\n§3.2.5 Reinforcement Fine-Tuning\n§3.3 Reasoning LLMs Evolution\n§4 Benchmarking Reasoning LLMs\n§4.1 Benchmark Categories\n§4.2 Evaluation Metrics\n§4.3 Performance Comparison\nFig. 2. The primary organizational structure of the survey.\n3.2.4), and Reinforcement Fine-Tuning (Section 3.2.5).\nEach section delves into the specific characteristics of\nthese methods and introduces representative reasoning\nLLMs for each approach. Section 3.3 traces the evolu-\ntionary stages of reasoning LLMs.\n3) Section 4 evaluates representative reasoning LLMs.\nSpecifically, Section 4.1 reviews current mainstream\nreasoning benchmarks, covering both plain text and\nmultimodal benchmarks across various task types. Sec-\ntion 4.2 outlines the current evaluation metrics, while\nSection 4.3 analyzes and compares the performance of\nmainstream reasoning LLMs with their foundational\ncounterparts based on these benchmarks.\n4) Section 5 highlights the limitations of existing reasoning\nLLMs and outlines several promising future develop-\nment directions for these models.\n5) Finally, we conclude the paper in Section 6 and provide\na real-time tracking GitHub Repository to monitor the\nlatest developments in the field.\nWe hope this survey serves as a valuable resource, fostering\ninnovation and progress in this rapidly evolving domain.\n1.2\nContribution of the Survey\nRecently, several analyses and replications of specific tech-\nnical approaches have been conducted [48]–[55], yet there\nremains a lack of systematic analysis and organization.\nResearch [56] has focused only on slow-thinking methods\nduring testing. Meanwhile, studies [57]–[59] have primarily\nconcentrated on training or achieving reasoning LLMs, often\nfrom the perspective of RL.\nOur survey distinguishes itself from and contributes to\nthe existing literature in the following ways:\n1) Rather than focusing on a single technical approach, we\noffer a comprehensive overview of the key concepts,\nmethods, and challenges involved in reasoning LLMs.\n2) We summarize the key advancements of early System 2\nand how they have paved the way for reasoning LLMs,\nspecifically in combination with foundational LLMs–a\ncrucial aspect often overlooked in previous works.\n3) We present a more thorough and inclusive summary of\nthe core methods necessary for constructing reasoning\nLLMs, including but not limited to RL.\n2\nFOUNDATIONS OF REASONING LLMS\nIn this section, we provide a concise overview of the\nprogress in foundational LLMs and the early development\nof key System 2 technologies, highlighting critical advance-\nments that, when combined with foundational LLMs, have\npaved the way for reasoning LLMs. These advancements\ninclude symbolic logic systems, MCTS, and RL.\n2.1\nFoundational LLMs\nThe development of foundational LLMs saw significant\nadvancements with the introduction of pretrained Trans-\nformers [18] in 2018-2019, notably through BERT [19] and\nGPT [21]. These models leveraged unsupervised pretrain-\ning on vast text corpora, followed by fine-tuning for task-\nspecific applications. This approach enabled them to de-\nvelop a broad language understanding before specializing\nin tasks such as sentiment analysis, entity recognition, and\nquestion answering. BERT’s bidirectional context processing\nimproved word understanding, while GPT excelled in text\ngeneration with its unidirectional design.\nThe release of GPT-2 [22] in 2019, with 1.5 billion param-\neters, marked a significant leap in generative performance,\nthough it also raised ethical concerns. GPT-3 [23], with\n175 billion parameters, further demonstrated the power\nof unsupervised pretraining, excelling in few-shot learning\nand performing well across a wide range of NLP tasks. In\nsubsequent years, multimodal models like CLIP [60] and\nDALL-E [61] emerged, integrating text and visual inputs.\nThese models enabled new tasks, such as generating images\nfrom text, and enhanced human-computer interaction.\n\nJOURNAL OF LATEX CLASS FILES, JANUARY 2025\n4\nBy 2023-2024, models such as GPT-4 [62], LLaMA [25],\nand LLaVA [27] demonstrated advanced capabilities in rea-\nsoning, contextual understanding, and multimodal reason-\ning, processing both text and images. The evolution of foun-\ndational LLMs has revolutionized AI, enabling more sophis-\nticated applications in language comprehension, problem-\nsolving, and human-machine collaboration.\nSummary: The development of foundational LLMs has\nprogressed from pretrained transformers like BERT to mul-\ntimodal models such as GPT-4, enhancing language un-\nderstanding, text generation, and image processing. This\nadvancement has led to significant breakthroughs in AI,\nimproving language comprehension, problem-solving, and\nhuman-computer interaction. Building on deep learning\nadvancements [63]–[66], foundational LLMs can learn exten-\nsive world knowledge and semantic relationships from vast\ntextual or multimodal data. This enables them to exhibit\nemergent capabilities such as In-Context Learning (ICL)\n[67], prompt engineering [68], and Chain-of-Thought (CoT)\nreasoning [2], significantly enhancing their adaptability and\ncreative problem-solving abilities.\nDespite this progress, foundational LLMs operate simi-\nlarly to System 1 reasoning, relying on fast, heuristic-driven\ndecision-making and lacking the step-by-step analysis char-\nacteristic of System 2. However, their developments lay\na solid foundation for future reasoning LLMs–especially\nwhen integrated with the following early System 2 technolo-\ngies. This combination paves the way for more versatile,\nflexible, and human-like reasoning models.\n2.2\nSymbolic Logic Systems\nSymbolic logic systems mark the earliest phase of AI, utiliz-\ning rules and logical principles to represent knowledge and\ndraw conclusions [69], [70]. They are particularly effective in\nstructured domains, where formal logic ensures precision.\nProlog, a logic programming language based on first-\norder logic, allows users to define facts, rules, and reason\nthrough queries. It has been pivotal in symbolic reasoning\nsystems, especially in NLP and expert systems [71]–[73].\nLogic-based systems like Prolog employ propositional and\npredicate logic for formal reasoning [74], [75]. From the\n1960s to the early 1980s, this approach dominated AI, with\nsystems like IBM’s LISP [76] for symbolic computation and\nResolution Theorem Provers [77] for automated reasoning.\nIn the 1970s, Marvin Minsky introduced Frames, which or-\nganized knowledge into structured frameworks, influencing\nboth expert systems and cognitive science [78].\nSummary: Symbolic logic systems were pivotal milestones\nin early AI development. Based on formal logic, they ex-\ncelled in well-defined problems, particularly in structured\nenvironments. However, they also exposed the limitations\nof rigid, rule-based systems. Despite these constraints, sym-\nbolic logic remains foundational to the progress of AI.\nRecent advancements in reasoning LLMs have greatly\nenhanced the emulation of human-like System 2 cogni-\ntive processes through sophisticated thought architectures,\nknown as Macro Action frameworks (Section 3.2.4). By\ncombining symbolic templates or rules with foundational\nLLMs, macro actions have significantly improved their rea-\nsoning capabilities. Integrating macro actions into founda-\ntional LLMs has transformed their ability to handle complex\nreasoning tasks, as hierarchical planning allows models to\nmake high-level decisions before delving into specific prob-\nlem details, mirroring symbolic logic’s structured approach.\n2.3\nMonte Carlo Tree Search\nMCTS is a simulation-based search algorithm for decision-\nmaking and planning [79]. It constructs a search tree\nthrough four steps: Selection, which chooses the child node\nwith the highest priority using the UCB1 formula:\nUCB1 = wi\nni\n+ c\n√\nln N\nni\n,\n(1)\nwhere wi is the total reward of node i, ni is its visit count, N\nis the parent node’s visit count, and c balances exploration\nand exploitation. Expansion adds new nodes, Simulation per-\nforms random rollouts to evaluate them, and Backpropagation\nupdates node statistics. MCTS has been widely used in tasks\nsuch as optimizing strategies in board games like Go [80]\nand in robotic path planning, where it helps robots navigate\ndynamic environments effectively [81].\nSummary: MCTS has played a crucial role in the develop-\nment of reasoning LLMs, particularly in Structural Search\n(Section 3.2.1). By simulating potential future reasoning\npaths and backpropagating estimated rewards, MCTS helps\nfoundational LLMs efficiently identify the most promising,\nhigh-reward paths. This process mirrors human-like plan-\nning, where future consequences of decisions are considered\nbefore taking action. By dynamically exploring multiple\nreasoning trajectories, MCTS enables models to avoid get-\nting stuck in suboptimal paths, making it easier to navigate\ncomplex decision spaces. This integration has significantly\nenhanced the ability of LLMs to handle intricate and dy-\nnamic reasoning problems, such as those requiring long-\nterm planning or multi-step logical inferences. It has al-\nlowed LLMs to make more strategic and informed decisions,\nimproving their overall performance in tasks that involve\nnuanced reasoning and strategic exploration.\n2.4\nReinforcement Learning\nRL is a type of machine learning where an agent learns to\nmake decisions by interacting with an environment and re-\nceiving feedback in the form of rewards, aiming to maximize\ncumulative rewards over time [82]. Early breakthroughs in\nRL, such as Q-learning [83] and DQNs [84], revolutionized\nthe field by enabling the handling of complex state spaces\nusing Deep Neural Networks (DNNs) [85]. These methods\npaved the way for scaling RL to real-world tasks, where\ntraditional tabular approaches fell short. The advent of deep\nRL marked a significant step forward, combining the power\nof deep learning with RL to process high-dimensional in-\nputs, such as images and unstructured data.\nA landmark achievement in deep RL was AlphaGo,\nwhich demonstrated RL’s potential by defeating a world\nchampion in the complex game of Go through self-play\n[86]. This success highlighted deep RL’s ability to thrive\nin environments with large, continuous action spaces and\nuncertainty. Building on this, AlphaZero advanced the ap-\nproach by mastering multiple board games—chess, Go, and\nShogi—using self-play, MCTS, and DNNs [87]. AlphaZero’s\nability to learn entirely from scratch, without prior human\n\nJOURNAL OF LATEX CLASS FILES, JANUARY 2025\n5\nReasoning LLMs\nTraditional Reasoning Models\nTraining \nApproach\nAdaptability \n& Learning\nProblem-\nSolving \nStrategy\nGenerality \n& Scalability\nLinear & Deductive Reasoning\nSmall Scale & Domain-Specific\nManual Knowledge Engineering\nRule-based & Symbolic Logic\nFinance\nMedical\nLaw\nDeterministic \nRigid \nRules, Logical \nDeduction \nErrors from \nHuman Experts\nExplicitly \nEncoded \nKnowledge\nExploratory & Multi-path Reasoning\nScalable & Generalize across Tasks\nSelf-Improvement  & Adaptive Reasoning\nData-driven & Probabilistic Reasoning\nLearn Policy\nTake Action\nFeedbacks from \nEnvironment\nTree Search like \nMCTS, ToT\nSelf-correct \nMistakes\nMath\nCode\nChat\n&\n&\nLarge-scale \nCorpus \nProbabilistic \nReasoning\nPPO / DPO / GRPO\nor\nor\nFollowing Logical \nPremises\nStructured \nFrameworks\nFig. 3. A comprehensive comparison of traditional reasoning models and reasoning LLMs. Reasoning LLMs offer significant advantages over\ntraditional models in areas such as training approaches, adaptability and learning, problem-solving strategies, and generality and scalability.\nknowledge, showcased RL’s power in environments requir-\ning long-term strategy and planning.\nAlphaStar further expanded the boundaries of deep RL\nby excelling in the real-time strategy game StarCraft II.\nUnlike board games, StarCraft II presents dynamic, partially\nobservable environments and demands multi-step, real-\ntime decision-making [88]. AlphaStar’s success in this do-\nmain demonstrated deep RL’s capacity to adapt to complex\ndecision-making scenarios that require both strategic plan-\nning and tactical execution. These advancements in RL and\ndeep RL have greatly expanded AI’s potential, transitioning\nfrom well-defined, static environments to dynamic, complex\nsettings that demand continuous learning and adaptation.\nSummary: Deep RL has proven highly effective in solving\ncomplex decision-making tasks. AlphaGo exemplifies this\nby learning strategies through self-play and defeating the\nworld champion in Go. This self-play concept laid the foun-\ndation for Self Improvement technology (Section 3.2.3) in\nreasoning LLMs, both relying on continuous feedback and\nadjustments to optimize strategies.\nIn RL, reward shaping has been crucial, especially for\nmulti-step reasoning tasks [89]. By adjusting the reward\nsignal to provide more granular feedback during intermedi-\nate steps, it helps agents navigate complex decision-making\npaths. This concept inspired the development of Reward\nModeling (Section 3.2.2), particularly the process reward\nmodel, in reasoning LLMs. This model offers step-by-step\nsupervision to identify and correct errors in the reasoning\nprocess. By mimicking human reasoning, the process re-\nward model ensures more robust and interpretable results,\nespecially in tasks like mathematical problem-solving and\ncode generation, where step-by-step evaluation is critical.\nMoreover, RL itself is a powerful tool for reasoning\nLLMs (Section 3.2.5). With a reward mechanism, RL guides\nfoundational LLMs to find optimal solutions, especially in\ndynamic reasoning problems. Its simplicity and efficiency\nmake RL invaluable for training and optimizing reasoning\nLLMs, enhancing the intelligence and self-evolution of AI\nmodels. The integration of RL has led to significant advance-\nments in reasoning LLMs, as demonstrated by DeepSeek-R1\n[31], offering more flexible and efficient solutions.\n3\nBLUEPRINTING REASONING LLMS\nIn this section, we first analyze the features of reasoning\nLLMs from both output behavior and training dynamics\nperspectives. We then provide a detailed overview of the\ncore methods that enable their advanced reasoning capa-\nbilities. Finally, we summarize the evolution of reasoning\nLLMs. A comprehensive comparison of traditional reason-\ning models and reasoning LLMs is shown in Figure 3.\n3.1\nAnalysis of the Features of Reasoning LLMs\n3.1.1\nOutput Behaviour Perspective\nExplore and Planning Structure: Recent empirical studies\nhave revealed that reasoning LLMs demonstrate a strong\ntendency for exploratory behavior in their output structures,\nespecially when compared to models such as WizardMath\n[90] and DeepSeekMath [91], which primarily rely on con-\nventional CoT reasoning approaches. This exploratory be-\nhavior is evident in the formulation of novel hypotheses\nand the pursuit of alternative solution paths. Research by\n[49] suggests that slow-thinking models engage in a la-\ntent generative process, particularly noticeable during the\nprediction of subsequent tokens. This claim is supported\nby [31], which observes that similar behaviors naturally\narise during RL scale training. Furthermore, the Quiet-STaR\nframework [92] introduces an auxiliary pre-training phase\nfocused on next-token prediction, highlighting the critical\nrole of internal deliberation and exploratory mechanisms\nprior to content generation. Collectively, these findings un-\nderscore the complex and dynamic nature of reasoning\nprocesses in advanced LLMs, emphasizing the interaction\nbetween exploration and structured reasoning within their\noperational frameworks.\n\nJOURNAL OF LATEX CLASS FILES, JANUARY 2025\n6\nVerification and Check Structure: Analysis of OpenAI’s\no1 [29] and o3 [30] models indicates that their reasoning\nframeworks incorporate both macro-level actions for long-\nterm strategic planning and micro-level actions, including\n“Wait”, “Hold on”, “Alternatively”, and “Let’s pause”. These\nmicro actions facilitate meticulous verification and itera-\ntive checking processes, ensuring precision in task execu-\ntion. Such a dual-layered approach underscores the mod-\nels’ capacity to balance overarching goals with granular,\ndetail-oriented operations, thereby enhancing their overall\nfunctionality and reliability. To emulate this characteristic,\nMarco-o1 [93], during the MCTS process for constructing\nLong-CoT, assigns each tree node the state of “Wait! Maybe\nI made some mistakes! I need to rethink from scratch”, thereby\nfacilitating the reflective nature of Long-CoT. Huatuo-o1 [94]\nemploys a multi-agent framework to address the issue of\nincorrect CoT generation during validation. This is achieved\nby incorporating a prompt with “Backtracking” and “Correc-\ntion” functionalities, which enables the correction process.\nLonger Inference Length & Time: Recent research [49]–\n[52] indicates that reasoning LLMs often generate outputs\nexceeding 2000 tokens to tackle complex problems in coding\nand mathematics. However, this extended output length can\nsometimes lead to overthinking, where the model spends\nexcessive time on a problem without necessarily improving\nthe solution. Studies [49] highlight that while autoregressive\ngeneration and Classic CoT can effectively solve simpler\nproblems, they struggle with more complex tasks. Research\n[95], [96] shows that in multimodal domains, many prob-\nlems demand careful observation, comparison, and delib-\neration. Additionally, Search-o1 [97] suggests that slow-\nthinking mechanisms are particularly beneficial in areas\nrequiring external knowledge or where potential knowledge\nconflicts arise. In medical scenarios, complex problems, such\nas those requiring test-time scaling techniques, demonstrate\nsignificant improvements [52].\nOverly Cautious & Simple Problem Trap: Currently, rea-\nsoning LLMs have demonstrated strong performance in do-\nmains such as competitive-level mathematics [31], [54], [98],\n[99], complex coding [100], medical question answering [52],\n[94], and multilingual translation [93], [101]. These scenarios\nrequire the model to perform fine-grained analysis of the\nproblem and execute careful logical reasoning based on\nthe given conditions. Interestingly, even for straightforward\nproblems like “2+3=?”, reasoning LLMs can exhibit over-\nconfidence or uncertainty. Recent research [102] notes that\no1-like models tend to generate multiple solution rounds for\neasier math problems, often exploring unnecessary paths.\nThis behavior contrasts with the lack of diverse exploratory\nactions for simpler questions, indicating a potential ineffi-\nciency in the model’s reasoning process.\n3.1.2\nTraining Dynamic Perspective\nAmazing Data Efficiency: Unlike traditional approaches\nthat focus on expanding instruction sets with uniformly\ndistributed difficulty levels, Studies [52], [54] suggest that\nconstructing Slow-thinking CoT datasets with a focus on\nhard samples leads to better generalization in fields like\nmedicine and mathematics. This approach diverges from\nthe conventional practice of collecting diverse and evenly\ndistributed instruction datasets.\nSelf \nImprovement\nSlow-thinking\nStructure \nSearch\nReinforcement\nFine-Tuning\nReasoning LLMs\nMacro \nAction\nReward \nModeling\nFig. 4. The core methods enabling reasoning LLMs.\nSparse Training Method: Contrary to conventional wisdom,\nthe development of effective reasoning LLMs does not re-\nquire extensive datasets or dense reward signals. For exam-\nple, STILL2 [51] demonstrated impressive performance us-\ning only 5,000 distilled samples, while Sky-T1 [99] achieved\nperformance parity with QwQ [98] using just 17,000 Long-\nCoT samples. Similarly, RedStar [54] achieved exceptional\nresults across both textual and multimodal tasks with only\n4,000 core LongCoT samples. In comparison to simple CoT,\nSlow-thinking Supervised Fine-Tuning (SFT) data exhibits\nremarkable sample efficiency, often delivering comparable\nresults with just 1/100th of the sample size. Additionally,\nresearch [103] emphasizes the significant training potential\nof online RL scaling algorithms, suggesting that non-dense\nRL supervision and even rule-based reward structures are\nsufficient for achieving high performance.\nParameter Characteristic: Training LLMs for slow-thinking,\nas characterized by the LongCoT approach, results in rel-\natively uniform gradient norms across different layers. In\ncontrast, fast-thinking, exemplified by the simplified CoT\nmethod, generates larger gradient magnitudes in the earlier\nlayers, along with significant variability in gradient norms\nacross layers. Empirical evidence suggests that larger mod-\nels, particularly those exceeding 30 billion parameters, are\nmore compatible with reasoning LLMs training due to their\nenhanced capacity for complex reasoning. Additionally, ex-\nperiments conducted by RedStar [54] show that the benefits\nof data scaling vary across model sizes, with scaling effects\nbeing more pronounced and effective in larger models. This\nfinding is supported by Deepseek-R1’s research [31], which\ndemonstrates that a 670-billion-parameter model achieves\nperformance metrics closely approximating those of the o1\nbenchmark, highlighting the scalability advantages of larger\narchitectures in advanced reasoning tasks.\n\nJOURNAL OF LATEX CLASS FILES, JANUARY 2025\n7\nTABLE 1\nSummary of Structure Search method.\nCategory\nReasoning LLMs\nCharacteristic\nActions\nReasoning Steps as Nodes\nRAP [14], ORM [104], Forest-of-Thought [105]\nActions represent intermediate reasoning steps.\nToken-level Decisions\nCodeTree [106], SPaR [107], TreeBoN [108]\nActions involve generating tokens.\nTask-specific Structures\nCWM [109], LLM-MCTS [110]\nActions are domain-specific.\nCorrection and Exploration\nRethinkMCTS [111], MCTSr [112]\nActions emphasize revisiting, refining, or backtracking to improve previous reasoning steps.\nRewards\nOutcome-based Rewards\nMC-NEST [113]\nCorrectness or validity of the final outcome.\nStepwise Evaluations\nRAP [14], SRA-MCTS [114]\nRewards are assigned at intermediate steps.\nSelf-evaluation Mechanisms\nSPaR [107], TreeBoN [108], MindStar [115]\nRewards rely on the model’s own confidence.\nDomain-specific Criteria\nLLM-MCTS [110], SR-MCTS [116]\nRewards are tailored to specific tasks.\nIterative Preference Learning\nLLaMA-Berry [117], Marco-o1 [93], ReST-MCTS* [118]\nRewards derive from comparing multiple solutions.\n3.2\nCore Method\nIn this section, we provide an overview of the core methods\nthat drive the advanced reasoning capabilities of reasoning\nLLMs, as shown in Figure 4. These include Structure Search,\nReward Modeling, Self Improvement, Macro Action, and\nReinforcement Fine-Tuning. We also highlight representa-\ntive reasoning LLMs for each method.\n3.2.1\nStructure Search\nReasoning LLMs aim to achieve high accuracy and depth\nin solving complex problems by emulating the deliberate\nand methodical nature of human reasoning. However, de-\nspite recent advancements, current foundational LLMs face\ninherent limitations when addressing intricate reasoning\ntasks. These limitations arise from their lack of an inter-\nnal world model to simulate environmental states, their\ninability to predict the long-term outcomes of reasoning\npaths, and their failure to iteratively refine reasoning steps\nbased on future states or rewards [8]. As a result, these\nshortcomings hinder foundational LLMs from effectively\nbalancing exploration and exploitation in vast reasoning\nspaces, creating challenges in tasks that require multi-step\nreasoning, such as complex mathematics, logical inference,\nor strategic decision-making [119].\nMCTS, a powerful search and optimization algorithm,\neffectively addresses these challenges by providing a struc-\ntured framework to explore and evaluate reasoning paths\nsystematically. It operates by constructing a reasoning tree,\nwhere each node represents a reasoning state, and ac-\ntions expand the tree by considering potential next steps.\nThrough the simulation of future states and the iterative\nbackpropagation of estimated rewards, MCTS allows foun-\ndational LLMs to efficiently identify high-reward reasoning\npaths, mirroring human planning processes. This approach\naligns with the core principles of reasoning LLMs, where\nthorough analysis and deliberate exploration are essential\nfor generating well-reasoned outputs. Recent methods, such\nas RAP [14], enhance foundational LLMs by integrating\nMCTS with a world model, enabling the system to itera-\ntively refine intermediate reasoning steps and improve fu-\nture predictions. Similarly, Forest-of-Thought [105] utilizes\nMCTS to dynamically explore multiple reasoning trajecto-\nries, revisiting flawed paths and refining outcomes.\nThe application of MCTS in reasoning tasks extends\nbeyond traditional problem-solving to highly specialized\ndomains. For example, frameworks like SRA-MCTS [114]\nand MC-NEST [120] showcase the utility of MCTS in tack-\nling technical challenges such as code generation and math-\nematical reasoning, where intermediate steps are iteratively\nevaluated and refined. In fields like instructional alignment,\nframeworks such as SPaR [107] and Marco-o1 [93] leverage\nMCTS to refine responses and align reasoning trajectories\nwith human preferences or desired outcomes. Additionally,\ntask-specific implementations like HuatuoGPT-o1 [94] un-\nderscore MCTS’s crucial role in navigating highly special-\nized domains, such as medical reasoning, where accuracy\nand robustness are paramount.\nMCTS also enables models to go beyond single-pass\nreasoning methods, such as CoT or Tree-of-Thought, by\nincorporating mechanisms to revisit, critique, and refine\nreasoning steps dynamically [111], [121]. This iterative ca-\npability is essential for tackling tasks with vast decision\nspaces or those requiring long-term planning, where ear-\nlier decisions can significantly impact final outcomes. By\nallowing LLMs to simulate, evaluate, and refine multiple\nreasoning paths, MCTS introduces a level of adaptability\nand strategic exploration that traditional approaches lack.\nAs shown by AlphaZero-like tree-search [104] and Search-o1\n[97], MCTS enables reasoning LLMs to not only achieve bet-\nter performance on specific tasks but also exhibit enhanced\ngeneralization capabilities across diverse domains.\nThe integration of MCTS into LLMs depends on defining\nactions and rewards to guide reasoning path exploration\nand assess quality. As shown in Table 1, we classify the\nactions in prior work into four categories:\n1) Reasoning Steps as Nodes: Actions represent inter-\nmediate reasoning steps or decisions, such as select-\ning rules, applying transformations, or generating sub-\nquestions [14], [104], [105], [119].\n2) Token-level Decisions: Actions involve generating to-\nkens or sequences (e.g., the next word, phrase, or code\nsnippet) [106]–[108], [122].\n3) Task-specific Structures: Actions are domain-specific,\nsuch as moving blocks in blocksworld, constructing\ngeometry in geometry problem-solving, or modifying\nworkflows in task planning [109], [110], [123].\n4) Self-correction and Exploration: Actions focus on re-\nvisiting, refining, or backtracking to improve previous\nreasoning steps [111], [112], [124].\nAdditionally, as shown in Table 1, we classify the reward\ndesign into five categories:\n1) Outcome-based Rewards: Rewards focus on the cor-\nrectness or validity of the final outcome or solution,\nincluding the validation of reasoning paths or task\nsuccess [113], [119], [123].\n2) Stepwise Evaluations: Rewards are assigned at inter-\nmediate steps based on the quality of each step or its\ncontribution toward the final outcome [14], [104], [114].\n\nJOURNAL OF LATEX CLASS FILES, JANUARY 2025\n8\nTABLE 2\nSummary of Reward Modeling method.\nCategory\nMethods\nData Source\nModel Refinement\nApplications\nCharacteristic\nStrategy\nLearning\nORM\nDIVERSE [127]\nPrompting\nFine-tuning\nSFT\nMultiple Reasoning Tasks\nWeighted Voting Verifier\nMATH-SHEPHERD [128]\nSampling\nFeedback-guided\nSFT & RL\nMath Reasoning\nCorrectness Score Assignment\nAutoPSV [129]\nPrompting\nFeedback-guided\nSFT\nMath / Commonsense Reasoning\nAutomated Process Supervision\nImplicit PRMs [130]\nSampling\nFine-tuning\nSFT & RL\nMath Reasoning\nObtaining PRM from ORM\nOVM [131]\nSampling\nFeedback-guided\nSFT\nMath Reasoning\nGuided Decoding\nMCTS\nReST-MCTS∗[132]\nSampling\nSelf-training\nSFT & RL\nMultiple Reasoning Tasks\nMCTS and Self-training\nOmegaPRM [133]\nMCTS with Binary Search\nFeedback-guided\nSFT\nMath Reasoning\nDivide-and-Conquer MCTS\nReARTeR [134]\nSampling\nFeedback-guided\nSFT & RL\nQA\nRetrieval-Augmented Generation\nConsensus Filtering [135]\nMCTS Data Construction\nFeedback-guided\nSFT\nMath Reasoning\nConsensus Filtering Mechanism\nPRM\nORPS [136]\nSampling\nFeedback-guided\nSFT\nCode Generation\nSupervising Outcome Refinement\nStep-DPO [137]\nSampling\nFeedback-guided\nSFT & RL\nMath Reasoning\nStep-wise Preference Pairs\nAdaptiveStep [138]\nResponse Dividing\nFeedback-guided\nSFT\nMath Reasoning, Code Generation\nDividing Reasoning Steps\n3) Self-evaluation Mechanisms: Rewards rely on the\nmodel’s own confidence or self-assessment (e.g., like-\nlihood, next-word probability, or confidence scores)\n[107], [108], [115].\n4) Domain-specific Criteria: Rewards are tailored to spe-\ncific tasks, such as symmetry and complexity in ge-\nometry or alignment with human preferences in text\ngeneration [110], [116], [123].\n5) Iterative Preference Learning: Rewards are derived\nfrom comparing multiple solutions or reasoning paths,\nguiding learning dynamically [93], [117], [118].\nSummary: Despite its advantages, structure search-based\n(i.e., MCTS) reasoning LLMs often suffer from substantial\ncomputational overhead due to the large number of sim-\nulations required. This makes them less suitable for tasks\nthat demand real-time decision-making or operate under\nresource constraints [125]. Additionally, the effectiveness\nof MCTS is highly dependent on well-designed reward\nmechanisms and action definitions, which can vary signif-\nicantly across different domains, thus posing challenges to\nits generalizability [126].\n3.2.2\nReward Modeling\nTwo primary training paradigms are used to tackle multi-\nstep reasoning tasks: outcome supervision and process su-\npervision. Outcome supervision emphasizes the correctness\nof the final answer at a higher level of granularity, and the\nresulting model is referred to as the Outcome Reward Model\n(ORM) [32], [139]. In contrast, process supervision provides\nstep-by-step labels for the solution trajectory, evaluating\nthe quality of each reasoning step. The resulting model is\nknown as the Process Reward Model (PRM) [37], [140],\n[141]. The main distinction between ORM and PRM is\nillustrated in Figure 5.\nPRM offers significant advantages [128], [142] in com-\nplex reasoning tasks for several key reasons. First, it pro-\nvides fine-grained, step-wise supervision, allowing for the\nidentification of specific errors within a solution path. This\nfeature is especially valuable for RL and automated error\ncorrection. Second, PRM closely mirrors human reasoning\nbehavior, which relies on accurate intermediate steps to\nreach correct conclusions. Unlike ORM, PRM avoids situ-\nations where incorrect reasoning can still lead to a correct fi-\nnal answer, thus ensuring more robust and interpretable rea-\nsoning. While PRM has primarily been applied to complex\nq\ns1\nP\nR\nM\nO\nR\nM\na\ns2 … sn\nq\ns1\na\ns2 … sn\nr\nr1\nr2\nrn\nra\nQuestion\nReasoning steps\nAnswer\nReward\nFig. 5. The comparison between ORM and PRM for assessing a com-\nplete solution trajectory. ORM only provides a single reward based on\nthe correctness of the final answer, while PRM evaluates the quality of\neach reasoning step throughout the process.\nmathematical problems, its benefits have recently driven\napplications in other fields. For instance, ORPS [136] uti-\nlizes PRM to address complex code generation challenges,\nwhile Step-DPO [137] combines process supervision with\nthe Direct Preference Optimization (DPO) algorithm [143]\nto improve long-chain mathematical reasoning. A summary\nof Reward Modeling method is presented in Table 2.\nSummary: Despite the advantages of PRMs, they present\nseveral challenges. The primary difficulty is obtaining pro-\ncess supervision-labeled data, which is often both costly and\ntime-consuming. To address concerns related to scalability,\nefficiency, and accuracy, researchers have explored vari-\nous automated annotation methods. For example, MATH-\nSHEPHERD [128] utilizes the correctness of the final an-\nswer to define the quality of intermediate steps based on\ntheir potential to lead to the correct outcome, automating\nthe step-wise data collection process. ReST-MCTS∗[132]\ncombines process reward guidance with MCTS to generate\nhigher-quality reasoning traces through extensive rollouts.\nSimilarly, OmegaPRM [133] employs the MCTS framework\nwhile introducing a divide-and-conquer algorithm for auto-\nmated process supervision data generation. Another novel\napproach involves using ORM to train a PRM. Yuan et\nal. [130] propose training a PRM implicitly by leveraging\n\nJOURNAL OF LATEX CLASS FILES, JANUARY 2025\n9\nTABLE 3\nSummary of Self Improvement method.\nStage\nMethods\nData Source\nModel Refinement\nApplications\nFeedback\nStrategy\nTraining\nSTaR [144]\nFew-shot\nLanguage Model\nSFT\nQA, Arithmetic Reasoning\nQuiet-STaR [92]\nToken-level Exploration\nLanguage Model\nRL\nQA, Arithmetic Reasoning\nV-STaR [145]\nSampling\nVerifier\nSFT\nArithmetic Reasoning, Code Generation\nB-STaR [146]\nSampling\nReward Model\nSFT\nArithmetic Reasoning, Code Generation\nrStar-Math [147]\nMCTS Data Construction\nReward Model\nSFT\nArithmetic Reasoning\nReST [148]\nSampling\nReward Model\nRL\nMachine Translation\nReST-EM [149]\nSampling\nLanguage Model\nEM for RL\nArithmetic Reasoning, Code Generation\nReST-MCTS* [132]\nSampling\nReward Model\nSFT, RL\nReasoning\nENVISIONS [150]\nSampling\nEnvironment Guided\nSFT\nWeb Agents, Reasoning\nRISE [151]\nSampling\nReward Function\nWeighted SFT\nArithmetic Reasoning\nSTIC [152]\nFew-shot\nLanguage Model\nSFT\nVision Language Model Tasks\nSIRLC [153]\nQuestion Answeing\nLanguage Model\nRL\nReasoning, Translation, Summary\nAlpacaFarm [154]\nExisting Data\nLanguage Model\nSFT\nNone (Intrinsic Evaluation)\nInference\nSelf-Refine [155]\nIndependent of Training Data\nLanguage Model\nFew-shot Demonstration\nCode Generation, Sentiment Reversal, Acronym Generation\nSelf-Check [156]\nIndependent of Training Data\nLanguage Model\nStep Check\nQA, Arithmetic Reasoning\nCRITIC [157]\nIndependent of Training Data\nLanguage Model\nExternal Tools\nQA, Arithmetic Reasoning, Detoxification\nROSE [158]\nIndependent of Training Data\nLanguage Model\nDistributed Prompt\nSafety, Knowledge\nSelf-Verification [159]\nIndependent of Training Data\nLanguage Model\nRe-Ranking\nArithmetic Reasoning\nSelfEval-Decoding [160]\nIndependent of Training Data\nLanguage Model\nBeam Search\nAritnmetic/Symbolic Reasoning\nIPS [161]\nIndependent of Training Data\nLanguage Model\nConstrained Decoding\nDialogue\nControl-DAG [162]\nIndependent of Training Data\nLanguage Model\nConstrained Decoding\nDialogue, Open-domain Generation\nLook-Back [163]\nIndependent of Training Data\nLanguage Model\nContrastive Decoding\nAlleviating Repetitions\nLeCo [164]\nIndependent of Training Data\nLanguage Model\nConstrained Decoding\nQA, Reasoning\nORM training on cheaper datasets, under mild reward\nparameterization assumptions. They also provide theoret-\nical guarantees for the performance of this implicit PRM,\ndemonstrating its practicality and cost-effectiveness.\nIn addition to data collection, PRMs face challenges\nrelated to trustworthiness [134], categorized as follows:\n1) Lack of Explanations: Current PRMs often generate\nscores for reasoning steps without sufficient explana-\ntions, limiting interpretability and hindering their use-\nfulness in refining reasoning during test-time.\n2) Bias in Training Data: Data collection methods, such\nas MCTS, tend to introduce distributional biases, as-\nsigning disproportionately higher scores to the majority\nof questions. As a result, PRMs struggle to effectively\nidentify erroneous reasoning steps.\n3) Early-Step Bias: PRMs show lower accuracy in pre-\ndicting rewards for earlier reasoning steps compared to\nthose closer to the final answer. This issue stems from\nthe increased randomness and uncertainty associated\nwith the initial steps in the reasoning process.\n3.2.3\nSelf Improvement\nReasoning LLMs exemplify a progression from weak to\nstrong supervision, while traditional CoT fine-tuning faces\nchallenges in scaling effectively. Self improvement, using the\nmodel’s exploration capabilities for self-supervision, gradu-\nally enhances LLMs performance in tasks such as translation\n[148], mathematical reasoning [144], [149], and multimodal\nperception [152]. This approach fosters exploration and ap-\nplication within reasoning LLMs [147], [165]. A summary of\nSelf Improvement method is presented in Table 3.\nTraining-based self improvement in LLMs can be cat-\negorized based on exploration and improvement strate-\ngies. The exploration phase focuses on data collection to\nfacilitate subsequent training improvements, with notable\nvariations in approach. STaR [144] uses few-shot examples\nfor data gathering, while ReST [148], ReST-EM [149], and\nENVISIONS [150] rely on multiple samplings of complete\ntrajectories. Quiet-STaR [92] explores at the token level, in-\ntroducing concepts like meta-tokens and non-myopic loss to\nenhance supervision. Additionally, ReST-MCTS* [132] and\nrStar-Math [147] generate training data through MCTS.\nImprovement strategies also exhibit significant diversity.\nFor instance, STaR and its derivatives, such as V-STaR [?]\nand B-STaR [146], combine filtering with SFT. ReST and its\nvariants typically introduce innovative reward calculation\nmethods to enhance RL training for policy models. RISE\n[151] incorporates external feedback, recording rewards and\nrefining responses through distillation during the improve-\nment process. Notably, rStar-Math [147] demonstrates that\nsmall models have achieved System 2 reflective capabilities\nthrough self-evolving training approaches.\nTest-time self improvement leverages the consistency\nof a model’s internal knowledge to correct hallucinations\nduring inference. These approaches can be categorized\ninto three main types: methods that refine answers using\nprompts [155], [156], approaches that utilize external tools\n[157], and techniques that leverage logits without the need\nfor external tools or prompts [163], [164].\n3.2.4\nMacro Action\nRecent advancements in LLMs have driven progress in emu-\nlating human-like System 2 cognitive processes via sophisti-\ncated thought architectures, often referred to as macro action\nframeworks. These structured reasoning systems go beyond\ntraditional token-level autoregressive generation by intro-\nducing hierarchical cognitive phases, such as strategic plan-\nning, introspective verification, and iterative refinement.\nThis approach not only enhances the depth of reasoning but\nalso broadens the solution space, enabling more robust and\ndiverse problem-solving pathways. A summary of Macro\nAction method is presented in Table 4.\nWe classify the progress of macro action into two aspects:\n1) Test-time Scaling through Macro Action Operational-\nization: Recent research identifies two key method-\nologies for improving reasoning performance during\n\nJOURNAL OF LATEX CLASS FILES, JANUARY 2025\n10\nTABLE 4\nSummary of Macro Action method.\nMethods\nUsage\nAction Attribute\nRepresentative Action\nAction Source\nAction Number\nLearning\nReflection\nModality\nSelf-Check [166]\nVerification\nHuman-Designed\n4\nICL\n✓\nT\nTarget Extraction, Information Collection, Step Regeneration, Result Comparsion\nLeMa [167]\nSynthetic Data\nHuman-Designed\n3\nICL & SFT\n✓\nT\nIncorrect Step Recognition, Explanation, Correct Solution:\nREFINER [168]\nVerification/Exploration\nHuman-Designed\n2\nICL & SFT\n✓\nT\nCritic, Generate\nHiICL-MCTS [169]\nExploration\nHuman-Designed\n5\nICL\n✓\nT\nSystem Analysis, One-Step Thought, Divide and Conquer, ..., Self-Reflection and Refinement\nSUPERCORRECT [170]\nDistill\nIn-Context Learning\nDynamic\nSFT & RL\n✗\nT\n–\nReasonFlux [171]\nSynthetic Data/Exploration\nHuman-Designed\n∼500\nICL & SFT & RL\n✗\nT\n–\nrStar [172]\nExploration\nHuman-Designed\n5\nICL & RL\n✓\nT\nOne-step thought, Propose Next Sub-question & Answer, ..., Rephrase question\nLLaMA-Berry [173]\nExploration\nHuman-Designed\n2\nICL & RL\n✓\nT\nReflection, Error Re-correction\nHuatuo-o1 [94]\nSynthetic Data\nHuman-Designed\n4\nICL & SFT\n✓\nT\nBacktracking, Exploring New Paths, Verification, Correction\nMarco-o1 [93]\nVerification\nHuman-Designed\n1\nICL & SFT\n✓\nT\nReflection\nBoT [174]\nExploration\nIn-Context Learning\nDynamic\nICL\n✗\nT\nSolving Quadratic Equation, Array Sorting, ..., Search Algorithms)\nrStar-Math [147]\nExploration\nIn-Context Learning\n1\nICL & RL\n✓\nT\nPython comment\nMulberry [175]\nSynthetic Data\nIn-Context Learning\n1\nICL & SFT\n✓\nT\nReflection\nLLaVA-CoT [176]\nSynthetic Data/Exploration\nHuman-Designed\n4\nSFT\n✗\nI T\nSummary, Caption, Reasoning, Conclusion\nLLaMAV-o1 [177]\nVerification/Exploration\nHuman-Designed\n4173\nCurriculum Learning\n✓\nI T\nDetailed Caption Generation, Logical Reasoning, ... Final Answer Generation\nAtomThink [178]\nSynthetic Data/Exploration\nIn-Context Learning\n>100\nSFT & RL\n✓\nI T\nVariable Definition, Calculations, Graphs Analysis , ..., Verification\nRedStar [54]\nDistill\nHuman-Designed\n2\nSFT\n✓\nI T\nWait, Alternately\nAuto-CoT [179]\nExploration\nIn-Context Learning\n2\nICL\n✗\nT\nQuestion clustering, Demonstration Sampling\nPoT [180]\nVerification\nIn-Context Learning\n1\nICL\n✗\nT\nCode language conversion\nPAL [181]\nVerification\nIn-Context Learning\n1\nICL\n✗\nT\nCode language conversion\nDecomposed Prompt [182]\nExploration\nHuman-Designed\n3\nICL\n✗\nT\nPeoblem Split, Subproblem Solving, Answer Merge\nLeast-to-Most [183]\nExploration\nHuman-Designed\n2\nICL\n✗\nT\nProblem Decomposition, Subproblem Solving\ninference and test-time scaling. HiICL-MCTS [169] em-\nploys a deliberate search through seed data to gener-\nate action-chain templates consisting of macro actions,\nthereby facilitating an action-chain-guided approach to\ntest-time reasoning. ReasonFlux [171] utilizes an iter-\native test-time scaling framework, harnessing external\nhigh-level thought templates to iteratively refine and\nupdate the current CoT.\n2) Macro Action-Enhanced Data Synthesis Paradigms: A\nkey application of macro actions in complex reasoning\nis in the synthesis of reasoning data. In data synthesis\nand training frameworks, macro action architectures\nenhance reasoning diversity and generalization. Recent\nresearch has shown that integrating or synthesizing a\nCoT process with macro actions within the reasoning\nsequence can significantly improve the data efficiency\nof the reasoning chain. For instance, LLaVA-CoT [176]\nenhances CoT data synthesis by externalizing interme-\ndiate reasoning steps across multiple modalities. Atom-\nThink [178] generates the AMATH-SFT dataset using\na structured g1 prompt [184], achieving superior per-\nformance on long-horizon reasoning tasks compared to\ntraditional CoT approaches. CoAct [185] introduces a\ndual-agent collaborative reasoning framework, where\na global planning agent executes overarching macro-\nactions, while a local execution agent carries out specific\nsub-actions within those broader actions.\nMacro actions also play a crucial role in enhancing Self-\nimprovement frameworks. rStar-Math [147] utilizes high-\nlevel deliberate search through Code-augmented CoT, gen-\nerating diverse and reliable solutions while achieving proac-\ntive search capabilities. Satori [186] integrates CoT with\nRL, incorporating “<reflect>”-style macro actions to diversify\nexploration and alleviate policy saturation in online RL en-\nvironments. Huatuo-o1 [94] combines hierarchical planning\nwith domain-specific knowledge bases to improve medical\nreasoning. Additionally, ReasonFlux [171] dynamically re-\nconfigures reasoning templates (e.g., breaking down calcu-\nlus problems into symbolic and numeric phases) to align\nwith the problem structure.\n3.2.5\nReinforcement Fine-Tuning\nReinforcement Fine-Tuning (RFT) [187] is an innovative\ntechnique recently introduced by OpenAI, designed to en-\nable developers and engineers to fine-tune existing models\nfor specific domains or complex tasks. Unlike general SFT,\nRFT focuses on optimizing the model’s reasoning process by\nusing a reward mechanism to guide the model’s evolution,\nthereby enhancing its reasoning capabilities and accuracy.\nThe core of RFT lies in improving the model’s performance\nin a specific domain with minimal high-quality training\ndata [188], an appropriate reward model [189], and a stable\noptimization process [190]. A summary of RFT method is\npresented in Table 5.\nDeepSeek-R1 [31], which employs a verifier reward-\nbased strategy, has shown significant performance improve-\nments compared to traditional methods like SoS [191]. Key\nadvantages include:\n1) Simplified Training Pipeline: RL supervision stream-\nlines data construction and training processes, eliminat-\ning the need for complex stepwise search mechanisms.\n2) Enhanced Scalability: Online RL training facilitates\nefficient scaling on large datasets, particularly for com-\nplex reasoning tasks.\n3) Emergent Properties: DeepSeek-R1 [31] demonstrates\nunique emergent capabilities, such as Long-CoT reason-\ning, which are difficult to achieve through SFT alone.\nDespite its strengths, RFT faces the following challenges:\n1) Unclear Mechanism behind Reasoning: The underly-\ning mechanisms driving the reasoning improvements in\nDeepSeek-R1 remain poorly understood. For example,\nwhile DeepSeek-R1 exhibits emergent properties (e.g.,\n“Emergent Length Increasing”, “Aha moments”), stud-\nies such as [219] suggest that capabilities like Long-CoT\nmight already exist in the base model, rather than solely\nemerging from RL training. Furthermore, performance\ngains observed in smaller models (e.g., Qwen-Math-\n2B/7B [220]) occur without noticeable “Aha moments”,\ncomplicating causal interpretations.\n2) Reward Model Saturation: Many existing RL algo-\nrithms face reward model saturation, typically mani-\nfested as exploration collapse after around 100 train-\ning steps. Although DeepSeek-R1 alleviates this issue\nthrough specialized reward formatting, methods like\nReFT [189] and Satori [186] propose alternating sam-\npling and SFT distillation to combat reward hacking\nand exploration collapse.\n3) Unstable\nLong-CoT\nGeneration:\nLong\nreasoning\nchains generated by RFT are prone to instability, includ-\ning context overflow, failure to return final answers, and\n\nJOURNAL OF LATEX CLASS FILES, JANUARY 2025\n11\nTABLE 5\nSummary of RFT method.\nMethods\nModel Attribute\nIncentivize Attibute\nApplication & Benchmark\nFoundational LLMs\nModality\nReward Type\nAlgorithm\nLearning\nIncentivize Sample\nReason RFT Project\nDeepSeek-R1-Zero [31]\nDeepSeek-V3\nT\nRule-Outome-Reward\nGPRO\nRL\n800K\nMultiple Tasks\nDeepSeek-R1 [31]\nDeepSeek-V3\nT\nRule-Outcome-Reward\nGPRO\nRL & SFT\n800K\nMultiple Tasks\nKimi v1.5 [192]\n–\nI T\nRule-Outcome-Reward\nPPO∗\nRL & SFT\n–\nMultiple Tasks\nReFT [189]\nGalactica, CodeLLama\nT\nRule-Outcome-Reward\nPPO∗\nRL & SFT\n3k/7k/8k/15k\nGSM8k/SVAMP/MathQA\nRFTT [193]\nLLaMA-3-3/8B-Instruct,Qwen-2.5-7B-Instruct\nT\nRule-Outcome-Reward\nReinforce++\nRL & SFT\n1.2K\nMultiple Math Task\nSatori [186]\nQwen-2.5-Math-7B\nT\nRule-Outcome-Reward\nPPO\nRL & SFT\n66K\nMultiple Math Task\nQCLASS [194]\nLlama-2-7B-Chat\nT\nProcess-Reward\nQNet\nRL & SFT\n1.9K/1.5K/3.3K\nWebShop, ALFWorld, SciWorld\nPRIME [195]\nQwen2.5-Math-7B\nT\nRule-Process-Outcome-Reward\nPPO\nRL & SFT\n150K\nMath, Code Tasks\nDeepScaleR [196]\nDeepSeek-R1-Distill-Qwen-1.5B\nT\nRule-Outcome-Reward\nIteratively GPRO\nRL\n40K\nMultiple Math Task\nPURE [197]\nQwen2.5-Math-7B\nT\nRule-Process-Outcome-Reward\nPPO+RLOO\nRL\n8K\nMultiple Math Task\nSimpleRL [103]\nQwen2.5-Math-7B\nT\nRule-Outcome-Reward\nPPO\nRL\n8K\nMultiple Math Task\nOpen-R1 [198]\nQwen2.5-1.5B-Instruct\nT\nRule-Outcome-Reward\nGPRO\nRL & SFT\n8K\nMultiple Math, Code Task\nTinyZero [199]\nQwen2.5-0.5B/3B\nT\nRule-Outcome-Reward\nGPRO\nRL\n–\nCountDown Task\nOta-Zero [200]\nQwen-2.5-Series, DeepSeek-Series, Rho, Llama-3.x\nT\nRule-Outcome-Reward\nGRPO\nRL\n0.5K\nCountDown Task\nOta [201]\nRHO-1b/Qwen2.5-3B\nT\nRule-Outcome-Reward\nGPRO/PPO\nRL\n7.5K\nGSM8K\nLIMR [202]\nQwen-Math-7B\nT\nRule-Outcome-Reward\nPPO\nRL\n1.3K\nMultiple Math Task\nCritic-RL [203]\nQwen2.5-Coder-32B\nT\nRule-Outcome-Reward\nGPRO∗\nRL & SFT\n18.8K\nMultiple Code Task\nLogic-R1 [204]\nQwen2.5-7B-Instruct-1M\nT\nRule-Outcome-Reward\nREINFORCE++∗\nRL\n5K\nMultiple Math, Logic Task\nOnline-DPO-R1 [205]\nQwen2.5-MATH-7B\nT\nRule-Outcome-Reward\nDPO\nRL& SFT\n207.5K\nMultiple Math Task\nOpenReason-Zero [206]\nQwen-2.5-7B/32B\nT\nRule-Outcome-Reward\nPPO\nRL\n57K\nMultiple Math Task, GPQA, MMLU\nRLHF-V [207]\nOmniLMM-12B\nI T\nProcess-Reward\nDDPO\nRL\n1.4K\nMultiple Tasks\nRLAIF [208]\nPaLM 2 Extra-Small\nT\nRule-Outome-Reward\nRLAIF\nRL\n–\nSummary and Conversation Generation\nMM-RLHF [209]\nLLaVA-onevision-7B\nI T V\nProcess-Reward\nMM-DPO\nRL\n120K\nMM-RLHF-RewardBench/SafetyBench\nAlign-DS-V [210]\nLLaVA-v1.5-7B,Qwen2-VL\nI T V\nProcess-Reward\nPPO, DPO\nRL & SFT\n200K\nAlign-Anything, Eval-Anything\nR1V [211]\nQwen2-VL,Qwen2.5-VL\nI T\nRule-Outome-Reward\nGRPO\nRL\n70K/70K/8K\nMultiple Tasks\nVLM-R1 [212]\nQwen2.5-VL\nI T\nRule-Outome-Reward\nGRPO\nRL\n120K\nMultiple Tasks\nLMM-R1 [213]\nQwen2.5-VL\nI T\nRule-Outome-Reward\nPPO/RLOO\nRL\n8K\nMultiple Tasks\nOpen-R1-Video [214]\nQwen2-VL-7B\nI T V\nRule-Outome-Reward\nGRPO\nRL\n4K\nMultiple Tasks\nEasy-R1 [215]\nQwen2.5-VL\nI T\nRule-Outome-Reward\nGRPO\nRL\n3K\nMultiple Tasks\nAnalysis RFT Project\nDemystify-LongCoT [216]\nLlama-3.1-8B, Qwen2.5 -7B-Math\nT\nRule-Outcome-Reward\nPPO/Reinforce++\nRL & SFT\n7.5K\nMultiple Math, MMLU\nRLHF-Scale [217]\nGLM4-9B\nT\nProcess-Reward\nPPO\nRL\n11K\nMultiple Tasks\nMD-CoT [218]\n–\n–\nProcess-Reward\nPPO\nRL\n–\n–\nsensitivity to reward shaping [102]. For instance, meth-\nods like [216] inadvertently introduce cosine reward\nfunctions, which degrade performance with increased\niterations. O1-Prune [221] uses post-hoc length pruning\ntechniques [192] (via RL/SFT) to stabilize outputs.\nFuture directions for RFT may include several exciting\nand innovative advancements, such as:\n1) Efficient and Stable RL Frameworks: There is a need\nto develop more robust RL algorithms that prevent re-\nward saturation and exploration collapse. [216] reveals\nthat REINFORCE++ [222] underperforms when com-\nbined with KL divergence regularization, suggesting\nthe need for alternative methods. Future work should\nrevisit classic RL algorithms in the context of modern\nLLMs training to optimize both stability and efficiency.\n2) Scaling RFT: Current RL-Supervise models rely on\ncurated, verifiable prompts selected from large-scale\ndatasets. Future research should focus on synthesizing\nhigh-quality, diverse prompts to improve generaliza-\ntion. [217] shows that merely scaling policy/reward\nmodels or increasing sample sizes results in diminish-\ning returns, while expanding the scope of PRM and\nR1 training data holds greater promise. Hybrid ap-\nproaches, such as combining RL with SFT or curriculum\nlearning, should be explored to enhance scalability.\n3) Controlling\nLong-CoT\nStability: Adaptive reward\nshaping mechanisms are needed to balance reasoning\nlength, coherence, and answer correctness. Techniques\nsuch as O1-Prune [221] demonstrate the value of post-\nhoc length regularization, but dynamic in-training con-\ntrols are necessary. Hierarchical RL frameworks should\nbe investigated to decompose long reasoning chains\ninto manageable sub-tasks, reducing instability.\n4) Theoretical and Empirical Analysis: It is essential to\nclarify the relationship between RL training and the\ncapabilities of the base model. For instance, it should\nbe determined whether emergent properties (e.g., Long-\nCoT) arise from RL optimization or are latent traits\nof the base model. Systematic studies on reward de-\nsign principles (e.g., sparse vs. dense rewards, multi-\nobjective balancing) should be conducted to avoid un-\nintended behaviors such as reward hacking.\nSummary: RFT presents a promising direction for advanc-\ning LLMs reasoning, as evidenced by DeepSeek-R1 [31].\nHowever, challenges such as reward saturation, unstable\nlong reasoning chains, and unclear emergent mechanisms\nrequire urgent attention. Future efforts should prioritize\nalgorithmic innovation, scalable prompt synthesis, and the-\noretical grounding to fully unlock the potential of RL-driven\nreasoning LLMs.\n3.3\nEvolutionary of Reasoning LLMs\nThe evolution of reasoning LLMs has progressed by several\ndistinct stages, with various strategies developed to over-\ncome the limitations of direct autoregressive inference and\nbuild more advanced slow-thinking reasoning architectures.\nIn the early stages, reasoning LLMs primarily focused on\nenhancing pre-trained LLMs with external reasoning algo-\nrithms, without altering the underlying model parameters.\nApproaches such as Tree of Thoughts [223] and Reasoning\nvia Planning [14] utilized LLMs-driven Breadth-First Search,\nDepth-First Search, and MCTS [79], [105], [108], [224] to\nsimulate human-like reasoning processes. These methods\nrepresented reasoning as tree or graph traversals, where\nintermediate reasoning states were depicted as nodes, and\nvarious reasoning strategies produced distinct reasoning\npaths. The final decision was made through additional vot-\ning mechanisms [3] or Monte Carlo-based value estimation\nto identify the optimal path.\nHowever, these externalized slow-reasoning approaches\nintroduced several challenges:\n1) Limited Exploration Space: The search-based methods\nrequired predefined constraints on the breadth, depth,\n\nJOURNAL OF LATEX CLASS FILES, JANUARY 2025\n12\nTABLE 6\nStatistics of benchmarks for reasoning LLMs.\nDomain\nBenchmark\nVenue\nLanguage\nSize\nLevel\nMath\nAIME 2024 [226]\n-\nEnglish\n30\nCompetition\nMATH-500 [37]\nICLR 2024\nEnglish\n500\nCompetition\nAMC 2023 [227]\n-\nEnglish\n30\nCompetition\nOlympiad Bench [228]\nACL 2024\nEnglish/Chinese\n8,476\nCompetition\nCode\nCodeforces\n-\nEnglish\n-\nExpert\nSWE-bench [229]\nICLR 2024\nEnglish\n2,294\nExpert\nLiveCodeBench [230]\nArXiv 2024\nEnglish\n-\nExpert\nScience\nGPQA Diamond [231]\nCOLM 2024\nEnglish\n448\nUniversity\nMMLU-Pro [232]\nNeurIPS 2024\nEnglish\n12,032\nHybrid\nAgent\nWebShop [233]\nNeurIPS 2022\nEnglish\n1,600\nHybrid\nWebArena [234]\nICLR 2024\nEnglish\n812\nHybrid\nSciWorld [235]\nEMNLP 2022\nEnglish\n7,200\nHybrid\nTextCraft [236]\nNAACL 2024\nEnglish\n200\nHybrid\nMedicine\nJAMA Clinical Challenge [237]\nNAACL 2025\nEnglish\n1,524\nExpert\nMedbullets [237]\nNAACL 2025\nEnglish\n308\nExpert\nMedQA [238]\nArXiv 2020\nEnglish/Chinese\n61,097\nExpert\nMultimodality\nMMMU [239]\nCVPR 2024\nEnglish\n11,500\nHybrid\nMathVista [240]\nICLR 2024\nEnglish\n6,141\nMiddle School\nMathVision [241]\nNeurIPS 2024\nEnglish\n3,040\nMiddle/High School\nCMMaTH [242]\nCOLING 2025\nEnglish/Chinese\n23,856\nMiddle/High School\nPGPS9K [243]\nIJCAI 2023\nEnglish\n9,023\nMiddle School\nand granularity of the search space, which often re-\nstricted the LLM’s exploration to a narrow reasoning\nspace. Furthermore, the reasoning strategies across dif-\nferent child nodes of the same parent node frequently\nlacked sufficient diversity, further limiting exploration.\n2) Limited Experience Sharing: Exploration experiences\nand\nreasoning\ninformation\nacross\ndifferent\npaths\ncould only be assessed based on reward models\nor\nself-consistency\namong\noutcomes.\nAdditionally,\nsearch-based methods significantly increased compu-\ntational overhead, relying on reward models such as\nPRM/ORM for tree pruning or speculative decoding\ntechniques to accelerate inference.\nTo overcome these limitations, subsequent models such as\nrSTaR [172], LLaMAV-o1 [177], HiICL-MCTS [169], Mul-\nberry [175], g1 [184], and Thinking-Claude [225] introduced\nricher action spaces. These enhanced action spaces offered\nhigh-level planning cues, broadening the model’s explo-\nration scope and enabling more comprehensive structured\nsearch processes. However, this approach necessitated care-\nful design of the action spaces to ensure their effectiveness.\nWith the introduction of models like o1 [29] and QwQ\n[98], external reasoning paradigms were internalized within\nthe LLM’s context. These models initially performed ex-\nploratory macro-planning to generate an initial reasoning\npath, followed by contextual exploration of alternative\npaths. Through mechanisms like “Rethink” and “Verifica-\ntion”, these models produced extended reasoning chains.\nTo replicate this internalized capability, STILL-1 [224] lin-\nearized tree search outputs into long reasoning chains with\nattributes such as “Rethink”, “Wait”, and “Explore New\nPath”. Similarly, STILL-2 [53] and sky-T1 [99] synthesized\nlong reasoning chains using distillation techniques. How-\never, the linearized reasoning chains derived from search-\nbased methods struggled to match the quality of those\nproduced by distillation approaches.\nRecent advancements, including DeepSeek-R1 [31] and\nKimi-k1.5 [192], have demonstrated the potential of RL to\nenhance models like DeepSeek-V3 [17], resulting in the\nemergence of complex behaviors such as long reasoning\nchains, reflective reasoning, and advanced planning ca-\npabilities. Remarkably, these sophisticated behaviors were\nachieved through simple RL scaling. SimpleRL [103] sought\nto replicate these capabilities using a streamlined pipeline\nand minimal codebase, while R1V [211] explored the devel-\nopment of multimodal reasoning models based on multi-\nmodal foundation architectures.\nSummary: The evolution of reasoning LLMs has shifted\nfrom externally augmented reasoning to internally embed-\nded reasoning. Recent developments emphasize the poten-\ntial of RL-based scaling to unlock advanced capabilities.\n4\nBENCHMARKING REASONING LLMS\nThe development of a robust benchmark is crucial for doc-\numenting the advancements in reasoning LLMs capabilities\nand for identifying promising research directions for future\nprogress. Here, we review the benchmarks from three key\naspects: categories, evaluation metrics, and performance\ncomparisons, while offering our reflections and insights.\n4.1\nBenchmark Categories\nWe categorize reasoning benchmarks by task type, which\ncan be broadly divided into math, code, scientific, agent,\nmedical, and multimodal reasoning. The detailed statistics\nfor these benchmarks are presented in Table 6.\n4.1.1\nBenchmark Introduction\n1) Math Problems: We document the current popular\ncompetition-level mathematical benchmarks to show-\ncase the capabilities of reasoning LLMs, including\nAIME 2024 [226], MATH-500 [37], AMC 2023 [227], and\nOlympiad Bench [228].\n\nJOURNAL OF LATEX CLASS FILES, JANUARY 2025\n13\nTask Types\nTechnical Proposals\nReasoning Paradigms\nMath\n• Pass@k\n• Cons@k\n• ……\nCode\n• Elo\n• Percentile\n• ……\nScience\n• Exact Match\n• Accuracy\n• ……\nORM,  PRM\nRM@k, Best-of-N, ……\nSelf-Consistency\nGreedy Decoding, Beam Search,  \nMajor@k, …….\nRL \nCumulative Reward,  \nSample Efficiency, ……\nSolution 1 \nSolution 2  \nSolution 3  \nConclusion \nSolution k  \nOutcome \nEfficiency\nProcess \nEfficiency\n……\nFig. 6. Various evaluation metrics of reasoning LLMs divided by task types, technical proposals, and reasoning paradigms.\n2) Code Problems: Code problems requires solid founda-\ntion and high logical thinking to evaluate the reasoning\nability of reasoning LLMs such as Codeforces, SWE-\nbench [229], and LiveCodeBench [230].\n3) Scientific Problems: Scientific benchmarks, i.e., GPQA\nDiamond [231] and MMLU-Pro [232], involve multi-\ndomains\nreasoning\nabout\nchemistry,\nbiology,\nand\nphysics, which requires extensive knowledge accumu-\nlation and integrated reasoning.\n4) Agent Reasoning: Realistic tasks often involve complex\nplanning and tool usage, leading to the creation of agent\nreasoning benchmarks [244]. For example, WebShop\n[233] and WebArena [234] focus on web operations,\nwhile SciWorld [235] and TextCraft [236] are centered\naround scientific research.\n5) Medical Reasoning: Medicine fundamentally involves\ncomplex reasoning, spanning tasks from diagnostic de-\ncision making to treatment planning. Benchmarks of\nJAMA Clinical Challenge [237], Medbullets [237], and\nMedQA [238] offer model measurements that mimic the\ndoctor’s disease diagnosis.\n6) Multimodal Reasoning: Multimodal reasoning, such\nas benchmarks of MMMU [239] and MathVista [240],\nrequires cross-modal thinking in combination with\ntext and images. Especially for those visual-centered\nproblems, in benchmarks MathVision [241], MathVerse\n[245], CMMaTH [242], and PGPS9K [243], put forward\nhigher requirements for reasoning LLMs.\n4.1.2\nSummary\nThe field of LLMs has advanced rapidly in recent years,\nwith benchmark performance consistently improving. Sim-\nple reasoning benchmarks, such as GSM8K [32], MATH-500\n[37], and ScienceQA [246], have approached performance\nsaturation. Recent studies on reasoning LLMs [54], [147]\nshow that models designed for long reasoning chains do not\nsignificantly outperform those designed for shorter chains\non these benchmarks. This highlights the urgent need to\nestablish new benchmarks that more effectively assess the\nreasoning capabilities of reasoning LLMs. Moreover, current\nbenchmarks are limited, focusing mainly on solid reasoning\ntasks. Soft reasoning benchmarks, lacking explicitly defined\ncorrect answers, offer a more nuanced evaluation, better\ncapturing the complexities and subtleties of human-like\nreasoning. Furthermore, it is essential to address the issue\nof data leakage in evaluation processes [247]. Ensuring the\nconfidentiality and neutrality of evaluation data is critical to\npreserving the integrity and reliability of benchmark results.\n4.2\nEvaluation Metrics\nDepending on task types, technical proposals, and rea-\nsoning paradigms, various evaluation metrics have been\nintroduced for reasoning LLMs as shown in Figure 6. These\nmetrics are designed to more accurately assess the model’s\nperformance in handling complex reasoning tasks, ensuring\nthat both the quality and coherence of the generated solu-\ntions are effectively measured.\n4.2.1\nTask Types\nIn terms of benchmark categories, mathematical reasoning\ntypically uses two main metrics: Pass@k and Cons@k. The\nPass@k metric evaluates the model’s ability to generate a\ncorrect solution within k attempts, measuring the likelihood\nof success within a limited number of tries. On the other\nhand, Cons@k assesses whether the model consistently pro-\nduces correct or logically coherent solutions, highlighting\nthe stability and reliability of its reasoning capabilities. For\ncode tasks, the key metrics are Elo and Percentile, both\nof which measure the relative skill in generating correct\ncode compared to other models or human programmers.\nIn scientific tasks, evaluation generally employs Exact Match\n(EM) and Accuracy for fill-in-the-blank and multiple-choice\nquestions, respectively. The EM metric judges whether the\nmodel’s output exactly matches the expected solution, while\nAccuracy measures the proportion of correct answers out of\nthe total number of questions.\n4.2.2\nTechnical Proposals\nBased on technical routes, the schemes with ORM or PRM\noften leverage RM@k and Best-of-N two evaluation indica-\ntors. RM@k measures whether the reward model can rank\nthe good answer higher in the top k candidates according\nto reward score, and Best-of-N chooses the solution with\nhighest score from N generated reasoning trajectories. Meth-\nods for self-consistency are evaluated using Greedy Decoding,\n\nJOURNAL OF LATEX CLASS FILES, JANUARY 2025\n14\nTABLE 7\nPerformance of Different Models, including Basic LLMs and Reasoning LLMs, on Plain Text Benchmarks. The red denotes the highest result, and\nthe blue denotes the second highest result.\nModel\nMath\nCode\nGeneral\nAIME 2024\nMATH-500\nLiveCodeBench\nCodeforces\nSWE Verified\nMMLU\nGPQA-Diamond\n(Pass@1)\n(Pass@1)\n(Pass@1-CoT)\n(Percentile)\n(Resolved)\n(Pass@1)\n(Pass@1)\nBasic LLMs\nGPT-4o [16]\n9.3\n74.6\n34.2\n23.6\n38.8\n87.2\n49.9\nClaude-3.5-Sonnet [248]\n16.0\n78.3\n33.8\n20.3\n50.8\n88.3\n65.0\nGemini-2.0-Pro [249]\n-\n91.8\n36.0\n-\n-\n86.5\n64.7\nDeepseek-V3 [17]\n39.2\n90.2\n36.2\n58.7\n42.0\n88.5\n59.1\nReasoning LLMs\nEurus-2-7B-PRIME [195]\n26.7\n79.2\n-\n-\n-\n-\n-\nInternLM3-8B-Instruct [250]\n20.0\n83.0\n-\n-\n-\n76.6\n37.4\nrStar-Math-7B [147]\n46.7\n81.6\n-\n-\n-\n82.7\n54.9\nSTILL-2-32B [53]\n46.7\n90.2\n-\n-\n-\n-\n-\nRedstar-code-math [54]\n53.3\n91.2\n-\n-\n-\n-\n-\nSearch-o1 [97]\n56.7\n86.4\n33.0\n-\n-\n-\n63.6\nQwQ [98]\n50.0\n90.6\n41.9\n62.0\n-\n-\n54.5\ns1-32B [251]\n56.7\n93.0\n-\n-\n-\n-\n59.6\nOpenAI o1-mini [252]\n63.6\n90.0\n53.8\n93.4\n41.6\n85.2\n60.0\nLIMO-32B [253]\n57.1\n94.8\n-\n-\n-\n-\n66.7\nKimi k1.5 long-CoT [192]\n77.5\n96.2\n62.5\n94.0\n-\n-\n-\nDeepSeek-R1 [31]\n79.8\n97.3\n65.9\n96.3\n49.2\n90.8\n71.5\nOpenAI-o1 [29]\n79.2\n96.4\n63.4\n96.6\n48.9\n91.8\n75.7\nOpenAI o3-mini [30]\n87.3\n97.9\n84.6\n-\n49.3\n86.9\n79.7\nTABLE 8\nPerformance of Models, including Basic LLMs and Reasoning LLMs, on\nMultimodal Benchmarks. The red denotes the highest result, and the\nblue denotes the second highest result.\nModel\nMMMU\nMathvista\nMathvision\nOlympiadbench\nBasic LLMs\nGPT-4o [16]\n69.1\n63.8\n30.4\n25.9\nClaude-3.5-Sonnet [248]\n70.4\n65.3\n35.6\n-\nGemini 2.0 Pro [249]\n72.7\n-\n-\n-\nReasoning LLMs\nLLaVA-CoT [176]\n-\n54.8\n-\n-\nQvQ-72B-preview [254]\n70.3\n71.4\n35.9\n20.4\nKimi k1.5 long-CoT [192]\n70.0\n74.9\n-\n-\nOpenAI-o1 [29]\n77.3\n71.0\n-\n-\nBeam Search, and Major@k. Greedy Decoding and Beam Search\ncontrol the randomness of the inference process by limiting\nthe sampling range. Major@k selects the solution with the\nmost consistent results from k candidate solutions. In RL,\nmetrics reflect both performance in achieving desired out-\ncomes and the efficiency of the learning process. For exam-\nple, Cumulative Reward measures the total reward received\nby the agent over time, while Sample Efficiency assesses the\nefficiency of the agent’s sample usage during learning.\n4.2.3\nReasoning Paradigms\nFor reasoning paradigm of the multi-turn solution gener-\nation in reasoning LLMs, Outcome Efficiency and Process\nEfficiency [102] are proposed recently to evaluate the effi-\nciency of long thinking specifically. Outcome Efficiency metric\nempirically evaluates how effectively later solutions con-\ntribute to accuracy improvements, formulating as the ratio\nof efficient tokens that contribute to reaching the correct\nanswer, to all output tokens. Process Efficiency metric eval-\nuates the contribution of later solutions to solution diversity\nempirically, concretely representing as the ratio of tokens of\ndistinct solutions to all solution tokens. These two indicators\nreveal to the overthinking issue of existing reasoning LLMs\nto simple problems certainly.\n4.2.4\nSummary\nMost of the existing evaluation metrics are judged according\nto the final answer. It is imperative to develop a com-\nprehensive assessment framework that considers various\naspects of the reasoning process in view of the large in-\nference computation consumption. Current popular eval-\nuation frameworks, such as LMMs-Eval [255], OpenCom-\npass [256], and PRMBench [257], lack efficiency and their\nmetrics do not adequately account for the computational\nand temporal efficiency of the reasoning process. To address\nthese shortcomings, we highly recommend exploring more\nefficient proxy tasks as potential solutions. By identifying\nand utilizing tasks that better capture the nuances of long\nreasoning chains, we can develop more robust and effective\nevaluation metrics to enhance the overall assessment frame-\nwork, ensuring that it not only measures the accuracy of the\nfinal output but also evaluates the efficiency and coherence\nof the reasoning process throughout.\n4.3\nPerformance Comparison\nIn this section, we compare the performance of different rea-\nsoning LLMs and their corresponding foundational LLMs\non plain text benchmarks, such as math and code problems,\nas well as on multimodal benchmarks. The comprehensive\nreal-time leaderboard is available on this website.\n4.3.1\nPerformance on Plain Text Benchmarks\nAs shown in Table 7, reasoning LLMs, such as DeepSeek-R1\n[31] and OpenAI-o1/o3 [29], [30], demonstrate exceptional\nperformance across a wide range of tasks, including math,\ncoding, and other general tasks. These models achieve high\nscores on multiple plain-text benchmarks, such as AIME\n2024, MATH-500, and LiveCodeBench, showcasing their\nrobust text-based reasoning abilities. In contrast, founda-\ntional LLMs, like GPT-4o [62], Claude-3.5-Sonnet [248], and\nDeepSeek-V3 [17], generally perform less effectively than\nreasoning LLMs, particularly in math and coding tasks (e.g.,\nAIME 2024 and Codeforces). For example, OpenAI-o1 out-\nperforms GPT-4o by 69.9% and 73% on these tasks, respec-\ntively. Moreover, DeepSeek-R1, based on the DeepSeek-V3\narchitecture, surpasses its predecessor on all benchmarks,\nfurther highlighting the advantages of the reasoning LLMs.\n\nJOURNAL OF LATEX CLASS FILES, JANUARY 2025\n15\n4.3.2\nPerformance on Multimodal Benchmarks\nAs shown in Table 8, reasoning LLMs continue to excel\nin multimodal tasks. OpenAI-o1 [29] performs strongly in\nvision tasks, achieving the highest score of 77.3% on MMMU\nand outperforming its corresponding foundational LLM,\nGPT-4o [62], by 7.2% on MathVista. However, the perfor-\nmance improvement in multimodal tasks is less pronounced\ncompared to text-only tasks. This can be attributed in part\nto the limitations of current multimodal reasoning LLM\ntechniques, as well as the lack of sufficient datasets to fully\nassess the multimodal capabilities of reasoning LLMs.\n4.3.3\nSummary\nIn summary, reasoning LLMs show strong performance\nacross both plain text and multimodal benchmarks, partic-\nularly excelling in math and coding tasks, where they out-\nperform foundational LLMs by a large margin. Although the\nimprovement in multimodal tasks is not as pronounced as in\ntext-only tasks, reasoning LLMs still surpass their counter-\nparts, highlighting their potential for processing both image\nand text data. These results emphasize the versatility and\neffectiveness of reasoning LLMs across a broad spectrum of\nreasoning tasks, with potential for further advancements in\nmultimodal reasoning techniques.\n5\nCHALLENGES & FUTURE DIRECTIONS\nDespite the rapid advancements in reasoning LLMs, several\nchallenges persist, limiting their generalizability and practi-\ncal applicability. This section outlines these challenges and\nhighlights potential research directions to address them.\n5.1\nEfficient Reasoning LLMs\nWhile reasoning LLMs excel at solving complex problems\nvia extended inference, their reliance on long autoregressive\nreasoning within large-scale architectures presents signif-\nicant efficiency challenges. For example, many problems\non platforms like Codeforces require over 10,000 tokens of\nreasoning, resulting in high latency. As noted in [102], even\nwhen a reasoning LLM identifies the correct solution early,\nit often spends considerable time verifying its reasoning.\nRecent reports, such as Deepseek-R1 [31], suggest that self-\nimprovement via RL is more effective in larger models,\nwhile smaller-scale large language models (SLMs) (e.g., 3B\nand 7B models as explored by [103] and [199], [216]) struggle\nto match performance in slow-thinking reasoning tasks.\nFuture research should focus on two key areas: (1) in-\ntegrating external reasoning tools to enable early stopping\nand verification mechanisms, thus improving the efficiency\nof long inference chains, and (2) exploring strategies to\nimplement slow-thinking reasoning capabilities in SLMs\nwithout sacrificing performance.\n5.2\nCollaborative Slow & Fast-thinking Systems\nA key challenge in reasoning LLMs is the loss of fast-\nthinking capabilities, which results in inefficiencies when\nsimple tasks require unnecessary deep reasoning. Unlike\nhumans, who fluidly switch between fast (System 1) and\nslow (System 2) thinking, current reasoning LLMs struggle\nto maintain this balance. While reasoning LLMs ensure\ndeliberate and thorough reasoning, fast-thinking systems\nrely on prior knowledge for quick responses. Despite efforts\nsuch as the System 1-2 switcher [95], speculative decoding\n[258]–[260], and interactive continual learning [261], inte-\ngrating both modes of thinking remains challenging. This\noften leads to inefficiencies in domain-specific tasks and\nunderutilized strengths in more complex scenarios.\nFuture research should focus on developing adaptive\nswitching mechanisms, joint training frameworks, and co-\nevolution strategies to harmonize the efficiency of fast-\nthinking systems with the precision of reasoning LLMs.\nAchieving this balance is crucial for advancing the field and\ncreating more versatile AI systems.\n5.3\nReasoning LLMs For Science\nReasoning LLMs play a crucial role in scientific research\n[262], enabling deep, structured analysis that goes beyond\nthe heuristic-based fast-thinking models. Their value be-\ncomes especially clear in fields that demand complex rea-\nsoning, such as medicine and mathematics. In medicine,\nparticularly in differential diagnosis and treatment plan-\nning, reasoning LLMs (e.g., inference-time scaling) enhance\nAI’s step-by-step reasoning, improving diagnostic accuracy\nwhere traditional scaling methods fall short [52]. In mathe-\nmatics, approaches like FunSearch [263] incorporate slow-\nthinking principles to push beyond previous discoveries,\nshowcasing the potential of AI-human collaboration.\nBeyond these fields, reasoning LLMs can foster advance-\nments in physics, engineering, and computational biology\nby refining model formulation and hypothesis testing. In-\nvesting in reasoning LLMs research not only bridges the\ngap between AI’s computational power and human-like\nanalytical depth but also paves the way for more reliable,\ninterpretable, and groundbreaking scientific discoveries.\n5.4\nDeep Integration of Neural and Symbolic Systems\nDespite significant advancements in reasoning LLMs, their\nlimited transparency and interpretability restrict their per-\nformance in more complex real-world reasoning tasks. The\nreliance on large-scale data patterns and lack of clear rea-\nsoning pathways makes it challenging to handle intricate\nor ambiguous problems effectively. Early symbolic logic\nsystems, while less adaptable, offered better explainability\nand clearer reasoning steps, leading to more reliable perfor-\nmance in such cases.\nA promising future direction is the deep integration\nof neural and symbolic systems. Google’s AlphaGeome-\ntry [264] and AlphaGeometry2 [265] combine reasoning\nLLMs with symbolic engines, achieving breakthroughs in\nthe International Olympiad in Mathematics (IMO). In par-\nticular, AlphaGeometry2 utilizes the Gemini-based model\n[249], [266], [267] and a more efficient symbolic engine, im-\nproving performance by reducing rule sets and enhancing\nkey concept handling. The system now covers a broader\nrange of geometric concepts, including locus theorems and\nlinear equations. A new search algorithm and knowledge-\nsharing mechanism accelerate the process. This system\nsolved 84% of IMO geometry problems (2000-2024), sur-\npassing gold medalists’ averages. In contrast, reasoning\nLLMs like OpenAI-o1 [29] failed to solve any problems. The\n\nJOURNAL OF LATEX CLASS FILES, JANUARY 2025\n16\nintegration of neural and symbolic systems offers a balanced\napproach, improving both adaptability and interpretability,\nwith vast potential for complex real-world reasoning tasks\nbeyond mathematical geometry problems.\n5.5\nMultilingual Reasoning LLMs\nCurrent reasoning LLMs perform well in high-resource lan-\nguages like English and Chinese, demonstrating strong ca-\npabilities in tasks such as translation and various reasoning\ntasks [93], [101]. These models excel in environments where\nlarge-scale data and diverse linguistic resources are avail-\nable. However, their performance in low-resource languages\nremains limited [268], facing challenges related to data spar-\nsity, stability, safety, and overall performance. These issues\nhinder the effectiveness of reasoning LLMs in languages\nthat lack substantial linguistic datasets and resources.\nFuture research should prioritize overcoming the chal-\nlenges posed by data scarcity and cultural biases in low-\nresource languages. Innovations such as parameter shar-\ning across reasoning LLMs and the incremental injection\nof domain-specific knowledge could help mitigate these\nchallenges, enabling faster adaptation of slow-thinking ca-\npabilities to a broader range of languages. This would\nnot only enhance the effectiveness of reasoning LLMs in\nthese languages but also ensure more equitable access to\nadvanced AI technologies.\n5.6\nMultimodal Reasoning LLMs\nExtending slow-thinking reasoning capabilities from text-\nbased domains to multimodal contexts remains a significant\nchallenge, especially in tasks requiring fine-grained percep-\ntion [96]. While approaches like Virgo [269] have attempted\nto distill text-based slow-thinking reasoning into multi-\nmodal LLMs, their performance improvements in tasks\nsuch as MathVision [241], which demand detailed visual\nunderstanding, have been marginal.\nKey research directions include developing hierarchical\nreasoning LLMs that enable fine-grained cross-modal un-\nderstanding and generation, tailored to the unique charac-\nteristics of modalities such as audio, video, and 3D data.\n5.7\nSafe Reasoning LLMs\nThe rapid development of reasoning LLMs like OpenAI-o1\n[29] and DeepSeek-R1 [31] has led to the rise of superintelli-\ngent models capable of continuous self-evolution. However,\nthis progress brings challenges in safety and control. RL, a\nkey training method, introduces risks such as reward hack-\ning, generalization failures, and language mixing, which\ncan lead to harmful outcomes. Ensuring the safety of such\nsystems like DeepSeek-R1 is urgent. While RL enhances\nreasoning, its uncontrollable nature raises concerns about\nsafely guiding these models. SFT addresses some issues but\nis not a complete solution. A hybrid approach combining\nRL and SFT is needed to reduce harmful outputs while\nmaintaining model effectiveness [270].\nAs these models surpass human cognitive capabilities,\nensuring their safe, responsible, and transparent use is cru-\ncial. This requires ongoing research to develop methods for\ncontrolling and guiding their actions, thereby balancing AI\npower with ethical decision-making.\n6\nCONCLUSION\nThis paper presents a comprehensive survey that advances\nresearch on reasoning LLMs. We begin with an overview of\nthe progress in foundational LLMs and key early System 2\ntechnologies, including symbolic logic, MCTS, and RL, ex-\nploring how each, when combined with foundational LLMs,\nhas paved the way for reasoning LLMs. We then provide\na detailed feature analysis of the latest reasoning LLMs,\nexamining the core methods that enable their advanced rea-\nsoning capabilities and highlighting representative models.\nThrough a review of mainstream reasoning benchmarks and\nperformance comparisons, we offer valuable insights into\nthe current state of the field. Looking ahead, we identify\npromising research directions and continue to track devel-\nopments via our real-time GitHub Repository. This survey\naims to inspire innovation and foster progress in the rapidly\nevolving field of reasoning LLMs.\nREFERENCES\n[1]\nW. Hua and Y. Zhang, “System 1+ system 2= better world:\nNeural-symbolic chain of logic reasoning,” in Findings of the\nAssociation for Computational Linguistics: EMNLP 2022, 2022, pp.\n601–612.\n[2]\nJ. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le,\nD. Zhou et al., “Chain-of-thought prompting elicits reasoning in\nlarge language models,” Advances in neural information processing\nsystems, vol. 35, pp. 24 824–24 837, 2022.\n[3]\nX. Wang, J. Wei, D. Schuurmans, Q. V. Le, E. H. Chi, S. Narang,\nA. Chowdhery, and D. Zhou, “Self-Consistency Improves Chain\nof Thought Reasoning in Language Models,” in The Eleventh\nInternational Conference on Learning Representations, 2023.\n[4]\nD. Zhou, N. Sch¨arli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuur-\nmans, C. Cui, O. Bousquet, Q. V. Le et al., “Least-to-Most Prompt-\ning Enables Complex Reasoning in Large Language Models,” in\nThe Eleventh International Conference on Learning Representations,\n2023.\n[5]\nE. Zelikman, Y. Wu, J. Mu, and N. D. Goodman, “STaR: Self-\ntaught reasoner bootstrapping reasoning with reasoning,” in\nProc. the 36th International Conference on Neural Information Pro-\ncessing Systems, vol. 1126, 2024.\n[6]\nJ. S. B. Evans, “Heuristic and analytic processes in reasoning,”\nBritish Journal of Psychology, vol. 75, no. 4, pp. 451–468, 1984.\n[7]\nD. Kahneman, “Maps of bounded rationality: Psychology for\nbehavioral economics,” American economic review, vol. 93, no. 5,\npp. 1449–1475, 2003.\n[8]\nJ. Huang and K. C.-C. Chang, “Towards Reasoning in Large\nLanguage Models: A Survey,” in Findings of the Association for\nComputational Linguistics: ACL 2023, 2023, pp. 1049–1065.\n[9]\nS. Qiao, Y. Ou, N. Zhang, X. Chen, Y. Yao, S. Deng, C. Tan,\nF. Huang, and H. Chen, “Reasoning with Language Model\nPrompting: A Survey,” in Proceedings of the 61st Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long\nPapers), 2023, pp. 5368–5393.\n[10]\nB. Wang, S. Min, X. Deng, J. Shen, Y. Wu, L. Zettlemoyer, and\nH. Sun, “Towards Understanding Chain-of-Thought Prompting:\nAn Empirical Study of What Matters,” in Proceedings of the 61st\nAnnual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), 2023, pp. 2717–2739.\n[11]\nO. Shaikh, H. Zhang, W. Held, M. Bernstein, and D. Yang, “On\nSecond Thought, Let’s Not Think Step by Step! Bias and Toxicity\nin Zero-Shot Reasoning,” in Proceedings of the 61st Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long\nPapers), 2023, pp. 4454–4470.\n[12]\nH. Shao, S. Qian, H. Xiao, G. Song, Z. Zong, L. Wang, Y. Liu, and\nH. Li, “Visual cot: Advancing multi-modal language models with\na comprehensive dataset and benchmark for chain-of-thought\nreasoning,” in The Thirty-eight Conference on Neural Information\nProcessing Systems Datasets and Benchmarks Track, 2024.\n[13]\nZ. Zhang, A. Zhang, M. Li, and A. Smola, “Automatic Chain of\nThought Prompting in Large Language Models,” in The Eleventh\nInternational Conference on Learning Representations, 2023.\n\nJOURNAL OF LATEX CLASS FILES, JANUARY 2025\n17\n[14]\nS. Hao, Y. Gu, H. Ma, J. Hong, Z. Wang, D. Wang, and\nZ. Hu, “Reasoning with Language Model is Planning with World\nModel,” in Proceedings of the 2023 Conference on Empirical Methods\nin Natural Language Processing, 2023, pp. 8154–8173.\n[15]\nY. Zhang, “Meta prompting for agi systems,” arXiv preprint\narXiv:2311.11482, 2023.\n[16]\nOpenAI,\n“Hello\nGPT-4o,”\nMay\n2024.\n[Online].\nAvailable:\nhttps://openai.com/index/hello-gpt-4o/\n[17]\nA. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng,\nC. Zhang, C. Ruan et al., “Deepseek-v3 technical report,” arXiv\npreprint arXiv:2412.19437, 2024.\n[18]\nA. Vaswani, “Attention is all you need,” Advances in Neural\nInformation Processing Systems, 2017.\n[19]\nJ. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: Pre-\ntraining of Deep Bidirectional Transformers for Language Under-\nstanding,” in Proceedings of the 2019 Conference of the North Ameri-\ncan Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers), 2019, pp. 4171–\n4186.\n[20]\nY. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy,\nM. Lewis, L. Zettlemoyer, and V. Stoyanov, “RoBERTa: A\nRobustly Optimized BERT Pretraining Approach,” CoRR, vol.\nabs/1907.11692, 2019.\n[21]\nA. Radford, “Improving language understanding by generative\npre-training,” 2018.\n[22]\nA. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever\net al., “Language models are unsupervised multitask learners,”\nOpenAI blog, vol. 1, no. 8, p. 9, 2019.\n[23]\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhari-\nwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al.,\n“Language models are few-shot learners,” Advances in neural\ninformation processing systems, vol. 33, pp. 1877–1901, 2020.\n[24]\nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright,\nP. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray et al., “Train-\ning language models to follow instructions with human feed-\nback,” Advances in neural information processing systems, vol. 35,\npp. 27 730–27 744, 2022.\n[25]\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\nT. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar et al.,\n“Llama: Open and efficient foundation language models,” arXiv\npreprint arXiv:2302.13971, 2023.\n[26]\nW. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min,\nB. Zhang, J. Zhang, Z. Dong et al., “A survey of large language\nmodels,” arXiv preprint arXiv:2303.18223, 2023.\n[27]\nH. Liu, C. Li, Q. Wu, and Y. J. Lee, “Visual Instruction Tuning,” in\nThirty-seventh Conference on Neural Information Processing Systems,\n2023.\n[28]\nD. Zhang, Y. Yu, J. Dong, C. Li, D. Su, C. Chu, and D. Yu,\n“MM-LLMs: Recent Advances in MultiModal Large Language\nModels,” in Findings of the Association for Computational Linguis-\ntics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-\n16, 2024.\nAssociation for Computational Linguistics, 2024, pp.\n12 401–12 430.\n[29]\nOpenAI,\n“Learning\nto\nreason\nwith\nLLMs,”\nSeptem-\nber\n2024.\n[Online].\nAvailable:\nhttps://openai.com/index/\nlearning-to-reason-with-llms/\n[30]\n——, “OpenAI o3-mini,” January 2025. [Online]. Available:\nhttps://openai.com/index/openai-o3-mini/\n[31]\nD. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu,\nS. Ma, P. Wang, X. Bi et al., “DeepSeek-R1: Incentivizing Rea-\nsoning Capability in LLMs via Reinforcement Learning,” arXiv\npreprint arXiv:2501.12948, 2025.\n[32]\nK. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,\nM. Plappert, J. Tworek, J. Hilton, R. Nakano et al., “Train-\ning verifiers to solve math word problems,” arXiv preprint\narXiv:2110.14168, 2021.\n[33]\nT. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, “Large\nlanguage models are zero-shot reasoners,” Advances in neural\ninformation processing systems, vol. 35, pp. 22 199–22 213, 2022.\n[34]\nY. Liu, A. Singh, C. D. Freeman, J. D. Co-Reyes, and P. J. Liu,\n“Improving large language model fine-tuning for solving math\nproblems,” arXiv preprint arXiv:2310.10047, 2023.\n[35]\nX. Zhu, J. Wang, L. Zhang, Y. Zhang, Y. Huang, R. Gan, J. Zhang,\nand Y. Yang, “Solving Math Word Problems via Cooperative\nReasoning induced Language Models,” in Proceedings of the 61st\nAnnual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), 2023, pp. 4471–4485.\n[36]\nP. Lu, L. Qiu, K.-W. Chang, Y. N. Wu, S.-C. Zhu, T. Rajpurohit,\nP. Clark, and A. Kalyan, “Dynamic Prompt Learning via Policy\nGradient for Semi-structured Mathematical Reasoning,” in The\nEleventh International Conference on Learning Representations, 2023.\n[37]\nH. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee,\nJ. Leike, J. Schulman, I. Sutskever, and K. Cobbe, “Let’s Verify\nStep by Step,” in The Twelfth International Conference on Learning\nRepresentations, 2024.\n[38]\nF. Yao, C. Tian, J. Liu, Z. Zhang, Q. Liu, L. Jin, S. Li, X. Li,\nand X. Sun, “Thinking like an expert: Multimodal hypergraph-\nof-thought (hot) reasoning to boost foundation modals,” arXiv\npreprint arXiv:2308.06207, 2023.\n[39]\nY. Yao, Z. Li, and H. Zhao, “Beyond Chain-of-Thought, Effec-\ntive Graph-of-Thought Reasoning in Language Models,” arXiv\npreprint arXiv:2305.16582, 2023.\n[40]\nY. Wen, Z. Wang, and J. Sun, “Mindmap: Knowledge graph\nprompting sparks graph of thoughts in large language models,”\narXiv preprint arXiv:2308.09729, 2023.\n[41]\nB. Lei, C. Liao, C. Ding et al., “Boosting logical reasoning in\nlarge language models through a new framework: The graph of\nthought,” arXiv preprint arXiv:2308.08614, 2023.\n[42]\nM. Jin, Q. Yu, D. Shu, H. Zhao, W. Hua, Y. Meng, Y. Zhang, and\nM. Du, “The impact of reasoning step length on large language\nmodels,” arXiv preprint arXiv:2401.04925, 2024.\n[43]\nM. Besta, N. Blach, A. Kubicek, R. Gerstenberger, M. Podstawski,\nL. Gianinazzi, J. Gajda, T. Lehmann, H. Niewiadomski, P. Nyczyk\net al., “Graph of thoughts: Solving elaborate problems with\nlarge language models,” in Proceedings of the AAAI Conference on\nArtificial Intelligence, vol. 38, no. 16, 2024, pp. 17 682–17 690.\n[44]\nP. Cheng, T. Hu, H. Xu, Z. Zhang, Y. Dai, L. Han, and N. Du, “Self-\nplaying Adversarial Language Game Enhances LLM Reasoning,”\narXiv preprint arXiv:2404.10642, 2024.\n[45]\nH. You, R. Sun, Z. Wang, L. Chen, G. Wang, H. Ayyubi, K.-\nW. Chang, and S.-F. Chang, “IdealGPT: Iteratively Decomposing\nVision and Language Reasoning via Large Language Models,”\nin Findings of the Association for Computational Linguistics: EMNLP\n2023, 2023, pp. 11 289–11 303.\n[46]\nP. Wu and S. Xie, “V?: Guided Visual Search as a Core Mechanism\nin Multimodal LLMs,” in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 2024, pp. 13 084–\n13 094.\n[47]\nZ. Chen, R. Sun, W. Liu, Y. Hong, and C. Gan, “GENOME: Gener-\native Neuro-Symbolic Visual Reasoning by Growing and Reusing\nModules,” in International Conference on Learning Representations,\n2024.\n[48]\nS. Wu, Z. Peng, X. Du, T. Zheng, M. Liu, J. Wu, J. Ma, Y. Li, J. Yang,\nW. Zhou et al., “A Comparative Study on Reasoning Patterns of\nOpenAI’s o1 Model,” arXiv preprint arXiv:2410.13639, 2024.\n[49]\nV. Xiang, C. Snell, K. Gandhi, A. Albalak, A. Singh, C. Blagden,\nD. Phung, R. Rafailov, N. Lile, D. Mahan et al., “Towards System\n2 Reasoning in LLMs: Learning How to Think With Meta Chain-\nof-Though,” arXiv preprint arXiv:2501.04682, 2025.\n[50]\nY. Qin, X. Li, H. Zou, Y. Liu, S. Xia, Z. Huang, Y. Ye, W. Yuan,\nH. Liu, Y. Li et al., “O1 Replication Journey: A Strategic Progress\nReport–Part 1,” arXiv preprint arXiv:2410.18982, 2024.\n[51]\nZ. Huang, H. Zou, X. Li, Y. Liu, Y. Zheng, E. Chern, S. Xia, Y. Qin,\nW. Yuan, and P. Liu, “O1 Replication Journey–Part 2: Surpassing\nO1-preview through Simple Distillation, Big Progress or Bitter\nLesson?” arXiv preprint arXiv:2411.16489, 2024.\n[52]\nZ. Huang, G. Geng, S. Hua, Z. Huang, H. Zou, S. Zhang, P. Liu,\nand X. Zhang, “O1 Replication Journey–Part 3: Inference-time\nScaling for Medical Reasoning,” arXiv preprint arXiv:2501.06458,\n2025.\n[53]\nY. Min, Z. Chen, J. Jiang, J. Chen, J. Deng, Y. Hu, Y. Tang,\nJ. Wang, X. Cheng, H. Song, W. X. Zhao, Z. Liu, Z. Wang, and\nJ.-R. Wen, “Imitate, Explore, and Self-Improve: A Reproduction\nReport on Slow-thinking Reasoning Systems,” arXiv preprint\narXiv:2412.09413, 2024.\n[54]\nH. Xu, X. Wu, W. Wang, Z. Li, D. Zheng, B. Chen, Y. Hu,\nS. Kang, J. Ji, Y. Zhang et al., “RedStar: Does Scaling Long-\nCoT Data Unlock Better Slow-Reasoning Systems?” arXiv preprint\narXiv:2501.11284, 2025.\n[55]\nZ. Zeng, Q. Cheng, Z. Yin, B. Wang, S. Li, Y. Zhou, Q. Guo,\nX. Huang, and X. Qiu, “Scaling of Search and Learning: A\n\nJOURNAL OF LATEX CLASS FILES, JANUARY 2025\n18\nRoadmap to Reproduce o1 from Reinforcement Learning Per-\nspective,” arXiv preprint arXiv:2412.14135, 2024.\n[56]\nY. Ji, J. Li, H. Ye, K. Wu, J. Xu, L. Mo, and M. Zhang, “Test-\ntime Computing: from System-1 Thinking to System-2 Thinking,”\narXiv preprint arXiv:2501.02497, 2025.\n[57]\nM. Besta, J. Barth, E. Schreiber, A. Kubicek, A. Catarino, R. Ger-\nstenberger, P. Nyczyk, P. Iff, Y. Li, S. Houliston et al., “Reasoning\nLanguage Models: A Blueprint,” arXiv preprint arXiv:2501.11223,\n2025.\n[58]\nY. Zhang, S. Mao, T. Ge, X. Wang, A. de Wynter, Y. Xia, W. Wu,\nT. Song, M. Lan, and F. Wei, “LLM as a Mastermind: A Survey of\nStrategic Reasoning with Large Language Models,” arXiv preprint\narXiv:2404.01230, 2024.\n[59]\nF. Xu, Q. Hao, Z. Zong, J. Wang, Y. Zhang, J. Wang, X. Lan,\nJ. Gong, T. Ouyang, F. Meng et al., “Towards Large Reasoning\nModels: A Survey of Reinforced Reasoning with Large Language\nModels,” arXiv preprint arXiv:2501.09686, 2025.\n[60]\nA. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agar-\nwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., “Learning\ntransferable visual models from natural language supervision,”\nin International conference on machine learning.\nPMLR, 2021, pp.\n8748–8763.\n[61]\nA. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford,\nM. Chen, and I. Sutskever, “Zero-shot text-to-image generation,”\nin International conference on machine learning.\nPmlr, 2021, pp.\n8821–8831.\n[62]\nOpenAI, “GPT-4 Technical Report,” 2023.\n[63]\nG. E. Hinton, S. Osindero, and Y.-W. Teh, “A fast learning\nalgorithm for deep belief nets,” Neural computation, vol. 18, no. 7,\npp. 1527–1554, 2006.\n[64]\nG. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly,\nA. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath et al., “Deep\nneural networks for acoustic modeling in speech recognition:\nThe shared views of four research groups,” IEEE Signal processing\nmagazine, vol. 29, no. 6, pp. 82–97, 2012.\n[65]\nA. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classi-\nfication with deep convolutional neural networks,” Advances in\nneural information processing systems, vol. 25, 2012.\n[66]\nY. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” nature, vol.\n521, no. 7553, pp. 436–444, 2015.\n[67]\nQ. Dong, L. Li, D. Dai, C. Zheng, J. Ma, R. Li, H. Xia, J. Xu, Z. Wu,\nB. Chang et al., “A survey on in-context learning,” in Proceedings\nof the 2024 Conference on Empirical Methods in Natural Language\nProcessing, 2024, pp. 1107–1128.\n[68]\nJ. White, Q. Fu, S. Hays, M. Sandborn, C. Olea, H. Gilbert,\nA. Elnashar, J. Spencer-Smith, and D. C. Schmidt, “A prompt\npattern catalog to enhance prompt engineering with chatgpt,”\narXiv preprint arXiv:2302.11382, 2023.\n[69]\nC. I. Lewis, C. H. Langford, and P. Lamprecht, Symbolic logic.\nDover publications New York, 1959, vol. 170.\n[70]\nR. Carnap, Introduction to symbolic logic and its applications.\nCourier Corporation, 2012.\n[71]\nA. Colmerauer, “An introduction to Prolog III,” Communications\nof the ACM, vol. 33, no. 7, pp. 69–90, 1990.\n[72]\nW. F. Clocksin and C. S. Mellish, Programming in PROLOG.\nSpringer Science & Business Media, 2003.\n[73]\nK. R. Apt et al., From logic programming to Prolog.\nPrentice Hall\nLondon, 1997, vol. 362.\n[74]\nM. P. Singh, A. S. Rao, and M. P. Georgeff, Formal methods in DAI:\nLogic-based representation and reasoning.\nMIT Press Cambridge,\n1999, vol. 8.\n[75]\nR. G. Jeroslow, “Computation-oriented reductions of predicate to\npropositional logic,” Decision Support Systems, vol. 4, no. 2, pp.\n183–197, 1988.\n[76]\nJ. McCarthy, “History of LISP,” in History of programming lan-\nguages, 1978, pp. 173–185.\n[77]\nL. Bachmair and H. Ganzinger, “Resolution Theorem Proving.”\nHandbook of automated reasoning, vol. 1, no. 02, 2001.\n[78]\nM. Minsky et al., “A framework for representing knowledge,”\n1974.\n[79]\nC. B. Browne, E. Powley, D. Whitehouse, S. M. Lucas, P. I.\nCowling, P. Rohlfshagen, S. Tavener, D. Perez, S. Samothrakis,\nand S. Colton, “A survey of monte carlo tree search methods,”\nIEEE Transactions on Computational Intelligence and AI in games,\nvol. 4, no. 1, pp. 1–43, 2012.\n[80]\nS. Gelly and D. Silver, “Monte-Carlo tree search and rapid action\nvalue estimation in computer Go,” Artificial Intelligence, vol. 175,\nno. 11, pp. 1856–1875, 2011.\n[81]\nM. ´Swiechowski, K. Godlewski, B. Sawicki, and J. Ma´ndziuk,\n“Monte Carlo tree search: A review of recent modifications and\napplications,” Artificial Intelligence Review, vol. 56, no. 3, pp. 2497–\n2562, 2023.\n[82]\nR. S. Sutton, A. G. Barto et al., Reinforcement learning: An introduc-\ntion.\nMIT press Cambridge, 1998, vol. 1, no. 1.\n[83]\nC. J. Watkins and P. Dayan, “Q-learning,” Machine learning, vol. 8,\npp. 279–292, 1992.\n[84]\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski\net al., “Human-level control through deep reinforcement learn-\ning,” nature, vol. 518, no. 7540, pp. 529–533, 2015.\n[85]\nR. R. Torrado, P. Bontrager, J. Togelius, J. Liu, and D. Perez-\nLiebana, “Deep reinforcement learning for general video game\nai,” in 2018 IEEE Conference on Computational Intelligence and\nGames (CIG).\nIEEE, 2018, pp. 1–8.\n[86]\nD. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van\nDen Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershel-\nvam, M. Lanctot et al., “Mastering the game of Go with deep\nneural networks and tree search,” nature, vol. 529, no. 7587, pp.\n484–489, 2016.\n[87]\nD. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang,\nA. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton et al., “Mastering\nthe game of go without human knowledge,” nature, vol. 550, no.\n7676, pp. 354–359, 2017.\n[88]\nO. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu,\nA. Dudzik, J. Chung, D. H. Choi, R. Powell, T. Ewalds,\nP. Georgiev et al., “Grandmaster level in StarCraft II using multi-\nagent reinforcement learning,” nature, vol. 575, no. 7782, pp. 350–\n354, 2019.\n[89]\nA. Y. Ng, D. Harada, and S. Russell, “Policy invariance under\nreward transformations: Theory and application to reward shap-\ning,” in Icml, vol. 99.\nCiteseer, 1999, pp. 278–287.\n[90]\nH. Luo, Q. Sun, C. Xu, P. Zhao, J. Lou, C. Tao, X. Geng, Q. Lin,\nS. Chen, and D. Zhang, “Wizardmath: Empowering mathemat-\nical reasoning for large language models via reinforced evol-\ninstruct,” arXiv preprint arXiv:2308.09583, 2023.\n[91]\nZ. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang,\nM. Zhang, Y. Li, Y. Wu et al., “Deepseekmath: Pushing the limits\nof mathematical reasoning in open language models,” arXiv\npreprint arXiv:2402.03300, 2024.\n[92]\nE. Zelikman, G. Harik, Y. Shao, V. Jayasiri, N. Haber, and N. D.\nGoodman, “Quiet-star: Language models can teach themselves\nto think before speaking,” arXiv preprint arXiv:2403.09629, 2024.\n[93]\nY. Zhao, H. Yin, B. Zeng, H. Wang, T. Shi, C. Lyu, L. Wang,\nW. Luo, and K. Zhang, “Marco-o1: Towards open reasoning mod-\nels for open-ended solutions,” arXiv preprint arXiv:2411.14405,\n2024.\n[94]\nJ. Chen, Z. Cai, K. Ji, X. Wang, W. Liu, R. Wang, J. Hou, and\nB. Wang, “Huatuogpt-o1, towards medical complex reasoning\nwith llms,” arXiv preprint arXiv:2412.18925, 2024.\n[95]\nG. Sun, M. Jin, Z. Wang, C.-L. Wang, S. Ma, Q. Wang, Y. N. Wu,\nY. Zhang, and D. Liu, “Visual agents as fast and slow thinkers,”\narXiv preprint arXiv:2408.08862, 2024.\n[96]\nH. Wei, Y. Yin, Y. Li, J. Wang, L. Zhao, J. Sun, Z. Ge, and\nX. Zhang, “Slow Perception: Let’s Perceive Geometric Figures\nStep-by-step,” arXiv preprint arXiv:2412.20631, 2024.\n[97]\nX. Li, G. Dong, J. Jin, Y. Zhang, Y. Zhou, Y. Zhu, P. Zhang,\nand Z. Dou, “Search-o1: Agentic search-enhanced large reasoning\nmodels,” arXiv preprint arXiv:2501.05366, 2025.\n[98]\nQ.\nTeam,\n“QwQ:\nReflect\nDeeply\non\nthe\nBoundaries\nof\nthe Unknown,” November 2024. [Online]. Available: https:\n//qwenlm.github.io/blog/qwq-32b-preview/\n[99]\nN.\nTeam,\n“Sky-T1:\nTrain\nyour\nown\nO1\npreview\nmodel\nwithin $450,” https://novasky-ai.github.io/posts/sky-t1, 2025,\naccessed: 2025-01-09.\n[100] Y. Zhang, S. Wu, Y. Yang, J. Shu, J. Xiao, C. Kong, and\nJ. Sang, “o1-coder: an o1 replication for coding,” arXiv preprint\narXiv:2412.00154, 2024.\n[101] J. Wang, F. Meng, Y. Liang, and J. Zhou, “DRT-o1: Optimized\nDeep Reasoning Translation via Long Chain-of-Thought,” arXiv\npreprint arXiv:2412.17498, 2024.\n[102] X. Chen, J. Xu, T. Liang, Z. He, J. Pang, D. Yu, L. Song, Q. Liu,\nM. Zhou, Z. Zhang et al., “Do NOT Think That Much for 2\n\nJOURNAL OF LATEX CLASS FILES, JANUARY 2025\n19\n+ 3=? On the Overthinking of o1-Like LLMs,” arXiv preprint\narXiv:2412.21187, 2024.\n[103] W. Zeng, Y. Huang, W. Liu, K. He, Q. Liu, Z. Ma, and J. He,\n“7B Model and 8K Examples: Emerging Reasoning with Re-\ninforcement Learning is Both Effective and Efficient,” https:\n//hkust-nlp.notion.site/simplerl-reason, 2025, notion Blog.\n[104] Z. Wan, X. Feng, M. Wen, S. M. McAleer, Y. Wen, W. Zhang, and\nJ. Wang, “Alphazero-like tree-search can guide large language\nmodel decoding and training,” in Forty-first International Confer-\nence on Machine Learning, 2024.\n[105] Z. Bi, K. Han, C. Liu, Y. Tang, and Y. Wang, “Forest-of-Thought:\nScaling Test-Time Compute for Enhancing LLM Reasoning,”\nCoRR, vol. abs/2412.09078, 2024.\n[106] J. Li, H. Le, Y. Zhou, C. Xiong, S. Savarese, and D. Sahoo,\n“CodeTree: Agent-guided Tree Search for Code Generation with\nLarge Language Models,” CoRR, vol. abs/2411.04329, 2024.\n[107] J. Cheng, X. Liu, C. Wang, X. Gu, Y. Lu, D. Zhang, Y. Dong,\nJ. Tang, H. Wang, and M. Huang, “SPaR: Self-Play with Tree-\nSearch Refinement to Improve Instruction-Following in Large\nLanguage Models,” CoRR, vol. abs/2412.11605, 2024.\n[108] J. Qiu, Y. Lu, Y. Zeng, J. Guo, J. Geng, H. Wang, K. Huang, Y. Wu,\nand M. Wang, “TreeBoN: Enhancing Inference-Time Alignment\nwith Speculative Tree-Search and Best-of-N Sampling,” CoRR,\nvol. abs/2410.16033, 2024.\n[109] N. Dainese, M. Merler, M. Alakuijala, and P. Marttinen, “Gener-\nating Code World Models with Large Language Models Guided\nby Monte Carlo Tree Search,” CoRR, vol. abs/2405.15383, 2024.\n[110] Z. Zhao, W. S. Lee, and D. Hsu, “Large Language Models\nas Commonsense Knowledge for Large-Scale Task Planning,”\nin Advances in Neural Information Processing Systems 36: Annual\nConference on Neural Information Processing Systems 2023, NeurIPS\n2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023.\n[111] Q. Li, W. Xia, K. Du, X. Dai, R. Tang, Y. Wang, Y. Yu, and\nW. Zhang, “RethinkMCTS: Refining Erroneous Thoughts in\nMonte Carlo Tree Search for Code Generation,” CoRR, vol.\nabs/2409.09584, 2024.\n[112] D. Zhang, X. Huang, D. Zhou, Y. Li, and W. Ouyang, “Accessing\nGPT-4 level Mathematical Olympiad Solutions via Monte Carlo\nTree Self-refine with LLaMa-3 8B,” CoRR, vol. abs/2406.07394,\n2024.\n[113] G. Rabby, F. Keya, P. Zamil, and S. Auer, “MC-NEST – Enhancing\nMathematical Reasoning in Large Language Models with a\nMonte Carlo Nash Equilibrium Self-Refine Tree,” 2024. [Online].\nAvailable: https://arxiv.org/abs/2411.15645\n[114] B. Xu, Y. Lin, Y. Li, and Y. Gao, “SRA-MCTS: Self-driven Rea-\nsoning Augmentation with Monte Carlo Tree Search for Code\nGeneration,” CoRR, vol. abs/2411.11053, 2024.\n[115] J. Kang, X. Z. Li, X. Chen, A. Kazemi, and B. Chen, “MindStar:\nEnhancing Math Reasoning in Pre-trained LLMs at Inference\nTime,” CoRR, vol. abs/2405.16265, 2024.\n[116] P. Kadam, “GPT-Guided Monte Carlo Tree Search for Sym-\nbolic Regression in Financial Fraud Detection,” CoRR, vol.\nabs/2411.04459, 2024.\n[117] D. Zhang, J. Wu, J. Lei, T. Che, J. Li, T. Xie, X. Huang, S. Zhang,\nM. Pavone, Y. Li, W. Ouyang, and D. Zhou, “LLaMA-Berry:\nPairwise Optimization for O1-like Olympiad-Level Mathematical\nReasoning,” CoRR, vol. abs/2410.02884, 2024.\n[118] D. Zhang, S. Zhoubian, Y. Yue, Y. Dong, and J. Tang, “ReST-\nMCTS*: LLM Self-Training via Process Reward Guided Tree\nSearch,” CoRR, vol. abs/2406.03816, 2024.\n[119] Y. Xie, A. Goyal, W. Zheng, M. Kan, T. P. Lillicrap, K. Kawaguchi,\nand M. Shieh, “Monte Carlo Tree Search Boosts Reasoning via\nIterative Preference Learning,” CoRR, vol. abs/2405.00451, 2024.\n[120] G. Rabby, F. Keya, P. Zamil, and S. Auer, “MC-NEST - Enhanc-\ning Mathematical Reasoning in Large Language Models with\na Monte Carlo Nash Equilibrium Self-Refine Tree,” CoRR, vol.\nabs/2411.15645, 2024.\n[121] J. Y. Koh, S. McAleer, D. Fried, and R. Salakhutdinov, “Tree Search\nfor Language Model Agents,” CoRR, vol. abs/2407.01476, 2024.\n[122] J.\nLiu,\nA.\nCohen,\nR.\nPasunuru,\nY.\nChoi,\nH.\nHajishirzi,\nand A. Celikyilmaz, “Don’t throw away your value model!\nGenerating more preferable text with Value-Guided Monte-\nCarlo\nTree\nSearch\ndecoding,”\n2024.\n[Online].\nAvailable:\nhttps://arxiv.org/abs/2309.15028\n[123] C. Zhang, J. Song, S. Li, Y. Liang, Y. Ma, W. Wang, Y. Zhu, and\nS. Zhu, “Proposing and solving olympiad geometry with guided\ntree search,” CoRR, vol. abs/2412.10673, 2024.\n[124] H. Jiang, Y. Ma, C. Ding, K. Luan, and X. Di, “Towards\nIntrinsic Self-Correction Enhancement in Monte Carlo Tree\nSearch Boosted Reasoning via Iterative Preference Learning,”\n2024. [Online]. Available: https://arxiv.org/abs/2412.17397\n[125] H. Xu, “No Train Still Gain. Unleash Mathematical Reasoning of\nLarge Language Models with Monte Carlo Tree Search Guided\nby Energy Function,” CoRR, vol. abs/2309.03224, 2023.\n[126] M. Kemmerling, D. L¨utticke, and R. H. Schmitt, “Beyond games:\na systematic review of neural Monte Carlo tree search applica-\ntions,” Appl. Intell., vol. 54, no. 11-12, pp. 1020–1046, 2024.\n[127] Y. Li, Z. Lin, S. Zhang, Q. Fu, B. Chen, J.-G. Lou, and W. Chen,\n“Making large language models better reasoners with step-aware\nverifier,” arXiv preprint arXiv:2206.02336, 2022.\n[128] P. Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and\nZ. Sui, “Math-shepherd: Verify and reinforce llms step-by-step\nwithout human annotations,” in Proceedings of the 62nd Annual\nMeeting of the Association for Computational Linguistics (Volume 1:\nLong Papers), 2024, pp. 9426–9439.\n[129] J. Lu, Z. Dou, W. Hongru, Z. Cao, J. Dai, Y. Feng, and Z. Guo,\n“Autopsv: Automated process-supervised verifier,” in The Thirty-\neighth Annual Conference on Neural Information Processing Systems,\n2024.\n[130] L. Yuan, W. Li, H. Chen, G. Cui, N. Ding, K. Zhang, B. Zhou,\nZ. Liu, and H. Peng, “Free process rewards without process\nlabels,” arXiv preprint arXiv:2412.01981, 2024.\n[131] F. Yu, A. Gao, and B. Wang, “OVM, Outcome-supervised Value\nModels for Planning in Mathematical Reasoning,” in Findings of\nthe Association for Computational Linguistics: NAACL 2024, 2024,\npp. 858–875.\n[132] D. Zhang, S. Zhoubian, Z. Hu, Y. Yue, Y. Dong, and J. Tang, “Rest-\nmcts*: Llm self-training via process reward guided tree search,”\narXiv preprint arXiv:2406.03816, 2024.\n[133] L. Luo, Y. Liu, R. Liu, S. Phatale, H. Lara, Y. Li, L. Shu, Y. Zhu,\nL. Meng, J. Sun et al., “Improve Mathematical Reasoning in\nLanguage Models by Automated Process Supervision,” arXiv\npreprint arXiv:2406.06592, 2024.\n[134] Z. Sun, Q. Wang, W. Yu, X. Zang, K. Zheng, J. Xu, X. Zhang,\nS. Yang, and H. Li, “ReARTeR: Retrieval-Augmented Rea-\nsoning with Trustworthy Process Rewarding,” arXiv preprint\narXiv:2501.07861, 2025.\n[135] Z.\nZhang,\nC.\nZheng,\nY.\nWu,\nB.\nZhang,\nR.\nLin,\nB.\nYu,\nD. Liu, J. Zhou, and J. Lin, “The lessons of developing pro-\ncess reward models in mathematical reasoning,” arXiv preprint\narXiv:2501.07301, 2025.\n[136] Z. Yu, W. Gu, Y. Wang, Z. Zeng, J. Wang, W. Ye, and S. Zhang,\n“Outcome-Refining Process Supervision for Code Generation,”\narXiv preprint arXiv:2412.15118, 2024.\n[137] X. Lai, Z. Tian, Y. Chen, S. Yang, X. Peng, and J. Jia, “Step-dpo:\nStep-wise preference optimization for long-chain reasoning of\nllms,” arXiv preprint arXiv:2406.18629, 2024.\n[138] Y. Liu, J. Lu, Z. Chen, C. Qu, J. K. Liu, C. Liu, Z. Cai, Y. Xia,\nL. Zhao, J. Bian et al., “AdaptiveStep: Automatically Divid-\ning Reasoning Step through Model Confidence,” arXiv preprint\narXiv:2502.13943, 2025.\n[139] F. Yu, A. Gao, and B. Wang, “Outcome-supervised veri-\nfiers for planning in mathematical reasoning,” arXiv preprint\narXiv:2311.09724, 2023.\n[140] J. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang,\nA. Creswell, G. Irving, and I. Higgins, “Solving math word prob-\nlems with process-and outcome-based feedback,” arXiv preprint\narXiv:2211.14275, 2022.\n[141] Y. Li, Z. Lin, S. Zhang, Q. Fu, B. Chen, J.-G. Lou, and W. Chen,\n“Making language models better reasoners with step-aware veri-\nfier,” in Proceedings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), 2023, pp. 5315–\n5333.\n[142] Z. Wu, Y. Hu, W. Shi, N. Dziri, A. Suhr, P. Ammanabrolu, N. A.\nSmith, M. Ostendorf, and H. Hajishirzi, “Fine-grained human\nfeedback gives better rewards for language model training,”\nAdvances in Neural Information Processing Systems, vol. 36, pp.\n59 008–59 033, 2023.\n[143] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon,\nand C. Finn, “Direct preference optimization: Your language\nmodel is secretly a reward model,” Advances in Neural Information\nProcessing Systems, vol. 36, 2024.\n[144] E. Zelikman, Y. Wu, J. Mu, and N. D. Goodman, “STaR: Boot-\nstrapping Reasoning With Reasoning,” in Advances in Neural\n\nJOURNAL OF LATEX CLASS FILES, JANUARY 2025\n20\nInformation Processing Systems 35: Annual Conference on Neural\nInformation Processing Systems 2022, NeurIPS 2022, New Orleans,\nLA, USA, November 28 - December 9, 2022, 2022.\n[145] A. Hosseini, X. Yuan, N. Malkin, A. Courville, A. Sordoni,\nand R. Agarwal, “V-STaR: Training Verifiers for Self-Taught\nReasoners,” 2024. [Online]. Available: https://arxiv.org/abs/\n2402.06457\n[146] W.\nZeng,\nY.\nHuang,\nL.\nZhao,\nY.\nWang,\nZ.\nShan,\nand\nJ. He, “B-STaR: Monitoring and Balancing Exploration and\nExploitation in Self-Taught Reasoners,” 2024. [Online]. Available:\nhttps://arxiv.org/abs/2412.17256\n[147] X. Guan, L. L. Zhang, Y. Liu, N. Shang, Y. Sun, Y. Zhu, F. Yang,\nand M. Yang, “rStar-Math: Small LLMs Can Master Math\nReasoning with Self-Evolved Deep Thinking,” 2025. [Online].\nAvailable: https://arxiv.org/abs/2501.04519\n[148] C¸ . G¨ulc¸ehre, T. L. Paine, S. Srinivasan, K. Konyushkova,\nL. Weerts, A. Sharma, A. Siddhant, A. Ahern, M. Wang, C. Gu,\nW. Macherey, A. Doucet, O. Firat, and N. de Freitas, “Rein-\nforced Self-Training (ReST) for Language Modeling,” CoRR, vol.\nabs/2308.08998, 2023.\n[149] A. Singh, J. D. Co-Reyes, R. Agarwal, A. Anand, P. Patil,\nX. Garcia, P. J. Liu, J. Harrison, J. Lee, K. Xu, A. Parisi, A. Kumar,\nA. Alemi, A. Rizkowsky, A. Nova, B. Adlam, B. Bohnet,\nG. Elsayed, H. Sedghi, I. Mordatch, I. Simpson, I. Gur, J. Snoek,\nJ. Pennington, J. Hron, K. Kenealy, K. Swersky, K. Mahajan,\nL. Culp, L. Xiao, M. L. Bileschi, N. Constant, R. Novak,\nR. Liu, T. Warkentin, Y. Qian, Y. Bansal, E. Dyer, B. Neyshabur,\nJ. Sohl-Dickstein, and N. Fiedel, “Beyond Human Data: Scaling\nSelf-Training for Problem-Solving with Language Models,” 2024.\n[Online]. Available: https://arxiv.org/abs/2312.06585\n[150] F. Xu, Q. Sun, K. Cheng, J. Liu, Y. Qiao, and Z. Wu, “Interac-\ntive Evolution: A Neural-Symbolic Self-Training Framework For\nLarge Language Models,” CoRR, vol. abs/2406.11736, 2024.\n[151] Y.\nQu,\nT.\nZhang,\nN.\nGarg,\nand\nA.\nKumar,\n“Recursive\nIntrospection: Teaching Language Model Agents How to Self-\nImprove,” 2024. [Online]. Available: https://arxiv.org/abs/2407.\n18219\n[152] Y. Deng, P. Lu, F. Yin, Z. Hu, S. Shen, Q. Gu, J. Zou, K.-W. Chang,\nand W. Wang, “Enhancing Large Vision Language Models\nwith Self-Training on Image Comprehension,” 2024. [Online].\nAvailable: https://arxiv.org/abs/2405.19716\n[153] J. Pang, P. Wang, K. Li, X. Chen, J. Xu, Z. Zhang, and Y. Yu,\n“Language Model Self-improvement by Reinforcement Learning\nContemplation,” in The Twelfth International Conference on Learn-\ning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024.\nOpenReview.net, 2024.\n[154] Y. Dubois, C. X. Li, R. Taori, T. Zhang, I. Gulrajani, J. Ba,\nC. Guestrin, P. Liang, and T. B. Hashimoto, “AlpacaFarm: A\nSimulation Framework for Methods that Learn from Human\nFeedback,” in Advances in Neural Information Processing Systems\n36: Annual Conference on Neural Information Processing Systems\n2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023,\n2023.\n[155] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegr-\neffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang, S. Gupta,\nB. P. Majumder, K. Hermann, S. Welleck, A. Yazdanbakhsh, and\nP. Clark, “Self-Refine: Iterative Refinement with Self-Feedback,”\nin Advances in Neural Information Processing Systems 36: Annual\nConference on Neural Information Processing Systems 2023, NeurIPS\n2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023.\n[156] N. Miao, Y. W. Teh, and T. Rainforth, “SelfCheck: Using LLMs\nto Zero-Shot Check Their Own Step-by-Step Reasoning,” in The\nTwelfth International Conference on Learning Representations, ICLR\n2024, Vienna, Austria, May 7-11, 2024.\nOpenReview.net, 2024.\n[157] Z. Gou, Z. Shao, Y. Gong, Y. Shen, Y. Yang, N. Duan, and W. Chen,\n“CRITIC: Large Language Models Can Self-Correct with Tool-\nInteractive Critiquing,” in The Twelfth International Conference on\nLearning Representations, ICLR 2024, Vienna, Austria, May 7-11,\n2024.\nOpenReview.net, 2024.\n[158] Q. Zhong, L. Ding, J. Liu, B. Du, and D. Tao, “ROSE Doesn’t Do\nThat: Boosting the Safety of Instruction-Tuned Large Language\nModels with Reverse Prompt Contrastive Decoding,” 2024.\n[Online]. Available: https://arxiv.org/abs/2402.11889\n[159] Y. Weng, M. Zhu, F. Xia, B. Li, S. He, S. Liu, B. Sun, K. Liu,\nand J. Zhao, “Large Language Models are Better Reasoners with\nSelf-Verification,” in Findings of the Association for Computational\nLinguistics: EMNLP 2023, Singapore, December 6-10, 2023.\nAsso-\nciation for Computational Linguistics, 2023, pp. 2550–2575.\n[160] Y. Xie, K. Kawaguchi, Y. Zhao, X. Zhao, M.-Y. Kan, J. He, and\nQ. Xie, “Self-Evaluation Guided Beam Search for Reasoning,”\n2023. [Online]. Available: https://arxiv.org/abs/2305.00633\n[161] Y. Yao, H. Wu, Q. Xu, and L. Song, “Fine-grained Conversational\nDecoding via Isotropic and Proximal Search,” 2023. [Online].\nAvailable: https://arxiv.org/abs/2310.08130\n[162] J. Chen, W. Lin, J. Mei, and B. Byrne, “Control-DAG: Constrained\nDecoding for Non-Autoregressive Directed Acyclic T5 using\nWeighted Finite State Automata,” 2024. [Online]. Available:\nhttps://arxiv.org/abs/2404.06854\n[163] N. Xu, C. Zhou, A. Celikyilmaz, and X. Ma, “Look-back\nDecoding for Open-Ended Text Generation,” 2023. [Online].\nAvailable: https://arxiv.org/abs/2305.13477\n[164] Y. Yao, H. Wu, Z. Guo, B. Zhou, J. Gao, S. Luo, H. Hou, X. Fu, and\nL. Song, “Learning From Correctness Without Prompting Makes\nLLM Efficient Reasoner,” CoRR, vol. abs/2403.19094, 2024.\n[165] T. Anthony, Z. Tian, and D. Barber, “Thinking Fast and Slow\nwith Deep Learning and Tree Search,” 2017. [Online]. Available:\nhttps://arxiv.org/abs/1705.08439\n[166] N. Miao, Y. W. Teh, and T. Rainforth, “Selfcheck: Using llms to\nzero-shot check their own step-by-step reasoning,” arXiv preprint\narXiv:2308.00436, 2023.\n[167] S. An, Z. Ma, Z. Lin, N. Zheng, J.-G. Lou, and W. Chen, “Learn-\ning from mistakes makes llm better reasoner,” arXiv preprint\narXiv:2310.20689, 2023.\n[168] Z. Li, X. Hu, A. Liu, K. Zheng, S. Huang, and H. Xiong, “Refiner:\nRestructure retrieval content efficiently to advance question-\nanswering capabilities,” arXiv preprint arXiv:2406.11357, 2024.\n[169] J. Wu, M. Feng, S. Zhang, F. Che, Z. Wen, and J. Tao, “Beyond ex-\namples: High-level automated reasoning paradigm in in-context\nlearning via mcts,” arXiv preprint arXiv:2411.18478, 2024.\n[170] L. Yang, Z. Yu, T. Zhang, M. Xu, J. E. Gonzalez, B. Cui, and S. Yan,\n“Supercorrect: Supervising and correcting language models with\nerror-driven insights,” arXiv preprint arXiv:2410.09008, 2024.\n[171] L. Yang, Z. Yu, B. Cui, and M. Wang, “ReasonFlux: Hierarchical\nLLM Reasoning via Scaling Thought Templates,” 2025. [Online].\nAvailable: https://arxiv.org/abs/2502.06772\n[172] Z. Qi, M. Ma, J. Xu, L. L. Zhang, F. Yang, and M. Yang, “Mutual\nreasoning makes smaller llms stronger problem-solvers,” arXiv\npreprint arXiv:2408.06195, 2024.\n[173] D. Zhang, J. Wu, J. Lei, T. Che, J. Li, T. Xie, X. Huang, S. Zhang,\nM. Pavone, Y. Li et al., “Llama-berry: Pairwise optimization for\no1-like olympiad-level mathematical reasoning,” arXiv preprint\narXiv:2410.02884, 2024.\n[174] L. Yang, Z. Yu, T. Zhang, S. Cao, M. Xu, W. Zhang, J. E. Gonzalez,\nand B. Cui, “Buffer of Thoughts: Thought-Augmented Reasoning\nwith Large Language Models,” arXiv preprint arXiv:2406.04271,\n2024.\n[175] H. Yao, J. Huang, W. Wu, J. Zhang, Y. Wang, S. Liu, Y. Wang,\nY. Song, H. Feng, L. Shen et al., “Mulberry: Empowering mllm\nwith o1-like reasoning and reflection via collective monte carlo\ntree search,” arXiv preprint arXiv:2412.18319, 2024.\n[176] G. Xu, P. Jin, L. Hao, Y. Song, L. Sun, and L. Yuan, “LLaVA-o1:\nLet Vision Language Models Reason Step-by-Step,” arXiv preprint\narXiv:2411.10440, 2024.\n[177] O. Thawakar, D. Dissanayake, K. More, R. Thawkar, A. Heakl,\nN.\nAhsan,\nY.\nLi,\nM.\nZumri,\nJ.\nLahoud,\nR.\nM.\nAnwer\net al., “LlamaV-o1: Rethinking Step-by-step Visual Reasoning in\nLLMs,” arXiv preprint arXiv:2501.06186, 2025.\n[178] K. Xiang, Z. Liu, Z. Jiang, Y. Nie, R. Huang, H. Fan, H. Li,\nW. Huang, Y. Zeng, J. Han et al., “AtomThink: A Slow Think-\ning Framework for Multimodal Mathematical Reasoning,” arXiv\npreprint arXiv:2411.11930, 2024.\n[179] Z. Zhang, A. Zhang, M. Li, and A. Smola, “Automatic chain of\nthought prompting in large language models. arxiv 2022,” arXiv\npreprint arXiv:2210.03493.\n[180] W. Chen, X. Ma, X. Wang, and W. W. Cohen, “Program of\nthoughts prompting: Disentangling computation from reasoning\nfor numerical reasoning tasks,” arXiv preprint arXiv:2211.12588,\n2022.\n[181] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan,\nand G. Neubig, “Pal: Program-aided language models,” in Inter-\nnational Conference on Machine Learning.\nPMLR, 2023, pp. 10 764–\n10 799.\n\nJOURNAL OF LATEX CLASS FILES, JANUARY 2025\n21\n[182] T. Khot, H. Trivedi, M. Finlayson, Y. Fu, K. Richardson,\nP. Clark, and A. Sabharwal, “Decomposed prompting: A\nmodular approach for solving complex tasks,” arXiv preprint\narXiv:2210.02406, 2022.\n[183] D. Zhou, N. Sch¨arli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuur-\nmans, C. Cui, O. Bousquet, Q. Le et al., “Least-to-most prompting\nenables complex reasoning in large language models,” arXiv\npreprint arXiv:2205.10625, 2022.\n[184] B.\nKlieger\net\nal.,\n“g1:\nUsing\nLlama-3.1\n70b\non\nGroq\nto\ncreate o1-like reasoning chains,” 2024. [Online]. Available:\nhttps://github.com/bklieger-groq/g1\n[185] X. Hou, M. Yang, W. Jiao, X. Wang, Z. Tu, and W. X. Zhao,\n“CoAct: A Global-Local Hierarchy for Autonomous Agent Col-\nlaboration,” arXiv preprint arXiv:2406.13381, 2024.\n[186] M. Shen, G. Zeng, Z. Qi, Z.-W. Hong, Z. Chen, W. Lu, G. Wornell,\nS. Das, D. Cox, and C. Gan, “Satori: Reinforcement Learning with\nChain-of-Action-Thought Enhances LLM Reasoning via Autore-\ngressive Search,” arXiv preprint arXiv:2502.02508, 2025.\n[187] OpenAI, “Reinforcement fine-tuning,” 2024.\n[188] Y. Chang, X. Wang, J. Wang, Y. Wu, L. Yang, K. Zhu, H. Chen,\nX. Yi, C. Wang, Y. Wang et al., “A survey on evaluation of large\nlanguage models,” ACM Transactions on Intelligent Systems and\nTechnology, vol. 15, no. 3, pp. 1–45, 2024.\n[189] L. Trung, X. Zhang, Z. Jie, P. Sun, X. Jin, and H. Li, “Reft:\nReasoning with reinforced fine-tuning,” in Proceedings of the 62nd\nAnnual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), 2024, pp. 7601–7614.\n[190] interconnects.ai, “Blob reinforcement fin-tuning,” 2024.\n[191] K. Gandhi, D. Lee, G. Grand, M. Liu, W. Cheng, A. Sharma, and\nN. D. Goodman, “Stream of search (sos): Learning to search in\nlanguage, 2024,” URL https://arxiv. org/abs/2404.03683, 2024.\n[192] K. Team, A. Du, B. Gao, B. Xing, C. Jiang, C. Chen, C. Li, C. Xiao,\nC. Du, C. Liao et al., “Kimi k1. 5: Scaling Reinforcement Learning\nwith LLMs,” arXiv preprint arXiv:2501.12599, 2025.\n[193] K. Zhang, Q. Yao, B. Lai, J. Huang, W. Fang, D. Tao, M. Song,\nand S. Liu, “Reasoning with reinforced functional token tuning,”\narXiv preprint arXiv:2502.13389, 2025.\n[194] Z.\nLin,\nY.\nTang,\nX.\nYao,\nD.\nYin,\nZ.\nHu,\nY.\nSun,\nand\nK.-W. Chang, “QLASS: Boosting Language Agent Inference\nvia Q-Guided Stepwise Search,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2502.02584\n[195] G. Cui, L. Yuan, Z. Wang, H. Wang, W. Li, B. He, Y. Fan, T. Yu,\nQ. Xu, W. Chen et al., “Process Reinforcement through Implicit\nRewards,” arXiv preprint arXiv:2502.01456, 2025.\n[196] M. Luo, S. Tan, J. Wong, X. Shi, W. Tang, M. Roongta, C. Cai,\nJ. Luo, T. Zhang, E. Li, R. A. Popa, and I. Stoica, “DeepScaleR:\nSurpassing O1-Preview with a 1.5B Model by Scaling RL,” 2025,\nnotion Blog.\n[197] J. Cheng, L. Li, G. Xiong, J. Shao, and Y. Lv, “Stop gamma decay:\nMin-form credit assignment is all process reward model needs\nfor reasoning,” 2025, notion Blog.\n[198] H. Team, “Open r1: A fully open reproduction of deepseek-r1.”\nhttps://github.com/huggingface/open-r1, 2025, github Project.\n[199] J. Pan, J. Zhang, X. Wang, L. Yuan, H. Peng, and A. Suhr,\n“TinyZero,” https://github.com/Jiayi-Pan/TinyZero, 2025, ac-\ncessed: 2025-01-24.\n[200] Z. Liu, C. Chen, W. Li, T. Pang, C. Du, and M. Lin, “There may not\nbe aha moment in r1-zero-like training — a pilot study,” https:\n//oatllm.notion.site/oat-zero, 2025, notion Blog.\n[201] Z. Liu, C. Chen, C. Du, W. S. Lee, and M. Lin, “Oat: A\nresearch-friendly\nframework\nfor\nllm\nonline\nalignment,”\n[https://github.com/sail-sg/oat](https://github.com/sail-\nsg/oat), 2025.\n[202] X. Li, H. Zou, and P. Liu, “Limr: Less is more for rl scaling,” arXiv\npreprint arXiv:2502.11886, 2025.\n[203] Z. Xie, L. Chen, W. Mao, J. Xu, L. Kong et al., “Teaching language\nmodels to critique via reinforcement learning,” arXiv preprint\narXiv:2502.03492, 2025.\n[204] T. Xie, Z. Gao, Q. Ren, H. Luo, Y. Hong, B. Dai, J. Zhou, K. Qiu,\nZ. Wu, and C. Luo, “Logic-rl: Unleashing llm reasoning with\nrule-based reinforcement learning,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2502.14768\n[205] H. Zhang, J. Yao, C. Ye, W. Xiong, and T. Zhang, “Online-dpo-r1:\nUnlocking effective reasoning without the ppo overhead,” 2025,\nnotion Blog.\n[206] J. Hu, Y. Zhang, Q. Han, D. Jiang, and H.-Y. S. Xiangyu Zhang,\n“Open-reasoner-zero: An open source approach to scaling re-\ninforcement learning on the base model,” https://github.com/\nOpen-Reasoner-Zero/Open-Reasoner-Zero, 2025.\n[207] T. Yu, Y. Yao, H. Zhang, T. He, Y. Han, G. Cui, J. Hu, Z. Liu,\nH.-T. Zheng, M. Sun et al., “Rlhf-v: Towards trustworthy mllms\nvia behavior alignment from fine-grained correctional human\nfeedback,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2024, pp. 13 807–13 816.\n[208] H. Lee, S. Phatale, H. Mansoor, K. R. Lu, T. Mesnard, J. Ferret,\nC. Bishop, E. Hall, V. Carbune, and A. Rastogi, “Rlaif: Scaling\nreinforcement learning from human feedback with ai feedback,”\n2023.\n[209] Y.-F. Zhang, T. Yu, H. Tian, C. Fu, P. Li, J. Zeng, W. Xie, Y. Shi,\nH. Zhang, J. Wu et al., “Mm-rlhf: The next step forward in\nmultimodal llm alignment,” arXiv preprint arXiv:2502.10391, 2025.\n[210] J.\nJi,\nJ.\nZhou,\nH.\nLou,\nB.\nChen,\nD.\nHong,\nX.\nWang,\nW. Chen, K. Wang, R. Pan, J. Li, M. Wang, J. Dai, T. Qiu,\nH. Xu, D. Li, W. Chen, J. Song, B. Zheng, and Y. Yang,\n“Align\nanything:\nTraining\nall-modality\nmodels\nto\nfollow\ninstructions with language feedback,” 2024. [Online]. Available:\nhttps://arxiv.org/abs/2412.15838\n[211] L. Chen, L. Li, H. Zhao, Y. Song, and Vinci, “R1-V: Reinforcing\nSuper Generalization Ability in Vision-Language Models with\nLess Than $3,” https://github.com/Deep-Agent/R1-V, 2025, ac-\ncessed: 2025-02-02.\n[212] H. Shen, Z. Zhang, Q. Zhang, R. Xu, and T. Zhao, “Vlm-r1: A\nstable and generalizable r1-style large vision-language model,”\nhttps://github.com/om-ai-lab/VLM-R1, 2025, accessed: 2025-\n02-15.\n[213] Y. Peng, G. Zhang, X. Geng, and X. Yang, “Lmm-r1,” https://\ngithub.com/TideDra/lmm-r1, 2025, accessed: 2025-02-13.\n[214] X. Wang and P. Peng, “Open-r1-video,” https://github.com/\nWang-Xiaodong1899/Open-R1-Video, 2025.\n[215] Y. Zheng, J. Lu, S. Wang, and Y. Xiong, “EasyR1: An Effi-\ncient, Scalable, Multi-Modality RL Training Framework,” https:\n//github.com/hiyouga/EasyR1, 2025.\n[216] E. Yeo, Y. Tong, M. Niu, G. Neubig, and X. Yue, “Demystify-\ning Long Chain-of-Thought Reasoning in LLMs,” arXiv preprint\narXiv:2502.03373, 2025.\n[217] Z. Hou, P. Du, Y. Niu, Z. Du, A. Zeng, X. Liu, M. Huang, H. Wang,\nJ. Tang, and Y. Dong, “Does RLHF Scale? Exploring the Impacts\nFrom Data, Model, and Method,” arXiv preprint arXiv:2412.06000,\n2024.\n[218] J. Kim, D. Wu, J. Lee, and T. Suzuki, “Metastable Dynamics of\nChain-of-Thought Reasoning: Provable Benefits of Search, RL\nand Distillation,” 2025. [Online]. Available: https://arxiv.org/\nabs/2502.01694\n[219] Z. Liu, C. Chen, W. Li, T. Pang, C. Du, and M. Lin, “There May\nNot be Aha Moment in R1-Zero-like Training — A Pilot Study,”\nhttps://oatllm.notion.site/oat-zero, 2025, notion Blog.\n[220] A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu,\nF. Huang, H. Wei et al., “Qwen2. 5 technical report,” arXiv preprint\narXiv:2412.15115, 2024.\n[221] H. Luo, L. Shen, H. He, Y. Wang, S. Liu, W. Li, N. Tan, X. Cao, and\nD. Tao, “O1-Pruner: Length-Harmonizing Fine-Tuning for O1-\nLike Reasoning Pruning,” arXiv preprint arXiv:2501.12570, 2025.\n[222] J. Hu, “REINFORCE++: A Simple and Efficient Approach\nfor\nAligning\nLarge\nLanguage\nModels,”\narXiv\npreprint\narXiv:2501.03262, 2025.\n[223] J. Muralidharan and T. Thomas, “Deliberate Problem-solving\nwith a Large Language Model as a Brainstorm Aid Using a\nChecklist for Prompt Generation,” The Journal of the Association\nof Physicians of India, vol. 72, no. 5, pp. 89–90, 2024.\n[224] J. Jiang, Z. Chen, Y. Min, J. Chen, X. Cheng, J. Wang, Y. Tang,\nH. Sun, J. Deng, W. X. Zhao et al., “Technical Report: Enhancing\nLLM Reasoning with Reward-guided Tree Search,” arXiv preprint\narXiv:2411.11694, 2024.\n[225] F. Lyu et al., “Thinking Claude,” 2024. [Online]. Available:\nhttps://github.com/richards199999/Thinking-Claude\n[226] AI-MO,\n“Aime\n2024,”\nhttps://huggingface.co/datasets/\nAI-MO/aimo-validation-aime, 2024.\n[227] ——, “Amc 2023,” https://huggingface.co/datasets/AI-MO/\naimo-validation-amc, 2024.\n[228] C. He, R. Luo, Y. Bai, S. Hu, Z. L. Thai, J. Shen, J. Hu, X. Han,\nY. Huang, Y. Zhang et al., “Olympiadbench: A challenging bench-\nmark for promoting agi with olympiad-level bilingual multi-\nmodal scientific problems,” arXiv preprint arXiv:2402.14008, 2024.\n\nJOURNAL OF LATEX CLASS FILES, JANUARY 2025\n22\n[229] C. E. Jimenez, J. Yang, A. Wettig, S. Yao, K. Pei, O. Press,\nand K. R. Narasimhan, “SWE-bench: Can Language Models\nResolve Real-world Github Issues?” in The Twelfth International\nConference on Learning Representations, 2024. [Online]. Available:\nhttps://openreview.net/forum?id=VTF8yNQM66\n[230] N. Jain, K. Han, A. Gu, W.-D. Li, F. Yan, T. Zhang, S. Wang,\nA. Solar-Lezama, K. Sen, and I. Stoica, “Livecodebench: Holistic\nand contamination free evaluation of large language models for\ncode,” arXiv preprint arXiv:2403.07974, 2024.\n[231] D.\nRein,\nB.\nL.\nHou,\nA.\nC.\nStickland,\nJ.\nPetty,\nR.\nY.\nPang, J. Dirani, J. Michael, and S. R. Bowman, “GPQA:\nA Graduate-Level Google-Proof Q&A Benchmark,” in First\nConference on Language Modeling, 2024. [Online]. Available:\nhttps://openreview.net/forum?id=Ti67584b98\n[232] Y. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren,\nA. Arulraj, X. He, Z. Jiang et al., “Mmlu-pro: A more robust and\nchallenging multi-task language understanding benchmark,”\narXiv preprint arXiv:2406.01574, 2024.\n[233] S. Yao, H. Chen, J. Yang, and K. Narasimhan, “Webshop: Towards\nscalable real-world web interaction with grounded language\nagents,” Advances in Neural Information Processing Systems, vol. 35,\npp. 20 744–20 757, 2022.\n[234] S. Zhou, F. F. Xu, H. Zhu, X. Zhou, R. Lo, A. Sridhar,\nX. Cheng, Y. Bisk, D. Fried, U. Alon et al., “WebArena: A\nRealistic Web Environment for Building Autonomous Agents,”\narXiv\npreprint\narXiv:2307.13854,\n2023.\n[Online].\nAvailable:\nhttps://webarena.dev\n[235] R.\nWang,\nP.\nJansen,\nM.-A.\nCˆot´e,\nand\nP.\nAmmanabrolu,\n“ScienceWorld: Is your Agent Smarter than a 5th Grader?” 2022.\n[Online]. Available: https://arxiv.org/abs/2203.07540\n[236] A. Prasad, A. Koller, M. Hartmann, P. Clark, A. Sabharwal,\nM. Bansal, and T. Khot, “ADaPT: As-Needed Decomposition and\nPlanning with Language Models,” in Findings of the Association\nfor Computational Linguistics: NAACL 2024, 2024, pp. 4226–4252.\n[237] H. Chen, Z. Fang, Y. Singla, and M. Dredze, “Benchmarking\nLarge Language Models on Answering and Explaining Challeng-\ning Medical Questions,” arXiv preprint arXiv:2402.18060, 2024.\n[238] D. Jin, E. Pan, N. Oufattole, W.-H. Weng, H. Fang, and\nP. Szolovits, “What disease does this patient have? a large-scale\nopen domain question answering dataset from medical exams,”\nApplied Sciences, vol. 11, no. 14, p. 6421, 2021.\n[239] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens,\nD. Jiang, W. Ren, Y. Sun et al., “Mmmu: A massive multi-\ndiscipline multimodal understanding and reasoning benchmark\nfor expert agi,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2024, pp. 9556–9567.\n[240] P. Lu, H. Bansal, T. Xia, J. Liu, C. Li, H. Hajishirzi, H. Cheng, K.-\nW. Chang, M. Galley, and J. Gao, “MathVista: Evaluating Mathe-\nmatical Reasoning of Foundation Models in Visual Contexts,” in\nInternational Conference on Learning Representations (ICLR), 2024.\n[241] K. Wang, J. Pan, W. Shi, Z. Lu, M. Zhan, and H. Li, “Measuring\nmultimodal mathematical reasoning with math-vision dataset,”\narXiv preprint arXiv:2402.14804, 2024.\n[242] Z.-Z. Li, M.-L. Zhang, F. Yin, Z.-L. Ji, J.-F. Bai, Z.-R. Pan, F.-H.\nZeng, J. Xu, J.-X. Zhang, and C.-L. Liu, “Cmmath: A chinese\nmulti-modal math skill evaluation benchmark for foundation\nmodels,” arXiv preprint arXiv:2407.12023, 2024.\n[243] M.-L. Zhang, F. Yin, and C.-L. Liu, “A Multi-Modal Neural\nGeometric Solver with Textual Clauses Parsed from Diagram,”\nin IJCAI, 2023.\n[244] Z. Xi, Y. Ding, W. Chen, B. Hong, H. Guo, J. Wang, D. Yang,\nC. Liao, X. Guo, W. He, S. Gao, L. Chen, R. Zheng, Y. Zou, T. Gui,\nQ. Zhang, X. Qiu, X. Huang, Z. Wu, and Y.-G. Jiang, “AgentGym:\nEvolving Large Language Model-based Agents across Diverse\nEnvironments,” 2024.\n[245] R. Zhang, D. Jiang, Y. Zhang, H. Lin, Z. Guo, P. Qiu, A. Zhou,\nP. Lu, K.-W. Chang, P. Gao et al., “MathVerse: Does Your Multi-\nmodal LLM Truly See the Diagrams in Visual Math Problems?”\narXiv preprint arXiv:2403.14624, 2024.\n[246] P. Lu, S. Mishra, T. Xia, L. Qiu, K.-W. Chang, S.-C. Zhu, O. Tafjord,\nP. Clark, and A. Kalyan, “Learn to Explain: Multimodal Reason-\ning via Thought Chains for Science Question Answering,” in The\n36th Conference on Neural Information Processing Systems (NeurIPS),\n2022.\n[247] Y. Li, Y. Guo, F. Guerin, and C. Lin, “An open-source data\ncontamination report for large language models,” in Findings of\nthe Association for Computational Linguistics: EMNLP 2024, 2024,\npp. 528–541.\n[248] Claude, “Claude 3.5 Sonnet,” June 2024. [Online]. Available:\nhttps://www.anthropic.com/news/claude-3-5-sonnet\n[249] G.\nDeepMind,\n“Gemini\n2.0\nPro,”\nOctober\n2024.\n[Online].\nAvailable: https://deepmind.google/technologies/gemini/pro/\n[250] I. Team, “InternLM2 Technical Report,” 2024.\n[251] N. Muennighoff, Z. Yang, W. Shi, X. L. Li, L. Fei-Fei, H. Hajishirzi,\nL. Zettlemoyer, P. Liang, E. Cand`es, and T. Hashimoto, “s1:\nSimple test-time scaling,” arXiv preprint arXiv:2501.19393, 2025.\n[252] OpenAI,\n“OpenAI\no1-mini,”\nSeptember\n2024.\n[Online].\nAvailable:\nhttps://openai.com/index/\nopenai-o1-mini-advancing-cost-efficient-reasoning/\n[253] Y. Ye, Z. Huang, Y. Xiao, E. Chern, S. Xia, and P. Liu,\n“LIMO: Less is More for Reasoning,” 2025. [Online]. Available:\nhttps://arxiv.org/abs/2502.03387\n[254] Q.\nTeam,\n“QVQ:\nTo\nSee\nthe\nWorld\nwith\nWisdom,”\nDecember 2024. [Online]. Available: https://qwenlm.github.\nio/blog/qvq-72b-preview/\n[255] K. Zhang, B. Li, P. Zhang, F. Pu, J. A. Cahyono, K. Hu, S. Liu,\nY. Zhang, J. Yang, C. Li, and Z. Liu, “LMMs-Eval: Reality Check\non the Evaluation of Large Multimodal Models,” 2024. [Online].\nAvailable: https://arxiv.org/abs/2407.12772\n[256] O.\nContributors,\n“OpenCompass:\nA\nUniversal\nEvaluation\nPlatform\nfor\nFoundation\nModels,”\nhttps://github.com/\nopen-compass/opencompass, 2023.\n[257] M. Song, Z. Su, X. Qu, J. Zhou, and Y. Cheng, “PRMBench:\nA Fine-grained and Challenging Benchmark for Process-Level\nReward Models,” arXiv preprint arXiv:2501.03124, 2025. [Online].\nAvailable: https://arxiv.org/pdf/2501.03124\n[258] Y. Leviathan, M. Kalman, and Y. Matias, “Fast inference from\ntransformers via speculative decoding,” in International Confer-\nence on Machine Learning, 2023, pp. 19 274–19 286.\n[259] X. Ning, Z. Lin, Z. Zhou, Z. Wang, H. Yang, and Y. Wang,\n“Skeleton-of-thought: Large language models can do parallel\ndecoding,” Proceedings ENLSP-III, 2023.\n[260] X. Miao, G. Oliaro, Z. Zhang, X. Cheng, Z. Wang, Z. Zhang,\nR. Y. Y. Wong, A. Zhu, L. Yang, X. Shi et al., “SpecInfer:\nAccelerating Generative Large Language Model Serving with\nTree-based Speculative Inference and Verification,” arXiv preprint\narXiv:2305.09781, 2023.\n[261] B. Qi, X. Chen, J. Gao, D. Li, J. Liu, L. Wu, and B. Zhou,\n“Interactive continual learning: Fast and slow thinking,” in Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2024, pp. 12 882–12 892.\n[262] Y. Zheng, S. Sun, L. Qiu, D. Ru, C. Jiayang, X. Li, J. Lin, B. Wang,\nY. Luo, R. Pan et al., “OpenResearcher: Unleashing AI for Accel-\nerated Scientific Research,” arXiv preprint arXiv:2408.06941, 2024.\n[263] B. Romera-Paredes, M. Barekatain, A. Novikov, M. Balog, M. P.\nKumar, E. Dupont, F. J. Ruiz, J. S. Ellenberg, P. Wang, O. Fawzi\net al., “Mathematical discoveries from program search with large\nlanguage models,” Nature, vol. 625, no. 7995, pp. 468–475, 2024.\n[264] T. H. Trinh, Y. Wu, Q. V. Le, H. He, and T. Luong, “Solving\nolympiad geometry without human demonstrations,” Nature,\nvol. 625, no. 7995, pp. 476–482, 2024.\n[265] Y. Chervonyi, T. H. Trinh, M. Olˇs´ak, X. Yang, H. Nguyen,\nM. Menegali, J. Jung, V. Verma, Q. V. Le, and T. Luong, “Gold-\nmedalist Performance in Solving Olympiad Geometry with Al-\nphaGeometry2,” arXiv preprint arXiv:2502.03544, 2025.\n[266] G. Team, R. Anil, S. Borgeaud, J.-B. Alayrac, J. Yu, R. Soricut,\nJ. Schalkwyk, A. M. Dai, A. Hauth, K. Millican et al., “Gemini:\na family of highly capable multimodal models,” arXiv preprint\narXiv:2312.11805, 2023.\n[267] G. Team, P. Georgiev, V. I. Lei, R. Burnell, L. Bai, A. Gulati,\nG. Tanzer, D. Vincent, Z. Pan, S. Wang et al., “Gemini 1.5:\nUnlocking multimodal understanding across millions of tokens\nof context,” arXiv preprint arXiv:2403.05530, 2024.\n[268] N. Chen, Z. Zheng, N. Wu, L. Shou, M. Gong, Y. Song, D. Zhang,\nand J. Li, “Breaking language barriers in multilingual math-\nematical reasoning: Insights and observations,” arXiv preprint\narXiv:2310.20246, 2023.\n[269] Y. Du, Z. Liu, Y. Li, W. X. Zhao, Y. Huo, B. Wang, W. Chen, Z. Liu,\nZ. Wang, and J.-R. Wen, “Virgo: A preliminary exploration on\nreproducing o1-like mllm,” arXiv preprint arXiv:2501.01904, 2025.\n[270] M. Parmar and Y. Govindarajulu, “Challenges in Ensuring AI\nSafety in DeepSeek-R1 Models: The Shortcomings of Reinforce-\nment Learning Strategies,” arXiv preprint arXiv:2501.17030, 2025.\n",
  "metadata": {
    "source_path": "papers/arxiv/From_System_1_to_System_2_A_Survey_of_Reasoning_Large_Language_Models_99813130a7c0a443.pdf",
    "content_hash": "99813130a7c0a443cb219cd7e456d25328e1b175e57e7288f1da8b900b228256",
    "arxiv_id": null,
    "title": "From_System_1_to_System_2_A_Survey_of_Reasoning_Large_Language_Models_99813130a7c0a443",
    "author": "",
    "creation_date": "D:20250225030844Z",
    "published": "2025-02-25T03:08:44",
    "pages": 22,
    "size": 1602781,
    "file_mtime": 1740470086.168533
  }
}