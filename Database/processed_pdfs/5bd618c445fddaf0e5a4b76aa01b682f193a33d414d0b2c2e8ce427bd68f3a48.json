{
  "text": "UrduLLaMA 1.0: Dataset Curation, Preprocessing, and Evaluation in\nLow-Resource Settings\nLayba Fiaz, Munief Hassan Tahir, Sana Shams, Sarmad Hussain\nCenter for Language Engineering,\nAl-Khawarizmi Institute of Computer Science,\nUniversity of Engineering and Technology, Lahore\n{firstname.secondname}@kics.edu.pk\nAbstract\nMultilingual Large Language Models (LLMs)\noften provide suboptimal performance on low-\nresource languages like Urdu. This paper in-\ntroduces UrduLLaMA 1.0, a model derived\nfrom the open-source Llama-3.1-8B-Instruct\narchitecture and continually pre-trained on 128\nmillion Urdu tokens, capturing the rich diver-\nsity of the language. To enhance instruction-\nfollowing and translation capabilities, we lever-\nage Low-Rank Adaptation (LoRA) to fine tune\nthe model on 41,000 Urdu instructions and ap-\nproximately 50,000 English-Urdu translation\npairs. Evaluation across three machine trans-\nlation datasets demonstrates significant perfor-\nmance improvements compared to state-of-the-\nart (SOTA) models, establishing a new bench-\nmark for Urdu LLMs. These findings under-\nscore the potential of targeted adaptation strate-\ngies with limited data and computational re-\nsources to address the unique challenges of\nlow-resource languages.\n1\nIntroduction\nThe field of language modeling has experienced a\ntransformative shift, driven by the rapid evolution\nof Large Language Models (LLMs) that have set\nnew standards in natural language understanding\nand generation. While proprietary models like Ope-\nnAI’s ChatGPT (OpenAI, 2022) offer impressive\ncapabilities, their closed nature restricts research\naccessibility.\nOn the other hand, open models\nsuch as LLaMA(Grattafiori et al., 2024) and Mis-\ntral(Jiang et al., 2023) — though smaller in scale —\nhave achieved competitive results across many lan-\nguages. Nevertheless, both open and closed LLMs\nface significant challenges when applied to low-\nresource languages like Urdu. A primary hurdle\nis the inadequate representation of Urdu in train-\ning data, which results in limited vocabulary and\npoor encoding capabilities. This gap in data inclu-\nsion severely hampers the performance of LLMs on\nUrdu NLP tasks, as evidenced by recent benchmark\nstudies(Arif et al., 2024; Tahir et al., 2025). Over-\ncoming this limitation is crucial to unlocking the\nfull potential of LLMs for Urdu and other underrep-\nresented languages. In this research, we tackle this\nchallenge by developing an Urdu-specific LLM.\nWe begin by continually pretraining Llama-3.1-8B-\nInstruct (Grattafiori et al., 2024) on 128 million\nUrdu tokens to enhance the model’s foundational\nrepresentation of the language. This is followed by\ninstruction fine tuning using 41,000 instructions to\nimprove conversational capabilities, and additional\nfine tuning on 50,369 English–Urdu parallel sen-\ntence pairs to boost translation proficiency. The\ntranslation quality was evaluated using the BLEU\nscore across three MT datasets, which showed\nthat the UrduLLaMA 1.0 model outperformed the\nLlama-3.1-8B-Instruct base model. This trend was\nfurther validated through human evaluation, where\ntwo experts found that translations from the UrduL-\nLaMA 1.0 model were more accurate than those\ngenerated by the base model.\nThe paper is structured as follows: Section 1\nintroduces the study and Section 2 presents the\nrelated work. Section 3 details the dataset curation\nand Section 4 explains the steps taken to preprocess\nthe data. This is followed by Section 5, which\ndescribes the development process of UrduLLaMA\n1.0 including the experimental and training details,\nand Section 6, which covers the evaluation and\ndiscussion leading to Section 7, which concludes\nthe paper.\n2\nRelated Work\nDue to the lack of continual pretraining work on\nLLMs for Urdu, this section examines the closest\nrelated works. This includes models developed for\nAsian languages and low-resource languages built\nusing the LLaMA framework.\nTamil-Llama (Balachandran, 2023), an Asian\nlanguage model built on LLaMA 2 (Touvron et al.,\n2023), incorporates 16,000 Tamil tokens and uti-\narXiv:2502.16961v1  [cs.CL]  24 Feb 2025\n\nlizes the Low-Rank Adaptation (LoRA) (Hu et al.,\n2021) technique for efficient training on Tamil\ndatasets. The model was trained on an Nvidia\nA100 GPU with 80GB of VRAM for 48 hours, fol-\nlowed by instruct fine tuning on translated Alpaca\ndatasets (Taori et al., 2023) and a custom subset\nof the OpenOrca (Lian et al., 2023) dataset for 60\nhours using Microsoft Azure’s Standard NC24 ads\nA100v4 instance. Performance evaluations indicate\nsignificant improvements in Tamil text generation,\nwith the Tamil-Llama 13B model outperforming\nOpenAI’s GPT-3.5-turbo on Tamil language tasks.\nTaiwan-LLM (Lin and Chen, 2023), an LLM\nfor Traditional Chinese, underwent continual pre-\ntraining on LLaMA 2 (Touvron et al., 2023) us-\ning 35.1 billion tokens and a diverse instruction\nset derived from 17 fine tuning datasets, includ-\ning 20,000 user feedback instances. The training\nprocess leveraged the Transformer Reinforcement\nLearning (TRL) library (Hu et al., 2023), along\nwith DeepSpeed ZeRO-2 (Rajbhandari et al., 2020)\nand FlashAttention-2 (Dao, 2023) to optimize mem-\nory usage and enhance training efficiency. Utiliz-\ning up to 48 NVIDIA H100 Tensor Core GPUs,\nTaiwan-LLM demonstrated superior performance\nin understanding and generating Traditional Chi-\nnese text, surpassing models such as GPT-4 and\nClaude-2.1.\nPersianLLaMA (Abbasi et al., 2023), the first\nlarge-scale Persian language model, was trained\nfrom scratch on 184 million tokens from Persian\nWikipedia and 9 billion tokens from the OSCAR\ndataset (Ortiz Suárez et al., 2020). The training\nprocess leveraged DeepSpeed (Rasley et al., 2020)\nand TencentPretrain (Zhao et al., 2023), two ad-\nvanced frameworks for optimizing deep learning,\nutilizing two A100 GPUs with 80GB of VRAM\nover 12 days. Additionally, they conducted an ex-\nperiment using LoRA (Hu et al., 2021) with the\noriginal English LLaMA weights, training on a\nsingle A100 GPU with 80GB of VRAM for over\n70 hours. Their evaluations indicate that PersianL-\nLaMA significantly outperformed its competitors\nin both understanding and generating Persian text.\nAiravata (Gala et al., 2024) is an instruction-\ntuned model for Hindi, built by fine tuning Open-\nHathi (SarvamAI, 2023), on 404k instruction\ninstances from diverse Hindi instruction-tuning\ndatasets. OpenHathi (SarvamAI, 2023) is again\na model built on the LLaMA 2 7B architecture.\nThe training employed both full fine tuning and su-\npervised fine tuning using LoRA (Hu et al., 2021).\nTheir results demonstrated that Airavata signifi-\ncantly outperforms OpenHathi on most tasks, high-\nlighting the effectiveness of fine tuning in aligning\nthe base model to a variety of tasks. The details\nregarding their training infrastructure were not pro-\nvided in their paper.\nSeaLLMs (Nguyen et al., 2024b) is an innova-\ntive series of language models focused on South-\neast Asian (SEA) languages. Built upon LLaMA 2\n(Touvron et al., 2023) and Mistral 7B (Jiang et al.,\n2023), SeaLLMs underwent continued pretraining\nwith an extended vocabulary, followed by a hy-\nbrid approach for instruction and alignment tun-\ning. Their evaluation claims that SeaLLMs signifi-\ncantly outperform ChatGPT-3.5 in non-Latin lan-\nguages, such as Thai, Khmer, Lao, and Burmese,\nby large margins, while remaining lightweight and\ncost-effective to operate. The authors did not pro-\nvide detailed information regarding the training\ninfrastructure in their paper.\nA study related to Chinese LLaMA (Cui et al.,\n2024) extended different variants of LLaMA 2\n(Touvron et al., 2023) by adding 20,000 Chinese\ntokens to the existing vocabulary. The model was\npre-trained using LoRA (Hu et al., 2021) and fine\ntuned on Chinese instruction datasets formatted ac-\ncording to Alpaca (Taori et al., 2023). Training\nwas conducted on A40 GPUs (48GB VRAM), with\nup to 48 GPUs used depending on the model size.\nThe parameter-efficient training with LoRA was\ncarried out using the PEFT library1. Additionally,\nDeepSpeed (Rasley et al., 2020) was employed to\noptimize memory efficiency during training. Ex-\nperimental results demonstrate that the newly pro-\nposed model significantly improves the original\nLLaMA’s ability to understand and generate Chi-\nnese content.\nAnother study (Chen et al., 2024b) conducted a\ntwo-stage continual pretraining of LLaMA 3 8B\n(Grattafiori et al., 2024) for Chinese. Initially, they\nperformed experiments on TinyLLaMA (Zhang\net al., 2024), and then applied their findings to train\nLLaMA 3 using 100B tokens, followed by fine\ntuning on Synthetic Scientific QA Data. The ex-\nperiments were implemented using Hugging Face\nTransformers (Wolf et al., 2020), incorporating\nFlash Attention and DeepSpeed ZeRO (Rasley\net al., 2020) to optimize training efficiency. The\nstudy leveraged computing resources provided by\n1https://github.com/huggingface/peft\n\nFigure 1: Development of UrduLLaMA 1.0\nthe Public Computing Cloud at Renmin University\nof China. Their extensive experiments on a number\nof evaluation benchmarks show that their approach\ncan largely improve the performance of the back-\nbone models, including both the general abilities\nand the scientific reasoning abilities without hurt-\ning the original capacities.\nLatxa (Etxaniz et al., 2024) is a family of large\nlanguage models for Basque ranging from 7 to 70\nbillion parameters. Latxa is based on Llama 2 (Tou-\nvron et al., 2023), which they continue pretraining\non their own Basque corpus comprising of 4.2B\ntokens. The training of Latxa has been conducted\nusing the GPT-Neox (Black et al., 2022) library.\nAs infrastructure, they have leveraged the CINECA\nHPC Leonardo computing cluster located in Italy,\nwhich is powered by 3,456 nodes each containing\n4x custom A100 64GB GPUs. They claimed that\nLatxa outperforms all previous open models they\ncompared to by a large margin.\nVinaLLaMA (Nguyen et al., 2023), an open-\nweight, state-of-the-art (SOTA) Large Language\nModel for the Vietnamese language, was built upon\nLLaMA 2 (Touvron et al., 2023) with an additional\n800 billion trained tokens followed by fine tuning\non 1 million sample instruction of Vietnamese and\nEnglish. For our pretraining phase, they utilized a\ncluster consisting of eight nodes, each equipped\nwith 8x Intel Habana Gaudi2 Processors. This\nphase was completed over the course of one week.\nIn contrast, the fine tuning phase was conducted\nmore rapidly, utilizing a single node of Google\nCloud TPU v5e, and completed within a single\nday. They claim to achieve state-of-the-art results\non different key benchmarks, showcasing fluency\nin Vietnamese and a deep understanding of their\nculture.\nVI-MISTRAL-X (Vo, 2024) is another LLM de-\nsigned expressly for the Vietnamese language. It\nperformed continual pretraining on 8 billion to-\nkens selected from CulturaX (Nguyen et al., 2024a),\non the Mistral architecture, using 8 Nvidia H100\n80GB SXM5 GPUs. Following the pretraining\nphase, vi-mistral-x underwent a series of fine tun-\ning processes aimed at aligning the model with\nspecific NLP tasks. Through comprehensive test-\ning on various benchmarks, they have shown that\nvi-mistral-x has outperformd existing Vietnamese\nLLMs in several key areas.\nThis literature review presents the absence of\ndedicated Urdu LLMs highlights a significant need\nfor such resources. This paper introduces the first\ncontinual pretraining followed by fine tuning of\nLlama-3.1-8B-Instruct model for Urdu, leveraging\na dataset of 128 million tokens and using the LoRA\n(Hu et al., 2021) training approach for instruct tun-\ning. This pioneering effort aims to pave the way\nfor future research and development of more so-\nphisticated Urdu language models.\n3\nDataset Curation\nA pivitol challenge for building LLMs, particu-\nlarly in low resource languages, is the availability\nof sizeable high-quality data for building founda-\ntion LLMs. As the quality and diversity of data\nsignificantly influence the capabilities of LLMs\n(Chen et al., 2024a), we supplemented our in-house\ndataset with data from several publicly available\nsources, including CC-100 (Wenzek et al., 2020),\nthe Urdu corpus from OSCAR (Ortiz Suárez et al.,\n2020), the Urdu Web Corpus (Shafiq et al., 2020),\nUrdu data from XLSum (Hasan et al., 2021), and\n(Goldhahn et al., 2012). The raw text underwent\na comprehensive pre-processing pipeline, outlined\nin Section 4, to ensure language-specific content,\nmaintain quality, and remove duplicates. A sum-\nmary of the collected datasets and the impact of\nprocessing is provided in Table 1.\n\nTable 1: Summary of Token Count Reduction Across Different Data Sources for Urdu-LLaMA\nSource\nOriginal Token Count\nToken Count After Processing\nReduction\nPercentage Reduction (%)\nPublically Available Resources\n798,260,573\n541,151,638\n257,108,935\n32.20\nInhouse\n639,786,525\n606,053,446\n33,733,079\n5.30\nUrduLLaMA 1.0 Dataset\n1,438,047,098\n1,147,205,084\n290,842,014\n-\nUrduLLaMA 1.0 Dataset (in Billion)\n1.43\n1.14\n0.29\n-\n4\nPreprocessing Pipeline\nThis section outlines the pre-processing steps ap-\nplied to construct our dataset.\nWe primarily\nadopted approaches similar to those proposed by\n(Zeng et al., 2021), (Ennen et al., 2023), and (Lu\net al., 2024). The steps are as follows:\n• Language Filtering: This step was performed\nat the document level to retain only language-\nrich documents. Similar approaches have been\nadopted by others, such as the Falcon team\n(Penedo et al., 2023) for creating RefinedWeb,\nwhere they used the fastText language classifier\nfrom CCNet (Wenzek et al., 2020) at the docu-\nment level. This method has also been utilized by\n(Nguyen et al., 2024a) for building CulturaX and\nby (Laurençon et al., 2022) for constructing the\nBigScience ROOTS corpus. In our case, we con-\nducted language filtering using the CLE Urdu Lan-\nguage Identification API2, applying a threshold of\n0.9 to ensure the retention of predominantly Urdu\ndocuments. To validate our choice of the CLE\nUrdu Language Identification API, we conducted\nexperiments comparing it with fastText, assess-\ning the effectiveness of both in identifying Urdu\ncontent within documents containing varying pro-\nportions of Urdu and non-Urdu text. The results of\nwhich are summarized in Table 2, demonstrating\nthat the CLE Urdu Language Identification API\nprovided scores more aligned with the expected\ncomposition of test data.\n• Data Standardization: Data standardization in-\nvolves the normalization and transformation of\ntext data to make it more manageable and com-\nprehensible during the model training process (Lu\net al., 2024). Since syntax of the Urdu language\nrequires specialized techniques, we applied as de-\nscribed in (Nazir et al., 2024). Major steps include\nUnicode-based filtering, replacing non-standard\ncharacters with their standard forms, and handling\n2https://tech.cle.org.pk/api_langid\nUrdu-specific features such as end symbols, po-\netic symbols, and quotation marks. Additionally,\nsome documents had varying lengths, so we split\nthe text to maintain an average context length of\n512 tokens.\n• Quality Filtering: To enhance the dataset’s qual-\nity, motivated by the data processing pipeline\nfrom (Laurençon et al., 2022) and (Nguyen et al.,\n2024a), we utilized various dataset metrics to iden-\ntify and filter outlying documents. Filtering was\napplied based on stopword ratios, flagged word ra-\ntios, and empty documents. The threshold values\nfor the stopword ratio and flagged word ratio were\nset at 0.1 and 0.025, respectively, which align\nwith the threshold values used in the BigScience\nROOTS project (Laurençon et al., 2022).\nIn addition to filtering, we implemented Person-\nally Identifiable Information (PII) removal to pro-\ntect sensitive data. We employed rule-based ap-\nproach leveraging regular expressions regexes li-\nbrary to detect and remove sensitive information\nsuch as phone numbers, identification numbers,\nand email addresses. These measures ensured that\nthe dataset was free from personally identifiable\ninformation, enhancing privacy and usability for\nmodel training.\n• Deduplication:\nDespite thorough data cleaning, the remaining,\ndataset still contain a substantial amount of re-\npeated data due to various reasons, including in-\nformation being reposted on the web, multiple\nreferences to the same articles and plagiarism.\nThe duplicated data can thus cause memorization\nand significantly hinder generalization for LLMs\n(Lee et al., 2022). Therefore deduplication is re-\nquired as it decreases memorization of training\ndata (Kandpal et al., 2022). Initially, dedupli-\ncation was performed within individual datasets,\nfollowed by an overall deduplication across all\ndatasets to address potential similarities among\ndifferent sources. We applied deduplication at two\n\nTable 2: Language Identification Experiments with CLE Urdu Language Identification API and fastText\nFile Composition\nCLE\nUrdu\nLanguage\nIdentification API\nfastText Model Score\n80% Urdu, 20% non-urdu\n0.827\nlang: ur , prob: 0.991\n50% Urdu, 50% non-urdu\n0.503\nlang: ur , prob: 0.847\n25% Urdu, 75% non-urdu\n0.257\nlang: en , prob: 0.439\n100% Urdu\n1.000\nlang: ur , prob: 0.997\n100% Urdu (with urdu numerials)\n1.000\nlang: ur , prob: 0.994\nOnly Numericals\n0.000\nlang: ru , prob: 0.349\nNote: The \"lang\" represents the detected language, \"prob\" represents the probability score.\nThe language codes are as follows: \"ur\" = Urdu, \"en\" = English, \"ru\" = Russian.\nlevels with Table 3 summarizes the results of this\nprocess:\n1. Exact Document Deduplication: We applied\nthe SimHash technique, as used in the creation\nof WuDaoCorpora (Yuan et al., 2021) cor-\npus, Roots Corpus for BigScience’s BLOOMZ\nmodel (Abadji et al., 2022) and (Laurençon\net al., 2022), to perform deduplication. A hash\nwas generated from the content of each docu-\nment (ignoring spaces) to uniquely identify it.\nIf a duplicate hash was found, the correspond-\ning document was removed.\n2. Inside Document Deduplication:\nThe second step involved deduplicating indi-\nvidual lines within the documents. Following\nthe approach outlined by (Laurençon et al.,\n2022), we performed a line-by-line compari-\nson to identify and remove repeated content.\nDuplicate lines, regardless of their position\nwithin a document, were eliminated.\nTable 3: Impact of Deduplication on Dataset\nStep\nToken Count\nOriginal Dataset\n1.43 Billion\nAfter Processing\n1.14 Billion\nAfter Overall Deduplication 1.08 Billion\n5\nUrduLLaMA 1.0\nLlama-3.1-8B-Instruct,\nas\nintroduced\nin\n(Grattafiori et al., 2024) by Meta,\nis built\nupon an extensive pretraining corpus of 15 trillion\ntokens.\nWe leverage this model architecture\nfor continual pretraining due to its open-source\navailability and the inclusion of Urdu language\ndata in its training, making it a suitable choice\nfor our research.\nThe complete process of\nUrduLLaMA 1.0 making is illustrated in Figure\n1, follows four key stages: data collection, data\nprocessing, continual pretraining, and fine tuning,\neach playing a vital role in enhancing the model’s\nlinguistic understanding and task adaptability.\n5.1\nContinual Pretraining\nThe UrduLLaMA 1.0 model is trained on the\nCausal Language Modeling (CLM) task, enabling\nit to predict and generate the next word in a se-\nquence. This stage plays a crucial role in refin-\ning LLaMA’s proficiency in Urdu by allowing the\nmodel to grasp the language’s intricate syntactic\nstructures, semantic nuances, and unique linguistic\ntraits. Leveraging its autoregressive nature, CLM\nmirrors the human process of language comprehen-\nsion and generation, which is inherently context-\ndependent. Consequently, by the end of this ini-\ntial training phase, LLaMA acquires the ability to\ngenerate and interpret Urdu text with contextual\nrelevance and linguistic accuracy.\n5.1.1\nPretraining Dataset\nDue to hardware limitations, 128 million tokens\nwere used for continual pretraining of Llama-3.1-\n8B-Instruct (Grattafiori et al., 2024) from the cu-\nrated dataset as explained in Section 3.\n5.1.2\nPretraining Setup\nThe foundational model of UrduLLaMA 1.0 is ini-\ntialized with the original Llama-3.1-8B-Instruct\nweights and want through pretraining using the\nbf16 precision setting. Our pretraining strategy in-\nvolved full fine tuning, where all model parameters,\nincluding embeddings, LM heads, and attention\n\nweights, were trained. The training utilized an\nAdamW optimizer with a learning rate of 2e-5, and\ngradient accumulation steps of 1. Memory opti-\nmization techniques such as activation checkpoint-\ning, activation offloading, and memory-efficient\nFSDP wrapping were applied to manage the large\nmodel size effectively. Pretraining was conducted\nby utilizing 3 Nvidia L40 48GB GPUs. The model\nwas trained for 3 epochs on the dataset, with the\ntraining process spanning approximately 2 weeks.\nThe detailed parameters are described in Table 4.\nTable 4: Continual Pretraining Hyperparameters\nConfiguration\nValue\nBase Model\nLLaMA 3.1\nParameters\n8B\nTraining Tokens\n128 Million\nEpochs\n3\nBatch Size\n1\nInitial Learning Rate\n2e-4\nDropout Rate\n0.1\nMax Sequence Length\n512\nTraining Precision\nFP16\n5.2\nInstruct Tuning\nLanguage models pre-trained using the causal lan-\nguage modeling (CLM) objective often struggle\nto follow user instructions and sometimes gener-\nate irrelevant or unintended content (Balachandran,\n2023). This limitation arises because the CLM\nobjective is designed to predict the next token in\na sequence rather than understand or respond to\ninstructions effectively (Ouyang et al., 2022). To\naddress this issue and align the model’s behavior\nwith user intentions, we employed instruction fine\ntuning. This step refines the LLM’s capabilities,\nallowing it to interpret and execute task-specific\ninstructions more effectively in natural language.\nRather than the traditional approach of adapting\nto specific datasets, instruction fine tuning focuses\non a wide array of tasks articulated through lan-\nguage, ensuring the LLM’s adaptability without\ntask-specific alterations.\n5.2.1\nInstruct Tuning Dataset\nWe instruct tuned our model using 41,000 instruc-\ntions, sources from two main sources. The first\nsource included 26,000 instances from (Khalil,\n2024), a cleaned and translated version of the Stan-\nford Alpaca dataset (Ruebsamen, 2024). This re-\nfined dataset addresses key issues such as hallu-\ncination, responses, ambiguous instructions, and\nlow-quality samples, ensuring higher-quality train-\ning data. The second source comprises 15,000\ntranslated instances from the Dolly dataset (Saeed,\n2023), an Urdu translation of the original Dolly\ndataset (Conover et al., 2023), covering a wide\nrange of NLP tasks related instructions.\n5.2.2\nInstruct Tuning Setup\nFor the instruction fine tuning, we incorporated\nthe LoRA method (Hu et al., 2021), where we inte-\ngrated LoRA adapters into the attention vectors and\nsubsequently trained the embeddings, LM heads,\nand the newly incorporated LoRA parameters. For\nthe training infrastructure, we utilized Nvidia A100\n40GB GPU. The detailed hyperparameters used for\ninstructional fine tuning are listed in Table 5.\nTable 5: Fine Tuning Hyperparameters\nConfiguration\nValue\nTraining Data\n41,000\nEpoches\n3\nBatch Size\n1\nInitial Learning Rate\n2e-4\nDropout Rate\n0.1\nMax Sequence Length\n512\nLoRA Rank\n64\nLoRA Alpha\n128\nTraining Precision\nFP16\n5.3\nFine Tuning on Machine Translation Task\nfine tuning is performed to adapt the pre-trained\nmodel, which has already learned general patterns\nand representations to specific task. Instead of\nfocusing solely on specific linguistic pairs, fine tun-\ning on machine translation datasets equipped our\nUrduLLaMA 1.0 to understand and translate text\nin a more sophisticated way, addressing domain-\nspecific challenges.\nThis process ensured that\nthe UrduLLaMA 1.0 can effectively adapt to var-\nied translation tasks, such as handling language-\nspecific syntax, idiomatic expressions, and cultural\nnuances.\nWe fine tuned the model using an in-house Ma-\nchine Translation MT Corpus. This dataset is col-\n\nlected from various online sources covering diverse\ndomains such as Banking3, Law4, Weather5, Agri-\nculture6, and Food7. The dataset comprises 62,970\nentries, with a train-test split of 50,376 and 12,594\nrespectively. The model is fine tuned using the\ntraining split using parameters described in Table\n5.\n6\nEvaluation on Machine Translation\nTask\n6.1\nAutomatic Evaluation\nFor automatic evaluation, we tested the model on a\ntotal of 16,299 instances, comprising test splits\nfrom the inhouse MT corpus, TICO-19 (Anas-\ntasopoulos et al., 2020), and the Tatoeba Chal-\nlenge (Tiedemann, 2020). The TICO-19 dataset fo-\ncuses on COVID-19-related domains such as health\nand public awareness, while the Tatoeba Chal-\nlenge spans diverse domains for general-purpose\ntranslations. Specifically, the evaluation included\n12,594 instances from the in-house test set, 2,042\ninstances from TICO-19, and 1,663 instances from\nthe Tatoeba Challenge. This consistent test split\nwas designed to align with SOTA models for a fair\nand reliable comparison.\nTranslation quality was assessed using the BLEU\nscore 8, consistent with the metrics employed in\nSOTA model evaluation, providing an objective\nand standardized measure of the model’s perfor-\nmance on Urdu translations. The SOTA models\nevaluated are Llama-3.1-8B-Instruct (Grattafiori\net al., 2024), the base model used for continual pre-\ntraining; seamless-m4t-v2-large (Communication\net al., 2023), a unified multilingual and multimodal\ntranslation model; and opus-mt-en-ur (Tiedemann\nand Thottingal, 2020), a lightweight machine trans-\nlation system tailored for low-resource languages,\nincluding Urdu. The results, showcasing a detailed\ncomparison of translation quality across all models,\nare presented in Table 6.\nThe results of this experiment are also confined\nin Figure 2.\nThe results demonstrate that Ur-\nduLLaMA 1.0 significantly outperforms the base\n3https://www.sbp.org.pk/index.html\n4https://lgcd.punjab.gov.pk/\n5https://nwfc.pmd.gov.pk/new/\nweekly-outlook-en.php\n6https://aari.punjab.gov.pk/\n7https://shireenanwer.com/recipes/main-course/\nzafrani-koftay/\n8https://www.nltk.org/_modules/nltk/translate/\nbleu_score.html\nTable 6: Performance Comparison of Different Models\non Machine Translation Datasets\nModel\nIn-house\nTICO-19\nTatoeba Challenge\nUrduLLaMA 1.0\n28.01\n13.12\n15.16\nLlama-3.1-8B-Instruct\n10.87\n10.04\n12.49\nopus-mt-en-ur\n3.27\n5.65\n12.10\nseamless-m4t-v2-large\n17.44\n19.22\n22.76\nLlama-3.1-8B-Instruct model across all datasets,\nhighlighting the effectiveness of our approach in\nimproving translation performance for Urdu. On\nthe In-house dataset, UrduLLaMA 1.0 achieves the\nhighest BLEU score, indicating its superior abil-\nity to handle domain-specific data. This suggests\nthat fine tuning on Urdu text enhances the model’s\nability to capture language-specific nuances and\ncontextual meanings. In contrast, Llama-3.1-8B-\nInstruct shows lower performance, reinforcing that\ngeneral-purpose multilingual models require adap-\ntation for better Urdu translation. Interestingly,\nseamless-m4t-v2-large also performed well, though\nnot surpassing UrduLLaMA 1.0, while opus-mt-\nen-ur struggles significantly, likely due to limited\nexposure to the dataset’s domain.\nFigure 2: Automatic Evaluation of Different Models on\nMT Datasets\nFor the TICO-19 dataset, UrduLLaMA 1.0 again\nsurpassed Llama-3.1-8B-Instruct and opus-mt-en-\nur, demonstrating that fine tuning allows the model\nto generalize better to unseen test sets. However,\nseamless-m4t-v2-large achieved the highest BLEU\nscore, suggesting that massively multilingual pre-\ntrained models remain competitive on general pur-\npose translation tasks and also indicating that pre-\ntraining alone is insufficient for high-quality Urdu\ntranslations.\nIn the Tatoeba Challenge dataset, UrduLLaMA\n1.0 continues to outperform Llama-3.1-8B-Instruct\n\nand opus-mt-en-ur, demonstrating the impact of\ntargeted fine tuning. However, seamless-m4t-v2-\nlarge achieves the best performance, likely due to\nits extensive multilingual training data. OpusMT\nshows improvement compared to the MT corpus\ndataset but remained behind UrduLLaMA 1.0 and\nseamless-m4t-v2-large, emphasizing the advantage\nof fine tuning over purely pretrained models.\nThese results highlight the importance of adapt-\ning general-purpose LLMs for specific languages.\nFine tuning Llama-3.1-8B-Instruct on Urdu data\nhas proven to be an effective approach, signifi-\ncantly improving translation quality over the base\nmodel. While seamless-m4t-v2-large remains a\nstrong competitor due to its broad multilingual ca-\npabilities, UrduLLaMA 1.0’s strong performance\non in-domain data suggests that domain-specific\nfine tuning is crucial for optimal results.\n6.2\nHuman Evaluation\nFor the human evaluation of MT, we conducted\na blind review. Two native Urdu linguists partici-\npated in this evaluation, where each was presented\nwith an English source sentence along with transla-\ntions from the four models, without any indication\nof model identity. They were instructed to select\nthe translation they found most accurate and natu-\nral. A total of 300 test sentences were used with\n100 sentences per dataset were randomly selected\nfor manual evaluation. The preferences of both\nlinguists were then aggregated to compare model\nperformance. This evaluation provides valuable\ninsight into the comparative quality of Urdu MT,\nreflecting the preferences of native Urdu experts in\nassessing translation accuracy and fluency summa-\nrized in Table 7 and interpreted in Figure 3. The\ndifference in trends between human and automatic\nevaluation is due to the larger amount of testing\ndata used in automatic evaluation.\nTable 7: Human Evaluation of Different Models on\nMachine Translation Datasets\nModel\nMT Corpus\nTICO-19\nTatoeba Challenge\nUrduLLaMA 1.0\n23\n25.5\n24.5\nLlama-3.1-8B-Instruct\n51\n9.5\n3\nopus-mt-en-ur\n1\n7\n10.5\nseamless-m4t-v2-large\n25\n58\n62\nThe results show that seamless-m4t-v2-large pro-\nvided better translations overall, and it is because\nof its specialized nature for translation and large\namount of multilingual training data. However,\nFigure 3: Human Evaluation of Different Models on\nMT Datasets\nUrduLLaMA 1.0, while a more general model,\nshowed improvements over its base version in\nTICO-19 and Tatoeba Challenge. This progress\nis promising, as it suggests that with further refine-\nment and more diverse training data, UrduLLaMA\n1.0 has the potential to rival specialized models.\nThe observed improvements in UrduLLaMA 1.0\nare encouraging and highlight its adaptability and\npotential for high-quality translation in broader con-\ntexts.\n7\nConclusion\nIn this paper, we introduced UrduLLaMA 1.0, a\nmodel specifically tailored for the Urdu language.\nWe presented a comprehensive data processing\npipeline to curate and prepare high-quality train-\ning data, addressing the challenge of limited pub-\nlicly available Urdu datasets. UrduLLaMA 1.0 was\ncontinuoly pre-trained on a portion of this dataset\nusing the Llama-3.1-8B-Instruct architecture, fol-\nlowed by instruction tuning to enable the model\nto understand and generate responses in a natural\nconversational format. This fine tuning leveraged a\ncombination of the Alpaca and Dolly datasets. Fur-\nthermore, we performed fine tuning on a machine\ntranslation dataset to enhance the model’s trans-\nlation capabilities. Our evaluation results demon-\nstrate that UrduLLaMA 1.0 outperforms its base\nmodel, exhibiting substantial improvements in ma-\nchine translation tasks for Urdu. This work repre-\nsents a significant step toward advancing the per-\nformance of LLMs for low-resource languages like\nUrdu and sets a new benchmark for future research\nin this domain.\n\nLimitations\nOur model was trained on a limited portion of the\nUrdu dataset due to computational and cost con-\nstraints. As a result, it exhibits gaps in knowledge,\nparticularly in capturing the nuances of Urdu cul-\nture and literature. While this version serves as\na foundational step, its full potential can only be\nunlocked with access to a more extensive dataset\nto enhance its contextual understanding.\nAdditionally, detoxification processes were not\nincorporated during training, leaving the model un-\ncensored and potentially prone to generating harm-\nful or offensive content, which requires caution\nduring deployment.\nEvaluating LLMs also presents a significant chal-\nlenge, especially for underrepresented languages\nlike Urdu, due to the lack of standardized bench-\nmarks outside the European linguistic domain. Al-\nthough this paper introduces a tailored evaluation\napproach for Urdu machine translation, it remains\nlimited in scope and does not comprehensively as-\nsess the model’s performance across diverse appli-\ncations.\nEthics Statement\nThis research utilizes publicly available, open-\nsource datasets that do not contain personal or iden-\ntifiable information, ensuring no associated risks.\nAll work and ideas presented are original, with AI\nmodels used solely for grammatical correction and\nwriting enhancement. Proper citations have been\nmade for all models and datasets used. Moreover\nas a generative model, it retains the potential to\ngenerate harmful or offensive content if prompted\ninappropriately, underscoring the need for responsi-\nble usage and careful oversight during deployment.\nReferences\nJulien Abadji, Pedro Ortiz Suarez, Laurent Romary, and\nBenoît Sagot. 2022. Towards a cleaner document-\noriented multilingual crawled corpus. In Proceedings\nof the Thirteenth Language Resources and Evalua-\ntion Conference, pages 4344–4355, Marseille, France.\nEuropean Language Resources Association.\nMohammad Amin Abbasi, Arash Ghafouri, Mahdi\nFirouzmandi, Hassan Naderi, and Behrouz Minaei\nBidgoli. 2023. Persianllama: Towards building first\npersian large language model.\nAntonios Anastasopoulos, Alessandro Cattelan, Zi-\nYi Dou, Marcello Federico, Christian Federmann,\nDmitriy Genzel, Franscisco Guzmán, Junjie Hu, Mac-\nduff Hughes, Philipp Koehn, Rosie Lazar, Will Lewis,\nGraham Neubig, Mengmeng Niu, Alp Öktem, Eric\nPaquin, Grace Tang, and Sylwia Tur. 2020. TICO-19:\nthe translation initiative for COvid-19. In Proceed-\nings of the 1st Workshop on NLP for COVID-19 (Part\n2) at EMNLP 2020, Online. Association for Compu-\ntational Linguistics.\nSamee Arif, Abdul Hameed Azeemi, Agha Ali Raza,\nand Awais Athar. 2024. Generalists vs. specialists:\nEvaluating large language models for Urdu. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2024, pages 7263–7280, Miami, Florida,\nUSA. Association for Computational Linguistics.\nAbhinand Balachandran. 2023. Tamil-llama: A new\ntamil language model based on llama 2.\nSidney Black, Stella Biderman, Eric Hallahan, Quentin\nAnthony, Leo Gao, Laurence Golding, Horace\nHe, Connor Leahy, Kyle McDonell, Jason Phang,\nMichael Pieler, Usvsn Sai Prashanth, Shivanshu Puro-\nhit, Laria Reynolds, Jonathan Tow, Ben Wang, and\nSamuel Weinbach. 2022. GPT-NeoX-20B: An open-\nsource autoregressive language model. In Proceed-\nings of BigScience Episode #5 – Workshop on Chal-\nlenges & Perspectives in Creating Large Language\nModels, pages 95–136, virtual+Dublin. Association\nfor Computational Linguistics.\nHao Chen, Abdul Waheed, Xiang Li, Yidong Wang,\nJindong Wang, Bhiksha Raj, and Marah I. Abdin.\n2024a. On the diversity of synthetic data and its\nimpact on training large language models.\nJie Chen, Zhipeng Chen, Jiapeng Wang, Kun Zhou, Yu-\ntao Zhu, Jinhao Jiang, Yingqian Min, Wayne Xin\nZhao, Zhicheng Dou, Jiaxin Mao, Yankai Lin, Rui-\nhua Song, Jun Xu, Xu Chen, Rui Yan, Zhewei Wei,\nDi Hu, Wenbing Huang, and Ji-Rong Wen. 2024b.\nTowards effective and efficient continual pre-training\nof large language models.\nSeamless Communication, Loïc Barrault, Yu-An Chung,\nMariano Cora Meglioli, David Dale, Ning Dong,\nPaul-Ambroise Duquenne, Hady Elsahar, Hongyu\nGong, Kevin Heffernan, John Hoffman, Christopher\nKlaiber, Pengwei Li, Daniel Licht, Jean Maillard,\nAlice Rakotoarison, Kaushik Ram Sadagopan, Guil-\nlaume Wenzek, Ethan Ye, Bapi Akula, Peng-Jen\nChen, Naji El Hachem, Brian Ellis, Gabriel Mejia\nGonzalez, Justin Haaheim, Prangthip Hansanti, Russ\nHowes, Bernie Huang, Min-Jae Hwang, Hirofumi In-\naguma, Somya Jain, Elahe Kalbassi, Amanda Kallet,\nIlia Kulikov, Janice Lam, Daniel Li, Xutai Ma, Rus-\nlan Mavlyutov, Benjamin Peloquin, Mohamed Ra-\nmadan, Abinesh Ramakrishnan, Anna Sun, Kevin\nTran, Tuan Tran, Igor Tufanov, Vish Vogeti, Carleigh\nWood, Yilin Yang, Bokai Yu, Pierre Andrews, Can\nBalioglu, Marta R. Costa-jussà, Onur Celebi, Maha\nElbayad, Cynthia Gao, Francisco Guzmán, Justine\nKao, Ann Lee, Alexandre Mourachko, Juan Pino,\nSravya Popuri, Christophe Ropers, Safiyyah Saleem,\nHolger Schwenk, Paden Tomasello, Changhan Wang,\n\nJeff Wang, and Skyler Wang. 2023. Seamlessm4t:\nMassively multilingual & multimodal machine trans-\nlation.\nMike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie,\nJun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell,\nMatei Zaharia, and Reynold Xin. 2023. Free dolly:\nIntroducing the world’s first truly open instruction-\ntuned llm.\nYiming Cui, Ziqing Yang, and Xin Yao. 2024. Efficient\nand effective text encoding for chinese llama and\nalpaca.\nTri Dao. 2023. Flashattention-2: Faster attention with\nbetter parallelism and work partitioning.\nPhilipp Ennen, Po-Chun Hsu, Chan-Jan Hsu, Chang-Le\nLiu, Yen-Chen Wu, Yin-Hsiang Liao, Chin-Tung Lin,\nDa-Shan Shiu, and Wei-Yun Ma. 2023. Extending\nthe pre-training of bloom for improved support of\ntraditional chinese: Models, methods and results.\nJulen Etxaniz, Oscar Sainz, Naiara Perez, Itziar Ald-\nabe, German Rigau, Eneko Agirre, Aitor Ormazabal,\nMikel Artetxe, and Aitor Soroa. 2024. Latxa: An\nopen language model and evaluation suite for basque.\nJay Gala, Thanmay Jayakumar, Jaavid Aktar Husain,\nAswanth Kumar M, Mohammed Safi Ur Rahman\nKhan, Diptesh Kanojia, Ratish Puduppully, Mitesh M.\nKhapra, Raj Dabre, Rudra Murthy, and Anoop\nKunchukuttan. 2024. Airavata: Introducing hindi\ninstruction-tuned llm.\nDirk Goldhahn, Thomas Eckart, and Uwe Quasthoff.\n2012. Building large monolingual dictionaries at the\nLeipzig corpora collection: From 100 to 200 lan-\nguages. In Proceedings of the Eighth International\nConference on Language Resources and Evaluation\n(LREC‘12), pages 759–765, Istanbul, Turkey. Euro-\npean Language Resources Association (ELRA).\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,\nAbhinav Pandey, Abhishek Kadian, Ahmad Al-\nDahle, Aiesha Letman, Akhil Mathur, Alan Schel-\nten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh\nGoyal, Anthony Hartshorn, Aobo Yang, Archi Mi-\ntra, Archie Sravankumar, Artem Korenev, Arthur\nHinsvark, Arun Rao, Aston Zhang, Aurelien Ro-\ndriguez, Austen Gregerson, Ava Spataru, Baptiste\nRoziere, Bethany Biron, Binh Tang, Bobbie Chern,\nCharlotte Caucheteux, Chaya Nayak, Chloe Bi,\nChris Marra, Chris McConnell, Christian Keller,\nChristophe Touret, Chunyang Wu, Corinne Wong,\nCristian Canton Ferrer, Cyrus Nikolaidis, Damien Al-\nlonsius, Daniel Song, Danielle Pintz, Danny Livshits,\nDanny Wyatt, David Esiobu, Dhruv Choudhary,\nDhruv Mahajan, Diego Garcia-Olano, Diego Perino,\nDieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy,\nElina Lobanova, Emily Dinan, Eric Michael Smith,\nFilip Radenovic, Francisco Guzmán, Frank Zhang,\nGabriel Synnaeve, Gabrielle Lee, Georgia Lewis An-\nderson, Govind Thattai, Graeme Nail, Gregoire Mi-\nalon, Guan Pang, Guillem Cucurell, Hailey Nguyen,\nHannah Korevaar, Hu Xu, Hugo Touvron, Iliyan\nZarov, Imanol Arrieta Ibarra, Isabel Kloumann, Is-\nhan Misra, Ivan Evtimov, Jack Zhang, Jade Copet,\nJaewon Lee, Jan Geffert, Jana Vranes, Jason Park,\nJay Mahadeokar, Jeet Shah, Jelmer van der Linde,\nJennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu,\nJianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang,\nJiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park,\nJoseph Rocca, Joshua Johnstun, Joshua Saxe, Jun-\nteng Jia, Kalyan Vasuden Alwala, Karthik Prasad,\nKartikeya Upasani, Kate Plawiak, Ke Li, Kenneth\nHeafield, Kevin Stone, Khalid El-Arini, Krithika Iyer,\nKshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal\nLakhotia, Lauren Rantala-Yeary, Laurens van der\nMaaten, Lawrence Chen, Liang Tan, Liz Jenkins,\nLouis Martin, Lovish Madaan, Lubo Malo, Lukas\nBlecher, Lukas Landzaat, Luke de Oliveira, Madeline\nMuzzi, Mahesh Pasupuleti, Mannat Singh, Manohar\nPaluri, Marcin Kardas, Maria Tsimpoukelli, Mathew\nOldham, Mathieu Rita, Maya Pavlova, Melanie Kam-\nbadur, Mike Lewis, Min Si, Mitesh Kumar Singh,\nMona Hassan, Naman Goyal, Narjes Torabi, Niko-\nlay Bashlykov, Nikolay Bogoychev, Niladri Chatterji,\nNing Zhang, Olivier Duchenne, Onur Çelebi, Patrick\nAlrassy, Pengchuan Zhang, Pengwei Li, Petar Va-\nsic, Peter Weng, Prajjwal Bhargava, Pratik Dubal,\nPraveen Krishnan, Punit Singh Koura, Puxin Xu,\nQing He, Qingxiao Dong, Ragavan Srinivasan, Raj\nGanapathy, Ramon Calderer, Ricardo Silveira Cabral,\nRobert Stojnic, Roberta Raileanu, Rohan Maheswari,\nRohit Girdhar, Rohit Patel, Romain Sauvestre, Ron-\nnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan\nSilva, Rui Hou, Rui Wang, Saghar Hosseini, Sa-\nhana Chennabasappa, Sanjay Singh, Sean Bell, Seo-\nhyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sha-\nran Narang, Sharath Raparthy, Sheng Shen, Shengye\nWan, Shruti Bhosale, Shun Zhang, Simon Van-\ndenhende, Soumya Batra, Spencer Whitman, Sten\nSootla, Stephane Collot, Suchin Gururangan, Syd-\nney Borodinsky, Tamar Herman, Tara Fowler, Tarek\nSheasha, Thomas Georgiou, Thomas Scialom, Tobias\nSpeckbacher, Todor Mihaylov, Tong Xiao, Ujjwal\nKarn, Vedanuj Goswami, Vibhor Gupta, Vignesh\nRamanathan, Viktor Kerkez, Vincent Gonguet, Vir-\nginie Do, Vish Vogeti, Vítor Albiero, Vladan Petro-\nvic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whit-\nney Meers, Xavier Martinet, Xiaodong Wang, Xi-\naofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xin-\nfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Gold-\nschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen,\nYiwen Song, Yuchen Zhang, Yue Li, Yuning Mao,\nZacharie Delpierre Coudert, Zheng Yan, Zhengxing\nChen, Zoe Papakipos, Aaditya Singh, Aayushi Sri-\nvastava, Abha Jain, Adam Kelsey, Adam Shajnfeld,\nAdithya Gangidi, Adolfo Victoria, Ahuva Goldstand,\nAjay Menon, Ajay Sharma, Alex Boesenberg, Alexei\nBaevski, Allie Feinstein, Amanda Kallet, Amit San-\ngani, Amos Teo, Anam Yunus, Andrei Lupu, An-\ndres Alvarado, Andrew Caples, Andrew Gu, Andrew\nHo, Andrew Poulton, Andrew Ryan, Ankit Ramchan-\ndani, Annie Dong, Annie Franco, Anuj Goyal, Apara-\njita Saraf, Arkabandhu Chowdhury, Ashley Gabriel,\nAshwin Bharambe, Assaf Eisenman, Azadeh Yaz-\n\ndan, Beau James, Ben Maurer, Benjamin Leonhardi,\nBernie Huang, Beth Loyd, Beto De Paola, Bhargavi\nParanjape, Bing Liu, Bo Wu, Boyu Ni, Braden Han-\ncock, Bram Wasti, Brandon Spence, Brani Stojkovic,\nBrian Gamido, Britt Montalvo, Carl Parker, Carly\nBurton, Catalina Mejia, Ce Liu, Changhan Wang,\nChangkyu Kim, Chao Zhou, Chester Hu, Ching-\nHsiang Chu, Chris Cai, Chris Tindal, Christoph Fe-\nichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty,\nDaniel Kreymer, Daniel Li, David Adkins, David\nXu, Davide Testuggine, Delia David, Devi Parikh,\nDiana Liskovich, Didem Foss, Dingkang Wang, Duc\nLe, Dustin Holland, Edward Dowling, Eissa Jamil,\nElaine Montgomery, Eleonora Presani, Emily Hahn,\nEmily Wood, Eric-Tuan Le, Erik Brinkman, Este-\nban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun,\nFelix Kreuk, Feng Tian, Filippos Kokkinos, Firat\nOzgenel, Francesco Caggioni, Frank Kanayet, Frank\nSeide, Gabriela Medina Florez, Gabriella Schwarz,\nGada Badeer, Georgia Swee, Gil Halpern, Grant\nHerman, Grigory Sizov, Guangyi, Zhang, Guna\nLakshminarayanan, Hakan Inan, Hamid Shojanaz-\neri, Han Zou, Hannah Wang, Hanwen Zha, Haroun\nHabeeb, Harrison Rudolph, Helen Suk, Henry As-\npegren, Hunter Goldman, Hongyuan Zhan, Ibrahim\nDamlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis,\nIrina-Elena Veliche, Itai Gat, Jake Weissman, James\nGeboski, James Kohli, Janice Lam, Japhet Asher,\nJean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jen-\nnifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy\nTeboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe\nCummings, Jon Carvill, Jon Shepard, Jonathan Mc-\nPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang,\nKai Wu, Kam Hou U, Karan Saxena, Kartikay Khan-\ndelwal, Katayoun Zand, Kathy Matosich, Kaushik\nVeeraraghavan, Kelly Michelena, Keqian Li, Ki-\nran Jagadeesh, Kun Huang, Kunal Chawla, Kyle\nHuang, Lailin Chen, Lakshya Garg, Lavender A,\nLeandro Silva, Lee Bell, Lei Zhang, Liangpeng\nGuo, Licheng Yu, Liron Moshkovich, Luca Wehrst-\nedt, Madian Khabsa, Manav Avalani, Manish Bhatt,\nMartynas Mankus, Matan Hasson, Matthew Lennie,\nMatthias Reso, Maxim Groshev, Maxim Naumov,\nMaya Lathi, Meghan Keneally, Miao Liu, Michael L.\nSeltzer, Michal Valko, Michelle Restrepo, Mihir Pa-\ntel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark,\nMike Macey, Mike Wang, Miquel Jubert Hermoso,\nMo Metanat, Mohammad Rastegari, Munish Bansal,\nNandhini Santhanam, Natascha Parks, Natasha\nWhite, Navyata Bawa, Nayan Singhal, Nick Egebo,\nNicolas Usunier, Nikhil Mehta, Nikolay Pavlovich\nLaptev, Ning Dong, Norman Cheng, Oleg Chernoguz,\nOlivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin\nKent, Parth Parekh, Paul Saab, Pavan Balaji, Pe-\ndro Rittner, Philip Bontrager, Pierre Roux, Piotr\nDollar, Polina Zvyagina, Prashant Ratanchandani,\nPritish Yuvraj, Qian Liang, Rachad Alao, Rachel\nRodriguez, Rafi Ayub, Raghotham Murthy, Raghu\nNayani, Rahul Mitra, Rangaprabhu Parthasarathy,\nRaymond Li, Rebekkah Hogan, Robin Battey, Rocky\nWang, Russ Howes, Ruty Rinott, Sachin Mehta,\nSachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara\nChugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov,\nSatadru Pan, Saurabh Mahajan, Saurabh Verma,\nSeiji Yamamoto, Sharadh Ramaswamy, Shaun Lind-\nsay, Shaun Lindsay, Sheng Feng, Shenghao Lin,\nShengxin Cindy Zha, Shishir Patil, Shiva Shankar,\nShuqiang Zhang, Shuqiang Zhang, Sinong Wang,\nSneha Agarwal, Soji Sajuyigbe, Soumith Chintala,\nStephanie Max, Stephen Chen, Steve Kehoe, Steve\nSatterfield, Sudarshan Govindaprasad, Sumit Gupta,\nSummer Deng, Sungmin Cho, Sunny Virk, Suraj\nSubramanian, Sy Choudhury, Sydney Goldman, Tal\nRemez, Tamar Glaser, Tamara Best, Thilo Koehler,\nThomas Robinson, Tianhe Li, Tianjun Zhang, Tim\nMatthews, Timothy Chou, Tzook Shaked, Varun\nVontimitta, Victoria Ajayi, Victoria Montanez, Vijai\nMohan, Vinay Satish Kumar, Vishal Mangla, Vlad\nIonescu, Vlad Poenaru, Vlad Tiberiu Mihailescu,\nVladimir Ivanov, Wei Li, Wenchen Wang, Wen-\nwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng\nTang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo\nGao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia,\nYe Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi,\nYoungjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao,\nYundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary\nDeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang,\nZhiwei Zhao, and Zhiyu Ma. 2024. The llama 3 herd\nof models.\nTahmid Hasan, Abhik Bhattacharjee, Md Saiful Islam,\nKazi Samin, Yuan-Fang Li, Yong-Bin Kang, M. So-\nhel Rahman, and Rifat Shahriyar. 2021. Xl-sum:\nLarge-scale multilingual abstractive summarization\nfor 44 languages.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2021. Lora: Low-rank adaptation of\nlarge language models.\nShengchao Hu, Li Shen, Ya Zhang, Yixin Chen, and\nDacheng Tao. 2023. On transforming reinforcement\nlearning by transformer: The development trajectory.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, Lélio Renard Lavaud,\nMarie-Anne Lachaux, Pierre Stock, Teven Le Scao,\nThibaut Lavril, Thomas Wang, Timothée Lacroix,\nand William El Sayed. 2023. Mistral 7b.\nNikhil Kandpal, Eric Wallace, and Colin Raffel. 2022.\nDeduplicating training data mitigates privacy risks in\nlanguage models.\nMahwiz Khalil. 2024. Urdu alpaca filtered dataset. Ac-\ncessed: 2025-02-07.\nHugo Laurençon, Lucile Saulnier, Thomas Wang,\nChristopher Akiki, Albert Villanova del Moral,\nTeven Le Scao, Leandro Von Werra, Chenghao\nMou, Eduardo González Ponferrada, Huu Nguyen,\nJörg Frohberg, Mario Šaško, Quentin Lhoest, An-\ngelina McMillan-Major, Gerard Dupont, Stella Bi-\nderman, Anna Rogers, Loubna Ben allal, Francesco\nDe Toni, Giada Pistilli, Olivier Nguyen, Somaieh\n\nNikpoor, Maraim Masoud, Pierre Colombo, Javier\nde la Rosa, Paulo Villegas, Tristan Thrush, Shayne\nLongpre, Sebastian Nagel, Leon Weber, Manuel\nMuñoz, Jian Zhu, Daniel Van Strien, Zaid Alyafeai,\nKhalid Almubarak, Minh Chien Vu, Itziar Gonzalez-\nDios, Aitor Soroa, Kyle Lo, Manan Dey, Pedro Or-\ntiz Suarez, Aaron Gokaslan, Shamik Bose, David\nAdelani, Long Phan, Hieu Tran, Ian Yu, Suhas Pai,\nJenny Chim, Violette Lepercq, Suzana Ilic, Margaret\nMitchell, Sasha Alexandra Luccioni, and Yacine Jer-\nnite. 2022. The bigscience roots corpus: A 1.6tb com-\nposite multilingual dataset. In Advances in Neural\nInformation Processing Systems, volume 35, pages\n31809–31826. Curran Associates, Inc.\nKatherine Lee, Daphne Ippolito, Andrew Nystrom,\nChiyuan Zhang, Douglas Eck, Chris Callison-Burch,\nand Nicholas Carlini. 2022. Deduplicating training\ndata makes language models better. In Proceedings\nof the Machine Learning Research (PMLR), pages\n8424–8445, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nW. Lian, B. Goodson, E. Pentland, A. Cook, C. Vong,\nand Teknium. 2023. Openorca: An open dataset of\ngpt-augmented flan reasoning traces.\nYen-Ting Lin and Yun-Nung Chen. 2023.\nTaiwan\nllm: Bridging the linguistic divide with a culturally\naligned language model.\nYuting Lu, Chao Sun, Yuchao Yan, Hegong Zhu, Dong-\ndong Song, Qing Peng, Li Yu, Xiaozheng Wang, Jian\nJiang, and Xiaolong Ye. 2024. A comprehensive sur-\nvey of datasets for large language model evaluation.\nIn 2024 5th Information Communication Technolo-\ngies Conference (ICTC), pages 330–336.\nShahzad Nazir, Muhammad Asif, Mariam Rehman, and\nShahbaz Ahmad. 2024.\nMachine learning based\nframework for fine-grained word segmentation and\nenhanced text normalization for low resourced lan-\nguage. PeerJ Computer Science, 10:e1704.\nQuan Nguyen, Huy Pham, and Dung Dao. 2023. Vinal-\nlama: Llama-based vietnamese foundation model.\nThuat Nguyen, Chien Van Nguyen, Viet Dac Lai,\nHieu Man, Nghia Trung Ngo, Franck Dernoncourt,\nRyan A. Rossi, and Thien Huu Nguyen. 2024a. Cul-\nturaX: A cleaned, enormous, and multilingual dataset\nfor large language models in 167 languages. In Pro-\nceedings of the 2024 Joint International Conference\non Computational Linguistics, Language Resources\nand Evaluation (LREC-COLING 2024), pages 4226–\n4237, Torino, Italia. ELRA and ICCL.\nXuan-Phi Nguyen, Wenxuan Zhang, Xin Li, Mahani\nAljunied, Zhiqiang Hu, Chenhui Shen, Yew Ken\nChia, Xingxuan Li, Jianyu Wang, Qingyu Tan, Liy-\ning Cheng, Guanzheng Chen, Yue Deng, Sen Yang,\nChaoqun Liu, Hang Zhang, and Lidong Bing. 2024b.\nSeaLLMs - large language models for Southeast Asia.\nIn Proceedings of the 62nd Annual Meeting of the\nAssociation for Computational Linguistics (Volume 3:\nSystem Demonstrations), pages 294–304, Bangkok,\nThailand. Association for Computational Linguistics.\nOpenAI. 2022. Introducing chatgpt. https://openai.\ncom/blog/chatgpt.\nPedro Javier Ortiz Suárez, Laurent Romary, and Benoît\nSagot. 2020. A monolingual approach to contextual-\nized word embeddings for mid-resource languages.\nIn Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 1703–\n1714, Online. Association for Computational Linguis-\ntics.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow,\nRuxandra Cojocaru, Alessandro Cappelli, Hamza\nAlobeidli, Baptiste Pannier, Ebtesam Almazrouei,\nand Julien Launay. 2023. The refinedweb dataset for\nfalcon llm: Outperforming curated corpora with web\ndata, and web data only.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and\nYuxiong He. 2020. Zero: Memory optimizations to-\nward training trillion parameter models. In Proceed-\nings of the IEEE Conference on High Performance\nComputing, Networking, Storage, and Analysis.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase,\nand Yuxiong He. 2020. Deepspeed: System opti-\nmizations enable training deep learning models with\nover 100 billion parameters. In Proceedings of the\n26th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining, KDD ’20,\npage 3505–3506, New York, NY, USA. Association\nfor Computing Machinery.\nGene Ruebsamen. 2024. Alpacadatacleaned. Accessed:\n2025-02-07.\nAaqib Saeed. 2023.\nDatabricks dolly 15k urdu\ndataset.\nhttps://huggingface.co/datasets/\naaqibsaeed/databricks-dolly-15k-ur.\nAc-\ncessed: 2025-02-07.\nSarvamAI. 2023. Openhathi series: An approach to\nbuild bilingual llms frugally. https://www.sarvam.\nai/blogs/openhathi-series.\nHafiz Muhammad Shafiq, Bilal Tahir, and Muham-\nmad Amir Mehmood. 2020. Towards building a urdu\nlanguage corpus using common crawl. Journal of\nIntelligent & Fuzzy Systems, 39(2):2445–2455.\nMunief Hassan Tahir, Sana Shams, Layba Fiaz, Farah\nAdeeba, and Sarmad Hussain. 2025. Benchmark-\ning the performance of pre-trained llms across urdu\nnlp tasks. In Proceedings of the First Workshop on\n\nChallenges in Processing South Asian Languages\n(CHiPSAL 2025), pages 17–34, Lahore, Pakistan. In-\nternational Committee on Computational Linguistics.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yan-\nnick Dubois, Xi Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023.\nStanford al-\npaca: An instruction-following llama model. https:\n//github.com/tatsu-lab/stanford_alpaca.\nJörg Tiedemann. 2020. The tatoeba translation chal-\nlenge – realistic data sets for low resource and multi-\nlingual MT. In Proceedings of the Fifth Conference\non Machine Translation, pages 1174–1182, Online.\nAssociation for Computational Linguistics.\nJörg Tiedemann and Santhosh Thottingal. 2020. OPUS-\nMT – building open translation services for the world.\nIn Proceedings of the 22nd Annual Conference of\nthe European Association for Machine Translation,\npages 479–480, Lisboa, Portugal. European Associa-\ntion for Machine Translation.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023. Llama 2: Open foundation and fine-\ntuned chat models.\nJames Vo. 2024.\nVi-mistral-x:\nBuilding a viet-\nnamese language model with advanced continual pre-\ntraining.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzmán, Ar-\nmand Joulin, and Edouard Grave. 2020.\nCCNet:\nExtracting high quality monolingual datasets from\nweb crawl data. In Proceedings of the Twelfth Lan-\nguage Resources and Evaluation Conference, pages\n4003–4012, Marseille, France. European Language\nResources Association.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nSha Yuan, Hanyu Zhao, Zhengxiao Du, Ming Ding,\nXiao Liu, Yukuo Cen, Xu Zou, Zhilin Yang, and\nJie Tang. 2021. Wudaocorpora: A super large-scale\nchinese corpora for pre-training language models. AI\nOpen, 2:65–68.\nWei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi Liao,\nZhiwei Wang, Xin Jiang, ZhenZhang Yang, Kaisheng\nWang, Xiaoda Zhang, Chen Li, Ziyan Gong, Yi-\nfan Yao, Xinjing Huang, Jun Wang, Jianfeng Yu,\nQi Guo, Yue Yu, Yan Zhang, Jin Wang, Hengtao\nTao, Dasen Yan, Zexuan Yi, Fang Peng, Fangqing\nJiang, Han Zhang, Lingfeng Deng, Yehong Zhang,\nZhe Lin, Chao Zhang, Shaojie Zhang, Mingyue Guo,\nShanzhi Gu, Gaojun Fan, Yaowei Wang, Xuefeng\nJin, Qun Liu, and Yonghong Tian. 2021. Pangu-\nalpha: Large-scale autoregressive pretrained chinese\nlanguage models with auto-parallel computation.\nPeiyuan Zhang, Guangtao Zeng, Tianduo Wang, and\nWei Lu. 2024. Tinyllama: An open-source small\nlanguage model.\nZhe Zhao, Yudong Li, Cheng Hou, Jing Zhao, Rong\nTian, Weijie Liu, Yiren Chen, Ningyuan Sun,\nHaoyan Liu, Weiquan Mao, Han Guo, Weigang Gou,\nTaiqiang Wu, Tao Zhu, Wenhang Shi, Chen Chen,\nShan Huang, Sihong Chen, Liqun Liu, Feifei Li, Xi-\naoshuai Chen, Xingwu Sun, Zhanhui Kang, Xiaoy-\nong Du, Linlin Shen, and Kimmo Yan. 2023. Ten-\ncentPretrain: A scalable and flexible toolkit for pre-\ntraining models of different modalities. In Proceed-\nings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 3: System\nDemonstrations), pages 217–225, Toronto, Canada.\nAssociation for Computational Linguistics.\n",
  "metadata": {
    "source_path": "papers/arxiv/UrduLLaMA_10_Dataset_Curation_Preprocessing_and_Evaluation_in\n__Low-Resource_Settings_5bd618c445fddaf0.pdf",
    "content_hash": "5bd618c445fddaf0e5a4b76aa01b682f193a33d414d0b2c2e8ce427bd68f3a48",
    "arxiv_id": null,
    "title": "UrduLLaMA_10_Dataset_Curation_Preprocessing_and_Evaluation_in\n__Low-Resource_Settings_5bd618c445fddaf0",
    "author": "",
    "creation_date": "D:20250225023321Z",
    "published": "2025-02-25T02:33:21",
    "pages": 13,
    "size": 359809,
    "file_mtime": 1740470202.1562548
  }
}