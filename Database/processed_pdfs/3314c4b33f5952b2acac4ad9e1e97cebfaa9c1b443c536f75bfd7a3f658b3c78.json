{
  "text": "MANUSCRIPT SUBMITTED TO IEEE FOR POSSIBLE PUBLICATION\n1\nDiffusion Models for Tabular Data:\nChallenges, Current Progress, and Future Directions\nZhong Li\n, Qi Huang*\n, Lincen Yang*\n, Jiayang Shi\n, Zhao Yang\n,\nNiki van Stein\n(IEEE Member), Thomas B¨ack\n(IEEE Fellow), Matthijs van Leeuwen\nAbstract—In recent years, generative models have achieved\nremarkable performance across diverse applications, including\nimage generation, text synthesis, audio creation, video gener-\nation, and data augmentation. Diffusion models have emerged\nas superior alternatives to Generative Adversarial Networks\n(GANs) and Variational Autoencoders (VAEs) by addressing\ntheir limitations, such as training instability, mode collapse, and\npoor representation of multimodal distributions. This success has\nspurred widespread research interest. In the domain of tabular\ndata, diffusion models have begun to showcase similar advan-\ntages over GANs and VAEs, achieving significant performance\nbreakthroughs and demonstrating their potential for addressing\nunique challenges in tabular data modeling. However, while\ndomains like images and time series have numerous surveys\nsummarizing advancements in diffusion models, there remains\na notable gap in the literature for tabular data. Despite the\nincreasing interest in diffusion models for tabular data, there\nhas been little effort to systematically review and summarize\nthese developments. This lack of a dedicated survey limits a clear\nunderstanding of the challenges, progress, and future directions\nin this critical area. This survey addresses this gap by providing\na comprehensive review of diffusion models for tabular data.\nCovering works from June 2015, when diffusion models emerged,\nto December 2024, we analyze nearly all relevant studies, with\nupdates maintained in a GitHub repository. Assuming readers\npossess foundational knowledge of statistics and diffusion models,\nwe employ mathematical formulations to deliver a rigorous and\ndetailed review, aiming to promote developments in this emerging\nand exciting area.\nIndex Terms—Diffusion Models, Tabular Data, Generative\nModels\nI. INTRODUCTION\nTabular data is a data modality in which information is orga-\nnized into rows, representing individual records, and columns,\nrepresenting features or attributes. It is ubiquitous in real-\nworld domains, including but not limited to healthcare [1],\nfinance [2], education [3], transportation [4], psychology [5],\netc. The demand for high-quality generative models in these\ndomains is acute due to data privacy regulations such as\nGDPR [6] and CCPA [7]. Consequently, real user data is\noften restricted from public release, whereas synthetic data\ngenerated by generative models can preserve machine learning\nutility while being legally shareable [8]. Beyond privacy\nconcerns, real-world tabular datasets often contain missing\nvalues, which can arise due to human errors or technical\nmalfunctions like sensor failures. To address this, generative\n*Qi and Lincen contributed equally.\n#Zhao Yang is with VU Amsterdam. All other authors are with\nLIACS,\nLeiden\nUniversity,\nthe\nNetherlands.\nCorresponding\nEmail:\nz.li@liacs.leidenuniv.nl (Zhong Li)\nmodels have been employed for missing value imputation,\ndemonstrating promising performance [9]. Furthermore, tab-\nular data often presents challenges related to imbalanced\nclass distributions [10], where certain categories dominate and\nresult in biased models. Generative models can help mitigate\nthis issue by generating synthetic samples for underrepre-\nsented classes, improving model performance on minority\ncategories [11]. Overall, these multifaceted applications under-\nscore the growing importance of generative models for tabular\ndata, ranging from privacy protections [12], [13], missing\nvalue imputation [14], [15], to training data augmentation [16].\nDeep generative models mainly include Energy-based Mod-\nels (EBMs) [17], Variational Autoencoders (VAEs) [18], Gen-\nerative Adversarial Networks (GANs) [19], Autoregressive\nModels [20], Normalized Flows [21], and Diffusion Models\n[22]. Diffusion Models offer several advantages that make\nthem a preferred choice for many generative tasks. Unlike\nGANs, which often suffer from mode collapse and unstable\ntraining due to adversarial loss dynamics [23], Diffusion Mod-\nels are inherently stable and effectively capture the full data\ndistribution. Moreover, they produce sharper and more realistic\noutputs than VAEs, avoiding the blurry reconstructions in\nimages caused by Gaussian latent space assumptions [24].\nCompared to EBMs, Diffusion Models do not rely on com-\nputationally expensive sampling methods like Markov chain\nMonte Carlo (MCMC) and are easier to train [25]. They also\novercome the limitations of Normalizing Flows by avoiding\nbijective transformation constraints and Jacobian calculations\n[21], [25], enabling more powerful expressivity. Finally, unlike\nAutoregressive Models, Diffusion Models are not constrained\nby sequential generation [26], allowing them to utilize flexible\narchitectures to learn arbitrary data distributions for diverse\ndata types. These strengths together make Diffusion Models\na highly flexible and robust choice for generative modeling\ntasks.\nMore concretely, diffusion models [22], [27], [28] are\nlikelihood-based generative models designed to learn the un-\nderlying distribution of training data and generate samples that\nclosely resemble it. Typically, a diffusion model comprises a\nforward diffusion Markov process, which gradually transforms\ntraining data into pure noise, and a reverse denoising Markov\nprocess, which reconstructs realistic synthetic data from the\nnoise. Given the remarkable performance of diffusion models\nin generating high-quality and diverse synthetic samples across\nvarious domains—such as images [27], [28], audio [29], [30],\ntext [31], [32], video [33], and graphs [34]—recent studies\n[35], [8], [36], [37], [38], [39] have explored their applica-\narXiv:2502.17119v1  [cs.LG]  24 Feb 2025\n\nMANUSCRIPT SUBMITTED TO IEEE FOR POSSIBLE PUBLICATION\n2\ntion to tabular data. These studies demonstrate that diffusion\nmodels outperform GAN- and VAE-based approaches across\na wide range of benchmarks in tabular data modeling.\nHowever, the exploration of diffusion models in tabular\ndata modeling is arguably still in its infancy due to the\ninherent challenges posed by the unique characteristics of\ntabular data. Unlike images or audio, tabular data presents\ndistinct challenges: missing values, where some values are\nabsent, requiring models to handle incomplete data; non-\nGaussian distributions, where numerical features typically do\nnot follow a Gaussian distribution; multimodal distributions,\nwhere a column’s distribution can exhibit multiple modes;\nhighly imbalanced categorical columns, where major cate-\ngories often dominate minor categories in terms of frequency;\nheterogeneous features, where different columns can have\nvarying data types such as numerical or categorical; mixed-type\nsingle features, where a single column may contain values of\ndifferent data types; feature dependencies, where correlations\namong features must be accounted for; and small datasets,\nwhere the training data is often limited compared to datasets\nfor images or text. As a result, directly adapting diffusion\nmodels designed for other modalities, particularly images or\ntext, is insufficient for tabular data. Their unique characteristics\nnecessitate the development of tailored diffusion models, and\nsignificant research efforts are required to invent or refine\ndiffusion models that effectively address the complexities of\ntabular data modeling.\nWhile numerous surveys summarize advancements in diffu-\nsion models across various domains [40], [41] or on specific\ndata modalities such as images [42], text [43], videos [33],\ntime series [44], and graphs [34], surveys specifically focused\non generative models for tabular data [45], [46] primarily\ncover traditional methods, VAEs and GANs, with little to no\nattention given to diffusion models. In other words, despite\nthe increasing interest in diffusion models for tabular data and\ntheir demonstrated breakthroughs in performance, there has\nbeen little effort [47] to systematically review and summarize\nthese developments. This lack of a dedicated survey limits\na clear understanding of the challenges, progress, and future\ndirections in this critical area.\nTo address this gap in the literature, we present the first\ncomprehensive survey dedicated to diffusion models for tabu-\nlar data. Our main contributions are as follows: 1) We provide\na overview of the historical development of generative models\nfor tabular data, and on this foundation, we identify the key\nchallenges in developing generative models for tabular data;\n2) We recap the fundamental concepts of diffusion models,\ndescribe the most widely used diffusion model frameworks\nfor tabular data modeling, and highlight typical applications\nof diffusion models in the tabular domain; and 3) We offer a\ndetailed and extensive review of diffusion models for tabular\ndata. Covering works from June 2015, marking the inception\nof diffusion models, to December 2024, we analyze nearly\nall relevant studies and maintain a GitHub repository for con-\ntinuous updates. We also review diffusion models in discrete\nspaces that can be useful for generating categorical features in\ntabular data, followed by summarizing evaluation metrics for\nassessing the performance of tabular generative models.\nThis survey is organized as follows. Section II discusses\nthe historical evolution of tabular data generation, the unique\nchallenges associated with modeling tabular data, and the\ntaxonomy of diffusion models for tabular data. Section III\nintroduces the preliminaries of diffusion models and their\napplications in tabular data. Sections IV, V, VI, and VII,\nprovide an in-depth review of diffusion models specifically\ndesigned for tabular data. In Section VIII, we further explore\ndiscrete diffusion models and evaluation metrics relevant to\ntabular data modeling. Finally, Section IX offer a discussion\non future directions and concludes the survey.\nII. GENERATIVE MODELS FOR TABULAR DATA\nIn this section, we start by reviewing the development\nhistory of generative models for tabular data. We then discuss\nthe unique characteristics of tabular data and the challenges\nthese pose for developing generative models. Finally, we\noutline the taxonomy of diffusion models for tabular data\nadopted in this survey, categorized based on their applications.\nA. History of Generative Models for Tabular Data\nAs shown in Figure 1, prior to the advent of models\nexplicitly designed for data generation (e.g., VAEs [48], GANs\n[19], and Diffusion Models [22]), probabilistic models like\nCopula [49], Gaussian Mixture Models [50] and Bayesian Net-\nworks [51] were commonly employed for data synthesis. Later,\nspecialized methods, including distance-based approaches like\nSMOTE [52] and its variants (e.g., Borderline-SMOTE [53],\nSMOTETomek [54], and SMOTE-ENC [55]), as well as\nADASYN [56], and probabilistic models like Synthpop [57],\nwere introduced for data synthesis and imputation. However,\ndistance-based methods, including SMOTE and its variants,\nencounter challenges when dealing with large datasets and\ncomplex data distributions. Additionally, probabilistic meth-\nods, such as Copulas and Synthpop, often struggle with\nheterogeneous data, impose predefined distributions, and are\nprone to assumption biases [45]. In contrast, deep generative\nmodels explicitly designed for data generation have gained in-\ncreasing prominence in the tabular domain, offering significant\nadvancements and more successful applications compared to\ntraditional tabular data generation techniques. For example,\n• VAE [48] based methods such as TVAE [58] and GOG-\nGLE [59] are shown to achieve superior performance.\nImportantly, GOGGLE is the first to explicitly model the\ncorrelations among features, by using a VAE-based model\nwith GNN as the encoder and decoder models. However,\nVAEs-based methods are prone to certain limitations,\nincluding blurry outputs in the generated data due to the\ninherent randomness introduced by the latent space and\npotential difficulty in balancing reconstruction loss and\nregularization during training, which can affect the qual-\nity of the synthetic data. Additionally, VAEs may strug-\ngle with accurately capturing multimodal distributions, a\ncommon characteristic in real-world tabular datasets.\n• GAN [19] based methods have demonstrated promising\nresults in tabular data synthesis, with key examples in-\ncluding CTGAN [58] and its variants such as CTABGAN\n\nMANUSCRIPT SUBMITTED TO IEEE FOR POSSIBLE PUBLICATION\n3\nTraditional Machine Learning  for Tabular Data:\nCopula\n(1970s)\nBayesian network\n(1980s, 1990s)\nVAEs (2013)\n⚫TVAE (2019)\n⚫GOGGLE (2023)\n⚫…\nVAEs for Tabular Data:\nGANs (2014)\nDiffusion Models \n(2015, 2020)\nDMs for Tabular Data:\n⚫CTGAN (2019)\n⚫CTBGAN (2021)\n⚫CTBGAN+ (2024)\n⚫…\nGANs for Tabular Data:\n⚫SOS (2022)\n⚫STaSy (2023)\n⚫TabDDPM (2023)\n⚫CoDi (2023)\n⚫TabSyn (2024)\n⚫TabUnite (2024)\n⚫…\n⚫Copula (1970s)\n⚫Gaussian Mixture Models (1990s)\n⚫Bayesian Networks (1980s, 1990s)\n⚫SMOTE (2002)\n⚫Borderline-SMOTE (2005)\n⚫ADASYN (2008) \n⚫Synthpop (2016)\n⚫…\nTransformers (2017)\n⚫GReaT (2023)\n⚫TabuLa (2023)\n⚫TabMT (2024)\n⚫…\nLLMs for Tabular Data:\nFig. 1. Timeline of Generative Models for Tabular Data: Below the timeline, key advancements in traditional machine learning models and deep generative\nmodels are shown, while above the timeline, their extensions in tabular data, such as data synthesis and imputation, are highlighted.\n[60] and CTABGAN+ [61]. These models typically use\nGaussian Mixture Models to model continuous features,\nwhich may be suboptimal for certain real-world data.\nAdditionally, they handle categorical features using one-\nhot encoding, which can substantially increase data di-\nmensionality. Moreover, GANs inherently suffer from\nwell-known limitations such as mode collapse, where the\ngenerator fails to capture the full data distribution, and\ntraining instability that makes the optimization process\ndifficult and often requires careful hyperparameter tuning\nand regularization.\n• LLMs [62] have also been explored for tabular data\nsynthesis. For example, GReaT [63] employs large lan-\nguage models by converting each row into a natural\nlanguage sentence and learning sentence-level distribu-\ntions using GPT [64]. This approach suffers from some\nlimitations, including potential information loss during\ndata-to-text conversion, increased dimensionality leading\nto higher computational costs, and context length limi-\ntations, which can affect scalability when handling large\ndatasets.\n• Diffusion Models [22] have demonstrated superior perfor-\nmance compared to VAEs and GANs in image synthesis\ntasks [65]. Recent studies, including but not limited to\nSOS [66], STaSy [35], TabDDPM [8], CoDi [37], and\nTabSyn [38], indicate that these advantages extend to\ntabular data synthesis as well.\nSince this survey focuses specifically on diffusion models for\ntabular data, we recommend readers refer to dedicated survey\npapers [45], [46], [67] for a comprehensive review of other\ngenerative models for tabular data.\nB. Challenges with Generative Models for Tabular Data\nTraining generative models in tabular data can be inherently\nmore challenging than in image or text data due to the\nfollowing challenges.\n1) Missing Values: This phenomenon often happens in real-\nworld tabular datasets for several reasons [68], e.g., privacy\nconcerns (people’s refusal to answer questions about their em-\nployment or income information in census data [69]), difficulty\nof data collection (drop-out in studies and merging unrelated\ndata in healthcare data [70]), human operation errors when\nprocessing data [71], or machine error due to malfunctioning\nof equipment [71]. Furthermore, the missing value problem\ncan be categorized into [72]: Missing At Random (MAR),\nMissing Completely At Random (MCAR), and Missing Not\nAt Random (MNAR). Most generative models for tabular data\ncannot be directly trained on incomplete data [39].\n2) Intricate Individual Feature Distribution: It is pointed\nout [58] that a feature (namely a column in a table) in tabular\ndata may have complicated distributions in the sense that: 2.1)\nfor numerical feature, the distribution can be non-Gaussian\nand/or it can have multiple distribution modes; 2.2) for a\ncategorical feature, the distribution of categories can be highly\nimbalanced. For example, instances of the major category can\ntake more 95% while those of minor categories take only 5%\nin total; 2.3) within the same feature type (namely numerical or\ncategorical), different features usually have different statistical\nproperties (e.g., feature-wise marginal distribution) due to the\nfact that the meanings of different features can be various. In\ncontrast, pixel values of each image in an image dataset are\nusually assumed to follow the same distribution.\n3) Heterogeneous Features: In the context of tabular data,\nheterogeneous features refer to columns that contain different\ntypes of data, which is considered the most challenging issue\nby the community [36]. These types may include numerical,\ncategorical, ordinal, boolean, text, or datetime data. This diver-\nsity makes processing and modeling such data more complex\nthan homogeneous datasets, which consist of only one type of\nfeature. This is because most generative models only focus on\nlearning distributions either on numerical or discrete domains.\nIt is unclear whether it is effective by simply combining\ndifferent types of models that are separately designed for\ndifferent types of features.\n4) Feature Dependencies (Correlations between Features):\nTabular data synthesis should learn the joint probability of\n\nMANUSCRIPT SUBMITTED TO IEEE FOR POSSIBLE PUBLICATION\n4\nmultiple columns to generate a sample, and these columns\nare usually not independent to each other (namely their joint\nprobability cannot be decomposed into the production of their\nmarginal probabilities). Unlike image data which contains\nonly continuous pixel values with local spatial correlations or\ntext data which comprises tokens sharing the same dictionary\nspace [73], capturing the correlations among features has\nbeen a long-standing challenge in tabular data synthesis [36].\nHowever, the heterogeneous nature of tabular data makes it\neven more challenging than capturing the correlations among\npurely categorical or numerical features.\n5) Mixed-Type Feature: A mixed-type feature refers to\na single feature (column in a dataset) that contains more\nthan one type of data. Unlike heterogeneous features, which\ndescribe the diversity of feature types across columns, mixed-\ntype features involve a single column containing values of\ndifferent data types, such as numerical and categorical data\nmixed together.\n6) Small Data Size: Compared to image or text datasets,\nwhich typically have massive amounts of available data, tab-\nular datasets are usually smaller in volume. Specifically, the\nnumber of instances or samples is often limited, making it\nchallenging to train data-hungry deep models effectively.\n7) Domain\nSpecific\nConstraints:\nReal-world\ntabular\ndatasets span various domains, such as healthcare, finance,\nand engineering, each with domain-specific features and\nconstraints. For example, healthcare data must maintain\nrealistic ranges for age or blood pressure, while finance\ndata requires valid transaction amounts and balances. Unlike\nimages or text, these constraints are often difficult to\nverify without domain expertise, making the generation of\nhigh-quality synthetic tabular data particularly challenging.\nC. Taxonomy of Diffusion Models for Tabular Data\nTaxonomy on Diffusion Models\n for Tabular Data\nSection 4: Data Augmentation\nSection 5: Data Imputation\nSection 6: Trustworthy Data Synthesis\nSection 7: Anomaly Detection\nClavaDDPM, GNN-TabSyn\nSingle Table Synthesis\nTabCSDI, TabSyn, TabDiff, SimpDM, MTabGen, \nDDPM-Perlin, NewImp, DiffPuter\nPrivacy-Preserving\nFairness-Preserving\nGeneric Models\nModels for Healthcare\nTabADM, DTE, NSCBAD, DDPM-TAD, FraudDDPM\nModels for Finance\nMutli-relational database Synthesis\nSOS, STaSy, TabDDPM,\nCoDi, AutoDiff, MissDiff,\nTabSyn, Forest-Diffusion,\nTabDiff, TabUnite\nMedDiff, EHR-TabDDPM\nDPM-EHR, FlexGen-EHR,\nEHRDiff, EHR-D3PM\nFinDiff, EntTabDiff, Imb-FinDiff\nSiloFuse, FedTabDiff, DP-Fed-FinDiff\nFairTabDDPM\nFig. 2. Taxonomy of Diffusion Models for Tabular Data.\nGenerative models have achieved notable success across a\nwide range of real-world applications for tabular data, with\ndiffusion models emerging as a powerful approach alongside\nother generative frameworks such as GANs and VAEs. These\napplications address critical challenges in domains such as\nhealthcare and finance, where tabular data is ubiquitous.\nBroadly, generative models for tabular data can be divided\ninto four key categories: data augmentation, data imputation,\ntrustworthy data synthesis, and anomaly detection.\nWhile this taxonomy applies to tabular generative models\nin general, the focus of this survey is on diffusion models.\nTherefore, we will systematically examine diffusion-based\napproaches within each category, discussing how they lever-\nage the diffusion process to enhance tabular data modeling.\nThis application-driven taxonomy was chosen instead of a\nclassification based on the underlying diffusion model types\n(discussed in Section III) because it provides a more intuitive\nand practical perspective on how these models are applied to\nreal-world problems. Practitioners and researchers are often\nmore concerned with their end-use cases rather than the\nunderlying mathematical formulations. This taxonomy allows\nus to systematically categorize and evaluate the growing body\nof literature on diffusion models for tabular data, highlighting\nkey contributions, challenges, and future directions in each\ncategory.\nThe relevant methods are explored in detail in Sec-\ntions IV–VII. For data augmentation (see Section IV), we will\nreview methods that generate synthetic data. The section data\nimputation (see Section V) will discuss approaches focused\non handling missing values in tabular datasets. Next, we will\nexplore trustworthy data synthesis (see Section VI), which\nincludes privacy-preserving and fairness-preserving data gen-\neration techniques. Finally, in the anomaly detection section\n(see Section VII), we will review methods that leverage\ndiffusion models to identify anomalies by learning the normal\ndata distribution.\nFor clarity, each section (Sections IV, V, VI, and VII)\nbegins with a brief introduction to the background, problem\ndefinitions and notations used, accompanied by a summary\ntable of the relevant works. This is followed by an in-depth\nchronological review of the papers, emphasizing how they\naddress key challenges, such as handling mixed feature types,\npreserving feature dependencies, and managing missing data,\nalong with their performance evaluations and limitations. To\nensure completeness, before reviewing diffusion models for\ntabular data, we first introduce the core mechanism of diffusion\nmodels and the prominent frameworks used for tabular data\nmodeling in Section III. Readers already familiar with these\nconcepts may skip this section.\nIII. PRELIMINARIES OF DIFFUSION MODELS\nIn this section, we first introduce the core mechanism of\ndiffusion models, covering both the forward diffusion process\n(gradual addition of noise) and the reverse process (learn-\ning to denoise). We then present prominent diffusion model\nframeworks, including DDPMs (Gaussian Diffusion Models),\nMultinomial Diffusion Models, Score-based Generative Mod-\nels (SGMs), Score-based Generative Models through Stochas-\ntic Differential Equations (SDEs), and conditional diffusion\nmodels. For clarity, the notation used throughout this survey\nis summarized in Table I.\n\nMANUSCRIPT SUBMITTED TO IEEE FOR POSSIBLE PUBLICATION\n5\nTABLE I\nSUMMARY OF NOTATION USED IN THIS SURVEY\nNotation\nDescription\nR\nTable, where each row represents a sample and each\ncolumn a feature or the prediction target\nX\nTabular data matrix\nx := (xnum, xcat)\nAn instance or sample, namely a row in the table, con-\nsisting of numerical and/or categorical feature values\nxnum\nThe numerical part of the instance\nxcat\nThe categorical part of the instance\nx\nAn instance or sample, namely a row in the table\nj\nixt\nThe j-th feature value of the i-th sample at time t\nj\nixnum\nt\nThe j-th numerical feature value of the i-th sample at\ntime t\nj\nixcat\nt\nThe j-th categorical feature value of the i-th sample\nat time t\nj\nixnum\nThe j-th numerical feature value of the i-th sample\nj\nixcat\nThe j-th categorical feature value of the i-th sample\nj\nix\nThe j-th feature of the i-th sample\njx\nThe j-th feature of the given sample\nix\nthe i-th sample\njxt\nthe j-th feature of the given sample at time point t\nixt\nthe i-th sample at time t\nxt\nthe sample at time t\nMnum\nThe number of numerical features in the table\nMcat\nThe number of categorical features in the table\nz\nLatent embedding of x in the diffusion model\nϵ\nGaussian noise added during the diffusion process\nT\nTotal number of timesteps in the diffusion process\npdata(·), pnoise(·)\nProbability distribution of the data or noise\nL\nLoss function used for training the model\nθ\nParameters of the model\nSθ(·)\nThe score network function used to predict the score\nϵθ(·)\nThe noise function used to predict the added noise\nA. Mathematics of Diffusion Models\nDiffusion probabilistic models, or more commonly known as\ndiffusion models [22], are deep generative models defined from\na forward diffusion process and a reverse denoising process.\nSpecifically, the diffusion process aims to gradually corrupt a\nsample x0 (drawn from the training data distribution qdata(·))\nto a noisy instance xT (defined by a prior distribution qnoise(·))\nby using the following process:\nq(x1:T |x0) :=\nT\nY\nt=1\nq(xt|xt−1),\n(1)\nwith q(xt|xt−1) the forward transition probability. Meanwhile,\nthe denoising process attempts to remove noises and generate\na synthetic but realistic sample ˆx0 from xT as follows:\npθ(x0:T ) := p(xT )\nT\nY\nt=1\npθ(xt−1|xt),\n(2)\nwith pθ(xt−1|xt) an approximation of the reverse of the\nforward transition probability, which is learned by a neural\nnetwork with parameters θ. Particularly, they learn θ by\nminimizing the following variational upper bound (LVUB) on\nthe negative log-likelihood:\n−log p(x) ≤Eq(x1|x0)[−log pθ(x0|x1)\n|\n{z\n}\nL0(Lrecons)\n]\n+ DKL[q(xT |x0)||p(xT )]\n|\n{z\n}\nLT (Lprior)\n+\nT\nX\nt=2\nEq(xt|x0) DKL[q(xt−1|xt, x0)||pθ(xt−1|xt)]\n|\n{z\n}\nLt(Ldiffusion)\n.\n(3)\nHere, L0 can be interpreted as a reconstruction term that\npredicts the log probability of original sample x0 given the\nnoised latent x1, while LT measures how the final corrupted\nsample xT resembles the noise prior distribution; Meanwhile,\nLt measures how close is the estimated transition probability\npθ(xt−1|xt) to the ground-truth posterior transition probability\nq(xt−1|xt, x0).\nNote that the formulations provided above define only the\ngeneric structure of diffusion models. Depending on the spe-\ncific data types (namely continuous or discrete), the definitions\nof prior noise distribution p(xT ), forward transition probabil-\nity q(xt|xt−1), the reverse of forward transition probability\npθ(xt−1|xt), and their training objectives can be different. In\nthe following, we concisely review the key frameworks of dif-\nfusion models most commonly used in tabular data modeling:\n1) Gaussian diffusion model, 2) multinomial diffusion model,\n3) score-based generative model, 4) score-based generative\nmodel via SDEs, and 5) conditional diffusion models.\n1) Gaussian Diffusion Models: Gaussian diffusion models,\noften known as Denoising Diffusion Probabilistic Models\n(DDPMs) [27], is a family of diffusion probabilistic models\nthat operate in continuous spaces. They define the diffusion\nand reverse processes with the following instantiations:\np(xT ) := N(xT ; 0, I),\n(4a)\nq(xt|xt−1) := N(xt;\np\n1 −βtxt−1, βtI),\n(4b)\npθ(xt−1|xt) := N(xt−1; µθ(xt, t), Σθ(xt, t)),\n(4c)\nwhere Gaussian noises are gradually injected into the sample\nbased on a time-dependent variance schedule {βt}T\nt=1, with\nβt ∈(0, 1) determining the amount of noise added at time\nstep t. To approximate pθ(xt−1|xt), [27] propose to define:\nµθ(xt, t) =\n1\n√αt\n\u0010\nxt −\nβt\n√1 −¯αt\nϵθ(xt, t)\n\u0011\n,\nΣθ(xt, t) = σtI,\n(5)\nwhere σt controls the noise level added at time step t, αt :=\n1 −βt, ¯αt := Qt\ni=1 αi, and ϵθ is a neural network to predict\nground truth noise ϵ ∼N(0, I) that has been added to noise\nsample xt. As a result, they propose to optimize the following\nsimplified objective function (rather than Eq. 3):\nLGauss\nsimple(θ) := EtEx0∼q(x0)Eϵ∼N(0,I)\n\u0002\nλ(t)∥ϵ −ϵθ(xt, t)∥2\n2\n\u0003\n,\n(6)\nwhere λ(t) is a weighting function to adjust the noise scales,\nand ∥· ∥2 denotes the Euclidean norm.\n\nMANUSCRIPT SUBMITTED TO IEEE FOR POSSIBLE PUBLICATION\n6\n2) Multinomial Diffusion Models: Multinomial diffusion\nmodels [31] [32] is a representative (also the first) diffusion\nprobabilistic model in discrete spaces. Their diffusion and\ndenoising processes operate in discrete spaces, designed to\ngenerate categorical data xt ∈{0, 1}K (i.e., one-hot encoding\nwith K distinct values). They are defined as follows:\np(xT ) := Cat(xT ; 1/K),\n(7a)\nq(xt|xt−1) := Cat(xt; (1 −βt)xt−1 + βt/K),\n(7b)\npθ(xt−1|xt) :=\nK\nX\nˆx0=1\nq(xt−1|xt, ˆx0)pθ(ˆx0|xt),\n(7c)\nwith Cat(·) categorical distribution and K the number of\ncategories (the computation between scalars and vectors are\ndone in an element-wise way). Importantly, uniform noise\n(rather than Gaussian noise) is added to the sample according\nto the noise schedule βt. As we can see, pθ(xt−1|xt) is pa-\nrameterized as q(xt−1|xt, ˆx0(xt, t)), with ˆx0(xt, t) predicted\nby a neural network, which can be trained via the multinomial\ndiffusion loss defined using Eq. 3.\nIt is important to note that the multinomial diffusion model\ncan handle only one categorical feature at a time. In other\nwords, for a table with C categorical features, C separate\nmultinomial diffusion models would need to be built. More\nimportantly, several novel diffusion models for discrete data\nhave been proposed recently; these will be reviewed in Sec-\ntion VIII-A for better readability.\n3) Score-based Generative Models (SGMs): For these mod-\nels, the forward diffusion process follows the same structure\nas the Gaussian diffusion model, whereas the reverse process\nis defined differently, as shown below. Given an instance\nx and its distribution p(x), its score function is defined as\n∇x log p(x). To estimate the score function, one can train a\nneural network Sθ(·) with the following objective:\nEx∼p(x)∥Sθ(x) −∇x log p(x)∥2\n2.\n(8)\nHowever, Song & Ermon [74] point out that the estimated\nscore functions are inevitably imprecise in low density regions\nwhen the low-dimensional manifolds are embedded into a\nhigh-dimensional space. To mitigate this, in the diffusion\nprocess they propose to perturb the original data x with a\nsequence of random Gaussian noises with intensifying scales\n0 < σ1 < · · · < σT . In other words, pσ1\n≈p(x0),\npσT ≈N(0, I), and pσt ≈N(xt; x0, σ2\nt I). In the reverse\nprocess, they utilize a noise-conditioned score network Sθ(·)\nto approximate ∇x log pσt(x), which analytically equals to\n(x0 −xt)/σt. As a result, the training objective is as follows:\n1\nT\nT\nX\nt=1\nλ(σt)Ep(x0)Ext∼pσt(xt|x)\n\r\r\r\rSθ(xt, σt) + (xt −x0\nσt\n)\n\r\r\r\r\n2\n2\n.\n(9)\nAfter training Sθ(·), new samples are generated with the\nannealed Langevin dynamics (see [74] for details). Note that\nSGMs are defined in discrete time space, a special case cor-\nresponding to the variance exploding form in the generalized\nversion presented in the sequel.\n4) Score-based Generative Models through Stochastic Dif-\nferential Equations (SDEs) [28]: This is a continuous-time\ngeneralization of the denoising diffusion models (DDPMs that\ncorrespond to variance preserving form) and score based gen-\nerative models (SGMs that correspond to variance exploding\nform). Particularly, the diffusion process is defined with the\nfollowing itˆo stochastic differential equation [75]:\ndx = f(x, t)dt + g(t)dw,\n(10)\nwhere f(x, t) = f(t)x, and f(·), g(·) are referred to as the\ndrift and diffusion coefficients of xt, respectively. Moreover,\nw is the standard Wiener process. The most widely studied and\ncommonly used diffusion models can be broadly categorized\ninto three main types: 1) Variance Exploding (VE), 2) Variance\nPreserving (VP), and 3) sub-Variance Preserving (sub-VP)\nbased on the types of functions f(·) and g(·) as follows:\nf(x, t) =\n\n\n\n\n\n0,\nif VE,\n−1\n2γtx,\nif VP,\n−1\n2γtx,\nif sub-VP,\n(11)\ng(t) =\n\n\n\n\n\n\n\nq\nd[σ2\nt ]\ndt ,\nif VE,\n√γt,\nif VP,\nq\nγt(1 −e−2\nR t\n0 γs ds),\nif sub-VP,\n(12)\nwhere γt and σt are noise functions w.r.t. the time variable t.\nMeanwhile, the denoising process is defined as the reverse of\nthe diffusion process:\ndx = [f(x, t) −g(t)2∇x log pt(x)]dt + g(t)d ¯w,\n(13)\nwhere ¯w is a Wiener process running backward in time, and\nthe score function ∇x log pt(x) is approximated by a learnable\nneural network Sθ(x, t). However, directly approximating the\nscore function is computationally intractable and thus they\npropose to train Sθ(·) by estimating the transition probability\n∇xt log p(xt|x0) as follows [28]:\narg min\nθ\nEt\nn\nλ(t)Ex0\nh\nExt|x0\nh\n∥Sθ(xt, t) −∇xt log p(xt|x0)∥2\n2\niio\n,\n(14)\nwhere ∇xt log p(xt|x0) follows the Gaussian distribution and\ncan be collected during the diffusion process. Moreover, λ(t)\nis used to trade-off between sample quality and likelihood.\nAfter training Sθ(·), we can generate new samples with the\nfollowing two methods: 1) the predictor-corrector framework,\nor 2) the probability flow framework. In general, the prob-\nability flow framework is preferred due to its fast sampling\nand exact log-probability computation compatibilities (see [28]\nfor more details). In short, the probability flow employs the\nfollowing neural ordinary differential equation (NODE) based\nmodel [76]:\ndx =\n\u0012\nf(x, t) −1\n2g(t)2∇x log pt(x)\n\u0013\ndt,\n(15)\nwhich describes a deterministic process whose marginal prob-\nability is equivalent to that of the original reverse SDE (namely\nEq. 13) [28].\n\nMANUSCRIPT SUBMITTED TO IEEE FOR POSSIBLE PUBLICATION\n7\n5) Conditional Diffusion Models: The diffusion models\nintroduced in Sections III-A1, III-A2, III-A3, and III-A4 are all\nunconditional, meaning the posterior estimator function pθ(·)\ndoes not know the label of the data it is modeling. Given a\nlabel vector y, [22] and [65] suggest that this can be achieved\nthrough a so-called conditional reverse process in DDPM,\ndefined as:\npθ,ϕ(xt−1|xt, y) ∝pθ(xt−1|xt)pϕ(y|xt−1),\n(16)\nwhich requires to train a classifier pϕ(·) and thus is known\nas classifier-guided DDPM. Dhariwa and Nichol [65] further\napproximate the logarithm of it with a perturbed Gaussian\ntransition as follows:\nlog(pθ,ϕ(xt−1|xt, y)) ≈log(p(z)) + C,\n(17)\nwhere C is a constant, z ∼N(µ + Σg, Σ), and g =\n∇xt−1 log(pϕ(y|xt−1))|xt−1=µ is computed from the clas-\nsifier pϕ(·). To avoid training a separate classifier, Ho &\nSalimans [77] propose a classifier-free guided DDPM as\nfollows:\n¯ϵ(xt, y, t) = ˆϵ(xt, t) + ωg[ˆϵ(xt, y, t) −ˆϵ(xt, t)],\n(18)\nwhere ˆϵ(·) is the noise estimator ϵθ(·) defined in DDPM.\nMoreover, [ˆϵ(xt, y, t) −ˆϵ(xt, t)] is guidance of y and ωg the\nguidance weight.\nIV. DIFFUSION MODELS FOR DATA AUGMENTATION\nData augmentation is a long-standing research problem\nin tabular data [78]. In general, it can be divided into two\ndifferent tasks: 1) data synthesis, which is the process of\ngenerating synthetic data that mimics the characteristics of\nreal-world data. Depending on whether we generate a single\ntable or a set of connected tables, it can be further divided\ninto single table synthesis or multi-relational dataset synthesis;\nand 2) over-sampling, which balances an imbalanced table by\nincreasing the number of samples in the minority class(es).\nParticularly, over-sampling can be considered as a special case\nof single table synthesis where we only generate a part of the\ntable. Accordingly, in this survey, we categorize relevant works\ninto diffusion models for single-table synthesis (including over-\nsampling) and diffusion models for multi-relational dataset\nsynthesis, providing their formal definitions and reviewing\nrelated studies in Section IV-A and IV-B, respectively.\nA. Diffusion Models for Single Table Synthesis\nThis subsection reviews works on diffusion models for\nsingle table synthesis (including over-sampling). For clarity,\nwe further subdivide related works into three sub-categories: 1)\ngeneric diffusion models for tabular data, 2) diffusion models\nfor tabular data in healthcare domain, and 3) diffusion models\nfor tabular data in finance domain. To ensure readers can\nfollow along, we present the notation and problem definitions\nbelow and provide a summary of the models in Table II.\nGiven a table R, we utilize j\nixt to denote the j-th feature\nvalue of the i-th sample at time point t. On this basis, let\nthe number of numerical features be Mnum and the number\nof categorical features be Mcat. By reorganizing the order of\nthe features, a sample (i.e., a row) can be represented as\nx = (xnum, xcat), where xnum ∈RMnum and xcat ∈ZMcat.\nThe j-th categorical feature can have Cj distinct feature\nvalues {1, · · · , Cj}, namely jxcat ∈{1, · · · , Cj}. With these\nnotations, the single table synthesis problem can be defined as\nfollows:\nProblem 1 (Single Table Synthesis). Given a table R = {x},\nwe aim to learn a generative model pθ(R) to generate a\nsynthetic table ˆR = {ˆx} with high-quality and diversity.\nTABLE II\nOVERVIEW OF DIFFUSION MODELS FOR SINGLE TABLE SYNTHESIS. ‘*’\nREFERS TO A NAME CREATED BY US FOR SIMPLICITY.\nModel Name\nYear\nVenue\nFeature Type\nDomain\nSOS [66]\n2022\nKDD\nNum by Default\nGeneric\nSTaSy [35]\n2023\nICLR\nNum+Cat\nGeneric\nTabDDPM [8]\n2023\nICML\nNum+Cat\nGeneric\nCoDi [37]\n2023\nICML\nNum+Cat\nGeneric\nMissDiff [79]\n2023\nICMLW\nNum+Cat\nGeneric\nAutoDiff [36]\n2023\nNeurIPSW\nNum+Cat+Mixed\nGeneric\nDPM-EHR* [80]\n2023\nNeurIPSW\nNum+Cat\nHealthcare\nFinDiff [81]\n2023\nICAIF\nNum+Cat\nFinance\nMedDiff [82]\n2023\nArXiv\nNum by default\nHealthcare\nEHR-TabDDPM [83]\n2023\nArXiv\nNum+Cat\nHealthcare\nTabSyn [38]\n2024\nICLR\nNum+Cat\nGeneric\nFlexGen-EHR [84]\n2024\nICLR\nCat+TS\nHealthcare\nEHRDiff [85]\n2024\nTMLR\nCat+Num+TS\nHealthcare\nForest-Diffusion [39]\n2024\nAISTATS\nNum+Cat\nGeneric\nTabDiff [73]\n2024\nNeurIPSW\nNum+Cat\nGeneric\nEntTabDiff [86]\n2024\nICAIF\nNum+Cat\nFinance\nImb-FinDiff [87]\n2024\nICAIF\nNum+Cat\nFinance\nEHR-D3PM [88]\n2024\nArXiv\nCat\nHealthcare\nTabUnite [89]\n2024\nOpenReview\nNum+Cat\nGeneric\nCDTD [90]\n2024\nOpenReview\nNum+Cat\nGeneric\n1) Generic Diffusion Models for Single Table Synthesis: In\nthis part, we review generic diffusion models for single table\nsynthesis, which represent the most significant advancements\nin tabular data generation. Unlike domain-specific models\ntailored for particular fields, such as healthcare or finance,\ngeneric models are designed to handle diverse types of tabular\ndata, including mixed-type features (numerical, categorical)\nand datasets with varying scales, sparsity, and correlations.\nThese models aim to be universally applicable across different\ndomains, making them highly valuable for a wide range of\nmachine learning tasks.\nSOS [66] is the first to apply score-based generative models\n(SGMs via SDEs [28]) for tabular data oversampling. Specif-\nically, given a training dataset Xtrain, which can be divided\ninto M distinct subsets X1, . . . , XM based on the labels of\ninterest, they train a separate score network Sθm for each\nsubset corresponding to a specific class Xm without altering\nthe diffusion or denoising processes. SOS introduces two\nmethods for generating new instances in the target minority\nclass: 1) Style-transfer generation: A sample x0 is first drawn\nfrom the major class, converted into a noised instance xT using\nthe shared diffusion process, and then denoised into a sample\nin the target class (ˆx0) using the specific denoising process of\nthe target class Sθtarget; and 2) Plan generation: A noise sample\nxT is drawn from a Gaussian distribution and denoised into a\nsample in the target class (ˆx0) using the denoising process of\nthe target class Sθtarget.\n\nMANUSCRIPT SUBMITTED TO IEEE FOR POSSIBLE PUBLICATION\n8\nBesides, they propose a fine-tuning method to prevent the\nreverse SDE process from moving too far into the “gray\nregion,” where instances from different classes overlap. How-\never, their method primarily focuses on numerical features and\ndoes not explicitly handle categorical features. The model is\nevaluated with respect to machine learning utility.\nSimilar to SOS, Na¨ıve-STaSy [35] attempts to directly\nextend SGMs via SDE for tabular data but encounters training\ndifficulties. To address these challenges and enhance training\nstability, STaSy [35] incorporates self-paced learning, which\ngradually trains the model from easier to more complex sam-\nples, and fine-tuning techniques that adjust model parameters\nincrementally. Specifically, for self-paced learning, they define\nthe denoising score matching loss for the i-th sample ix as\nfollows (based on Eq. 14):\nli = EtEixt\n\u0002\nλ(t)∥Sθ(ixt, t) −∇ixt log p(ixt| ix0)∥2\n2\n\u0003\n,\n(19)\nand define the objective (based on self-paced learning) as:\nmin\nθ,v\nN\nX\ni=1\nvili + r(v; α, β),\n(20)\nwhere r(·) is a self-paced regularizer and α and β are its hy-\nperparameters. To solve the reverse SDE process, they utilize\nthe probability flow framework. Next, as the probability flow\nframework enables the computation of exact log-probability,\nthey further fine-tune θ with Eq. 19 based on the exact log-\nprobability.\nTo handle mixed feature types, they apply one-hot encoding\nfor preprocessing categorical columns and use a softmax func-\ntion followed by rounding for post-processing. For numerical\ncolumns, they employ a min-max scaler for preprocessing and\nits inverse transformation for post-processing. On this basis,\nthey employ a single diffusion model (namely SDEs) for all\nfeatures. However, their method has certain limitations: 1) it is\nslower compared to GAN-based methods; 2) SGMs are known\nto be unstable for high-dimensional data (e.g., high-resolution\nimages), which may pose challenges for high-dimensional\ntabular datasets. They evaluate their approach using metrics\nsuch as machine learning utility, log probability, and diversity.\nUnlike STaSy, which uses a single diffusion model for\nall feature types, TabDDPM [8] employs Gaussian diffusion\n(DDPM) for numerical features after applying quantile trans-\nformations and multinomial diffusion with one-hot encoding\nfor categorical features. These two processes are modeled\nindependently, treating numerical and categorical features\nseparately. Moreover, each categorical feature is considered\nindependent, and a distinct forward diffusion process is applied\nto each one. The model is trained by minimizing the following\nloss function:\nLTabDDPM = LGauss\nsimple +\nPC\ni LMult\ni\nC\n(21)\nwhere LGauss\nsimple represents the simplified loss function for Gaus-\nsian diffusion (refer to Eq. 6), and LMult\ni\ndenotes the loss\nfunction for the multinomial diffusion model of the i-th\ncategorical feature (refer to Eq. 3 for LVUM), with C being the\ntotal number of categorical features. For conditional synthesis,\nthey adopt two approaches: 1) for classification datasets, they\nemploy a class-conditional model to learn pθ(xt−1|xt, y),\nwhere y is the label scalar; and 2) for regression datasets,\nthey treat the target column as an additional numerical feature\nand learn the joint distribution among all numerical features.\nAs a result, TabDDPM may struggle to capture feature\ncorrelations effectively, including: 1) correlations between\nnumerical and categorical features, 2) correlations among\ncategorical features, and 3) correlations between categorical\nfeatures and the target column in regression datasets. The\nmodel’s performance is evaluated based on machine learning\nutility, fidelity, and privacy.\nWhile STaSy [35] handles categorical features through one-\nhot encoding and perform sampling in the continuous space,\nthis approach can lead to suboptimal performance due to\npotential sampling errors. Additionally, treating categorical\nfeatures as continuous variables may hinder the accurate\nmodeling of correlations between numerical and categorical\nfeatures. To address these limitations, CoDi [37] proposes\nto handle numerical and categorical features separately using\ntwo distinct diffusion models, similar to TabDDPM [8]. Un-\nlike TabDDPM, however, CoDi conditions the two diffusion\nmodels on each other, enabling them to co-evolve during\ntraining to better capture cross-feature correlations. To further\nstrengthen the connection between the models, CoDi employs\na contrastive learning approach with negative sampling.\nSimilar to STaSy, they use a min-max scaler to preprocess\nnumerical features and apply one-hot encoding to categorical\nfeatures before utilizing the diffusion models. However, CoDi\nis specifically tailored for datasets containing both numerical\nand categorical features, making it unsuitable for datasets with\nonly numerical or only categorical features. The model is\nevaluated based on machine learning utility and diversity.\nAutoDiff [36] addresses the challenge of heterogeneous\nfeatures by integrating an autoencoder with a score-based\ndiffusion model. Specifically, the autoencoder is employed\nto map the original heterogeneous features into a continuous\nlatent space, where the resulting embeddings serve as input\nfor the forward diffusion process. After the reverse diffusion\nprocess is completed, the output of the diffusion model is\npassed through the autoencoder’s decoder to reconstruct sam-\nples in the original heterogeneous feature space. To handle\nthe issue of mixed-type features, they introduce a dummy\nvariable that encodes the frequency of repeated values within\nsuch features. Additionally, they mitigate the challenge of\nfeature correlations by learning the joint distribution of feature\nembeddings in the continuous latent space. Building on this\napproach, they propose two models: STaSy-AutoDiff, which\nadapts the framework of STaSy, and TabDDPM-AutoDiff,\nwhich builds on TabDDPM, each utilizing AutoDiff as the\nunderlying diffusion model.\nThey evaluate model performance across three key criteria:\nfidelity, machine learning utility, and privacy. In particular,\ntheir models exhibit weaker privacy guarantees compared to\nother approaches. They attribute this to two factors: the ten-\ndency of a highly sophisticated autoencoder to overfit the input\ndata, and the inherent memorization behavior of diffusion\nmodels [91]. Additionally, they note that while TabDDPM\n\nMANUSCRIPT SUBMITTED TO IEEE FOR POSSIBLE PUBLICATION\n9\ndemonstrates high fidelity for individual features, it fails to\neffectively capture the correlation structures among features.\nThe vanilla diffusion models typically require complete\n(fully observed) training data, while tabular data often suffers\nfrom missing values. Ouyang et al. [79] observe that the\n“impute-then generate” pipeline may lead to learning bias\ndue to the fact that single imputation cannot capture the\ndata variability. To train a diffusion model on data with\nmissing values, they propose MissDiff. This is the first tabular\ngenerative model that can handle mixed-type features with\nmissing values, where the missing values are directly used in\nthe training without prior imputations. Specifically, MissDiff\nattempts to directly incorporate the uncertainty of missing\nvalues into the learning process as follows: it masks the\nregression loss of Denoising Score Matching, and finds the\noptimal parameters θ by optimizing the following objective:\nT\n2 Et{λ(t)Ep(xobs\n0 ,m)Ep(xobs\nt |xobs\n0 )Lmiss},\n(22)\nwith Lmiss := ∥Sθ(xobs\nt , t) −∇xobs\nt log p(xobs\nt |xobs\n0 ) ⊙m∥2\n2, and\nm = I{xobs\n0\n= “na”} denotes the missing entries, while\nthe meanings of the other symbols remain consistent with\nthose defined in Section III-A4. Specifically, to ensure that the\nfunction p(·) and its corresponding score function are well-\ndefined, they replace “na” in numerical features with 0 and\nintroduce a separate category for “na” in categorical features.\nTo handle mixed-type features, they adopt the same techniques\nas STaSy [35] and CoDi [35], applying min-max scaling\nand its inverse during generation for numerical features, and\nusing one-hot encoding for categorical features. For categorical\nfeature generation, they apply a softmax function followed by\na rounding operation. The model’s quality is evaluated based\non fidelity and machine learning utility.\nTabSyn [38] claims to be the first to explore the application\nof latent diffusion models for tabular data synthesis (although\nAutoDiff [36] also addressed this earlier). The authors empha-\nsize key challenges when applying diffusion models directly\nto the original data space with mixed feature types: 1) simple\nencoding strategies, such as one-hot encoding and analog bit\nencoding, often lead to suboptimal performance; and 2) em-\nploying separate models for different feature types hampers the\nability to capture cross-feature correlations. To overcome these\nlimitations, TabSyn proposes a diffusion model that operates\nin a joint latent space where both numerical and categorical\nfeatures are transformed into continuous embeddings, enabling\nthe model to effectively capture inter-feature dependencies.\nThis approach allows TabSyn to handle a wide range of data\ntypes while preserving their inherent correlations. The pipeline\nof TabSyn is as follows:\n1) They first utilize a VAE to transform raw numerical\nand categorical features into continuous embeddings.\nSpecifically, they learn a unique tokenizer for each fea-\nture (after applying one-hot encoding to each categorical\nfeature) and feed the feature-wise embeddings into a\nVAE (with Transformer-based encoder and decoder) to\ncapture correlations among features.\n2) Once the VAE model is well-trained, they train a score-\nbased diffusion model using SDEs with Gaussian noise\nin the latent space.\n3) Finally, they learn a detokenizer to recover real feature\nvalues from the embeddings.\nNote that TabSyn lacks the capability to handle miss-\ning values directly. It requires data preprocessing, including\nhandling missing values and feature transformations, which\nmay introduce bias or noise. The model’s performance is\nevaluated based on fidelity, utility, diversity, and privacy.\nNotably, ablation studies reveal that substituting the VAE with\nsimple one-hot encoding for categorical features results in\nthe poorest performance, indicating that treating categorical\nfeatures as continuous is inadequate. Furthermore, their results\ndemonstrate that TabSyn-DDPM operating in the latent space\noutperforms TabDDPM in the original data space, under-\nscoring the significance of learning latent embeddings to\nenhance diffusion modeling. However, note that TabSyn can be\nmodified for missing data imputation by drawing an analogy\nto image inpainting (REPAINT [92]), where missing parts of\nan image are restored.\nRather than relying on neural networks, which are com-\nmonly used as universal function approximators [93] to esti-\nmate Sθ(·) or ϵθ(·) in diffusion models, Forest-Diffusion [39]\nemploys XGBoost [94], leveraging its strength in tabular data\nprediction and classification. Importantly, XGBoost can natu-\nrally handle missing data by learning the best split, allowing\nthe model to be trained directly on incomplete datasets—an\nadvantage over most generative models that require com-\nplete data. For categorical data, dummy encoding is applied\nduring pre-processing, and dummy variables are rounded to\nthe nearest class during post-processing. Specifically, Forest-\nDiffusion adopts the variance-preserving formulation of score-\nbased generative models (see Section III-A4). To estimate\nscores using XGBoost instead of neural networks, the method\ninvolves four key steps:\n1) The original dataset X (size [N, D]) is duplicated nnoise\ntimes, corresponding to the number of discretized noise\nlevels.\n2) Different noise levels are added to each duplicated\ndataset, meaning each sample x receives nnoise noise\nvariants.\n3) Linear interpolations between the original dataset and\nnoise are computed for various time points t: Xi(t) =\ntX +(1−t)Zi,\n∀t ∈[t1, . . . ,tnt],\ni ∈[1, . . . , nnoise],\nwhere Zi represents the noise matrix for the i-th dupli-\ncation.\n4) An XGBoost model is trained per noise level t, predict-\ning the score S(t) := ∇xt log pt(xt|x0), resulting in nt\nmodels.\nFor data imputation, Forest-Diffusion draws an analogy\nto image inpainting, where missing parts of an image are\nrestored. They adopt REPAINT [92], noting that while image\ninpainting models are trained on complete images, tabular\nimputation models operate on incomplete data, which XG-\nBoost can naturally handle. For conditional synthesis, they\ntrain a separate XGBoost model for each label in classification\n\nMANUSCRIPT SUBMITTED TO IEEE FOR POSSIBLE PUBLICATION\n10\ntasks. In regression tasks, they follow a strategy similar to\nTabDDPM by treating the target column as an additional\nnumerical feature. The quality of Forest-Diffusion is evaluated\nin terms of fidelity, diversity, machine learning utility, and\nstatistical inference.\nTabDiff [73] identifies several limitations in prior diffusion\nmethods for tabular data: 1) additional encoding overhead in\nlatent diffusion models like TabCSDI [68] and TabSyn [38]; 2)\nimperfect discrete-time diffusion modeling approaches, such\nas those used in TabDDPM [8] and CoDi [37]; and 3)\ninsufficient handling of feature-wise distribution heterogeneity\nin multi-modal frameworks. To address these issues, TabDiff\nemploys a continuous-time diffusion framework with the fol-\nlowing key steps:\n1) Numerical features are normalized, and categorical fea-\ntures are transformed using one-hot encoding, with an\nadditional [MASK] class.\n2) A joint diffusion process is proposed, where noise\nschedules for each feature are learnable.\n3) Denoising is performed simultaneously for all features\nusing a single model, followed by inverse transforma-\ntions to recover the original format.\nDespite claiming a “joint” diffusion process, TabDiff ac-\ntually employs separate diffusion models for numerical and\ncategorical features: 1) A single SDEs-based (with VE for-\nmulation, see Section III-A4) diffusion model is used for all\nnumerical features; and 2) For each categorical feature, a sep-\narate masking diffusion model (detailed in Section VIII-A4)\nis employed. Moreover, to better capture individual feature\ndistributions, TabDiff introduces feature-specific noise sched-\nules: 1) A power mean numerical schedule σnum\nρi (t), where\nρi is a learnable parameter for the i-th numerical feature;\nand 2) A log-linear categorical schedule σcat\nκj(t), where κj\nis a learnable parameter for the j-th categorical feature.\nThe model’s performance is evaluated using fidelity, machine\nlearning utility, and privacy metrics.\nTabUnite [89] identifies feature heterogeneity as a key chal-\nlenge in tabular data generation, where datasets comprise both\nnumerical and categorical features, often interrelated—e.g., a\nperson’s numerical salary may be linked to their categorical\neducation level and age. The authors propose that addressing\nthis challenge requires effective encoding schemes for pre-\nprocessing input features before applying tabular generative\nmodels, such as diffusion models, which typically depend\non continuous transformations for denoising score matching.\nThey highlight several limitations of existing approaches: 1)\nseparate generative processes for numerical and categorical\nfeatures hinder the modeling of cross-feature correlations; 2)\nsuboptimal encoding heuristics for categorical features—such\nas one-hot encoding—result in sparse high-dimensional repre-\nsentations [95], potentially causing underfitting in generative\nmodels [96]; and 3) learned latent embeddings are often\nparameter inefficient.\nTo address these challenges, they utilize Quantile Trans-\nformer for continuous feature encoding, while categorical fea-\ntures are processed using analog bits encoding [97], PSK en-\ncoding, or dictionary encoding. They integrate these encodings\ninto a unified data space and apply a single diffusion or flow\nmodel to generate high-quality tabular data. Notably, PSK and\ndictionary encodings, proposed by the authors, aim to provide\nmore efficient and compact representations while preserving\nfeature correlations. The model’s performance is evaluated\nbased on fidelity, machine learning utility, and privacy.\nCDTD [90] points out that existing diffusion models that\ncan handle both numerical and categorical features, such as\nSTaSy [35], CoDi [37], TabDDPM [8] and Forest-Diffusion\n[39], are built upon the advances from the image domain. In\nother words, the noise schedules are not specifically designed\nto handle mixed features in tabular data, where different\nfeatures have different characteristics. As a result, the noise\nschedules may not be directly transferable from the modalities\nof images and/or texts to tabular data, and these models\nmay suffer from the following limitations: 1) the diffusion\nprocesses and their loss for different types of features are not\naligned or balanced; as a result, models that simply combine\ndifferent diffusion losses (for the numerical and categorical\nfeatures) may implicitly favor the synthesis of some features\nor feature types over others [98]; 2) these methods do not scale\nto large datasets; and 3) these methods do not perform well\non categorical features with a large number of categories.\nTo address these challenges, CDTD enhances diffusion\nmodels for tabular data by improving noise schedules and\ndiffusion losses. Specifically, they integrate score matching\n[99] and score interpolation [100], embedding categorical\nfeatures into a continuous space and applying a unified\nGaussian diffusion process. This enables better modeling of\nfeature correlations. Three noise scheduling strategies are\nexplored: 1) a single learnable noise schedule for all features,\n2) separate schedules for numerical and categorical features,\nand 3) individual schedules per feature. To further refine\nmixed-feature handling, they introduce diffusion-specific loss\nnormalization and improved model initialization. Using em-\nbeddings instead of one-hot encoding allows scaling to high-\ncardinality categorical features, while the ODE formulation\naccelerates sampling. Evaluations based on machine learning\nutility, fidelity, and privacy demonstrate that CDTD effectively\ncaptures feature correlations, with learnable noise schedules\nsignificantly enhancing synthetic data quality.\n2) Diffusion Models for Single Table Synthesis in Health-\ncare Domain: In this part, we review diffusion models for\nsingle table synthesis in the healthcare domain, with a pri-\nmary focus on Electronic Health Records (EHRs) data. EHRs\nrepresent a digital collection of patient health information,\nencompassing a wide variety of data types, including de-\nmographics, medical history, diagnoses, medications, lab test\nresults, and treatment outcomes. EHR data is crucial for\nadvancing medical research, enabling predictive modeling, and\nimproving healthcare decision-making [101]. However, due to\nthe sensitive nature of the information, directly sharing EHR\ndata poses significant privacy risks. In this context, synthetic\nEHR data generated by advanced models, such as diffusion\nmodels, can serve as a privacy-preserving alternative [88],\nallowing researchers to conduct exploratory analyses, model\ndevelopment, and validation without compromising patient\nconfidentiality. In recent years, diffusion models have emerged\nas a promising approach for generating high-quality synthetic\n\nMANUSCRIPT SUBMITTED TO IEEE FOR POSSIBLE PUBLICATION\n11\nEHR data, which will be reviewed below.\nMedDiff [82] is the first to employ diffusion model for\ngenerating EHRs. Building on the DDIM framework [102],\nMedDiff accelerates the generation process using Ander-\nson acceleration [103], a numerical method that improves\nthe convergence speed of fixed-point iterations. Furthermore,\nit enables conditional data generation by incorporating the\nclassifier-guided sampling technique [65]. Specifically, the\nconditional noise prediction is formulated as: ˆϵθ(xt, t, y) =\nϵθ(xt, t) −√1 −¯αt∇xt log fϕ(y | xt), where ϵθ(xt, t) is the\nnoise estimator of the unconditional diffusion model, and fϕ(·)\nrepresents the trained classifier. However, MedDiff is limited\nto handling continuous features, while EHR data often con-\ntains both numerical (e.g., blood test results) and categorical\n(e.g., gender, ethnicity) attributes. The model’s performance\nis evaluated based on fidelity and machine learning utility.\nMeanwhile, rather than relying on DDIM, EHR-TabDDPM\n[83] is the first to utilize DDPM for generating mixed-type\ntabular EHR data. This approach directly applies TabDDPM\n[8] to synthesize EHR datasets. The model’s quality is assessed\nbased on fidelity, machine learning utility, and privacy.\nDPM-EHR [80] highlights the limitations of GANs, such as\nunstable training and mode collapse, which lead to suboptimal\nquality and diversity in synthetic data. To overcome these\nissues, they are among the first to employ diffusion models for\ngenerating longitudinal Electronic Health Records (EHR) with\nmixed-type features. For categorical features, they utilize one-\nhot encoding followed by the direct application of Gaussian\ndiffusion models, in contrast to other approaches (which will\nbe introduced later) that often adopt multinomial diffusion\nmodels. The quality of their method is assessed based on\nfidelity, diversity, privacy, and machine learning utility.\nHe et al. [84] highlight that existing diffusion-based EHR\nmodels either handle numerical and categorical features sep-\narately or lack the capability to generate categorical features\naltogether. In clinical datasets collected from hospitals, numer-\nical features (e.g., respiration rate) and categorical features\n(e.g., diagnosis and admission type) often exhibit inherent\nlogical relationships. However, models such as TabDDPM\nface challenges in accurately capturing and representing these\nrelationships, as they rely on separate diffusion models for\nnumerical and categorical data. Moreover, they indicate two\nkey challenges in generating EHR data: 1) EHRs typically\ncomprise both static tabular measurements and temporal lon-\ngitudinal records, resulting in heterogeneous features, and\n2) EHRs often suffer from missing data. They observe that\nexisting diffusion models struggle to maintain performance\nwhen handling missing modalities in such heterogeneous data.\nTo overcome these limitations, they propose FlexGen-EHR,\na model specifically designed to handle missing modalities\nwhile generating both static tabular and temporal longitudinal\nrecords simultaneously. Unlike most prior diffusion models\nthat assume complete training data, this work addresses the\nmore realistic scenario of NMCAR (Not Missing Completely\nAt Random), where feature values may be missing for certain\nmodalities across records. To handle this challenge, they\nintroduce an optimal transport module that aligns and em-\nphasizes a common feature space within the heterogeneous\nstructure of EHRs. The FlexGen-EHR framework operates as\nfollows: 1) an LSTM-based encoder processes temporal fea-\ntures, mapping them into a latent space, while an MLP-based\nencoder processes static features into a separate latent space; 2)\nthese latent representations are concatenated and aligned using\nthe optimal transport module to enhance coherence across\nmodalities; 3) a latent diffusion model is applied in the unified\nlatent space; and 4) two decoders are used to reconstruct the\ndenoised representations back into temporal and static features,\nrespectively. The proposed model is evaluated in terms of\nfidelity, machine learning utility, and privacy.\nEHRDiff [85] focuses on unconditional EHR data genera-\ntion, without explicitly modeling conditional temporal struc-\ntures. Specifically, it represents all features—categorical and\ncontinuous—as real values normalized to the range [0, 1],\napplying one-hot encoding for categorical features and nor-\nmalization for continuous ones. Using this representation,\nEHRDiff employs SDEs based diffusion models, while adapt-\ning the reparameterization approach from [104]. The reverse\ndenoising process is formulated through probability flow or-\ndinary differential equations. The model’s performance is\nassessed using fidelity, machine learning utility, and privacy.\nEHR-D3PM [88] introduces a guided diffusion model for\ngenerating discrete tabular medical codes in EHRs, supporting\nboth conditional and unconditional generation. Specifically, in\nthe context of categorical EHRs, the goal is to generate a\nsequence x(all) := [x(1), . . . , x(L)], where each x(l) represents\nthe one-hot encoding of a categorical feature (i.e., a token). For\nthe unconditional diffusion process, they utilize a D3PM [32]\nwith a multinomial distribution, adding a multinomial noise\nvector qnoise independently to each token. Consequently, the\ndiffusion process for the sequence is defined as:\nq(x(all)\nt\n| x(all)\nt−1) =\nL\nY\nl=1\nCat\n\u0010\nx(l)\nt ; βtx(l)\nt−1 + (1 −βt)qnoise\n\u0011\n,\n(23)\nwith a corresponding reverse denoising process.\nFor conditional generation, they propose a training-free\nconditional generator: pθ(x(all) | c) ∝pθ(x(all)) · p(c | x(all)),\nwhere pθ(·) represents an unconditional EHR generator, and\np(c | x(all)) is a classifier that approximates the true (but\nunknown) classifier qdata(c | x(all)). Additionally, they custom-\ndesign the energy function and apply energy-guided Langevin\ndynamics at the latent layer of the predictor network to\nperform sampling. The model’s quality is evaluated in terms\nof fidelity, machine learning utility, and privacy.\n3) Diffusion Models for Single Table Synthesis in Finance\nDomain: In this part, we review diffusion models for single\ntable synthesis in the finance domain, where the generation\nof realistic tabular data is critical. Financial data typically\nconsists of highly sensitive and heterogeneous information,\nsuch as transaction records, account balances, credit histories,\nand market data. The ability to generate high-quality synthetic\nfinancial data has significant implications for various appli-\ncations, including risk modeling, fraud detection, algorithmic\ntrading, and privacy-preserving data sharing [12]. Recent ad-\nvancements in diffusion-based financial data synthesis will be\nreviewed below.\n\nMANUSCRIPT SUBMITTED TO IEEE FOR POSSIBLE PUBLICATION\n12\nFinDiff [81] targets the synthesis of financial tabular data\nwith mixed-type features. The framework comprises four key\nstages: 1) Data Pre-processing: Each categorical feature is\ntransformed into a continuous embedding vector, ensuring that\nsimilar categories are positioned closely in the embedding\nspace. These embeddings replace the original categorical val-\nues. Meanwhile, numerical features undergo only normaliza-\ntion without embedding; 2) Embeddings Assembly: The nor-\nmalized numerical features, categorical feature embeddings,\ntime embeddings, and label embeddings are concatenated\nto form a unified input representation; 3) Model Training\nand Sampling: A DDPM model is directly applied to the\ncontinuous embedding space for training and generation; and\n4) Post-processing: The denoised sample is mapped back\nto the original space. Categorical features are restored by\nassigning each to the closest category in the embedding space,\nwhile numerical features are denormalized. Particularly, by\nembedding categorical features in a continuous space, FinDiff\ncaptures latent structures and correlations among categorical\nvariables. However, this method may be less effective if the\nsemantic meaning of categories is ambiguous or unavailable.\nThe model’s performance is assessed in terms of fidelity,\nmachine learning utility, and privacy.\nEntTabDiff [86] highlights a critical limitation of prior\ntabular generative models: they often overlook the importance\nof a special column in financial tabular data that represents\nentities, such as companies or individuals. These models\ntypically treat the entity column as a standard categorical\nfeature, leading to several drawbacks: 1) increased privacy\nrisks, 2) inability to generate synthetic data conditioned on\nspecific entities, and 3) restriction to generating data only\nfor entities present in the original dataset. To address these\nchallenges, EntTabDiff introduces a novel approach for gener-\nating financial tabular data conditioned on entities. The method\nconsists of two key components: 1) learning the distribution of\nentities in the training data to generate synthetic entities, and 2)\ngenerating tabular data conditioned on these synthetic entities,\nwhich is achieved by employing a cross-attention mechanism.\nFor pre-processing, it follows the FinDiff approach [81] by\nembedding categorical features and normalizing numerical\nones. The model’s performance is evaluated across fidelity,\nmachine learning utility, and privacy.\nImb-FinDiff [87] is the first attempt to employ diffusion\nmodels to generate financial tabular data that explicitly ad-\ndresses the class imbalance challenge. Imb-FinDiff is an exten-\nsion of FinDiff [81], consisting of three modules: embedding\nmodule, diffusion module, and prediction module.\nFirst, the embedding module includes four sub-modules:\n1) categorical feature embedding that converts individual\ncategorical feature into continuous embeddings, and Gaussian\nforward diffusion is performed on the obtained embeddings;\n2) numerical feature embedding that transforms individual\nnumerical feature into continuous embeddings, and Gaussian\nforward diffusion is performed on the obtained embeddings; 3)\ndiffusion timestep embedding that employs positional encod-\nings [20] to transform timestep into continuous embedding;\nand 4) data class embedding that embeds class labels into\ncontinuous embeddings by following TabDDPM [8].\nSecond, the denoising diffusion module aims to predict the\nadded noises and class labels in three stages: 1) embedding\nprojection that projects the embeddings of categorical fea-\ntures, numerical features, timestep, and class labels from their\noriginal embedding spaces into a joint embedding space; 2)\nembedding synthesis that constructs a combined vector of\nthe projected embeddings of categorical features, numerical\nfeatures, timestep, and class labels in the joint embedding\nspace; 3) prediction de-embedding that projects the combined\nvector back to their original embedding spaces with two\nprojection head functions, one for predicting added noises and\nanother for predicting class labels. These two functions will\nbe processed by the third module as follows.\nThe prediction module consists of two objectives: 1)\ntimestep noise loss that ensures accurate denoising by comput-\ning the mean-squared errors between the ground truth noises\nand the predicted noises; and 2) class label loss that ensures\nclass-specific accuracy by measuring the mean-squared errors\nbetween the ground truth labels and the predicted labels. By\nincorporating the class label loss, Imb-FinDiff enhances its\ncapability of generating minority classes samples. The model\nis evaluated using fidelity and machine learning utility.\nB. Diffusion Models for Multi-relational Data Synthesis\nMost tabular data synthesis methods focus on single-table\ngeneration; however, real-world datasets often consist of multi-\nple interconnected tables, highlighting the importance of multi-\nrelational data synthesis—a challenge that remains largely\nunderexplored. In this subsection, we first present the notation\nand problem definition. Then, we review related works in\nchronological order, with a summary provided in Table III.\nTABLE III\nOVERVIEW OF DIFFUSION MODELS FOR MULTI-RELATIONAL DATA\nSYNTHESIS\nModel Name\nYear\nVenue\nFeature Type\nDomain\nClavaDDPM [105]\n2024\nNeurIPS\nNum by default\nGeneric\nGNN-TabSyn [106]\n2024\nNeurIPSW\nNum+Cat\nGeneric\nIn addition to capturing the characteristics of individual\ntables, multi-relational data synthesis requires modeling re-\nlationships across multiple tables that are connected (i.e.,\nconstrained by foreign keys). A multi-relational database R\nconsists of P tables {R1, ..., RP }. Each table Rp has primary\nkey which is the unique identifiers of rows. Given two tables\nRp and Rq, we say Rp refers to Rq (or Rp has a foreign key\nconstraint with Rq, or Rp has a parent-child relationship with\nRq where Rq is the parent and Rp is the child) if Rp has a\nfeature (called foreign key) that refers to the primary key of\nRq. With these notations, the multi-relational data synthesis\nproblem can be defined as follows [105]:\nProblem 2 (Multi-relational Data Synthesis). Given R =\n{R1, ..., RP }, we aim to generate a synthetic database ˜R =\n{ ˜R1, ..., ˜RP } that preserves the structure, foreign-key con-\nstraints, as well as correlations among features (including\ninter-column correlations with the same table, inter-table cor-\n\nMANUSCRIPT SUBMITTED TO IEEE FOR POSSIBLE PUBLICATION\n13\nrelations, and intra-group correlations with the same foreign\nkey group).\nRather than focusing on single-table synthesis, ClavaD-\nDPM [105] addresses the generation of multi-relational\ndatabases comprising multiple interconnected tables. The au-\nthors identify key limitations in existing approaches, partic-\nularly regarding scalability for large datasets and the ability\nto capture long-range dependencies (e.g., correlations across\nfeatures in different tables). To overcome these challenges,\nClavaDDPM introduces clustering labels as intermediaries to\nmodel inter-table relationships. Built upon TabDDPM [8] as\nits backbone, ClavaDDPM differs by employing a unified\nDDPM model for handling both numerical and categorical\nfeatures, aiming to reduce computational overhead. Unlike\nTabDDPM, which uses separate diffusion models for different\nfeature types, ClavaDDPM converts categorical features into a\ncontinuous space via label encoding (i.e., mapping categories\nto integer values) and processes all features within a single\ndiffusion framework. Additionally, it adopts classifier-guided\nsampling [65] to enhance sample generation.\nClavaDDPM’s performance is evaluated using both multi-\ntable and single-table metrics. Multi-table metrics include\ncardinality for intra-group correlation, column-wise density\nestimation, pair-wise column correlation, and average 2-way\ndependency to assess both short- and long-range correlations.\nSingle-table metrics include α-precision, β-recall, and ma-\nchine learning utility.\nGNN-TabSyn [106] highlights a key limitation of existing\ndata synthesis methods: the lack of explicit modeling for the\ntopological structure of relational databases, which hampers\ntheir ability to capture inter-column dependencies across ta-\nbles. To address this, the authors represent a relational database\nas a heterogeneous graph, where nodes correspond to rows,\nand edges represent foreign key relationships connecting these\nrows, with both nodes and edges assigned specific types. This\napproach enables the modeling of table relationships induced\nby foreign key constraints. They employ graph neural networks\n(GNNs) to learn latent embeddings from this heterogeneous\ngraph, effectively capturing both the structural information and\ninter-table dependencies. Building on these embeddings, they\nextend the latent tabular diffusion model, TabSyn [38], to sup-\nport conditional generation by incorporating the GNN-derived\nembeddings as conditions to guide the diffusion process. This\nintegration allows the model to generate data that reflects\ncomplex relational structures. However, a noted limitation\nof GNN-TabSyn is its inability to synthesize unseen graph\nstructures. The model’s performance is evaluated using multi-\ntable fidelity (via the DDA metric [107]), privacy, and machine\nlearning utility.\nV. DIFFUSION MODELS FOR DATA IMPUTATION\nMissing values imputation is a long-standing research prob-\nlem in data mining and machine learning community [108].\nThese approaches can be divided into two categories [109]: 1)\niterative approaches that estimate the conditional distribution\nof one feature based on other features (e.g., MICE[110]); and\n2) deep generative approaches that train a generative model to\ngenerate values in missing parts based on observed parts (e.g.,\nMIDA [111] and MIWAE [9] based on denoisng AEs, HIVAE\n[112] based on VAEs, and GAIN [15] based on GANs). We\nformally define the missing data imputation problem below.\nProblem 3 (Tabular Data Imputation). Given a D-dimensional\ntraining dataset X = {ix}N\ni=1, a feature j ∈{1, ..., D} of ix\nis denoted as j\nix, where the j-th feature can be numerical\nor categorical. Besides, any feature j can suffer from missing\nvalues. Let X = (R∪∅)D be the input space, where R denotes\nthe real number space for a numerical feature. For brevity,\nwe also utilize R to denote the corresponding range if it is a\ncategorical feature. Data imputation aims to find a function\nf(·): X →RD which can replace the missing values with\nreasonable values.\nTABLE IV\nOVERVIEW OF DIFFUSION MODELS FOR DATA IMPUTATION\nModel Name\nYear\nVenue\nFeature Type\nDomain\nTabCSDI [68]\n2022\nNeurIPSW\nNum+Cat\nGeneric\nTabDiff [73]\n2024\nNeurIPSW\nNum+Cat\nGeneric\nSimpDM [113]\n2024\nCIKM\nNum+Cat\nGeneric\nMTabGen [114]\n2024\nArXiv\nNum+Cat\nGeneric\nDDPM-Perlin [115]\n2024\nKBS\nNum\nGeneric\nNewImp [116]\n2024\nNeurIPS\nNum\nGeneric\nDiffPuter [117]\n2024\nOpenReview\nNum+Cat\nGeneric\nTabCSDI [68] is the first to apply diffusion models for\nmissing value imputation in tabular data. TabCSDI is an\nextension of CSDI [118], a method initially designed for time\nseries data that lacks direct support for categorical features.\nThe core idea of CSDI involves partitioning the input x into\nan observed part (conditioning set xcon) and an unobserved\npart (target set xtar). Instead of unconditionally generating the\nentire input, a conditional diffusion model is trained using\nthe observed part as a condition. The generative (denoising)\nprocess is defined as:\npθ(xtar\nt−1|xtar\nt , xcon\n0 ) = N(xtar\nt−1; µθ(xtar\nt , t|xcon\n0 ), σI),\n(24)\nwhere the goal is to learn µθ(·) to predict the missing values.\nTo enable simultaneous handling of numerical and cate-\ngorical features, three encoding techniques are explored for\ncategorical features: one-hot encoding, analog bits encoding\n[97], and feature tokenization [119]. The diffusion process of\nCSDI is then performed on the pre-processed input z0 rather\nthan the raw input x0, resulting in zT . The denoising process\nis subsequently applied to obtain ˆz0, and the final imputed\nvalues ˆx0 are recovered from ˆz0 using appropriate decoding\ntechniques corresponding to the chosen encoding methods.\nThe quality of imputation is evaluated in terms of machine\nlearning utility.\nTabDiff [73], originally proposed for data synthesis (see\nSection IV-A), can also be adapted for missing value impu-\ntation by employing classifier-free guidance for conditional\nimputation. Let y = {[ynum, ycat]} denote the set of observed\nfeatures used as conditions, and x = {[xnum, xcat]} represent\nthe set of features with missing values to be imputed. TabDiff\nis extended for conditional generation by fixing the observed\n\nMANUSCRIPT SUBMITTED TO IEEE FOR POSSIBLE PUBLICATION\n14\nfeatures yt as y while denoising only xt during the reverse\nprocess. For numerical features, the imputation of xnum\n0\nis\nachieved through an interpolation of the conditional and un-\nconditional estimates in DDPM:\nϵθ(xnum\nt\n, t) = (1 −ω)ϵθ(xnum\nt\n, y, t) + ωϵθ(xnum\nt\n, t).\n(25)\nFor categorical features, the imputed values xcat\n0 are predicted\nusing a weighted interpolation of the conditional and uncon-\nditional probabilities:\nlog ˜pθ(xcat\ns | xcat\nt , y) = (1 + ω) log pθ(xcat\ns | xcat\nt , y)\n+ ω log pθ(xcat\ns | xcat\nt ),\n(26)\nwhere pθ(xcat\ns\n| xcat\nt ) is defined as in the Masking Diffusion\nModel (see Eq. 35).\nLiu et al. [113] identify that vanilla diffusion models are\nsub-optimal for tabular data imputation due to two key mis-\nmatches between the imputation task and other tasks where\ndiffusion models have shown success. First, there is a mis-\nmatch in learning objectives: in generation tasks, diversity is\na critical goal, making diffusion models sensitive to the initial\nnoise xT , where varying xT results in diverse generated sam-\nples ˆx0. However, in imputation tasks, accuracy is paramount,\nrequiring the imputed values to closely match the ground truth.\nThis sensitivity to initial noise, while promoting diversity in\ngeneration, can harm accuracy in imputation. Second, there\nis a data scale mismatch: unlike image and text domains,\nwhich typically have tens of thousands to millions of training\nsamples, tabular datasets often consist of only a few thousand\nsamples. This limited data scale can hinder the diffusion\nmodel’s ability to fully capture the underlying tabular data\nmanifold, resulting in sub-optimal imputation performance.\nTo address these mismatch issues, they propose SimpDM,\nwhich incorporates self-supervised techniques. To tackle the\nobjective mismatch, a self-supervised alignment mechanism is\nintroduced to regularize the diffusion model’s output, ensuring\nconsistent and accurate imputations for the same observed\ndata. To mitigate the data scale mismatch, they adopt a\nperturbation-based data augmentation strategy to enhance the\ntraining set. Specifically, for handling tabular data with both\nnumerical and categorical features, they follow the same ap-\nproach as TabDDPM [8], employing joint Gaussian diffusion\nfor numerical features and separate multinomial diffusion\nmodels for categorical features. Consequently, SimpDM inher-\nits similar limitations. They evaluate the model’s quality based\non the RMSE between actual values and imputed values.\nMTabGen [114] extends TabDDPM [8] for handling mixed-\ntype tabular data by employing a Gaussian diffusion model\nfor numerical features and multinomial diffusion models for\ncategorical features. Unlike TabDDPM, MTabGen first applies\na Gaussian quantile transformation to normalize numerical\nfeatures and uses ordinal encoding for categorical features.\nAll heterogeneous features are then projected into a unified\ncontinuous latent space, with each feature’s embedding mod-\neled independently during the diffusion process, where noise\ncomponents are sampled separately for each feature.\nParticularly, MTabGen introduces three key improvements\nover\nprior\ntabular\ndiffusion\nmodels.\nFirst,\nthey\nuse\na\ntransformer-based encoder-decoder as the denoising model (in\ncontrast to existing methods that rely on MLPs) to better\ncapture feature correlations. Second, they enhance the con-\nditioning mechanism by incorporating condition information\ninto the transformer’s attention mechanism. Specifically, they\nreduce learning bias and improve correlation modeling be-\ntween unmasked (observed) and masked (missing) features by\nusing a transformer encoder for unmasked feature embeddings,\nwhich are then fed into a conditioning attention module. In\ncontrast, prior methods such as TabCSDI [68] and TabDDPM\n[8] directly add condition embeddings to masked feature em-\nbeddings. Third, they adopt a unified framework with dynamic\nmasking, enabling the model to handle varying numbers of\nvisible features. Consequently, MTabGen can perform both\ndata synthesis and data imputation within a single framework,\ntreating data synthesis as a special case of data imputation\nwhere all feature values are missing. They evaluate the model’s\nquality in terms of fidelity, machine learning utility, and\nprivacy for data synthesis, and machine learning utility for\ndata imputation.\nSimilar to the seminal work TabCSDI, DDPM-Perlin [115]\nseparates the input x into conditional (observed) part xcon\nand target (missing) part xtar. Based on this, they model the\nreverse imputation process, using the observed part to impute\nthe unobserved part as follows:\npθ(xtar\n0:T |xcon\n0 ) := p(xtar\nT )\nT\nY\nt=1\npθ(xtar\nt−1|xtar\nt , xcon\n0 ),\n(27)\nwith p(xtar\nT ) ∼N(0, I), and pθ(xtar\nt−1|xtar\nt , xcon\n0 ) is defined as:\npθ(xtar\nt−1|xtar\nt , xcon\n0 ) = N(xtar\nt−1; µθ(xtar\nt , t|xcon\n0 ), Σθ(xtar\nt , t|xcon\n0 )).\n(28)\nBy connecting this function to the unconditional DDPM\nmodel, they define:\nµθ(xtar\nt , t|xcon\n0 ) = µDDPM\nθ\n(xtar\nt , t, ϵθ(xtar\nt , t|xcon\n0 ));\nΣθ(xtar\nt , t|xcon\n0 ) = ΣDDPM\nθ\n(xtar\nt , t),\n(29)\nwhere µDDPM\nθ\n(·) and ΣDDPM\nθ\n(·) are the standard DDPM func-\ntions, and ϵθ(·) is a neural network predicting the ground truth\nnoise ϵ. In the standard DDPM model, a clean sample x0 is\ngradually corrupted to a noisy sample xt by adding Gaussian\nnoise:\nxt = √¯αtx0 +\n√\n1 −¯αtϵ,\nϵ ∼N(0, I).\n(30)\nIn this work, they replace Gaussian noise with Perlin noise:\nxt = √¯αtx0 + ϵPerlin(\n√\n1 −¯αt),\n(31)\nwhere the Perlin noise is defined as:\nϵPerlin(x) =\nS\nX\ni=0\npi · noise(2ix),\n(32)\nwith S as the number of steps, pi as the persistence degree at\nstep i, and noise(·) as the noise function.\nHowever, it is important to note that Eq. 31 lacks theoretical\nsupport, as Perlin noise does not possess the additive properties\nof Gaussian noise. Furthermore, this approach is limited to\nnumerical features. They evaluate the model’s performance in\nterms of RMSE under three missing data scenarios: MNAR\n\nMANUSCRIPT SUBMITTED TO IEEE FOR POSSIBLE PUBLICATION\n15\n(missing not at random), MAR (missing at random) and\nMCAR (missing completely at random).\nChen et al. [116] highlight that directly applying diffu-\nsion models to missing value imputation yields suboptimal\nperformance due to two key issues: 1) the inherent diversity\nof samples generated by diffusion models hampers accurate\ninference of missing values, echoing similar concerns raised\nin [113]; and 2) the data masking reduces the number of\ncompletely observable samples that can be used for model\ntraining, and it is challenging to ensure that the masking\nmechanism aligns well with the true missing mechanism in\nthe testing data. To address these challenges, they propose\nNewImp, a diffusion-based imputation method under the\nWasserstein Gradient Flow framework [120]. This framework\nreformulates diffusion models to derive a cost functional that\ngoverns the diffusion process. Specifically, to tackle the first\nchallenge, they incorporate a negative entropy regularization\nterm into the cost functional, aiming to suppress the sample\ndiversity; For the second challenge, they eliminate the needs\nfor data masking by replacing the conditional distribution\nbased cost functional with joint distribution one. However,\ntheir model can handle only numerical features. The model’s\nperformance is evaluated using MAE (mean absolute error)\nand WASS (Wasserstein-2 distance) [109] under two missing\ndata scenarios: MAR and MCAR.\nBroadly, deep learning-based imputation methods can be\ncategorized into two main families: 1) predictive models that\ninfer missing entries based on observed ones, guided by\nmasking mechanisms, and 2) generative models that estimate\nthe joint distribution of both missing and observed entries,\nenabling imputation via conditional sampling. DiffPuter [117]\nargues that predictive models often outperform generative\nmodels in imputation tasks due to the inherent challenge\nof incomplete likelihood estimation in generative approaches.\nSpecifically, generative models must estimate the joint distri-\nbution while the missing entries remain unknown, leading to\npotential errors in density estimation.\nTo overcome this limitation, they propose integrating the\nExpectation-Maximization (EM) algorithm [121] with diffu-\nsion models. The EM-based approach operates as follows: 1)\nduring the M-step, the missing data x(miss) is fixed, and a\ndiffusion model is trained to learn the joint distribution of\nobserved and missing entries, i.e., pθ(x) = pθ(x(obs), x(miss));\n2) during the E-step, the model parameters θ are fixed,\nand the missing values x(miss) are updated using the reverse\ndenoising process of the learned diffusion model. They further\ndemonstrate theoretically that the diffusion model’s training\nprocess corresponds to the E-step, while its sampling process\ncorresponds to the M-step in the EM framework. For prepro-\ncessing, they apply one-hot encoding to transform categorical\nfeatures into numerical representations, after which a Gaussian\ndiffusion model is applied to all features. They evaluate the\nperformance using MAE and RMSE for continuous features,\nand Accuracy for discrete features, under three missing data\nscenarios: MNAR, MAR, and MCAR.\nVI. DIFFUSION MODELS FOR TRUSTWORTHY DATA\nSYNTHESIS\nTrustworthy data synthesis encompasses two key aspects:\nprivacy-preserving data synthesis and fairness-preserving data\nsynthesis. The former aims to use diffusion models to generate\nsynthetic tabular data that protects privacy in sensitive datasets\n(e.g., healthcare, finance) while preserving data utility. For\nprivacy-preserving data synthesis task, we further define cross-\nsilo tabular synthesis problem and federated learning based\ntabular synthesis problem as follows, respectively.\nProblem 4 (Cross-Silo Tabular Synthesis). Consider there are\nL distinct parties {P1, ..., PL}, each party stores a subset of\nfeatures Xl ∈RN×Dl with Dl the number of features stored\nat party Pl. Altogether, we have X = X1||X2||...||XL (“||”\nmeans column-wise concatenation) and X ∈RN×D with\nD = PL\nl=1 Dl. The goal of cross-silo tabular synthesis is\nto generate a synthetic dataset ˜X = ˜X1|| ˜X2||...|| ˜XL that\nare distributionally similar to the original dataset X while\nmaintaining the private information of the actual values.\nProblem 5 (Federated Tabular Synthesis). Consider there are\nM distinct clients {Q1, ..., QM}, each client stores a subset\nof samples Xm ∈RNm×D with Nm the number of samples\nstored at client Qm. Altogether, we have X = X1⊕X2⊕· · ·⊕\nXM (“⊕” denotes row-wise concatenation), and X ∈RN×D\nwith with N = PM\nm=1 Nm. The goal of federated learning\nbased tabular synthesis is to generate a synthetic dataset ˜X =\n˜X1 ⊕˜X2 ⊕... ⊕˜XM that are distributionally similar to the\noriginal dataset X while maintaining the private information\nof the actual values.\nMeanwhile, fairness-preserving data synthesis aims to use\ndiffusion models to generate synthetic tabular data that main-\ntains fairness w.r.t. sensitive features (e.g., age, gender, race,\netc.) while preserving data utility. Formally, it is defined as:\nProblem 6 (Fairness-Preserving Data Synthesis). Given a D-\ndimensional dataset X = {ix}N\ni=1, where each data point ix ∈\nRD represents a record with D features, let S ⊆{1, . . . , D}\ndenote the set of indices corresponding to sensitive features.\nThe goal is to learn a generative function g(·) to generate\nsynthetic data ˆX = g(X) such that: 1) The synthetic data ˆX =\n{ ˆ\nix}N\ni=1 preserves fairness with respect to sensitive features in\nS, ensuring no unfair bias is introduced; and 2) The synthetic\ndata ˆX maintains the statistical properties and utility of the\noriginal dataset X for downstream tasks.\nTABLE V\nOVERVIEW OF DIFFUSION MODELS FOR TRUSTWORTHY DATA\nSYNTHESIS.\nModel Name\nYear\nVenue\nFeature Type\nDomain\nSiloFuse [122]\n2024\nICDE\nNum+Cat\nGeneric\nFedTabDiff [123]\n2024\nArXiv\nNum+Cat\nGeneric\nFairTabDDPM* [124]\n2024\nArXiv\nNum+Cat\nGeneric\nDP-Fed-FinDiff [125]\n2024\nArXiv\nNum+Cat\nFinance\nSiloFuse [122] is designed to generate tabular data where\nthe features are distributed across multiple silos (known\n\nMANUSCRIPT SUBMITTED TO IEEE FOR POSSIBLE PUBLICATION\n16\nas feature-partitioned or vertically-partitioned dataset) rather\nthan being centrally stored. To ensure privacy, they employ a\ndistributed latent diffusion model: 1) they utilize autoencoders\nto convert actual feature values (including sensitive features)\ninto embeddings. In this way, they can also handle categor-\nical features by unifying numerical and categorical features\ninto a shared continuous latent space. As a result, they can\ncapture feature correlations across silos by centralizing the\nembeddings; 2) they employ a Gaussian diffusion model to\nlearn to create synthetic embeddings; 3) they leverage stacked\ndistributed training to reduce communication: the autoencoders\nare trained locally at the client silos, while the latent diffusion\nmodel is trained at the central server.\nThey evaluate the quality of generated data in terms of\nfidelity, machine learning utility, and privacy. They observe\nthat latent DDPM outperforms TabDDPM on datasets that\nhave many spare features, while TabDDPM performs better\non datasets with a small number of features. In other words,\ncombining Gaussian diffusion and multinomial diffusion may\nwork better for datasets with low features size and sparsity,\nwhile operating in the latent space could benefit highly sparse\ndatasets with large cardinality in discrete values.\nFedTabDiff [123] addresses the challenge of generating\nsynthetic tabular data with mixed-type features while preserv-\ning privacy. They propose a federated learning framework that\nintegrates DDPM with Federated Learning to enhance privacy.\nEach client Qm locally maintains a decentralized FinDiff\nmodel fm(·) [81] and collaborates in training a central model\nf(·) hosted by a trusted entity. A synchronous update scheme\nis employed for federated optimization over R rounds. In each\nround, a subset of clients receives the current central model\nparameters, performs local optimization, and sends updated\nparameters back for aggregation. For this aggregation, they\napply the Federated Averaging (FedAvg) technique [126],\ncomputing a weighted average of the model updates. For\ndata pre-processing, they apply quantile transformation to\nnumerical features and use the same embedding method as in\nFinDiff for categorical features. The framework’s performance\nis evaluated in terms of fidelity, machine learning utility,\ndiversity, and privacy.\nDP-Fed-FinDiff [125] integrates differential privacy, feder-\nated learning, and DDPM to generate high-fidelity synthetic\ntabular data while preserving privacy. Similar to FedTabDiff\n[123], it assumes sensitive data is distributed across mul-\ntiple clients and cannot be shared due to privacy regula-\ntions. However, it argues that federated learning alone (as\nin FedTabDiff) is insufficient for ensuring data privacy. To\naddress this, differential privacy is introduced, providing a\nstrict mathematical guarantee that the inclusion of a single\nsample does not significantly affect analysis outcomes. This\nallows for precise control over the privacy budget, ensuring\nindividual data confidentiality.\nTheir approach builds on FinDiff [81] as the generative\nmodel for mixed-type tabular data. Like FedTabDiff, FinDiff\nis trained with federated learning, but its parameter update\nprocess is modified to incorporate differential privacy via the\nGaussian Mechanism [127]. Unlike FinDiff, they apply Quan-\ntile Transformation to numerical features while using the same\nembedding techniques for categorical features. Evaluations on\nprivacy, utility, and fidelity reveal a tradeoff between privacy\nprotection and data utility/fidelity.\nFairTabDDPM [124] indicates that prior tabular diffusion\nmodels may generate biased synthetic data by inheriting bias\ninherent in the training data. To mitigate bias and maintain\nsynthetic data quality, they propose to build balanced joint\ndistributions of sensitive columns (e.g., gender, race) and target\nlabel column. Specifically, FairTabDDPM works as follows:\n1) the input data x0 = (xnum\n0\n, xcat\n0 ) passes through T steps\nof diffusion and leads to x1, x2, · · · , xT . Like TabDDPM, a\nGaussian diffusion model is used for numerical features and\nmultinomial diffusion models are used for categorical features\nseparately; 2) the intermediate noisy samples x1, x2, · · · , xT\nare embedded into z1, z2, · · · , zT with a MLP encoder; 3) the\nreverse process denoises zT sequentially into ˆzT −1, · · · , ˆz1 by\nconditioning on the embeddings (with a MLP) of label column\ny and sensitive columns s1, · · · , sP ; Note that while prior\ntabular diffusion models are mainly unconditional or condi-\ntioned solely on the label columns, FairTabDDPM models the\ndata distribution of mixed features conditioned on both label\ncolumn and multiple sensitive columns ( by extending Eq. 18);\nand 4) the denoised latent representation ˆz1 is decoded into\nthe reconstructed data ˆx0 using an MLP decoder.\nFor data preprocessing, they apply quantile transformation\nto normalize numerical features and one-hot encoding for\ncategorical features. The model’s performance is evaluated in\nterms of fidelity, diversity, privacy, machine learning utility,\nand fairness metrics, including class imbalance within sensitive\nattributes (demographic parity ratio) and joint distributions\nwith the target label (equalized odds ratio).\nVII. DIFFUSION MODELS FOR ANOMALY DETECTION\nAnomaly detection aims to train diffusion models to learn\nthe “normal” distribution of data from the training set and\nidentify anomalies as deviations from this learned distribution\nin the test data. Formally, it is defined as follows:\nProblem\n7\n(Tabular Anomaly Detection). Given a D-\ndimensional training dataset Xtrain = {ix}Ntrain\ni=1 , where each\ndata point ix ∈RD represents a record with D features, the\nobjective is to learn a function f(·) : RD →{0, 1} that distin-\nguishes between normal and anomalous data points. During\ntesting, given a D-dimensional test dataset Xtest = {jx}Ntest\nj=1,\nthe function f(·) predicts f(jx) = 1 if jx is an anomaly\nand f(jx) = 0 if it is normal. Anomalies are identified as\ndata points in Xtest that significantly deviate from the normal\npatterns learned from Xtrain.\nTabADM [128] is the first diffusion-based approach for\nunsupervised anomaly detection in tabular data. Unlike semi-\nsupervised methods that require pure normal training data,\nTabADM handles fully unlabeled training data mixed with\nanomalies. It estimates the data distribution via a robust\ndiffusion model, assigning anomaly scores to test samples\nbased on their likelihood of being generated by the model.\nSpecifically, TabADM comprises two stages: training and\ninference. During training, it follows the standard DDPM\n\nMANUSCRIPT SUBMITTED TO IEEE FOR POSSIBLE PUBLICATION\n17\nTABLE VI\nOVERVIEW OF DIFFUSION MODELS FOR ANOMALY DETECTION\nModel Name\nYear\nVenue\nFeature Type\nDomain\nTabADM [128]\n2023\nArXiv\nNum by default\nGeneric\nDTE [129]\n2024\nICLR\nNum by default\nGeneric\nSDAD [130]\n2024\nInf. Sci.\nNum by default\nGeneric\nNSCBAD [131]\n2024\nOpenReview\nNum by default\nGeneric\nFraudDiffuse [132]\n2024\nICAIF\nNum+Cat\nFinance\nFraudDDPM [133]\n2024\nISIJ\nNum+Cat\nFinance\nprocess with an additional sample rejection procedure to\nmitigate the influence of anomalies. Specifically, in each batch,\nit sorts samples by loss in descending order and removes the\ntop k highest-loss samples before gradient descent. During\ninference, it computes an anomaly score for a test sample\nx by evaluating its reconstruction loss: for each timestep\nt ∈{1, . . . , T}, it generates noise ϵt, predicts noise ϵθ(xt, t),\nand computes the loss as lt(x) := ∥ϵt −ϵθ(xt, t)∥2\n2. The\nfinal anomaly score is obtained by summing loss values across\nall timesteps, i.e., PT\nt=1 lt(x). Consequently, samples in low-\ndensity regions of the learned distribution exhibit high loss\nvalues and are classified as anomalies.\nEmpirical results on 32 datasets, evaluated using AUCROC\nand AUCPR, demonstrate its superior performance on high-\ndimensional datasets. However, TabADM has several limita-\ntions: 1) it lacks explainability, 2) it incurs high training and\ninference costs compared to baselines, and 3) it cannot handle\nmissing values.\nLivernoche et al. [129] observe that DDPM achieves com-\npetitive anomaly detection performance in both unsupervised\nand semi-supervised settings for tabular data. However, its\nhigh computational cost makes it impractical for large datasets\nor data streams. Since anomaly detection only requires measur-\ning deviations between input and reconstructed samples, they\nargue that modeling score functions in the reverse process is\nunnecessary, enabling simplification of diffusion models for\nthis task. A key observation is that the choice of the starting\ntimestep for the reverse diffusion process significantly impacts\nperformance, yet it is often arbitrary in existing methods.\nThey further note that the diffusion (starting) time given a\nnoisy sample follows an inverse Gamma distribution. Based\non this, they introduce DTE, which estimates the posterior\ndistribution over diffusion time for noisy input samples. The\nrationale is that anomalous samples, being distant from the\nnormal data manifold, tend to be assigned higher estimated\ntimesteps, corresponding to more noise and thus indicating\nhigher anomaly likelihood. For each sample, DTE estimates\nthe diffusion time distribution and uses its mode/mean as the\nanomaly score.\nDTE derives an analytical form for the posterior over\ndiffusion time and employs two estimation methods: 1) a non-\nparametric KNN-based estimator and 2) a parametric neural\nnetwork-based estimator for improved generalizability and\nscalability. They explore the interpretability of diffusion-based\nanomaly detection using deterministic ODE flow for input\nreconstruction. Notably, they find that leveraging pre-trained\nembeddings for images significantly enhances performance,\nhighlighting the potential of latent diffusion models. For pre-\nprocessing, they standardize data using z-score normalization.\nThey evaluate DTE using ROCAUC and ROCPR but note that\nit natively supports only numerical features.\nWhile generative models are powerful at modeling complex\ndatasets to capture their distributional patterns, SDAD [130]\npoint out that existing generative model-based anomaly de-\ntection methods suffer from three limitations: 1) they usually\nprioritize the generative process to generate new synthetic\nsamples resembling the training data, while overlooking the\nimportance of obtaining discriminative representations; 2)\nVAEs generate low-quality samples and GANs are prone to\nmode collapse, leading to coarse data reconstructions and\nhindering the precise identification of anomalies; 3) generative\nmodels usually have intricate architecture, requiring extensive\ntraining data and limiting their performance on small datasets.\nTo address these issues, SDAD incorporates self-supervised\nlearning to enhance diffusion-based anomaly detection as fol-\nlows: 1) it employs an auxiliary module with two pretext tasks\nto train an encoder, improving the separation between normal\nand anomalous samples in latent space; 2) using this trained\nencoder, raw input samples are mapped to the latent space,\nwhere a denoising diffusion process models the distribution\nof normal data; and 3) in inference, test samples with high\nreconstruction errors in latent space (rather than in original\nspace) are identified as anomalies. SDAD is designed for the\nsemi-supervised setting, where training is performed exclu-\nsively on normal samples. It is evaluated using AUCROC and\nAUCPR. However, its negative sampling strategy in SSL relies\non standard Gaussian noise to generate pseudo anomalies,\nwhich may not be as representative as real anomalies.\nInstead of relying on density estimation p(x) for anomaly\ndetection, NSCBAD [131] leverages the score function\n∇x log p(x) to detect anomalies as follows: 1) Training stage:\nThey train a Noising Conditional Score Network (NCSN)\n[74], denoted as Sθ(·), to learn the score function character-\nizing the data distribution. Specifically, they optimize Sθ(·)\nusing a simplified loss function where the score network\ndirectly predicts the ground-truth noise ϵ. Mathematically,\nthey demonstrate that the network learns σt∇x log p(xt|x0);\n2) Inference stage: Anomaly scores are computed based on\nthe likelihood of a test sample x within the learned score.\nConcretely, they measure the distance between the predicted\nscore Sθ∗(xt, t) and the ground-truth noise ϵ(t) at a fixed\ntime t, averaging over 70 noise realizations. The anomaly\nscore is given by:\n1\n70\nP70\ni=1 ∥Sθ∗(xt, t) −ϵi(t)∥2,\nϵi(t) ∼\nN(0, I),\ni ∈{1, ..., 70}.\nNSCBAD is evaluated using AUC-ROC, AUC-PR, and F1-\nscore. They highlight that DDPM-based anomaly detection\nmethods rely on a stepwise reverse denoising process (i.e.,\na sequential Markovian chain for inference), leading to signif-\nicantly longer inference times compared to NSCBAD.\nFraudDiffuse [132] extends diffusion models to generate\nsynthetic fraudulent transactions, addressing class imbalance\nand evolving fraud patterns.\nFirst, they employ a learnable embedding layer (label encod-\ning) to map categorical features into continuous embeddings,\nwhich are then concatenated with numerical features in a\n\nMANUSCRIPT SUBMITTED TO IEEE FOR POSSIBLE PUBLICATION\n18\nshared continuous space. Second, they apply a DDPM model\nin the latent space with the following modifications: 1) Instead\nof using a Gaussian prior for noise addition, they leverage\nthe distribution of normal transactions to better capture the\nbehaviors of fraudulent transactions at the decision boundary.\nSpecifically, during the forward diffusion process, noise is\nsampled from N(µnt, Σnt), where µnt and Σnt are the mean\nand covariance estimated from normal transactions. Similarly,\nin the reverse process, xT is initialized from N(µnt, Σnt)\nrather than from a standard Gaussian distribution. 2) In ad-\ndition to the typical mean squared error (MSE) loss between\npredicted and ground-truth noise, they introduce a probability-\nbased loss that quantifies the likelihood of predicted noise\noriginating from N(µnt, Σnt). The final loss function combines\nboth loss terms. Third, they incorporate a contrastive loss to\nenhance the similarity between real and synthetic fraudulent\nsamples, further refining the learned representations.\nFraudDiffuse operates under a supervised setting, where\nboth labeled normal and fraudulent transactions are available.\nPerformance is evaluated based on machine learning utility.\nSimilar to FraudDiffuse [132], FraudDDPM [133] lever-\nages diffusion models to generate synthetic transaction data,\nenhancing fraud detection performance. However, unlike\nFraudDiffuse, they adopt a two-model approach: one diffu-\nsion model is trained on normal transaction data to generate\nsynthetic normal transactions, while another is trained on\nabnormal transactions to synthesize fraudulent data.\nThey employ Gaussian diffusion models for both numerical\nand categorical features, where categorical data is transformed\ninto a continuous space using one-hot encoding or embeddings\nvectors. The generated synthetic normal and fraudulent trans-\nactions are then merged to form a synthetic supervised dataset.\nFinally, this synthetic dataset is combined with real-world\ntransaction data to create a balanced training set, effectively\nserving as an oversampling technique. This balanced dataset\nis then used to train fraud detection systems utilizing classical\nsupervised learning algorithms such as Random Forest. They\nevaluate the quality in terms of machine learning utility.\nVIII. A REVIEW ON DISCRETE DIFFUSION MODELS AND\nPERFORMANCE EVALUATION METRICS\nA. Diffusion Models for Discrete Data.\nDiffusion models for discrete data can be divided into\ntwo categories [134]: 1) the first category of works converts\ndiscrete structures into a continuous latent space and then\ndirectly applies Gaussian diffusion model in the latent space.\nThis line of works include [97], [100], [135], [136], [137],\n[138], [139], [140]; 2) the second category of works directly\ndefines the diffusion process on discrete structures, and we\nwill focus on this category in the remaining of this section.\n1) Binary Diffusion Model: This line of work is for the first\ntime studied in [22], which explored the scenario of binary\nrandom features.\n2) Multinomial Diffusion Model: Multinomial Diffusion\n[31], [141] that provides transition matrices with uniform\ndistribution; The diffusion and denoising processes have been\ndefined in Section III-A1.\n3) D3PM Model: D3PM [32] generalizes Multinomial Dif-\nfusion by providing transition matrices with uniform distribu-\ntion, transition matrices that mimic Gaussian kernels in con-\ntinuous spaces, matrices using nearest neighbors in embedding\nspace, and matrices with absorbing states. Specifically, the\nMarkov forward diffusion process is defined as:\nq(xt|xt−1) = Cat(xt; ¯Qtx0) = Cat(xt; Qt · Qt−1 · · · Q1x0),\n(33)\nwhere Qt can be defined in a general and flexible form to\nreflect different types of transition matrices.\n4) Masking (Absorbing) Diffusion Model: Let V = {x ∈\n{0, 1}K : PK\ni=1 xi = 1} be the one-hot encoding vectors\nof all scalar discrete random variables x with K categories\n(Note that it is reasonable to define V in this way since in text\nmodeling the words share the same dictionary space. However,\nin tabular data, different categorical features usually have\ndifferent number of categories and thus there can be multiple\nVs). Moreover, let Cat(·; π) the categorical distribution over\nK categories, where π ∈∆K defines the probabilities and\n∆K is the K-simplex. In addition, we assume that the extra\nK-th category/position is a special [MASK] token, and m =\n(0, 0, · · · , 1) be the one-hot encoding vector for this mask (i.e.,\nmK = 1 while mk = xk for k ∈{0, 1, · · · , K −1}). For\nmasking (absorbing) diffusion models [134], [142], [73], the\ndiffusion process interpolates between the original categorical\ndistribution Cat(·; x0) (with x0 ∈V) and the target distribution\nCat(·; m), namely the input x0 can transition to a masked state\nwith some probability as follows:\nq(xt|x0) = Cat(xt; αtx0 + (1 −αt)m),\n(34)\nwhere αt ∈[0, 1] is strictly decreasing w.r.t. time t, with\nα0 ≈1 while α1 ≈0. Importantly, an input will remain in\na masked state once it transitions to this state, and all inputs\nwill be masked with probability 1 at time step T. The reverse\ndenoising process can be defined as:\npθ(xs|xt) =\n(\nCat(xs; xt),\nif xt ̸= m,\nCat(xs; (1−αs)m+(αs−αt)xθ(xt,t)\n1−αt\n),\nif xt = m,\n(35)\nwhere 0 < s < t < 1, and xθ(xt, t) : V × [0, 1] →∆K\nis a neural network that approximates the unknown x0. For\nthe denoising model xθ(xt, t), the clean input is never input\nwhile an masked token is always unchanged during the reverse\nprocess. The corresponding loss function is:\nLmask = Eq\nZ t=1\nt=0\nα′\nt\n1 −αt\nlog[⟨xθ(xt, t), x0⟩]dt,\n(36)\nwhere ⟨·⟩means inner product and α′\nt is the first order\nderivative of αt. Note that masking diffusion model is a strict\nsubset of D3PM by setting Qt|s = αt|sI + (1 −αt|s)1mT for\nthe forward diffusion process, with improvements including\nparameterization that enables simplified objective and propos-\ning well-engineered training recipes.\nOther recent advancements in this field include 1) auto-\nregressive diffusion models [143], [144], 2) editing-based\noperations for discrete diffusion [145], [146], 3) discrete\ndiffusion model in continuous time structure [147], [148], and\n4) generation acceleration in discrete diffusion model [149].\n\nMANUSCRIPT SUBMITTED TO IEEE FOR POSSIBLE PUBLICATION\n19\nB. Performance Evaluation Metrics\nThe performance of tabular data generative models is of-\nten evaluated from various but complementary perspectives,\nincluding fidelity, diversity, utility, privacy, and runtime.\n1) Fidelity Metrics: they aim to quantify the similarity\nbetween original and synthetic data by assessing whether key\nproperties of the original data are preserved in the synthetic\ndata. This line of evaluation can be further divided into\ncolumn-wise density estimation, which assesses the similarity\nbetween a generated column and its original counterpart, and\npairwise column correlation estimation, which evaluates how\nwell the correlations between columns in the generated data\nmatch those in the original data. Moreover, other popular\nfidelity metrics include detection metrics, α-Precision, MAE\nand RMSE (specifically for data imputation task).\nColumn-wise density estimation. This includes\n• Kolmogorov-Sirmov Test (KST) for numerical features.\nThis test evaluates to which extent two one-dimensional\nprobability distributions differ, defined as follows:\nKST = sup\nx |F1(x) −F2(x)|,\n(37)\nwhere Fi(x) is the empirical distribution of i-th proba-\nbility distribution at point x.\n• Total Variation Distance (TVD) for categorical features.\nThis statistics captures the largest possible difference in\nthe probability of any event under two different (one-\ndimensional) probability distributions, defined as follows:\nTVD = 1\n2\nX\nx∈X\n|p1(x) −p2(x)|,\n(38)\nwhere p1(x) and p2(x) are the probability assigned to x\nby these two probability distributions.\nPairwise columns correlations estimation. This includes\n• The difference between Pearson Correlation Coefficient\nMatrices among numerical features (DPCM) on real data\nand original data. The Pearson Correlation Coefficient\naims to measure the linear correlation between two nu-\nmerical features x and y as follows:\nρ(x, y) =\ncov(x, y)\nstd(x), std(y),\n(39)\nwhere cov(·) is the covariance and std(·) stands for the\nstandard deviation.\n• The difference between Contingency Similarity Matrices\namong categorical features (DCSM) on real data and\noriginal data. The contingency table summarizes the rela-\ntionship between two categorical features by organizing\nthe data into rows and columns. Each cell in the table\nrepresents the frequency of occurrences for a specific\ncombination of the two features.\n• The difference between Contingency Similarity Matrices\non real data and original data (among categorical and\nnumerical features), where one needs to first group nu-\nmerical features into discrete intervals via bucketing.\nDetection Metric. The metric evaluates the difficulty of\ndetecting the synthetic data from the real data by using the\nclassifier-two-sample-test, which often employs a machine\nlearning model to label whether a sample is synthetic or real.\nα-Precision. It measures the fidelity of synthetic data by\nindicating whether each synthetic sample resembles the real\ndata [150].\nRMSE and MAE. These metrics evaluate the performance\nof data imputation by computing the divergence between the\nground-truth missing values and the imputed missing values.\nParticularly,\n• The Mean Absolute Error (MAE) is defined as:\nMAE = 1\nn\nn\nX\ni=1\n|yi −ˆyi| ,\n(40)\nwhere n is the total number of data points, yi is the actual\nvalue, and ˆyi is the imputed value;\n• The Root Mean Squared Error (RMSE) is defined as:\nRMSE =\nv\nu\nu\nt 1\nn\nn\nX\ni=1\n(yi −ˆyi)2.\n(41)\n2) Diversity Metrics: they aim to evaluate the extent to\nwhich the synthetic data maintain the entire distribution of the\nreal data. The two most popular metrics are coverage score\nand β-Recall.\nCoverage Score. Specifically, for a categorical column, it\ncalculates the number of unique categories from the real data\ncolumn that are present in the corresponding synthetic column.\nFor a numerical column, this metric evaluates how closely\nthe range (namely the minimum and maximum) of synthetic\ncolumn aligns with that of real column.\nβ-Recall. It evaluates whether the generated samples can\ncover the entire distribution of real data [150].\n3) Utility Metrics: they aim to quantify the impact on\ndownstream tasks when synthetic data is used to replace\noriginal data during the training phase. This metric is usually\nknown as machine learning utility and it is defined as follows.\nMachine Learning Utility. The machine learning utility is\noften evaluated using the Training on Synthetic and Testing on\nReal (TSTR) scheme [151]. In this approach, the data is split\ninto training and test sets. A generative model is then used to\ngenerate synthetic data of the same size as the training set.\nTwo models are trained: one on the original training data and\nanother on the synthetic data. Both models are subsequently\ntested on the test set, and their performances are compared. We\nconclude that the synthetic data has a good machine learning\nutility if their performance are comparable.\n4) Privacy Metrics: These metrics aim to quantify the\nextent to which the identification of original samples is possi-\nble. They mainly include Distance to Closet Records (DCR),\nAttribute Inference Attack [152], and Membership Inference\nAttack [153].\nDistance to Closet Records (DCR). It measures the extent\nto which the synthetic samples resemble the original data\nsamples. Given a generated sample ˆx, its DCR is computed\nas DCR(ˆx) = min\nx∈X d(ˆx, x) with d(·) a distance metric. As a\nresult, the DCR of a synthetic dataset is computed as the DCR\nmedian of all synthetic samples. Low DCR values indicate\n\nMANUSCRIPT SUBMITTED TO IEEE FOR POSSIBLE PUBLICATION\n20\nthat the synthetic sample may violate the privacy requirement.\nHowever, it is important to note that random noises can\nproduce higher DCR values (indicating better performances).\nTherefore, the DCR metric needs to be considered jointly with\nother evaluation metrics such as machine learning utilities.\nIX. DISCUSSION AND CONCLUSIONS\nThanks to notable advancements in diffusion models for\ntabular data, some of the challenges associated with generative\nmodels for tabular data, as discussed in Section II-B, have\nbeen partially addressed. However, many challenges remain\nunresolved, and new ones continue to emerge. Below, we\noutline these challenges and propose potential future research\ndirections:\n• Scalability: Diffusion models are typically computa-\ntionally intensive, which can be challenging with high-\ndimensional tabular data. Therefore, more research efforts\nshould be given to develop techniques for reducing the\ncomputational cost of diffusion models on large tabular\ndatasets, such as efficient sampling and training methods.\n• Evaluation Metrics: Unlike images where visual quality\nis a metric, evaluating tabular data generation quality\nis complex and may require domain-specific measures\nespecially in domains such as healthcare and finance.\n• Privacy Concerns: While current diffusion models for\ntabular data synthesis provide basic privacy preservation,\nresearch into techniques with strong theoretical guaran-\ntees, such as differential privacy, remains limited and\nrequires further exploration.\n• Enhanced Interpretability: Developing methods to en-\nhance the interpretability of diffusion models is crucial\nfor their practical application in tabular data modeling .\n• Cross-Modality Integration: Future research into how\ndiffusion models for tabular data might be combined with\nthose for image, text, or time-series data, enabling cross-\nmodal applications.\n• Benchmarking\nand\nStandardization:\nStandardized\nbenchmarks and evaluation metrics tailored for diffusion\nmodels on tabular data are needed.\n• Hybrid Models: Diffusion models can be combined with\nother model architectures (e.g., GANs or VAEs) to better\nhandle tabular data. For instance, AutoDiff [36] combines\nAutoencoder with Diffusion Models.\n• Modeling Feature Correlations: Most existing diffusion\nmodels are not optimized to capture feature correlations,\nespecially those among categorical features and those\namong numerical and categorical features. Ideally, a tab-\nular diffusion model should be able to model numerical\nand categorical simultaneously.\nRecent studies indicate that diffusion models can achieve\nsuperior performance in tabular data modeling. To support\nfurther exploration and progress in this area, we present a\ncomprehensive survey on diffusion models for tabular data.\nThis survey includes a concise introduction to the fundamental\ntheory behind diffusion models and a categorized review of the\nexisting literature based on their applications. Furthermore,\nwe highlight key challenges and propose potential research\ndirections for advancing the field. We aim for this survey to\nserve as a valuable resource for researchers and practitioners,\nencouraging further innovation and development in generative\ndiffusion models for tabular data.\nREFERENCES\n[1] I. Yoo, P. Alafaireet, M. Marinov, K. Pena-Hernandez, R. Gopidi, J.-\nF. Chang, and L. Hua, “Data mining in healthcare and biomedicine:\na survey of the literature,” Journal of medical systems, vol. 36, pp.\n2431–2448, 2012.\n[2] M. F. Dixon, I. Halperin, and P. Bilokon, Machine learning in finance.\nSpringer, 2020, vol. 1170.\n[3] A. Algarni, “Data mining in education,” International Journal of\nAdvanced Computer Science and Applications, vol. 7, no. 6, pp. 456–\n461, 2016.\n[4] S. Anand, P. Padmanabham, A. Govardhan, and R. H. Kulkarni,\n“An extensive review on data mining methods and clustering models\nfor intelligent transportation system,” Journal of Intelligent Systems,\nvol. 27, no. 2, pp. 263–273, 2018.\n[5] M. W. King and P. A. Resick, “Data mining in psychological treatment\nresearch: a primer on classification and regression trees.” Journal of\nconsulting and clinical psychology, vol. 82, no. 5, p. 895, 2014.\n[6] G. GDPR, “General data protection regulation,” Regulation (EU), vol.\n679, 2016.\n[7] C.\nS.\nLegislature,\n“California\nconsumer\nprivacy\nact\nof\n2018\n(ccpa),” 2018, accessed: 2024-12-27. [Online]. Available: https:\n//oag.ca.gov/privacy/ccpa\n[8] A. Kotelnikov, D. Baranchuk, I. Rubachev, and A. Babenko, “Tabd-\ndpm: Modelling tabular data with diffusion models,” in International\nConference on Machine Learning.\nPMLR, 2023, pp. 17 564–17 579.\n[9] P.-A. Mattei and J. Frellsen, “Miwae: Deep generative modelling and\nimputation of incomplete data sets,” in International conference on\nmachine learning.\nPMLR, 2019, pp. 4413–4423.\n[10] H. Kaur, H. S. Pannu, and A. K. Malhi, “A systematic review on\nimbalanced data challenges in machine learning: Applications and\nsolutions,” ACM computing surveys (CSUR), vol. 52, no. 4, pp. 1–36,\n2019.\n[11] V. A. Fajardo, D. Findlay, C. Jaiswal, X. Yin, R. Houmanfar, H. Xie,\nJ. Liang, X. She, and D. B. Emerson, “On oversampling imbalanced\ndata with deep conditional generative models,” Expert Systems with\nApplications, vol. 169, p. 114463, 2021.\n[12] S. A. Assefa, D. Dervovic, M. Mahfouz, R. E. Tillman, P. Reddy,\nand M. Veloso, “Generating synthetic data in finance: opportunities,\nchallenges and pitfalls,” in Proceedings of the First ACM International\nConference on AI in Finance, 2020, pp. 1–8.\n[13] M. Hernandez, G. Epelde, A. Alberdi, R. Cilla, and D. Rankin,\n“Synthetic data generation for tabular health records: A systematic\nreview,” Neurocomputing, vol. 493, pp. 28–45, 2022.\n[14] J. You, X. Ma, Y. Ding, M. J. Kochenderfer, and J. Leskovec, “Handling\nmissing data with graph representation learning,” Advances in Neural\nInformation Processing Systems, vol. 33, pp. 19 075–19 087, 2020.\n[15] J. Yoon, J. Jordon, and M. Schaar, “Gain: Missing data imputation\nusing generative adversarial nets,” in International conference on\nmachine learning.\nPMLR, 2018, pp. 5689–5698.\n[16] J. Fonseca and F. Bacao, “Tabular and latent space synthetic data\ngeneration: a literature review,” Journal of Big Data, vol. 10, no. 1, p.\n115, 2023.\n[17] Y. LeCun, S. Chopra, R. Hadsell, M. Ranzato, F. Huang et al., “A\ntutorial on energy-based learning,” Predicting structured data, vol. 1,\nno. 0, 2006.\n[18] D. P. Kingma, “Auto-encoding variational bayes,” arXiv preprint\narXiv:1312.6114, 2013.\n[19] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,”\nAdvances in neural information processing systems, vol. 27, 2014.\n[20] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,”\n2023. [Online]. Available: https://arxiv.org/abs/1706.03762\n[21] I. Kobyzev, S. J. Prince, and M. A. Brubaker, “Normalizing flows:\nAn introduction and review of current methods,” IEEE transactions on\npattern analysis and machine intelligence, vol. 43, no. 11, pp. 3964–\n3979, 2020.\n\nMANUSCRIPT SUBMITTED TO IEEE FOR POSSIBLE PUBLICATION\n21\nTABLE VII\nOVERVIEW OF DIFFUSION MODELS FOR TABULAR DATA. THE COLUMN “NUM” INDICATES WHETHER IT CAN HANDLE NUMERICAL FEATURES (‘MM’ =\nMIN-MAX SCALER, ’QT’= QUANTILE TRANSFORMER, ‘ZS’ = Z-SCORE NORMALIZATION), WHILE “CAT” INDICATES WHETHER IT CAN HANDLE\nCATEGORICAL FEATURES (‘OH’ = ONE-HOT ENCODING, ‘AB’= ANALOG BITS ENCODING, ‘DC’ = DICTIONARY ENCODING,‘DE’ = DUMMY ENCODING,\n‘IE’ = INTEGER ENCODING, ‘OE’ = ORDINAL ENCODING, ‘PSK’ = PSK ENCODING, ’FT’ = FEATURE TOKENIZATION OR LEARNED EMBEDDING).\n“#DATASETS” REPRESENTS THE NUMBER OF DATASETS BENCHMARKED. “?/57 A” INDICATES THE NUMBER OF DATASETS USED FROM ADBENCH [154].\n“COMPLETE” INDICATES WHETHER THE MODEL REQUIRES COMPLETE DATA (I.E., WITHOUT MISSING VALUES) TO TRAIN.\nModel Name\nYear\nVenue\nNum\nCat\nTasks\nBackbone\n#Datasets\nMetrics\nCode\nComplete\nDomain\nSOS [66]\n2022\nKDD\n✓\n✗\nSynthesis\nSDEs\n6\nUtility\n✓\n✓\nGeneric\nSTaSy [35]\n2023\nICLR\n✓(MM)\n✓(OH)\nSynthesis\nSDEs\n15\nFidelity, Utility, Diversity\n✓\n✓\nGeneric\nTabDDPM [8]\n2023\nICML\n✓(QT)\n✓(OH)\nSynthesis\nDDPM+MLD\n16\nFidelity, Utility, Privacy\n✓\n✓\nGeneric\nCoDi [37]\n2023\nICML\n✓(MM)\n✓(OH)\nSynthesis\nDDPM+MLD\n15\nUtility, Diversity\n✓\n✓\nGeneric\nAutoDiff [36]\n2023\nNeurIPSW\n✓\n✓\nSynthesis\nAny\n15\nFidelity, Utility, Privacy\n✓\n✓\nGeneric\nMissDiff [79]\n2023\nICMLW\n✓(MM)\n✓(OH)\nSynthesis\nSDEs\n3\nFidelity, Utility\n✗\n✗\nGeneric\nTabSyn [38]\n2024\nICLR\n✓\n✓(OH)\nSynthesis, Imputation\nSDEs\n6\nFidelity, Utility, Diversity, Privacy\n✓\n✓\nGeneric\nForest-Diffusion [39]\n2024\nAISTATS\n✓\n✓(DE)\nSynthesis, Imputation\nSDEs\n27\nFidelity, Diversity, Utility\n✓\n✗\nGeneric\nTabDiff [73]\n2024\nNeurIPSW\n✓(MM)\n✓(OH)\nSynthesis, Imputation\nSDEs+MSD\n7\nFidelity, Diversity, Utility\n✓\n✓\nGeneric\nTabUnite [89]\n2024\nOpenReview\n✓(QT)\n✓(AB, PSK, DC)\nSynthesis\nSDEs+MSD\n10\nFidelity, Diversity, Utility\n✓\n✓\nGeneric\nCDTD [90]\n2024\nOpenReview\n✓\n✓(FT)\nSynthesis\nLatent SDEs\n11\nFidelity, Utility, Privacy\n✗\n✓\nGeneric\nMedDiff [82]\n2023\nArXiv\n✓\n✗\nSynthesis\nDDIM\n2\nFidelity, Utility\n✗\n✓\nHealthcare\nEHR-TabDDPM [83]\n2023\nArXiv\n✓(QT)\n✓(OH)\nSynthesis\nDDPM\n4\nFidelity, Utility, Privacy\n✗\n✓\nHealthcare\nDPM-EHR* [80]\n2023\nNeurIPSW\n✓\n✓(OH)\nSynthesis\nDDPM\n2\nFidelity, Diversity, Utility, Privacy\n✗\n✓\nHealthcare\nFlexGen-EHR [84]\n2024\nICLR\n✓+ TS (FT)\n✓(FT)\nSynthesis\nDDPM\n2\nFidelity, Utility, Privacy\n✗\n✗\nHealthcare\nEHRDiff [85]\n2024\nTMLR\n✓(MM)\n✓(OH)\nSynthesis\nSDEs\n3\nFidelity, Utility, Privacy\n✓\n✓\nHealthcare\nEHR-D3PM [88]\n2024\nArXiv\n✗\n✓(OH)\nSynthesis\nD3PM\n3\nFidelity, Utility, Privacy\n✗\n✓\nHealthcare\nFinDiff [81]\n2023\nICAIF\n✓(ZS)\n✓(OH)\nSynthesis\nDDPM\n3\nFidelity, Utility, Privacy\n✓\n✓\nFinance\nEntTabDiff [86]\n2024\nICAIF\n✓(ZS)\n✓(OH)\nSynthesis\nDDPM\n3\nFidelity, Utility, Privacy\n✗\n✓\nFinance\nImb-FinDiff [87]\n2024\nICAIF\n✓(ZS)\n✓(OH)\nSynthesis\nDDPM\n4\nFidelity, Utility\n✗\n✓\nFinance\nClavaDDPM [105]\n2024\nNeurIPS\n✓\n✓(IE)\nMulti-Relational Synthesis\nDDPM\n5\nFidelity, Diversity, Utility, Dependency\n✓\n✓\nGeneric\nGNN-TabSyn [106]\n2024\nNeurIPSW\n✓\n✓(IE)\nMulti-Relational Synthesis\nDDPM\n6\nFidelity, Utility, Privacy\n✓\n✓\nGeneric\nTabCSDI [68]\n2023\nNeurIPSW\n✓\n✓(OH, AB, FT)\nImputation\nConditional DDPM\n7\nAccuracy\n✓\n✗\nGeneric\nTabDiff [73]\n2024\nNeurIPSW\n✓\n✓\nImputation\nConditional DDPM\n7\nAccuracy\n✓\n✗\nGeneric\nSimpDM [113]\n2024\nCIKM\n✓(QT)\n✓(OH)\nImputation\nDDPM+MLD\n17\nAccuracy\n✓\n✗\nGeneric\nMTabGen [114]\n2024\nArXiv\n✓(QT)\n✓(OE)\nImputation\nDDPM+MLD\n10\nUtility\n✗\n✗\nGeneric\nDDPM-Perlin [115]\n2024\nKBS\n✓\n✗\nImputation\nDDPM\n10\nAccuracy\n✗\n✗\nGeneric\nDiffPuter [117]\n2024\nOpenReview\n✓\n✓(OH)\nImputation\nDDPM\n10\nAccuracy\n✓\n✗\nGeneric\nSiloFuse [122]\n2024\nICDE\n✓(FT)\n✓(FT)\nTrustworthy Synthesis\nLatent DDPM\n9\nFidelity, Utility, Privacy\n✗\n✓\nGeneric\nFedTabDiff [123]\n2024\nArXiv\n✓(QT)\n✓(FT)\nTrustworthy Synthesis\nDDPM\n2\nFidelity, Utility, Privacy\n✓\n✓\nGeneric\nFairTabDDPM* [124]\n2024\nArXiv\n✓(QT)\n✓(OH)\nTrustworthy Synthesis\nDDPM\n3\nFidelity, Diversity, Utility, Privacy, Fairness\n✓\n✓\nGeneric\nDP-Fed-FinDiff [125]\n2024\nArXiv\n✓(QT)\n✓(FT)\nTrustworthy Synthesis\nDDPM\n4\nFidelity, Utility, Privacy\n✗\n✓\nFinance\nTabADM [128]\n2023\nArXiv\n✓\n✗\nAnomaly Detection\nDDPM\n32/57 A\nAccuracy\n✗\n✓\nGeneric\nDTE [129]\n2024\nICLR\n✓(ZS)\n✗\nAnomaly Detection\nDDPM\n57/57 A\nAccuracy\n✓\n✓\nGeneric\nSDAD [130]\n2024\nInf. Sci.\n✓\n✗\nAnomaly Detection\nLatent DDPM\n10/57 A\nAccuracy\n✓\n✓\nGeneric\nNSCBAD [131]\n2024\nOpenReview\n✓\n✗\nAnomaly Detection\nSDEs\n57/57 A\n15 others\nAccuracy\n✓\n✓\nGeneric\nFraudDiffuse [132]\n2024\nICAIF\n✓\n✓(FT)\nAnomaly Detection, Synthesis\nLatent DDPM\n2\nUtility\n✗\n✓\nFinance\nFraudDDPM [133]\n2024\nISIJ\n✓\n✓(OH, FT)\nAnomaly Detection, Synthesis\nDDPM\n4\nUtility\n✗\n✓\nFinance\n[22] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli,\n“Deep unsupervised learning using nonequilibrium thermodynamics,”\nin International conference on machine learning.\nPMLR, 2015, pp.\n2256–2265.\n[23] H. Thanh-Tung and T. Tran, “Catastrophic forgetting and mode collapse\nin gans,” in 2020 international joint conference on neural networks\n(ijcnn).\nIEEE, 2020, pp. 1–10.\n[24] B. Dai and D. Wipf, “Diagnosing and enhancing vae models,” in\nInternational Conference on Learning Representations, 2019.\n[25] D. Carbone, “Hitchhiker’s guide on energy-based models: a compre-\nhensive review on the relation with other generative models, sampling\nand statistical physics,” arXiv preprint arXiv:2406.13661, 2024.\n[26] C.-C. Lin, A. Jaech, X. Li, M. R. Gormley, and J. Eisner, “Limitations\nof autoregressive models and their alternatives,” in Proceedings of the\n2021 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies (NAACL-\nHLT), 2021.\n[27] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic\nmodels,” Advances in neural information processing systems, vol. 33,\npp. 6840–6851, 2020.\n[28] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon,\nand B. Poole, “Score-based generative modeling through stochastic\ndifferential equations,” in International Conference on Learning Rep-\nresentations.\n[29] N. Chen, Y. Zhang, H. Zen, R. J. Weiss, M. Norouzi, and W. Chan,\n“Wavegrad: Estimating gradients for waveform generation,” in Inter-\nnational Conference on Learning Representations, 2020.\n[30] Z. Kong, W. Ping, J. Huang, K. Zhao, and B. Catanzaro, “Diffwave:\nA versatile diffusion model for audio synthesis,” in International\nConference on Learning Representations, 2020.\n[31] E. Hoogeboom, D. Nielsen, P. Jaini, P. Forr´e, and M. Welling, “Argmax\nflows and multinomial diffusion: Learning categorical distributions,”\nAdvances in Neural Information Processing Systems, vol. 34, pp.\n12 454–12 465, 2021.\n[32] J. Austin, D. D. Johnson, J. Ho, D. Tarlow, and R. Van Den Berg,\n“Structured denoising diffusion models in discrete state-spaces,” Ad-\nvances in Neural Information Processing Systems, vol. 34, pp. 17 981–\n17 993, 2021.\n[33] Z. Xing, Q. Feng, H. Chen, Q. Dai, H. Hu, H. Xu, Z. Wu, and Y.-G.\nJiang, “A survey on video diffusion models,” ACM Computing Surveys,\nvol. 57, no. 2, pp. 1–42, 2024.\n[34] C. Liu, W. Fan, Y. Liu, J. Li, H. Li, H. Liu, J. Tang, and Q. Li,\n“Generative diffusion models on graphs: methods and applications,”\nin Proceedings of the Thirty-Second International Joint Conference on\nArtificial Intelligence, 2023, pp. 6702–6711.\n[35] J. Kim, C. Lee, and N. Park, “Stasy: Score-based tabular data synthe-\nsis,” in The Eleventh International Conference on Learning Represen-\ntations, 2023.\n[36] N. Suh, X. Lin, D.-Y. Hsieh, M. Honarkhah, and G. Cheng, “Autodiff:\ncombining auto-encoder and diffusion model for tabular data synthe-\nsizing,” in NeurIPS 2023 Workshop on Synthetic Data Generation with\nGenerative AI.\n[37] C. Lee, J. Kim, and N. Park, “Codi: Co-evolving contrastive diffusion\nmodels for mixed-type tabular synthesis,” in International Conference\non Machine Learning.\nPMLR, 2023, pp. 18 940–18 956.\n[38] H. Zhang, J. Zhang, Z. Shen, B. Srinivasan, X. Qin, C. Faloutsos,\nH. Rangwala, and G. Karypis, “Mixed-type tabular data synthesis with\nscore-based diffusion in latent space,” in The Twelfth International\nConference on Learning Representations, 2024.\n[39] A. Jolicoeur-Martineau, K. Fatras, and T. Kachman, “Generating and\nimputing tabular data via diffusion and flow-based gradient-boosted\ntrees,” in International Conference on Artificial Intelligence and Statis-\ntics.\nPMLR, 2024, pp. 1288–1296.\n[40] L. Yang, Z. Zhang, Y. Song, S. Hong, R. Xu, Y. Zhao, W. Zhang,\nB. Cui, and M.-H. Yang, “Diffusion models: A comprehensive survey\nof methods and applications,” ACM Computing Surveys, vol. 56, no. 4,\npp. 1–39, 2023.\n[41] H. Cao, C. Tan, Z. Gao, Y. Xu, G. Chen, P.-A. Heng, and S. Z.\nLi, “A survey on generative diffusion models,” IEEE Transactions on\nKnowledge and Data Engineering, 2024.\n[42] F.-A. Croitoru, V. Hondru, R. T. Ionescu, and M. Shah, “Diffusion\nmodels in vision: A survey,” IEEE Transactions on Pattern Analysis\nand Machine Intelligence, vol. 45, no. 9, pp. 10 850–10 869, 2023.\n[43] Y. Zhu and Y. Zhao, “Diffusion models in nlp: A survey,” arXiv preprint\narXiv:2303.07576, 2023.\n[44] L. Lin, Z. Li, R. Li, X. Li, and J. Gao, “Diffusion models for time-\n\nMANUSCRIPT SUBMITTED TO IEEE FOR POSSIBLE PUBLICATION\n22\nseries applications: a survey,” Frontiers of Information Technology &\nElectronic Engineering, vol. 25, no. 1, pp. 19–41, 2024.\n[45] A. X. Wang, S. S. Chukova, C. R. Simpson, and B. P. Nguyen,\n“Challenges and opportunities of generative models on tabular data,”\nApplied Soft Computing, p. 112223, 2024.\n[46] D.-K. Kim, D. Ryu, Y. Lee, and D.-H. Choi, “Generative models for\ntabular data: A review,” Journal of Mechanical Science and Technology,\nvol. 38, no. 9, pp. 4989–5005, 2024.\n[47] H. Koo and T. E. Kim, “A comprehensive survey on generative\ndiffusion models for structured data,” arXiv e-prints, pp. arXiv–2306,\n2023.\n[48] D. P. Kingma, M. Welling et al., “An introduction to variational\nautoencoders,” Foundations and Trends® in Machine Learning, vol. 12,\nno. 4, pp. 307–392, 2019.\n[49] A. Sklar, “Random variables, joint distribution functions, and copulas,”\nKybernetika, vol. 9, no. 6, pp. 449–460, 1973.\n[50] D. A. Reynolds et al., “Gaussian mixture models.” Encyclopedia of\nbiometrics, vol. 741, no. 659-663, 2009.\n[51] P. Rabaey, J. Deleu, S. Heytens, and T. Demeester, “Clinical reasoning\nover tabular data and text with bayesian networks,” in International\nConference on Artificial Intelligence in Medicine.\nSpringer, 2024, pp.\n229–250.\n[52] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer,\n“Smote: synthetic minority over-sampling technique,” Journal of ar-\ntificial intelligence research, vol. 16, pp. 321–357, 2002.\n[53] H. Han, W.-Y. Wang, and B.-H. Mao, “Borderline-smote: a new over-\nsampling method in imbalanced data sets learning,” in International\nconference on intelligent computing.\nSpringer, 2005, pp. 878–887.\n[54] A. X. Wang, S. S. Chukova, and B. P. Nguyen, “Synthetic minority\noversampling using edited displacement-based k-nearest neighbors,”\nApplied Soft Computing, vol. 148, p. 110895, 2023.\n[55] M. Mukherjee and M. Khushi, “Smote-enc: A novel smote-based\nmethod to generate synthetic data for nominal and continuous features,”\nApplied system innovation, vol. 4, no. 1, p. 18, 2021.\n[56] H. He, Y. Bai, E. A. Garcia, and S. Li, “Adasyn: Adaptive synthetic\nsampling approach for imbalanced learning,” in 2008 IEEE interna-\ntional joint conference on neural networks (IEEE world congress on\ncomputational intelligence).\nIeee, 2008, pp. 1322–1328.\n[57] B. Nowok, G. M. Raab, and C. Dibben, “synthpop: Bespoke creation\nof synthetic data in r,” Journal of statistical software, vol. 74, pp. 1–26,\n2016.\n[58] L. Xu, M. Skoularidou, A. Cuesta-Infante, and K. Veeramachaneni,\n“Modeling tabular data using conditional gan,” Advances in neural\ninformation processing systems, vol. 32, 2019.\n[59] T. Liu, Z. Qian, J. Berrevoets, and M. van der Schaar, “Goggle:\nGenerative modelling for tabular data by learning relational structure,”\nin The Eleventh International Conference on Learning Representations,\n2023.\n[60] Z. Zhao, A. Kunar, R. Birke, and L. Y. Chen, “Ctab-gan: Effective\ntable data synthesizing,” in Asian Conference on Machine Learning.\nPMLR, 2021, pp. 97–112.\n[61] Z. Zhao, A. Kunar, R. Birke, H. Van der Scheer, and L. Y. Chen, “Ctab-\ngan+: Enhancing tabular data synthesis,” Frontiers in big Data, vol. 6,\np. 1296508, 2024.\n[62] S. Minaee, T. Mikolov, N. Nikzad, M. Chenaghlu, R. Socher, X. Ama-\ntriain, and J. Gao, “Large language models: A survey,” arXiv preprint\narXiv:2402.06196, 2024.\n[63] V. Borisov, K. Sessler, T. Leemann, M. Pawelczyk, and G. Kasneci,\n“Language models are realistic tabular data generators,” in The\nEleventh International Conference on Learning Representations, 2023.\n[Online]. Available: https://openreview.net/forum?id=cEygmQNOeI\n[64] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman,\nD. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., “Gpt-4\ntechnical report,” arXiv preprint arXiv:2303.08774, 2023.\n[65] P. Dhariwal and A. Nichol, “Diffusion models beat gans on image\nsynthesis,” Advances in neural information processing systems, vol. 34,\npp. 8780–8794, 2021.\n[66] J. Kim, C. Lee, Y. Shin, S. Park, M. Kim, N. Park, and J. Cho, “Sos:\nScore-based oversampling for tabular data,” in Proceedings of the 28th\nACM SIGKDD Conference on Knowledge Discovery and Data Mining,\n2022, pp. 762–772.\n[67] X. Fang, W. Xu, F. A. Tan, Z. Hu, J. Zhang, Y. Qi, S. H.\nSengamedu, and C. Faloutsos, “Large language models (LLMs)\non tabular data: Prediction, generation, and understanding - a\nsurvey,” Transactions on Machine Learning Research, 2024. [Online].\nAvailable: https://openreview.net/forum?id=IZnrCGF9WI\n[68] S. Zheng and N. Charoenphakdee, “Diffusion models for missing value\nimputation in tabular data,” in NeurIPS 2022 First Table Representation\nWorkshop.\n[69] L. Lillard, J. P. Smith, and F. Welch, “What do we really know about\nwages? the importance of nonreporting and census imputation,” Journal\nof Political Economy, vol. 94, no. 3, Part 1, pp. 489–506, 1986.\n[70] B. J. Wells, K. M. Chagin, A. S. Nowacki, and M. W. Kattan,\n“Strategies for handling missing data in electronic health record derived\ndata,” Egems, vol. 1, no. 3, 2013.\n[71] T. Emmanuel, T. Maupong, D. Mpoeleng, T. Semong, B. Mphago, and\nO. Tabona, “A survey on missing data in machine learning,” Journal\nof Big data, vol. 8, pp. 1–37, 2021.\n[72] D. B. Rubin, “Inference and missing data,” Biometrika, vol. 63, no. 3,\npp. 581–592, 1976.\n[73] J. Shi, M. Xu, H. Hua, H. Zhang, S. Ermon, and J. Leskovec, “Tabdiff:\na unified diffusion model for multi-modal tabular data generation,” in\nNeurIPS 2024 Third Table Representation Learning Workshop.\n[74] Y. Song and S. Ermon, “Generative modeling by estimating gradients\nof the data distribution,” Advances in neural information processing\nsystems, vol. 32, 2019.\n[75] P. E. Kloeden, E. Platen, P. E. Kloeden, and E. Platen, Stochastic\ndifferential equations.\nSpringer, 1992.\n[76] R. T. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud, “Neural\nordinary differential equations,” Advances in neural information pro-\ncessing systems, vol. 31, 2018.\n[77] J. Ho and T. Salimans, “Classifier-free diffusion guidance,” in NeurIPS\n2021 Workshop on Deep Generative Models and Downstream Appli-\ncations, 2021.\n[78] L. Cui, H. Li, K. Chen, L. Shou, and G. Chen, “Tabular data aug-\nmentation for machine learning: Progress and prospects of embracing\ngenerative ai,” arXiv preprint arXiv:2407.21523, 2024.\n[79] Y. Ouyang, L. Xie, C. Li, and G. Cheng, “Missdiff: Training diffusion\nmodels on tabular data with missing values,” in ICML 2023 Workshop\non Structured Probabilistic Inference {\\&} Generative Modeling,\n2023.\n[80] I. Nicholas, H. Kuo, F. Garcia, A. Sonnerborg, M. Bohm, R. Kaiser,\nM. Zazzi, L. Jorm, and S. Barbieri, “Synthetic health-related lon-\ngitudinal data with mixed-type variables generated using diffusion\nmodels,” in NeurIPS 2023 Workshop on Synthetic Data Generation\nwith Generative AI, 2023.\n[81] T. Sattarov, M. Schreyer, and D. Borth, “Findiff: Diffusion models for\nfinancial tabular data generation,” in Proceedings of the Fourth ACM\nInternational Conference on AI in Finance, 2023, pp. 64–72.\n[82] H. He, S. Zhao, Y. Xi, and J. C. Ho, “Meddiff: Generating electronic\nhealth records using accelerated denoising diffusion model,” arXiv\npreprint arXiv:2302.04355, 2023.\n[83] T. Ceritli, G. O. Ghosheh, V. K. Chauhan, T. Zhu, A. P. Creagh, and\nD. A. Clifton, “Synthesizing mixed-type electronic health records using\ndiffusion models,” arXiv preprint arXiv:2302.14679, 2023.\n[84] H. He, Y. Xi, Y. Chen, B. Malin, J. Ho et al., “A flexible generative\nmodel for heterogeneous tabular ehr with missing modality,” in The\nTwelfth International Conference on Learning Representations, 2024.\n[85] H. Yuan, S. Zhou, and S. Yu, “Ehrdiff: Exploring realistic ehr synthesis\nwith diffusion models,” Transactions on Machine Learning Research,\n2024.\n[86] C. Liu and C. Liu, “Entity-based financial tabular data synthesis\nwith diffusion models,” in Proceedings of the 5th ACM International\nConference on AI in Finance, 2024, pp. 547–554.\n[87] M. Schreyer, T. Sattarov, A. Sim, and K. Wu, “Imb-findiff: Conditional\ndiffusion models for class imbalance synthesis of financial tabular\ndata,” in Proceedings of the 5th ACM International Conference on AI\nin Finance, 2024, pp. 617–625.\n[88] J. Han, Z. Chen, Y. Li, Y. Kou, E. Halperin, R. E. Tillman, and Q. Gu,\n“Guided discrete diffusion for electronic health record generation,”\narXiv preprint arXiv:2404.12314, 2024.\n[89] J. Si, Z. Ou, M. Qu, and Y. Li, “Tabunite: Efficient encoding schemes\nfor flow and diffusion tabular generative models,” 2024. [Online].\nAvailable: https://openreview.net/forum?id=Zoli4UAQVZ\n[90] M.\nMueller,\nK.\nGruber,\nand\nD.\nFok,\n“Continuous\ndiffusion\nfor\nmixed-type\ntabular\ndata,”\n2024.\n[Online].\nAvailable:\nhttps:\n//arxiv.org/abs/2312.10431\n[91] N. Carlini, J. Hayes, M. Nasr, M. Jagielski, V. Sehwag, F. Tramer,\nB. Balle, D. Ippolito, and E. Wallace, “Extracting training data from\ndiffusion models,” in 32nd USENIX Security Symposium (USENIX\nSecurity 23), 2023, pp. 5253–5270.\n\nMANUSCRIPT SUBMITTED TO IEEE FOR POSSIBLE PUBLICATION\n23\n[92] A. Lugmayr, M. Danelljan, A. Romero, F. Yu, R. Timofte, and\nL. Van Gool, “Repaint: Inpainting using denoising diffusion probabilis-\ntic models,” in Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, 2022, pp. 11 461–11 471.\n[93] K. Hornik, M. Stinchcombe, and H. White, “Multilayer feedforward\nnetworks are universal approximators,” Neural networks, vol. 2, no. 5,\npp. 359–366, 1989.\n[94] T. Chen and C. Guestrin, “Xgboost: A scalable tree boosting system,”\nin Proceedings of the 22nd acm sigkdd international conference on\nknowledge discovery and data mining, 2016, pp. 785–794.\n[95] E. Poslavskaya and A. Korolev, “Encoding categorical data: Is\nthere yet anything’hotter’than one-hot encoding?” arXiv preprint\narXiv:2312.16930, 2023.\n[96] R. Krishnan, D. Liang, and M. Hoffman, “On the challenges of\nlearning with inference networks on sparse, high-dimensional data,”\nin International conference on artificial intelligence and statistics.\nPMLR, 2018, pp. 143–151.\n[97] T. Chen, R. ZHANG, and G. Hinton, “Analog bits: Generating discrete\ndata using diffusion models with self-conditioning,” in The Eleventh\nInternational Conference on Learning Representations.\n[98] C. Ma, S. Tschiatschek, R. Turner, J. M. Hern´andez-Lobato, and\nC. Zhang, “Vaem: a deep generative model for heterogeneous mixed\ntype data,” Advances in Neural Information Processing Systems,\nvol. 33, pp. 11 237–11 247, 2020.\n[99] A. Hyv¨arinen and P. Dayan, “Estimation of non-normalized statistical\nmodels by score matching.” Journal of Machine Learning Research,\nvol. 6, no. 4, 2005.\n[100] S. Dieleman, L. Sartran, A. Roshannai, N. Savinov, Y. Ganin, P. H.\nRichemond, A. Doucet, R. Strudel, C. Dyer, C. Durkan et al., “Contin-\nuous diffusion for categorical data,” arXiv preprint arXiv:2211.15089,\n2022.\n[101] P. Yadav, M. Steinbach, V. Kumar, and G. Simon, “Mining electronic\nhealth records (ehrs) a survey,” ACM Computing Surveys (CSUR),\nvol. 50, no. 6, pp. 1–40, 2018.\n[102] J. Song, C. Meng, and S. Ermon, “Denoising diffusion implicit mod-\nels,” in International Conference on Learning Representations, 2021.\n[103] D. G. Anderson, “Iterative procedures for nonlinear integral equations,”\nJournal of the ACM (JACM), vol. 12, no. 4, pp. 547–560, 1965.\n[104] T. Karras, M. Aittala, T. Aila, and S. Laine, “Elucidating the design\nspace of diffusion-based generative models,” Advances in neural infor-\nmation processing systems, vol. 35, pp. 26 565–26 577, 2022.\n[105] W. Pang, M. Shafieinejad, L. Liu, and X. He, “Clavaddpm: Multi-\nrelational data synthesis with cluster-guided diffusion models,” Ad-\nvances in Neural Information Processing Systems, 2024.\n[106] V. Hudovernik, “Relational data generation with graph neural networks\nand latent diffusion models,” in NeurIPS 2024 Third Table Represen-\ntation Learning Workshop, 2024.\n[107] V. Hudovernik, M. Jurkoviˇc, and E.\nˇStrumbelj, “Benchmarking\nthe fidelity and utility of synthetic relational data,” arXiv preprint\narXiv:2410.03411, 2024.\n[108] W.-C. Lin and C.-F. Tsai, “Missing value imputation: a review and\nanalysis of the literature (2006–2017),” Artificial Intelligence Review,\nvol. 53, pp. 1487–1509, 2020.\n[109] D. Jarrett, B. C. Cebere, T. Liu, A. Curth, and M. van der Schaar,\n“Hyperimpute: Generalized iterative imputation with automatic model\nselection,” in International Conference on Machine Learning. PMLR,\n2022, pp. 9916–9937.\n[110] S. Van Buuren and C. G. Oudshoorn, “Multivariate imputation by\nchained equations,” 2000.\n[111] L. Gondara and K. Wang, “Mida: Multiple imputation using denoising\nautoencoders,” in Advances in Knowledge Discovery and Data Min-\ning: 22nd Pacific-Asia Conference, PAKDD 2018, Melbourne, VIC,\nAustralia, June 3-6, 2018, Proceedings, Part III 22.\nSpringer, 2018,\npp. 260–272.\n[112] A. Nazabal, P. M. Olmos, Z. Ghahramani, and I. Valera, “Handling\nincomplete heterogeneous data using vaes,” Pattern Recognition, vol.\n107, p. 107501, 2020.\n[113] Y. Liu, T. Ajanthan, H. Husain, and V. Nguyen, “Self-supervision im-\nproves diffusion models for tabular data imputation,” in Proceedings of\nthe 33rd ACM International Conference on Information and Knowledge\nManagement, 2024, pp. 1513–1522.\n[114] M. Villaiz´an-Vallelado, M. Salvatori, C. Segura, and I. Arapakis,\n“Diffusion models for tabular data imputation and synthetic data\ngeneration,” arXiv preprint arXiv:2407.02549, 2024.\n[115] A. Wibisono, P. Mursanto, S. See et al., “Natural generative noise\ndiffusion model imputation,” Knowledge-Based Systems, vol. 301, p.\n112310, 2024.\n[116] Z. Chen, H. Li, F. Wang, O. Zhang, H. Xu, X. Jiang, Z. Song,\nand H. Wang, “Rethinking the diffusion models for missing data\nimputation: A gradient flow perspective,” in The Thirty-eighth Annual\nConference on Neural Information Processing Systems, 2024.\n[117] H. Zhang, L. Fang, and P. S. Yu, “Unleashing the potential of\ndiffusion models for incomplete data imputation,” 2024. [Online].\nAvailable: https://arxiv.org/abs/2405.20690\n[118] Y. Tashiro, J. Song, Y. Song, and S. Ermon, “Csdi: Conditional\nscore-based diffusion models for probabilistic time series imputation,”\nAdvances in Neural Information Processing Systems, vol. 34, pp.\n24 804–24 816, 2021.\n[119] Y. Gorishniy, I. Rubachev, V. Khrulkov, and A. Babenko, “Revisiting\ndeep learning models for tabular data,” Advances in Neural Information\nProcessing Systems, vol. 34, pp. 18 932–18 943, 2021.\n[120] P. Mokrov, A. Korotin, L. Li, A. Genevay, J. M. Solomon, and\nE. Burnaev, “Large-scale wasserstein gradient flows,” Advances in\nNeural Information Processing Systems, vol. 34, pp. 15 243–15 256,\n2021.\n[121] A. P. Dempster, N. M. Laird, and D. B. Rubin, “Maximum likelihood\nfrom incomplete data via the em algorithm,” Journal of the royal\nstatistical society: series B (methodological), vol. 39, no. 1, pp. 1–22,\n1977.\n[122] A.\nShankar,\nH.\nBrouwer,\nR.\nHai,\nand\nL.\nChen,\n“\nSiloFuse:\nCross-silo Synthetic Data Generation with Latent Tabular Diffusion\nModels\n,”\nin\n2024\nIEEE\n40th\nInternational\nConference\non\nData\nEngineering\n(ICDE).\nLos\nAlamitos,\nCA,\nUSA:\nIEEE\nComputer Society, May 2024, pp. 110–123. [Online]. Available:\nhttps://doi.ieeecomputersociety.org/10.1109/ICDE60146.2024.00016\n[123] T. Sattarov, M. Schreyer, and D. Borth, “Fedtabdiff: Federated learning\nof diffusion probabilistic models for synthetic mixed-type tabular data\ngeneration,” arXiv preprint arXiv:2401.06263, 2024.\n[124] Z. Yang, P. Guo, K. Zanna, and A. Sano, “Balanced mixed-\ntype tabular data synthesis with diffusion models,” arXiv preprint\narXiv:2404.08254, 2024.\n[125] T. Sattarov, M. Schreyer, and D. Borth, “Differentially private federated\nlearning of diffusion models for synthetic tabular data generation,”\narXiv preprint arXiv:2412.16083, 2024.\n[126] B. McMahan and D. Ramage, “Federated learning: Collaborative\nmachine learning without centralized training data,” Google Research\nBlog, vol. 3, 2017.\n[127] C. Dwork, A. Roth et al., “The algorithmic foundations of differential\nprivacy,” Foundations and Trends® in Theoretical Computer Science,\nvol. 9, no. 3–4, pp. 211–407, 2014.\n[128] G. Zamberg, M. Salhov, O. Lindenbaum, and A. Averbuch, “Tabadm:\nUnsupervised tabular anomaly detection with diffusion models,” arXiv\npreprint arXiv:2307.12336, 2023.\n[129] V. Livernoche, V. Jain, Y. Hezaveh, and S. Ravanbakhsh, “On diffusion\nmodeling for anomaly detection,” in The Twelfth International Confer-\nence on Learning Representations, 2024.\n[130] S. Li, J. Yu, Y. Lu, G. Yang, X. Du, and S. Liu, “Self-supervised\nenhanced denoising diffusion for anomaly detection,” Information\nSciences, vol. 669, p. 120612, 2024.\n[131] Anonymous, “Anomaly detection by estimating gradients of the\ntabular data distribution,” in Submitted to The Thirteenth International\nConference on Learning Representations, 2024, under review. [Online].\nAvailable: https://openreview.net/forum?id=7QDIFrtAsB\n[132] R. Roy, D. Tiwari, and A. Pandey, “Frauddiffuse: Diffusion-aided\nsynthetic fraud augmentation for improved fraud detection,” in Pro-\nceedings of the 5th ACM International Conference on AI in Finance,\n2024, pp. 90–98.\n[133] Y. Pushkarenko and V. Zaslavskyi, “Synthetic data generation for\nfraud detection using diffusion models,” Information & Security: An\nInternational Journal, vol. 55, no. 2, pp. 185–198, 2024. [Online].\nAvailable: https://doi.org/10.11610/isij.5534\n[134] S. S. Sahoo, M. Arriola, Y. Schiff, A. Gokaslan, E. Marroquin,\nJ. T. Chiu, A. Rush, and V. Kuleshov, “Simple and effective masked\ndiffusion language models,” arXiv preprint arXiv:2406.07524, 2024.\n[135] I. Gulrajani and T. B. Hashimoto, “Likelihood-based diffusion language\nmodels,” Advances in Neural Information Processing Systems, vol. 36,\n2024.\n[136] X. Han, S. Kumar, and Y. Tsvetkov, “Ssd-lm: Semi-autoregressive\nsimplex-based diffusion language model for text generation and mod-\nular control,” in The 61st Annual Meeting Of The Association For\nComputational Linguistics, 2023.\n[137] X. Li, J. Thickstun, I. Gulrajani, P. S. Liang, and T. B. Hashimoto,\n“Diffusion-lm improves controllable text generation,” Advances in\nNeural Information Processing Systems, vol. 35, pp. 4328–4343, 2022.\n\nMANUSCRIPT SUBMITTED TO IEEE FOR POSSIBLE PUBLICATION\n24\n[138] J. Lovelace, V. Kishore, C. Wan, E. Shekhtman, and K. Q. Weinberger,\n“Latent diffusion for language generation,” Advances in Neural Infor-\nmation Processing Systems, vol. 36, 2024.\n[139] R. Strudel, C. Tallec, F. Altch´e, Y. Du, Y. Ganin, A. Mensch,\nW. Grathwohl, N. Savinov, S. Dieleman, L. Sifre et al., “Self-\nconditioned embedding diffusion for text generation,” arXiv preprint\narXiv:2211.04236, 2022.\n[140] F. Regol and M. Coates, “Diffusing gaussian mixtures for generating\ncategorical data,” in Proceedings of the AAAI Conference on Artificial\nIntelligence, vol. 37, no. 8, 2023, pp. 9570–9578.\n[141] J. Song, C. Meng, and S. Ermon, “Denoising diffusion implicit mod-\nels,” in International Conference on Learning Representations, 2021.\n[142] J. Shi, K. Han, Z. Wang, A. Doucet, and M. K. Titsias, “Simplified\nand generalized masked diffusion for discrete data,” arXiv preprint\narXiv:2406.04329, 2024.\n[143] E. Hoogeboom, A. A. Gritsenko, J. Bastings, B. Poole, R. van den Berg,\nand T. Salimans, “Autoregressive diffusion models,” in International\nConference on Learning Representations, 2021.\n[144] J. Ye, Z. Zheng, Y. Bao, L. Qian, and Q. Gu, “Diffusion language mod-\nels can perform many tasks with scaling and instruction-finetuning,”\narXiv preprint arXiv:2308.12219, 2023.\n[145] A. Jolicoeur-Martineau, K. Li, R. Pich´e-Taillefer, T. Kachman, and\nI. Mitliagkas, “Gotta go fast when generating data with score-based\nmodels,” arXiv preprint arXiv:2105.14080, 2021.\n[146] M. Reid, V. J. Hellendoorn, and G. Neubig, “Diffuser: Discrete diffu-\nsion via edit-based reconstruction,” arXiv preprint arXiv:2210.16886,\n2022.\n[147] A. Campbell, J. Benton, V. De Bortoli, T. Rainforth, G. Deligiannidis,\nand A. Doucet, “A continuous time framework for discrete denoising\nmodels,” Advances in Neural Information Processing Systems, vol. 35,\npp. 28 266–28 279, 2022.\n[148] H. Sun, L. Yu, B. Dai, D. Schuurmans, and H. Dai, “Score-based\ncontinuous-time discrete diffusion models,” in The Eleventh Interna-\ntional Conference on Learning Representations, 2023.\n[149] Z. Chen, H. Yuan, Y. Li, Y. Kou, J. Zhang, and Q. Gu, “Fast sampling\nvia de-randomization for discrete diffusion models,” 2024. [Online].\nAvailable: https://openreview.net/forum?id=m4Ya9RkEEW\n[150] A. Alaa, B. Van Breugel, E. S. Saveliev, and M. van der Schaar, “How\nfaithful is your synthetic data? sample-level metrics for evaluating and\nauditing generative models,” in International Conference on Machine\nLearning.\nPMLR, 2022, pp. 290–306.\n[151] E. Choi, S. Biswal, B. Malin, J. Duke, W. F. Stewart, and J. Sun, “Gen-\nerating multi-label discrete patient records using generative adversarial\nnetworks,” in Machine learning for healthcare conference.\nPMLR,\n2017, pp. 286–305.\n[152] J. Jia and N. Z. Gong, “{AttriGuard}: A practical defense against\nattribute inference attacks via adversarial machine learning,” in 27th\nUSENIX Security Symposium (USENIX Security 18), 2018, pp. 513–\n529.\n[153] R. Shokri, M. Stronati, C. Song, and V. Shmatikov, “Membership\ninference attacks against machine learning models,” in 2017 IEEE\nsymposium on security and privacy (SP).\nIEEE, 2017, pp. 3–18.\n[154] S. Han, X. Hu, H. Huang, M. Jiang, and Y. Zhao, “Adbench: Anomaly\ndetection benchmark,” in Thirty-Sixth Conference on Neural Informa-\ntion Processing Systems Datasets and Benchmarks Track, 2022.\n",
  "metadata": {
    "source_path": "papers/arxiv/Diffusion_Models_for_Tabular_Data_Challenges_Current_Progress_and\n__Future_Directions_3314c4b33f5952b2.pdf",
    "content_hash": "3314c4b33f5952b2acac4ad9e1e97cebfaa9c1b443c536f75bfd7a3f658b3c78",
    "arxiv_id": null,
    "title": "Diffusion_Models_for_Tabular_Data_Challenges_Current_Progress_and\n__Future_Directions_3314c4b33f5952b2",
    "author": "",
    "creation_date": "D:20250225024642Z",
    "published": "2025-02-25T02:46:42",
    "pages": 24,
    "size": 1004057,
    "file_mtime": 1740470182.8469665
  }
}