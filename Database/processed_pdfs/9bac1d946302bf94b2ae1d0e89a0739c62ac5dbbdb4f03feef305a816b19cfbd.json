{
  "text": "Under review as at ICLR Workshop on Neural Network Weights as a New Data Modality 2025\nTHE EMPIRICAL IMPACT OF REDUCING SYMMETRIES ON\nTHE PERFORMANCE OF DEEP ENSEMBLES AND MOE\nAndrei Chernov\nIndependent Researcher\nchernov.andrey.998@gmail.com\nOleg Novitskij\nHSE University\noanovitskii@edu.hse.ru\nABSTRACT\nRecent studies have shown that reducing symmetries in neural networks enhances lin-\near mode connectivity between networks without requiring parameter space alignment,\nleading to improved performance in linearly interpolated neural networks. However, in\npractical applications, neural network interpolation is rarely used; instead, ensembles of\nnetworks are more common. In this paper, we empirically investigate the impact of re-\nducing symmetries on the performance of deep ensembles and Mixture of Experts (MoE)\nacross five datasets. Additionally, to explore deeper linear mode connectivity, we intro-\nduce the Mixture of Interpolated Experts (MoIE). Our results show that deep ensembles\nbuilt on asymmetric neural networks achieve significantly better performance as ensemble\nsize increases compared to their symmetric counterparts. In contrast, our experiments do\nnot provide conclusive evidence on whether reducing symmetries affects both MoE and\nMoIE architectures.\n1\nINTRODUCTION\nIn the last decade, neural networks have proven to be one of the most important algorithms in the field of\nmachine learning. Despite their undeniable empirical success, many fundamental questions remain unan-\nswered. One such question concerns parameter space symmetries: for any given set of neural network\nparameters, there exist numerous ‘twins’ that produce exactly the same output for every input while having\ndifferent parameter values.\nThere are multiple sources of symmetry in neural network architectures. One prominent example is permu-\ntation symmetry in fully connected layers. Consider a standard multi-layer perceptron (MLP). If we swap\ntwo neurons in a hidden layer along with their incoming and outgoing weights, the network will produce\nthe exact same output. As a result, any hidden layer of size n has n! different sets of parameters that yield\nidentical outputs. Activation functions such as ReLU Nair & Hinton (2010) can also produce symmetries\nWiese et al. (2023).\nThe effects of parameter symmetries have been studied in various areas, including neuron interpretability\nGodfrey et al. (2022), optimization Neyshabur et al. (2015), and Bayesian deep learning Kurle et al. (2022).\nIn this work, we primarily focus on the impact of symmetries on model accuracy. It has been shown in Lim\net al. (2024) that eliminating redundant parameters in neural networks improves linear mode connectivity,\nthereby enhancing the performance of networks whose parameters are obtained by interpolating between\ntwo trained models. In this study, we present the Mixture of Interpolated Experts model (see Section 4.3 for\ndetails) and investigate how parameter symmetries influence its performance.\nHowever, the practical application of interpolated neural networks remains controversial, as such architec-\ntures usually do not provide a performance boost. The most common approach to leveraging multiple neural\n1\narXiv:2502.17391v1  [cs.LG]  24 Feb 2025\n\nUnder review as at ICLR Workshop on Neural Network Weights as a New Data Modality 2025\nnetworks is through ensembles, where outputs from several models are aggregated to form a final predic-\ntion—examples include Deep Ensembles Lakshminarayanan et al. (2017) and Mixture of Experts (MoE)\nFedus et al. (2022). In this paper, we empirically investigate how reducing symmetry impacts the perfor-\nmance of Deep Ensembles and MoE on 5 datasets.\n2\nRELATED WORK\n2.1\nW-ASYMMETRIC MLP\nVarious methods for breaking parameter symmetries in neural networks have been studied, including ap-\nproaches to removing permutation symmetries Pourzanjani et al. (2017); Pittorino et al. (2022), scaling\nsymmetries Badrinarayanan et al. (2015), and sign symmetries Wiese et al. (2023). However, in most of\nthese approaches, the neural network architectures or training processes deviate from standard practices,\nmaking them difficult to apply in practice. In this work, we fully adopt the approach from Lim et al. (2024)\nto break symmetries in neural networks. This method randomly freezes a portion of the neural network’s\nweights before training, keeping them unchanged throughout training (see Section 4.1 for details). Notably,\nit does not require any special modifications to the training process. Authors of Lim et al. (2024) showed\nthat breaking symmetries improves linear mode connectivity between two independently trained neural net-\nworks. In this paper, we investigate the empirical impact of reducing symmetries on the performance of\nDeep Ensembles and Mixture of Experts.\n2.2\nNEURAL NETWORK ENSEMBLES\nIn this study, we employ two different approaches for ensembling neural networks. The first approach,\nknown as Deep Ensembles Lakshminarayanan et al. (2017), trains k neural networks independently and\naverages their outputs to obtain the final prediction.\nThe second approach is the Mixture of Experts (MoE) Yuksel et al. (2012), which consists of two main\ncomponents: experts and a gating network. Each expert generates an output, but unlike Deep Ensembles,\nthe final prediction is obtained through a weighted average of the experts’ outputs. The weights for each\nexpert are dynamically predicted by the gating network rather than being fixed. Recently, MoE architectures\nutilizing MLP models as experts have gained popularity Fedus et al. (2022) especially in NLP Du et al.\n(2022) and CV domains Puigcerver et al. (2023); Riquelme et al. (2021). In this work, we adapt MoE\narchitectures for tabular data from Chernov (2025). We cover it in detail in Section 4.2.\n3\nDATASETS\nFor our work, we selected five datasets to cover different problems:\n• Regression: California Housing Prices dataset Pace & Barry (1997).\n• Binary classification: Churn Modeling1 and Adult Income Kohavi et al. (1996).\n• Multi-class classification: MNIST Deng (2012) and Otto Group Product2.\nAppendix A.1 summarizes the key attributes of these datasets. To ensure consistency, we applied a standard-\nized preprocessing pipeline. Each dataset was split into training, validation, and testing sets with an overall\npartitioning of 64% for training, 16% for validation, and 20% for testing. Real-valued features were scaled\n1https://www.kaggle.com/shrutimechlearn/churn-modelling\n2https://www.kaggle.com/c/otto-group-product-classification-challenge/data\n2\n\nUnder review as at ICLR Workshop on Neural Network Weights as a New Data Modality 2025\nusing a StandardScaler, and for classification tasks, the splits were stratified by the target variable.\nAdditional preprocessing steps were applied to each dataset as follows:\n• Churn Modeling dataset: Non-informative columns such as RowNumber, CustomerId, and\nSurname were removed.\n• Otto Group Product dataset: The id column was dropped, and the target variable was encoded\nusing a LabelEncoder.\n• Adult Income dataset: The target variable was transformed by mapping <=$50K to 0 and >$50K\nto 1. Categorical features were processed using a OneHotEncoder, with missing values imputed\nas MissingValue, while numerical missing values were filled with 0.\n• California Housing Prices dataset: Since this dataset contains no missing values, its numerical\nfeatures were simply scaled.\n• MNIST dataset: Grayscale images were preprocessed by normalizing and centering pixel values.\n.\n4\nMODELS\n4.1\nW-ASYMMETRIC MLP\nIn this paper, we fully adopt the implementation of W-Asymmetric MLP (WMLP) from Lim et al. (2024),\nwhere it was theoretically proven that this approach significantly reduces parameter symmetries. This is\nachieved by freezing a small portion of the weights, approximately O(n1/4) for details see Algorithm 1\nIt is important to emphasize that in ensemble networks utilizing different WMLP models, the frozen neu-\nrons—both in value and position—remain identical across all instances. For the hidden layers, we use the\nGeLU activation function from Hendrycks & Gimpel (2016) in both MLP and WMLP.\n4.2\nMIXTURE OF EXPERTS\nIn Chernov (2025), it was shown that MoE performs at least as well as a vanilla MLP on tabular data while\nrequiring significantly fewer parameters. In this paper, we compare the performance of MoE with MLP as\nexperts against MoE with WMLP experts.\nFrom Chernov (2025), we utilize both the vanilla MoE, where logistic regression is used as a gating neural\nnetwork, and the Gumbel Gating MoE (GG MoE), which employs the Gumbel-softmax function instead of\nthe standard Softmax activation for logistic regression. Following the original paper, we use 10 samples\nfrom the Gumbel-softmax distribution during inference.\n4.3\nMIXTURE OF INTERPOLATED EXPERTS\nSince Lim et al. (2024) demonstrated that reducing symmetries improves the performance of linearly inter-\npolated neural networks, we evaluate the performance of the Mixture of Interpolated Experts (MOIE). MOIE\nuses the same gating function as MoE but, instead of computing a weighted average of the final outputs, it\nlinearly interpolates the weights of the experts to produce an output:\nˆy = Expert architecture\n k\nX\ni\nαi(x)Wi(x)\n!\n,\n3\n\nUnder review as at ICLR Workshop on Neural Network Weights as a New Data Modality 2025\nAlgorithm 1 WMLP Weight and Bias Initialization with Masking\nRequire: Number of layers L = 4, hidden dimension d ∈{64, 128, 256}, and mask seeding parameter\nmask num.\n1: Define fixed weights per output unit:\nn(1)\nfix = 2,\nn(l)\nfix =\n\u001a4,\nif d = 256,\n3,\notherwise\nfor l > 1.\n2: for l = 1, . . . , L do\n3:\nLet W (l) ∈Routl×inl be the weight matrix.\n4:\nfor i = 1, . . . , outl do\n5:\nGenerate Mask:\n6:\nFor each output unit, select a random subset of n(l)\nfix input indices to be fixed.The fixed\n7:\npositions are determined using a seed based on l and the output unit index, ensuring\n8:\nreproducibility within an ensemble.\n9:\nfor j = 1, . . . , inl do\n10:\nif j is in the fixed subset for unit i then\n11:\nSet W (l)\nij ∼N(0, 1). The random seed for fixed weights depends on the layer l and\n12:\nweight position. These weights are then frozen.\n13:\nelse\n14:\nInitialize W (l)\nij using Kaiming Uniform Initialization with parameter\n√\n5. The random\n15:\nseed for non-frozen weights depends on the repetition number, the estimator index in the\n16:\nensemble, l, and the weight’s position.\n17:\nend if\n18:\nend for\n19:\nend for\n20:\nInitialize Bias:\n21:\nSet b(l) uniformly in\n\u0014\n−\n1\n√inl\n,\n1\n√inl\n\u0015\n,\n▷The random seed for bias initialization depends on the repetition number, the estimator index, l, and\nthe bias position.\n22: end for\nwhere ˆy is the final prediction, k is the number of experts, α is an output from a gating network, and Wi\nrepresents the model parameters of each expert. The expert architecture is selected from MLP, WMLP.\n5\nEXPERIMENTS\n5.1\nSETUP\nIn this section, we describe the details of the training and evaluation procedures applied to Deep Ensembles\n(Section 5.1.1), MoE and MoIE (Section 5.1.2).\n4\n\nUnder review as at ICLR Workshop on Neural Network Weights as a New Data Modality 2025\n5.1.1\nDEEP ENSEMBLE\nWe trained models with a batch size of 256. For constructing Deep Ensembles, we trained 64 instances of\nboth the MLP and WMLP models, each initialized with a different random seed to ensure variability in the\nfree weights.\nFor WMLP, a fixed number of random weights per row, denoted as nfix, was selected and frozen in each layer.\nThese frozen weights were sampled from a N(0, I) distribution. To reduce variance in the final metrics, we\nrepeated training and evaluation 10 times independently and reported the average evaluation metrics on the\ntest sets.\nFor Deep Ensembles, we utilized MLP and WMLP blocks. Their structure consisted of an input layer that\nmapped the number of dataset features to a hidden dimension (hidden dim), followed by two hidden layers\nof size hidden dim × hidden dim, and an output layer of size hidden dim × out features, where out features\nwas set to 1 for regression and to the number of classes for classification. Experiments for Deep Ensembles\nwere conducted for hidden dim values of 64, 128, and 256.\nLoss functions were selected based on the task: MSELoss for regression and CrossEntropyLoss for\nclassification, with RMSE and accuracy serving as the evaluation metrics, respectively. Optimization was\ncarried out using the AdamW optimizer with a learning rate of 1 × 10−3 and a weight decay of 3 × 10−2.\nEach network was trained for up to 1000 epochs, with batch size = 256 and early stopping triggered if the\nvalidation loss did not improve for 16 consecutive epochs. Training was performed in parallel on 64 CPUs.\nAfter each training iteration, we logged the training time, the number of epochs executed, and the perfor-\nmance metric for each of the 64 MLP and WMLP models. Finally, the individual models were aggregated\ninto Deep Ensembles of 2, 4, 8, 16, 32, and 64 networks. For regression tasks, ensemble predictions were\ncomputed as the mean of the individual outputs, while for classification tasks the logits were averaged and\nthe final prediction was determined via the argmax function. For each ensemble, both the ensemble perfor-\nmance metric and an interpolation metric—derived from averaging the model weights—were recorded.\n5.1.2\nMOE AND MOIE\nIn experiments with MoE and MoIE, we used both MLP and WMLP architectures, along with the same loss\nfunctions, evaluation metrics, training procedures, and optimizer parameters as described in Section 5.1.1.\nFor these experiments, the expert hidden dimension was fixed at 64. In the WMLP architecture, the number\nof fixed weights per output unit, nfix, was set to 2 for the input layer and 3 for subsequent layers. The number\nof experts was varied among [2, 4, 8, 16, 32, 64]. We conducted experiments for all models described in\nSections 4.2 and 4.3.\n5.2\nRESULTS\nFigure 1 presents the experimental results, showing the average performance improvements in test metrics\nacross different random seeds. Specifically, we report accuracy for classification tasks and RMSE for re-\ngression tasks, measuring improvement relative to the average performance of a single neural network. The\nresults are presented for each dataset and hidden dimension configuration and indicate that Deep Ensembles\nwith WMLP models improve significantly more than with MLP models and this improvement increases as\nthe ensemble size increases.\nA possible explanation for this behavior could be that WMLP deep ensembles perform worse than MLP\nensembles in terms of absolute test metric values. However, this is not the case, as demonstrated in Appendix\nA.2. Given that WMLP models retain the universal approximation property, as shown in Lim et al. (2024),\nwe believe this is a promising finding that could encourage the adoption of asymmetric neural networks in\nensembles for practical applications.\n5\n\nUnder review as at ICLR Workshop on Neural Network Weights as a New Data Modality 2025\nLikewise, Figure 2 shows the relative performance of MoE and MoIE with a varying number of experts\ncompared to their corresponding models with two experts. As discussed in Section 5.1.2, the hidden size of\neach expert remains constant. Although MoE with WMLP experts and MoIE with WMLP experts tend to\noutperform their counterparts with MLP experts on 4 out of 5 datasets, the improvements are less convincing\ncompared to Deep Ensembles. We also report absolute metrics in Appendix A.2.\nOne potential reason for unclear results in MoE and MoIE might be that the models tend to overfit, meaning\nthat as the number of parameters increases, test metrics deteriorate. We did not apply any regularization\ntechniques to the neural network architectures to avoid overcomplicating the analysis. However, addressing\nthis issue is essential, and the experimental setup for MoE and MoIE should be adjusted in future work.\n6\nCONCLUSION\nIn this paper, we empirically demonstrated that the performance of Deep Ensembles improves significantly\nwith increasing ensemble size when using W-Asymmetric MLP models compared to vanilla MLP models.\nThis result may serve as a first step toward understanding the practical impact of reducing symmetry in\nneural networks.\nHowever, based on our experiments, we cannot conclude that W-Asymmetric MLP improves the perfor-\nmance of either the Mixture of Experts (MoE) or the Mixture of Interpolated Experts (MoIE) models. As\ndiscussed in Section 5.2, the experimental setup for MoE should be refined in future work\n6\n\nUnder review as at ICLR Workshop on Neural Network Weights as a New Data Modality 2025\n2\n4\n8\n16\n32\n64\nEnsemble Size\n2\n3\n4\nImprovement (%)\nCALIFORNIA, HIDDEN SIZE = 64\n2\n4\n8\n16\n32\n64\nEnsemble Size\n2\n3\n4\n5\nImprovement (%)\nCALIFORNIA, HIDDEN SIZE = 128\n2\n4\n8\n16\n32\n64\nEnsemble Size\n2\n3\n4\n5\n6\n7\nImprovement (%)\nCALIFORNIA, HIDDEN SIZE = 256\n2\n4\n8\n16\n32\n64\nEnsemble Size\n1.0\n1.5\n2.0\n2.5\n3.0\nImprovement (%)\nOTTO, HIDDEN SIZE = 64\n2\n4\n8\n16\n32\n64\nEnsemble Size\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nImprovement (%)\nOTTO, HIDDEN SIZE = 128\n2\n4\n8\n16\n32\n64\nEnsemble Size\n1.5\n2.0\n2.5\n3.0\n3.5\nImprovement (%)\nOTTO, HIDDEN SIZE = 256\n2\n4\n8\n16\n32\n64\nEnsemble Size\n0.6\n0.8\n1.0\n1.2\nImprovement (%)\nMNIST, HIDDEN SIZE = 64\n2\n4\n8\n16\n32\n64\nEnsemble Size\n0.6\n0.8\n1.0\nImprovement (%)\nMNIST, HIDDEN SIZE = 128\n2\n4\n8\n16\n32\n64\nEnsemble Size\n0.6\n0.8\n1.0\nImprovement (%)\nMNIST, HIDDEN SIZE = 256\n2\n4\n8\n16\n32\n64\nEnsemble Size\n0.2\n0.3\n0.4\n0.5\n0.6\nImprovement (%)\nADULT, HIDDEN SIZE = 64\n2\n4\n8\n16\n32\n64\nEnsemble Size\n0.2\n0.4\n0.6\n0.8\nImprovement (%)\nADULT, HIDDEN SIZE = 128\n2\n4\n8\n16\n32\n64\nEnsemble Size\n0.4\n0.6\n0.8\n1.0\nImprovement (%)\nADULT, HIDDEN SIZE = 256\n2\n4\n8\n16\n32\n64\nEnsemble Size\n0.2\n0.4\n0.6\n0.8\nImprovement (%)\nCHURN, HIDDEN SIZE = 64\n2\n4\n8\n16\n32\n64\nEnsemble Size\n0.25\n0.50\n0.75\n1.00\nImprovement (%)\nCHURN, HIDDEN SIZE = 128\n2\n4\n8\n16\n32\n64\nEnsemble Size\n0.5\n1.0\n1.5\nImprovement (%)\nCHURN, HIDDEN SIZE = 256\nMLP ENSEMBLE\nWMLP ENSEMBLE\nFigure 1: Deep ensembles’ relative improvement in performance. The graphics depicts the relative im-\nprovement in performance of both MLP and WMLP models compared to a single MLP and WMLP neural\nnetwork, respectively.\n7\n\nUnder review as at ICLR Workshop on Neural Network Weights as a New Data Modality 2025\n2\n4\n8\n16\n32\n64\n128\nNumber of Experts\n2\n1\n0\n1\n2\nImprovement (%)\nCALIFORNIA, GATING: STANDARD\n2\n4\n8\n16\n32\n64\n128\nNumber of Experts\n0\n1\n2\n3\nImprovement (%)\nCALIFORNIA, GATING: GUMBEL\n2\n4\n8\n16\n32\n64\n128\nNumber of Experts\n0.0\n0.5\n1.0\nImprovement (%)\nOTTO, GATING: STANDARD\n2\n4\n8\n16\n32\n64\n128\nNumber of Experts\n0.0\n0.5\n1.0\n1.5\nImprovement (%)\nOTTO, GATING: GUMBEL\n2\n4\n8\n16\n32\n64\n128\nNumber of Experts\n0.6\n0.4\n0.2\n0.0\n0.2\nImprovement (%)\nMNIST, GATING: STANDARD\n2\n4\n8\n16\n32\n64\n128\nNumber of Experts\n0.0\n0.2\n0.4\n0.6\n0.8\nImprovement (%)\nMNIST, GATING: GUMBEL\n2\n4\n8\n16\n32\n64\n128\nNumber of Experts\n1.00\n0.75\n0.50\n0.25\n0.00\nImprovement (%)\nADULT, GATING: STANDARD\n2\n4\n8\n16\n32\n64\n128\nNumber of Experts\n0.0\n0.1\n0.2\n0.3\n0.4\nImprovement (%)\nADULT, GATING: GUMBEL\n2\n4\n8\n16\n32\n64\n128\nNumber of Experts\n1.0\n0.5\n0.0\n0.5\nImprovement (%)\nCHURN, GATING: STANDARD\n2\n4\n8\n16\n32\n64\n128\nNumber of Experts\n0.5\n0.0\n0.5\n1.0\nImprovement (%)\nCHURN, GATING: GUMBEL\nMLP\nWMLP\nIMLP\nIWMLP\nFigure 2: MoE and MoIE relative improvement. In these graphics, MLP represents MoE with vanilla MLP\nexperts, WMLP denotes MoE with WMLP experts, IMLP corresponds to MoIE with vanilla MLP experts,\nand IWMLP refers to MoIE with WMLP experts. The relative improvement of all models is shown in\ncomparison to their corresponding model architectures with two experts.\n8\n\nUnder review as at ICLR Workshop on Neural Network Weights as a New Data Modality 2025\nREFERENCES\nVijay Badrinarayanan, Bamdev Mishra, and Roberto Cipolla. Understanding symmetries in deep networks.\narXiv preprint arXiv:1511.01029, 2015.\nAndrei Chernov. Moe vs. mlp on tabular data. arXiv preprint arXiv:2502.03608, 2025.\nLi Deng. The mnist database of handwritten digit images for machine learning research [best of the web].\nIEEE signal processing magazine, 29(6):141–142, 2012.\nNan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun,\nYanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-\nof-experts. In International Conference on Machine Learning, pp. 5547–5569. PMLR, 2022.\nWilliam Fedus, Jeff Dean, and Barret Zoph. A review of sparse expert models in deep learning. arXiv\npreprint arXiv:2209.01667, 2022.\nCharles Godfrey, Davis Brown, Tegan Emerson, and Henry Kvinge. On the symmetries of deep learning\nmodels and their internal representations. Advances in Neural Information Processing Systems, 35:11893–\n11905, 2022.\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415,\n2016.\nRon Kohavi et al. Scaling up the accuracy of naive-bayes classifiers: A decision-tree hybrid. In Kdd,\nvolume 96, pp. 202–207, 1996.\nRichard Kurle, Ralf Herbrich, Tim Januschowski, Yuyang Bernie Wang, and Jan Gasthaus. On the detri-\nmental effect of invariances in the likelihood for variational inference. Advances in Neural Information\nProcessing Systems, 35:4531–4542, 2022.\nBalaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncer-\ntainty estimation using deep ensembles. Advances in neural information processing systems, 30, 2017.\nDerek Lim, Theo Moe Putterman, Robin Walters, Haggai Maron, and Stefanie Jegelka. The empirical impact\nof neural parameter symmetries, or lack thereof. arXiv preprint arXiv:2405.20231, 2024.\nVinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Pro-\nceedings of the 27th international conference on machine learning (ICML-10), pp. 807–814, 2010.\nBehnam Neyshabur, Russ R Salakhutdinov, and Nati Srebro. Path-sgd: Path-normalized optimization in\ndeep neural networks. Advances in neural information processing systems, 28, 2015.\nR Kelley Pace and Ronald Barry. Sparse spatial autoregressions. Statistics & Probability Letters, 33(3):\n291–297, 1997.\nFabrizio Pittorino, Antonio Ferraro, Gabriele Perugini, Christoph Feinauer, Carlo Baldassi, and Riccardo\nZecchina. Deep networks on toroids: removing symmetries reveals the structure of flat regions in the\nlandscape geometry. In International Conference on Machine Learning, pp. 17759–17781. PMLR, 2022.\nArya A Pourzanjani, Richard M Jiang, and Linda R Petzold. Improving the identifiability of neural networks\nfor bayesian inference. In NIPS workshop on bayesian deep learning, volume 4, pp. 31, 2017.\nJoan Puigcerver, Carlos Riquelme, Basil Mustafa, and Neil Houlsby. From sparse to soft mixtures of experts.\narXiv preprint arXiv:2308.00951, 2023.\n9\n\nUnder review as at ICLR Workshop on Neural Network Weights as a New Data Modality 2025\nCarlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andr´e Su-\nsano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. Advances\nin Neural Information Processing Systems, 34:8583–8595, 2021.\nJonas Gregor Wiese, Lisa Wimmer, Theodore Papamarkou, Bernd Bischl, Stephan G¨unnemann, and David\nR¨ugamer. Towards efficient mcmc sampling in bayesian neural networks by exploiting symmetry. In\nJoint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 459–474.\nSpringer, 2023.\nSeniha Esen Yuksel, Joseph N Wilson, and Paul D Gader.\nTwenty years of mixture of experts.\nIEEE\ntransactions on neural networks and learning systems, 23(8):1177–1193, 2012.\nA\nAPPENDIX\nA.1\nSUMMARY OF DATASETS\nTable 1 provides a summary of the datasets used in this paper.\nTable 1: Datasets description\nDataset\nTask\nInstances\nFeature Details\nTarget Variable\nChurn Modelling\nBinary\nClassifi-\ncation\n10,000\nCustomer\nattributes\n(e.g.,\nCreditScore,\nflagGeography, Gender,\nAge, Tenure, Balance,\nNumOfProducts)\nCustomer\nchurn\n(Exited: yes/no)\nOtto Group Prod-\nuct\nMulti-class Clas-\nsification\n61,878\n93 anonymized numer-\nical features (feat 1 to\nfeat 93)\nProduct category\n(9 classes)\nAdult Income\nBinary\nClassifi-\ncation\n48,842\nMixed continuous and\ncategorical\nvariables\n(e.g.,\nage,\nworkclass,\neducation,\noccupation,\netc.)\nIncome\nlevel\n(“>50K”\nas\n1,\n“<=50K” as 0)\nCalifornia Hous-\ning Prices\nRegression\n20,640\n8 numerical predictors\n(MedInc,\nHouseAge,\nAveRooms, AveBedrms,\nPopulation,\nAveOccup,\nLatitude, Longitude)\nMedian\nhouse\nvalue (in $100K\nunits)\nMNIST\nMulti-class Clas-\nsification\n70,000\nPreprocessed\nrectified\ninto\na\nsingle\nvector\n28×28 pixel grayscale\nimages (784 features per\nimage)\nHandwritten digit\n(10 classes: 0–9)\nA.2\nABSOLUTE METRICS FOR ALL MODELS ON TEST DATASET\nFigures 3 and 4 present the absolute results of the experiments described in Section 5. In Figure 3, the\nchange of the relevant metric for deep ensembles using both MLP and WMLP models is shown as a function\n10\n\nUnder review as at ICLR Workshop on Neural Network Weights as a New Data Modality 2025\nof ensemble size. The bold lines indicate the mean performance across different random seeds, while the\nshaded regions represent the ± one standard deviation intervals. Additionally, the figure displays the mean\nmetric values and intervals for a baseline, which were calculated by aggregating the test metrics of 64\nsingle MLP and 64 single WMLP models; these baseline values were subsequently used in Figure 1. It\ncan be observed that the metric improves as the ensemble size increases. Notably, although WMLP models\nmay yield inferior performance when used individually, the WMLP ensemble tends to outperform the MLP\nensemble.\n11\n\nUnder review as at ICLR Workshop on Neural Network Weights as a New Data Modality 2025\n2\n4\n8\n16\n32\n64\n0.52\n0.53\n0.54\nRMSE\nCALIFORNIA, HIDDEN SIZE = 64\n2\n4\n8\n16\n32\n64\n0.51\n0.52\n0.53\nCALIFORNIA, HIDDEN SIZE = 128\n2\n4\n8\n16\n32\n64\n0.50\n0.52\n0.54\nCALIFORNIA, HIDDEN SIZE = 256\n2\n4\n8\n16\n32\n64\n0.80\n0.81\n0.82\nAccuracy\nOTTO, HIDDEN SIZE = 64\n2\n4\n8\n16\n32\n64\n0.80\n0.81\n0.82\nOTTO, HIDDEN SIZE = 128\n2\n4\n8\n16\n32\n64\n0.80\n0.81\n0.82\n0.83\nOTTO, HIDDEN SIZE = 256\n2\n4\n8\n16\n32\n64\n0.970\n0.975\n0.980\nAccuracy\nMNIST, HIDDEN SIZE = 64\n2\n4\n8\n16\n32\n64\n0.975\n0.980\nMNIST, HIDDEN SIZE = 128\n2\n4\n8\n16\n32\n64\n0.975\n0.980\nMNIST, HIDDEN SIZE = 256\n2\n4\n8\n16\n32\n64\n0.850\n0.852\n0.854\n0.856\nAccuracy\nADULT, HIDDEN SIZE = 64\n2\n4\n8\n16\n32\n64\n0.850\n0.855\nADULT, HIDDEN SIZE = 128\n2\n4\n8\n16\n32\n64\n0.845\n0.850\n0.855\nADULT, HIDDEN SIZE = 256\n2\n4\n8\n16\n32\n64\nEnsemble Size\n0.855\n0.860\n0.865\nAccuracy\nCHURN, HIDDEN SIZE = 64\n2\n4\n8\n16\n32\n64\nEnsemble Size\n0.85\n0.86\nCHURN, HIDDEN SIZE = 128\n2\n4\n8\n16\n32\n64\nEnsemble Size\n0.84\n0.85\n0.86\nCHURN, HIDDEN SIZE = 256\nMLP ENSEMBLE\nWMLP ENSEMBLE\nMEAN MLP 64 MODELS METRIC\nMEAN WMLP 64 MODELS METRIC\nFigure 3: Deep ensemble absolute metrics.\n12\n\nUnder review as at ICLR Workshop on Neural Network Weights as a New Data Modality 2025\nSimilarly, Figure 4 illustrates the change of the corresponding metric for MoE and MoIE models as a function\nof the number of experts. Analogous to Figure 3, the plot shows the mean performance along with the ±\nstandard deviation intervals obtained from aggregating results over various random seeds. The baseline\nvalues corresponding to the case of two experts were used to compute the relative improvements shown in\nFigure 2. It is evident that the use of Gumbel-softmax leads to better performance compared to the standard\nsoftmax, and, in most cases, MoE with WMLP experts or MoIE with WMLP (IWMLP) achieves higher\nquality than MoE with MLP experts or MoIE with MLP/WMLP, respectively.\n13\n\nUnder review as at ICLR Workshop on Neural Network Weights as a New Data Modality 2025\n\u0000\u0001\n\u0002\n\u0003\u0004\n\u0005\u0000\u0004\u0001\n\u0003\u0000\u0002\n\u0006\u0007\b\u0000\u0006\u0007\b\u0001\n\u0006\u0007\b\u0004\n\u0006\u0007\b\u0002\n\u0000\u0001\u0002\u0003\n\u0000\u0001\u0002\u0003\u0004\u0005\u0006\u0007\u0003\u0001\b\t\n\u0001\u000b\u0003\u0007\n\f\t\r\u000b\u0001\u0007\u000e\u0001\u0006\u000e\n\u0000\u0001\n\u0002\n\u0003\u0004\n\u0005\u0000\u0004\u0001\n\u0003\u0000\u0002\n\u0006\u0007\b\u0003\n\u0006\u0007\b\u0000\u0006\u0007\b\u0005\n\u0006\u0007\b\u0001\n\u0000\u0001\u0002\u0003\u0004\u0005\u0006\u0007\u0003\u0001\b\t\n\u0001\u000b\u0003\u0007\n\f\t\n\u000f\u0010\u0011\u0012\u0002\n\u0000\u0001\n\u0002\n\u0003\u0004\n\u0005\u0000\u0004\u0001\n\u0003\u0000\u0002\n\u0006\u0007\t\u0002\b\n\u0006\u0007\t\n\u0006\n\u0006\u0007\t\n\b\n\u0006\u0007\u0002\u0006\u0006\n\u0006\u0007\u0002\u0006\b\n\u0004\u0005\u0005\u0006\u0007\b\u0005\t\n\u0005\u000b\u000b\u0005\b\t\n\u0001\u000b\u0003\u0007\n\f\t\r\u000b\u0001\u0007\u000e\u0001\u0006\u000e\n\u0000\u0001\n\u0002\n\u0003\u0004\n\u0005\u0000\u0004\u0001\n\u0003\u0000\u0002\n\u0006\u0007\t\n\b\n\u0006\u0007\u0002\u0006\u0006\n\u0006\u0007\u0002\u0006\b\n\u0006\u0007\u0002\u0003\u0006\n\u0006\u0007\u0002\u0003\b\n\u0005\u000b\u000b\u0005\b\t\n\u0001\u000b\u0003\u0007\n\f\t\n\u000f\u0010\u0011\u0012\u0002\n\u0000\u0001\n\u0002\n\u0003\u0004\n\u0005\u0000\u0004\u0001\n\u0003\u0000\u0002\n\u0006\u0007\n\u0004\u0006\n\u0006\u0007\n\u0004\b\n\u0006\u0007\n\t\u0006\n\u0004\u0005\u0005\u0006\u0007\b\u0005\t\n\u0010\u0007\u0003\r\u000b\b\t\n\u0001\u000b\u0003\u0007\n\f\t\r\u000b\u0001\u0007\u000e\u0001\u0006\u000e\n\u0000\u0001\n\u0002\n\u0003\u0004\n\u0005\u0000\u0004\u0001\n\u0003\u0000\u0002\n\u0006\u0007\n\u0004\b\u0006\n\u0006\u0007\n\u0004\t\b\n\u0006\u0007\n\t\u0006\u0006\n\u0006\u0007\n\t\u0000\b\n\u0006\u0007\n\t\b\u0006\n\u0010\u0007\u0003\r\u000b\b\t\n\u0001\u000b\u0003\u0007\n\f\t\n\u000f\u0010\u0011\u0012\u0002\n\u0000\u0001\n\u0002\n\u0003\u0004\n\u0005\u0000\u0004\u0001\n\u0003\u0000\u0002\n\u0006\u0007\u0002\u0001\u0006\u0006\n\u0006\u0007\u0002\u0001\u0000\b\n\u0006\u0007\u0002\u0001\b\u0006\n\u0006\u0007\u0002\u0001\t\b\n\u0006\u0007\u0002\b\u0006\u0006\n\u0006\u0007\u0002\b\u0000\b\n\u0004\u0005\u0005\u0006\u0007\b\u0005\t\n\u0001\u000e\u000f\u0002\u000b\b\t\n\u0001\u000b\u0003\u0007\n\f\t\r\u000b\u0001\u0007\u000e\u0001\u0006\u000e\n\u0000\u0001\n\u0002\n\u0003\u0004\n\u0005\u0000\u0004\u0001\n\u0003\u0000\u0002\n\u0006\u0007\u0002\u0001\u0002\n\u0006\u0007\u0002\b\u0006\n\u0006\u0007\u0002\b\u0000\u0006\u0007\u0002\b\u0001\n\u0006\u0007\u0002\b\u0004\n\u0001\u000e\u000f\u0002\u000b\b\t\n\u0001\u000b\u0003\u0007\n\f\t\n\u000f\u0010\u0011\u0012\u0002\n\u0000\u0001\n\u0002\n\u0003\u0004\n\u0005\u0000\u0004\u0001\n\u0003\u0000\u0002\n\u0007\u0013\u0014\u0015\u0016\u0017\t\u0018\u0019\t\u0012\u001a\u001b\u0016\u0017\u001c\u001d\n\u0006\u0007\u0002\u0001\n\u0006\u0007\u0002\b\n\u0006\u0007\u0002\u0004\n\u0004\u0005\u0005\u0006\u0007\b\u0005\t\n\u0000\u001e\u000f\u0006\u0007\b\t\n\u0001\u000b\u0003\u0007\n\f\t\r\u000b\u0001\u0007\u000e\u0001\u0006\u000e\n\u0000\u0001\n\u0002\n\u0003\u0004\n\u0005\u0000\u0004\u0001\n\u0003\u0000\u0002\n\u0007\u0013\u0014\u0015\u0016\u0017\t\u0018\u0019\t\u0012\u001a\u001b\u0016\u0017\u001c\u001d\n\u0006\u0007\u0002\b\u0006\n\u0006\u0007\u0002\b\b\n\u0006\u0007\u0002\u0004\u0006\n\u0006\u0007\u0002\u0004\b\n\u0000\u001e\u000f\u0006\u0007\b\t\n\u0001\u000b\u0003\u0007\n\f\t\n\u000f\u0010\u0011\u0012\u0002\n\u0010\u0002\u001f\n \u0010\u0002\u001f\n\u0003\u0010\u0002\u001f\n\u0003 \u0010\u0002\u001f\nFigure 4: MoE/MoIE absolute metrics.\n14\n",
  "metadata": {
    "source_path": "papers/arxiv/The_Empirical_Impact_of_Reducing_Symmetries_on_the_Performance_of_Deep\n__Ensembles_and_MoE_9bac1d946302bf94.pdf",
    "content_hash": "9bac1d946302bf94b2ae1d0e89a0739c62ac5dbbdb4f03feef305a816b19cfbd",
    "arxiv_id": null,
    "title": "The_Empirical_Impact_of_Reducing_Symmetries_on_the_Performance_of_Deep\n__Ensembles_and_MoE_9bac1d946302bf94",
    "author": "",
    "creation_date": "D:20250225030631Z",
    "published": "2025-02-25T03:06:31",
    "pages": 14,
    "size": 441794,
    "file_mtime": 1740470152.2929301
  }
}