{
  "text": "Lean and Mean: Decoupled Value Policy Optimization with\nGlobal Value Guidance\nChenghua Huang * ♦Lu Wang ♣♥Fangkai Yang ♣Pu Zhao ♣Zhixu Li ♦Qingwei Lin ♣\nDongmei Zhang ♣Saravan Rajmohan ♣Qi Zhang ♣\n♦School of Computer Science, Fudan University\n♣Microsoft\nhuangch22@m.fudan.edu.cn, {wlu, fangkaiyang, puzhao, dongmeiz}@microsoft.com\nAbstract\nProximal Policy Optimization (PPO)-based Re-\ninforcement Learning from Human Feedback\n(RLHF) is essential for aligning large language\nmodels (LLMs) with human preferences. It re-\nquires joint training of an actor and critic with\na pretrained, fixed reward model for guidance.\nThis approach increases computational complex-\nity and instability due to actor-critic interdepen-\ndence. Additionally, PPO lacks access to true\nenvironment rewards in LLM tasks, limiting its\nadaptability. Under such conditions, pretraining\na value model or a reward model becomes equiv-\nalent, as both provide fixed supervisory signals\nwithout new ground-truth feedback. To address\nthese issues, we propose Decoupled Value Policy\nOptimization (DVPO), a lean framework that\nreplaces traditional reward modeling with a pre-\ntrained global value model (GVM). The GVM is\nconditioned on policy trajectories and predicts\ntoken-level return-to-go estimates.\nBy decou-\npling value model from policy training (via frozen\nGVM-driven RL objectives), DVPO eliminates\nactor-critic interdependence, reducing GPU mem-\nory usage by 40% and training time by 35% com-\npared to conventional RLHF. Experiments across\nbenchmarks show DVPO outperforms efficient\nRLHF methods (e.g., DPO) while matching state-\nof-the-art PPO in performance.\n1. Introduction\nLarge Language Models (LLMs) have demonstrated state-\nof-the-art performance across a range of tasks (Achiam\net al., 2023; Bubeck et al., 2023), including dialogue gener-\nation (Ouyang et al., 2022), summarization (Stiennon et al.,\n2020), and code synthesis (Gao et al., 2023a). 1 To further\nalign these models with human preferences, Reinforcement\nLearning from Human Feedback (RLHF) has become an es-\nsential technique (Christiano et al., 2017; Bai et al., 2022b;\nSong et al., 2024). RLHF leverages human-labeled feed-\nback to guide model behavior, producing more accurate and\nuser-friendly responses over time (Bai et al., 2022a; Ganguli\net al., 2022). Among RLHF algorithms, Proximal Policy\nOptimization (PPO) is widely adopted due to its balance\nbetween stability and performance (Schulman et al., 2017;\nvon Werra et al., 2020; Huang et al., 2024c).\nDespite its effectiveness, PPO-based RLHF comes with sig-\nnificant challenges. It requires joint training of an actor\n(policy) and a critic (value function), while relying on a\nfixed, pretrained reward model (Ziegler et al., 2019; Ouyang\net al., 2022; Wang et al., 2024). This joint optimization\nintroduces dense computational complexity and training in-\nstability, as the actor and critic must co-adapt over time (Yao\net al., 2023; Huang et al., 2024b). Unlike in traditional re-\ninforcement learning (RL), where continuous environment\ninteraction provides updated rewards, RLHF in LLMs lacks\naccess to ground-truth environment rewards during train-\ning. As a result, the reward model cannot evolve to reflect\nchanges in the policy’s behavior, further complicating value\nestimation and policy optimization (Moskovitz et al., 2023).\nTo mitigate these issues, several works have proposed sim-\nplifying the RLHF process by reducing the number of train-\nable components. Direct Preference Optimization (DPO)\nbypasses training both reward and value models, optimizing\npolicies directly on preference data (Rafailov et al., 2024;\nAzar et al., 2024; Zhao et al., 2023; Park et al., 2024). While\nefficient, DPO lacks the iterative refinement of true rein-\nforcement learning and struggles with distribution shifts (Xu\net al., 2024). Other reward-only approaches, such as Re-\nMax (Li et al., 2023) and GRPO (Shao et al., 2024), use\nfinal rewards applied uniformly across all tokens but suffer\n1Work is done during * an internship at Microsoft. ♥corre-\nsponding author.\n1\narXiv:2502.16944v1  [cs.LG]  24 Feb 2025\n\nLean and Mean: Decoupled Value Policy Optimization with Global Value Guidance\nfrom high variance due to coarse token-level supervision,\nfailing to optimize return-to-go dynamics and resulting in\ninefficient policy updates (Ahmadian et al., 2024; Gunter\net al., 2024; Hu, 2025)\nIn the context of RLHF without ground-truth environment\nrewards, we observe that pretraining a reward model and\npretraining a value model become theoretically equivalent.\nBoth models provide static supervisory signals about ex-\npected outcomes and cannot improve without additional\nreward feedback. This insight allows us to simplify the\nRLHF pipeline by eliminating the need for joint actor-critic\ntraining. Instead, we propose Decoupled Value Policy Op-\ntimization (DVPO), a framework that eliminates joint actor-\ncritic training while preserving fine-grained value guidance.\nIt pretrains a global value model (GVM) conditioned on pol-\nicy trajectories. The GVM predicts token-level return-to-go\nvalues and serves as a stable guide for policy optimization,\nreducing both training complexity and instability. Impor-\ntantly, our method requires no external information beyond\nthe standard RLHF data used in PPO. Once the GVM is\ntrained, policy optimization proceeds via a standard RL\nobjective, such as a PPO-like update, without requiring\nfurther updates to the value model. This design reduces\nboth training complexity and instability while maintaining\nfine-grained reward supervision.\nWe provide theoretical justification for this approach by\nproving that, under the constraint of no new reward feed-\nback, pretraining a reward model and a global value model\nare functionally interchangeable. Our experiments across\nmultiple benchmarks (MT-Bench (Zheng et al., 2023),\nAlpaca-Eval (Dubois et al., 2024), Arena-Hard (Li et al.,\n2024), etc.) demonstrate that the proposed method achieves\nperformance comparable to state-of-the-art RLHF methods\nwhile reducing training time and GPU usage by 40% and\n35%, respectively. These results highlight the scalability\nand stability of our approach in large-scale LLM fine-tuning.\n2. Related Work\nReinforcement Learning in Language Model Optimiza-\ntion. Reinforcement learning has emerged as a prevalent\nmethod for fine-tuning large language models (LLMs), with\nProximal Policy Optimization (Schulman et al., 2017) and\nits variations (Ramamurthy et al., 2022; Wu et al., 2023)\nbeing the most prevalent methods. These techniques largely\nadhere to the actor-critic paradigm (Sutton, 2018), This ap-\nproach alternates between training a value estimator for the\ncurrent policy and leveraging it to enhance policy perfor-\nmance. This bilevel process may result in a suboptimal\npolicy, as demonstrated by empirical studies (Gao et al.,\n2023b). Moreover, the alternating optimization of the policy\nand critic models, along with the use of rewards provided by\nthe reward model as environmental feedback, necessitates\nloading four models (including the reference model) simulta-\nneously during training. This significantly increases training\ncomplexity and computational resource consumption (Yao\net al., 2023; Hu et al., 2024).\nTraining Efficiency. Many recent studies have sought to\nmitigate the computational complexity and resource con-\nsumption of the reinforcement learning (RL) step in RLHF.\nMethods such as DPO (Rafailov et al., 2024) and its vari-\nants (Meng et al., 2024; Ethayarajh et al., 2024; Hong et al.,\n2024) bypass reward modeling and the actor-critic learning\nframework by directly learning from preferences. However,\nexisting research indicates that due to their offline nature,\nthese approaches exhibit a performance gap compared to on-\nline RL (Xu et al., 2024). Some recent works have proposed\na reward-only approach to reduce the training cost of the RL\nphase (Li et al., 2023; Gunter et al., 2024; Shao et al., 2024;\nAhmadian et al., 2024). However, this method lacks value\nestimation and assigns the same reward score to each token,\nleading to high variance and instability during training (Hu,\n2025). Unlike these approaches, our method pre-trains a\nglobal value model (GVM) and leverages it to guide RL\ntraining, providing token-level supervision signals. This\nnot only reduces training resource consumption but also\nstabilizes the training process, achieving performance com-\nparable to the original PPO.\nValue-base Inference. Some recent works have attempted\nto learn a value function and use it to guide the decoding\nphase of LLMs, thereby bypassing the RL optimization\nstage (Han et al., 2024; Kong et al., 2024; Mao et al., 2024).\nHowever, this approach significantly increases inference\ncomplexity and raises inference costs. In contrast, we lever-\nage the learned value model to guide RL training, where the\npre-trained value model helps the policy model converge\nmore stably (Noukhovitch et al., 2024).\n3. Method\nWe propose DVPO for RLHF in which a GVM is trained\nonce and then fixed to guide policy updates. This approach\nremoves the need for joint policy–value training and miti-\ngates the associated computational overhead and instability.\nAs shown in Figure 1, our method comprises two primary\nstages: (1) Train GVM: Use offline trajectories (states,\nactions, returns, and policy-specific data) to learn a policy-\nconditioned action-value function Qϕ. (2) Decoupled Value\nPolicy Optimization: Freeze Qϕ and optimize a policy us-\ning a standard RL objective (e.g., PPO), taking advantage\nestimates from the fixed GVM.\nIn what follows, we first define our setting (§3.1), then in-\ntroduce the GVM and its training procedure (§3.2), describe\nhow we fix the GVM for stable policy optimization (§3.3),\nand finally present a theoretical analysis showing that pre-\n2\n\nLean and Mean: Decoupled Value Policy Optimization with Global Value Guidance\nPolicy Model 1\nPolicy Model 2\nPolicy Model n\nx\ny1\ny2\nyn\nr1\nr2\nrn\nQ1\nQ2\nQn\nValue \nModel\nGlobal Value Model (GVM) Training\nx\nPolicy Model\ny\nReference\nModel\nValue\nModel\nKL\nQ\nDecoupled Value Policy Optimization (DVPO)\nTD \nLearning\nReward Model Training\nx\nPolicy Model\ny\nReference\nModel\nValue\nModel\nKL\nValue \nLearning\nPPO\nPolicy Model 1\nPolicy Model 2\nPolicy Model n\nx\ny1\ny2\nyn\nr1\nr2\nrn\nReward \nModel\nSFT\nReward\nModel\nQ\nTrained\nModels\nFrozen\nModels\nFigure 1. Overview of Decoupled Value Policy Optimization\n(DVPO) and PPO in RLHF. DVPO eliminates the need for a re-\nward model and decouples policy and value learning during policy\noptimization. In contrast, PPO requires training a reward model\nbefore policy optimization. DVPO instead trains a global value\nmodel using the same offline data as the reward model. During\npolicy training, no additional ground-truth rewards are obtained.\ntraining a reward model and pretraining a global value\nmodel yield equivalent information when no new ground-\ntruth rewards are available (§3.4).\n3.1. Problem Setting\nWe model the sequence generation task in NLP as a Markov\nDecision Process (MDP). The response consists of T tokens,\ndenoted by y = y<T +1 := [y1, y2, . . . , yT ], where yt ∈Y\nand Y is the vocabulary. We assume y<1 = [ ], indicating\nan empty prefix. Given a prompt x and the first t −1 to-\nkens y<t, the language model (LM) predicts the probability\ndistribution for the next token as:\nπθ(· | [x, y<t]).\nIn this MDP formulation, the state is defined as st =\n[x, y<t], representing the prompt (i.e., x) and the gener-\nated response up to step t. The action is the next generated\ntoken at = yt. The ground-truth reward at the sentence\nlevel, denoted by r(x, y), is provided by human feedback.\nTo transform the sentence-level reward into token-level re-\nwards r(st, at), we apply a simplified form of temporal\ndifference (TD) learning. Specifically, we assign a reward\nof 0 to each intermediate step and use the final step’s reward\nas the sentence-level reward value.\nBy this definition, the cumulative return from step t onwards\nis expressed as: Gt = PT\nk=t γk−tr(sk, ak), which simpli-\nfies to Gt = γT −tr(x, y) when all intermediate rewards are\nzero.\nIn RLHF, we rely on offline data D, which contains state–\naction trajectories, returns, and policy behaviors.\nThe\ndataset is defined as:\nD =\n\b\n(τi, si\nt, ai\nt, Gi\nt)\n\tN\ni=1,\nwhere τi represents the i-th policy behavior, typically ex-\npressed as a sequence of question–answer pairs, (si\nt, ai\nt) is a\nstate–action pair sampled under that policy. Note that τi and\n(si\nt, ai\nt) may refer to different or identical question–answer\npairs under the same policy. No additional ground-truth\nrewards are collected during training.\n3.2. Training a Global Value Model (GVM)\nOur key insight is to learn a policy-conditioned action-value\nfunction Qϕ(τ, s, a) from offline data, where τ represents\na sampled trajectory capturing how the policy behaves in\nunrelated contexts. This conditioning approximates how\nwell a particular policy—embodied by τ—would perform\nwhen taking action a in state s.\nPolicy Conditioning via Trajectories.\nTraditional actor–\ncritic methods require online adaptation of the value func-\ntion to the actor’s evolving behavior. In contrast, we aim for\na single, global Qϕ that generalizes across different policies,\nthus avoiding iterative re-learning. Instead of conditioning\non explicit policy parameters, we leverage trajectories τ\nrandomly sampled from the policy in question. Each τ is a\nsequence of question–answer pairs (in LLM tasks) or other\ninteractions that reveal distinct policy traits (e.g., stylistic\ntendencies, correctness, domain expertise).\nFormally, we parametrize:\nQϕ(τ, s, a) ≈E\nh ∞\nX\nt=0\nγt r(st, at)\n\f\f\f s0 = s, a0 = a, τ\ni\n,\nwhere τ implicitly determines which policy π(· | s) we are\napproximating.\nIn practice, we train the global value model Qϕ(τ, s, a) us-\ning Temporal Difference (TD) learning. The target return Gi\nis estimated based on future rewards and value predictions.\nSpecifically, Gi is computed as:\nGt = r(st, at) + γQϕ(τ, st+1, at+1),\n3\n\nLean and Mean: Decoupled Value Policy Optimization with Global Value Guidance\nwhere r(st, at) is the reward for taking action at in state\nst, and Qϕ(τ, st+1, at+1) is the predicted value of the next\nstate-action pair.\nThe training objective for the global value model is to mini-\nmize the TD loss:\nLGVM(ϕ) = E(τt,st,at,rt,st+1,at+1)∈D\n\u0002\u0000rt+\nγQϕ(τ, st+1, at+1) −Qϕ(τ, st, at)\n\u00012\u0003\n,\n(1)\nThis TD-based loss drives Qϕ to iteratively adjust its esti-\nmates of the expected return, ensuring that the value func-\ntion accurately reflects both immediate and future rewards.\nThis approach is particularly suitable for offline RLHF sce-\nnarios, as it can handle sequences with deterministic state\ntransitions.\nBatch Normalization for Value Imbalance.\nOffline\nRLHF data may exhibit skewed returns: for instance, pre-\ndominantly positive if most responses are high-quality. This\ncan push Qϕ to extreme positive values and slow conver-\ngence. To stabilize training, we adopt batch normalization\non the predicted values. For a mini-batch of size B,\nµB =\n1\nB\nB\nX\nt=1\nQϕ(τ, st, at),\nσ2\nB =\n1\nB\nB\nX\ni=1\n\u0000Qϕ(τ, st, at) −µB\n\u00012,\n(2)\nwe then normalize each output:\neQϕ(τ, st, at) = Qϕ(τ, st, at) −µB\nσB + ϵ\n.\nThis procedure (with small ϵ for numerical stability) pre-\nvents extreme values from dominating training and improves\ngradient flow.\n3.3. Decoupled-Value Policy Optimization\nIn traditional actor–critic methods, both the policy (actor)\nand value (critic) are trained simultaneously, which intro-\nduces instability due to their interdependence. This issue is\nexacerbated in offline RLHF settings, where no new environ-\nment rewards can be collected to correct misaligned updates.\nTo address these challenges, we propose a decoupled-value\npolicy optimization approach: the global value model Qϕ is\ntrained and fixed before policy optimization, decoupling the\nlearning dynamics of the actor and critic.\nPolicy Optimization Objective.\nOnce the global value\nmodel Qϕ converges, we fix its parameters and use it to\nguide policy updates. Let πθ(a | s) be the policy to be\noptimized. We adopt a clipped PPO objective, which stabi-\nlizes policy updates by limiting the impact of large policy\nchanges:\nLPPO(θ) = E\nh\nmin\n\u0000rt(θ) ˆAt, clip\n\u0000rt(θ), 1−ϵ, 1+ϵ\n\u0001 ˆAt\n\u0001i\n,\n(3)\nwhere rt(θ) =\nπθ(at|st)\nπθold(at|st) is the importance sampling ratio,\nand ϵ is a clipping parameter to prevent overly large updates.\nThe advantage function measures the relative quality of an\naction compared to the expected value of the state. We de-\nfine the advantage using the fixed, normalized value model:\nˆAt = eQϕ(τ, st, at),\nwhere eQϕ is the batch-normalized value estimate computed\nduring the GVM training stage (see §3.2). In offline RLHF,\nthis static advantage definition provides a reliable signal\nfor policy improvement without requiring dynamic value\nupdates.\nOur decoupled-value approach offers two key advantages:\n(1) Stability via Fixed Value: By fixing Qϕ, we eliminate\nthe “moving target” problem inherent in actor–critic meth-\nods, leading to more stable and predictable policy updates.\n(2) Efficient Offline Use: Since no additional environment\nrewards can be collected, the static Qϕ provides all neces-\nsary supervisory information, enabling efficient reuse of the\noffline dataset for policy optimization.\n3.4. Theoretical Analysis: Equivalence of Pretrained\nReward and Global Value Models\nIn an offline RLHF scenario where no new ground-truth\nrewards are available, pretraining either a reward model or a\nGVM provides essentially the same supervisory information\nfor policy optimization. Below, we present a rigorous proof\nof this equivalence and discuss its implications.\n3.4.1. PRELIMINARIES\nAssume we have two possible pretrained models from the\nsame offline dataset. Reward Model, Rϕ(s, a), approx-\nimates an unknown ground-truth reward r(s, a). Global\nValue Model, Qψ(s, a), approximates the action-value func-\ntion for a any given policy π. Here, Qψ(s, a) is a global\nvalue model conditioned on the trajectory τπ, state s, and\naction a. However, to simplify the proof, we remove the\nexplicit dependence on τπ by merging it into the definition\nof the state.\nBecause no new rewards are collected during policy train-\ning, neither model can be updated to reflect policy changes.\nHence, whichever model is used, the agent has a fixed signal\nto optimize against. We formalize this equivalence next.\nTheorem 3.1 (Equivalence of Pretrained Reward and GVM).\nSuppose:\n4\n\nLean and Mean: Decoupled Value Policy Optimization with Global Value Guidance\n1. Rϕ(s, a) is a pretrained reward model whose approxi-\nmation error to the true reward r(s, a) is at most ϵR in\nthe offline data distribution.\n2. Qψ(s, a) is a pretrained global value model whose\napproximation error to Qπ(s, a) is at most ϵQ in the\nsame data distribution.\n3. No further ground-truth rewards can be obtained to\nupdate either model.\nThen any policy gradient method that employs either Rϕ\nor Qψ as its supervisory signal will yield policy updates\ndiffering by at most a constant factor dependent on (ϵR, ϵQ).\nAs ϵR, ϵQ →0, the two approaches become equivalent in\nguiding policy optimization.\nProof. Deriving a Value Function from Rϕ.\nGiven\nRϕ(s, a) and a fixed policy π, one can solve the Bellman\nequation offline to obtain eQϕ(s, a; π):\neQϕ(s, a; π) = Rϕ(s, a)+γEs′∼P\nh\nEa′∼π(·|s′)[ eQϕ(s′, a′; π)]\ni\n.\nUnder standard coverage assumptions in offline RL, eQϕ\nconverges to an approximation of Qπ, up to an error that\ndepends on ϵR and the data distribution.\nDeriving a Reward Signal from Qψ.\nConversely, if\nQψ(s, a) approximates Qπ(s, a) for a policy π, the (un-\nknown) reward can be approximated by rearranging the\nBellman equation:\nr(s, a) = Qπ(s, a) −γEs′∼P,a′∼π\n\u0002\nQπ(s′, a′)\n\u0003\n.\nReplacing Qπ with Qψ yields\neRψ(s, a) = Qψ(s, a) −γEs′∼P,a′∼π\n\u0002\nQψ(s′, a′)\n\u0003\n.\nHence, Qψ can serve as a surrogate reward model for policy\nupdates.\nEquivalence in Policy Gradient. Let ∇θJR be the policy\ngradient update using Rϕ (converted to a value function\neQϕ), and ∇θJQ the update using Qψ directly. By standard\npolicy gradient theorems,\n∥∇θJR −∇θJQ∥≤κ(ϵR, ϵQ),\nfor some bounded function κ, given that both methods rely\non fixed models and the policy remains within the offline\ndataset’s support. As ϵR, ϵQ →0, the updates converge to\nthe same directions in parameter space, leading to the same\noptimal policy.\nNo New Ground-Truth Rewards. Since no fresh envi-\nronment rewards r(s, a) can be obtained, neither Rϕ nor\nQψ can be refined once training begins. Thus, each model\nacts as a static supervisor. If both models are sufficiently\naccurate within the relevant state–action distribution, their\ninduced policy gradients differ only by a bounded approxi-\nmation error.\nThe above proof confirm that a pretrained reward model Rϕ\nor a pretrained global value model Qψ provides equivalent\nguidance for offline policy optimization, proving Theorem\n3.1.\n4. Experiment Setup\nIn this section, we conduct extensive experiments to validate\nthe superiority of DVPO. Specifically, based on real-world\nscenarios of RLHF implementation, we perform preference\noptimization under two settings: Base and Instruction.\n4.1. Training settings\nFor the Base setting, we follow the training pipeline of\n(Huang et al., 2024a), perform preference optimization\naccording to the following process.\nFirst, we select a\nbase model for instruction fine-tuning (LLaMA3 8B2 and\nLLaMA3 3B3(Dubey et al., 2024)) to obtain the SFT model.\nThen, we initialize the preference optimization process\nfrom the SFT model and conduct training on the Ultrafeed-\nback(Cui et al., 2023) dataset. This setting encompasses\nthe standard RLHF process and is highly transparent, as\nthe SFT model is trained on open-source data and has not\nundergone preference optimization.\nFor the Instruction setting, We select an off-the-shelf\ninstruction-tuned model as the SFT model. To facilitate\ncomparison with related work(Li et al., 2023; Rafailov et al.,\n2024), we choose the mistral-instruction-v0.24. These mod-\nels have undergone extensive instruction fine-tuning pro-\ncesses based on the base model, resulting in stronger ca-\npabilities compared to the SFT model in the Base setting.\nHowever, the instruction fine-tuning process is not transpar-\nent. We perform preference optimization on the instruction\nmodels.\n4.2. Evaluation\nWe evaluate the response quality of the models using\nthe most popular instruction-following benchmarks: MT-\nBench(Zheng et al., 2023), Arena-Hard(Li et al., 2024), and\nAlpaca-Eval(Dubois et al., 2024). These benchmarks con-\nsist of a series of open-ended questions and employ LLM as\na judge approach to score or compare the responses gener-\n2https://huggingface.co/meta-llama/\nMeta-Llama-3-8B\n3https://huggingface.co/meta-llama/\nLlama-3.2-3B\n4https://huggingface.co/mistralai/\nMistral-7B-Instruct-v0.2\n5\n\nLean and Mean: Decoupled Value Policy Optimization with Global Value Guidance\nated by the models. Additionally, we compare the win rate\non the held-out test set.\nMT-Bench comprises 80 multi-turn questions. The evalua-\ntion is conducted using GPT-4 as an automated judge, which\nscores the models’ responses on a scale from 1 to 10 based\non criteria such as relevance, coherence, and informative-\nness. Arena-Hard features a dataset of 500 prompts curated\nfrom Chatbot Arena’s live data. The evaluation employs\nan LLM-as-a-judge framework, where models’ responses\nare compared, and win rates are calculated to determine per-\nformance. AlpacaEval consists of 805 paired instructions.\nEvaluations are performed using an LLM-based automatic\nevaluator, such as GPT-4, which compares the responses of\ndifferent models to the same prompt and determines a win-\nner. To mitigate biases like verbosity, AlpacaEval employs\nlength-controlled win rates.\n4.3. Baseline\nFor the Base setting, as previously described, we carried\nout experiments on the Ultrafeedback dataset. Following\nthe experimental setup of (Huang et al., 2024a), the dataset\nis divided into three parts: 30% for SFT, 50% for reward\nlearning, and 20% for RL optimization. The value model is\ntrained on the reward learning data and then used to perform\npreference optimization on the SFT model using the 20%\nRL optimization data. For the standard PPO process, the\nlearned reward model is used similarly to optimize the SFT\nmodel on the 20% RL optimization data. In addition, we\ncompare the results with the original SFT model.\nFor the Instruction setting, we select Mistral-instruction-\nv0.2 as the starting point and perform RL preference op-\ntimization on the Ultrafeedback dataset. For this purpose,\nwe utilize the existing reward model, UltraRM5, aligning\nwith the experimental setup of ReMax(Li et al., 2023). We\ncompare our model with other models of similar parameter\nsizes, including LLaMA3-8B-Instruction6, DPO, ReMax7,\netc.\n5. Experiment results\nIn this section, we present the main experimental results for\nBase and Instruction settings, as well as the computational\nefficiency advantages of DVPO compared to standard RLHF\nmethods. For details on the performance of the GVM, please\nrefer to the Appendix C.\n5https://huggingface.co/openbmb/\nUltraRM-13b\n6https://huggingface.co/meta-llama/\nMeta-Llama-3-8B-Instruct\n7https://huggingface.co/ziniuli/\nMistral-7B-ReMax-v0.1\n5.1. DVPO on Base setting\nAs mentioned earlier, to clearly evaluate the feasibility of\nDVPO compared to existing methods, we conducted experi-\nments on publicly available datasets. The datasets were split\nproportionally for supervised fine-tuning (SFT) and reward\nlearning (value learning). Subsequently, the resulting value\nmodel (reward model) was used to guide the policy opti-\nmization for preference learning. Given the relatively small\namount of data at each stage (with the RL phase account-\ning for approximately 20% of the entire dataset), our focus\nin the Base setting is on relative improvement rather than\nabsolute performance metrics. The results are presented in\nTable 1.\nDVPO demonstrates superior optimization performance.\nIn experiments conducted on LLaMA3-3B and LLaMA3-\n8B, DVPO consistently outperforms SFT significantly.\nSpecifically, under the Base setting of LLaMA3-8B, DVPO\nachieves improvements of 0.2 on MT-Bench, 1.5 on Arena-\nHard, and 1.74 on Alpaca-Eval compared to the SFT\nmodel. The improvements are even more pronounced with\nLLaMA3-3B, where DVPO achieves a 0.51 increase on MT-\nBench, a 5-point increase on Arena-Hard, and a 4.14-point\nincrease on Alpaca-Eval. Similarly, as shown in Figure 2,\non the held-out test set, DVPO achieves a higher win rate\ncompared to the SFT model. These results validate the\nrobustness and effectiveness of DVPO.\nThe pre-trained value model can provide a sufficiently\nhigh performance ceiling. According to the experimental\nresults in the Base setting, the final convergence perfor-\nmance of DVPO is very close to that of standard PPO (and\nin some cases, slightly exceeds it), indicating that the su-\npervision signals provided by the pre-trained value model\ncan support a sufficiently high performance ceiling. As an\nactor-critic method, PPO trains the policy model and value\nmodel simultaneously, with the reward model providing\nenvironmental feedback. The value model, as a bridge be-\ntween immediate rewards (sentence-level) and long-term\nreturns, offers finer-grained supervision signals (token-level)\ncompared to immediate rewards alone. In DVPO, the global\nvalue model(GVM) is pre-trained on responses from dif-\nferent policies, enabling it to provide supervision feedback\nto various policies during the RL phase. The experimen-\ntal results demonstrate that the fine-grained environmental\nfeedback provided by DVPO can achieve a performance ceil-\ning comparable to that of the actor-critic framework while\nsignificantly reducing training complexity and instability.\n5.2. DVPO on Instruction setting\nTo evaluate the effectiveness of DVPO in most RLHF sce-\nnarios (optimization starting from a pre-aligned model), we\nselected Mistral-Instruction-v0.2 as the SFT model. An ex-\nisting UltraRM reward model was used to collect labeled\n6\n\nLean and Mean: Decoupled Value Policy Optimization with Global Value Guidance\nTable 1. Performance under the Base setting. The suffix “3B” indi-\ncates that llama3.2-3B-Base was used as the base model, while\n“8B” indicates that llama3-8B-Base was used as the base model.\nBoth PPO and DVPO were initialized from SFT and optimized.\nCompared to PPO, DVPO achieved the best benchmark scores\nacross models with different parameter sizes.\nMODEL\nMTBENCH\nARENA HARD\nALPACAEVAL2\nSFT-8B\n4.87\n10.3\n9.79\nPPO-8B\n4.98\n11.7\n11.14\nDVPO-8B\n5.01\n11.8\n11.33\nSFT-3B\n5.22\n10.4\n8.19\nPPO-3B\n5.33\n13.5\n11.54\nDVPO-3B\n5.73\n15.1\n12.33\n0\n20\n40\n60\n80\n100\nPercentage\nLlama3 8B\nDVPO vs PPO\nLlama3 8B\nDVPO vs SFT\nLlama3 3B\nDVPO vs PPO\nLlama3 3B\nDVPO vs SFT\nModel Comparison\n28.02%\n43.49%\n28.48%\n30.58%\n43.68%\n25.73%\n32.33%\n38.49%\n29.09%\n32.56%\n39.95%\n27.48%\nLeft wins\nTie\nRight wins\nFigure 2. Results of the model on the Ultrafeedback held-out test-\nset. We employed GPT4o as a judge to assess the quality of\nmodel-generated responses. Performance is measured using the\nwin rate, where Left represents DVPO, and Right represents the\nbaseline model for comparison.\ndata for training the value model, value model also initial-\nized from Mistral-Instruction-v0.2. The trained value model\nthen guided the preference optimization of the SFT model.\nThe results are presented in Table 2. Experimental findings\ndemonstrate that our method exhibits significant advantages\nover other approaches. Our main findings are as follows:\nDVPO significantly enhances the performance of instruc-\ntion models. Compared to the original Mistral-Instruction-\nv0.2, DVPO consistently demonstrates performance im-\nprovements across all benchmarks. Specifically, it achieves\na 0.19 improvement on MT-Bench and increases the win\nrate relative to GPT-4 by 12.1% on Arena-Hard. Addition-\nally, it enhances the length-controlled win rate by 10.32%\non Alpaca-Eval. Furthermore, compared to larger models\nsuch as LLaMA3-8B-Instruction, DVPO also exhibits no-\ntable advantages, outperforming 4.1% in Arena-Hard and\n4.51% in Alpaca-Eval. These results indicate that the pre-\ntrained value model can effectively and reliably guide the\nTable 2. Performance under the Instruction setting. Mistral 7B\nrefers to Mistral-7B-instruction-v0.2, and Llama 8B refers to\nLlama3-8B-instruction. DVPO outperformed other preference\nalignment methods.\nMODEL\nMTBENCH\nARENA HARD\nALPACAEVAL2\nMISTRAL 7B\n6.60\n12.6\n17.11\nREMAX\n6.67\n21.9\n20.55\nDPO\n6.30\n16.3\n26.80\nDVPO\n6.79\n24.7\n27.43\nLLAMA3 8B\n6.90\n20.6\n22.92\nGPT4\n7.93\n35.0\n30.20\nTable 3. Computational results under the Base setting. For the\n3B model, we conducted experiments using Zero-1. For the 8B\nmodel, due to PPO encountering out-of-memory (OOM) issues\nwith the Zero-1 training strategy, we employed Zero-3 instead.\nDetails can be found in the Appendix. We report the GPU memory\nconsumption per GPU (GB), the time required per step (seconds),\nand the number of steps needed to reach convergence under an\n8×A100 training environment.\nMODEL\nMEMORY/PER GPU\nTIME/S\nTRAINING STEP\nPPO-8B\n78.96G\n70\n1250\nDVPO-8B\n60.49G\n48\n810\nPPO-3B\n41.43G\n23\n600\nDVPO-3B\n27.50G\n13\n450\noptimization of policy models.\nFiner-grained feedback signals lead to superior perfor-\nmance. Compared to reward-only methods such as ReMax,\nDVPO demonstrates significant performance advantages on\nMt-bench, Arena hard and Alpaca Eval. In ReMax, the feed-\nback signal is at the sentence level, assigning a single reward\nscore to the entire sentence and treating all tokens as equally\ncontributing to the overall score. This design prevents the\npolicy model from learning fine-grained preferences dur-\ning the RL phase, resulting in suboptimal performance. In\ncontrast, DVPO assigns a return value to each token, which\nfacilitates more effective training of the policy model.\nFurthermore, DVPO outperforms direct preference opti-\nmize(DPO) approaches.\nIn addition to providing fine-\ngrained returns, DVPO retains the on-policy characteristics\nof the original PPO, enabling a larger exploration space and\noffering a higher performance ceiling.\n5.3. DVPO Computation Efficiency\nDVPO improves upon RLHF by leveraging a pre-trained\nvalue model to provide environmental feedback, resulting\n7\n\nLean and Mean: Decoupled Value Policy Optimization with Global Value Guidance\n100\n200\n300\n400\n500\n600\nSteps\n5.0\n5.2\n5.4\n5.6\n5.8\n6.0\n6.2\nReward Score\nLlama3 3B\nLLama 3B PPO\n200\n400\n600\n800\n1000\n1200\nSteps\n1.10\n1.15\n1.20\n1.25\n1.30\n1.35\n1.40\nReward Score\nLlama3 8B\nLLama 8B PPO\n0.4\n0.6\n0.8\n1.0\n1.2\nValue Score\nLLama 3B DVPO\n0.10\n0.11\n0.12\n0.13\n0.14\n0.15\n0.16\n0.17\n0.18\nValue Score\nLLama 8B DVPO\nTraining Curve\nFigure 3. Learning curve of the policy model during the RL stage under the Base setting. DVPO demonstrates faster and more stable\nconvergence compared to other methods.\nin enhanced computational efficiency, stability, and conver-\ngence speed.\nMemory Efficiency. As shown in Table 3, the experimen-\ntal setup for standard PPO and DVPO was kept identical.\nUnder standard training conditions, DVPO achieves approx-\nimately a 50% reduction in memory usage. For instance,\nwith LLaMA-3B, we trained using Zero-1 on 8 A100 GPUs\nwith a batch size of 4. Standard PPO requires 41.43 GB of\nmemory due to the need to simultaneously load 4 models\n(policy model, critic model, reference model, and reward\nmodel), with two models (policy model and critic model) re-\nquiring activation. In contrast, DVPO only requires 27.5 GB\nof memory, as it only loads the policy model, global value\nmodel, and reference model, with only the policy model\nbeing activated during training. For LLaMA-8B, we used\nDeepSpeed Zero-3 with a batch size of 4. Even under this\nconfiguration, DVPO effectively reduces memory usage,\nenabling support for larger batch sizes during training.\nTime Efficiency. DVPO accelerates the training process,\nrequiring only half the time per batch compared to standard\nPPO under the same batch size. This improvement stems\nfrom the fact that, during the generation phase, DVPO re-\nquires outputs from only three models, whereas standard\nPPO relies on four. Additionally, during the backpropaga-\ntion phase, PPO updates both the policy and value models,\nwhile DVPO updates only the policy model. Overall, un-\nder identical settings (batch size), DVPO achieves a 1.76x\nspeedup in training (1.76 ≈23/13).\nTraining Step. We observe that using a pre-trained value\nmodel to provide token-level fine-grained environmental\nfeedback enables faster model convergence, which aligns\nwith the findings in (Noukhovitch et al., 2024). Specifi-\ncally, under identical experimental settings (same batch size,\nexperience replay count, etc.), DVPO requires only approxi-\nmately 3/4 of the interaction steps to converge. As shown\nin Figure 3, in the LLaMA3-8B experiment, the reward\ncurve of PPO reaches its peak at 1250 steps, whereas DVPO\nachieves optimal performance around 810 steps. Similarly,\nin LLaMA3-3B, PPO reaches its peak at 600 steps, while\nDVPO converges in 450 steps. Since DVPO provides token-\nlevel supervision feedback and is pre-trained in advance, it\ncan be considered a form of warm-starting, thereby acceler-\nating policy convergence.\nPolicy Training Curve. As shown in Figure 3. In the\nLlama3 3B setting, DVPO achieves faster convergence and\nexhibits more stable performance compared to PPO, with a\nsmoother increase in reward scores and consistent improve-\nments in value scores throughout training. In the Llama3\n8B setting, DVPO also exhibits a more stable training curve,\nindicating better training stability and efficiency relative to\nPPO. These results substantiate the advantages of DVPO,\nparticularly its capability to achieve faster and more stable\nconvergence across models with varying parameter scales.\nFurthermore, DVPO demonstrates the ability to converge to\na stable state more rapidly. In the Llama3 3B experiments,\nDVPO achieved convergence at approximately 450 steps,\nwhereas PPO required around 600 steps to converge. Simi-\nlarly, in the Llama3 8B experiments, DVPO converged at\naround 800 steps, while PPO completed convergence only\nafter 1200 steps.\n8\n\nLean and Mean: Decoupled Value Policy Optimization with Global Value Guidance\n6. Conclusion\nWe propose Decoupled Value Policy Optimization (DVPO),\na framework that eliminates joint actor-critic training in\nRLHF by leveraging a pretrained global value model\n(GVM). Unlike traditional PPO-based RLHF, which suf-\nfers from training instability and computational overhead,\nDVPO stabilizes policy optimization through static, token-\nlevel return-to-go estimates. Our theoretical analysis proves\nthe functional equivalence of reward and value models under\nthe constraint of no new reward feedback, justifying the use\nof a fixed GVM for efficient and scalable optimization. Em-\npirical results demonstrate that DVPO achieves comparable\nperformance to state-of-the-art RLHF methods on multiple\nbenchmarks while reducing training time and GPU usage\nby over 35%. Future work will focus on refining the value\nmodel’s training process to enhance prediction accuracy.\nImpact Statement\nThis paper presents work whose goal is to advance the field\nof Machine Learning. There are many potential societal\nconsequences of our work, none which we feel must be\nspecifically highlighted here.\nReferences\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I.,\nAleman, F. L., Almeida, D., Altenschmidt, J., Altman, S.,\nAnadkat, S., et al. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774, 2023.\nAhmadian, A., Cremer, C., Gall´e, M., Fadaee, M., Kreutzer,\nJ., Pietquin, O., ¨Ust¨un, A., and Hooker, S.\nBack\nto basics: Revisiting reinforce style optimization for\nlearning from human feedback in llms. arXiv preprint\narXiv:2402.14740, 2024.\nAzar, M. G., Guo, Z. D., Piot, B., Munos, R., Rowland, M.,\nValko, M., and Calandriello, D. A general theoretical\nparadigm to understand learning from human preferences.\nIn International Conference on Artificial Intelligence and\nStatistics, pp. 4447–4455. PMLR, 2024.\nBai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., Das-\nSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T.,\net al. Training a helpful and harmless assistant with rein-\nforcement learning from human feedback. arXiv preprint\narXiv:2204.05862, 2022a.\nBai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J.,\nJones, A., Chen, A., Goldie, A., Mirhoseini, A., McKin-\nnon, C., et al. Constitutional ai: Harmlessness from ai\nfeedback. arXiv preprint arXiv:2212.08073, 2022b.\nBubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J.,\nHorvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y.,\nLundberg, S., et al. Sparks of artificial general intel-\nligence: Early experiments with gpt-4. arXiv preprint\narXiv:2303.12712, 2023.\nChristiano, P. F., Leike, J., Brown, T., Martic, M., Legg,\nS., and Amodei, D. Deep reinforcement learning from\nhuman preferences. Advances in neural information pro-\ncessing systems, 30, 2017.\nCui, G., Yuan, L., Ding, N., Yao, G., Zhu, W., Ni, Y., Xie, G.,\nLiu, Z., and Sun, M. Ultrafeedback: Boosting language\nmodels with high-quality feedback. 2023.\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle,\nA., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan,\nA., et al. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783, 2024.\nDubois, Y., Galambosi, B., Liang, P., and Hashimoto, T. B.\nLength-controlled alpacaeval: A simple way to debias\nautomatic evaluators. arXiv preprint arXiv:2404.04475,\n2024.\nEthayarajh, K., Xu, W., Muennighoff, N., Jurafsky, D., and\nKiela, D. Kto: Model alignment as prospect theoretic\noptimization. arXiv preprint arXiv:2402.01306, 2024.\nGanguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y.,\nKadavath, S., Mann, B., Perez, E., Schiefer, N., Ndousse,\nK., et al. Red teaming language models to reduce harms:\nMethods, scaling behaviors, and lessons learned. arXiv\npreprint arXiv:2209.07858, 2022.\nGao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang,\nY., Callan, J., and Neubig, G. Pal: Program-aided lan-\nguage models. In International Conference on Machine\nLearning, pp. 10764–10799. PMLR, 2023a.\nGao, L., Schulman, J., and Hilton, J. Scaling laws for reward\nmodel overoptimization. In International Conference on\nMachine Learning, pp. 10835–10866. PMLR, 2023b.\nGunter, T., Wang, Z., Wang, C., Pang, R., Narayanan, A.,\nZhang, A., Zhang, B., Chen, C., Chiu, C.-C., Qiu, D.,\net al. Apple intelligence foundation language models.\narXiv preprint arXiv:2407.21075, 2024.\nHan, S., Shenfeld, I., Srivastava, A., Kim, Y., and Agrawal, P.\nValue augmented sampling for language model alignment\nand personalization. arXiv preprint arXiv:2405.06639,\n2024.\nHong, J., Lee, N., and Thorne, J. Orpo: Monolithic prefer-\nence optimization without reference model. In Proceed-\nings of the 2024 Conference on Empirical Methods in\nNatural Language Processing, pp. 11170–11189, 2024.\n9\n\nLean and Mean: Decoupled Value Policy Optimization with Global Value Guidance\nHu, J.\nReinforce++: A simple and efficient approach\nfor aligning large language models.\narXiv preprint\narXiv:2501.03262, 2025.\nHu, J., Wu, X., Wang, W., Zhang, D., Cao, Y., et al. Open-\nrlhf: An easy-to-use, scalable and high-performance rlhf\nframework. arXiv preprint arXiv:2405.11143, 2024.\nHuang, C., Fan, Z., Wang, L., Yang, F., Zhao, P., Lin,\nZ., Lin, Q., Zhang, D., Rajmohan, S., and Zhang, Q.\nSelf-evolved reward learning for llms. arXiv preprint\narXiv:2411.00418, 2024a.\nHuang, S., Liu, T., and Von Werra, L. The n implementation\ndetails of rlhf with ppo. In The Third Blogpost Track at\nICLR 2024, 2024b.\nHuang, S., Noukhovitch, M., Hosseini, A., Rasul, K., Wang,\nW., and Tunstall, L. The n+ implementation details of rlhf\nwith ppo: A case study on tl; dr summarization. arXiv\npreprint arXiv:2403.17031, 2024c.\nKong, L., Wang, H., Mu, W., Du, Y., Zhuang, Y., Zhou, Y.,\nSong, Y., Zhang, R., Wang, K., and Zhang, C. Aligning\nlarge language models with representation editing: A\ncontrol perspective. arXiv preprint arXiv:2406.05954,\n2024.\nLi, T., Chiang, W.-L., Frick, E., Dunlap, L., Wu, T., Zhu, B.,\nGonzalez, J. E., and Stoica, I. From crowdsourced data to\nhigh-quality benchmarks: Arena-hard and benchbuilder\npipeline. arXiv preprint arXiv:2406.11939, 2024.\nLi, Z., Xu, T., Zhang, Y., Yu, Y., Sun, R., and Luo, Z.-\nQ. Remax: A simple, effective, and efficient method\nfor aligning large language models.\narXiv preprint\narXiv:2310.10505, 2023.\nMao, X., Li, F.-L., Xu, H., Zhang, W., and Luu, A. T.\nDon’t forget your reward values:\nLanguage model\nalignment via value-based calibration. arXiv preprint\narXiv:2402.16030, 2024.\nMeng, Y., Xia, M., and Chen, D. Simpo: Simple preference\noptimization with a reference-free reward. arXiv preprint\narXiv:2405.14734, 2024.\nMoskovitz, T., Singh, A. K., Strouse, D., Sandholm, T.,\nSalakhutdinov, R., Dragan, A. D., and McAleer, S. Con-\nfronting reward model overoptimization with constrained\nrlhf. arXiv preprint arXiv:2310.04373, 2023.\nNoukhovitch, M., Lavoie, S., Strub, F., and Courville, A. C.\nLanguage model alignment with elastic reset. Advances\nin Neural Information Processing Systems, 36, 2024.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\net al. Training language models to follow instructions\nwith human feedback. Advances in neural information\nprocessing systems, 35:27730–27744, 2022.\nPark, R., Rafailov, R., Ermon, S., and Finn, C. Disentan-\ngling length from quality in direct preference optimiza-\ntion. arXiv preprint arXiv:2403.19159, 2024.\nRafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Er-\nmon, S., and Finn, C. Direct preference optimization:\nYour language model is secretly a reward model. Ad-\nvances in Neural Information Processing Systems, 36,\n2024.\nRamamurthy, R., Ammanabrolu, P., Brantley, K., Hessel,\nJ., Sifa, R., Bauckhage, C., Hajishirzi, H., and Choi,\nY. Is reinforcement learning (not) for natural language\nprocessing: Benchmarks, baselines, and building blocks\nfor natural language policy optimization. arXiv preprint\narXiv:2210.01241, 2022.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and\nKlimov, O. Proximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347, 2017.\nShao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang,\nH., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Push-\ning the limits of mathematical reasoning in open language\nmodels. arXiv preprint arXiv:2402.03300, 2024.\nSong, F., Yu, B., Li, M., Yu, H., Huang, F., Li, Y., and Wang,\nH. Preference ranking optimization for human alignment.\nIn Proceedings of the AAAI Conference on Artificial In-\ntelligence, volume 38, pp. 18990–18998, 2024.\nStiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R.,\nVoss, C., Radford, A., Amodei, D., and Christiano,\nP. F. Learning to summarize with human feedback. Ad-\nvances in Neural Information Processing Systems, 33:\n3008–3021, 2020.\nSutton, R. S. Reinforcement learning: An introduction. A\nBradford Book, 2018.\nvon Werra, L., Belkada, Y., Tunstall, L., Beeching, E.,\nThrush, T., Lambert, N., Huang, S., Rasul, K., and\nGallou´edec, Q. Trl: Transformer reinforcement learn-\ning. https://github.com/huggingface/trl,\n2020.\nWang, B., Zheng, R., Chen, L., Liu, Y., Dou, S., Huang, C.,\nShen, W., Jin, S., Zhou, E., Shi, C., et al. Secrets of rlhf\nin large language models part ii: Reward modeling. arXiv\npreprint arXiv:2401.06080, 2024.\nWu, T., Zhu, B., Zhang, R., Wen, Z., Ramchandran, K., and\nJiao, J. Pairwise proximal policy optimization: Harness-\ning relative feedback for llm alignment. arXiv preprint\narXiv:2310.00212, 2023.\n10\n\nLean and Mean: Decoupled Value Policy Optimization with Global Value Guidance\nXu, S., Fu, W., Gao, J., Ye, W., Liu, W., Mei, Z., Wang,\nG., Yu, C., and Wu, Y.\nIs dpo superior to ppo for\nllm alignment? a comprehensive study. arXiv preprint\narXiv:2404.10719, 2024.\nYao, Z., Aminabadi, R. Y., Ruwase, O., Rajbhandari, S., Wu,\nX., Awan, A. A., Rasley, J., Zhang, M., Li, C., Holmes,\nC., et al.\nDeepspeed-chat: Easy, fast and affordable\nrlhf training of chatgpt-like models at all scales. arXiv\npreprint arXiv:2308.01320, 2023.\nZhao, Y., Joshi, R., Liu, T., Khalman, M., Saleh, M., and\nLiu, P. J. Slic-hf: Sequence likelihood calibration with\nhuman feedback. arXiv preprint arXiv:2305.10425, 2023.\nZheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z.,\nZhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging\nllm-as-a-judge with mt-bench and chatbot arena. Ad-\nvances in Neural Information Processing Systems, 36:\n46595–46623, 2023.\nZiegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford,\nA., Amodei, D., Christiano, P., and Irving, G. Fine-tuning\nlanguage models from human preferences. arXiv preprint\narXiv:1909.08593, 2019.\n11\n\nLean and Mean: Decoupled Value Policy Optimization with Global Value Guidance\nA. Training Details\nSFT training. We use the following hyperparameters for instruction fine-tuning training. We employ a learning rate of 2e-5\nwith cosine decay, 2 warmup steps, and a batch size of 16. We calculate the loss only for the target tokens rather than the\nfull input sequence, and we train for 3 epochs on the training data. we conduct the training on 8 NVIDIA A100 80G GPUs.\nReward training. To enable the model to learn the relative ranking among different responses, we use a pair-wise loss. We\nutilize the LoRA method to train the RM on the SFT baseline, with a rank of 8, a LoRA alpha of 32, and a LoRA dropout of\n0.1. The task type is sequence classification. We use a learning rate of 2e-5 with linear decay and the AdamW optimizer for\ntraining over 2 epochs, with a batch size of 4. We conduct the training on 8 NVIDIA A100 80G GPUs .\nPPO training and GVPO training. For PPO training, we use a learning rate of 2e-6 and set the generate sample length\nto 1024. We employ a batch size of 8 and a mini-batch size of 2, with 4 PPO epochs and 2 gradient accumulation step.\nThe target KL divergence is set to 0.1 and initial KL coefficient is set to 0.2. For a fair comparison, GVPO training was\nconducted using the same hyperparameter settings (e.g., batch size, sequence length, etc.).\nGlobal value model training. We initialize the value model from the SFT model. The training is conducted with a batch\nsize of 32, a sequence length of 1024, and a learning rate of 2e-6. We employ the LoRA method to train the value model on\nthe SFT baseline, using a rank of 8, a LoRA alpha of 32, and a LoRA dropout of 0.1.\nB. GVM case study\nAs an island, is Beijing located in Asia?\nQuestion:\nResponse2:\n_Be\nij\ning\n_is\n_not\n_an\n_island\n…\nValue1:\n0.2099\n0.0333\n-0.0821\n-0.2597\n-0.1936\n-0.4590\n-0.1500\n…\nResponse1:\n_Be\nij\n_is\n_an\n_island\n…\ning\n_located\nValue2:\n-0.4766\n-0.2493\n0.1845\n-0.6177\n-0.0883\n-0.1289\n-0.1526\n…\nFigure 4. An example of the supervisory signal provided by a Global Value Model (GVM). The GVM is capable of providing token-level\nfeedback. In this example, the GVM assigns a lower value to the incorrect response (response2: “is an island”) and a higher value to\nthe critical token “not” in the correct response (response1: “not an island”).\nFor the same question, ”As an island, is Beijing located in Asia?”, the value model provides fine-grained supervisory signals\nfor two different responses. The GVM assigns specific values to each token in the responses. These values represent the\nmodel’s assessment of the importance or correctness of each token in the given context.\nAs shown in Figure 4. For Response 1, the critical token “not” is given a higher value (0.2099), highlighting its significance\nin forming the correct response, ”not an island.” For Response 2, the GVM assigns lower values to incorrect tokens, such as\n“is” (-0.6177) and ”an” (-0.4766), indicating their contribution to the incorrect response, “is an island.” This token-level\nevaluation demonstrates the GVM’s ability to guide learning by penalizing incorrect responses and reinforcing critical\ntokens in correct responses, thereby enhancing training accuracy and interpretability.\nC. GVM performance\nWe evaluate the performance of the GVM from multiple perspectives. Specifically, we observe that the GVM assigns higher\nvalue scores to good actions and lower value scores to bad actions, as illustrated in Figure 4. We evaluate the global value\nmodel(GVM) on a held-out test set. For each question, we expect the value model to assign higher values to good responses\ncompared to bad ones. We calculate the accuracy under various metrics (mean, P1, etc.) to assess the model’s performance.\n12\n\nLean and Mean: Decoupled Value Policy Optimization with Global Value Guidance\nTable 4 presents the performance metrics of the Global Value Model (GVM) initialized from Mistral-7B-instruction-v0.2.\nThe metrics include the mean value, which indicates the average score assigned by the GVM, and various percentiles (P1, P5,\nP10, P90, P95, and P99), representing the corresponding value distributions. The accuracy metric evaluates the proportion\nof cases where the GVM correctly assigns a higher value to a good response compared to a bad response, thereby reflecting\nthe effectiveness of the GVM in providing accurate feedback.\nAs shown in the table, the mean accuracy is 64.51%, with percentile values ranging from 56.02% (P1) to 61.47% (P90),\nindicating consistent performance across different value ranges. These results demonstrate the GVM’s ability to provide\nreliable token-level evaluations.\nTable 4. The performance of the GVM initialized from Mistral-7B-instruction-v0.2. Here, mean represents the average value of the\npredictions, P1 indicates the 1st percentile value, and accuracy reflects the proportion of cases where the value for a good response exceeds\nthe value for a bad response, which is considered as correct feedback from the global value model.\nMETRIC\nACCURACY\nMEAN VALUE\n64.51\nP1 VALUE\n56.02\nP5 VALUE\n58.46\nP10 VALUE\n59.83\nP90 VALUE\n61.47\nP95 VALUE\n60.30\nP99 VALUE\n59.94\nD. GPT4 evaluation Prompt\nA crucial element of our experimental framework is the evaluation of win rates using GPT-4. In this section, we provide\nthe prompts utilized to generate win rates for both the summarization and dialogue experiments. All experiments were\nconducted using the gpt-4o-20240806 model. The sequence of responses was randomized for each evaluation to ensure\nunbiased results.\nGPT-4 as judge system prompt:\nReview the user’s question and the corresponding response using the additive 5-pointscoring system described below. Points\nare accumulated based on the satisfaction of each criterion:\n- Add 1 point if the response is relevant and provides some information related to the user’s inquiry, even if it is incomplete\nor contains some irrelevant content.\n- Add another point if the response addresses a substantial portion of the user’s question, but does not completely resolve\nthe query or provide a direct answer.\n- Award a third point if the response answers the basic elements of the user’s question in a useful way, regardless of whether\nit seems to have been written by an Al Assistant or if it has elements typically found in blogs or search results.\n- Grant a fourth point if the response is clearly written from an Al Assistant’s perspective, addressing the user’s question\ndirectly and comprehensively, and is well-organized and helpful, even if there is slight room for improvement in clarity,\nconciseness or focus.\n- Bestow a fifth point for a response that is impeccably tailored to the user’s question by an AI Assistant, without extraneous\ninformation, refecting expert knowledge, and demonstrating a high-quality, engaging, and insightful answer.\nAfter examining the user’s instruction and the response, please first give the total score. Then provide a brief explanation of\nyour total score, up to 100 words. Output with the following format: Score: {total score} Evaluation evidence: {your brief\nexplanation here}\nRemember to assess from the Al Assistant perspective, utilizing web search knowledge as necessary. To evaluate the\nresponse in alignment with this additive scoring model, we’ll systematically attribute points based on the outlined criteria.\n13\n\nLean and Mean: Decoupled Value Policy Optimization with Global Value Guidance\nUser input prompt:\n<Question>:\n{question}\n<response>:\n{answer}\nTable 5. A case study for GPT4 evaluation: LLama3 8B SFT\nInput\nCompare and contrast Harry Potter with Percy Jackson series.\nLlama3 8B SFT\nSure, I’d be happy to help you compare and contrast the Harry Potter and Percy Jackson series!Both\nHarry Potter and Percy Jackson are popular fantasy series that have captured the hearts of readers\naround the world. Here are some key similarities and differences between the two:Similarities:1.\nMagical elements: Both series feature a magical aspect, with Hogwarts School of Witchcraft and\nWizardry being the setting for Harry Potter and Camp Half-Blood for Percy Jackson.2. Mythological\nreferences: Both series draw heavily on Greek mythology, with Percy Jackson’s stories being based\non the Labors of Hercules and other Greek myths.3. Relatable protagonists: Both Harry and Percy\nare relatable and charming protagonists that readers can connect with.Differences:1. Setting: While\nboth series take place in magical worlds, the settings are quite different. Harry Potter’s stories take\nplace in a more traditional school setting, while Percy Jackson’s stories take place in a summer\ncamp atmosphere.2. Tone: Harry Potter is a more lighthearted and comedic series, while Percy\nJackson takes a darker and more serious tone, exploring themes of mortality and responsibility.3.\nPlot structure: Harry Potter has a more linear plot structure, with each book building on the previous\none’s events. Percy Jackson, on the other hand, has a more episodic structure, with each book\nfocusing on a different adventure or challenge for the characters.4. Themes: While both series\nexplore themes of good and evil, Harry Potter focuses more on the importance of friendship and\nloyalty, while Percy Jackson delves into the consequences of power and responsibility.In conclusion,\nwhile both Harry Potter and Percy Jackson share many similarities, their differences in setting, tone,\nplot structure, and themes offer unique reading experiences for readers. Both series are highly\nrecommended for anyone looking for a magical and engaging read!\nGPT4-as-Judge\nScore: 3\nEvaluation evidence: The response provides a relevant comparison of the Harry Potter and Percy\nJackson series, highlighting key similarities and differences in magical elements, settings, tone, plot\nstructure, and themes. It addresses the user’s question by outlining basic elements and offering a\nuseful comparison. However, the answer contains inaccuracies, such as misrepresenting the tone of\nboth series and the nature of mythological references in Percy Jackson. Additionally, the response\ncould be more precise and direct, as some points are not entirely accurate or comprehensive. Thus,\nit meets the criteria for a basic, useful response but lacks thoroughness and precision for a higher\nscore.\n14\n\nLean and Mean: Decoupled Value Policy Optimization with Global Value Guidance\nTable 6. A case study for GPT4 evaluation: LLama3 8B PPO\nInput\nCompare and contrast Harry Potter with Percy Jackson series.\nLlama3 8B PPO\nSure, I’d be happy to help you compare and contrast the Harry Potter and Percy Jackson series!Both\nHarry Potter and Percy Jackson are popular fantasy series that have captured the hearts of readers\naround the world. Here are some key similarities and differences between the two:Similarities:1.\nMagical elements: Both series feature a magical aspect, with Hogwarts School of Witchcraft and\nWizardry being the setting for Harry Potter and Camp Half-Blood being the setting for Percy\nJackson.2. Teenage protagonists: The main characters in both series are teenagers, navigating their\nway through adolescence while also facing epic challenges and learning valuable life lessons.3.\nEpic adventures: Both series involve the protagonists on thrilling quests, battling mythical creatures,\nand uncovering secrets that could change the course of history.4. Portal to another world: In both\nseries, there is a gateway to another world (Harry’s cupboard under the stairs and the portal at\nCamp Half-Blood) that allows the protagonists access to a realm filled with magical creatures and\nunparalleled adventure.Differences:1. Setting: While both series take place in magical worlds, the\nsettings are quite different. Harry Potter is set in a medieval-style world with castles and magical\ncreatures, while Percy Jackson is set in a modern-day world with technology and urban landscapes.2.\nTone: The tone of the two series is quite different. Harry Potter is lighthearted and comedic, while\nPercy Jackson is darker and more serious, with themes of mortality and morality.3. Mythological\nreferences: While both series draw on mythological references, the specific myths and legends used\nare different. Harry Potter relies more on European and British myths, while Percy Jackson draws\non Greek and Roman mythology.4. Magic systems: The magic systems in the two series are also\ndifferent. Harry Potter uses a spell-based magic system, while Percy Jackson uses a more physical,\nathletic approach to magic.5. Protagonists: The protagonists in the two series are quite different.\nHarry Potter is a boy wizard who discovers his true identity and heritage, while Percy Jackson is a\ndemigod who must navigate the complexities of his divine heritage and mortal life.6. Plots: The\nplots of the two series are distinct. Harry Potter focuses on the battle between good and evil, with\nHarry as the prophesized hero who must face off against Voldemort. Percy Jackson, on the other\nhand, is about the struggles of the demigods and the impact of their mortal lives on their divine\nheritage.In conclusion, both Harry Potter and Percy Jackson are captivating fantasy series that share\nsome similarities, but also differ in many ways. While both offer engaging and entertaining reads,\nthey appeal to different audiences and perspectives.\nGPT4-as-Judge\nScore: 3\nEvaluation evidence: The response provides a relevant comparison of the Harry Potter and Percy\nJackson series, addressing the user’s question with a mix of similarities and differences. It covers\nkey aspects such as magical elements, protagonists, and settings, offering a basic understanding\nof both series. However, the response contains inaccuracies, particularly in tone descriptions\nand settings, which detracts from its comprehensiveness and accuracy. While it offers a useful\ncomparison, it does not fully address the user’s question with the depth or precision expected from\nan AI Assistant, nor does it reflect expert knowledge or exceptional insight.\n15\n\nLean and Mean: Decoupled Value Policy Optimization with Global Value Guidance\nTable 7. A case study for GPT4 evaluation: LLama3 8B DVPO\nInput\nCompare and contrast Harry Potter with Percy Jackson series.\nLlama3 8B DVPO\nSure, I’d be happy to help! Both the Harry Potter and Percy Jackson series are popular and\nsuccessful book series that have captivated readers worldwide. Here are some key similarities and\ndifferences between the two:Similarities:1. Magical elements: Both series feature magical aspects,\nsuch as spells, wizards, and fantastical creatures.2. Young protagonists: The main characters in\nboth series are young and face challenges and adventures that come with growing up in a magical\nworld.3. Family themes: Both series emphasize the importance of family and the bonds between\nfamily members.4. Friendship: The series also highlight the importance of friendship and the power\nof having loyal and supportive friends.Differences:1. Genre: While both series can be classified as\nfantasy, Harry Potter is more traditionally fantasy-oriented, while Percy Jackson is urban fantasy\nwith a contemporary setting.2. Setting: Harry Potter is set in a parallel universe where wizardry and\nmagic are prevalent, while Percy Jackson is set in present-day America and Greece.3. Magic system:\nThe magic systems in each series are different. In Harry Potter, magic is cast using wands and\nspells, while in Percy Jackson, magic is accessed through the use of magical coins and enchanted\nitems.4. Gods and myths: Harry Potter is based on the idea of witches and wizards living among us,\nwhile Percy Jackson is based on Greek mythology and features the gods and goddesses of ancient\nGreece.5. Tone: Harry Potter is written in a more traditional, epic style, while Percy Jackson is\nwritten in a more contemporary, humorous tone.Ultimately, both series offer engaging, imaginative\nstorytelling that captures the hearts and minds of readers. While they both explore similar themes,\ntheir unique settings, magic systems, and tones make them distinct from one another. I hope this\nhelps! Let me know if you have any other questions.\nGPT4-as-Judge\nScore: 5\nEvaluation evidence: The response thoroughly addresses the user’s question by comparing and\ncontrasting the Harry Potter and Percy Jackson series. It identifies key similarities, such as\nmagical elements, young protagonists, and themes of family and friendship. The response also\neffectively highlights differences in genre, setting, magic systems, mythological influences, and\ntone. The information is well-organized, relevant, and engaging, demonstrating a comprehensive\nunderstanding of both series. The response is tailored specifically to the user’s inquiry, providing\ninsightful and expert knowledge without any extraneous information.\n16\n",
  "metadata": {
    "source_path": "papers/arxiv/Lean_and_Mean_Decoupled_Value_Policy_Optimization_with_Global_Value\n__Guidance_84c8f1f39d04d1d9.pdf",
    "content_hash": "84c8f1f39d04d1d971a11a530c73685afc1c756ca2011ef6c4cc51ea16c215a4",
    "arxiv_id": null,
    "title": "Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance",
    "author": "Chenghua Huang, Lu Wang, Fangkai Yang, Pu Zhao, Zhixu Li, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan, Qi Zhang, ♦ School of Computer Science, Fudan University, ♣ Microsoft, huangch22@m.fudan.edu.cn, {wlu, fangkaiyang, puzhao, dongmeiz}@microsoft.com",
    "creation_date": "D:20250225023156Z",
    "published": "2025-02-25T02:31:56",
    "pages": 16,
    "size": 817765,
    "file_mtime": 1740470202.9083052
  }
}