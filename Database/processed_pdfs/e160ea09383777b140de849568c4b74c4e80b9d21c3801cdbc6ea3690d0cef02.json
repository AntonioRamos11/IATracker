{
  "text": "The Geometry of Refusal in Large Language Models:\nConcept Cones and Representational Independence\nTom Wollschl¨ager * 1 Jannes Elstner * 1 Simon Geisler 1 Vincent Cohen-Addad 2\nStephan G¨unnemann 1 Johannes Gasteiger 2 3\nAbstract\nThe safety alignment of large language mod-\nels (LLMs) can be circumvented through adver-\nsarially crafted inputs, yet the mechanisms by\nwhich these attacks bypass safety barriers re-\nmain poorly understood. Prior work suggests\nthat a single refusal direction in the model’s ac-\ntivation space determines whether an LLM re-\nfuses a request. In this study, we propose a novel\ngradient-based approach to representation engi-\nneering and use it to identify refusal directions.\nContrary to prior work, we uncover multiple in-\ndependent directions and even multi-dimensional\nconcept cones that mediate refusal. Moreover,\nwe show that orthogonality alone does not im-\nply independence under intervention, motivating\nthe notion of representational independence that\naccounts for both linear and non-linear effects.\nUsing this framework, we identify mechanisti-\ncally independent refusal directions. We show\nthat refusal mechanisms in LLMs are governed\nby complex spatial structures and identify func-\ntionally independent directions, confirming that\nmultiple distinct mechanisms drive refusal behav-\nior. Our gradient-based approach uncovers these\nmechanisms and can further serve as a foundation\nfor future work on understanding LLMs.1\n1. Introduction\nThe breakthrough of scaling large language models (LLMs)\nhas led to an unprecedented leap in capabilities, driving\nwidespread real-world adoption (OpenAI, 2022). How-\never, these advancements also introduce serious risks. As\nartificial intelligence becomes more powerful, it can be\nmisused for harmful purposes, such as attacking critical\n*Equal contribution\n1School of Computation, Information &\nTechnology and Munich Data Science Institute, Technical Univer-\nsity of Munich 2Google Research 3Now at Anthropic. Correspon-\ndence to: Tom Wollschl¨ager <tom.wollschlaeger@tum.de>.\n1 Resources & code: cs.cit.tum.de/daml/geometry-of-refusal\nFigure 1. An example of a 3D\nconcept cone with its basis vec-\ntors. All directions in the cone\nmediate refusal.\ninfrastructure or spread-\ning misinformation. En-\nsuring that these mod-\nels remain aligned with\nhuman values has be-\ncome a crucial research\nchallenge (Liu et al.,\n2023; Schwinn et al.,\n2025).\nDespite signif-\nicant progress,\nLLMs,\nlike all machine learning\nmodels, remain vulnera-\nble to adversarial attacks\nthat can bypass align-\nment mechanisms and\ninduce harmful outputs\n(Szegedy et al., 2014; Carlini et al., 2024).\nRecent work in interpretability has provided valuable in-\nsights into how LLMs encode and process information\n(Nanda et al., 2024; Wang et al., 2022; Cunningham et al.,\n2023; Heinzerling & Inui, 2024).\nPrior studies (Bel-\nrose et al., 2023; Gurnee & Tegmark, 2023; Marks &\nTegmark, 2024) suggest that concepts—ranging from sim-\nple to complex—are often encoded linearly in the model’s\nresidual stream. Methods such as representation engineering\n(Zou et al., 2023a) allow researchers to use input prompts\nto analyze model behavior by extracting and manipulating\nsuch concepts. However, the mechanisms enabling adver-\nsarial jailbreaks that bypass alignment safeguards remain\npoorly understood. Some evidence suggests that refusals to\nharmful queries are mediated by a single “refusal direction”\nin activation space (Arditi et al., 2024), and that jailbreaks\nrely on manipulating this direction (Yu et al., 2024), yet\nthese assumptions require further examination.\nIn this work, we go beyond extracting concepts using com-\nmon input prompt methods by introducing a novel gradient-\nbased approach to representation engineering which we use\nto investigate the mechanisms underlying refusal behavior\nin LLMs. We extract refusal-mediating directions more\neffectively, improving both precision and control while min-\nimizing unintended side effects, which we demonstrate in\n1\narXiv:2502.17420v1  [cs.LG]  24 Feb 2025\n\nThe Geometry of Refusal in Large Language Models\nSection 4. Unlike prior work that assumes model refusal is\ncontrolled by a single linear direction, we show in Section 5\nthat there exist multi-dimensional polyhedral cones which\ncontain infinite refusal directions; we show an illustrative\nexample in Figure 1. To further characterize refusal mech-\nanisms in language models, we introduce representational\nindependence, a criterion for identifying directions that re-\nmain mutually unaffected under intervention, capturing both\nlinear and non-linear dependencies across layers. In Sec-\ntion 6, we demonstrate that even under this strict notion of\nindependence, multiple complementary refusal directions\nexist.\nTo summarize, our core contributions are:\n• We show that our gradient-based representation engi-\nneering can advance general LLM understanding and\nspecifically demonstrate its efficacy for understanding\nrefusal mechanisms.\n• We introduce representational independence, a practi-\ncal framework for characterizing how different inter-\nventions interact within an LLM’s activation space, and\nuse it to find independent refusal directions.\n• We show that rather than a single refusal direction,\nthere exist multi-dimensional cones in which all direc-\ntions mediate refusal.\n2. Background\nNotation.\nLet f : TNseq →∆Nseq×|T| denote a language model, where\n∆|T| is the probability simplex over vocabulary T. Given\na prompt p = (t1, . . . , tNseq) ∈TNseq consisting of tokens\nti, each token is first embedded: x(0)\ni\n= EMBED(ti). The\nmodel then processes the token sequence through L layers,\nwhere at each layer l = 1, . . . , L and token position i the\nfollowing transformation is applied:\n˜x(l)\ni\n= x(l)\ni\n+ ATTN(l)(x(l)\n1:i), x(l+1)\ni\n= ˜x(l)\ni\n+ MLP(˜x(l)\ni )\nThe final residual stream x(L+1)\ni\nis unembedded to yield log-\nits: ℓi = UNEMBED(x(L+1)\ni\n). The softmax function con-\nverts these logits into a probability distribution over tokens:\nP(t | t1, . . . , ti) = softmax(ℓi)t. We omit technical details\nthat are not critical for this work such as LayerNorm.\nExtracting refusal directions. Paired prompts of harmful\nand harmless requests allow the extraction of a directional\nfeature from the model’s residual stream as shown by prior\nwork (Panickssery et al., 2024; Bolukbasi et al., 2016; Burns\net al., 2024). Recent studies obtain this direction by com-\nputing the difference-in-means (DIM) (Panickssery et al.,\n2024; Arditi et al., 2024; Stolfo et al., 2024) between model\nrepresentations on datasets of harmful prompts Dharm and\nharmless prompts Dgood:\nv(l)\ni\n=\n1\n|Dharm|\n\nX\np′∈Dharm\nx(l)\ni (p′)\n\n−\n1\n|Dsafe|\n\nX\np∈Dsafe\nx(l)\ni (p)\n\n\nHere, x(l)\ni (p) represents the residual stream activations at\nposition i, layer l for input prompt p.\nAdversarial steering attacks. The extracted harmfulness\ndirection can be used to manipulate the model’s refusal\nbehavior. With white-box access, an attacker can prompt\nthe model with harmful queries and suppress activations\nin the harmfulness direction, thereby reducing the model’s\nprobability of refusal. This can be done through directional\nablation of r (where ˆr denotes the unit vector) (Zou et al.,\n2023a):\n˜x(l)\ni\n= x(l)\ni\n−ˆrˆr⊤x(l)\ni ,\n(1)\nwhich projects the residual stream to a subspace orthogonal\nto r, or alternatively through activation subtraction:\nˇx(l)\ni\n= x(l)\ni\n−α · ˆr,\n(2)\nwhich subtracts a scaled r from the residual stream.We\nfollow common practice to apply both operations across all\ntoken positions and ablation across all layers while doing\nsubtraction only at a single layer.\n3. Related Work\nAdversarial attacks for LLMs. Many studies have ex-\nplored hand-crafted adversarial techniques, such as persona\nmodulation (Shah et al., 2023), language modifications (Zhu\net al., 2023), or prompt engineering using repetitions and\npersuasive phrasing (Rao et al., 2024). Other works take a\nmore systematic approach, employing techniques like ge-\nnetic algorithms and random search (Chen et al., 2024),\ndiscrete optimization over input tokens (Zou et al., 2023b),\nor gradient-based methods to identify high-impact pertur-\nbations (Geisler et al., 2024). While identifying these vul-\nnerabilities enables adversarial fine-tuning (Xhonneux et al.,\n2024) or improved training through Reinforcement Learning\nwith Human Feedback (RLHF), recent works suggest that\nrobustness remains a challenge (Zou et al., 2023a; Schwinn\net al., 2024; Geisler et al., 2024; Scholten et al., 2025).\nInterpretability of LLMs. A parallel line of research fo-\ncuses on understanding the internal mechanisms of LLMs,\nas their natural language outputs provide a unique oppor-\ntunity to link internal states to interpretable behaviors. In-\nterpretability research has led to the identification of var-\nious “features”—concepts represented by distinct activa-\ntion patterns (Cunningham et al., 2023)—as well as “cir-\ncuits”, which are subnetworks that implement a specific\nfunction or behavior. Prominent examples are backup cir-\ncuits (Nanda et al., 2024) and information mover circuits\n2\n\nThe Geometry of Refusal in Large Language Models\n(Wang et al., 2022). Many interpretability insights rely on\nextracting features using paired inputs with opposing se-\nmantics (Burns et al., 2024) and then manipulating residual\nstream activations to elicit specific behaviors (Panickssery\net al., 2024). Representation engineering, as proposed by\nZou et al. (2023a), investigates the linear representation of\nconcepts such as truthfulness, honesty, and fairness in LLMs.\nThe effectiveness of these methods supports the hypothesis\nthat many features are encoded linearly in LLMs (Marks\n& Tegmark, 2024). These insights allow researchers to pin-\npoint and manipulate concept representations or specific\ncircuits, enabling targeted debugging of behaviors, mitigat-\ning biases, and advancing safer, more reliable AI systems.\nUnderstanding Refusal Mechanisms. Recent research\nhas focused on understanding the mechanisms underlying\nrefusal behaviors in LLMs. For example, removing safety-\ncritical neurons has been shown to decrease robustness (Wei\net al., 2024; Li et al., 2024b). Zheng et al. (2024) demon-\nstrate that adding explicit safety prompts shifts the internal\nrepresentation along a harmfulness direction. O’Brien et al.\n(2024) propose to use sparse autoencoders to identify latent\nfeatures that mediate refusal. The most relevant work to ours\nis Arditi et al. (2024), which builds on Zou et al. (2023a)\nand examines the representation of refusal in LLMs. Their\nwork suggests that a single direction a model’s activation\nspace determines whether the model accepts or refuses a\nrequest. We challenge this notion by showing that refusal is\nmediated through more nuanced mechanisms.\n4. Gradient-based Refusal Directions\nResearch Question: Can gradient-based representa-\ntion engineering identify refusal directions?\nTo investigate the refusal mechanisms in language models,\nwe propose a gradient-based algorithm that identifies direc-\ntions controlling refusal in the model’s activation space. We\nrefer to it as Refusal Direction Optimization (RDO). Un-\nlike prior approaches that extract refusal directions using\npaired prompts of harmless and harmful instructions (Arditi\net al., 2024), our method leverages gradients to find better\ndirections instead of solely relying on model activations.\nSimilar to (Park et al., 2023), we define two key properties\nfor refusal directions:\nDefinition 4.1. Refusal Properties:\n• Monotonic Scaling: when using the direction for acti-\nvation addition/subtraction\nˇx(l)\ni\n= x(l)\ni\n+ α · r, the model’s probability of refus-\ning instructions should scale monotonically with α.\n• Surgical Ablation:\nablating the refusal direction\nthrough projection\n˜x(l)\ni\n= x(l)\ni\n−ˆrˆr⊤x(l)\ni\nshould\nAlgorithm 1 Refusal Direction Optimization (RDO)\nInput: Frozen model f, scaling coefficient α, addition layer\nindex ladd, learning rate η, loss weights λabl, λadd, λret, and\ndata D = {(pharm,i, psafe,i, tanswer,i, trefusal,i, tretain,i)}N\ni=1.\nOutput: Refusal direction r\n1: Initialize r randomly\n2: while not converged do\n3:\nSample batch B ∼D\n4:\nL ←COMPUTELOSS(r, f, B)\n5:\nr ←r −η∇rL\n6:\nr ←r/||r||2\n7: end while\n1: function COMPUTELOSS(r, f, B)\n2:\npharm, psafe, tanswer, trefusal, tretain = B\n3:\nLablation = CELOSS(fablate(r)(pharm), tanswer)\n4:\nLaddition = CELOSS(fadd(αˆr,ladd)(psafe), trefusal)\n5:\nLretain = KL(fablate(r)(psafe), f(psafe), tretain)\n6:\nL = λablLablation + λaddLaddition + λretLretain\n7:\nreturn L\ncause the model to answer previously refused harmful\nprompts, while preserving normal behavior on harm-\nless inputs.\nWe can encode the desired refusal properties into loss func-\ntions, allowing us to find corresponding refusal vectors r\nusing gradient descent. For the monotonic scaling property,\nwe train the model to refuse harmless instructions psafe when\nrunning the model f with a modified forward pass fadd(r,l)\nin which we add r to the activations at layer l. We mini-\nmize the cross-entropy between the model output and target\nrefusal response trefusal. For the surgical ablation property,\nwe similarly compute the cross-entropy between a harmful\nresponse target tanswer and the output of a modified forward\npass fablate(r) to make the model respond to harmful instruc-\ntions. A key strength of our gradient-based approach is the\nability to control any predefined objective and thus we can\ncontrol the extent to which other concepts are affected dur-\ning interventions. For this, we use a retain loss based on the\nKullback-Leibler (KL) divergence to ensure that directional\nablation of r on harmless instructions does not change the\nmodel’s output over a target response tretain. Algorithm 1\nshows the full training procedure for our refusal directions.\nSetup. We construct a dataset of harmless and harmful\nprompts from the ALPACA (Taori et al., 2023) and SALAD-\nBENCH (Li et al., 2024a) datasets (see Appendix A.1). An\nimportant consideration for our algorithm is the choice of\ntargets tanswer and trefusal. Generally, language models differ\nin their refusal and response styles, which is why we use\nmodel-specific targets rather than generating them via un-\ncensored LLMs as in Zou et al. (2024). Specifically, we use\n3\n\nThe Geometry of Refusal in Large Language Models\nFigure 2. Attack success rates of refusal directions for different models. We compare the DIM direction baseline that is extracted from\nprompts against our Refusal Direction Optimization for jailbreaking with directional ablation and activation subtraction.\nthe DIM refusal direction to generate our targets, though\nany effective attack can work. For the harmful answers\ntanswer, we ablate the DIM direction and generate 30 tokens.\nSimilarly, we use activation addition on harmless instruc-\ntions to produce refusal targets trefusal. For helpful answers\non harmless instructions that should be retained tretain, we\ngenerate 29 tokens without intervention. The retain loss\nLretain is applied over the last 30 tokens, such that the last\ntoken of the model’s chat template is included. We detail\nhyperparameters and implementation in Appendix A.\nEvaluation. We evaluate our method by training a refusal\ndirection on various models from the Gemma 2 (Team et al.,\n2024), Qwen2.5 (Yang et al., 2024), and Llama-3 (Dubey\net al., 2024) families and compare against the DIM direction\nfor which we use the same setup as Arditi et al. (2024) but\nwith our expanded dataset. For a fair comparison, we train\nthe refusal direction at the same layer that the DIM direction\nis extracted from, and during activation addition/subtraction\nset the scaling coefficient α to the norm of the DIM direc-\ntion. We evaluate the jailbreak Attack Success Rate (ASR)\non JAILBREAKBENCH (Chao et al., 2024) using the STRON-\nGREJECT fine-tuned judge (Souly et al., 2024). For in-\nducing refusal via activation addition, we test 128 harmless\ninstructions sampled from ALPACA using substring match-\ning of common refusal phrases. Model completions for\nevaluation are generated using greedy decoding with a max-\nimum generation length of 512 tokens.\nDoes the direction mediate refusal? In Figure 2, we show\nthat for jailbreaking, our approach is competitive when us-\ning directional ablation and, on average, outperforms DIM\nwhen subtracting the refusal direction. Notably, despite\nnot being explicitly optimized for subtraction-based attacks,\nour direction naturally generalizes to this setting. Figure 9\nshows that adding the refusal direction to harmless inputs\ninduces refusal more effectively with RDO than with DIM,\nfurther indicating that our method manipulates refusal more\neffectively.\nIs the direction more precise? To measure the side effects\nwhen intervening with the directions we track benchmark\nperformance. Arditi et al. (2024) show that directional abla-\ntion with the DIM direction tends to have little impact on\nbenchmark performance, except for TruthfulQA (Lin et al.,\n2021). In Table 1, we show that RDO impacts TruthfulQA\nperformance much less severely, reducing the error by 40%\non average.\nTable 1. TruthfulQA benchmark performance for directional abla-\ntion with the DIM or RDO directions, compared to the baseline\n(no intervention). Higher values indicate better performance.\nChat model\nDIM\nRDO (ours)\nBaseline\nGEMMA 2 2B\n47.8\n51.4 (+3.6)\n55.8\nGEMMA 2 9B\n52.8\n56.7 (+3.9)\n61.1\nLLAMA 3 8B\n48.7\n51.0 (+2.3)\n52.8\nQWEN 2.5 1.5B\n42.9\n44.0 (+1.1)\n46.5\nQWEN 2.5 3B\n54.2\n54.5 (+0.3)\n57.2\nQWEN 2.5 7B\n58.7\n60.0 (+1.3)\n63.1\nQWEN 2.5 14B\n63.3\n67.9 (+4.6)\n70.8\nIs our method versatile? Hyperparameter tuning of the\nretain loss weight λret in Algorithm 1 allows for balancing\nbetween attack success and side effects (see Appendix B).\nWe observe that for many models—especially those in the\nQwen 2.5 family—for the majority of estimated DIM direc-\ntions, the side-effects are too high, rendering it an unsuccess-\nful attack (see Figure 16). Our method is more flexible than\nprevious work as we can choose the target layer freely while\nlimiting side effects through the retain loss (if possible).\nKey Takeaways. Our RDO yields more effective re-\nfusal directions with fewer side effects, establishing\nthat gradient-based representation engineering is an\neffective approach for extracting meaningful direc-\ntions, while allowing for more modeling freedom\nsuch as incorporating side constraints.\n4\n\nThe Geometry of Refusal in Large Language Models\nFigure 3. Attack success rate for multi-dimensional cones for Gemma 2, Qwen 2.5 and Llama 3. The cone performance is measured via\nthe performance of Monte Carlo samples which are depicted as boxplot.\n5. Multi-dimensional Refusal Cones\nResearch Question: Is refusal in LLMs governed by\na single direction, or does it emerge from a more\ncomplex underlying geometry?\nWe extend RDO to higher dimensions by searching for\nregions in activation space where all vectors control re-\nfusal behavior. For this, we optimize an orthonormal basis\nB = [b1, . . . , bN] spanning an N-dimensional polyhedral\ncone RN = {PN\ni=1 λibi | λi ≥0}\\{0}, where all direc-\ntions r ∈RN satisfy the refusal properties (Definition 4.1).\nSince all directions in the cone correspond to the same re-\nfusal concept, we also refer to this as a concept cone. The\nconstraint λi ≥0 ensures that all directions within the\ncone consistently strengthen refusal behavior. Without this\nconstraint, allowing negative coefficients could introduce\nopposing effects, reducing the overall effectiveness. En-\nforcing orthogonality of the basis vectors prevents finding\nco-linear directions. Note that in practice, directions in acti-\nvation space cannot be scaled arbitrarily high without model\ndegeneration, which effectively bounds λi.\nIn Algorithm 2, we describe the procedure to find the cone’s\nbasis vectors. The basis vectors are initialized randomly\nand iteratively optimized using projected gradient descent.\nWe compute the previous losses defined in Algorithm 1 on\nMonte Carlo samples from the cone, as well as on the ba-\nsis vectors themselves. Computing the loss on the basis\nvectors improves both stability and the lower bounds of\nthe ASR. This is because the basis vectors are the bound-\naries of the cone and thus tend to degrade first. After each\nstep, we project the basis back onto the cone using the\nAlgorithm 2 Refusal Cone Optimization (RCO)\n1: Initialize B = [b1, . . . , bn] randomly\n2: while not converged do\n3:\nSample batch B ∼D\n4:\nLsample ←Er∼Sample(B)[COMPUTELOSS(r, f, B)]\n5:\nLbasis ←1\nn\nPn\ni=1 COMPUTELOSS(bi, f, B)\n6:\nL = Lsample + Lbasis\n7:\nB ←B −η∇BL\n8:\nB ←GRAMSCHMIDT(B)\n9: end while\n1: function SAMPLE(B)\n2:\ns ∼Unif(x ∈Rn\n+ : ||x||2 = 1)\n3:\nr = Bs\n4:\nreturn r\nGram-Schmidt orthogonalization procedure. Because the\ndirectional ablation operation uses the normalized ˆr rather\nthan r, sampling convex combinations of the basis vectors\nand normalizing them would introduce a bias towards the\nbasis vectors themselves. Instead, we sample unit vectors in\nthe cone uniformly to ensure better coverage of the space.\nCan we find refusal concept cones? We train cones of in-\ncreasing dimensionality using the same experimental setup\nas described in Section 4. We measure the cone’s effective-\nness in mediating refusal by sampling 256 vectors from each\ncone and computing the ASRs of the samples for directional\nablation. We show the results in Figure 3 and confirm that\nthe directions in the cones have the desired refusal proper-\nties in Figure 14. Notably, we identify refusal-mediating\ncones with dimensions up to five across all tested models.\nThis suggests that the activation space in language models\nexhibits a general property where refusal behavior is en-\n5\n\nThe Geometry of Refusal in Large Language Models\nFigure 4. Refusal evaluation for different cone dimensions for the Qwen2.5 model family. The cone performance for models with fewer\nparameters degrades faster with increasing cone dimension compared to larger models.\ncoded within multi-dimensional cones rather than a single\nlinear direction.\nDo larger models contain higher-dimensional cones?\nIn Figure 4, we evaluate the effect of model size within the\nQwen 2.5 family. We observe that across all model sizes, the\nlower bounds of cone performance degrade significantly as\ndimensionality increases. In other words, a higher number\nof sampled directions have low ASR. Larger models appear\nto support higher-dimensional refusal cones. A plausible\nexplanation is that models with larger residual stream di-\nmensions (e.g., 5120 for the 14B model vs. 1536 for the\n1.5B model) allow for more distinct and orthogonal direc-\ntions that mediate refusal. Finally, in Figure 11, we confirm\nthat directions sampled from these cones effectively induce\nrefusal behavior, further supporting the notion that multiple\naxes contribute to the model’s refusal decision.\nDo different directions uniquely influence refusal?\nTo further investigate the role of different vectors, we as-\nsess whether multiple sampled cone directions influence\nthe model in complementary ways. Specifically, we sample\nvarying numbers of directions from Gemma-2-2B’s four-\ndimensional refusal cone and, for each prompt, select the\nmost effective one under directional ablation (more details\nin Appendix A). To ensure a fair comparison, we use tem-\nperature sampling with the single-dimension RDO direction\nto generate the same number of attacks and similarly select\nthe most effective instance. We study Gemma 2 2B and\nsample from its four-dimensional cone, since performance\ndegrades significantly for larger dimensions (see Figure 10).\nFigure 5 shows that sampling multiple directions leads to\nhigher ASR compared to sampling with various temper-\natures in the low-sample regime. For a higher number of\nsamples, the randomness dominates the success of the attack.\nHowever, the higher ASR in the low-sample regime suggests\nthat different directions capture distinct, complementary as-\npects of the refusal mechanism. Additionally, Figure 13\nreveals that ASR increases with cone dimensionality but\nplateaus at four dimensions. This trend indicates that higher-\ndimensional cones offer an advantage over single-direction\nmanipulation, likely by influencing complementary mech-\nanisms. The plateau likely occurs because the model does\nnot support higher-dimensional refusal cones.\nFigure 5. ASR for best-of-N sampling using N samples from the\n4-dimensional refusal cone of Gemma-2-2B, compared to best-of-\nN sampling with temperature T using the single-dimension RDO.\nKey Takeaways.\nWe show that refusal mecha-\nnisms in LLMs span high-dimensional polyhedral\ncones, capturing diverse aspects of refusal behav-\nior. This highlights their geometric complexity and\ndemonstrates the effectiveness of our gradient-based\nmethod in identifying intricate structures.\n6\n\nThe Geometry of Refusal in Large Language Models\n6. Mechanistic Understanding of Directions\nResearch Question: Are there genuinely indepen-\ndent directions that influence a model’s refusal be-\nhavior? Can we access the discovered refusal direc-\ntions through perturbations in the token space?\nIn the previous section, we demonstrated that refusal\nbehavior spans a multi-dimensional cone with infinitely\nmany directions. However, whether the orthogonal refusal-\nmediating basis vectors manipulate independent mecha-\nnisms remains an open question. In this section, we conduct\na mechanistic analysis to investigate how these directions\ninteract within the model’s activation space and whether\nthey can be directly influenced through input manipulation.\nThis allows us to determine whether they are merely latent\nproperties of the network or actively utilized by the model\nin response to specific prompts.\n6.1. Representational Independence\nWe defined the basis vectors of the cones to be orthogonal,\nwhich is often considered an indicator of causal indepen-\ndence. The intuition is that if two vectors are orthogonal,\nthey each influence a third vector without interfering with\nthe other. Mathematically, for the directions r, v and repre-\nsentation x(l)\ni\nwe have:\nif rT v = 0 then rT (x(l)\ni\n−vvT x(l)\ni ) = rT x(l)\ni .\nHowever, despite this mathematical property, recent work\nby Park et al. (2024) suggests that in language models, con-\nclusions about causal independence cannot be drawn us-\ning orthogonality measured with the Euclidean scalar prod-\nuct. Although their assumptions differ from ours, especially\nsince they assume a one-to-one mapping from output feature\nto direction in activation space, their experiments suggest\nthat independent directions are almost orthogonal. This mo-\ntivates a deeper empirical examination of how orthogonal\nrefusal directions in language models interact in practice.\nAre orthogonal directions independent? To explore this,\nwe first use RDO to identify a direction r for Gemma 2 2B\nthat is orthogonal to the DIM direction v, i.e., r⊤v = 0. We\nthen measure how much one direction is influenced when ab-\nlating the other direction by monitoring the cosine similarity\ncos(λ, µ) =\nλ⊤µ\n||λ||·||µ|| between the prompt’s representation\nin the residual stream x and the directions v and r. Specifi-\ncally, we track: cos(r, x(l)\ni (pharm)) and cos(v, x(l)\ni (pharm))\nat the last token position and for all layers l ∈{0, . . . , L}\non 128 harmful instructions in our validation set. Intuitively,\nablating a causally independent direction in earlier layers\nshould not intervene with the reference direction in later\nlayers. Otherwise, there is some indirect influence through\nthe non-linear transformations of the neural network.\nFigure 6. Influence of representational independence. Figure (a)\nshows the cosine similarity between RDO⊥, a refusal direction\northogonal to DIM, and the model activations in a normal forward\npass (solid line) compared to a forward pass where DIM is ablated\n(striped line). Figure (b) shows the reverse scenario. In Figure (c)\nand (d) we contrast how the DIM direction and a representationally\nindependent direction (RepInd) influence each other.\nThe top row of Figure 6 shows how the cosine similarity be-\ntween the RDO and DIM directions changes under interven-\ntion. The left plot shows the cosine similarity between the\nRDO direction and the activations on a normal forward pass\n(solid line) and while ablating the DIM direction (dashed\nline). The right plot presents the reverse setting. Despite\nenforced orthogonality, ablating RDO indirectly reduces the\nrepresentation of the DIM direction in the model activations\nin the later layers, as measured by cosine similarity. This\neffect is reciprocal, suggesting that orthogonality alone does\nnot guarantee independence throughout the network.\nMotivated by this observation, we introduce a stricter notion\nof independence: Representational Independence (RepInd):\nDefinition 6.1. The directions λ, µ ∈Rd are representation-\nally independent (under directional ablation) with respect to\nthe activations x of a model in a set of layers l ∈L if:\n∀l ∈L : cos(x(l), λ) = cos(˜x(l)\nabl(µ), λ)\nand cos(x(l), µ) = cos(˜x(l)\nabl(λ), µ).\nThis means that, instead of relying solely on orthogonality,\nwe define two directions as representationally independent\nif ablating one has no effect on how much the other is repre-\nsented in the model activations. To enforce this property, we\nextend Algorithm 1 with an additional loss term that penal-\nizes changes in cosine similarity at the last token position\n7\n\nThe Geometry of Refusal in Large Language Models\nFigure 7. Performance of RepInd directions. Each direction is\nrepresentationally independent to all previous directions and the\nDIM direction.\nwhen ablating on harmful instructions:\nLRepInd = 1\n|L|\nX\nl∈L\nh\u0000cos(x(l), r) −cos(˜x(l)\nabl(v), r)\n\u00012\n+\n\u0000cos(x(l), v) −cos(˜x(l)\nabl(r), v)\n\u00012i\n.\nDo independent directions exist? With this extension, we\ncan find a direction that is RepInd from the DIM direction,\nyet still fulfills the refusal properties from Definition 4.1.\nWe show the representational independence in the second\nrow of Figure 6, where we see that the RepInd and DIM\ndirection barely affect each other’s representation under\ndirectional ablation.\nWe iteratively search for additional directions that are not\nonly RepInd to DIM but also of all previously identified\nRepInd directions. Despite these strong constraints, we suc-\ncessfully identify at least five such directions that maintain\nan ASR significantly above random vector intervention (Fig-\nure 7), as well as a refusal cone with RepInd basis vectors\n(Figure 12). However, in Figure 7 and Figure 12 perfor-\nmance degrades more rapidly compared to the results in\nSection 4 and Section 5. This decline could be attributed\nto the increased difficulty of the optimization problem due\nto additional constraints. Alternatively, it may suggest that\nGemma 2 2B possesses a limited number of directions that\nindependently contribute to refusal. If the latter is true, this\nimplies that the directions in the refusal cones exhibit non-\nlinear dependencies. Nevertheless, these results show that\nrefusal in LLMs is mediated by multiple independent mech-\nanisms, underpinning the idea that refusal behavior is more\nnuanced than previously assumed.\n6.2. Manipulation from input\nCan we access these directions from the input? Having\nfound several independent directions that are distinct from\nDIM, we investigate whether these directions can ever be\n”used” by the model, by checking if they are accessible from\nthe input or if they live in regions that no combination of\ninput tokens activates. To this end, we use GCG (Zou et al.,\n2023b) to train adversarial suffixes, which are extensions to\nthe prompts that aim to circumvent the safety alignment. In\naddition to the standard cross-entropy loss on an affirmative\ntarget, we add a loss term that incentivizes the suffix to\nablate RepInd 1.\nFigure 8. Representation\nof\nRepInd 1 in model activations\non harmful instructions before\nand after adversarial attacks with\nGCG.\nIn Figure 8, we show\nthe cosine similarities\nbetween RepInd 1 and\nthe model activations on\nboth harmful prompts\npharm from JAILBREAK-\nBENCH and the same\nprompts with adversarial\nsuffixes padv. We observe\nthat GCG is able to create\nsuffixes that significantly\nreduce how much RepInd\n1 is represented. These\nsuffixes successfully jail-\nbreak the model 36% of the time, which is similar to the\nASR of RepInd 1.\nKey Takeaways. We demonstrate the ability to\nidentify independent refusal directions, revealing\nthat these directions correspond to distinct underly-\ning concepts and can be directly accessed through\ninput manipulations. This further underscores the\nutility of our representational independence frame-\nwork, which provides a generalizable approach for\nanalyzing and understanding a wide range of repre-\nsentational interventions in LLMs.\n7. Limitations\nWhile our work provides new insights into the geometry\nof refusal in LLMs, some limitations remain. The refusal\ndirections we compute are all optimized on the same targets,\nwhich may limit their ability to capture fully distinct mecha-\nnisms. Extending our method to incorporate diverse targets\nor leveraging reinforcement learning with a judge-based\nreward function could help identify additional independent\nmechanisms (Geisler et al., 2025). Furthermore, while we\nestablish the existence of higher-dimensional refusal cones,\nwe cannot rule out the possibility of other yet-undiscovered\nregions in the model that mediate refusal.\n8. Conclusion\nThis work advances the understanding of refusal mecha-\nnisms in LLMs by introducing gradient-based representation\nengineering as a powerful tool for identifying and analyz-\ning refusal directions. Our method yields more effective\n8\n\nThe Geometry of Refusal in Large Language Models\nrefusal directions with fewer side effects, demonstrating its\nviability for extracting meaningful structures while allowing\nfor greater modeling flexibility. We establish that refusal\nbehaviors can be better understood via high-dimensional\npolyhedral cones in activation space rather than a single lin-\near direction, highlighting their complex spatial structures.\nAdditionally, we introduce representational independence\nand show that within this space of independent directions\nmultiple refusal directions exist and correspond to distinct\nmechanisms. Our gradient-based representation engineer-\ning approach can be extended to identify various concepts\nbeyond refusal by simply changing the optimization targets.\nThe generated findings provide new insights into the ge-\nometry of aligned LLMs, highlighting the importance of\nstructured, gradient-based approaches in LLM interpretabil-\nity and safety.\nAcknowledgements\nThis project was conducted in collaboration with and sup-\nported by funding from Google Research. We thank Do-\nminik Fuchsgruber and Leo Schwinn for feedback on an\nearly version of the manuscript.\nImpact Statement\nUnderstanding how refusal mechanisms in language models\nwork could potentially aid adversaries in developing more\neffective attacks. However, our research aims to deepen the\nunderstanding of refusal mechanisms to help the commu-\nnity develop more robust and reliable safety systems. By\nfocusing on open-source models requiring white-box access,\nour findings are primarily applicable to improving defensive\ncapabilities rather than compromising deployed systems.\nWe believe the positive impact of advancing model align-\nment and safety through better theoretical understanding\noutweighs the potential risks, making this research valuable\nto share with the research community.\nReferences\nArditi, A., Obeso, O., Syed, A., Paleka, D., Panickssery,\nN., Gurnee, W., and Nanda, N. Refusal in language\nmodels is mediated by a single direction, 2024. URL\nhttps://arxiv.org/abs/2406.11717.\nBelrose, N., Schneider-Joseph, D., Ravfogel, S., Cotterell,\nR., Raff, E., and Biderman, S. Leace: Perfect linear\nconcept erasure in closed form, 2023. URL https:\n//arxiv.org/abs/2306.03819.\nBolukbasi, T., Chang, K.-W., Zou, J., Saligrama, V., and\nKalai, A. Man is to computer programmer as woman is\nto homemaker? debiasing word embeddings, 2016. URL\nhttps://arxiv.org/abs/1607.06520.\nBurns, C., Ye, H., Klein, D., and Steinhardt, J. Discov-\nering latent knowledge in language models without su-\npervision, 2024. URL https://arxiv.org/abs/\n2212.03827.\nCarlini, N., Nasr, M., Choquette-Choo, C. A., Jagielski, M.,\nGao, I., Awadalla, A., Koh, P. W., Ippolito, D., Lee, K.,\nTramer, F., and Schmidt, L. Are aligned neural networks\nadversarially aligned?, 2024. URL https://arxiv.\norg/abs/2306.15447.\nChao, P., Debenedetti, E., Robey, A., Andriushchenko, M.,\nCroce, F., Sehwag, V., Dobriban, E., Flammarion, N.,\nPappas, G. J., Tram`er, F., Hassani, H., and Wong, E.\nJailbreakbench: An open robustness benchmark for jail-\nbreaking large language models. In NeurIPS Datasets\nand Benchmarks Track, 2024.\nChen, Z., Zhu, J., and Chen, A. Eliciting Offesnive Re-\nsponses from Large Language Models: A Genetic Algo-\nrithm. Springer, 2024.\nCunningham, H., Ewart, A., Riggs, L., Huben, R.,\nand Sharkey, L.\nSparse Autoencoders Find Highly\nInterpretable Features in Language Models, Octo-\nber 2023.\nURL http://arxiv.org/abs/2309.\n08600. arXiv:2309.08600 [cs].\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle,\nA., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan,\nA., et al. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783, 2024.\nFiotto-Kaufman, J., Loftus, A. R., Todd, E., Brinkmann,\nJ., Juang, C., Pal, K., Rager, C., Mueller, A., Marks, S.,\nSharma, A. S., et al. Nnsight and ndif: Democratizing\naccess to foundation model internals.\narXiv preprint\narXiv:2407.14561, 2024.\nGao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi,\nA., Foster, C., Golding, L., Hsu, J., Le Noac’h, A., Li,\nH., McDonell, K., Muennighoff, N., Ociepa, C., Phang,\nJ., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika,\nL., Tang, E., Thite, A., Wang, B., Wang, K., and Zou,\nA. A framework for few-shot language model evaluation,\n07 2024. URL https://zenodo.org/records/\n12608602.\nGeisler, S., Wollschl¨ager, T., Abdalla, M. H. I., Gasteiger,\nJ.,\nand G¨unnemann,\nS.\nAttacking Large Lan-\nguage Models with Projected Gradient Descent, Febru-\nary 2024.\nURL http://arxiv.org/abs/2402.\n09154. arXiv:2402.09154 [cs].\nGeisler, S., Wollschl¨ager, T., Abdalla, M. H. I., Gasteiger,\nJ., and G¨unnemann, S. Reinforce adversarial attacks on\nlarge language models: An adaptive, distributional, and\nsemantic objective, February 2025.\n9\n\nThe Geometry of Refusal in Large Language Models\nGurnee, W. and Tegmark, M. Language models represent\nspace and time. arXiv preprint arXiv:2310.02207, 2023.\nHeinzerling, B. and Inui, K. Monotonic representation of\nnumeric properties in language models. arXiv preprint\narXiv:2403.10381, 2024.\nLi, L., Dong, B., Wang, R., Hu, X., Zuo, W., Lin, D., Qiao,\nY., and Shao, J. Salad-bench: A hierarchical and com-\nprehensive safety benchmark for large language models.\narXiv preprint arXiv:2402.05044, 2024a.\nLi, T., Wang, Z., Liu, W., Wu, M., Dou, S., Lv, C., Wang,\nX., Zheng, X., and Huang, X. Revisiting jailbreaking for\nlarge language models: A representation engineering per-\nspective, 2024b. URL https://arxiv.org/abs/\n2401.06824.\nLin, S., Hilton, J., and Evans, O. Truthfulqa: Measuring\nhow models mimic human falsehoods. arXiv preprint\narXiv:2109.07958, 2021.\nLin, Z., Wang, Z., Tong, Y., Wang, Y., Guo, Y., Wang, Y.,\nand Shang, J. Toxicchat: Unveiling hidden challenges\nof toxicity detection in real-world user-ai conversation.\narXiv preprint arXiv:2310.17389, 2023.\nLiu, Y., Yao, Y., Ton, J.-F., Zhang, X., Cheng, R. G. H.,\nKlochkov, Y., Taufiq, M. F., and Li, H. Trustworthy llms:\nA survey and guideline for evaluating large language\nmodels’ alignment. arXiv preprint arXiv:2308.05374,\n2023.\nMarks, S. and Tegmark, M. The geometry of truth: Emer-\ngent linear structure in large language model represen-\ntations of true/false datasets, 2024. URL https://\narxiv.org/abs/2310.06824.\nNanda, N., Olah, C., Olsson, C., Elhage, N., and\nHume, T.\nAttribution patching: Activation patching\nat industrial scale.\nhttps://www.neelnanda.\nio/mechanistic-interpretability/\nattribution-patching,\n2024.\nAccessed:\n2025-01-10.\nO’Brien, K., Majercak, D., Fernandes, X., Edgar, R., Chen,\nJ., Nori, H., Carignan, D., Horvitz, E., and Poursabzi-\nSangde, F. Steering language model refusal with sparse\nautoencoders, 2024.\nURL https://arxiv.org/\nabs/2411.11296.\nOpenAI.\nIntroducing chatgpt, November 2022.\nURL\nhttps://openai.com/blog/chatgpt/.\nAc-\ncessed: 2025-01-26.\nPanickssery, N., Gabrieli, N., Schulz, J., Tong, M., Hubinger,\nE., and Turner, A. M. Steering Llama 2 via Contrastive\nActivation Addition, July 2024. URL http://arxiv.\norg/abs/2312.06681. arXiv:2312.06681 [cs].\nPark, K., Choe, Y. J., and Veitch, V. The linear represen-\ntation hypothesis and the geometry of large language\nmodels. arXiv preprint arXiv:2311.03658, 2023.\nPark, K., Choe, Y. J., and Veitch, V. The Linear Represen-\ntation Hypothesis and the Geometry of Large Language\nModels, July 2024. URL http://arxiv.org/abs/\n2311.03658. arXiv:2311.03658 [cs].\nRao, A., Vashistha, S., Naik, A., Aditya, S., and\nChoudhury, M.\nTricking LLMs into Disobedience:\nFormalizing,\nAnalyzing,\nand Detecting Jailbreaks,\nMarch 2024.\nURL http://arxiv.org/abs/\n2305.14965. arXiv:2305.14965 [cs].\nScholten, Y., G¨unnemann, S., and Schwinn, L. A proba-\nbilistic perspective on unlearning and alignment for large\nlanguage models. In The Thirteenth International Con-\nference on Learning Representations, 2025.\nSchwinn, L., Dobre, D., Xhonneux, S., Gidel, G., and\nG¨unnemann, S. Soft prompt threats: Attacking safety\nalignment and unlearning in open-source LLMs through\nthe embedding space. In The Thirty-eighth Annual Con-\nference on Neural Information Processing Systems, 2024.\nSchwinn, L., Scholten, Y., Wollschl¨ager, T., Xhonneux,\nS., Casper, S., G¨unnemann, S., and Gidel, G.\nAd-\nversarial alignment for llms requires simpler, repro-\nducible, and more measurable objectives. arXiv preprint\narXiv:2502.11910, 2025.\nShah, R., Feuillade-Montixi, Q., Pour, S., Tagade, A.,\nCasper, S., and Rando, J.\nScalable and Transferable\nBlack-Box Jailbreaks for Language Models via Persona\nModulation, November 2023. URL http://arxiv.\norg/abs/2311.03348. arXiv:2311.03348 [cs].\nSouly, A., Lu, Q., Bowen, D., Trinh, T., Hsieh, E., Pandey,\nS., Abbeel, P., Svegliato, J., Emmons, S., Watkins, O.,\nand Toyer, S. A strongreject for empty jailbreaks, 2024.\nStolfo, A., Balachandran, V., Yousefi, S., Horvitz, E., and\nNushi, B. Improving instruction-following in language\nmodels through activation steering, 2024. URL https:\n//arxiv.org/abs/2410.12877.\nSzegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan,\nD., Goodfellow, I., and Fergus, R. Intriguing properties of\nneural networks, 2014. URL https://arxiv.org/\nabs/1312.6199.\nTaori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li,\nX., Guestrin, C., Liang, P., and Hashimoto, T. B.\nStanford\nalpaca:\nAn\ninstruction-following\nllama\nmodel.\nhttps://github.com/tatsu-lab/\nstanford_alpaca, 2023.\n10\n\nThe Geometry of Refusal in Large Language Models\nTeam, G., Riviere, M., Pathak, S., Sessa, P. G., Hardin,\nC., Bhupatiraju, S., Hussenot, L., Mesnard, T., Shahri-\nari, B., Ram´e, A., et al. Gemma 2: Improving open\nlanguage models at a practical size.\narXiv preprint\narXiv:2408.00118, 2024.\nWang, K., Variengien, A., Conmy, A., Shlegeris, B., and\nSteinhardt, J. Interpretability in the Wild: a Circuit for\nIndirect Object Identification in GPT-2 small, Novem-\nber 2022.\nURL http://arxiv.org/abs/2211.\n00593. arXiv:2211.00593 [cs].\nWang, W., Tu, Z., Chen, C., Yuan, Y., Huang, J.-t., Jiao,\nW., and Lyu, M. R. All languages matter: On the multi-\nlingual safety of large language models. arXiv preprint\narXiv:2310.00905, 2023.\nWei, B., Huang, K., Huang, Y., Xie, T., Qi, X., Xia, M.,\nMittal, P., Wang, M., and Henderson, P. Assessing the\nbrittleness of safety alignment via pruning and low-rank\nmodifications, 2024.\nURL https://arxiv.org/\nabs/2402.05162.\nXhonneux, S., Sordoni, A., G¨unnemann, S., Gidel, G., and\nSchwinn, L. Efficient adversarial training in LLMs with\ncontinuous attacks. In The Thirty-eighth Annual Confer-\nence on Neural Information Processing Systems, 2024.\nYang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu,\nB., Li, C., Liu, D., Huang, F., Wei, H., et al. Qwen2. 5\ntechnical report. arXiv preprint arXiv:2412.15115, 2024.\nYu, L., Do, V., Hambardzumyan, K., and Cancedda, N.\nRobust llm safeguarding via refusal feature adversarial\ntraining. arXiv preprint arXiv:2409.20089, 2024.\nZheng, C., Yin, F., Zhou, H., Meng, F., Zhou, J.,\nChang, K.-W., Huang, M., and Peng, N.\nOn\nPrompt-Driven Safeguarding for Large Language Models,\nJune 2024. URL http://arxiv.org/abs/2401.\n18018. arXiv:2401.18018 [cs].\nZhu, S., Zhang, R., An, B., Wu, G., Barrow, J., Wang, Z.,\nHuang, F., Nenkova, A., and Sun, T. AutoDAN: Inter-\npretable Gradient-Based Adversarial Attacks on Large\nLanguage Models, December 2023.\nURL http://\narxiv.org/abs/2310.15140. arXiv:2310.15140\n[cs].\nZou, A., Phan, L., Chen, S., Campbell, J., Guo, P., Ren,\nR., Pan, A., Yin, X., Mazeika, M., Dombrowski, A.-\nK., Goel, S., Li, N., Byun, M. J., Wang, Z., Mallen,\nA., Basart, S., Koyejo, S., Song, D., Fredrikson, M.,\nKolter, J. Z., and Hendrycks, D. Representation Engineer-\ning: A Top-Down Approach to AI Transparency, Octo-\nber 2023a. URL http://arxiv.org/abs/2310.\n01405. arXiv:2310.01405 [cs].\nZou, A., Wang, Z., Kolter, J. Z., and Fredrikson, M. Uni-\nversal and Transferable Adversarial Attacks on Aligned\nLanguage Models, July 2023b. URL http://arxiv.\norg/abs/2307.15043. arXiv:2307.15043 [cs].\nZou, A., Phan, L., Wang, J., Duenas, D., Lin, M., An-\ndriushchenko, M., Kolter, J. Z., Fredrikson, M., and\nHendrycks, D. Improving alignment and robustness with\ncircuit breakers. In The Thirty-eighth Annual Conference\non Neural Information Processing Systems, 2024.\n11\n\nThe Geometry of Refusal in Large Language Models\nA. Setup Details\nA.1. Datasets\nWe construct our experimental dataset using harmful and harmless instructions from established benchmarks. For harmful\ninstructions, we draw from SALADBENCH (Li et al., 2024a), a comprehensive collection of adversarial prompts from\ndiverse sources. We exclude the Multilingual (Wang et al., 2023) and ToxicChat (Lin et al., 2023) sources since they are\nunsuited as harmful instructions. Afterwards, we sample up to 256 instructions from each remaining source. This results in\n1,184 instructions for training and 128 for validation. We sample equal numbers of harmless instructions from the ALPACA\ndataset, and additionally reserve 128 more harmless instructions for testing.\nA.2. Models\nWe exclusively use chat models for our experiments, but omit ”IT” and ”INSTRUCT” from model names. We use each chat\nmodel’s default chat template throughout our analysis.\nTable 2. Model families, sizes, and references.\nModel family\nSizes\nReference\nQWEN2.5 INSTRUCT\n1.5B, 3B, 7B, 14B\nYang et al. (2024)\nGEMMA 2 IT\n2B, 9B\nTeam et al. (2024)\nLLAMA-3 INSTRUCT\n8B\nDubey et al. (2024)\nA.3. Hyperparameters and Implementation\nTable 3. Hyperparameters for all algorithms\nComponent\nParameter\nValue\nTraining\nTotal Batch Size\n16\nGradient Accumulation Steps\n16\nBase Learning Rate\n0.01\nLearning Rate Reduction\nEvery 5 batches if plateaued\nLearning Rate Factor\nDivide by 1/10 up to 2 times\nOptimizer\nAdamW\nWeight Decay\n0\nMain Loss\nAblation Loss Weight λabl\n1.0\nAddition Loss Weight λadd\n0.2\nRetain Loss Weight λret\n1.0\nMonte Carlo Sampling\nSamples per Accumulation Step\n16\nEffective Samples per Batch\n256\nRepInd\nRepInd Loss Weight λind\n200\nLayer Cutoff\n0.9\nTable 3 presents the hyperparameters used in our algorithms. Since our method converges before completing a full epoch,\nwe do not utilize validation scores during training. Instead, after convergence, we apply the direction selection algorithm\nfrom Arditi et al. (2024) to identify the optimal refusal direction from the last 20 training steps.\nImplementation and Evaluation Framework. All algorithms and exploratory experiments are implemented using the\nNNsight (Fiotto-Kaufman et al., 2024) library. Additionally, we use the LM Evaluation Harness (Gao et al., 2024) to run\nTruthfulQA (Lin et al., 2021) with default settings, with the exception that we enable the use of each model’s default chat\ntemplates.\nRetain and Representational Independence Loss Computation. The retain loss is computed as the KL divergence\nbetween the probability distributions derived from the logits of the model with and without directional ablation, masked\n12\n\nThe Geometry of Refusal in Large Language Models\nFigure 9. Refusal scores of different models on harmless instructions after activation addition that aims to induce refusal.\nover a target response and the last token of the chat template. The resulting value is then averaged across tokens. For a\nsingle instruction psafe with its target tretain, we formalize the loss as follows:\nLretain = KL(fablate(r)(psafe), f(psafe), tretain) = 1\n|I|\nX\ni∈I\nX\nt∈T\nf(psafe + tretain)i,t log\nf(psafe + tretain)i,t\nfablate(psafe + tretain)i,t\n,\nwhere I contains the target token indexes and the last instruction token’s index, the subscript i, t denotes the model output at\nsequence position i and vocabulary index t as defined in Section 2, and + denotes concatenation.\nFor the implementation of the representational independence loss, LRepInd, we compute the average loss over the tokens\nin the harmful instructions pharm. The RepInd loss is computed over the first 90% of layers, as applying it too close to the\nunembedding layer overly constrains the model’s output.\nSelection of Refusal and Independent Directions In Algorithm 1, after training the refusal directions to convergence, we\nagain use the direction selection algorithm from Arditi et al. (2024) to identify the most effective directions from the final 20\ntraining steps.\nIn Section 5, we extend this selection process to determine a basis where all basis vectors effectively mediate refusal (from\nthe last 20 bases of the training). If no such basis exists, we instead select the basis where the samples are most effective for\ndirectional ablation using the refusal score heuristic from the selection algorithm.\nTraining Procedure for Representational Independence Directions In Section 6, our approach to training and validating\nrepresentationally independent (RepInd) directions differs because of high variance between different runs. For each RepInd\ndirection, we train five candidate vectors and select the one with the lowest refusal score on our validation set. This process\nis repeated five times, ultimately producing our final set of RepInd directions. The RepInd loss is computed as the sum of\nlosses over all vectors that the current vector should remain independent of.\nB. Additional Experiments\nIn this section, we present further experimental results. Figure 9 demonstrates that adding the refusal direction successfully\ninduces refusal behavior across all models for both DIM and RDO. Similarly, Figure 10 illustrates the refusal cone\nperformance for Gemma 2, confirming the existence of higher-dimensional refusal cones within the Gemma 2 family. The\nresults suggest that the maximum cone dimensionality may be four, as the lower bounds of the ASR drop sharply beyond\nthis point. In Figure 11 we apply refusal cones to various Qwen 2.5 models across different dimensions, revealing that\ninducing refusal is significantly easier than conducting an attack. Notably, most directions even in high-dimensional cones\nremain effective at inducing refusal responses.\nFigure 13 examines the attack success rate when sampling multiple vectors from various N-dimensional refusal cones\nand selecting the best-performing sample per prompt for Gemma 2, 2B. We observe that ASR improves with increasing\ncone dimensionality but plateaus at four dimensions, suggesting that higher-dimensional cones provide an advantage over\nsingle-direction manipulation by capturing complementary mechanisms. The plateau likely results from the model’s inability\n13\n\nThe Geometry of Refusal in Large Language Models\nFigure 10. Attack success rates in refusal cones of different dimensions for the Gemma 2 model family. We observe that for the Gemma 2\n2B the lower bounds start to degrade significantly for dimension 5.\nFigure 11. Using refusal cones to induce refusal across various Qwen 2.5 models with different dimensions. We observe that inducing\nrefusal is generally easier than executing an attack. In this setting, nearly all dimensions maintain strong performance in eliciting refusal\nresponses, even for benign requests.\n14\n\nThe Geometry of Refusal in Large Language Models\nFigure 12. Attack success rates in refusal cones of different dimensions for Gemma 2 2B where the basis vectors are trained to be\nrepresentationally independent.\nto encode higher-dimensional refusal cones, a hypothesis further supported by Figure 10.\nMoving on to the ablation study, Figure 15 presents an analysis of the relationship between the retain loss weight and model\nperformance on the Qwen 2.5 3B model. The left plot illustrates the performance under directional ablation and activation\nsubtraction, with results averaged over five runs per loss weight. For this model, a loss weight of 1 or lower yields the\nbest generalization, while increasing the penalty for unintended side effects on harmless instructions significantly degrades\nperformance. On the right, we examine how the retain loss weights generalize to the validation KL score. This allows us to\nabstract from specific training conditions and evaluate how effectively the loss weights transfer beyond the training setup.\nFinally, we assess the performance of the baseline across different (layer, token) combinations. Figure 16 visualizes the\neffectiveness of the direction selection algorithm from Arditi et al. (2024) for DIM directions in the Qwen 2.5 7B model.\nAmong the evaluated token and layer pairs, only one direction is found to be effective for both inducing refusal through\nactivation addition and maintaining low side effects. Transparent data points indicate (layer, token) combinations that were\nfiltered out due to their inability to induce refusal reliably. Additionally, the red line represents the KL-divergence threshold,\nused to estimate potential side effects of directional ablation on harmless instructions.\n15\n\nThe Geometry of Refusal in Large Language Models\nFigure 13. Attack success rates when sampling vectors from the N-dimensional refusal cones and selecting the best-performing sample\nper prompt for Gemma 2 2B. ASR increases with cone dimensionality but plateaus at four dimensions, suggesting that higher-dimensional\ncones provide an advantage over single-direction manipulation by capturing complementary mechanisms. The plateau likely arises\nbecause Algorithm 2 cannot find an additional basis vector that preserves the refusal properties in the cone, suggesting that the model does\nnot support a cone of this dimension. Figure 10 also provides evidence for this claim.\nFigure 14. Refusal scores of refusal vectors sampled from Gemma 2 2B refusal cones compared to the DIM direction when scaling the\nnorm of the added direction α for the activation addition intervention. The refusal score is the heuristic from Arditi et al. (2024) here, and\nwe compute it on 64 harmful validation instructions, with mean and standard deviation over 64 samples per alpha.\n16\n\nThe Geometry of Refusal in Large Language Models\nFigure 15. The left plot shows the relationship between the retain loss weight and performance when using the trained direction for\ndirectional ablation and activation subtraction on the Qwen 2.5 3B model, with mean and standard deviation over 5 runs per loss weight.\nFor this specific model, a loss weight of 1 or smaller results in the best generalization, and performance degrades significantly as the\ndirection is penalized more for unintended side-effects on harmless instructions. On the right, we also show performance depending on\nhow the directions generalized to the validation KL-score.\nFigure 16. Analysis of the selection direction algorithm from Arditi et al. (2024) for the DIM directions of Qwen 2.5 7B. Among the\ntoken and layer combinations, only a single direction is identified as viable for both inducing refusal via activation addition and having\nlow side-effects. Transparent points represent (layer, token) pairs that are filtered out because of ineffectiveness in inducing refusal. The\nred line indicates the KL-divergence threshold used to estimate potential side-effects of directional ablation on harmless instructions.\n17\n\nThe Geometry of Refusal in Large Language Models\nC. Assets\nIn the following, we show the licenses for all the assets we used in this work: different models from Table 4 and the datasets\nthat we use for evaluation and training; see Table 5.\nC.1. Models\nTable 4. The list of models used in this work.\nModel\nSource\nAccessed via\nLicense\nQwen 2.5 {1.5B, 7B, 14B}\nYang et al. (2024)\nLink\nApache 2.0 License\nQwen 2.5 {3B}\nYang et al. (2024)\nLink\nQwen Research License\nGemma 2 2B\nTeam et al. (2024)\nLink\nApache 2.0 License\nGemma 2 9B\nTeam et al. (2024)\nLink\nGemma Terms of Use\nLlama-3 8B\nDubey et al. (2024)\nLink\nMeta Llama 3 Community License\nStrongREJECT Judge\nSouly et al. (2024)\nLink\nMIT License\nC.2. Datasets\nTable 5. The list of datasets used in this work.\nDataset\nSource\nAccessed via\nLicense\nSALADBENCH\nLi et al. (2024a)\nLink\nApache License 2.0\nALPACA\nTaori et al. (2023)\nLink\nApache License 2.0\nJAILBREAKBENCH\nChao et al. (2024)\nLink\nMIT License\nTRUTHFULQA\nLin et al. (2021)\nLink\nApache License 2.0\n18\n",
  "metadata": {
    "source_path": "papers/arxiv/The_Geometry_of_Refusal_in_Large_Language_Models_Concept_Cones_and\n__Representational_Independence_e160ea09383777b1.pdf",
    "content_hash": "e160ea09383777b140de849568c4b74c4e80b9d21c3801cdbc6ea3690d0cef02",
    "arxiv_id": null,
    "title": "The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence",
    "author": ", Tom Wollschläger, Jannes Elstner, Simon Geisler, Vincent Cohen-Addad,   , Stephan Günnemann, Johannes Gasteiger",
    "creation_date": "D:20250225030858Z",
    "published": "2025-02-25T03:08:58",
    "pages": 18,
    "size": 2807004,
    "file_mtime": 1740470085.532491
  }
}