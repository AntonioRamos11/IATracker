{
  "text": "DBudgetKV: Dynamic Budget in KV Cache Compression\nfor Ensuring Optimal Performance\nXuanfan Ni*\nLiyan Xu*‡\nChenyang Lyu Longyue Wang\nMo Yu‡ Lemao Liu‡ Fandong Meng‡ Jie Zhou‡ Piji Li†\n‡Pattern Recognition Center, WeChat AI\nxuanfanni@gmail.com\nliyanlxu@tencent.com\nlipiji.pz@gmail.com\nAbstract\nTo alleviate memory burden during inference\nof large language models (LLMs), numerous\nstudies have focused on compressing the KV\ncache by exploring aspects such as attention\nsparsity. However, these techniques often re-\nquire a pre-defined cache budget; as the opti-\nmal budget varies with different input lengths\nand task types, it limits their practical deploy-\nment accepting open-domain instructions. To\naddress this limitation, we propose a new KV\ncache compression objective: to always ensure\nthe full-cache performance regardless of spe-\ncific inputs, while maximizing KV cache prun-\ning as much as possible. To achieve this goal,\nwe introduce a novel KV cache compression\nmethod dubbed DBudgetKV, which features\nan attention-based metric to signal when the\nremaining KV cache is unlikely to match the\nfull-cache performance, then halting the prun-\ning process. Empirical evaluation spanning di-\nverse context lengths, task types, and model\nsizes suggests that our method achieves loss-\nless KV pruning effectively and robustly, ex-\nceeding 25% compression ratio on average.\nFurthermore, our method is easy to integrate\nwithin LLM inference, not only optimizing\nmemory space, but also showing reduced in-\nference time compared to existing methods.\n1\nIntroduction\nAs most large language models (LLMs) follows\nautoregressive generation (Minaee et al., 2024;\nChang et al., 2023; OpenAI, 2023), they rely on\nKV Cache to store intermediate states during in-\nference. Specifically, LLMs access the cached key\nand value vectors of past tokens during the atten-\ntion calculation, thereby speeding up inference by\nre-using these KV cache (Vaswani et al., 2017;\nAinslie et al., 2023; Shazeer, 2019).\nHowever,\n*Equal contributions. Partial work was done during Xu-\nanfan’s internship at WeChat AI.\n†Corresponding author.\nas the model size and the input length increase,\nthe memory required to maintain the KV cache\nalso grows proportionally. Since these vectors are\nusually stored within GPU memory, managing the\nKV cache efficiently has become crucial to mit-\nigate the overall memory consumption and com-\nputational overhead during LLM inference. For\ninstance, Llama3 8B model (Dubey et al., 2024)\nrequires 1 GB of KV cache memory for 2K-token\ninputs, while its 70B counterpart demands a gigan-\ntic memory up to 50GB for 20K tokens.\nConsequently, recent works have proposed to\neffectively reduce KV cache through pruning,\nleveraging the sparsity of the attention mecha-\nnism: certain positions are more pivotal dur-\ning the generation process, while those less im-\nportant ones could be pruned with minimal per-\nformance degradation; thus, the full KV cache is\nnot always necessary.\nThese observations have\nled to several optimization methods, such as H2O\n(Zhang et al., 2023), ScissorHands (Liu et al.,\n2023), StreamingLLM (Xiao et al., 2024), and\nFastGen (Ge et al., 2024), where they aim to iden-\ntify and retain only the most salient token posi-\ntions, discarding less critical ones based on cer-\ntain pruning criteria. Alongside, other orthogonal\nparadigms have also been proposed without hard-\npruning, such as KVMerger (Wang et al., 2024)\nand D2O (Wan et al., 2024) that merge KV vec-\ntors, and MLA (DeepSeek-AI et al., 2024) that op-\nerates attention in the latent space.\nWhile previous pruning methods have success-\nfully demonstrated that a decent size of KV cache\ncould be discarded in practice, there exists one\ncommon limitation that has not been addressed\nyet: these pruning techniques typically require a\npre-defined KV cache budget, of which its op-\ntimal threshold could vary significantly accord-\ning to specific tasks or inputs.\nHence, in real-\nworld scenarios, one would have to carefully tune\nmany budget thresholds for diverse domains, or\narXiv:2502.16886v1  [cs.CL]  24 Feb 2025\n\nset a uniform threshold but likely suffering large\ndegradation on certain tasks. We reckon that nei-\nther choice is a satisfactory solution, hindering the\nwidespread adoptation of KV pruning techniques.\nTo further illustrate, Table 1 shows that when\nthe budget is set to 50%, Llama3-8B-Instruct us-\ning H2O achieves over 98% of the full-cache\nperformance on NarrativeQA (Kociský et al.,\n2018), whereas under the same conditions on\nGSM8K (Cobbe et al., 2021), a widely-used math\nbenchmark, the performance drastically falls to\nless than 42% of the full-cache performance.\nTo address the aforementioned limitation, in\nthis work, we introduce a new objective for KV\ncache compression: our method aims to strike to-\nwards full-cache performance regardless of spe-\ncific inputs, while maximizing KV cache prun-\ning as much as possible.\nTowards this objec-\ntive, we propose an adaptive pruning method,\ndubbed DBudgetKV, which Dynamically adjusts\nKV cache retention per input, without the need of\npre-defined memory Budget. Particularly, DBud-\ngetKV automatically identifies input-dependent\nKV cache pruning that leads to minimal perfor-\nmance degradation. A key implication is that the\nmethod tends to realize a higher compression ra-\ntio for simpler tasks, while allocating more cache\nresources for complicated tasks.\nOur method features a two-step process: 1) after\nthe prefilling stage, rank all input tokens according\nto an importance metric; 2) discard the KV cache\nof each token sequentially, until hitting a stopping\ncriterion. In choosing the importance metric and\nthe stopping criterion, DBudgetKV takes into ac-\ncount two important factors from previous works:\ni) position bias, where tokens at the beginning and\nthe end are generally more significant than those in\nthe middle (Xiao et al., 2024; Jiang et al., 2024b);\nii) attention score, where tokens receiving higher\nscores usually contribute more information to sub-\nsequent generation (Minaee et al., 2024; Chang\net al., 2023; Zhang et al., 2023).\nWe then propose DBudgetKV of empirical ef-\nficacy and low overhead. Concretely, tokens are\nranked solely by their positions, with no addi-\ntional computation involved. The ranking strat-\negy signals high significance to the beginning and\nthe end positions, where the initial tokens are the\nmost important, followed by the remaining tokens\nin a reversing order. Next in the pruning process,\nwe design an attetion-based metric to determine\nthe stopping condition, such that the pruning halts\nwhen this metric signals that the remaining KV\ncache is unlikely to match the full-cache capacity,\nthereby fulfilling our objective to approximate full\nperformance with dynamic cache pruning.\nMore specifically, our attention metric com-\npares the difference between the Frobenius form\nof the full attention matrix and the reduced matrix\nby setting pruned positions’ score to zero. When\nthe difference exceeds a universal threshold, the\nprocess is halted. Especially, this metric empir-\nically correlates well with the generation perfor-\nmance regardless of inputs, serving as an estima-\ntor to bridge the pruning state and the subsequent\ngeneration. Furthermore, the entire process is im-\nplemented efficiently through PyTorch’s built-in\noperators, where we show that DBudgetKV not\nonly optimizes memory space, but also acheives\ntime reduction compared to the full-cache infer-\nence and three KV pruning methods.\nExperimental results on 13 datasets varying di-\nverse context lengths and tasks, e.g. mathemati-\ncal and commonsense reasoning, reading compre-\nhension and coding, demonstrate that DBudgetKV\nachieves our objective effectively and robustly.\nThe resulting inference is on par or even surpasses\nthe full-cache performance with multiple LLMs of\ndifferent sizes, including Llama3 (Dubey et al.,\n2024), Qwen2.5 (Yang et al., 2024), and Mis-\ntral (Jiang et al., 2024a). Meanwhile, the dynamic\npruning of DBudgetKV allocates relatively high\nbudget on math benchmarks but attains high com-\npression ratio by up to 85% on comprehension\ntasks. The average budget size reaches 63.7% us-\ning Llama3-8B. The overall results suggest that\nDBudgetKV is able to accomplish lossless KV\ncache pruning with strong generalizability.\nBe-\nsides, it does not rely on specific model architec-\ntures and can coexist with other KV cache opti-\nmization techniques. We further conduct more ex-\nperiments for in-depth analysis and ablation stud-\nies, depicting the rationality of our various design\nchoices and hyperparameters.\nOur contributions can be summarized below:\n• To the best of our knowledge, we are the first\nto propose the objective that ensures full-cache\nperformance while maximizing the KV cache\npruning, which holds greater practical values\nand deployment potentials.\n• We introduce DBudgetKV that employs a\nstraightforward two-step process, where its dy-\nnamic pruning is designed with minimal degra-\ndation to the generation performance.\n\nLayer 0\nPrompt Encoding\nLayer 2\nLayer 31\n×\n...\nPosition-based \nToken Importance\nAttention Matrix\nSet the \nValue to 0\nThreshold\nCompressed KV Cache\nEvict the Least Important Token\nOriginal KV Cache\nReduced Matrix\nModel Inference\nLayer 1\nFigure 1: The overall workflow of DBudgetKV in Section 3.2. Initially, tokens are ranked based on their positions,\nfollowed by the eviction of the least significant tokens (per layer), whose halting condition is determined by the\nnorm value of the reduced attention matrix. The KV cache for the remaining tokens are then preserved.\n• Extensive evaluation with diverse tasks and\nmodels indicates that compared to prior works,\nour method achieves lossless KV cache com-\npression effectively, efficiently and robustly.\n2\nRelated Work\nKV Cache Compression\nRecent advancements\nin KV cache compression have emerged to address\nmemory constraints for model inference.\nScis-\nsorhands (Liu et al., 2023) observes the repeti-\ntive nature of attention patterns and the persis-\ntence of token importance. It then preserves the\nmost important tokens based on attention scores\nto achieve KV cache compression. H2O (Zhang\net al., 2023) introduces dynamic eviction poli-\ncies that strategically balance retention of re-\ncent tokens and historically significant \"heavy hit-\nters\". StreamingLLM (Xiao et al., 2024) enables\ninfinite-length sequence processing without fine-\ntuning by stabilizing attention through landmark\ntokens. SnapKV (Li et al., 2024) enhances com-\npression efficiency through attention score-based\nselection and clustering of critical KV positions.\nFastGen (Ge et al., 2024) proposes an adaptive KV\ncache management system that customizes reten-\ntion strategies per attention head.\nHowever, these methods require setting a fixed\nmemory budget, which makes it difficult to con-\nsistently maintain full performance across differ-\nent domains and datasets, limiting their practical\napplication. In contrast, our method imposes no\nsuch constraints, targeting full-cache performance\nwhile allowing for dynamic budget size.\n3\nMethodology\nWe first elaborate our objective in Sec. 3.1 as a\nnew direction for KV cache compression. Distinct\nfrom previous works, we aim to compress the KV\ncache as much as possible while ensuring LLMs’\nperformance remains intact. We then delineate our\nproposed approach along with its motivation be-\nhind in Sec. 3.2. Important implementation details\nare next discussed in Sec. 3.3.\n3.1\nObjective of KV Cache Compression\nAs mentioned in Sec. 1, most previous works on\nKV cache pruning require a pre-defined memory\nbudget beforehand, either by fixing the number\nof cached positions directly (Cai et al., 2024; Li\net al., 2024), or retaining positions by a fixed ra-\ntio of input length (Zhang et al., 2023; Wan et al.,\n2024). While these approaches are effective in cer-\ntain circumstances, they may encounter practical\nchallenges when deployed in real-world scenarios.\nThe biggest issue is that the optimal budgets\nfor LLM inference in real-world scenarios are ev-\nidently infeasible to enumerate, as they could vary\nacross tasks and domains, especially for open-\ndomain instructions. For example, mathematical\ntasks are typically concise and require inference\nbased on all given conditions, thus many positions\nmay logically contribute to the generation, neces-\nsitating a relatively higher memory budget. Con-\nversely, for article reading comprehension, LLM\nmay only need a small set of critical positions for\nthe answer generation, featuring a smaller budget\nin many cases.\n\nAs a side effect, inference with previous KV\ncache pruning is likely to experience performance\ninstability across different inputs, which can be\nobserved by our analysis in Sec. 4.2. Overall, the\npractical value is thereby diminished.\nIn this work, we resort to a dynamic paradigm\nthat eliminates the need for manually setting a\nmemory budget. We introduce a new objective for\nKV cache compression, aiming to automatically\nreduce the KV cache as much as possible, while\nfulfilling the condition that the method should al-\nways strike towards full-cache performance. In\nthis way, one could utilize such pruning method\noff-the-shelf, without spending time and resources\non tuning usage-specific hyperparameters, which\nwe hope would further advance the research de-\nvelopment of KV cache compression techniques.\n3.2\nDBudgetKV\nTowards our objective, we design a novel ranking-\nbased pruning method applied after the prefilling\nstage, consisting of two steps.\nFirst, all tokens\nare ranked according to an importance metric per\nTransformers layer. Then, the system keeps re-\nmoving the KV cache sequentially, until hitting a\nstopping criterion. Ultimately, our proposed ap-\nproach does not need to fix an optimal budget for\nstoring KV cache. Rather, the stopping criterion\nis tied to the norm of attention matrices that works\nuniformly regardless of the inputs.\nThe motivation behind our two-step procedure\ncomes from two phenomena brought by previous\nworks. First, there exists position bias such that\npositions in the beginning and the end are typi-\ncally important for the subsequent generation, as\nobserved by StreamingLLM (Xiao et al., 2024).\nSecond, attention scores are usually an effective\nindicator of the token importance, as leveraged by\nmany pruning techniques (Liu et al., 2023; Zhang\net al., 2023; Li et al., 2024). For our objective, we\npropose to combine the two important properties\nin our approach, described in details as follows.\nImportance Ranking\nDenote a LLM input se-\nquence as X = {x1, x2, . . . , xn}, where each\nTransformers layer originally consists of n posi-\ntions of KV vectors per attention head. As the\npruning process starts from the least important KV\nvectors, all tokens are firstly ranked according to\nan importance strategy.\nTo this end, we resort to a simple ranking strat-\negy leveraging the position bias, without under-\ngoing any additional computation. As identified\nby StreamingLLM, a significant portion of atten-\ntions is allocated to the initial tokens, regardless\nof their relevance to the language modeling task,\nknown as attention sinks.\nInspired by this, we\nemploy a purely position-based importance eval-\nuation strategy: we assume that the first m to-\nkens are always crucial; while among the remain-\ning n −m tokens, those towards the end are\nmore significant.\nBy this strategy, the impor-\ntance in X is ranked from the highest to the low-\nest as {x1, x2, . . . , xm, xn, xn−1, . . . , xm+1}, de-\nnoted as b\nX, where m is a chosen hyperparameter\nthat works regardless of specific input sequences.\nIt should be noted that our choice on impor-\ntance ranking is empirically supported rather than\ntheoretically based. We have experimented other\nattention-based importance ranking, which are\nprovided in Sec. 4.4. Our position-based strategy\nis shown both empirically effective and superior\non computational overhead.\nStopping Criterion\nWith the initially ranked\nKV vectors, our method then sequantially decides\nwhether to continue or terminate pruning, which\nneeds an evaluation metric serving as the stopping\ncriterion. Since we evidently cannot know the pre-\ncise impact of a KV cache position before the gen-\neration, we require a metric that correlates well\nwith the resulting performance change when dis-\ncarding a position, serving as a bridge to ensure\na minimal degradation upon the full KV-cache\nperformance.\nInspired by Devoto et al. (2024), we utilize the\nFrobenius norm of the attention matrix that fits the\nrequirement well empirically. For a matrix M ∈\nRm×n, the Frobenius norm of M is the matrix-\nversion of the L2 norm, expressed as ||M||F =\nqPm\ni=1\nPn\nj=1 |Mi,j|2.\nWe conduct preliminary experiments to validate\nits reliability.\nWe employ Llama2-7B and ran-\ndomly mask a certain portion of the KV cache,\nsetting the corresponding values in the attention\nmatrix A ∈Rn×n to zero. As illustrated in Fig-\nure 2, there is a certain degree of positive correla-\ntion between its Frobenius norm and the model’s\nperformance, which are shown generalized across\ndifferent domains.\nAs the input sequence length n increases, the\ntime and space overhead for norm calculation also\ngrows significantly by O(n2). Hence, we next aim\nto reduce the computational scale from the naive\n\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\nThreshold (%)\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nAccuracy\nLlama-2-7B-Chat on GSM8K\nLast Row of Attention Matrix\nAttention Matrix\nFull-cache Performance\nFigure 2: Preliminary experiments on the relationship\nbetween the Frobenius norm difference (in percentage)\nobtained from randomly masking tokens, and the cor-\nresponding model performance on GSM8K.\nnorm calculation. We observe that the Frobenius\nnorm of the last row of A also correlates with\nthe model’s performance; equivalently saying, the\nattention distribution of the latest token is good\nenough to serve as an estimator. Formally, we uti-\nlize the last k rows (latest k tokens) of A and re-\nduce to a single attention vector A′ ∈R1×n. The\nscore sj for a position j ∈[1, n] in A′ is:\nsj =\nPn\ni=k Ai,j\nPn\ni=k 1{Ai,j̸=0}\n(1)\nWe then use the Frobenius norm of A′, equiva-\nlently its L2 norm, as the evaluation metric, which\nreduces the overall complexity to O(n).\nFinally, for each position in the ranking order,\nour method compares the difference between the\nFrobenius norm of the original attention matrix\nA, and the Frobenius norm of the reduced atten-\ntion vector A′ of which the corresponding posi-\ntions are set to 0. We set a universal threshold\nas 1%, such that when the new norm exceeds 1%\ndifference from the original norm, it hits the stop-\nping criterion. The KV vectors of the remaining\npositions are then all kept, while the preceding po-\nsitions would have their KV cache discarded. In\nthis way, our method achieves dynamic KV cache\npruning while targeting to maintain the full-cache\nperformance, assisted by the norm metric.\n3.3\nImplemetation Details\nThough our proposed pruning process is concep-\ntually sequential, it is implemented efficiently by\nPyTorch’s operators, such that the stopping posi-\ntions of all layers are identified directly in parallel\nwithout any looping operations. Empirical anal-\nysis in Sec. 4.5 shows that our proposed method\nachieves higher efficiency compared to three pop-\nular baselines.\nSpecifically, we reorder A′ based on Token\nImportance Ranking b\nX to obtain A′\nsort, where\nA′\nsort[0] corresponds to the least important token\nand A′\nsort[−1] corresponds to the most important\ntoken. Next, we compute the cumulative square-\nsum of each element in A′\nsort:\nA′\ncumsum[i] =\nv\nu\nu\nt\nn\nX\nk=i\n|A′\nsort[k]|2\n(2)\nsuch that A′\ncumsum[i] represents the Frobenius\nNorm of the attention matrix after removing all\ntokens to the left of ith token.\nWe then divide\neach element in A′\ncumsum by ||A′||F and subtract\nthis value from 1 to determine the difference be-\ntween each new matrix and A′. The torch where\noperation allows us to directly identify the posi-\ntion of the token that satisfies the condition of not\nexceeding a 1% difference, ultimately yielding the\nset of tokens for which the KV cache is called to\nretain. The final kv cache compression algorithm\nof DBudgetKV is presented in Algorithm 1. Since\nPyTorch allows specifying the dimension for oper-\nations, DBudgetKV can run in parallel across mul-\ntiple attention heads.\nRetaining Bottom Layers\nAdditionally,\nwe\nhave discovered that applying DBudgetKV to the\nmodel’s 0th and 1st layers results in poor perfor-\nmance when the model operates under a high bud-\nget. We hypothesize that this may be due to the\nrelatively uniform distribution of attention scores\nin these initial layers, leading to the premature dis-\ncarding of tokens that are crucial for subsequent\nlayers, ultimately causing the entire model’s in-\nference to collapse. A more detailed analysis of\nthis issue is provided in Appendix B. Based on this\nfinding, we initiate the KV cache compression op-\nerations starting from the model’s 2nd layer.\n4\nExperiments\n4.1\nExperimental Settings\nBackbones\nOur experiments are conducted with\nthree LLMs: Llama3-Instruct with size of 8B/70B,\nMistral-7B-Instruct-V0.3, and Qwen2.5-Instruct\nwith size of 7B/32B/72B. We implement DBud-\ngetKV upon the codebase of SnapKV1. For the re-\nduced attention matrix A′, we set k = 1 in practice\n(ablation provided in Sec. 4.4), and set m = 4 for\nimportance ranking.\n1https://github.com/FasterDecoding/SnapKV\n\nMethods\nMath & Science\nCR\nSingle-Doc QA\nMulti-Doc QA\nSummarization\nFSL\nCode\nAvg.\nGSM8K\nGPQA\nTheoQA\nThQA\nCoQA\nNrtvQA\nQasper\n2WkMQA\nMusique\nQMSum\nM-News\nTriviaQA\nLcc\nLlama3-8B-Instruct\nFull\n75.28\n29.02\n21.29\n25.59\n52.74\n24.06\n43.91\n35.33\n14.77\n22.27\n27.37\n70.26\n19.16\n100%\nOursk=1\n76.50\n30.13\n23.03\n26.09\n52.86\n23.44\n37.38\n36.21\n15.96\n21.95\n27.88\n64.24\n19.31\n+0.12%\nBudget\n93.2%\n86.7%\n92.8%\n78.7%\n81.5%\n48.7%\n46.4%\n45.8%\n43.7%\n15.0%\n76.4%\n41.0%\n78.0%\n63.68%\nH2O0.9\n74.60\n28.13\n21.69\n25.41\n52.83\n24.55\n41.73\n33.95\n15.23\n22.51\n27.64\n69.77\n19.13\n-0.39%\nSLM0.9\n72.78\n28.79\n20.75\n25.15\n52.83\n23.75\n43.63\n32.68\n15.86\n22.48\n27.21\n69.97\n19.66\n-0.59%\nSnapKV0.9\n70.20\n27.90\n20.08\n25.38\n52.72\n24.04\n43.93\n33.92\n15.56\n22.49\n27.65\n70.18\n19.05\n-1.12%\nH2O0.5\n31.08\n2.90\n16.47\n17.99\n42.37\n23.14\n43.05\n32.79\n15.77\n22.79\n26.24\n69.83\n19.18\n-17.63%\nSLM0.5\n3.41\n6.03\n8.84\n18.75\n44.68\n20.94\n37.73\n31.20\n15.29\n21.80\n25.93\n67.79\n19.12\n-24.73%\nSnapKV0.5\n12.96\n7.81\n16.87\n19.52\n42.84\n23.40\n43.93\n33.98\n15.94\n22.67\n26.07\n69.66\n19.19\n-17.03%\nMistral-7B-Instruct\nFull\n33.36\n29.24\n6.83\n20.82\n39.68\n28.74\n37.80\n33.87\n22.88\n22.19\n22.94\n86.87\n16.08\n100%\nOursk=1\n31.54\n29.02\n6.71\n20.81\n39.80\n27.20\n38.93\n33.46\n22.15\n22.39\n22.67\n86.81\n15.33\n-1.50%\nBudget\n89.6%\n90.5%\n84.2%\n92.3%\n84.1%\n78.0%\n97.9%\n86.7%\n74.4%\n84.3%\n89.1%\n87.2%\n89.4%\n86.75%\nH2O0.9\n19.18\n24.11\n6.96\n20.61\n39.74\n27.36\n38.38\n35.23\n23.24\n22.13\n23.41\n86.46\n16.01\n-4.29%\nSLM0.9\n31.16\n27.68\n5.89\n19.88\n39.15\n27.26\n37.66\n35.06\n21.94\n22.31\n21.73\n86.63\n13.67\n-4.44%\nSnapKV0.9\n27.60\n25.67\n7.10\n20.23\n39.76\n27.43\n38.27\n35.66\n22.99\n22.16\n23.31\n86.46\n16.07\n-1.90%\nH2O0.5\n2.50\n8.26\n5.89\n20.28\n38.53\n27.62\n36.97\n34.31\n23.17\n22.32\n22.60\n86.52\n16.45\n-14.31%\nSLM0.5\n2.43\n6.47\n1.14\n18.27\n32.76\n24.91\n33.80\n32.73\n20.68\n21.48\n18.54\n86.45\n13.58\n-27.61%\nSnapKV0.5\n3.03\n8.93\n6.83\n19.03\n39.22\n27.15\n38.27\n34.60\n22.94\n21.93\n22.68\n86.31\n16.18\n-13.41%\nQwen2.5-7B-Instruct\nFull\n88.02\n31.70\n29.85\n24.55\n61.43\n20.81\n43.17\n47.15\n30.70\n23.64\n24.24\n87.64\n2.44\n100%\nOursk=1\n88.02\n31.25\n30.39\n24.48\n60.01\n20.66\n42.74\n47.20\n29.56\n22.64\n24.04\n87.65\n3.58\n+2.63%\nBudget\n90.7%\n88.8%\n86.5%\n69.2%\n84.1%\n65.1%\n69.2%\n73.3%\n64.4%\n70.6%\n85.4%\n56.6%\n84.4%\n76.02%\nH2O0.9\n83.47\n26.56\n30.25\n24.40\n61.29\n20.85\n43.26\n47.90\n30.73\n23.42\n24.35\n87.57\n2.45\n-1.46%\nSLM0.9\n88.93\n30.80\n28.25\n24.30\n60.91\n19.86\n42.54\n46.02\n28.75\n23.06\n23.42\n87.03\n3.05\n-0.41%\nSnapKV0.9\n80.14\n30.13\n28.11\n24.30\n61.26\n20.90\n43.31\n47.81\n30.69\n23.90\n24.29\n87.57\n2.55\n-1.01%\nH2O0.5\n34.12\n14.73\n20.88\n23.10\n59.69\n21.59\n43.37\n47.60\n30.81\n21.00\n22.95\n85.54\n2.24\n-13.47%\nSLM0.5\n3.26\n1.34\n10.58\n23.75\n49.87\n19.29\n36.45\n42.42\n25.33\n21.24\n22.77\n87.33\n3.40\n-23.56%\nSnapKV0.5\n20.77\n12.28\n22.76\n22.25\n60.21\n20.93\n43.22\n47.45\n30.61\n23.76\n22.70\n87.57\n2.21\n-14.39%\nTable 1: Performance of DBudgetKV and its comparison with three KV cache pruning models on 13 datasets. Bold\nnumbers indicate the best results aside from full-cache. Italics represent the real budget utilized by DBudgetKV.\nThe correspondence between abbreviations and their full names of tasks and datasets can be found in Appendix.\nAvg. calculates the mean ratio of the model’s performance using different KV cache compression methods to its\nperformance with the full cache. All average results (except for budget) are adjusted by subtracting 1 to provide a\nmore intuitive understanding of the effectiveness of different pruning methods.\nDatasets\nFor comprehensive evaluation,\nwe\nevaluate DBudgetKV using datasets of both short\nand long context length. For tasks of relatively\nshort inputs, the models are assessed across math-\nametics, sicence, and commonsense reasoning on\nmultiple datasets including GSM8K (Cobbe et al.,\n2021), GPQA (Rein et al., 2023), TheoremQA\n(Chen et al., 2023), TruthfulQA (Lin et al., 2022),\nand CoQA (Reddy et al., 2019). For long-context\ntasks, we adopt tasks and datasets from Long-\nbench (Bai et al., 2024). Appendix C provides a\ndetailed description and statistics of these datasets\nalong with how they are utilized in experiments.\nEvaluation Protocol\nOur primary objective is\nto evaluate the capability of DBudgetKV to com-\npress KV cache while striking for a full-cache per-\nformance. To this end, we compare DBudgetKV\nwith the full KV cache performance and calculate\nthe average compression ratio per dataset. Addi-\ntionally, we select three previous KV cache com-\npression methods with fixed budget size, includ-\ning: Heavy Hitter Oracle (H2O) (Zhang et al.,\n2023), StreamingLLM (SLM) (Xiao et al., 2024),\nand SnapKV (Li et al., 2024). We compare with\ntwo budgets for these three methods at 90% and\n50% compression ratio respectively.\nApart from the main experiments, we further\nevaluate various design factors of DBudgetKV via\nablation studies, such as different strategies of to-\nken ranking importance, the selection of k for the\nattention matrix, and the necessity to retain bottom\nlayers. Finally, we also provide efficiency analysis\non the time and space overhead comparing against\nother KV cache pruning methods in Sec. 4.5.\n4.2\nMain Results\nThe main results are shown in Table 1 accord-\ning to our evaluation protocol, where DBudgetKV\ndemonstrates strong advantages with robust per-\nformance across diverse tasks and models:\n• DBudgetKV is able to fulfill our objective, ca-\npable of performing near-lossless dynamic com-\npression across different models, varying input\nlengths, and diverse task types. Interestingly, with\nLlama3-8B and Qwen2.5-7B, DBudgetKV even\nsurpasses the full-cache performance by 0.12%\n\nMethods\nGSM8K\nCoQA\nNQA\nMusique\nQMSum\nLlama3-70B-Instruct\nFull\n89.69\n60.36\n27.15\n29.31\n22.52\nOursk=1\n89.76\n60.38\n26.94\n28.88\n22.30\nBudget\n93.3%\n77.2%\n58.7%\n60.8%\n52.9%\nQwen2.5-32B-Instruct\nFull\n91.36\n58.03\n24.78\n40.04\n22.84\nOurs\n91.81\n57.29\n22.65\n40.54\n22.52\nBudget\n91.6%\n85.0%\n68.2%\n74.4%\n76.8%\nQwen2.5-72B-Instruct\nFull\n90.22\n54.14\n24.36\n42.13\n23.93\nOurs\n90.30\n54.19\n24.10\n41.70\n23.31\nBudget\n90.9%\n87.6%\n63.4%\n65.0%\n68.6%\nTable 2: Performance of LLMs with scales of 70B,\n34B, and 72B across five datasets using DBudgetKV.\nMethods\nGSM8K\nCoQA\nNQA\nMusique\nQMSum\nFull\n75.28\n52.74\n24.06\n14.77\n22.27\nPost=1%\nPerformance Using Different k\nk = 1\n76.50\n52.86\n23.44\n15.96\n21.95\nBudget\n93.2%\n81.5%\n48.7%\n43.7%\n15.0%\nk = 1%n\n76.19\n52.87\n23.17\n15.40\n21.94\nBudget\n95.1%\n94.8%\n77.3%\n77.2%\n59.5%\nk = 5%n\n75.59\n52.75\n21.62\n13.71\n21.43\nBudget\n98.6%\n96.2%\n45.1%\n30.0%\n33.1%\nk = 10%n\n76.72\n52.85\n9.74\n13.67\n21.11\nBudget\n96.3%\n96.8%\n32.0%\n19.7%\n29.3%\nk = 1\nPerformance with Different Ranking Method\nAttnt=1%\n2.35\n43.68\n13.37\n9.58\n17.62\nBudget\n20.0%\n9.7%\n6.4%\n6.4%\n6.4%\nAttnt=0.01%\n54.66\n51.58\n17.45\n14.79\n20.90\nBudget\n61.7%\n28.6%\n6.7%\n7.9%\n7.1%\nPosk=1\nPerformance with Different Threshold t\nt = 0.1%\n76.35\n52.84\n22.73\n17.04\n21.90\nBudget\n99.2%\n97.0%\n77.2%\n85.0%\n73.9%\nt = 10%\n26.23\n49.69\n18.35\n15.16\n18.81\nBudget\n55.8%\n37.2%\n15.27%\n18.5%\n6.5%\nTable 3: Performance comparison with different k val-\nues, ranking methods and universal thresholds.\nand 2.63% respectively, utilizing an average of\n63.68% and 76.02% KV cache.\nWith Mistral,\nDBudgetKV also manages to achieve nearly 20%\nKV cache compression with only a 1.5% per-\nformance reduction.\nThese results indicate that\nour proposed method can be practically deployed\nin real-world scenarios without worrying domain-\nspecific budget thresholds. In stark contrast, pre-\nvious methods achieve a consistent full-cache per-\nformance only when manually determined a high\nbudget ratio, e.g. 90%. However, when the budget\nis reduced, e.g. 50%, the degradation can become\nsevere on certain datasets, distinct from DBud-\ngetKV that automatically adjusts the pruning to al-\nways attain full performance.\n• DBudgetKV natually reflects the difficulty of\nthe generation task. As in Table 1, the dynamic\nbudget ratio is high on Math&Science datasets\n(over 90%), while much lower on QA or Summa-\nrization datasets (as low as 15%). This observation\nis in line with our intuition, where inference on\nconcise but hard tasks, such as math problems, re-\nquires more context and more precise calculation,\nresulting in higher budget allocation.\n• Besides the dynamic compression, DBud-\ngetKV also outperforms the three 90%-budget\nbaselines, while itself uses less than 90% budget,\nand achieves the highest score in 20 out of 39 com-\nparisons.\nThis suggests that by taking into ac-\ncount both the position bias and attention scores,\nour method accompolishes more effective pruning\nthan the baselines that only consider one aspect,\nhighlighting the importance of integrating both di-\nmensions for effective KV cache compression.\n4.3\nDBudgetKV’s Generalization Ability\nWe next conduct experiments to examine whether\nour design and hyperparameters can be general-\nized to LLMs of larger sizes.\nOn five datasets\nin Table 2, DBudgetKV with Llama3-70B and\nQwen2.5-32B/72B demonstrates consistent near\nfull-cache performance with the same DBud-\ngetKV setting.\nNotably, the averaged compres-\nsion ratio increases on datasets of longer context,\nachieving nearly a 50% lossless compression at its\npeak, validating the robustness and versatility for\nits general applicability.\n4.4\nAblation Study\nThe ablation study is conducted to explore the\nimpact of various configurations of DBudgetKV.\nWe select Llama-3-8B-Instruct and perform exper-\niments across five datasets presented in Table 3.\nAttention Matrix Reduction\nThe reduced at-\ntention matrix A′ in Sec. 3.2 aggregates attention\nscores from the last k positions. The upper part\nof Table 3 illustrates the model’s performance and\nthe actual budget when setting k = 1, 1%n, 5%n,\nand 10%n (n being the number of input tokens).\nIt is evident that setting k as 1 achieves a sig-\nnificantly reduced budget, thus a higher compres-\nsion ratio, with almost no change in performance\ncompared to 1%n and 5%n. On the other hand,\nwhile 10%n can compress more KV cache, it fails\nto maintain performance (for instance, on Narra-\ntiveQA, the former achieves a performance of 9.74\nusing 32% of the budget, whereas the latter scores\n21.44 using 48.7% of the budget). What’s even\n\nMethods\nGSM8K\nCoQA\nNarrativeQA\nMusique\nQMSum\nTriviaQA\nAvg.\nOverall\nPrune\nOverall\nPrune\nOverall\nPrune\nOverall\nPrune\nOverall\nPrune\nOverall\nPrune\nOverall\nPrune\nLlama3-8B-Instruct\nFull\n4.693\n—\n0.289\n—\n3.441\n—\n3.659\n—\n5.458\n—\n2.717\n—\n3.376\n—\nOursk=1\n4.638\n0.034\n0.331\n0.033\n3.159\n0.255\n3.658\n0.264\n5.330\n0.241\n2.684\n0.211\n3.300\n0.173\nH2O0.5\n4.686\n0.020\n0.290\n0.018\n3.243\n0.264\n3.679\n0.266\n5.398\n0.249\n2.612\n0.237\n3.318\n0.176\nSLM0.5\n6.071\n0.006\n0.301\n0.030\n3.230\n0.257\n3.784\n0.259\n5.454\n0.234\n2.628\n0.208\n3.578\n0.166\nSnapKV0.5\n5.874\n0.021\n0.285\n0.019\n3.307\n0.265\n3.721\n0.266\n5.519\n0.264\n2.650\n0.265\n3.559\n0.183\nLlama3-70B-Instruct\nFull\n17.504\n—\n1.167\n—\n6.285\n—\n6.565\n—\n13.993\n—\n5.302\n—\n8.469\n—\nOursk=1\n15.975\n0.154\n1.253\n0.290\n6.042\n2.345\n7.412\n2.352\n13.835\n3.028\n5.224\n1.602\n8.290\n1.623\nH2O0.5\n19.062\n0.114\n1.059\n0.240\n6.788\n2.298\n7.082\n2.307\n14.360\n3.712\n5.566\n1.308\n8.986\n1.663\nSLM0.5\n22.079\n0.150\n1.138\n0.229\n6.804\n2.069\n7.107\n2.053\n14.734\n3.700\n5.576\n1.367\n9.573\n1.595\nSnapKV0.5\n16.517\n0.117\n1.116\n0.241\n6.795\n2.474\n7.085\n2.468\n13.846\n2.793\n5.562\n1.083\n8.487\n1.529\nTable 4: The average inference time and pruning time of Llama3 with size of 8B/70B across six datasets, measured\nin seconds, with lower values indicating better performance.\nbetter is that since k = 1 requires the least amount\nof computation, relying solely on the scores from\nthe last token, the complexity of obtaining A′ be-\ncomes O(1), independent of the sequence length.\nThe advantages of both high efficacy and low over-\nhead make k = 1 a solid design choice for calcu-\nlating the norm metric.\nImportance Ranking Strategies\nDBudgetKV\nsimply regards positions as the significance of to-\nkens, while another intuitive strategy is to utilize\nattention scores to rank their importance. We in-\nvestigate its potentials through ranking by each to-\nken’s average attention score received from other\ntokens, similar to previous approaches such as\nH2O. The results are shown in the middle of Ta-\nble 3: under the same experimental settings, the\nattention-based ranking struggles to maintain the\nmodel’s performance. Though it achieves a high\ncompression ratio, the performance is not stable\nwith possible severe degradation, which deviates\nfrom our objective. Therefore, the position-based\nstrategy is more suited for our goal.\nAttention Norm Threshold\nAs we adopt the\nuniversal threshold as 1% for the attention norm\ndifference, we comprehensively study the effects\nof smaller or larger thresholds, as shown in the\nbottom part of Table 3. Intuitively, a larger thresh-\nold allows for a higher compression ratio, while\na smaller threshold does the opposite. The con-\nclusion is clear that when the threshold is set to a\nsmaller 0.1%, the average budget increases as ex-\npected, yet the model’s performance sees little im-\nprovement. Conversely, when the threshold is set\nto 10%, it prunes more KV cache, but the model’s\nperformance significantly deteriorates. Thus, we\ndeem 1% as a reasonable threshold for universal\nuses across models and tasks.\nAppendix D presents additional results and\nanalysis from the ablation study.\n4.5\nEfficiency Analysis\nIn this section, a quantitative study is conducted\non whether DBudgetKV improves inference time,\napart from the space reduction from KV cache\npruning.\nWe compare the time usage on six\ndatasets with Llama3-8B/70B using DBudgetKV,\nalong with three baselines with the 50% bud-\nget setting. Table 4 reports the average genera-\ntion time after the prefilling stage of each sample\n(Overall), as well as the time needed to complete\npruning (Prune) before the generation.\nFrom the results, DBudgetKV achieves the best\nperformance in 8 out of 12 comparisons of over-\nall generation time, and achieves the best average\ntime for both 8B and 70B LLMs, suggesting that\nalthough DBudgetKV requires a slightly longer\ntime for pruning itself compared to three other\napproaches, the total generation speed of DBud-\ngetKV is evidently advantageous. It also performs\nconsistently across models of different scales, un-\nderscoring its time efficiency.\n5\nConclusion\nIn this study, we introduce an innovative KV\ncache compression objective designed to approx-\nimate the full-cache performance, independent of\nspecific inputs, while optimizing resource utiliza-\ntion through targeted KV cache pruning. Our ap-\nproach, termed DBudgetKV, employs a straight-\nforward yet effective two-step process for each\nlayer of LLMs. Tokens are initially ranked solely\n\nbased on their positions, followed by the eviction\nof the least significant tokens as determined by\nthe norm value of the reduced attention matrix.\nComprehensive experiments conducted across di-\nverse datasets, encompassing a variety of tasks\nand context lengths, demonstrate that DBudgetKV\nachieves nearly lossless compression, while with\nnotable space and time reduction, successfully ful-\nfilling our objective for greater practical values.\nLimitations\nThe main limitation of DBudgetKV stems from\nthe gap between its current compression budget\nand the true optimal budget. This can be seen in\nTable 1 that for certain scenarios, e.g. QMSum\nwith Mistral-7B, DBudgetKV reaches 84.3% bud-\nget while 50% budget is also viable with no per-\nformance degradation. We regard this gap as room\nfor improvement in future work, facilitating more\naggressive KV cache compression while ensuring\nthe full-cache performance.\nAnother limitation of DBudgetKV is that\nthough it demonstrates almost lossless compres-\nsion, we rely on empirical experiments and there\nis no hard guarantee on the full-performance con-\nstraint.\nIn Table 1, while both Llama3-8B and\nQwen2.5-7B even surpass the full-cache perfor-\nmance, DBudgetKV with Mistral-7B shows trivial\ndegradation of 1.5%. More robust methods could\nbe developed as future work to further strengthen\nthis constraint.\nReferences\nJoshua Ainslie, James Lee-Thorp, Michiel de Jong,\nYury Zemlyanskiy, Federico Lebrón, and Sumit\nSanghai. 2023.\nGQA: training generalized multi-\nquery transformer models from multi-head check-\npoints.\nIn Proceedings of the 2023 Conference\non Empirical Methods in Natural Language Pro-\ncessing, EMNLP 2023, Singapore, December 6-10,\n2023, pages 4895–4901. Association for Computa-\ntional Linguistics.\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,\nJiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao\nLiu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang,\nand Juanzi Li. 2024. Longbench: A bilingual, mul-\ntitask benchmark for long context understanding. In\nProceedings of the 62nd Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), ACL 2024, Bangkok, Thailand, Au-\ngust 11-16, 2024, pages 3119–3137. Association for\nComputational Linguistics.\nZefan Cai, Yichi Zhang, Bofei Gao, Yuliang Liu,\nTianyu Liu,\nKeming Lu,\nWayne Xiong,\nYue\nDong, Baobao Chang, Junjie Hu, and Wen Xiao.\n2024. Pyramidkv: Dynamic KV cache compression\nbased on pyramidal information funneling. CoRR,\nabs/2406.02069.\nYupeng Chang, Xu Wang, Jindong Wang, Yuan Wu,\nKaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi,\nCunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang,\nYi Chang, Philip S. Yu, Qiang Yang, and Xing Xie.\n2023.\nA survey on evaluation of large language\nmodels. CoRR, abs/2307.03109.\nWenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan,\nXueguang Ma, Jianyu Xu, Xinyi Wang, and Tony\nXia. 2023. Theoremqa: A theorem-driven question\nanswering dataset. In Proceedings of the 2023 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2023, Singapore, December 6-\n10, 2023, pages 7889–7901. Association for Com-\nputational Linguistics.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, Christopher Hesse, and John Schulman.\n2021. Training verifiers to solve math word prob-\nlems. CoRR, abs/2110.14168.\nPradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan,\nNoah A. Smith, and Matt Gardner. 2021. A dataset\nof information-seeking questions and answers an-\nchored in research papers.\nIn Proceedings of the\n2021 Conference of the North American Chapter\nof the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2021,\nOnline, June 6-11, 2021, pages 4599–4610. Associ-\nation for Computational Linguistics.\nDeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingx-\nuan Wang, Bo Liu, Chenggang Zhao, Chengqi\nDengr, Chong Ruan, Damai Dai, Daya Guo, De-\njian Yang, Deli Chen, Dongjie Ji, Erhang Li,\nFangyun Lin, Fuli Luo, Guangbo Hao, Guanting\nChen, Guowei Li, H. Zhang, Hanwei Xu, Hao\nYang, Haowei Zhang, Honghui Ding, Huajian Xin,\nHuazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian\nLiang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jin Chen,\nJingyang Yuan, Junjie Qiu, Junxiao Song, Kai Dong,\nKaige Gao, Kang Guan, Lean Wang, Lecong Zhang,\nLei Xu, Leyi Xia, Liang Zhao, Liyue Zhang, Meng\nLi, Miaojun Wang, Mingchuan Zhang, Minghua\nZhang, Minghui Tang, Mingming Li, Ning Tian,\nPanpan Huang, Peiyi Wang, Peng Zhang, Qihao\nZhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin,\nRuiqi Ge, Ruizhe Pan, Runxin Xu, Ruyi Chen, S. S.\nLi, Shanghao Lu, Shangyan Zhou, Shanhuang Chen,\nShaoqing Wu, Shengfeng Ye, Shirong Ma, Shiyu\nWang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou,\nSize Zheng, T. Wang, Tian Pei, Tian Yuan, Tianyu\nSun, W. L. Xiao, Wangding Zeng, Wei An, Wen Liu,\nWenfeng Liang, Wenjun Gao, Wentao Zhang, X. Q.\nLi, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong\nLiu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen,\n\nXiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaox-\niang Wang, Xin Liu, Xin Xie, Xingkai Yu, Xinnan\nSong, Xinyi Zhou, Xinyu Yang, Xuan Lu, Xuecheng\nSu, Y. Wu, Y. K. Li, Y. X. Wei, Y. X. Zhu, Yanhong\nXu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng\nSun, Yaohui Li, Yaohui Wang, Yi Zheng, Yichao\nZhang, Yiliang Xiong, Yilong Zhao, Ying He, Ying\nTang, Yishi Piao, Yixin Dong, Yixuan Tan, Yiyuan\nLiu, Yongji Wang, Yongqiang Guo, Yuchen Zhu,\nYuduan Wang, Yuheng Zou, Yukun Zha, Yunxian\nMa, Yuting Yan, Yuxiang You, Yuxuan Liu, Z. Z.\nRen, Zehui Ren, Zhangli Sha, Zhe Fu, Zhen Huang,\nZhen Zhang, Zhenda Xie, Zhewen Hao, Zhihong\nShao, Zhiniu Wen, Zhipeng Xu, Zhongyu Zhang,\nZhuoshu Li, Zihan Wang, Zihui Gu, Zilin Li, and\nZiwei Xie. 2024.\nDeepseek-v2: A strong, eco-\nnomical, and efficient mixture-of-experts language\nmodel. Preprint, arXiv:2405.04434.\nAlessio Devoto, Yu Zhao, Simone Scardapane, and\nPasquale Minervini. 2024. A simple and effective\nl2 norm-based strategy for KV cache compression.\nCoRR, abs/2406.11430.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Let-\nman, Akhil Mathur, Alan Schelten, Amy Yang,\nAngela Fan, Anirudh Goyal, Anthony Hartshorn,\nAobo Yang, Archi Mitra, Archie Sravankumar,\nArtem Korenev, Arthur Hinsvark, Arun Rao, As-\nton Zhang, Aurélien Rodriguez, Austen Gregerson,\nAva Spataru, Baptiste Rozière, Bethany Biron,\nBinh Tang, Bobbie Chern, Charlotte Caucheteux,\nChaya Nayak, Chloe Bi, Chris Marra, Chris Mc-\nConnell, Christian Keller, Christophe Touret, Chun-\nyang Wu, Corinne Wong, Cristian Canton Fer-\nrer, Cyrus Nikolaidis, Damien Allonsius, Daniel\nSong, Danielle Pintz, Danny Livshits, David Es-\niobu, Dhruv Choudhary, Dhruv Mahajan, Diego\nGarcia-Olano, Diego Perino, Dieuwke Hupkes, Egor\nLakomkin, Ehab AlBadawy, Elina Lobanova, Emily\nDinan, Eric Michael Smith, Filip Radenovic, Frank\nZhang, Gabriel Synnaeve, Gabrielle Lee, Geor-\ngia Lewis Anderson, Graeme Nail, Grégoire Mialon,\nGuan Pang, Guillem Cucurell, Hailey Nguyen, Han-\nnah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,\nImanol Arrieta Ibarra, Isabel M. Kloumann, Ishan\nMisra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan\nGeffert, Jana Vranes, Jason Park, Jay Mahadeokar,\nJeet Shah, Jelmer van der Linde, Jennifer Billock,\nJenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi,\nJianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu,\nJoanna Bitton, Joe Spisak, Jongsoo Park, Joseph\nRocca, Joshua Johnstun, Joshua Saxe, Junteng Jia,\nKalyan Vasuden Alwala, Kartikeya Upasani, Kate\nPlawiak, Ke Li, Kenneth Heafield, Kevin Stone, and\net al. 2024. The llama 3 herd of models. CoRR,\nabs/2407.21783.\nKawin Ethayarajh. 2019. How contextual are contextu-\nalized word representations? Comparing the geom-\netry of BERT, ELMo, and GPT-2 embeddings. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 55–65,\nHong Kong, China. Association for Computational\nLinguistics.\nAlexander R. Fabbri, Irene Li, Tianwei She, Suyi Li,\nand Dragomir R. Radev. 2019. Multi-news: A large-\nscale multi-document summarization dataset and ab-\nstractive hierarchical model. In Proceedings of the\n57th Conference of the Association for Computa-\ntional Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers, pages\n1074–1084. Association for Computational Linguis-\ntics.\nAina Garí Soler and Marianna Apidianaki. 2021. Let‘s\nplay mono-poly: BERT can reveal words’ polysemy\nlevel and partitionability into senses. Transactions\nof the Association for Computational Linguistics,\n9:825–844.\nSuyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Ji-\nawei Han, and Jianfeng Gao. 2024. Model tells you\nwhat to discard: Adaptive KV cache compression\nfor llms. In The Twelfth International Conference\non Learning Representations, ICLR 2024, Vienna,\nAustria, May 7-11, 2024. OpenReview.net.\nDaya Guo, Canwen Xu, Nan Duan, Jian Yin, and Ju-\nlian J. McAuley. 2023. Longcoder: A long-range\npre-trained language model for code completion.\nIn International Conference on Machine Learning,\nICML 2023, 23-29 July 2023, Honolulu, Hawaii,\nUSA, volume 202 of Proceedings of Machine Learn-\ning Research, pages 12098–12107. PMLR.\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,\nand Akiko Aizawa. 2020. Constructing A multi-hop\nQA dataset for comprehensive evaluation of reason-\ning steps. In Proceedings of the 28th International\nConference on Computational Linguistics, COLING\n2020, Barcelona, Spain (Online), December 8-13,\n2020, pages 6609–6625. International Committee on\nComputational Linguistics.\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine\nRoux, Arthur Mensch, Blanche Savary, Chris Bam-\nford, Devendra Singh Chaplot, Diego de Las Casas,\nEmma Bou Hanna,\nFlorian Bressand,\nGianna\nLengyel,\nGuillaume Bour,\nGuillaume Lample,\nLélio Renard Lavaud,\nLucile Saulnier,\nMarie-\nAnne Lachaux, Pierre Stock, Sandeep Subramanian,\nSophia Yang, Szymon Antoniak, Teven Le Scao,\nThéophile Gervet, Thibaut Lavril, Thomas Wang,\nTimothée Lacroix, and William El Sayed. 2024a.\nMixtral of experts. CoRR, abs/2401.04088.\nHuiqiang Jiang, Yucheng Li, Chengruidong Zhang,\nQianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han,\nAmir Abdi, Dongsheng Li, Chin-Yew Lin, Yuqing\nYang, and Lili Qiu. 2024b. Minference 1.0: Accel-\nerating pre-filling for long-context llms via dynamic\nsparse attention. In Advances in Neural Information\nProcessing Systems 38: Annual Conference on Neu-\nral Information Processing Systems 2024, NeurIPS\n\n2024, Vancouver, BC, Canada, December 10 - 15,\n2024.\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics, ACL\n2017, Vancouver, Canada, July 30 - August 4, Vol-\nume 1: Long Papers, pages 1601–1611. Association\nfor Computational Linguistics.\nTomás Kociský, Jonathan Schwarz, Phil Blunsom,\nChris Dyer, Karl Moritz Hermann, Gábor Melis, and\nEdward Grefenstette. 2018. The narrativeqa read-\ning comprehension challenge. Trans. Assoc. Com-\nput. Linguistics, 6:317–328.\nYuhong Li, Yingbing Huang, Bowen Yang, Bharat\nVenkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai,\nPatrick Lewis, and Deming Chen. 2024.\nSnapkv:\nLLM knows what you are looking for before gener-\nation. In Advances in Neural Information Process-\ning Systems 38: Annual Conference on Neural In-\nformation Processing Systems 2024, NeurIPS 2024,\nVancouver, BC, Canada, December 10 - 15, 2024.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022.\nTruthfulqa: Measuring how models mimic human\nfalsehoods. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), ACL 2022, Dublin, Ire-\nland, May 22-27, 2022, pages 3214–3252. Associa-\ntion for Computational Linguistics.\nZichang Liu, Aditya Desai, Fangshuo Liao, Weitao\nWang, Victor Xie, Zhaozhuo Xu, Anastasios Kyril-\nlidis, and Anshumali Shrivastava. 2023.\nScis-\nsorhands: Exploiting the persistence of importance\nhypothesis for LLM KV cache compression at test\ntime. In Advances in Neural Information Processing\nSystems 36: Annual Conference on Neural Informa-\ntion Processing Systems 2023, NeurIPS 2023, New\nOrleans, LA, USA, December 10 - 16, 2023.\nShervin Minaee, Tomás Mikolov, Narjes Nikzad,\nMeysam Chenaghlu, Richard Socher, Xavier Am-\natriain, and Jianfeng Gao. 2024.\nLarge language\nmodels: A survey. CoRR, abs/2402.06196.\nOpenAI. 2023.\nGPT-4 technical report.\nCoRR,\nabs/2303.08774.\nSiva Reddy, Danqi Chen, and Christopher D. Manning.\n2019.\nCoqa: A conversational question answer-\ning challenge. Trans. Assoc. Comput. Linguistics,\n7:249–266.\nDavid Rein, Betty Li Hou, Asa Cooper Stickland,\nJackson Petty, Richard Yuanzhe Pang, Julien Di-\nrani, Julian Michael, and Samuel R. Bowman. 2023.\nGPQA: A graduate-level google-proof q&a bench-\nmark. CoRR, abs/2311.12022.\nNoam Shazeer. 2019. Fast transformer decoding: One\nwrite-head is all you need. CoRR, abs/1911.02150.\nHarsh Trivedi,\nNiranjan Balasubramanian,\nTushar\nKhot, and Ashish Sabharwal. 2022. Musique: Mul-\ntihop questions via single-hop question composition.\nTrans. Assoc. Comput. Linguistics, 10:539–554.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-\n9, 2017, Long Beach, CA, USA, pages 5998–6008.\nZhongwei Wan, Xinjian Wu, Yu Zhang, Yi Xin,\nChaofan Tao, Zhihong Zhu, Xin Wang, Siqi Luo,\nJing Xiong, and Mi Zhang. 2024.\nD2O: dy-\nnamic discriminative operations for efficient gener-\native inference of large language models.\nCoRR,\nabs/2406.13035.\nZheng Wang, Boxiao Jin, Zhongzhi Yu, and Minjia\nZhang. 2024.\nModel tells you where to merge:\nAdaptive KV cache merging for llms on long-\ncontext tasks. CoRR, abs/2407.08454.\nGuangxuan Xiao, Yuandong Tian, Beidi Chen, Song\nHan, and Mike Lewis. 2024.\nEfficient stream-\ning language models with attention sinks. In The\nTwelfth International Conference on Learning Rep-\nresentations, ICLR 2024, Vienna, Austria, May 7-11,\n2024. OpenReview.net.\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan\nHui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayi-\nheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian\nYang, Jianhong Tu, Jianwei Zhang, Jianxin Yang,\nJiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang,\nKeming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li,\nMingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji\nLin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xu-\nancheng Ren, Yang Fan, Yang Su, Yichang Zhang,\nYu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and\nZihan Qiu. 2024. Qwen2.5 technical report. CoRR,\nabs/2412.15115.\nZhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong\nChen, Lianmin Zheng, Ruisi Cai, Zhao Song,\nYuandong Tian, Christopher Ré, Clark W. Barrett,\nZhangyang Wang, and Beidi Chen. 2023.\nH2O:\nheavy-hitter oracle for efficient generative inference\nof large language models. In Advances in Neural\nInformation Processing Systems 36: Annual Con-\nference on Neural Information Processing Systems\n2023, NeurIPS 2023, New Orleans, LA, USA, De-\ncember 10 - 16, 2023.\nMing Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia\nMutuma, Rahul Jha, Ahmed Hassan Awadallah,\nAsli Celikyilmaz, Yang Liu, Xipeng Qiu, and\nDragomir R. Radev. 2021. Qmsum: A new bench-\nmark for query-based multi-domain meeting sum-\nmarization. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2021, Online, June 6-11,\n\n2021, pages 5905–5921. Association for Computa-\ntional Linguistics.\n\nA\nFull Algorithm of DBudgetKV\nAlgorithm 1 DBudgetKV\nInput: Prompt, Threshold t\nOutput: Compressed KV Cache\nCreate Empty List Kc, Vc\nfor Transformer Layer Li in LLM do\nQi, Ki, V i ←Li(Prompt)\nRi ←Postion-Based Importance Rank\nAi\nlast ←Attention(Qi[. . . , −1, :], KiT )\nF i\nb ←Frobenius(Ai\nlast)\nAi\nlast ←Square(Ai\nlast)\nReorder Ai\nlast by Rank R\nAi\ncumsum ←Cumsum(Ai\nlast)\nAi\ncumsum ←Sqrt(Ai\ncumsum)\nAi\nratio ←(F i\nb −Ai\ncumsum)/F i\nb\nIndexi ←Max(Where(Ai\nratio <= t))\nKi\nc ←Compress KibyRi[I :]\nV i\nc ←Compress V ibyRi[I :]\nAppend Ki\nc, V i\nc to Kc, Vc\nend for\nreturn Kc, Vc\nB\nFreeze the first two layers of LLM\nIn this section, we elaborate the necessity to freeze\nthe first and second layers in DBudgetKV as men-\ntioned in Section 3. In our preliminary studies, we\napply DBudgetKV to Llama2-7B-Chat and con-\nduct experiments on GSM8K and CoQA. We first\nperform a case study, followed by a comparison\nwith the effects of not freezing the bottom layers.\nAs shown in Table 5, when the threshold is set\nto 1% and no layers are frozen, the outputs for two\nexamples on GSM8K and CoQA are incorrect and\nlack logical coherence. However, the budget ex-\nceeds 90% in both cases. By observing the actual\nbudget of each layer, we can see that for these two\nexamples, the budgets for layer 0 and 1 are rela-\ntively low, while the budgets from layer 2 to layer\n30 become very high, with the last layer being\nlow again. We hypothesize that the model is still\nencoding the global semantics in its early layers,\nthus not yet able to identify truly important tokens\nin the first two layers, of which the attention dis-\ntribution is relatively uniform. This phenomenon\nhas also been observed by early works on Trans-\nformers analysis (Ethayarajh, 2019; Garí Soler and\nApidianaki, 2021). Not freezing early layers may\nlead to early eviction of important tokens, result-\ning in subsequent generations being unable to ac-\nGSM8K\nInput\nA robe takes 2 bolts of blue fiber a-\nnd half that much white fiber. How\nmany bolts in total does it take?\nBudget\nLayer 0: 67.71\nLayer 1: 82.29\nLayer 2∼30: 95.83\nLayer 31: 37.50\nAvg.: 93.90\nOutput\nI have determined by answering the\nanswer to the format of bolts bolts b-\nolts...(repeat)\nGround-Truth\n3\nCoQA\nInput\nYou are given a story and a question.\nAnswer the question as concisely as\nyou can...Question: What color was\nCotton?\nBudget\nLayer 0: 42.14\nLayer 1: 42.54\nLayer 2∼30: 99.19\nLayer 31: 79.64\nAvg.: 95.03\nOutput\nQuestion: Question: What is the que-\nstion: What is the question:\nGroud-Truth\nWhite\nTable 5: Case Study.\nDatasets\nLayer\nNone\n0\n0,1\n0,1,2\n0,1,2,3\n0,1,31\nGSM8K\nOursk=1\n0.014\n0.250\n0.264\n0.252\n0.264\n0.252\nBudget\n93.1%\n97.0%\n95.4%\n97.5%\n97.7%\n98.2%\nCoQA\nOursk=1\n1.47\n52.80\n53.58\n53.58\n53.32\n53.46\nBudget\n92.7%\n94.7%\n95.5%\n98.3%\n98.4%\n99.2%\nTable 6: Performance of Llama2-7B-Chat on GSM8K\nand CoQA using DBudgetKV with different frozen\nlayers.\ncess this information, ultimately causing the out-\nput to fail.\nBased on the above case study, we attempt to\nfreeze certain layers of the model and explore the\noptimal freezing configuration.\nWe continue to\nvalidate the results of freezing different layers,\nand the resuls are shown in Table 6. We can ob-\nserve that freezing the first two layers achieves a\nbalance between model performance and budget.\nMoreover, freezing the last layer does not signif-\nicantly enhance the model’s performance and in-\nstead leads to an increase in budget. DBudgetKV\nultimately opts to begin KV cache compression\nfrom the 2nd layer of the model.\n\nC\nDatasets Used in Experiments\nIn this section, we provide a comprehensive\noverview of all the tasks and datasets utilized in\nthe experiments in this paper.\nMath & Science\nThis task evaluates the model’s\nability to tackle mathematical and scientific prob-\nlems. By directly inputting questions and com-\nparing the model’s output with the correct an-\nswers, we calculate the model’s Accuracy on\nthese datasets: GSM8K is a dataset for evaluat-\ning model’s math-solving skills, featuring 8,000\nelementary-level math word problems requiring\nbasic arithmetic and reasoning.\nGPQA tests\nmodel’s understanding of physics concepts and\nproblem-solving across various topics, assessing\nscientific reasoning abilities. TheoremQA evalu-\nates model’s grasp and application of mathemati-\ncal theorems, ranging from simple applications to\ncomplex proofs, testing advanced math skills.\nCommonsense Reasoning (CR)\nThis task eval-\nuates model’s ability to make deductions and un-\nderstand everyday situations using implicit knowl-\nedge and logical inference. TruthfulQA (ThQA)\nevaluates model’s ability to generate accurate and\ntruthful responses, testing models on distinguish-\ning fact from fiction, especially in areas prone\nto misconceptions.\nWe use BLEU as the met-\nric. CoQA assesses model’s ability to understand\nand respond to questions in a conversational con-\ntext, focusing on maintaining coherence and con-\ntext throughout a dialogue. We use F1 Score as the\nmetric.\nSingle Document QA (Single-Doc QA)\nThis\ntask assesses the model’s reading comprehension\nskills when dealing with a single, extended doc-\nument.\nNarrativeQA (Kociský et al., 2018) is\na dataset designed to evaluate model’s ability to\ncomprehend and answer questions based on narra-\ntive texts, focusing on understanding stories and\ntheir underlying themes. Qasper (Dasigi et al.,\n2021) is a dataset aimed at assessing model’s ca-\npability to extract and answer questions from aca-\ndemic papers, emphasizing understanding com-\nplex scientific information. We employ F1 Score\nas the metric for above two datasets.\nMulti-Document QA (Multi-Doc QA)\nThis\ntask evaluates the model’s reading comprehen-\nsion capabilities across multiple extended doc-\numents.\n2WikiMultiHopQA (2WKMQA) (Ho\nMethods\nGSM8K\nCoQA\nNQA\nMusique\nQMSum\nFull\n75.28\n52.74\n24.06\n14.77\n22.27\nPost=1%\nPerformance Using Different k\nk = 1\n76.50\n52.86\n23.44\n15.96\n21.95\nBudget\n93.2%\n81.5%\n48.7%\n43.7%\n15.0%\nk = 1%n\n76.19\n52.87\n23.17\n15.40\n21.94\nBudget\n95.1%\n94.8%\n77.3%\n77.2%\n59.5%\nk = 2%n\n76.19\n52.82\n22.87\n14.34\n21.74\nBudget\n96.0%\n96.1%\n69.8%\n62.0%\n47.1%\nk = 3%n\n75.82\n52.80\n23.03\n13.61\n21.56\nBudget\n98.6%\n95.7%\n60.3%\n46.0%\n39.5%\nk = 4%n\n75.21\n52.76\n22.96\n13.45\n21.47\nBudget\n98.8%\n95.8%\n51.9%\n35.2%\n35.4%\nk = 5%n\n75.59\n52.75\n21.62\n13.71\n21.43\nBudget\n98.6%\n96.2%\n45.1%\n30.0%\n33.1%\nk = 6%n\n75.66\n52.81\n21.95\n14.33\n20.94\nBudget\n98.5%\n96.6%\n40.4%\n25.8%\n31.6%\nk = 7%n\n76.57\n52.88\n21.18\n13.66\n20.94\nBudget\n98.0%\n96.7%\n37.3%\n22.5%\n30.6%\nk = 8%n\n76.35\n52.86\n20.36\n13.75\n21.11\nBudget\n97.4%\n96.7%\n35.1%\n20.9%\n30.1%\nk = 9%n\n76.65\n52.84\n19.99\n13.66\n20.76\nBudget\n96.8%\n96.8%\n33.4%\n19.9%\n29.5%\nk = 10%n\n76.72\n52.85\n9.74\n13.67\n21.11\nBudget\n96.3%\n96.8%\n32.0%\n19.7%\n29.3%\nTable 7:\nPerformance comparison with different\nkvalues, ranking methods and universal thresholds.\net al., 2020) is a dataset designed to test model’s\nability to perform multi-hop reasoning and an-\nswer complex questions using information from\nmultiple Wikipedia articles.\nMuSiQue (Trivedi\net al., 2022) evaluates model’s skill in integrat-\ning and reasoning over information from multiple\nsources to answer comprehensive questions accu-\nrately.\nWe leverage F1 Score as the metric for\nabove two datasets.\nSummarization\nThis task examines the model’s\nability to comprehend and summarize lengthy\ndocuments.\nQMSum (Zhong et al., 2021) is a\ndataset for evaluating model’s ability to generate\nconcise summaries of meeting transcripts, focus-\ning on capturing the key points from multi-party\ndiscussions. Multi-News (M-News) (Fabbri et al.,\n2019) is a dataset that challenges models to create\ncoherent summaries by synthesizing information\nfrom multiple news articles on the same topic. We\nuse Rouge-L as the metric for above two datasets.\nFew-Shot Learning (FSL)\nThis task assesses\nthe model’s few-shot learning capabilities. Triv-\niaQA (Joshi et al., 2017) is a dataset designed to\nassess model’s ability to retrieve and answer ques-\ntions based on large collections of trivia, empha-\nsizing comprehension and factual recall. We use\nF1 Score as the metric.\n\nCode\nThis task evaluates the model’s ability to\ncomplete and generate code. LCC (Guo et al.,\n2023) is a dataset focused on evaluating models’\nability to understand and generate code by consid-\nering extended code contexts, enhancing the abil-\nity to reason over complex programming struc-\ntures. We use Edit Sim as the metric.\nD\nAblation\nIn this section, we present additional ablation\nstudy results. By setting various values for k, we\nexpand upon the results shown in Table 3. The ex-\npanded results are displayed in Table 7. These ex-\nperiments facilitate a deeper understanding of how\ndifferent parameter settings impact model perfor-\nmance and provide a basis for optimizing parame-\nter selection.\nAs shown in Table 7, setting k = 1 not only\nconserves pruning time but also achieves better\nmodel performance with a reduced budget.\n",
  "metadata": {
    "source_path": "papers/arxiv/DBudgetKV_Dynamic_Budget_in_KV_Cache_Compression_for_Ensuring_Optimal\n__Performance_2f4d481c243b3958.pdf",
    "content_hash": "2f4d481c243b3958ba890e7a37a06d2f24d062d9369afd9a6ff3541731a02ee5",
    "arxiv_id": null,
    "title": "DBudgetKV_Dynamic_Budget_in_KV_Cache_Compression_for_Ensuring_Optimal\n__Performance_2f4d481c243b3958",
    "author": "",
    "creation_date": "D:20250225022723Z",
    "published": "2025-02-25T02:27:23",
    "pages": 15,
    "size": 453140,
    "file_mtime": 1740470215.2731307
  }
}