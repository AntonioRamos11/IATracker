{
  "text": "Low-distortion and GPU-compatible Tree Embeddings in Hyperbolic Space\nMax van Spengler 1 Pascal Mettes 1\nAbstract\nEmbedding tree-like data, from hierarchies to on-\ntologies and taxonomies, forms a well-studied\nproblem for representing knowledge across many\ndomains. Hyperbolic geometry provides a natu-\nral solution for embedding trees, with vastly su-\nperior performance over Euclidean embeddings.\nRecent literature has shown that hyperbolic tree\nembeddings can even be placed on top of neu-\nral networks for hierarchical knowledge integra-\ntion in deep learning settings. For all applica-\ntions, a faithful embedding of trees is needed,\nwith combinatorial constructions emerging as the\nmost effective direction. This paper identifies and\nsolves two key limitations of existing works. First,\nthe combinatorial construction hinges on finding\nhighly separated points on a hypersphere, a no-\ntoriously difficult problem. Current approaches\nachieve poor separation, degrading the quality of\nthe corresponding hyperbolic embedding. We pro-\npose highly separated Delaunay tree embeddings\n(HS-DTE), which integrates angular separation\nin a generalized formulation of Delaunay embed-\ndings, leading to lower embedding distortion. Sec-\nond, low-distortion requires additional precision.\nThe current approach for increasing precision is\nto use multiple precision arithmetic, which ren-\nders the embeddings useless on GPUs in deep\nlearning settings. We reformulate the combinato-\nrial construction using floating point expansion\narithmetic, leading to superior embedding quality\nwhile retaining utility on accelerated hardware.\n1. Introduction\nTree-like structures such as hierarchies are key for knowl-\nedge representation, from biological taxonomies (Padial\net al., 2010) and phylogenetics (Kapli et al., 2020) to natu-\nral language (Miller, 1995; Tifrea et al., 2018; Yang et al.,\n2016), social networks (Freeman, 2004), visual understand-\n1VIS Lab, University of Amsterdam, The Netherlands. Corre-\nspondence to: Max van Spengler <m.w.f.vanspengler@uva.nl>.\nArXiv preprint.\ning (Desai et al., 2023) and more. To obtain faithful embed-\ndings, Euclidean space is ill-equiped; even simple trees lead\nto high distortion (Sonthalia & Gilbert, 2020). On the other\nhand, the exponential nature of hyperbolic space makes it\na natural geometry for embedding trees (Nickel & Kiela,\n2018). This insight has led to rapid advances in hyperbolic\nlearning, with superior embedding (Sala et al., 2018) and\nclustering (Chami et al., 2020) of tree-like data.\nRecent literature has shown that hyperbolic tree embeddings\nare not only useful on their own, they also form powerful tar-\nget embeddings on top of deep networks to unlock hierarchi-\ncal representation learning (Peng et al., 2021; Mettes et al.,\n2024). Deep learning with hyperbolic tree embeddings has\nmade it possible to effectively perform action recognition\n(Long et al., 2020), knowledge graph completion (Kolyvakis\net al., 2020), hypernymy detection (Tifrea et al., 2018) and\nmany other tasks in hyperbolic space. These early adop-\ntions of hyperbolic embeddings have shown a glimpse of\nthe powerful improvements that hierarchically aligned rep-\nresentations can bring to deep learning.\nThe rapid advances in hyperbolic deep learning under-\nline the need for hyperbolic tree embeddings compati-\nble with GPU accelerated software.\nCurrent tree em-\nbedding algorithms can roughly be divided in two cate-\ngories; optimization-based and constructive methods. The\noptimization-based methods, e.g. Poincar´e embeddings\n(Nickel & Kiela, 2017), hyperbolic entailment cones (Ganea\net al., 2018), and distortion optimization (Yu et al., 2022b),\ntrain embeddings using some objective function based on\nthe tree. While these approaches are flexible due to minimal\nassumptions, the optimization can be unstable, slow and\nresult in heavily distorted embeddings. Conversely, con-\nstructive methods traverse a tree once, placing the children\nof each node on a hypersphere around the node’s embedding\n(Sarkar, 2011; Sala et al., 2018). These methods are fast,\nrequire no hyperparameter tuning and have great error guar-\nantees. However, they rely on hyperspherical separation,\na notoriously difficult problem (Saff & Kuijlaars, 1997),\nand on multiple precision floating point arithmetic, which is\nincompatible with GPUs and other accelerated hardware.\nOur goal is to embed trees in hyperbolic space with minimal\ndistortion yet with the ability to operate on accelerated GPU\nhardware even when using higher precision. We do so in\n1\narXiv:2502.17130v1  [cs.LG]  24 Feb 2025\n\nLow-distortion and GPU-compatible Tree Embeddings in Hyperbolic Space\ntwo steps. First, we outline HS-DTE, a new generalization\nof Delaunay tree embeddings (Sarkar, 2011) to arbitrary\ndimensionality through hyperspherical separation. Second,\nwe propose HypFPE, a floating point expansion arithmetic\napproach to enhance our constructive hyperbolic tree em-\nbeddings. We develop new routines for computing hyper-\nbolic distances on floating point expansions and outline how\nto use these on hyperbolic embeddings. Furthermore, we\nprovide theoretical results demonstrating the effectiveness\nof these floating point expansion routines. Floating point\nexpansions allow for higher precision similar to multiple\nprecision arithmetic. However, our routines can be imple-\nmented using standard floating point operations, making\nthese compatible with GPUs. Experiments demonstrate that\nHS-DTE generates higher fidelity embeddings than other\nhyperbolic tree embeddings and that HypFPE further in-\ncreases the embedding quality for HS-DTE and other meth-\nods. We will make two software libraries available, one for\narbitrary-dimensional hyperbolic tree embeddings and one\nfor GPU-compatible floating point expansions.\n2. Preliminaries and related work\n2.1. Hyperbolic geometry preliminaries\nTo help explain existing constructive hyperbolic embedding\nalgorithms and our proposed approach, we outline the most\nimportant hyperbolic functions here. For a more thorough\noverview, we refer to (Cannon et al., 1997; Anderson, 2005).\nAkin to (Nickel & Kiela, 2017; Ganea et al., 2018; Sala et al.,\n2018), we focus on the Poincar´e ball model of hyperbolic\nspace. For n-dimensional hyperbolic space, the Poincar´e\nball model is defined as the Riemannian manifold (Dn, gn),\nwhere the manifold and Riemannian metric are defined as\nDn =\n\b\nx ∈Rn : ||x||2 < 1\n\t\n,\ngn = λxIn,\nλx =\n2\n1 −||x||2 .\n(1)\nUsing this model of hyperbolic space, we can compute\ndistances between x, y ∈Dn either as\ndD(x, y) = cosh−1\n\u0012\n1 + 2\n||x −y||2\n(1 −||x||2)(1 −||y||2)\n\u0013\n, (2)\nor as\ndD(x, y) = 2 tanh−1 \u0000|| −x ⊕y||\n\u0001\n,\n(3)\nwhere\nx ⊕y = (1 + 2⟨x, y⟩+ ||y||2)x + (1 −||x||2)y\n1 + 2⟨x, y⟩+ ||x||2||y||2\n,\n(4)\nis the M¨obius addition operation. These formulations are\ntheoretically equivalent, but suffer from different numerical\nerrors. This distance represents the length of the straight\nline or geodesic between x and y with respect to the Rie-\nmannian metric gn. Geodesics of the Poincar´e ball are\nEuclidean straight lines through the origin and circular arcs\nperpendicular to the boundary of the ball. We will use some\nisometries of hyperbolic space. More specifically, we will\nuse reflections in geodesic hyperplanes. A geodesic hyper-\nplane is an (n −1)-dimensional manifold consisting of all\ngeodesics through some point x ∈Dn which are orthogonal\nto a normal geodesic through x or, equivalenty, orthogonal\nto some normal tangent vector v ∈TxDn. For the Poincar´e\nball these are the Euclidean hyperplanes through the origin\nand the (n −1)-dimensional hyperspherical caps which are\nperpendicular to the boundary of the ball. We will denote\na geodesic hyperplane by Hx,v. Reflection in a geodesic\nhyperplane H0,v through the origin can be defined as in\nEuclidean space, so as a Householder transformation\nRH0,v(y) = (In −2vvT )y,\n(5)\nwhere ||v|| = 1. Reflection in the other type of geodesic\nhyperplane is a spherical inversion:\nRHx,v(y) = m +\nr2\n||y −m||2 (y −m),\n(6)\nwith m ∈Rn, r > 0 the center and radius of the hyper-\nsphere containing the geodesic hyperplane. We will denote\na reflection mapping some point x ∈Dn to another point\ny ∈Dn by Rx→y. The specific formulations and deriva-\ntions of the reflections that we use are in Appendix A.\n2.2. Related work\nHyperbolic tree embedding algorithms.\nExisting em-\nbedding methods can be divided into two categories:\noptimization-based methods and constructive methods. The\noptimization methods typically use the tree to define some\nloss function and use a stochastic optimization method such\nas SGD to directly optimize the embedding of each node,\ne.g. Poincar´e embeddings (Nickel & Kiela, 2017), hyper-\nbolic entailment cones (Ganea et al., 2018) and distortion\noptimization (Sala et al., 2018; Yu et al., 2022b). Poincar´e\nembeddings use a contrastive loss where related nodes are\npulled together and unrelated nodes are pushed apart. Hyper-\nbolic entailment cones attach an outwards radiating cone to\neach node embedding and define a loss that forces children\nof nodes into the cone of their parent. Distortion optimiza-\ntion directly optimizes for a distortion loss to embed node\npairs. Such approaches are flexible, but do not lead to arbi-\ntrarily low distortion and optimization is slow. Constructive\nmethods are either combinatorial methods (Sarkar, 2011;\nSala et al., 2018) or eigendecomposition methods (Sala et al.,\n2018). Combinatorial methods first place the root of a tree\nat the origin of the hyperbolic space and then traverse down\nthe tree, iteratively placing nodes as uniformly as possible\non a hypersphere around their parent. (Sarkar, 2011) pro-\nposes a 2-dimensional approach, where the points have to be\nseparated on a circle; a trivial task. For higher dimensions,\n2\n\nLow-distortion and GPU-compatible Tree Embeddings in Hyperbolic Space\n(Sala et al., 2018) place points on a hypercube inscribed\nwithin a hypersphere, which leads to suboptimal distribu-\ntion. We also follow a constructive approach, where we\nuse an optimization method for the hyperspherical separa-\ntion, leading to significantly higher quality embeddings. The\neigendecomposition method h-MDS (Sala et al., 2018) takes\na graph or tree metric and uses an eigendecomposition of\nthe corresponding distance matrix to generate low-distortion\nembeddings. However, it collapses nodes within some sub-\ntrees to a single point, leading to massive local distortion.\nDeep learning with hyperbolic tree embeddings.\nIn\ncomputer vision, a wide range of works have recently shown\nthe potential and effectiveness of using a hyperbolic em-\nbedding space (Khrulkov et al., 2020). Specifically, hier-\narchical prior knowledge can be embedded in hyperbolic\nspace, after which visual representations can be mapped\nto the same space and optimized to match this hierarchical\norganization. (Long et al., 2020) show that such a setup\nimproves hierarchical action recognition, while (Liu et al.,\n2020) use hierarchies with hyperbolic embeddings for zero-\nshot learning. Deep visual learning with hyperbolic tree\nembeddings has furthermore shown to improve image seg-\nmentation (Ghadimi Atigh et al., 2022), skin lesion recogni-\ntion (Yu et al., 2022b), video understanding (Li et al., 2024),\nhierarchical visual recognition (Ghadimi Atigh et al., 2021;\nDhall et al., 2020), hierarchical model interpretation (Gul-\nshad et al., 2023), open set recognition (Dengxiong & Kong,\n2023), continual learning (Gao et al., 2023), few-shot learn-\ning (Zhang et al., 2022), and more. Since such approaches\nrequire freedom in terms of embedding dimensionality, they\ncommonly rely on optimization-based approaches to embed\nthe prior tree-like knowledge. Similar approaches have also\nbeen investigated in other domains, from audio (Petermann\net al., 2023) and text (Dhingra et al., 2018; Le et al., 2019)\nto multimodal settings (Hong et al., 2023). In this work, we\nprovide a general-purpose and unconstrained approach for\nlow-distortion embeddings with the option to scale to higher\nprecisions without losing GPU-compatibility.\nFloating point expansions.\nFloating point expansions\n(FPEs) to increase precision in hyperbolic space was pro-\nposed by (Yu & De Sa, 2021) and implemented in a PyTorch\nlibrary (Yu et al., 2022a). However, their methodology is\nbased on older FPE arithmetic definitions and routines by\n(Priest, 1991; 1992; Richard Shewchuk, 1997). In the field\nof FPEs, more efficient and stable formulations have been\nproposed over the years with improved error guarantees\n(Joldes et al., 2014; 2015; Muller et al., 2016). In this pa-\nper, we build upon the most recent arithmetic framework\ndetailed in (Popescu, 2017). We have implemented this\nframework for PyTorch and extend its functionality to work\nwith hyperbolic embeddings.\n3. HS-DTE\nSetting and objective.\nWe are given a (possibly weighted)\ntree T = (V, E), where the nodes in V contain the concepts\nof our hierarchy and the edges in E represent parent-child\nconnections. The goal is to find an embedding ϕ : V →\nDn that accurately captures the semantics of the tree T,\nso where T can be accurately reconstructed from ϕ(V ).\nAn embedding ϕ is evaluated by first defining the graph\nmetric dT (u, v) on the tree as the length of the shortest path\nbetween the nodes u and v and then checking how much\nϕ distorts this metric. More specifically, for evaluation we\nuse the average relative distortion (Sala et al., 2018), the\nworst-case distortion (Sarkar, 2011) and the mean average\nprecision (Nickel & Kiela, 2017). Further details on these\nmetrics can be found in Appendix B.\nConstructive solution for hyperbolic embeddings.\nThe\nstarting point of our method is the Poincar´e ball implemen-\ntation of Sarkar’s combinatorial construction (Sarkar, 2011)\nas outlined by (Sala et al., 2018). A generalized formulation\nof this approach is outlined in Algorithm 1. The scaling\nfactor τ > 0 is used to scale the tree metric dT . A larger τ\nallows for a better use of the curvature of hyperbolic space,\ntheoretically making it easier to find strong embeddings.\nLower values can help avoid numerical issues that arise near\nthe boundary of the Poincar´e ball. When the dimension of\nthe embedding space satisfies n ≤log(degmax) + 1 and the\nscaling factor is set to\nτ = 1 + ϵ\nϵ\nlog\n\u0010\n4 degmax\n1\nn−1\n\u0011\n,\n(7)\nwith degmax the maximal degree of T, then the construction\nleads to a worst-case distortion bounded by 1 + ϵ, given\nthat the points on the hypersphere are sufficiently uniformly\ndistributed (Sala et al., 2018). When the dimension is n >\nlog(degmax)+1, the scaling factor should be τ = Ω(1), so it\ncan no longer be reduced by choosing a higher dimensional\nembedding space (Sala et al., 2018). The number of bits\nrequired for the construction is O( 1\nϵ\nℓ\nn log(degmax)) when\nn ≤log(degmax)+1 and O( ℓ\nϵ) when n > log(degmax)+1,\nwhere ℓis the longest path in the tree.\nThe difficulty of distributing points on a hypersphere.\nThe construction in Algorithm 1 provides a nice way of con-\nstructing embeddings in n-dimensional hyperbolic space\nwith arbitrarily low distortion. However, the bound on\nthe distortion for the τ in Equation 7 is dependent on our\nability to generate uniformly distributed points on the n-\ndimensional hypersphere. More specifically, given gener-\nated points x1, . . . , xdegmax, the error bound relies on the\nassumption that\nmin\ni̸=j sin ∠(xi, xj) ≥degmax\n−\n1\nn−1 .\n(8)\n3\n\nLow-distortion and GPU-compatible Tree Embeddings in Hyperbolic Space\nAlgorithm 1 Generalized Sarkar’s Dalaunay tree embedding\n1: Input: Tree T = (V, E) and scaling factor τ > 0.\n2: for v ∈V do\n3:\np ←parent(v)\n4:\nc1, . . . , cdeg(v)−1 ←children(v)\n5:\nReflect ϕ(p) with Rϕ(v)→0\n6:\nGenerate x1, . . . , xdeg(v) uniformly distributed points\non a hypersphere with radius 1\n7:\nGet rotation matrix A such that Rϕ(v)→0\n\u0000ϕ(p)\n\u0001\nis\naligned with Axdeg(v) and rotate\n8:\nScale points by γ = eτ −1\neτ +1\n9:\nReflect rotated and scaled points back: ϕ(ci) ←\nRϕ(v)→0(γAxi),\ni = 1, . . . , deg(v) −1\n10: end for\nWhile this assumption can theoretically always be achieved\n(Sala et al., 2018), generating points that actually satisfy it\nis not straightforward. In practice it is important to keep\nthe scaling factor τ as small as possible, since the required\nnumber of bits increases linearly with τ. Increasing the\nminimal angle beyond the condition in Equation 8 allows\nfor a smaller τ. This problem of maximizing the minimal\npairwise angle is commonly known as the Tammes problem\n(Tammes, 1930; Mettes et al., 2019) and it has been studied\nextensively, with many analytical and approximate solutions\ngiven for various specific combinations of dimensions n\nand number of points that have to be placed (Cohn, 2024).\nHowever, there exists no general closed-form solution that\nencompasses all possible combinations.\n(Sala et al., 2018) propose to generate points by placing\nthem on the vertices of an inscribed hypercube. This ap-\nproach comes with three limitations. First, the maximum\nnumber of points that can be generated with this method is\n2n, which is limited for small n. Second, for most config-\nurations this method results in a sub-optimal distribution,\nleading to an unnecessarily high requirement on τ. Third,\nthis method depends on finding binary sequences of length n\nwith maximal Hamming distances (see Appendix C), which\nis in general not an easy problem to solve. Their solution\nis to use the Hadamard code. This can only be used when\nthe dimension is a power of 2 and at least degmax; a severe\nrestriction, often incompatible with downstream tasks.\nDelaunay tree embeddings with separation.\nWe propose\nto improve the construction by distributing the points on\nthe hypersphere in step 6 of Algorithm 1 through optimiza-\ntion. Specifically, we use projected gradient descent to find\nx1, . . . , xk ∈Sn−1 such that\nx1, . . . , xk =\narg min\nw1,...,wk ∈Sn−1 L(w1, . . . , wk),\n(9)\nwhere L : (Sn−1)k →R is some objective function. Com-\nmon choices for this objective are the hyperspherical energy\nfunctions (Liu et al., 2018), given by\nEs(w1, . . . , wk) =\n\n\n\n\n\n\n\n\n\nkP\ni=1\nP\nj̸=i\n||wi −wj||−s,\ns > 0,\nkP\ni=1\nP\nj̸=i\nlog\n\u0000||wi −wj||−1\u0001\n,\ns = 0,\n(10)\nwhere s is a nonnegative integer parameterizing this set of\nfunctions. Minimizing these objective functions pushes the\nhyperspherical points apart, leading to a more uniform dis-\ntribution. However, these objectives are aimed at finding a\nlarge mean pairwise angle, allowing for the possibility of\nhaving a small minimum pairwise angle. Having a small\nminimum pairwise angle leads to the corresponding nodes\nand their descendants being placed too close together, lead-\ning to large distortion, as shown in the experiments. There-\nfore, we advocate the minimal angle maximization (MAM)\nobjective, aimed at maximizing this minimal angle\nE(w1, . . . , wk) = −\nk\nX\ni=1\nmin\nj̸=i ∠(wi, wj),\n(11)\nwhich pushes each wi away from its nearest neighbour\nand essentially optimizes directly for the objective of the\nTammes problem. We find that this method results in strong\nseparation when compared to highly specialized existing\nmethods used for specific cases of the Tammes problem\n(Cohn, 2024). More importantly, it leads to better separa-\ntion than the method used in current hyperbolic embeddings\n(Sala et al., 2018), allowing for the use of a smaller τ. More-\nover, this optimization method places no requirements on\nthe dimension, making it a suitable choice for downstream\ntasks. We refer to the resulting construction as the highly\nseparated Delaunay tree embedding (HS-DTE).\nWhen performing the construction using MAM, the output\nof the optimization can be cached and reused each time\na node with the same degree is encountered. Using this\napproach, the worst-case number of optimizations that has\nto be performed is O(\n√\nN) as shown by Theorem 3.1.\nTheorem 3.1. The worst-case number of optimizations p\nthat has to be performed when embedding a tree with the\ncombinatorial construction in Algorithm 1 with any objec-\ntive using caching is\np ≤\nl1\n2(1 +\n√\n16N −15)\nm\n.\n(12)\nProof. See Appendix D.\nIn practice we find the number of optimizations to be lower\ndue to frequent occurrence of low degree nodes for which\ncached points can be used, as shown in Appendix L.\nMAM optimization details. MAM is an easily optimizable\nobjective, that we train using projected gradient descent for\n4\n\nLow-distortion and GPU-compatible Tree Embeddings in Hyperbolic Space\n450 iterations with a learning rate of 0.01, reduced by a\nfactor of 10 every 150 steps, for every configuration. This\noptimization can generally be performed in mere seconds\nwhich, if necessary, can be further optimized through hyper-\nparameter configurations, early stopping, parallelization or\nhardware acceleration. As a result, the increase in compu-\ntation time of our method compared to (Sala et al., 2018)\nis minimal. Moreover, when compared to methods such\nas Poincar´e embeddings (Nickel & Kiela, 2017) which use\nstochastic gradient descent to directly optimize the embed-\ndings, we find that our method is orders of magnitude faster,\nwhile avoiding the need for costly hyperparameter tuning.\n4. HypFPE: High-precision GPU-compatible\nhyperbolic embeddings\nWhile hyperbolic space enjoys numerous potential bene-\nfits, it is prone to numerical error when using floating point\narithmetic. Especially as points move away from the origin,\nfloating point arithmetic struggles to accurately represent or\nperform computations with these points. For larger values\nof τ or maximal path lengths ℓ, the embeddings generated\nby the construction often end up in this problematic region\nof the Poincar´e ball. As such, the precision required for\nhyperbolic embeddings is often larger than the precision\nprovided by the floating point formats supported on GPUs.\nIncreased precision can be attained by switching to arbi-\ntrary precision arithmetic. However, this makes the result\nincompatible with existing deep learning libraries.\nHere, we propose HypFPE, a method to increase the preci-\nsion of constructive hyperbolic approaches through floating\npoint expansion (FPE) arithmetic. In this framework, num-\nbers are represented as unevaluated sums of floating point\nnumbers, typically of a fixed number of bits b. In other\nwords, a number f ∈R is represented by a floating point\nexpansion ˜f as\nf ≈˜f =\nt\nX\ni=1\n˜fi,\n(13)\nwhere the ˜fi are floating point numbers with a fixed number\nof bits and where t is the number of terms that the floating\npoint expansion ˜f consists of. Each term ˜fi is in the form\nof a GPU supported float format, such as float16, float32\nor float64. Moreover, to ensure that this representation is\nunique and uses bits efficiently, it is constrained to be ulp-\nnonoverlapping (Popescu, 2017).\nDefinition 4.1. A floating point expansion ˜f = ˜f1+. . .+ ˜ft\nis ulp-nonoverlapping if for all 2 ≤i ≤t, | ˜fi| ≤ulp( ˜fi−1),\nwhere ulp( ˜fi−1) is the unit in the last place of ˜fi−1.\nA ulp-nonoverlapping FPE consisting of t terms each with\nb bits precision has at worst t(b −1) + 1 bits of precision,\nsince exactly t −1 overlapping bits can occur. The corre-\nsponding arithmetic requires completely different routines\nfor computing basic operations, many of which have been\nintroduced by (Joldes et al., 2014; 2015; Muller et al., 2016;\nPopescu, 2017). An overview of these routines can be found\nin Appendix N. For an overview of the error guarantees we\nrefer to (Popescu, 2017). Each of these routines can be de-\nfined using ordinary floating point operations that exist for\ntensors in standard tensor libraries such as PyTorch, which\nare completely GPU compatible. Here, we have general-\nized all routines to tensor operations and implemented them\nin PyTorch by adding an extra dimension to each tensor\ncontaining the terms of the floating point expansion.\nApplying FPEs to the construction.\nIn the constructive\nmethod the added precision is warranted whenever numer-\nical errors lead to large deviations with respect to the hy-\nperbolic metric. In other words, if we have some x ∈Dn\nand its floating point representation ˜x, then it makes sense\nto increase the precision if\ndD(x, ˜x) ≫0.\n(14)\nFor the Poincar´e ball, this is the case whenever x lies some-\nwhere close to the boundary of the ball. In our construction,\nthis means that generation and rotation of points on the\nunit hypersphere can be performed in normal floating point\narithmetic, since the representation error in terms of dD will\nbe negligible. However, for large τ, the scaling of the hy-\npersphere points and the hyperspherical inversion require\nincreased precision as these map points close to the bound-\nary of Dn. Specifically, steps 5, 8 and 9 of Algorithm 1 may\nrequire increased precision. Note that these operations can\nbe performed using the basic operation routines shown in\nAppendix N. From the basic operations, more complicated\nnonlinear operations can be defined through the Taylor se-\nries approximations that are typically used for floating point\narithmetic. To compute the distortion of the resulting em-\nbeddings, the distances between the embedded nodes must\nbe computed either through the inverse hyperbolic cosine\nformulation of Equation 2 or through the inverse hyper-\nbolic tangent formulation of Equation 3. We show how to\naccurately compute distances using either formulation.\n4.1. The inverse hyperbolic cosine formulation\nFor Equation 2, normal floating point arithmetic may cause\nthe denominator inside the argument of cosh−1 to become 0\ndue to rounding. To solve this, we can use FPE arithmetic to\ncompute the argument of cosh−1 and then approximate the\ndistance by applying cosh−1 to the largest magnitude term\nof the FPE. This allows accurate computation of distances\neven for points near the boundary of the Poincar´e ball, as\nshown by Theorem 4.2 and Proposition 4.3.\nTheorem 4.2. Given x, y ∈Dn with ||x|| < 1 −ϵt−1 and\n||y|| < 1 −ϵt−1, an approximation d to equation 2 can be\n5\n\nLow-distortion and GPU-compatible Tree Embeddings in Hyperbolic Space\ncomputed with FPE representations with t terms and with a\nlargest magnitude approximation to cosh−1 such that, for\nsome small ϵ∗> 0,\n\f\f\f\fd−cosh−1\n\u0012\n1+2\n||x −y||2\n(1 −||x||2)(1 −||y||2)\n\u0013\f\f\f\f < ϵ∗. (15)\nProof. See Appendix E.\nProposition 4.3. The range of the inverse hyperbolic tan-\ngent formulation increases linearly in the number of terms t\nof the FPEs being used.\nProof. See Appendix F.\nTheorem 4.2 shows that we can accurately compute dis-\ntances on a larger domain than with normal floating point\narithmetic. Proposition 4.3 shows that the effective radius\nof the Poincar´e ball in which we can represent points and\ncompute distances increases linearly in the number of terms\nof our FPE expansions. Therefore, this effective radius in-\ncreases linearly with the number of bits. The same holds\nfor arbitrary precision floating point arithmetic, so FPE ex-\npansions require a similar number of bits for constructive\nmethods as arbitrary precision floating point arithmetic.\n4.2. The inverse hyperbolic tangent formulation\nFor Equation 3, the difficulty lies in the computation of\ntanh−1.\nWith normal floating point arithmetic, due to\nrounding errors, this function can only be evaluated on\n[−1 + ϵ, 1 −ϵ], where ϵ is the machine precision. This\nseverely limits the range of values, i.e., distances, that we\ncan compute. Therefore, we need to be able to compute the\ninverse hyperbolic tangent with FPEs. Inspired by (Felker\n& musl Contributors, 2024), we propose a new routine for\nthis computation, given in Algorithm 13 of Appendix N.\nHere, we approximate the logarithm in steps 7 and 9 as\nlog( ˜f) ≈log( ˜f1), which is accurate enough for our pur-\nposes. This algorithm can be used to accurately approximate\ntanh−1 while extending the range linearly in the number of\nterms t as shown by Theorem 4.4 and Proposition 4.5.\nTheorem 4.4. Given a ulp-nonoverlapping FPE x =\nPt\ni=1 xi ∈[−1 + ϵt−1, 1 −ϵt−1] consisting of floating\npoint numbers with a precision b > t, Algorithm 13 leads to\nan approximation y of the inverse hyperbolic tangent of x\nthat, for small ϵ∗> 0, satisfies\n|y −tanh−1(x)| ≤ϵ∗.\n(16)\nProof. See Appendix G.\nProposition 4.5. The range of algorithm 13 increases lin-\nearly in the number of terms t.\nProof. See Appendix H.\nMethod\ndim\nDave\nDwc\nMAP\n(Sala et al., 2018) ‡\n8\n0.734\n1143\n0.154\n(Sala et al., 2018) ⋆\n10\n0.361\n18.42\n0.998\nE0\n10\n0.219\n1.670\n1.000\nE1\n10\n0.204\n1.686\n1.000\nE2\n10\n0.190\n1.642\n1.000\nMAM\n10\n0.188\n1.635\n1.000\nTable 1. Comparing hyperspherical separation methods for the\nconstructive hyperbolic embedding of a binary tree with a depth\nof 8 edges when using float32 representations (24 bits precision)\nin 10 dimensions. Note that the Hadamard method (‡) cannot be\napplied in 10 dimensions, so there 8 is used instead.\nBased on these results, either formulation could be a good\nchoice for computing distances with FPEs. In practice, we\nfind that the tanh−1 formulation leads to larger numerical\nerrors, which is likely due to catastrophic cancellation errors\nin the dot product that is performed in Equation 4. Therefore,\nwe use the cosh−1 formulation in our experiments.\n5. Experiments\n5.1. Ablations\nMinimal angle maximization.\nTo test how well the pro-\nposed methods for hyperspherical separation perform, we\ngenerate points w1, . . . , wk on an 8-dimensional hyper-\nsphere for various numbers of points k and compute the\nminimal pairwise angle mini̸=j ∠(wi, wj). We compare to\nthe Hadamard generation method from (Sala et al., 2018)\nand the method that is used in their implementation, which\nprecomputes 1000 points using the method from (Lovisolo\n& Da Silva, 2001) and samples from these precomputed\npoints. Note that a power of 2 is chosen for the dimension\nto be able to make a fair comparison to the Hadamard con-\nstruction, since this method cannot be used otherwise. The\nresults are shown in Figure 1a. These results show that our\nMAM indeed leads to high separation in terms of the min-\nimal pairwise angle, that the precomputed approach leads\nto poor separation and that the Hadamard method only per-\nforms moderately well when the number of points required\nis close to the dimension of the space.\nTo verify that this minimal pairwise angle is important for\nthe quality of the construction, we perform the construc-\ntion on a binary tree with a depth of 8 edges using each\nof the hypersphere generation methods. The construction\nis performed in 10 dimensions except for the Hadamard\nmethod, since this cannot generate 10 dimensional points.\nAdditional results for dimensions 4, 7 and 20 are shown in\nAppendix I. Each method is applied using float32 represen-\ntations and a scaling factor of τ = 1.33. The results are\nshown in Table 1. These findings support our hypothesis\nthat the minimal pairwise angle is important for generating\nhigh quality embeddings and that the MAM is an excellent\nobjective function for performing the separation.\n6\n\nLow-distortion and GPU-compatible Tree Embeddings in Hyperbolic Space\n(a) Minimal hyperspherical energy ablation.\n(b) Floating point expansion ablation.\nFigure 1. Ablation studies on our construction and floating point expansion. (a) Minimal\npairwise angle (↑) of the hyperspherical points generated in step 6 of Algorithm 1 using the\nvarious generation methods. The dimension of the space is set to 8, so the Hadamard method\ncannot generate more than 8 points. The MAM objective consistently leads to a higher\nseparation angle. (b) The worst-case distortion (↓, Dwc) of the constructed embedding\nof the phylogenetic tree with the maximal admissable τ given the number of bits. The\nvertical dashed line shows the limit with standard GPU floating point formats (float64). The\nhorizontal dashed line is the best possible result Dwc = 1. FPE representations are required\nto get high quality embeddings without losing GPU-compatibility.\nFigure 2. Pairwise relative distortions\nof h-MDS (top) and HS-DTE (bottom)\napplied to the 5-ary tree with a scaling\nfactor τ = 5.0. Axes are ordered using a\nbreadth-first search of the tree.\nFPEs versus standard floating points.\nTo demonstrate\nthe importance of using FPEs for increasing precision,\nwe perform the construction on a phylogenetic tree ex-\npressing the genetic heritage of mosses in urban envi-\nronments (Hofbauer et al., 2016), made available by\n(Sanderson et al., 1994), using various precisions. This tree\nhas a maximum path length ℓ= 30, which imposes sharp\nrestrictions on the value of τ that we can choose before\nencountering numerical errors. We perform the construction\neither with normal floating point arithmetic using the usual\nGPU-supported float formats or with FPEs, using multiple\nfloat64 terms. The scaling factor τ is chosen to be close to\nthe threshold where numerical problems appear in order to\nobtain optimal results for the given precision. The results\nin terms of Dwc are shown in Figure 1b. As can be seen\nfrom these results, around 100 bits of precision are needed\nto obtain decent results, which can be achieved using FPEs\nwith 2 float64 terms. Without FPE expansions, the largest\nGPU-compatible precision is 53 bits, obtained by using\nfloat64. This precision yields a Dwc of 9.42, which is quite\npoor. These results illustrate the importance of FPEs for\nhigh quality GPU-compatible embeddings.\n5.2. Embedding complete m-ary trees\nTo demonstrate the strong performance of the combinato-\nrial constructions compared to other methods, we perform\nembeddings on several complete m-ary trees with a max\npath length of ℓ= 8 and branching factors m = 3, 5, 7.\nDue to the small ℓ, each experiment can be performed with\nnormal floating point arithmetic using float64 representa-\ntions. We compare our method with Poincar´e embeddings\n(PE) (Nickel & Kiela, 2017), hyperbolic entailment cones\n(HEC) (Ganea et al., 2018), distortion optimization (DO)\n(Sala et al., 2018; Yu et al., 2022b), h-MDS (Sala et al.,\n2018) and the combinatorial method with Hadamard (Sala\net al., 2018) or precomputed hyperspherical points (Lovi-\nsolo & Da Silva, 2001). For the constructive methods and\nfor h-MDS, a larger scaling factor improves performance,\nso we use τ = 5. For DO we find that increasing the scaling\nfactor does not improve performance, so we use τ = 1.0.\nPE and HEC are independent of the scaling factor.\nThe results on the various trees in 10 dimensions are shown\nin table 2 and additional results for dimensions 4, 7 and 20\nare shown in Appendix J. These illustrate the strength of\nthe combinatorial constructions. The optimization methods\nPE, HEC and DO perform relatively poor for all evalua-\ntion metrics. This performance could be increased through\nhyperparameter tuning and longer training. However, the\nresults will not come close to those of the other methods.\nThe h-MDS method performs well in terms of Dave, but\nvery poorly on Dwc and MAP. This is because h-MDS col-\nlapses leaf nodes, leading to massive local distortion within\nthe affected subtrees. However, between subtrees this dis-\ntortion is much smaller, explaining the low Dave. Figure\n2 illustrates the issue with h-MDS and the superiority of\nour approach. Each of the white squares in the h-MDS plot\ncorresponds to a collapsed subtree, which renders the em-\nbeddings unusable for downstream tasks since nearby leaf\nnodes cannot be distinguished. We conclude that HS-DTE\nobtains the strongest embeddings overall.\n7\n\nLow-distortion and GPU-compatible Tree Embeddings in Hyperbolic Space\n3-tree\n5-tree\n7-tree\nDave\nDwc\nMAP\nDave\nDwc\nMAP\nDave\nDwc\nMAP\n(Nickel & Kiela, 2017)\n0.17\n169\n0.8\n0.31\nNaN\n0.58\n0.84\nNaN\n0.24\n(Ganea et al., 2018)\n0.51\n184\n0.27\n0.81\n604\n0.24\n0.96\n788\n0.15\n(Yu et al., 2022b)\n0.16\n31.9\n0.57\n0.52\n545\n0.30\n0.93\n3230\n0.05\n(Sala et al., 2018) †\n0.03\nNaN\n0.52\n0.04\nNaN\n0.1\n0.03\nNaN\n0.05\n(Sala et al., 2018) ‡\n0.11\n1.14\n1.00\n0.12\n1.14\n1.00\n0.12\n1.14\n1.00\n(Sala et al., 2018) ⋆\n0.09\n1.18\n1.00\n0.13\n1.30\n1.00\n0.13\n1.31\n1.00\nHS-DTE\n0.06\n1.07\n1.00\n0.09\n1.09\n1.00\n0.10\n1.12\n1.00\nTable 2. Comparison of hyperbolic embedding algorithms on m-ary trees with a maximum path length of ℓ= 8. The h-MDS method\nis represented by †. The ‡ method is the combinatorial construction with the hyperspherical points being generated using the Hadamard\nconstruction, whereas the ⋆method samples hyperspherical points from the precomputed points generated with the hyperspherical\nseparation method from (Lovisolo & Da Silva, 2001). The h-MDS method outperforms the other methods in terms of Dave, but collapses\nnodes, leading to NaN values of the Dwc and making the embeddings unusable. HS-DTE has the second best Dave and outperforms all\nmethods in terms of Dwc. Each combinatorial construction has a perfect MAP.\nPrecision\nMosses\nWeevils\nCarnivora\nLichen\nbits\nDave\nDwc\nDave\nDwc\nDave\nDwc\nDave\nDwc\n(Nickel & Kiela, 2017)\n53\n0.68\n44350\n0.45\nNaN\n0.96\nNaN\n151\nNaN\n(Ganea et al., 2018)\n53\n0.90\n1687\n0.77\n566\n0.99\nNaN\n162\nNaN\n(Yu et al., 2022b)\n53\n0.83\n163\n0.57\n79.8\n0.99\nNaN\n-\n-\n(Sala et al., 2018) †\n53\n0.04\nNaN\n0.06\nNaN\n0.11\nNaN\n0.13\nNaN\n(Sala et al., 2018) ‡\n53\n-\n-\n0.79\n330\n0.26\n35.2\n0.49\n79.6\n(Sala et al., 2018) ⋆\n53\n0.78\n122\n0.54\n34.3\n0.23\n18.8\n0.55\n101\nHS-DTE\n53\n0.40\n9.42\n0.27\n2.03\n0.12\n11.7\n0.30\n23.5\nHypFPE + (Sala et al., 2018) ‡\n417\n-\n-\n0.07\n1.09\n0.05\n6.76\n0.12\n43.4\nHypFPE + (Sala et al., 2018) ⋆\n417\n0.08\n1.14\n0.05\n1.11\n0.03\n4.87\n0.11\n6.42\nHypFPE + HS-DTE\n417\n0.04\n1.06\n0.03\n1.04\n0.03\n2.03\n0.05\n3.30\nTable 3. Comparison of hyperbolic embedding algorithms on various trees. † represents h-MDS, ‡ the construction with Hadamard\nhyperspherical points and ⋆the construction with points sampled from a set precomputed with (Lovisolo & Da Silva, 2001). The best\nfloat64 performance is underlined and the best FPE performance is in bold. All embeddings are performed in a 10-dimensional space.\nHadamard generation cannot be used for the mosses tree, since it has a degmax greater than 8. Distortion optimization (Yu et al., 2022b)\ndoes not converge for the lichen tree due to large variation in edge weights. Overall, combining HypFPE and HS-DTE works best.\n5.3. Embedding phylogenetic trees\nLastly, we compare hyperbolic embeddings on phyloge-\nnetic trees. Moreover, we show how adding HypFPE to\nour method and the other combinatorial methods increases\nthe embedding quality when requiring GPU-compatibility.\nThe phylogenetic trees describe mosses (Hofbauer et al.,\n2016), weevils (Marvaldi et al., 2002), the European car-\nnivora (Roquet et al., 2014), and lichen (Zhao et al., 2016),\nobtained from (McTavish et al., 2015). The latter two trees\nare weighted trees which can be embedded by adjusting the\nscaling in step 8 of Algorithm 1. Each of the embeddings is\nperformed in 10-dimensional space. Other dimensions are\ngiven in Appendix K. The h-MDS method and the combina-\ntorial constructions are performed with the largest τ that can\nbe used with the given precision. The results are shown in\nTable 3. When using float64, HS-DTE outperforms each of\nthe optimization-based methods and the other combinatorial\napproaches from (Sala et al., 2018). While h-MDS obtains\nhigh average distortion, it collapses entire subtrees, leading\nto massive local distortion. Therefore, the HS-DTE embed-\ndings are of the highest quality. When adding HypFPE on\ntop of the combinatorial approaches, all performances go\nup, with the combination of HS-DTE and HypFPE leading\nto the best performance on both Dave and Dwc. Additional\nresults on graph-like data are shown in Appendix M.\n6. Conclusion\nIn this paper we introduce HS-DTE, a novel way of construc-\ntively embedding trees in hyperbolic space, which uses an\noptimization approach to effectively separate points on a hy-\npersphere. Empirically, we show that HS-DTE outperforms\nexisting methods, while maintaining the computational effi-\nciency of the combinatorial approaches. We also introduce\nHypFPE, a framework for floating point expansion arith-\nmetic on tensors, which is adapted to extend the effective\nradius of the Poincar´e ball. This framework can be used\nto increase the precision of computations, while benefiting\nfrom hardware acceleration, paving the way for highly accu-\nrate hyperbolic neural networks. It can be added on top of\nany of the combinatorial methods, leading to low-distortion\nand GPU-compatible hyperbolic tree embeddings.\n8\n\nLow-distortion and GPU-compatible Tree Embeddings in Hyperbolic Space\nAcknowledgements\nMax van Spengler acknowledges the University of Amster-\ndam Data Science Centre for financial support.\nReferences\nAbraham, I., Balakrishnan, M., Kuhn, F., Malkhi, D., Rama-\nsubramanian, V., and Talwar, K. Reconstructing approx-\nimate tree metrics. In Proceedings of the twenty-sixth\nannual ACM symposium on Principles of distributed com-\nputing, pp. 43–52, 2007.\nAnderson, J. W. Hyperbolic Geometry. Springer Undergrad-\nuate Mathematics Series. Springer, London, 2nd edition,\n2005. ISBN 978-1-85233-934-0.\nCannon, J. W., Floyd, W. J., Kenyon, R., Parry, W. R., et al.\nHyperbolic geometry. Flavors of geometry, 31(59-115):\n2, 1997.\nChami, I., Gu, A., Chatziafratis, V., and R´e, C. From trees\nto continuous embeddings and back: Hyperbolic hierar-\nchical clustering. Advances in Neural Information Pro-\ncessing Systems, 33:15065–15076, 2020.\nCohn, H. Table of spherical codes, 2024. URL https:\n//hdl.handle.net/1721.1/153543.\nDe Nooy, W., Mrvar, A., and Batagelj, V. Exploratory so-\ncial network analysis with Pajek: Revised and expanded\nedition for updated software, volume 46. Cambridge\nuniversity press, 2018.\nDengxiong, X. and Kong, Y. Ancestor search: General-\nized open set recognition via hyperbolic side information\nlearning. In Proceedings of the IEEE/CVF Winter Confer-\nence on Applications of Computer Vision, pp. 4003–4012,\n2023.\nDesai, K., Nickel, M., Rajpurohit, T., Johnson, J., and\nVedantam, S. R. Hyperbolic image-text representations.\nIn International Conference on Machine Learning, pp.\n7694–7731. PMLR, 2023.\nDhall, A., Makarova, A., Ganea, O., Pavllo, D., Greeff,\nM., and Krause, A. Hierarchical image classification\nusing entailment cone embeddings. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern\nrecognition workshops, pp. 836–837, 2020.\nDhingra, B., Shallue, C., Norouzi, M., Dai, A., and Dahl,\nG. Embedding text in hyperbolic spaces. In Proceedings\nof the Twelfth Workshop on Graph-Based Methods for\nNatural Language Processing, pp. 59–69, 2018.\nFelker, R. and musl Contributors. musl libc: A lightweight\nimplementation of the standard library for linux systems,\n2024.\nURL https://musl.libc.org.\nVersion\n1.2.5, retrieved on September 30, 2024.\nFreeman, L. The development of social network analysis. A\nStudy in the Sociology of Science, 1(687):159–167, 2004.\nGanea, O., B´ecigneul, G., and Hofmann, T. Hyperbolic\nentailment cones for learning hierarchical embeddings. In\nInternational conference on machine learning, pp. 1646–\n1655. PMLR, 2018.\nGao, Z., Xu, C., Li, F., Jia, Y., Harandi, M., and Wu, Y.\nExploring data geometry for continual learning. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pp. 24325–24334, 2023.\nGhadimi Atigh, M., Keller-Ressel, M., and Mettes, P. Hyper-\nbolic busemann learning with ideal prototypes. Advances\nin neural information processing systems, 34:103–115,\n2021.\nGhadimi Atigh, M., Schoep, J., Acar, E., Van Noord, N.,\nand Mettes, P. Hyperbolic image segmentation. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pp. 4453–4462, 2022.\nGoh, K.-I., Cusick, M. E., Valle, D., Childs, B., Vidal,\nM., and Barab´asi, A.-L. The human disease network.\nProceedings of the National Academy of Sciences, 104\n(21):8685–8690, 2007.\nGulshad, S., Long, T., and van Noord, N. Hierarchical\nexplanations for video action recognition. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 3703–3708, 2023.\nHofbauer, W. K., Forrest, L. L., Hollingsworth, P. M., and\nHart, M. L. Preliminary insights from dna barcoding\ninto the diversity of mosses colonising modern building\nsurfaces. Bryophyte Diversity and Evolution, 38(1):1–22,\n2016.\nHong, J., Hayder, Z., Han, J., Fang, P., Harandi, M., and\nPetersson, L. Hyperbolic audio-visual zero-shot learning.\nIn Proceedings of the IEEE/CVF international conference\non computer vision, pp. 7873–7883, 2023.\nJoldes, M., Muller, J.-M., and Popescu, V. On the com-\nputation of the reciprocal of floating point expansions\nusing an adapted newton-raphson iteration. In 2014 IEEE\n25th International Conference on Application-Specific\nSystems, Architectures and Processors, pp. 63–67. IEEE,\n2014.\nJoldes, M., Marty, O., Muller, J.-M., and Popescu, V. Arith-\nmetic algorithms for extended precision using floating-\npoint expansions. IEEE Transactions on Computers, 65\n(4):1197–1210, 2015.\n9\n\nLow-distortion and GPU-compatible Tree Embeddings in Hyperbolic Space\nKapli, P., Yang, Z., and Telford, M. J. Phylogenetic tree\nbuilding in the genomic age. Nature Reviews Genetics,\n21(7):428–444, 2020.\nKhrulkov, V., Mirvakhabova, L., Ustinova, E., Oseledets,\nI., and Lempitsky, V. Hyperbolic image embeddings. In\nProceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pp. 6418–6428, 2020.\nKolyvakis, P., Kalousis, A., and Kiritsis, D. Hyperbolic\nknowledge graph embeddings for knowledge base com-\npletion. In The Semantic Web: 17th International Con-\nference, ESWC 2020, Heraklion, Crete, Greece, May\n31–June 4, 2020, Proceedings 17, pp. 199–214. Springer,\n2020.\nLe, M., Roller, S., Papaxanthos, L., Kiela, D., and Nickel,\nM. Inferring concept hierarchies from text corpora via hy-\nperbolic embeddings. arXiv preprint arXiv:1902.00913,\n2019.\nLi, Y.-L., Wu, X., Liu, X., Wang, Z., Dou, Y., Ji, Y., Zhang,\nJ., Li, Y., Lu, X., Tan, J., et al. From isolated islands\nto pangea: Unifying semantic space for human action\nunderstanding. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pp.\n16582–16592, 2024.\nLiu, S., Chen, J., Pan, L., Ngo, C.-W., Chua, T.-S., and\nJiang, Y.-G. Hyperbolic visual embedding learning for\nzero-shot recognition. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npp. 9273–9281, 2020.\nLiu, W., Lin, R., Liu, Z., Liu, L., Yu, Z., Dai, B., and Song,\nL. Learning towards minimum hyperspherical energy.\nAdvances in neural information processing systems, 31,\n2018.\nLong, T., Mettes, P., Shen, H. T., and Snoek, C. G. Search-\ning for actions on the hyperbole. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 1141–1150, 2020.\nLovisolo, L. and Da Silva, E. Uniform distribution of points\non a hyper-sphere with applications to vector bit-plane\nencoding. IEE Proceedings-Vision, Image and Signal\nProcessing, 148(3):187–193, 2001.\nMacWilliams and Sloane. The theory of error-correcting\ncodes. Elsevier Science Publishers BV google schola, 2:\n39–47, 1977.\nMarvaldi, A. E., Sequeira, A. S., O’Brien, C. W., and Far-\nrell, B. D. Molecular and morphological phylogenetics\nof weevils (coleoptera, curculionoidea): do niche shifts\naccompany diversification? Systematic biology, 51(5):\n761–785, 2002.\nMcTavish, E. J., Hinchliff, C. E., Allman, J. F., Brown, J. W.,\nCranston, K. A., Holder, M. T., Rees, J. A., and Smith,\nS. A. Phylesystem: a git-based data store for community-\ncurated phylogenetic estimates. Bioinformatics, 31(17):\n2794–2800, 2015.\nMettes, P., Van der Pol, E., and Snoek, C. Hyperspherical\nprototype networks.\nAdvances in neural information\nprocessing systems, 32, 2019.\nMettes, P., Ghadimi Atigh, M., Keller-Ressel, M., Gu, J.,\nand Yeung, S. Hyperbolic deep learning in computer\nvision: A survey. International Journal of Computer\nVision, pp. 1–25, 2024.\nMiller, G. A. Wordnet: a lexical database for english. Com-\nmunications of the ACM, 38(11):39–41, 1995.\nMuller, J.-M., Popescu, V., and Tang, P. T. P. A new multi-\nplication algorithm for extended precision using floating-\npoint expansions. In 2016 IEEE 23nd Symposium on\nComputer Arithmetic (ARITH), pp. 39–46. IEEE, 2016.\nNickel, M. and Kiela, D. Poincar´e embeddings for learning\nhierarchical representations. Advances in neural informa-\ntion processing systems, 30, 2017.\nNickel, M. and Kiela, D. Learning continuous hierarchies\nin the lorentz model of hyperbolic geometry. In Interna-\ntional conference on machine learning, pp. 3779–3788.\nPMLR, 2018.\nPadial, J. M., Miralles, A., De la Riva, I., and Vences, M.\nThe integrative future of taxonomy. Frontiers in zoology,\n7:1–14, 2010.\nPeng, W., Varanka, T., Mostafa, A., Shi, H., and Zhao, G.\nHyperbolic deep neural networks: A survey. IEEE Trans-\nactions on pattern analysis and machine intelligence, 44\n(12):10023–10044, 2021.\nPetermann, D., Wichern, G., Subramanian, A., and Le Roux,\nJ.\nHyperbolic audio source separation.\nIn ICASSP\n2023-2023 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), pp. 1–5. IEEE,\n2023.\nPopescu, V. Towards fast and certified multiple-precision\nlibrairies. PhD thesis, Universit´e de Lyon, 2017.\nPriest, D. M. Algorithms for arbitrary precision floating\npoint arithmetic. University of California, Berkeley, 1991.\nPriest, D. M. On properties of floating point arithmetics:\nnumerical stability and the cost of accurate computations.\nUniversity of California, Berkeley, 1992.\n10\n\nLow-distortion and GPU-compatible Tree Embeddings in Hyperbolic Space\nRichard Shewchuk, J. Adaptive precision floating-point\narithmetic and fast robust geometric predicates. Discrete\n& Computational Geometry, 18:305–363, 1997.\nRoquet, C., Lavergne, S., and Thuiller, W. One tree to\nlink them all: a phylogenetic dataset for the european\ntetrapoda. PLoS currents, 6, 2014.\nSaff, E. B. and Kuijlaars, A. B. Distributing many points on\na sphere. The mathematical intelligencer, 19:5–11, 1997.\nSala, F., De Sa, C., Gu, A., and R´e, C. Representation trade-\noffs for hyperbolic embeddings. In International Con-\nference on Machine Learning, pp. 4460–4469. PMLR,\n2018.\nSanderson, M. J., Donoghue, M. J., Piel, W., and Eriksson, T.\nTreebase: a prototype database of phylogenetic analyses\nand an interactive tool for browsing the phylogeny of life.\nAmerican Journal of Botany, 81(6):183, 1994.\nSarkar, R. Low distortion delaunay embedding of trees in\nhyperbolic plane. In International Symposium on Graph\nDrawing, pp. 355–366. Springer, 2011.\nSonthalia, R. and Gilbert, A. Tree! i am no tree! i am a low\ndimensional hyperbolic embedding. Advances in Neural\nInformation Processing Systems, 33:845–856, 2020.\nTammes, P. M. L. On the origin of number and arrange-\nment of the places of exit on the surface of pollen-grains.\nRecueil des travaux botaniques n´eerlandais, 27(1):1–84,\n1930.\nTifrea, A., B´ecigneul, G., and Ganea, O.-E. Poincar\\’e\nglove: Hyperbolic word embeddings. arXiv preprint\narXiv:1810.06546, 2018.\nYang, Z., Yang, D., Dyer, C., He, X., Smola, A., and Hovy,\nE. Hierarchical attention networks for document classi-\nfication. In Proceedings of the 2016 conference of the\nNorth American chapter of the association for compu-\ntational linguistics: human language technologies, pp.\n1480–1489, 2016.\nYu, T. and De Sa, C. M. Representing hyperbolic space accu-\nrately using multi-component floats. Advances in Neural\nInformation Processing Systems, 34:15570–15581, 2021.\nYu, T., Guo, W., Li, J. C., Yuan, T., and De Sa,\nC. Mctensor: A high-precision deep learning library\nwith multi-component floating-point.\narXiv preprint\narXiv:2207.08867, 2022a.\nYu, Z., Nguyen, T., Gal, Y., Ju, L., Chandra, S. S., Zhang, L.,\nBonnington, P., Mar, V., Wang, Z., and Ge, Z. Skin lesion\nrecognition with class-hierarchy regularized hyperbolic\nembeddings.\nIn International conference on medical\nimage computing and computer-assisted intervention, pp.\n594–603. Springer, 2022b.\nZhang, B., Jiang, H., Feng, S., Li, X., Ye, Y., and Ye, R.\nHyperbolic knowledge transfer with class hierarchy for\nfew-shot learning. In IJCAI, pp. 3723–3729, 2022.\nZhao, X., Leavitt, S. D., Zhao, Z. T., Zhang, L. L., Arup,\nU., Grube, M., P´erez-Ortega, S., Printzen, C., ´Sliwa, L.,\nKraichak, E., et al. Towards a revised generic classifi-\ncation of lecanoroid lichens (lecanoraceae, ascomycota)\nbased on molecular, morphological and chemical evi-\ndence. Fungal Diversity, 78:293–304, 2016.\n11\n\nLow-distortion and GPU-compatible Tree Embeddings in Hyperbolic Space\nA. Geodesic hyperplane reflections\nIn this paper we will make use of reflections in geodesic hyperplanes through the origin to align points on a hypersphere\ncentered at the origin with some existing point on the hypersphere. More specifically, if we have points w, z ∈Dn with\n||w|| = ||z|| and we want to reflect w to z, then we can use a Householder reflection with v =\n(z−w)\n||z−w||, so\nRw→z(y) =\n\u0012\nIn −2(z −w)(z −w)T\n||z −w||2\n\u0013\ny.\n(17)\nTo see that this maps w to z, we can simply enter w into this map to see that\nRw→z(w) =\n\u0012\nIn −2(z −w)(z −w)T\n||z −w||2\n\u0013\nw\n(18)\n= w −\n2(⟨z, w⟩−||w||2)\n||z||2 −2⟨z, w⟩+ ||w||2 (z −w)\n(19)\n= w + ||z||2 −2⟨z, w⟩+ ||w||2 + ||w||2 −||z||2\n||z||2 −2⟨z, w⟩+ ||w||2\n(z −w)\n(20)\n= w +\n\u0012\n1 +\n||w||2 −||z||2\n||z||2 −2⟨z, w⟩+ ||w||2\n\u0013\n(z −w)\n(21)\n= w + z −w = z.\n(22)\nWe make use of reflections in geodesic hyperplanes not through the origin that reflect some given point w ∈Dn to the\norigin. This can be done through reflection in the hyperplane contained in the hypersphere with center m =\nw\n||w||2 and\nradius r =\nq\n1\n||w||2 −1. We easily verify that this hyperspherical inversion maps w to the origin.\nRw→0(w) =\nw\n||w||2 +\n1\n||w||2 −1\n||w −\nw\n||w||2 ||\n\u0010\nw −\nw\n||w||2\n\u0011\n(23)\n=\nw\n||w||2 +\n1 −||w||2\n||w||4 −2||w||2 + 1\n\u0010\n1 −\n1\n||w||2\n\u0011\nw\n(24)\n=\nw\n||w||2 +\n1\n1 −||w||2 · ||w||2 −1\n||w||2\nw\n(25)\n=\nw\n||w||2 −\nw\n||w||2 = 0.\n(26)\nTo show that this is a reflection in a geodesic hyperplane and, therefore, an isometry, we need to show that the hypersphere\ndefined by m and r is orthogonal to the boundary of Dn. This is the case when all the triangles formed by the line segments\nbetween 0, m and any point v in the intersection of the hypersphere and the boundary of Dn are right triangles. This is\nexactly the case when the Pythagorean theorem holds for each of these triangles. For each v we have that ||v|| = 1 and\n||v −m|| = r, so\n||v −0||2 + ||v −m||2 = 1 + r2\n(27)\n=\n1\n||w||2\n(28)\n= ||w||2\n||w||4\n(29)\n= ||m −0||2,\n(30)\nwhich shows that the Pythagorean theorem holds and, thus, that this hyperspherical inversion is a geodesic hyperplane\nreflection, so an isometry.\n12\n\nLow-distortion and GPU-compatible Tree Embeddings in Hyperbolic Space\nB. Evaluation metrics\nWe will use two distortion based evaluation metrics. The first one is the average relative distortion (Sala et al., 2018), given\nas\nDave(ϕ) =\n1\nN(N −1)\nX\nu̸=v\n|dD(ϕ(u), ϕ(v)) −dT (u, v)|\ndT (u, v)\n,\n(31)\nwhere N = |V | is the number of nodes. A low value for this metric is a necessary, but not sufficient condition for a high\nquality embedding, as it still allows for large local distortion. Therefore, we use a second distortion based metric, the\nworst-case distortion (Sarkar, 2011), given by\nDwc(ϕ) = max\nu̸=v\ndD(ϕ(u), ϕ(v))\ndT (u, v)\n\u0012\nmin\nu̸=v\ndD(ϕ(u), ϕ(v))\ndT (u, v)\n\u0013−1\n.\n(32)\nDave ranges from 0 to infinity and Dwc ranges from 1 to infinity, with smaller values indicating strong embeddings. A\nlarge Dave indicates a generally poor embedding, while a large Dwc indicates that at least some part of the tree is poorly\nembedded. Both values should be close to their minimum if an embedding is to be used for a downstream task. Lastly,\nanother commonly used evaluation metric for unweighted trees is the mean average precision (Nickel & Kiela, 2017), given\nby\nMAP(ϕ) = 1\nN\nX\nu∈V\n1\ndeg(u)\nX\nv∈NV (u)\n\f\f\fNV (u) ∩ϕ−1\u0010\nBD(u, v)\n\u0011\f\f\f\n\f\f\fϕ−1\n\u0010\nBD(u, v)\n\u0011\f\f\f\n,\n(33)\nwhere deg(u) denotes the degree of u in T, NV (u) denotes the nodes adjacent to u in V and where BD(u, v) ⊂Dn denotes\nthe closed ball centered at ϕ(u) with hyperbolic radius dD(ϕ(u), ϕ(v)), so which contains v itself. The MAP reflects\nhow well we can reconstruct neighborhoods of nodes while ignoring edge weights, making it less appropriate for various\ndownstream tasks.\nC. Placing points on the vertices of a hypercube\nThe discussion here is heavily based on (Sala et al., 2018). We include it here for completeness. When placing a point on\nthe vertex of an n-dimensional hypercube, there are 2n options, so each option can be represented by a binary sequence of\nlength n. For example, on a hypercube where each vertex v has ||v||∞= 1, each vertex is of the form (±1, . . . , ±1)T , so\nwe can represent v as some binary sequence s. The distance between two such vertices can then be expressed in terms of the\nHamming distance between the corresponding sequences as\nd(v1, v2) =\nq\n4dHamming(s1, s2),\nwhich shows that points placed on vertices of a hypercube are maximally separated if this Hamming distance is maximized.\nThis forms an interesting and well studied problem in coding theory where the objective is to find k binary sequences of\nlength n which have maximal pairwise Hamming distances. There are some specific combinations of n and k for which\noptimal solutions are known, such as the Hadamard code. However, for most combinations of n and k, the solution is still an\nopen problem (MacWilliams & Sloane, 1977). Therefore, properly placing points on the vertices of a hypercube currently\nrelies on the solution to an unsolved problem, making it difficult in practice.\nD. Proof of Theorem 3.1\nProof. For a tree T = (V, E) with N = |V |, we know that the degrees of the vertices satisfy\nX\nv∈V\ndeg(v) = 2|E| = 2(N −1).\n(34)\nSuppose W1, . . . , Wp ⊂Sn−1 are the sets of points on the hypersphere generated by the p optimizations that need to be\nran to perform the construction, then |Wi| ̸= |Wj|, since we use the cached result whenever nodes have the same degree.\nMoreover, |Wi| is equal to the degree of the node for which the points are generated, so\np\nX\ni=1\n|Wi| ≤\nX\nv∈V\ndeg(v) = 2(N −1).\n(35)\n13\n\nLow-distortion and GPU-compatible Tree Embeddings in Hyperbolic Space\nGiven this constraint, the largest possible value of p is when we can fit as many |Wi|’s in this sum as possible, which is\nwhen |W1|, . . . , |Wp| = 1, . . . , p. In that case\np\nX\ni=1\n|Wi| =\np\nX\ni=1\ni = p(p + 1)\n2\n≤2(N −1).\n(36)\nSolving for integer p yields\np ≤\nl1\n2(\n√\n16N −15 −1)\nm\n.\n(37)\nNote that this bound can be sharpened slightly by observing that each node v with deg(v) > 1 forces the existence of\ndeg(v) −1 leaf nodes with degree 1. However, the asymptotic behaviour remains O(\n√\nN).\nE. Proof of Theorem 4.2\nTheorem. Given x, y ∈Dn with ||x|| < 1 −ϵt−1 and ||y|| < 1 −ϵt−1, an approximation d to equation 2 can be computed\nwith FPE representations with t terms and with a largest magnitude approximation to cosh−1 such that\n\f\f\f\fd −cosh−1\n\u0012\n1 + 2\n||x −y||2\n(1 −||x||2)(1 −||y||2)\n\u0013\f\f\f\f < ϵ∗,\n(38)\nfor some small ϵ∗> 0.\nProof. We begin by noting that the accuracy of the largest magnitude approximation to cosh−1 depends on the underlying\nfloating point algorithm used for computing the inverse hyperbolic cosine. While this function cannot be computed up to\nmachine precision on its entire domain due to the large derivative near the lower end of its domain, it can still be computed\nquite accurately, i.e. there exists some small ϵ∗\n1 > 0 such that\n\f\f\f cosh−1(x) −cosh−1(˜x)\n\f\f\f < ϵ∗\n1,\n(39)\nwhere x ∈[1, R], where R is the greatest representable number and ˜x is the floating point approximation to x, so for which\nwe have\n|˜x −x|\n|x|\n< ϵ.\n(40)\nFor example, in PyTorch when using float64, we have ϵ∗\n1 ≈2.107 ∗10−8. If we can approximate the argument inside\ncosh−1 sufficiently accurately, then the largest magnitude approximation will be close enough to guarantee a small error.\nMore specifically, let\nz = 1 + 2\n||x −y||2\n(1 −||x||2)(1 −||y||2),\n(41)\nand let ˜z = ˜z1 + . . . + ˜zt with |˜zi| > |˜zj| for each i ̸= j be the approximation to z obtained through FPE arithmetic. If\n|z −˜z|\n|z|\n= |z −Pt\ni=1 ˜zi|\n|z|\n< 2ϵ,\n(42)\nwhere ϵ is the machine precision of the corresponding floating point format, then\n|z −˜z1| ≤|z −˜z| +\n\f\f\f\nt\nX\ni=2\n˜zi\n\f\f\f\n(43)\n< 2ϵ + 2ϵ|˜z1|\n(44)\n≤4ϵ|z| + 2ϵ|z −˜z1|,\n(45)\n14\n\nLow-distortion and GPU-compatible Tree Embeddings in Hyperbolic Space\nwhere we use that |˜z2| ≤ulp(˜z1) = ϵ|˜z1|, so that | Pt\ni=2 ˜zi| < 2ϵ|˜z1|. Now, we can rewrite to see that\n|z −˜z1|\n|z|\n<\n4ϵ\n1 −2ϵ < 8ϵ.\n(46)\nTherefore, by repeatedly using equation 39, we see that the largest magnitude approximation error is bounded by 16ϵ∗\n1. Our\nability to approximate the argument z as precisely as in equation 42 using FPEs follows from the error bounds of the FPE\narithmetic routines from (Popescu, 2017). This shows that the statement holds for ϵ∗= 16ϵ∗\n1.\nF. Proof of Proposition 4.3\nProposition. The range of the inverse hyperbolic tangent formulation increases linearly in the number of terms t of the FPEs\nbeing used.\nProof. When we use FPEs with t terms, we can represent points x, y ∈Dn such that ||x|| = 1 −ϵt−1 and ||y|| = 1 −ϵt−1.\nIf we set −y = x = (1 −ϵt−1, 0, . . . , 0)T , then\ncosh−1 \u0010\n1 + 2\n||x −y||2\n(1 −||x||2)(1 −||y||2)\n\u0011\n= cosh−1 \u0010\n1 + 4\n(1 −ϵt−1)2\n(1 −(1 −ϵt−1)2)2\n\u0011\n(47)\n≥cosh−1 \u0010\n1 +\n2\n4ϵ2t−2 −4ϵ3t−3 + ϵ4t−4\n\u0011\n(48)\n≥cosh−1\n\u0012\n1 +\n2\nϵ2t−2\n\u0011\n(49)\n= log\n\u0010\n1 +\n1\n2ϵ2t−2 +\nr\u0010\n1 +\n1\n2ϵ2t−2\n\u00112\n−1\n\u0013\n(50)\n≥log\n\u0010 1\nϵt−1\n\u0011\n(51)\n= (1 −t) log(ϵ)\n(52)\n= (t −1)| log(ϵ)|,\n(53)\nwhich shows that we can compute a distance that is bounded from below by O(t). Similar steps can be used to show that the\ndistance is also bounded from above by a O(t) term.\nG. Proof of Theorem 4.4\nTheorem. Given a ulp-nonoverlapping FPE x = Pt\ni=1 xi ∈[−1 + ϵt−1, 1 −ϵt−1] consisting of floating point numbers\nwith a precision b > t, Algorithm 13 leads to an approximation y of the inverse hyperbolic tangent of x that satisfies\n|y −tanh−1(x)| ≤ϵ∗,\n(54)\nfor some small ϵ∗> 0.\nProof. The accuracy of the x ∈(−0.5, 0.5) branch of the algorithm follows easily from the accuracy of the algorithm\nfor normal floating point numbers and the error bounds of the FPE routines from (Popescu, 2017), similar to the proof in\nAppendix E. The other branch can be a bit more problematic, due to the large derivatives near the boundary of the domain.\nFor 0.5 ≤|x| < 1 −ϵt−1, we use\ntanh−1(x) = 0.5 · sign(x) · log\n\u0010\n1 +\n2|x|\n1 −|x|\n\u0011\n.\n(55)\nLet z denote the argument of the logarithm, so\nz = 1 +\n2|x|\n1 −|x|,\n(56)\n15\n\nLow-distortion and GPU-compatible Tree Embeddings in Hyperbolic Space\nand let ˜z = ˜z1 + . . . + ˜zt denote that approximation of z obtained through FPE operations. Due to the error bounds given in\n(Popescu, 2017), for FPEs with t terms on the domain 0.5 ≤|x| < 1 −ϵt−1 we can assume that\n|z −˜z|\n|z|\n< 2ϵ,\n(57)\nwhere ϵ is the machine precision of the floating point terms. Now, since |˜z2| ≤ulp(˜z1) = ϵ|˜z1|, we can write\n|z −˜z1| ≤|z −˜z| +\n\f\f\f\nt\nX\ni=2\n˜zi\n\f\f\f\n(58)\n≤2ϵ|z| + 2|˜z2|\n(59)\n≤2ϵ|z| + 2ϵ|˜z1|\n(60)\n≤4ϵ|z| + 2ϵ|˜z1 −z|,\n(61)\nwhich can be rewritten as\n|z −˜z1| ≤\n4ϵ\n1 −2ϵ|z| ≤8ϵ|z|.\n(62)\nThis shows that we can write ˜z1 = (1 + δ)z, with |δ| < 8ϵ. Now, the error of the largest magnitude term approximation of\nthe logarithm is\n\f\f\fy −0.5 · sign(x) · log(z)\n\f\f\f =\n\f\f\f0.5 · sign(˜x) · log(˜z1) −0.5 · sign(x) · log(z)\n\f\f\f\n(63)\n= 0.5 ·\n\f\f\f log\n\u0010z\n˜z\n\u0011\f\f\f\n(64)\n= 0.5 ·\n\f\f\f log\n\u0010 ˜z1\nz\n\u0011\f\f\f\n(65)\n= 0.5 ·\n\f\f\f log\n\u0010(1 + δ)z\nz\n\u0011\f\f\f\n(66)\n= 0.5 · | log(1 + δ)|\n(67)\n≤0.5 · |δ|\n(68)\n≤4ϵ.\n(69)\nLastly, we introduce some error through the approximation of the natural logarithm. However, as long as no overflow\noccurs, this error is typically bounded by the machine precision. Therefore, if we can approximate z well enough, then we\ncan guarantee an accurate computation of tanh−1. So combining this result with the error bounds from (Popescu, 2017)\nconcludes the proof.\nH. Proof of Proposition 4.5\nProposition. The range of algorithm 13 increases linearly in the number of terms t.\nProof. The maximal values that we can encounter occur near the boundary of the domain, so set x = 1 −ϵt−1. Then,\n0.5 · sign(x) · log\n\u0010\n1 +\n2|x|\n1 −|x|\n\u0011\n= 0.5 · log\n\u0010\n1 + 2 −2ϵt−1\nϵt−1\n\u0011\n(70)\n≤0.5 · log\n\u0010ϵt−1 + 2\nϵt−1\n\u0011\n(71)\n≤0.5 · log\n\u0010 e\nϵt−1\n\u0011\n(72)\n= 0.5 · (1 −(t −1) log(ϵ))\n(73)\n= 0.5 · (1 + (t −1)| log(ϵ)|),\n(74)\nwhich shows that the range is bounded from above by O(t). A similar argument leads to a O(t) lower bound, showing that\nthe range indeed increases linearly in the number of terms t.\n16\n\nLow-distortion and GPU-compatible Tree Embeddings in Hyperbolic Space\nI. Binary tree embedding results for varying dimensions\nTable 4 shows results of the embedding of a binary tree with float32 representations in 4, 7, 10 or 20 dimensions. Here, we\nhave also tested an additional objective similar to MAM, where we use the cosines of the angles instead of the angles. We\nfind that MAM generally leads to the best or close to the best results for each choice of dimensions.\nDave\nDwc\n4\n7\n10\n20\n4\n7\n10\n20\n(Sala et al., 2018) ‡\n0.734\n0.734\n0.734\n0.734\n1143\n1143\n1143\n1143\nSala et al. (2018) ⋆\n0.235\n0.502\n0.361\n0.726\n10.51\n132\n18.42\n280.5\nE0\n0.192\n0.188\n0.219\n0.189\n1.655\n1.625\n1.670\n1.640\nE1\n0.190\n0.196\n0.204\n0.190\n1.619\n1.664\n1.686\n1.698\nE2\n0.194\n0.198\n0.190\n0.198\n1.666\n1.687\n1.642\n1.680\nCosine similarity\n0.189\n0.189\n0.188\n0.188\n1.636\n1.637\n1.635\n1.633\nMAM\n0.188\n0.188\n0.188\n0.189\n1.632\n1.623\n1.635\n1.631\nTable 4. Comparing hyperspherical separation methods for the constructive hyperbolic embedding of a binary tree with a depth of 8\nedges using float32 representations in 4, 7, 10 or 20 dimensions. ‡ uses Hadamard generated hypersphere points and ⋆uses precomputed\npoints from (Lovisolo & Da Silva, 2001).\nJ. Embedding m-ary trees in varying dimensions\nTables 5 and 6 show results of the embedding of various m-ary trees in dimensions 4, 7, 10 and 20, similar to Table 2. We\nfind that MS-DTE gives the best results overall.\nDave\n3-tree\n5-tree\n7-tree\n4\n7\n10\n20\n4\n7\n10\n20\n4\n7\n10\n20\n(Sala et al., 2018) †\n0.09\n0.07\n0.03\n0.01\n0.18\n0.05\n0.04\n0.03\n0.16\n0.13\n0.03\n0.02\n(Sala et al., 2018) ‡\n0.11\n0.11\n0.11\n0.11\n-\n-\n0.12\n0.12\n-\n-\n0.12\n0.12\n(Sala et al., 2018) ⋆\n0.08\n0.08\n0.09\n0.14\n0.10\n0.12\n0.13\n0.18\n0.12\n0.12\n0.13\n0.17\nHS-DTE\n0.06\n0.06\n0.06\n0.06\n0.09\n0.09\n0.09\n0.09\n0.10\n0.10\n0.10\n0.10\nTable 5. Comparison of average distortion of hyperbolic embedding algorithms on m-ary trees with a maximum path length of\nℓ= 8. The h-MDS method is represented by †. The ‡ method is the combinatorial construction with the hyperspherical points being\ngenerated using the Hadamard construction, whereas the ⋆method samples hyperspherical points from the precomputed points generated\nwith the hyperspherical separation method from (Lovisolo & Da Silva, 2001). The h-MDS method outperforms the other methods for\nhigher dimensions, but collapses nodes, making the embeddings unusable. HS-DTE has the best performance for smaller dimensions and\nsecond best performance for larger dimensions.\nK. Embedding phylogenetic trees in varying dimensions\nAdditional experiments involving the phylogenetic trees with embedding dimensions 4, 7, 10 and 20 are shown in Tables 7,\n8, 9 and 10. We observe that the precomputed points method struggles to separate points for higher dimensions, leading to\nhigher distortion. Moreover, we find that HS-DTE gives the best results overall in every setting.\n17\n\nLow-distortion and GPU-compatible Tree Embeddings in Hyperbolic Space\nDwc\n3-tree\n5-tree\n7-tree\n4\n7\n10\n20\n4\n7\n10\n20\n4\n7\n10\n20\n(Sala et al., 2018) †\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n(Sala et al., 2018) ‡\n1.14\n1.14\n1.14\n1.14\n-\n-\n1.14\n1.14\n-\n-\n1.14\n1.14\n(Sala et al., 2018) ⋆\n1.32\n1.22\n1.18\n1.23\n1.28\n1.30\n1.30\n1.34\n1.53\n1.25\n1.31\n1.26\nHS-DTE\n1.07\n1.07\n1.07\n1.07\n1.14\n1.10\n1.10\n1.10\n1.14\n1.13\n1.12\n1.12\nTable 6. Comparison of worst-case distortion of hyperbolic embedding algorithms on m-ary trees with a maximum path length of\nℓ= 8. The h-MDS method is represented by †. The ‡ method is the combinatorial construction with the hyperspherical points being\ngenerated using the Hadamard construction, whereas the ⋆method samples hyperspherical points from the precomputed points generated\nwith the hyperspherical separation method from (Lovisolo & Da Silva, 2001). HS-DTE has the best performance in all settings.\nDave\nMosses\nWeevils\n4\n7\n10\n20\n4\n7\n10\n20\nHypFPE + (Sala et al., 2018) ‡\n-\n-\n-\n0.09\n-\n-\n0.07\n0.07\nHypFPE + (Sala et al., 2018) ⋆\n0.06\n0.10\n0.08\n0.10\n0.03\n0.05\n0.05\n0.10\nHypFPE + HS-DTE\n0.04\n0.04\n0.04\n0.04\n0.03\n0.03\n0.03\n0.03\nTable 7. Comparison of average distortion of hyperbolic embedding algorithms on the mosses and weevils trees. ‡ represents the\nconstruction with Hadamard hyperspherical points and ⋆the construction with points sampled from a set precomputed with (Lovisolo &\nDa Silva, 2001). The best performance is in bold. The embeddings are performed in a 4, 7, 10 or 20-dimensional space. Overall, we find\nthat HS-DTE works best.\nDave\nCarnivora\nLichen\n4\n7\n10\n20\n4\n7\n10\n20\nHypFPE + (Sala et al., 2018) ‡\n0.04\n0.04\n0.04\n0.04\n0.12\n0.12\n0.12\n0.12\nHypFPE + (Sala et al., 2018) ⋆\n0.01\n0.03\n0.03\n0.06\n0.05\n0.10\n0.11\n0.19\nHypFPE + HS-DTE\n0.02\n0.02\n0.03\n0.02\n0.06\n0.06\n0.05\n0.05\nTable 8. Comparison of average distortion of hyperbolic embedding algorithms on the carnivora and lichen trees. ‡ represents the\nconstruction with Hadamard hyperspherical points and ⋆the construction with points sampled from a set precomputed with (Lovisolo &\nDa Silva, 2001). The best performance is in bold. The embeddings are performed in a 4, 7, 10 or 20-dimensional space. Overall, we find\nthat HS-DTE works best.\nL. Statistics of the trees used in the experiments\nSome statistics of the trees that are used in the experiments are shown in Table 11. Most notably, these statistics show that\nthe true number of optimizations that has to be performed is significantly lower than the worst-case number of optimizations\ngiven by Theorem 3.1. To see this, note that an optimization step using MAM has to be performed each time a node is\nencountered with a degree that did not appear before. The result of this optimization step can then be cached and used for\neach node with the same degree.\nM. Graph and tree-like graph embedding results\nThe graphs that we test our method on are a graph detailing relations between diseases (Goh et al., 2007) and a graph\ndescribing PhD advisor-advisee relations (De Nooy et al., 2018). In order to embed graphs with the combinatorial\nconstructions, the graphs need to be embedded into trees first. Following (Sala et al., 2018), we use (Abraham et al., 2007)\nfor the graph-to-tree embedding. The results of the subsequent tree embeddings are shown in Table 12. These distortions are\nwith respect to the tree metric of the embedded tree instead of with respect to the original graph. This is to avoid mixing the\ninfluence of the tree-to-hyperbolic space embedding method with that of the graph-to-tree embedding.\n18\n\nLow-distortion and GPU-compatible Tree Embeddings in Hyperbolic Space\nDwc\nMosses\nWeevils\n4\n7\n10\n20\n4\n7\n10\n20\nHypFPE + (Sala et al., 2018) ‡\n-\n-\n-\n1.10\n-\n-\n1.09\n1.09\nHypFPE + (Sala et al., 2018) ⋆\n1.36\n1.21\n1.14\n1.16\n1.25\n1.12\n1.11\n1.13\nHypFPE + HS-DTE\n1.09\n1.07\n1.06\n1.07\n1.05\n1.05\n1.04\n1.04\nTable 9. Comparison of worst-case distortion of hyperbolic embedding algorithms on the mosses and weevils trees. ‡ represents the\nconstruction with Hadamard hyperspherical points and ⋆the construction with points sampled from a set precomputed with (Lovisolo &\nDa Silva, 2001). The best performance is in bold. The embeddings are performed in a 4, 7, 10 or 20-dimensional space. Overall, we find\nthat HS-DTE works best.\nDwc\nCarnivora\nLichen\n4\n7\n10\n20\n4\n7\n10\n20\nHypFPE + (Sala et al., 2018) ‡\n6.76\n6.76\n6.76\n6.76\n43.4\n43.4\n43.4\n43.4\nHypFPE + (Sala et al., 2018) ⋆\n3.50\n4.06\n4.87\n13.0\n4.73\n5.44\n6.43\n36.0\nHypFPE + HS-DTE\n2.46\n2.45\n2.03\n2.35\n4.07\n4.63\n3.30\n7.17\nTable 10. Comparison of worst-case distortion of hyperbolic embedding algorithms on the carnivora and lichen trees. ‡ represents\nthe construction with Hadamard hyperspherical points and ⋆the construction with points sampled from a set precomputed with (Lovisolo\n& Da Silva, 2001). The best performance is in bold. The embeddings are performed in a 4, 7, 10 or 20-dimensional space. Overall, we\nfind that HS-DTE works best.\nFrom these results we again see that HypFPE + HS-DTE outperforms all other methods. However, it should be noted\nthat graphs cannot generally be embedded with arbitrarily low distortion in hyperbolic space and that the graph to tree\nembedding method will introduce significant distortion. Hyperbolic space is not a suitable target for embedding a graph that\nis not tree-like. Therefore, we define our method as a tree embedding method and not as a graph embedding method.\nN. FPE arithmetic\nAlgorithm 2 FPEAddition\n1: Input: FPEs x = x1 + . . . + xn, y = y1 + . . . + ym and number of output terms r.\n2: f ←MergeFPEs(x, y)\n3: s ←FPERenormalize(f, r)\n4: return s = s1 + . . . + sr\n19\n\nLow-distortion and GPU-compatible Tree Embeddings in Hyperbolic Space\nAlgorithm 3 MergeFPEs\n1: Input: FPEs x = x1 + . . . + xn, y = y1 + . . . + ym.\n2: z ←Concatenate(x, y)\n3: Sort terms in z in ascending order with respect to absolute value.\n4: return Sorted z = {z1, . . . , zn+m}.\nAlgorithm 4 FPERenormalize\n1: Input: List of floating point numbers x = x1, . . . , xn and number of output terms r.\n2: e ←VecSum(x)\n3: y ←VecSumErrBranch(e, r)\n4: return y = y1 + . . . + yr\nAlgorithm 5 VecSum\n1: Input: List of floating point numbers x1, . . . , xn.\n2: s ←xn\n3: for i ∈{n −1, . . . , 1} do\n4:\n(s, ei+1) ←2Sum(xi, s)\n5: end for\n6: e1 ←s\n7: return e1, . . . , en\nAlgorithm 6 VecSumErrBranch\n1: Input: List of floating point numbers e1, . . . , en and number of output terms m.\n2: j ←1\n3: ϵ ←e1\n4: for i ∈{1, n −1} do\n5:\n(rj, ϵ) ←2Sum(ϵ, ei+1)\n6:\nif ϵ ̸= 0 then\n7:\nif j ≥m then\n8:\nreturn r1, . . . , rm\n9:\nend if\n10:\nj ←j + 1\n11:\nelse\n12:\nϵ ←rj\n13:\nend if\n14: end for\n15: if ϵ ̸= 0 and j ≤m then\n16:\nrj ←ϵ\n17: end if\n18: return r0, . . . , rm\nAlgorithm 7 2Sum\n1: Input: floating point numbers x and y.\n2: s ←RN(x + y)\nwhere RN is rounding to nearest\n3: x′ ←RN(s −y)\n4: y′ ←RN(s −x′)\n5: δx ←RN(x −x′)\n6: δy ←RN(y −y′)\n7: e ←RN(δx + δy)\n8: return (s, e)\n20\n\nLow-distortion and GPU-compatible Tree Embeddings in Hyperbolic Space\nAlgorithm 8 Fast2Sum\n1: Input: Floating point numbers x and y with ⌊log2 |x|⌋≥⌊log2 |y|⌋\n2: s ←RN(x + y)\n3: z ←RN(s −x)\n4: e ←RN(y −z)\n5: return (s, e)\nAlgorithm 9 FPEMultiplication\n1: Input: FPEs x = x1 + . . . + xn, y = y1 + . . . + ym, number of output terms r, bin size b and precision p (for float64:\nb = 45, p = 53).\n2: tx1 ←⌊log2 |x1|⌋\n3: ty1 ←⌊log2 |y1|⌋\n4: t ←tx1 + ty1\n5: for i ∈{1, . . . , ⌊r · p/b⌋+ 2} do\n6:\nBi ←1.5 · 2t−ib+p−1\n7: end for\n8: for i ∈{1, . . . , min(n, r + 1)} do\n9:\nfor j ∈{1, . . . , min(m, r + 1 −i)} do\n10:\n(π′, e) ←2Prod(xi, yj)\n11:\nℓ←t −txi −tyi\n12:\nsh ←⌊ℓ/b⌋\n13:\nℓ←ℓ−sh · b\n14:\nB ←Accumulate(π′, e, B, sh, ℓ)\n15:\nend for\n16:\nif j < m then\n17:\nπ′ ←xi · yj\n18:\nℓ←t −txi −tyj\n19:\nsh ←⌊ℓ/b⌋\n20:\nℓ←ℓ−sh · b\n21:\nB ←Accumulate(π′, 0, B, sh, ℓ)\n22:\nend if\n23: end for\n24: for i ∈{1, . . . , ⌊r · p/b⌋+ 2} do\n25:\nBi ←Bi −1.5 · 2t−ib+p−1\n26: end for\n27: π ←VecSumErrBranch(B, r)\n28: return π1 + . . . + πr\n21\n\nLow-distortion and GPU-compatible Tree Embeddings in Hyperbolic Space\nAlgorithm 10 Accumulate\n1: Input: Floating point numbers π′, e, list of floating point numbers B and integers sh, ℓ.\n2: c ←p −b −1\n3: if ℓ< b −2c −1 then\n4:\n(Bsh, π′) ←Fast2Sum(Bsh, π′)\n5:\nBsh+1 ←Bsh+1 + π′\n6:\n(Bsh+1, e) ←Fast2Sum(Bsh+1, e)\n7:\nBsh+2 ←Bsh+2 + e\n8: else if ℓ< b −c then\n9:\n(Bsh, π′) ←Fast2Sum(Bsh, π′)\n10:\nBsh+1 ←Bsh+1 + π′\n11:\n(Bsh+1, e) ←Fast2Sum(Bsh+1, e)\n12:\n(Bsh+2, e) ←Fast2Sum(Bsh+2, e)\n13:\nBsh+3 ←Bsh+3 + e\n14: else\n15:\n(Bsh, p) ←Fast2Sum(Bsh, π′)\n16:\n(Bsh+1, π′) ←Fast2Sum(Bsh+1, π′)\n17:\nBsh+2 ←Bsh+2 + π′\n18:\n(Bsh+2, e) ←Fast2Sum(Bsh+2, e)\n19:\nBsh+3 ←Bsh+3 + e\n20: end if\n21: return B\nAlgorithm 11 FPEReciprocal\n1: Input: FPE x = x1 + . . . + x2k an number of output terms 2q.\n2: r1 = RN( 1\nx1 )\n3: for i ∈{1, . . . , q} do\n4:\nv ←FPEMultiplication(r, x, 2i+1)\n5:\nw ←FPERenormalize(−v1, . . . , −v2i+1, 2.0, 2i+1)\n6:\nr ←FPEMultiplication(r, w, 2i+1)\n7: end for\n8: return r1 + . . . + r2q\nAlgorithm 12 FPEDivision\n1: Input: FPEs x = x1 + . . . + xn, y = y1 + . . . + ym and number of output terms r.\n2: z ←FPEReciprocal(y, m)\n3: π ←FPEMultiplication(x, z, r)\n4: return π\nAlgorithm 13 FPEtanh−1\n1: Input: FPE ˜f = ˜f1 + . . . + ˜ft.\n2: if | ˜f| > 1 then\n3:\nreturn NaN\n4: else if | ˜f| = 1 then\n5:\nreturn ∞\n6: else if | ˜f| < 0.5 then\n7:\nreturn 0.5 · sign( ˜f) · log(1 + 2| ˜f| + 2| ˜\nf|·| ˜\nf|\n1−| ˜\nf| )\n8: else\n9:\nreturn 0.5 · sign( ˜f) · log(1 +\n2| ˜\nf|\n1−| ˜\nf|)\n10: end if\n22\n\nLow-distortion and GPU-compatible Tree Embeddings in Hyperbolic Space\nTree\nNodes\nUnique degrees\nTheoretical worst-case\ndegmax\nLongest path length\nm-ary trees\nVarying\n2\nVarying\nm + 1\nvarying\nMosses\n344\n11\n38\n16\n51\nWeevils\n195\n5\n29\n8\n29\nCarnivora\n548\n3\n45\n4\n192.4\nLichen\n481\n3\n48\n4\n0.972\nTable 11. Statistics for the trees used in the experiments. The number of unique degrees is excluding nodes with a degree of 1. This\nnumber is equal to the total number of optimizations that has to be performed when embedding the tree using HS-DTE. The theoretical\nworst-case shows the worst-case number of optimizations that has to be performed according to Theorem 3.1. Note that the true number\nof optimizations is often significantly lower than this worst-case number.\nPrecision\nDiseases\nCS PhDs\nDave\nDwc\nDave\nDwc\n(Nickel & Kiela, 2017)\n53\n0.40\nNaN\n0.72\nNaN\n(Ganea et al., 2018)\n53\n0.85\n4831\n0.94\n803\n(Yu et al., 2022b)\n53\n0.72\n1014\n0.91\n1220\n(Sala et al., 2018) †\n53\n0.06\nNaN\n0.08\nNaN\n(Sala et al., 2018) ‡\n53\n-\n-\n-\n-\n(Sala et al., 2018) ⋆\n53\n0.364\n5.07\n0.33\n3.84\nHS-DTE\n53\n0.28\n2.28\n0.29\n2.76\nHypFPE + (Sala et al., 2018) ‡\n417\n-\n-\n-\n-\nHypFPE + (Sala et al., 2018) ⋆\n417\n0.05\n1.16\n0.04\n1.14\nHypFPE + HS-DTE\n417\n0.04\n1.14\n0.04\n1.09\nTable 12. Comparison of hyperbolic embedding algorithms on graphs. † represents the h-MDS method, ‡ the construction with\nHadamard hyperspherical points and ⋆the construction with points sampled from a set precomputed with (Lovisolo & Da Silva, 2001).\nThe best float64 performance is underlined and the best FPE performance is in bold. All embeddings are performed in a 10-dimensional\nspace. Hadamard generation cannot be used, since each embedded graph has a degmax greater than 8. HypFPE + HS-DTE outperforms\nall methods.\n23\n",
  "metadata": {
    "source_path": "papers/arxiv/Low-distortion_and_GPU-compatible_Tree_Embeddings_in_Hyperbolic_Space_6d912f5a30a2bc7e.pdf",
    "content_hash": "6d912f5a30a2bc7e84c4bbe55ba77e3aebc06e6a559b486eb3c7ed870cdc421c",
    "arxiv_id": null,
    "title": "Low-distortion and GPU-compatible Tree Embeddings in Hyperbolic Space",
    "author": "Max van Spengler, Pascal Mettes",
    "creation_date": "D:20250225024743Z",
    "published": "2025-02-25T02:47:43",
    "pages": 23,
    "size": 1006974,
    "file_mtime": 1740470180.0907826
  }
}