{
  "text": "Published as a conference paper at ICLR 2025\nADVERSARIAL TRAINING FOR DEFENSE AGAINST\nLABEL POISONING ATTACKS\nMelis Ilayda Bal1∗, Volkan Cevher2,3, Michael Muehlebach1\n1Max Planck Institute for Intelligent Systems, Tübingen, Germany\n2LIONS, EPFL\n3AGI Foundations, Amazon\n{mbal, michaelm}@tuebingen.mpg.de,\nvolkan.cevher@epfl.ch, volkcevh@amazon.de\nABSTRACT\nAs machine learning models grow in complexity and increasingly rely on publicly\nsourced data, such as the human-annotated labels used in training large language\nmodels, they become more vulnerable to label poisoning attacks. These attacks,\nin which adversaries subtly alter the labels within a training dataset, can severely\ndegrade model performance, posing significant risks in critical applications. In this\npaper, we propose FLORAL, a novel adversarial training defense strategy based\non support vector machines (SVMs) to counter these threats. Utilizing a bilevel op-\ntimization framework, we cast the training process as a non-zero-sum Stackelberg\ngame between an attacker, who strategically poisons critical training labels, and\nthe model, which seeks to recover from such attacks. Our approach accommodates\nvarious model architectures and employs a projected gradient descent algorithm\nwith kernel SVMs for adversarial training. We provide a theoretical analysis\nof our algorithm’s convergence properties and empirically evaluate FLORAL’s\neffectiveness across diverse classification tasks. Compared to robust baselines and\nfoundation models such as RoBERTa, FLORAL consistently achieves higher robust\naccuracy under increasing attacker budgets. These results underscore the potential\nof FLORAL to enhance the resilience of machine learning models against label\npoisoning threats, thereby ensuring robust classification in adversarial settings.\n1\nINTRODUCTION\nThe susceptibility of machine learning models to the integrity of their training data is a growing\nconcern, particularly as these models become more complex and reliant on large volumes of publicly\nsourced data, such as the human-annotated labels used in training large language models (Kumar\net al., 2020; Cheng et al., 2020; Wang et al., 2023). Any compromise in training data can severely\nundermine a model’s performance and reliability (Dalvi et al., 2004; Szegedy et al., 2013)— leading\nto catastrophic outcomes in security-critical applications, such as fraud detection (Fiore et al., 2019),\nmedical diagnosis (Finlayson et al., 2019), and autonomous driving (Deng et al., 2020).\nOne of the most insidious forms of threat is the data poisoning (causative) attacks (Barreno et al.,\n2010), where adversaries subtly manipulate a subset of the training data, causing the model to learn\nerroneous input-output associations. These attacks can involve either feature or label perturbations.\nUnlike feature poisoning, which alters the input data itself, (triggerless) label poisoning is particularly\nchallenging to detect because only the labels are modified, leaving the input data unchanged, as\nillustrated in Figure 2. Deep learning models are inherently vulnerable to random label noise (Zhang\net al., 2017), and this susceptibility is magnified when the noise is adversarially crafted to be more\ndamaging. Figure 1b illustrates this vulnerability: The RoBERTa model (Liu et al., 2019) fine-tuned\nfor sentiment analysis suffers substantial performance degradation under label poisoning attacks (Zhu\net al., 2022), with severity growing as the attacker’s budget increases. In contrast, Figure 1c highlights\nFLORAL’s effectiveness in mitigating these attacks. Here, the adversarially labelled dataset is\ngenerated by poisoning the labels of the most influential training points (see Appendix C.3 for details).\n∗Corrresponding author. Code is available at https://github.com/melisilaydabal/floral.\n1\narXiv:2502.17121v1  [cs.LG]  24 Feb 2025\n\nPublished as a conference paper at ICLR 2025\nNonlinear SVM\nadversarial training via PGD:\n𝝀𝒕←𝑷𝒓𝒐𝒙(𝝀𝒕−𝟏, 𝑫𝒕\n𝒂𝒅𝒗)\nAttacker\npoison labels of points via \nrandomized top-k(𝝀𝒕−𝟏) rule\nAdversarial dataset 𝑫𝒕\n𝒂𝒅𝒗\nDual parameters 𝝀𝒕−𝟏\n(a) FLORAL illustration.\n0\n1000\n2000\n3000\n4000\nRounds\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nAccuracy\n(b) RoBERTa.\n0\n200\n400\n600\n800\n1000\nRounds\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nAccuracy\n(c) FLORAL.\nClean\nAdv_10%\nAdv_25%\nAdv_30%\nAdv_35%\nAdv_40%\nFigure 1: (a): The illustration of FLORAL defense, adversarial training under label poisoning attacks.\n(b): The test accuracy degradation of RoBERTa fine-tuned on the IMDB dataset with adversarial\nlabels, showing its vulnerability to such attacks. (c): FLORAL effectively mitigates the impact of\nlabel poisoning in (b), achieving significantly higher robust accuracy.\nA line of work has addressed label poisoning through designing triggerless attacks against SVMs (Big-\ngio et al., 2012; Xiao et al., 2012; 2015), backdoor attacks in vision contexts (Chen et al., 2022; Jha\net al., 2023) or combining label poisoning with adversarial attacks (Fowl et al., 2021; Geiping et al.,\n2021). Defense mechanisms typically focus on filtering (sanitization) techniques (Laishram & Phoha,\n2016; Paudice et al., 2018), kernel correction (Biggio et al., 2011), intrinsic dimensionality-based sam-\nple weighting (Weerasinghe et al., 2021) or robust learning (Steinhardt et al., 2017). Adversarial train-\ning (AT) (Goodfellow et al., 2015; Madry et al., 2017) is a widely adopted empirical defense against\ndata poisoning—particularly for feature perturbations—framing the interaction as a zero-sum game\nand training models on adversarially perturbed data (Huang et al., 2015; Kurakin et al., 2016). How-\never, as shown in our experiments (Section 4.1), conventional AT does not adequately defend against\nlabel poisoning attacks, and its direct application to label poisoning remains largely unexplored.\nIn this paper, we address robust classification under label poisoning attacks and introduce FLORAL\n(Flipping Labels for Adversarial Learning), an SVM-based adversarial training defense that can\nbe seamlessly adapted to other model architectures. We formulate our defense strategy as a bilevel\noptimization problem (Robey et al., 2024), enabling a computationally efficient generation of optimal\nlabel attacks, and forming a non-zero-sum Stackelberg game between an attacker (or adversary),\ntargeting critical training labels, and the model, recovering from such attacks. We propose a projected\ngradient descent algorithm tailored for kernel SVMs to solve the bilevel optimization problem. Our\nexperiments on various classification tasks demonstrate that FLORAL improves robustness in the\nface of adversarially manipulated labels by effectively leveraging the inherent robustness of SVMs\ncombined with the strengths of adversarial training, achieving enhanced model resilience against\nlabel poisoning while maintaining a balance with classification accuracy.\nContributions.\nOur main contributions are the following.\n• We propose FLORAL, a support vector machine-based adversarial training strategy that defends\nagainst label poisoning attacks. To the best of our knowledge, this is the first work to introduce\nadversarial training as a defense specifically for label poisoning attacks. We consider kernel\nSVMs in our formulation, however, as we show in our experiments, the method can be easily\nintegrated with other models such as neural networks.\n• We utilize a bilevel optimization formulation for the robust learning problem, leading to a\nnon-zero-sum Stackelberg game between an attacker who poisons the labels of influential training\npoints and the model trying to recover from such attacks. We provide a projected gradient descent\n(PGD)–based algorithm to solve the game efficiently.\n• We theoretically analyze the local asymptotic stability of our algorithm by proving that its iterative\nupdates remain bounded and characterizing its convergence to the Stackelberg equilibrium.\n• We empirically analyze FLORAL’s effectiveness through experiments on various classification\ntasks against robust baselines as well as foundation models such as RoBERTa. Our results\ndemonstrate that as the attacker’s budget increases, FLORAL maintains higher robust accuracy\ncompared to baselines trained on adversarial data.\n• Finally, we show the generalizability of FLORAL against attacks from the literature, alfa,\nalfa-tilt (Xiao et al., 2015) and LFA (Paudice et al., 2018), which aim to maximize the\ndifference in empirical risk between classifiers trained on tainted and untainted label sets.\n2\n\nPublished as a conference paper at ICLR 2025\nclassifier\nless robust\nfeature attack\nmore robust\nmore robust\nlabel attack\nall vulnerable\nFigure 2: Sensitivity of the decision boundary to label poisoning attacks. The vulnerability of\ndata points differs between feature perturbation and label poisoning attacks. Given a perfect classifier,\npoints near the decision boundary are less robust to feature attacks (Zhang et al., 2021; Xu et al.,\n2023), leading to localized shifts in classification regions when the attack is performed. In contrast,\nthe decision boundary has a broader sensitivity with respect to label poisoning attacks which can affect\nboth near-boundary and distant points. By injecting incorrect labels, these attacks can create more\nwidespread disruption and an overall degradation in classifier performance across the input space.\n2\nPROBLEM STATEMENT AND BACKGROUND\nWe tackle the problem of robust binary classification in the presence of label poisoning attacks (see\nSection 3 for an extension to multi-class classification). Given a training dataset D = {(xi, yi) ∈\n(X, Y)}n\ni=1, where X ⊆Rd are the input features and Y = {±1} are the binary labels (potentially in-\nvolving adversarial labels), we consider a kernel SVM classifier fλ(x) := sign(P\nj λjyjk(x, xj)+b),\nparametrized by λ ∈Rn and bias b ∈R, which assigns a label to each data point and is derived\nfrom the following quadratic program (dual formulation) (Boser et al., 1992; Hearst et al., 1998):\nD(fλ; D) : min\nλ∈Rn\n1\n2λTQλ −1Tλ\n(1)\nsubject to\nyTλ = 0,\n0 ≤λ ≤C,\n(2)\nwhere Q ∈Rn×n is a positive semi-definite matrix, with elements Qij = yiyjKij and 1 is the\nn-dimensional vector of all ones. Here, K is the Gram matrix with entries Kij = k(xi, xj), ∀i, j ∈\n[n] := {1, . . . , n}, derived from a kernel function k. A common kernel choice is the radial basis\nfunction (RBF), given as k(xi, xj) = exp(−γ ∥xi −xj∥2), with width parameter γ. The parameter\nC ≥0 is a regularization term, balancing the trade-off between maximizing the margin and\nminimizing classification errors. In this formulation, each dual variable λi, i ∈[n] corresponds to\nthe Lagrange multiplier associated with the misclassification constraint for the training point xi.\n3\nTHE FLORAL APPROACH\nIn the context of label poisoning attacks, the attacker’s objective is to maximize the model’s test\nclassification error by subtly altering the labels in the training dataset to an optimal adversarial\nconfiguration. Adversarial training (Goodfellow et al., 2015; Madry et al., 2017) can be extended\nto counter these attacks and minimize model sensitivity to disruptive labels by actively optimizing\nfor robustness under worst-case scenarios. In this setting, the attacker generates the optimal label\nattack within a budget of k flips to maximize the model’s loss, while the model seeks parameters that\nminimize this worst-case loss. A straightforward, yet naive (Robey et al., 2024), way to implement\nthis approach would be to use the following minimax formulation:\nmin\nλ∈Rn\n1\nn\nn\nX\ni=1\n\n\n\n\n\nmax\nP\ni∈[n] 1{yi̸=˜yi}=k\n˜yi∈Y,i∈[n]\nL (fλ(xi), ˜yi)\n\n\n\n\n\n,\n(3)\nwhere L denotes a loss function, which in the case of the kernel SVM is related to the hinge\nloss (Smola & Schölkopf, 1998), and ˜y represents the adversarial label set. This formulation is\nproblematic for multiple reasons:\n1. Misaligned objectives: The loss is only a surrogate for the test accuracy, which is the actual\nquantity of interest to both the learner and the attacker.\nHowever, from an optimization\nperspective, maximizing an upper bound (such as the hinge loss in SVMs) on the classification\nerror as in (3) is not meaningful as such a bound does not represent the true objective of the\nattacker. Hence, a non-zero-sum formulation would allow for a more nuanced representation\nof the attacker’s objectives (Yasodharan & Loiseau, 2019).\n3\n\nPublished as a conference paper at ICLR 2025\nAlgorithm 1 FLORAL\n1: Input: Initial kernel SVM model fλ0, training dataset D0 = {(xi, yi)}n\ni=1, xi ∈Rd, yi ∈{±1},\nattacker budget B ∈{0, . . . , n}, parameter k, where k ≪B, learning rate η > 0.\n2: for round t = 1, . . . , T do\n3:\n˜yt ←Solve (7-9) via randomized top-k : randomly selecting k points from top B w.r.t. λt−1.\n4:\nDt ←{(xi, ˜yt\ni)}n\ni=1\n*/ Adversarial dataset with selected k poisoned labels\n5:\nCompute gradient of the objective (4), ∇λD(fλt−1; Dt), based on λt−1, Dt as given in (10).\n6:\nTake a PGD step λt ←PROX S(˜yt)(λt−1 −η∇λD(fλt−1; Dt)), based on (11-12).\n*/ AT\n7: end for\n8: return fλT\n2. Ineffective defense against critical points: In the case of an SVM-based classifier, the minimax\nformulation would only safeguard against attacks targeting data points with the largest hinge loss,\ni.e., those farthest from the decision boundary. These attacks are easily distinguishable (Xiao\net al., 2012) as, e.g., soft margin SVMs are shown to be robust to outliers (Smola & Schölkopf,\n1998). In contrast, attacks targeting the critical points that define the decision boundary (support\nvectors) would be more effective in degrading the classifier’s performance.\n3. Combinatorial explosion: Even if a bilevel formulation is employed where the attacker\nminimizes the margin, the problem remains computationally challenging. Ordering data points\nby their margin and then searching for the best adversarial label set within a budget constraint\nresults in a vast combinatorial space.\nAs a result of these, we formulate our adversarial training routine as a non-zero-sum Stackelberg\ngame (Von Stackelberg, 2010; Conitzer & Sandholm, 2006) and propose FLORAL defense using\nthe bilevel optimization formulation (Bard, 2013):\nD(fλ;D) : min\nλ∈Rn\n1\n2λT ˜Qλ −1Tλ\n(4)\nsubject to\n˜y(λ)Tλ = 0\n(5)\n0 ≤λ ≤C\n(6)\nwhere ˜y(λ) ∈arg\nmax\ny′∈Yn,u∈{0,1}n λTu\n(7)\nsubject to\ny′\ni = yi(1 −2ui), ∀i ∈[n] (8)\nX\ni∈[n]\n1{yi ̸= y′\ni} = k.\n(9)\nIn the outer (model’s) problem, defined by (4-6), the SVM classifier is derived under an adversarial\nlabel set. The key difference from the formulation in Section 2 is that the elements of ˜Q are defined\nas ˜Qij = ˜yi˜yjKij. Meanwhile, the inner (attacker’s) problem, given by (7-9) identifies the top-k\nmost influential data points affecting the model’s decision boundary. The intuition behind this\napproach is similar to identifying the most responsible training points for the model’s prediction\nas in (Koh & Liang, 2017). However, rather than relying on influence functions (Hampel, 1974),\nthe attacker leverages the dual variables λ, which provides direct access to such influential points.\nThese points correspond to the support vectors, and the higher the value of a dual variable, the more\ncritical that data point is in determining the model’s decision boundary.\nWe address the bilevel optimization problem in (4-9) as a non-zero-sum Stackelberg game\n(Von Stackelberg, 2010) between the learning model, and the attacker acting as the leader and\nfollower, respectively, as shown in Figure 1a. The game begins with an initial kernel SVM model\nfλ0 and a training dataset D0, and proceeds iteratively. In each round t, the model shares its dual\nparameters with the attacker, who then generates an adversarially labelled dataset Dt using a\nrandomized top-k rule. That is, the attacker identifies the top-B data points based on their λt−1\nvalues, (constrained by the budget B) and flips the labels of k randomly chosen points among them.\nWe incorporate randomization to account for the attacker’s budget and to reduce the risk of settling\nin local optima. Adversarial training is performed via a projected gradient descent step using λt−1\nand Dt, after which the updated parameters, λt, are shared with the attacker. This iterative interplay\nbetween the attacker and defender model forms a soft-margin kernel SVM robust to adversarial label\npoisoning. Our overall approach is detailed in Algorithm 1.\n4\n\nPublished as a conference paper at ICLR 2025\nFLORAL’s effectiveness.\nFLORAL iteratively exposes the model to learn adversarial configurations\nof the decision boundary. Hence, when the training data is clean, the training process proactively\nadjusts the model to be less sensitive to the influence of individual poisoned labels. In cases where\npoisoned labels are already present in the initial training data, FLORAL effectively neutralizes their\nimpact by implicitly sanitizing the corrupted labels. This behavior is evaluated empirically and\ndetailed in Section 4.1 and Appendix D.\nThe attacker’s capability.\nThe attacker solves (7-9) with respect to the shared model parameters λ,\ngenerating label attacks by targeting the most influential support vectors. This white-box attack (Wu\net al., 2023) assumes that the attacker can access model parameters. To reflect practical constraints,\nwe limit the attacker’s budget to at most B label poisons per round, from which k points are randomly\nselected. While this scenario may still seem to give the attacker significant power, notably, (i) relying\non secrecy for security is generally considered poor practice (Biggio et al., 2013), and (ii) our method\nis designed to defend against the strongest possible attacker. Even in black-box attack scenarios,\nwhere the attacker lacks parameter access, FLORAL remains effective for generating transferable\nattacks (Zheng et al., 2023). In such cases, the attacker could fit a kernel SVM on the available data\nand apply a similar selection rule to craft adversarial labels.\nGradient of the objective (4).\nIn each round, the adversarial training PGD step requires computing\nthe gradient ∇λD(fλ; D) of the objective (4) based on λt−1 and Dt, which is defined as\n∇λD(fλt−1; Dt) = ˜Qλt−1 −1,\n(10)\nwhere ˜Q is the matrix with entries ˜Qij = ˜yt\ni ˜yt\njKij, ∀i, j ∈[n], detailed in Appendix B.\nProjection.\nThe feasible set S changes in each round t depending on the adversarial label set\n˜yt (see (6)). We introduce the variable zt := λt−1 −η∇λD(fλt−1; Dt) and define the projection\noperator PROXS(˜yt)(zt) : Rn →Rn as follows:\nPROXS(˜yt)(zt) : λt ∈arg min\nλ∈Rn\n1\n2∥λ −zt∥2\n(11)\nsubject to\n˜ytTλ = 0,\n0 ≤λ ≤C.\n(12)\nHowever, solving this quadratic program for large-scale instances is computationally challenging\nunless the specific problem structure is exploited. Therefore, we provide a scalable and efficient\nimplementation of Algorithm 1 that relies on a fixed point iteration strategy as detailed in Section 3.2.\nA form of geometry-aware AT.\nFLORAL aligns with geometry-aware AT principles (Zhang et al.,\n2021). Support vectors with large Lagrange multipliers (λ) play a critical role in defining the decision\nboundary (Hearst et al., 1998). In FLORAL, the attacker strategically identifies these points using\na randomized top-k rule. This method inherently integrates the geometric proximity to the decision\nboundary into the label attack, targeting points that significantly impact the hinge loss.\nRobust multi-class classification.\nWe extend our algorithm to multi-class classification tasks, as\ndetailed in Algorithm 3 in Appendix F. The primary modification involves adopting a one-vs-all\napproach and considering multiple attackers, each corresponding to a different class.\n3.1\nSTABILITY ANALYSIS\nWe theoretically analyze the stability of FLORAL (Algorithm 1) by (i) demonstrating that\nits iterative updates are bounded and (ii) characterizing its convergence to the Stackel-\nberg equilibrium.\nFor simplicity of notation, let us define the update rule at round t as\nλt := PROXS(yt)(zt) = PROXS(yt)(λt−1 −η∇λf(λt−1, yt)), where PROX is defined in (11-12).\nWe use the operator LFLIP : X × Y →Y to define label poisoning attack formulated in (7-9).\nLemma 1. Let (ˆλ, ˆy(ˆλ)) denote a Stackelberg equilibrium, i.e., ˆy(ˆλ) := LFLIP(ˆλ) and ˆλ :=\nPROXS(ˆy(ˆλ))(ˆz) = PROXS(ˆy(ˆλ))(ˆλ −η∇λf(ˆλ, ˆy(ˆλ))) and {λt}T\nt=0 be the sequence of iterates\ngenerated by FLORAL (Algorithm 1). The following bound holds for the iterates:\n∥λt −ˆλ∥∞≤∥zt −ˆz∥∞+ κy∥yt −ˆy(ˆλ)∥∞\n(13)\nwhere κy is a constant defined by the PROX operator and index set corresponding to λt ∈(0, C),\nas detailed in Appendix A.1, and ∥· ∥∞denotes the infinity norm.\nProof. See Appendix A.1 for the proof.\n5\n\nPublished as a conference paper at ICLR 2025\nLemma 2. Let (ˆλ, ˆy(ˆλ)) denote the Stackelberg equilibrium as before. The following bound holds\nfor the non-projected iterates {zt}T\nt=0 of FLORAL (Algorithm 1):\n∥zt −ˆz∥∞≤κλ∥λt−1 −ˆλ∥∞+ κ′\ny∥yt −ˆy(ˆλ)∥∞\n(14)\nwhere κλ and κ′\ny are kernel dependent constants that are below 1 for small enough η.\nProof. See Appendix A.2 for the proof.\nTheorem 3.1 (ε-local asymptotic stability). The Stackelberg equilibrium (ˆλ, ˆy(ˆλ)) defined as before,\nis ε-locally asymptotically stable for the Stackelberg game solved via Algorithm 1 for a small enough\nstep size η. This implies that for every ε > 0, there exists δ > 0 such that\n∥λ0 −ˆλ∥∞< δ ⇒∥λt −ˆλ∥∞< ε, ∀t > 0 and λt →ˆλ.\n(15)\nProof (sketch). The proof relies on characterizing the distance between the update λt at round t\nand the equilibrium ˆλ using Lemma 1 and Lemma 2, then leveraging the fact that the label flipping\noperator (LFLIP) formulated in (7-9) returns the same adversarial label set when λt is within an\nε distance from the equilibrium. The complete proof is given in Appendix A.3, with the global\nconvergence result discussed in Appendix A.4.\n3.2\nLARGE-SCALE IMPLEMENTATION\nWe scale our algorithm for large problem instances by approximating the projection operation (step 6\nin Algorithm 1) via a fixed-point iteration method, as outlined in Algorithm 2. The key idea leverages\nthe optimal λ⋆expression from Appendix A.1 and involves an iterative splitting of variables based\non non-projected λ values within the range [0, C]. In each iteration, the variable µ is updated using\nthe expression in Appendix A.1 until convergence to a specified error ϵ is achieved.\nAlgorithm 2 PROJECTIONVIAFIXEDPOINTITERATION\n1: Input: Non-projected λ0, adversarial label set ˜y = {˜yi}n\ni=1, yi ∈{±1}, parameters {C, ϵ} > 0.\n2: Initialize µ0 = 0.\n3: for round t = 1, . . . , Tproj do\n4:\nλt = CLIP[0,C](λ0 −µt−1˜y)\n*/ Clip to satisfy constraint in (12)\n5:\nif λt˜y = 0 then\n6:\nreturn λt\n7:\nend if\n8:\nIC, Iz ←indices of λt ≥C, λt ∈(0, C)\n*/ Variable splitting\n9:\nη ←max(| Iz |, 1)\n*/ To avoid empty Iz case\n10:\nµt ←η−|Iz|\nη\nµt−1 + 1\nη(P\ni∈IC C˜yi + P\ni∈Iz λi\nt˜yi)\n11:\nif | µt −µt−1 |≤ϵ then\n12:\nreturn CLIP[0,C](λ0 −µt˜y)\n13:\nend if\n14: end for\n3.3\nRELATED WORK\nLabel poisoning.\nBiggio et al. (2012) first analyzed label poisoning attacks, showing that flipping\na small number of training labels severely degrades SVM performance. Xiao et al. (2012) later\nformalized optimal label flip attacks under budget constraints as a bilevel optimization problem,\nwhich then expanded to transferable attacks on black-box models (Zhao et al., 2017), considering\narbitrary attacker objectives. Beyond SVMs, recent works have explored label poisoning in backdoor\nattack scenarios, where adversaries inject triggers or alter triggerless data with poisoned labels\nin multi-label settings (Jha et al., 2023; Chen et al., 2022). In contrast, our approach focuses on\ntriggerless poisoning attacks.\nDefenses against these attacks include heuristic-based kernel correction (Biggio et al., 2011), which\nuses expectation for Q in (4), though assuming independent label flipping with equal probability–a\ncondition not guaranteed in practice. Other defenses such as clustering-based filtering (Laishram &\nPhoha, 2016; Tavallali et al., 2022), data complexity analysis (Chan et al., 2018), re-labeling (Paudice\n6\n\nPublished as a conference paper at ICLR 2025\net al., 2018) and label smoothing (Rosenfeld et al., 2020) offer straightforward solutions, however,\nthey do not scale well to high-dimensional or large datasets. Sample weighting based on local\nintrinsic dimensionality (LID) (Weerasinghe et al., 2021; Ma et al., 2018) shows promise, but relies\non accurate and computationally expensive LID estimation. Our approach, however, avoids strong\nassumptions about the data distribution or the attacker, preserves feasibility, and scales effectively\nto large-scale problem instances as demonstrated in Section 4. Additionally, while learning under\nnoisy labels (Frénay & Verleysen, 2013; Natarajan et al., 2013; Hallaji et al., 2023; Zhang et al.,\n2024) may seem relevant, our work focuses specifically on adversarial label noise (Biggio et al.,\n2011), where the adversary intentionally crafts the most damaging label perturbations.\nAdversarial training (AT).\nAdversarial examples, introduced by Szegedy et al. (2013), revealed\nhow small perturbations cause misclassification in deep neural networks (DNNs). Building on\nthis, AT (Goodfellow et al., 2015) emerged as a prominent defense, training models on both\noriginal and adversarially perturbed data. Defenses have utilized adversarial examples generated\nby methods such as the Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2015), PGD (Madry\net al., 2017), Carlini & Wagner attack (Carlini & Wagner, 2017), among others (Chen et al., 2017;\nMoosavi-Dezfooli et al., 2015). For SVMs, Zhou et al. (2012) formulated convex AT for linear\nSVMs, later extended to kernel SVMs by Wu et al. (2021) via doubly stochastic gradients under\nfeature perturbations. Despite this progress, AT for label poisoning remains underexplored. FLORAL\nfills this gap, by leveraging AT specifically for label poisoning scenarios, using PGD to train models\non poisoned datasets rather than generating adversarial examples.\nIn parallel, game-theoretical approaches have modeled adversarial interactions as simultaneous\ngames, where classifiers and adversaries select strategies independently (Dalvi et al., 2004), or as\nStackelberg games with a leader-follower dynamic (Brückner & Scheffer, 2011; Zhou et al., 2019;\nChivukula et al., 2020). AT has further linked these concepts, particularly in simultaneous zero-sum\ngames (Hsieh et al., 2019; Pinot et al., 2020; Pal & Vidal, 2020) to non-zero-sum formulations\n(Robey et al., 2024). We adopt a sequential setup, using the Stackelberg framework where the leader\ncommits to a strategy and the follower responds accordingly.\n4\nEXPERIMENTS\nIn this section, we showcase the effectiveness of FLORAL across various robust classification tasks,\nutilizing the following datasets:\n• Moon (Pedregosa et al., 2011): We employed a synthetic benchmark dataset, D = {(xi, yi)}2000\ni=1\nwhere xi ∈R2 and yi ∈{±1}. Adversarial versions are generated by flipping the labels of points\nfarther from the decision boundary of a linear classifier trained on the clean dataset, using label\npoisoning levels (%) of {5, 10, 15, 20, 25}. The details on the adversarial datasets are given in\nAppendix C.1.\n• IMDB (Maas et al., 2011): A benchmark review sentiment analysis dataset with D =\n{(xi, yi)}50000\ni=1\nwhere xi ∈R768 and yi ∈{±1}.\nFor SVM training, we extracted 768-\ndimensional embeddings from the fine-tuned RoBERTa (Liu et al., 2019). We created adversarial\ndatasets by fine-tuning the RoBERTa-base model on the clean dataset to identify influential train-\ning points based on the gradient with respect to the inputs, then flipping their labels at poisoning\nlevels (%) of {10, 25, 30, 35, 40}.\n• MNIST (Deng, 2012): In Appendix E.3, we provide the additional experiments with the MNIST\ndataset in detail.\nExperimental setup.\nFor all SVM-based methods, we used RBF kernel, exploring various values\nof C and γ. We conducted five replications with different train/test splits, including the corresponding\nadversarial datasets for each dataset. In all FLORAL experiments, we constrain the attacker’s capability\nwith a limited budget. That is, the attacker identifies the most influential candidate points, with\nB = 2k, from the training set and randomly selects k ∈{1, 2, 5, 10, 25} to poison, where k represents\nthe % of points relative to the training set size. Detailed experimental configurations are provided in\nAppendix C (see Table 3).\nBaselines.\nWe benchmark FLORAL against the following baselines:\n1. (Vanilla) SVM with an RBF kernel, which serves as a basic benchmark (Hearst et al., 1998).\n2. LN-SVM (Biggio et al., 2011) applies a heuristic-based kernel matrix correction.\n7\n\nPublished as a conference paper at ICLR 2025\nTable 1: Test accuracies of methods trained on the Moon dataset, averaged over five runs. Highlighted\nvalues indicate the best performance in the \"Best\" (peak accuracy during training) and \"Last\"\n(final accuracy after training) columns. This notation is consistently applied in the subsequent tables.\nFLORAL outperforms baselines in most of the settings, providing a particularly robust defense in\nhighly adversarial scenarios. See Appendix E.1 (Table 4) for the results of other settings.\nSetting\nMethod\nFLORAL\nSVM\nNN\nNN-PGD\nLN-SVM\nCurie\nLS-SVM\nK-LID\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nClean\nC = 10, γ = 1\n0.968\n0.966\n0.968\n0.968\n0.960\n0.960\n0.966\n0.964\n0.940\n0.940\n0.941\n0.941\n0.881\n0.881\n0.966\n0.966\nDadv = 5%\nC = 10, γ = 1\n0.966\n0.966\n0.965\n0.957\n0.926\n0.926\n0.964\n0.937\n0.940\n0.940\n0.903\n0.903\n0.881\n0.881\n0.964\n0.964\nDadv = 10%\nC = 10, γ = 1\n0.924\n0.907\n0.912\n0.900\n0.859\n0.855\n0.927\n0.853\n0.869\n0.868\n0.907\n0.907\n0.894\n0.894\n0.908\n0.907\nDadv = 15%\nC = 10, γ = 1\n0.924\n0.917\n0.892\n0.823\n0.826\n0.826\n0.871\n0.871\n0.893\n0.829\n0.906\n0.858\n0.892\n0.823\n0.883\n0.826\nDadv = 20%\nC = 10, γ = 1\n0.875\n0.865\n0.840\n0.771\n0.788\n0.787\n0.854\n0.853\n0.839\n0.758\n0.859\n0.763\n0.840\n0.771\n0.830\n0.755\nDadv = 25%\nC = 10, γ = 1\n0.801\n0.768\n0.753\n0.717\n0.693\n0.647\n0.740\n0.655\n0.754\n0.693\n0.779\n0.697\n0.766\n0.721\n0.747\n0.690\nTable 2: Test accuracies of methods trained on the IMDB dataset, averaged over five replications.\nFLORAL demonstrates superior robustness compared to baselines, particularly in more adversarial\nscenarios. See also Figures 1b-1c.\nSetting\nMethod\nFLORAL\nRoBERTa\nSVM\nLN-SVM\nCurie\nLS-SVM\nK-LID\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nClean\n0.9113\n0.9113\n0.9119\n0.9110\n0.9113\n0.9113\n0.9113\n0.9113\n0.9116\n0.9113\n0.9108\n0.9108\n0.9116\n0.9115\nDadv = 10%\n0.9039\n0.9039\n0.9048\n0.9031\n0.9039\n0.9039\n0.9029\n0.9028\n0.9039\n0.9039\n0.9010\n0.9010\n0.9039\n0.9039\nDadv = 25%\n0.8896\n0.8896\n0.8827\n0.8612\n0.8887\n0.8886\n0.8860\n0.8860\n0.8889\n0.8888\n0.8771\n0.8769\n0.8885\n0.8883\nDadv = 30%\n0.8801\n0.8801\n0.8675\n0.8357\n0.8792\n0.8792\n0.8771\n0.8771\n0.8797\n0.8797\n0.8325\n0.8324\n0.8795\n0.8795\nDadv = 35%\n0.8713\n0.8713\n0.8270\n0.8053\n0.8660\n0.8660\n0.8646\n0.8646\n0.8695\n0.8695\n0.7667\n0.7667\n0.8700\n0.8700\nDadv = 40%\n0.8636\n0.8636\n0.7792\n0.7717\n0.8574\n0.8584\n0.8515\n0.8515\n0.8589\n0.8589\n0.7060\n0.7060\n0.8594\n0.8594\n3. Curie (Laishram & Phoha, 2016), utilizes the DBSCAN clustering (Ester et al., 1996) to identify\nand filter-out poisoned data points.\n4. LS-SVM (Paudice et al., 2018) applies label sanitization based on k-NN (Cover & Hart, 1967).\n5. K-LID (Weerasinghe et al., 2021), a weighted SVM based on kernel local intrinsic dimensionality.\n6. NN: A DNN trained using the SGD optimizer with momentum, serving as a non-linear baseline.\n7. NN-PGD: A DNN trained with PGD-AT (Madry et al., 2017), to evaluate a robust model\ndesigned to withstand feature perturbation attacks under label poisoning.\n8. RoBERTa (Liu et al., 2019), used in experiments with IMDB dataset to assess a fine-tuned\ntransformer-based language model’s robustness under label poisoning.\nAppendix G includes further comparisons with a least squares classifier using randomized smoothing\n(Rosenfeld et al., 2020) and a filtering-out defense based on regularized synthetic reduced nearest\nneighbor (Tavallali et al., 2022) on the Moon and MNIST datasets.\nPerformance metrics.\nWe assess our method using two key metrics: robust and clean accuracy,\ntracked over a test set with clean labels during training. Unlike feature perturbation studies, where\nrobust accuracy is gauged on adversarially perturbed test examples (Yang et al., 2020), in our study,\nrobust accuracy reflects model performance tested on clean labels using adversarially labelled training\ndata, thereby indicating the models’ resilience and generalization capabilities under poisoning.\nConversely, clean accuracy measures the performance of models trained and tested on clean-labelled\ndata, offering a benchmark for comparison under both adversarial and non-adversarial conditions.\nWe additionally report hinge loss on the clean-labelled test data (see Appendix E.2) in experiments\nwith the IMDB dataset.\n4.1\nEXPERIMENT RESULTS\nIn this section, we report the performance of FLORAL against the baseline methods on the Moon\ndataset, followed by the results of its integration with RoBERTa on the IMDB dataset.\nMoon.\nAs reported in Table 1 and Figure 3, FLORAL achieves higher robust accuracy across\nalmost all settings compared to baseline methods. Notably, in scenarios with more severe poisoning\nlevels, FLORAL significantly outperforms all baselines, which experience a marked drop in their\naccuracy. We report results under various kernel hyperparameters in Appendix E.1 (see Table 4 and\n8\n\nPublished as a conference paper at ICLR 2025\n0\n100\n200\n300\n400\n500\nRounds\n0.800\n0.825\n0.850\n0.875\n0.900\n0.925\n0.950\n0.975\n1.000\nAccuracy\nTest Accuracy\n(a) Clean.\n0\n100\n200\n300\n400\n500\nRounds\n0.750\n0.775\n0.800\n0.825\n0.850\n0.875\n0.900\n0.925\n0.950\nAccuracy\nTest Accuracy\n(b) Dadv = 10%.\n0\n100\n200\n300\n400\n500\nRounds\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nAccuracy\nTest Accuracy\n(c) Dadv = 15%.\n0\n100\n200\n300\n400\n500\nRounds\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nAccuracy\nTest Accuracy\n(d) Dadv = 20%.\n0\n100\n200\n300\n400\n500\nRounds\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nAccuracy\nTest Accuracy\n(e) Dadv = 25%.\nFLORAL\nVanilla SVM\nNN\nNN-PGD\nLN-SVM\nCurie\nLS-SVM\nK-LID\nFigure 3: Test accuracy of methods on the Moon dataset under varying label poisoning levels. For\nSVM models, C = 10, γ = 1 are used. See Appendix E (Figure 7) for results with other settings.\nAs the label poisoning level increases, the accuracy of methods generally declines, however, FLORAL\nmaintains higher robust accuracy across all adversarial settings, without compromising clean accuracy.\nFigure 7). We additionally visualize the decision boundaries of trained methods on the test dataset\nin Figure 4, which shows that FLORAL produces a smoother decision boundary compared to the\nbaselines, promoting generalization (see Figure 18 in Appendix E for the complete results).\nWhen the initial training data is clean, FLORAL provides a proactive defense by introducing\nadversarial labels during training, thereby effectively reducing the model’s sensitivity to potential\nlabel attacks. Notably, FLORAL matches performance on par with vanilla SVM on clean data,\ndemonstrating that its robust framework maintains high accuracy without compromising clean\naccuracy. In scenarios with already poisoned training data, FLORAL achieves robustness through\ntwo key mechanisms: (i) implicitly sanitizing corrupted labels, while (ii) introducing additional\nadversarial labels to further reduce model sensitivity to attacks. We analyze this sanitization effect\nin detail in Appendix D and show that FLORAL sanitizes 25 −35% portion of the initial poisoned\ntraining labels. Furthermore, FLORAL operates on dynamically evolving adversarial datasets during\ntraining, unlike baselines that are trained on fixed adversarially labelled datasets. This dynamic\nstrategy introduces new adversarial labels in each round, further testing and enhancing the model’s\nrobustness. These capabilities position FLORAL as a superior defense over baselines, particularly\nin maintaining robust accuracy under more challenging adversarial scenarios.\nFLORAL offers several distinct advantages over existing baselines, e.g., unlike LN-SVM, FLORAL\ndoes not rely on the strong assumption that training labels are independently flipped with equal\nprobability. Compared to Curie, FLORAL avoids a filter-out system that risks discarding data with\nvaluable feature representations. Further, in terms of scalability, Curie’s dependence on distance\nmetrics makes it vulnerable to the curse of dimensionality, diminishing its clustering performance\nin high-dimensional and complex datasets. To ensure a fair comparison, we calibrated the noise,\nconfidence level, and threshold value parameters of LN-SVM, Curie, and LS-SVM baselines,\naligning them with the poisoning level in each dataset. This ensures that the baselines are at their\nstrongest configurations, highlighting the robustness and scalability of FLORAL.\nIMDB.\nWe integrate FLORAL as a robust classifier head for RoBERTa. The test performance on the\nIMDB dataset, shown in Table 2 and Figures 1b-1c, reveals that FLORAL significantly improves ro-\nbustness, outperforming fine-tuned RoBERTa along with other baselines. Our approach also converges\nfaster to lower loss values, in more adversarial scenarios (see Table 5 and Figure 8 in Appendix E.2).\nIn Appendix E.2, we analyze the changes in influential training points—those that most affect model\npredictions—when applying FLORAL to RoBERTa-extracted embeddings (see Figures 9-10). The\nresults reveal some overlap in the identified points, under clean train data scenario. However, as the\ntraining data becomes more adversarial, FLORAL identifies different critical points, which effectively\nshape the decision boundary and contribute to improved robust accuracy.\nAdaptability.\nAs shown with the IMDB experiments, FLORAL integrates seamlessly with other\nmodel architectures (e.g., RoBERTa, NNs) by utilizing the last-layer embeddings for training. We\nadditionally demonstrate this in Appendix I, FLORAL integrated with an NN learns more robust\nrepresentations and achieves higher robust accuracy on the Moon and MNIST datasets.\n9\n\nPublished as a conference paper at ICLR 2025\n(a) FLORAL (Clean).\nDadv = 5%.\nDadv = 10%.\nDadv = 25%.\n(b) SVM (Clean).\nDadv = 5%.\nDadv = 10%.\nDadv = 25%.\n(c) NN (Clean).\nDadv = 5%.\nDadv = 10%.\nDadv = 25%.\n(d) NN-PGD (Clean).\nDadv = 5%.\nDadv = 10%.\nDadv = 25%.\nFigure 4: The decision boundaries on the Moon test dataset under varying label poisoning levels.\nSVM models use an RBF kernel with C = 10 and γ = 0.5. FLORAL generates a smooth decision\nboundary compared to baseline methods, which show drastic changes due to adversarial training\nlabel manipulations. For the complete results with other baselines, see Appendix E (Figure 18).\nSensitivity analysis.\nWe further examined the sensitivity of our approach to the attacker’s budget,\nand the results are detailed in Appendix E.4 with Figure 12.\nGeneralizability.\nWe demonstrate the effectiveness of FLORAL under different attacks: alfa,\nalfa-tilt (Xiao et al., 2015) and LFA (Paudice et al., 2018) in Appendix H. Our experiments\non the Moon and MNIST (Deng, 2012) datasets again confirmed that FLORAL can also defend and\nachieve higher robust accuracy in the presence of other types of label attacks.\nLimitations.\nDefense strategies may not be universally effective against all label poisoning attacks\ndue to their non-adaptive nature (Papernot et al., 2016). Our defense strategy relies on a white-box\nattack, where the attacker can access the model. While we also show the performance of our approach\nunder various label attacks from the literature, its efficacy may vary under different attack scenarios.\n5\nCONCLUSION\nIn this paper, we address the vulnerability of machine learning models to label poisoning attacks and\npropose FLORAL, an adversarial training defense strategy based on kernel SVMs. We formulate the\nproblem using bilevel optimization and frame the adversarial interaction between the learning model\nand the attacker as a non-zero-sum Stackelberg game. To compute the game equilibrium that solves\nthe optimization problem, we introduce a projected gradient descent-based algorithm and analyze its\nlocal stability and convergence properties. Our approach demonstrates superior empirical robustness\nacross various classification tasks compared to robust baseline methods.\nFuture research includes exploring SVM-based transfer attacks or integrating our approach to robust\nfine-tuning of foundation models for supervised downstream tasks. Additionally, a detailed analysis\nof how FLORAL alters the most influential training points for model predictions, e.g. when integrated\nwith foundation models such as RoBERTa could provide interesting insights.\n10\n\nPublished as a conference paper at ICLR 2025\nACKNOWLEDGEMENTS\nThis research was supported by the Max Planck & Amazon Science Hub. We also thank the German\nResearch Foundation for the support and Zhiyu He for the helpful comments on the manuscript. The\nwork was conducted during Volkan Cevher’s time at Amazon.\nREFERENCES\nJonathan F Bard. Practical bilevel optimization: algorithms and applications. Springer Science &\nBusiness Media, 2013.\nMarco Barreno, Blaine Nelson, Anthony D Joseph, and J Doug Tygar. The security of machine\nlearning. Machine Learning, 81:121–148, 2010.\nBattista Biggio, Blaine Nelson, and Pavel Laskov. Support vector machines under adversarial label\nnoise. Asian Conference on Machine Learning, pp. 97–112, 2011.\nBattista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against support vector machines.\nInternational Conference on Machine Learning, pp. 1467–1474, 2012.\nBattista Biggio, Giorgio Fumera, and Fabio Roli. Security evaluation of pattern classifiers under\nattack. IEEE Transactions on Knowledge and Data Engineering, 26(4):984–996, 2013.\nBernhard E Boser, Isabelle M Guyon, and Vladimir N Vapnik. A training algorithm for optimal\nmargin classifiers. Proceedings of the 5th Annual Workshop on Computational Learning Theory,\npp. 144–152, 1992.\nMichael Brückner and Tobias Scheffer. Stackelberg games for adversarial prediction problems.\nProceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data\nMining, pp. 547–555, 2011.\nNicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. IEEE\nSymposium on Security and Privacy, pp. 39–57, 2017.\nPatrick PK Chan, Zhi-Min He, Hongjiang Li, and Chien-Chang Hsu. Data sanitization against\nadversarial label contamination based on data complexity. International Journal of Machine\nLearning and Cybernetics, 9:1039–1052, 2018.\nKangjie Chen, Xiaoxuan Lou, Guowen Xu, Jiwei Li, and Tianwei Zhang. Clean-image backdoor:\nAttacking multi-label models with poisoned labels only. International Conference on Learning\nRepresentations, 2022.\nPin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order\noptimization based black-box attacks to deep neural networks without training substitute models.\nProceedings of the ACM Workshop on Artificial Intelligence and Security, pp. 15–26, 2017.\nHao Cheng, Zhaowei Zhu, Xingyu Li, Yifei Gong, Xing Sun, and Yang Liu. Learning with instance-\ndependent label noise: A sample sieve approach. ArXiv, 2010.02347, 2020.\nAneesh Sreevallabh Chivukula, Xinghao Yang, Wei Liu, Tianqing Zhu, and Wanlei Zhou. Game\ntheoretical adversarial deep learning with variational adversaries. IEEE Transactions on Knowledge\nand Data Engineering, 33(11):3568–3581, 2020.\nVincent Conitzer and Tuomas Sandholm. Computing the optimal strategy to commit to. Proceedings\nof the ACM Conference on Electronic Commerce, pp. 82–90, 2006.\nThomas Cover and Peter Hart. Nearest neighbor pattern classification. IEEE Transactions on\nInformation Theory, 13(1):21–27, 1967.\nNilesh Dalvi, Pedro Domingos, Mausam, Sumit Sanghai, and Deepak Verma. Adversarial classifica-\ntion. Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and\nData Mining, pp. 99–108, 2004.\n11\n\nPublished as a conference paper at ICLR 2025\nLi Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal\nProcessing Magazine, 29(6):141–142, 2012.\nYao Deng, Xi Zheng, Tianyi Zhang, Chen Chen, Guannan Lou, and Miryung Kim. An analysis of\nadversarial attacks and defenses on autonomous driving models. IEEE International Conference\non Pervasive Computing and Communications, pp. 1–10, 2020.\nMartin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu, et al. A density-based algorithm for\ndiscovering clusters in large spatial databases with noise. Knowledge Discovery and Data Mining,\n96(34):226–231, 1996.\nSamuel G. Finlayson, John D. Bowers, Joichi Ito, Jonathan L. Zittrain, Andrew L. Beam, and Isaac S.\nKohane. Adversarial attacks on medical machine learning. Science, 363(6433):1287–1289, 2019.\nUgo Fiore, Alfredo De Santis, Francesca Perla, Paolo Zanetti, and Francesco Palmieri. Using\ngenerative adversarial networks for improving classification effectiveness in credit card fraud\ndetection. Information Sciences, 479:448–455, 2019.\nLiam Fowl, Micah Goldblum, Ping-yeh Chiang, Jonas Geiping, Wojciech Czaja, and Tom Goldstein.\nAdversarial examples make strong poisons. Advances in Neural Information Processing Systems,\n34:30339–30351, 2021.\nBenoît Frénay and Michel Verleysen. Classification in the presence of label noise: a survey. IEEE\nTransactions on Neural Networks and Learning Systems, 25(5):845–869, 2013.\nJonas Geiping, Liam Fowl, Gowthami Somepalli, Micah Goldblum, Michael Moeller, and Tom\nGoldstein. What doesn’t kill you makes you robust (er): How to adversarially train against data\npoisoning. ArXiv, 2102.13624, 2021.\nIan J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial\nexamples. ArXiv, 1412.6572, 2015.\nEhsan Hallaji, Roozbeh Razavi-Far, Mehrdad Saif, and Enrique Herrera-Viedma. Label noise analysis\nmeets adversarial training: A defense against label poisoning in federated learning. Knowledge-\nBased Systems, 266:110384, 2023.\nFrank R Hampel. The influence curve and its role in robust estimation. Journal of the American\nStatistical Association, 69(346):383–393, 1974.\nMarti A. Hearst, Susan T Dumais, Edgar Osuna, John Platt, and Bernhard Scholkopf. Support vector\nmachines. IEEE Intelligent Systems and Their Applications, 13(4):18–28, 1998.\nYa-Ping Hsieh, Chen Liu, and Volkan Cevher. Finding mixed nash equilibria of generative adversarial\nnetworks. International Conference on Machine Learning, pp. 2810–2819, 2019.\nChih-Wei Hsu and Chih-Jen Lin. A comparison of methods for multiclass support vector machines.\nIEEE Transactions on Neural Networks, 13(2):415–425, 2002.\nRuitong Huang, Bing Xu, Dale Schuurmans, and Csaba Szepesvári. Learning with a strong adversary.\nArXiv, 1511.03034, 2015.\nRishi D. Jha, Jonathan Hayase, and Sewoong Oh. Label poisoning is all you need. Advances in\nNeural Information Processing Systems, 36:71029–71052, 2023.\nPang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions.\nInternational Conference on Machine Learning, pp. 1885–1894, 2017.\nRam Shankar Siva Kumar, Magnus Nyström, John Lambert, Andrew Marshall, Mario Goertzel, Andi\nComissoneru, Matt Swann, and Sharon Xia. Adversarial machine learning-industry perspectives.\nIEEE Security and Privacy Workshops, pp. 69–75, 2020.\nAlexey Kurakin, Ian J Goodfellow, and Samy Bengio. Adversarial examples in the physical world.\nInternational Conference on Learning Representations, 2016.\n12\n\nPublished as a conference paper at ICLR 2025\nRicky Laishram and Vir Virander Phoha. Curie: A method for protecting svm classifier from\npoisoning attack. ArXiv, 1606.01584, 2016.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining\napproach. ArXiv, 1907.11692, 2019.\nXingjun Ma, Bo Li, Yisen Wang, Sarah M Erfani, Sudanthi Wijewickrema, Grant Schoenebeck,\nDawn Song, Michael E Houle, and James Bailey. Characterizing adversarial subspaces using local\nintrinsic dimensionality. ArXiv, 1801.02613, 2018.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher\nPotts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting\nof the Association for Computational Linguistics: Human Language Technologies, pp. 142–150,\n2011.\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.\nTowards deep learning models resistant to adversarial attacks. ArXiv, 1706.06083, 2017.\nSeyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: A simple and\naccurate method to fool deep neural networks. IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 2574–2582, 2015.\nNagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with\nnoisy labels. Advances in Neural Information Processing Systems, 26, 2013.\nAmbar Pal and René Vidal. A game theoretic analysis of additive adversarial attacks and defenses.\nAdvances in Neural Information Processing Systems, 33:1345–1355, 2020.\nNicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in machine learning: from\nphenomena to black-box attacks using adversarial samples. ArXiv, 1605.07277, 2016.\nAndrea Paudice, Luis Muñoz-González, and Emil C Lupu. Label sanitization against label flipping\npoisoning attacks. ArXiv, 1803.00992, 2018.\nFabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier\nGrisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn:\nMachine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.\nRafael Pinot, Raphael Ettedgui, Geovani Rizk, Yann Chevaleyre, and Jamal Atif. Randomization\nmatters how to defend against strong adversarial attacks. International Conference on Machine\nLearning, pp. 7717–7727, 2020.\nAlexander Robey, Fabian Latorre, George Pappas, Hamed Hassani, and Volkan Cevher. Adver-\nsarial training should be cast as a non-zero-sum game. International Conference on Learning\nRepresentations, 2024.\nElan Rosenfeld, Ezra Winston, Pradeep Ravikumar, and Zico Kolter. Certified robustness to label-\nflipping attacks via randomized smoothing. International Conference on Machine Learning, pp.\n8230–8241, 2020.\nAlex J Smola and Bernhard Schölkopf. Learning with kernels, volume 4. Citeseer, 1998.\nJacob Steinhardt, Pang Wei W Koh, and Percy S Liang. Certified defenses for data poisoning attacks.\nAdvances in Neural Information Processing Systems, 30:3520–3532, 2017.\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,\nand Rob Fergus. Intriguing properties of neural networks. ArXiv, 1312.619, 2013.\nPooya Tavallali, Vahid Behzadan, Azar Alizadeh, Aditya Ranganath, and Mukesh Singhal. Adversar-\nial label-poisoning attacks and defense for general multi-class models based on synthetic reduced\nnearest neighbor. IEEE International Conference on Image Processing, pp. 3717–3722, 2022.\nHeinrich Von Stackelberg. Market structure and equilibrium. Springer Science & Business Media,\n2010.\n13\n\nPublished as a conference paper at ICLR 2025\nSong Wang, Zhen Tan, Ruocheng Guo, and Jundong Li. Noise-robust fine-tuning of pretrained\nlanguage models via external guidance. Findings of the Association for Computational Linguistics,\npp. 12528–12540, 2023.\nSandamal Weerasinghe, Tansu Alpcan, Sarah M. Erfani, and Christopher Leckie. Defending support\nvector machines against data poisoning attacks. IEEE Transactions on Information Forensics and\nSecurity, 16:2566–2578, 2021.\nBaoyuan Wu, Zihao Zhu, Li Liu, Qingshan Liu, Zhaofeng He, and Siwei Lyu. Attacks in adversarial\nmachine learning: A systematic survey from the life-cycle perspective. ArXiv, 2302.09457, 2023.\nHuimin Wu, Zhengmian Hu, and Bin Gu. Fast and scalable adversarial training of kernel svm via\ndoubly stochastic gradients. Proceedings of the AAAI Conference on Artificial Intelligence, 35(12):\n10329–10337, 2021.\nHan Xiao, Huang Xiao, and Claudia Eckert. Adversarial label flips attack on support vector machines.\nEuropean Conference on Artificial Intelligence, pp. 870–875, 2012.\nHuang Xiao, Battista Biggio, Blaine Nelson, Han Xiao, Claudia Eckert, and Fabio Roli. Support\nvector machines under adversarial label contamination. Neurocomputing, 160:53–62, 2015.\nYuancheng Xu, Yanchao Sun, Micah Goldblum, Tom Goldstein, and Furong Huang. Exploring and\nexploiting decision boundary dynamics for adversarial robustness. International Conference on\nLearning Representations, 2023.\nYao-Yuan Yang, Cyrus Rashtchian, Hongyang Zhang, Russ R Salakhutdinov, and Kamalika Chaud-\nhuri. A closer look at accuracy vs. robustness. Advances in Neural Information Processing Systems,\n33:8588–8601, 2020.\nSarath Yasodharan and Patrick Loiseau. Nonzero-sum adversarial hypothesis testing games. Advances\nin Neural Information Processing Systems, 32:11, 2019.\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding\ndeep learning requires rethinking generalization. ArXiv, 1611.03530, 2017.\nJingfeng Zhang, Jianing Zhu, Gang Niu, Bo Han, Masashi Sugiyama, and Mohan Kankanhalli.\nGeometry-aware instance-reweighted adversarial training. International Conference on Learning\nRepresentations, 2021.\nPeng-Fei Zhang, Zi Huang, Xin-Shun Xu, and Guangdong Bai. Effective and robust adversarial\ntraining against data and label corruptions. IEEE Transactions on Multimedia, 26:9477, 2024.\nMengchen Zhao, Bo An, Wei Gao, and Teng Zhang. Efficient label contamination attacks against\nblack-box learning models. International Joint Conference on Artificial Intelligence, pp. 3945–\n3951, 2017.\nMeixi Zheng, Xuanchen Yan, Zihao Zhu, Hongrui Chen, and Baoyuan Wu. Blackboxbench: A\ncomprehensive benchmark of black-box adversarial attacks. ArXiv, 2312.16979, 2023.\nYan Zhou, Murat Kantarcioglu, Bhavani M. Thuraisingham, and Bowei Xi. Adversarial support\nvector machine learning. Knowledge Discovery and Data Mining, pp. 1059–1067, 2012.\nYan Zhou, Murat Kantarcioglu, and Bowei Xi. A survey of game theoretic approach for adversarial\nmachine learning. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 9(3):\ne1259, 2019.\nD. Zhu, Michael A. Hedderich, Fangzhou Zhai, David Ifeoluwa Adelani, and Dietrich Klakow. Is\nBERT robust to label noise? A study on learning with noisy labels in text classification. ArXiv,\n2204.09371, 2022.\n14\n\nPublished as a conference paper at ICLR 2025\nAppendix\nTable of Contents\nA Theoretical Analysis Proofs\n16\nA.1\nProof of Lemma 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\nA.2\nProof of Lemma 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\nA.3\nProof of Theorem 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\nA.4\nGlobal convergence result . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nB\nThe Gradient of the Objective (4)\n20\nC Experiment Details\n21\nC.1\nDatasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\nC.2\nBaselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\nC.3\nRoBERTa Experiment Details . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\nD Effectiveness Analysis of FLORAL Defense\n22\nE Additional Experimental Results\n24\nE.1\nMoon\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\nE.2\nIMDB . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\nE.3\nMNIST\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\nE.4\nSensitivity Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\nF\nExtension to Multi-class Classification\n29\nG Comparison Against Additional Methods\n29\nH Experiments Under Different Label Attacks\n30\nH.1\nExperiments with the alfa-tilt attack\n. . . . . . . . . . . . . . . . . . . .\n31\nH.2\nExperiments with the alfa attack\n. . . . . . . . . . . . . . . . . . . . . . . .\n32\nH.3\nExperiments with the LFA attack . . . . . . . . . . . . . . . . . . . . . . . . .\n33\nI\nIntegration with Neural Networks\n34\n15\n\nPublished as a conference paper at ICLR 2025\nA\nTHEORETICAL ANALYSIS PROOFS\nIn this section, we present the proofs for the local asymptotic stability analysis of FLORAL (Algo-\nrithm 1). We begin by proving Lemma 1 in Section A.1, which establishes that the distance of the\nupdates of Algorithm 1 from the equilibrium of the game is bounded. In Section A.2, we prove\nLemma 2, demonstrating that the distance of the non-projected updates from the equilibrium of the\ngame is also bounded. Lastly, in Section A.3, we provide the proof of Theorem 3.1, which shows the\nlocal asymptotic stability of our algorithm, with a derivation of a global convergence result presented\nin Section A.4.\nA.1\nPROOF OF LEMMA 1\nOur objective is to prove that the distance of the iterates of Algorithm 1 from the Stackelberg\nequilibrium (ˆλ, ˆy(ˆλ)), specifically λt −ˆλ, is bounded. We begin by recalling the update rule at round\nt, λt := PROXS(yt)(zt) = PROXS(yt)(λt−1 −η∇λf(λt−1, yt)), where yt = ˆy(λt−1), S(yt) is the\nfeasible region defined by constraints (12), using the labels at round t. The operator PROX is defined\nbelow.\nDefinition 1 (PROX operator). The operator PROXS(yt)(zt) : Rn →Rn denotes the projection of\nzt ∈Rn onto the convex set S(yt) at round t of Algorithm 1. PROX minimizes the Euclidean distance\nand is defined by the following optimization problem:\nPROXS(yt)(zt) : λt ∈arg min\nλ∈Rn\n1\n2∥λ −zt∥2\n(16)\nsubject to\nyT\nt λ = 0\n(17)\n0 ≤λ ≤C\n(18)\nEquivalently, PROXS(yt) solves the following optimization problem:\nmin\nλ∈Rn\n0≤λ≤C\nsup\nµ∈R\n1\n2∥λ −zt∥2 + µyT\nt λ.\n(19)\nLemma 3 (Bounded iterates). The sequence {λt} generated by the iterative update rule λt :=\nPROXS(yt)(zt) = PROXS(yt)(λt−1 −η∇λf(λt−1, yt)) is bounded, i.e., ∥λt∥∞≤C, ∀t ≥0.\nProof. This follows immediately from the definition of S(yt).\nIn the following, our aim is to quantify the sensitivity of (19) with respect to its arguments yt and zt.\nLet λ⋆denote the optimal solution to the projection operation. We can express this solution through\nthe following steps. First, we simplify the expression by omitting the index t in (19). Then, we\nexploit the fact that the objective function is convex-concave with convex constraints, which allows\nus to interchange the order of the min and the sup. This yields\nmin\nλ∈Rn\n0≤λ≤C\nsup\nµ∈R\n1\n2∥λ −z∥2 + µyTλ\n= sup\nµ∈R\nmin\nλ∈Rn\n0≤λ≤C\n1\n2∥λ −z + µy∥2 −1\n2µ2∥y∥2.\nAt this stage, the optimization problem over λ reduces to the minimization of a quadratic function\nover box constraints. We can therefore express λ⋆based on the optimal choice µ⋆for µ as follows:\nλ⋆\ni =\n\n\n\n0,\nif zi −µ⋆(z, y)yi ≤0\nzi −µ⋆(z, y)yi,\nif 0 < zi −µ⋆(z, y)yi < C\nC,\nif zi −µ⋆(z, y)yi ≥C\n\n\nand choose µ⋆(z, y) such that yTλ⋆= 0,\n∀i ∈{1, . . . , n}. We use the notation µ⋆(z, y) to highlight the dependency of the multiplier µ⋆on\nthe variable z and the label y.\nWe introduce the CLIP[0,C](·) operator which clips the value of the given input to the interval [0, C].\nThis operator yields the following compact expression for λ⋆:\nλ⋆= CLIP[0,C](z −µ⋆(z, y)y), where µ⋆(z, y) is chosen such that yTλ⋆= 0.\n(20)\n16\n\nPublished as a conference paper at ICLR 2025\nBy substituting the previous expression for λ⋆into the equality constraint, we obtain\nyTCLIP[0,C](z −µ⋆(z, y)y) = 0,\n(21)\nwhich provides an equation that implicitly defines µ⋆(z, y). We further simplify (21) by indexing the\ncomponents of z −µ⋆(z, y)y with respect to their values, which yields\n0 =\nX\ni∈IC\nCyi +\nX\ni∈Iz\nyi(zi −µ⋆(z, y)yi)\n=\nX\ni∈IC\nCyi +\nX\ni∈Iz\nziyi −\nX\ni∈Iz\nµ⋆(z, y)y2\ni\n=\nX\ni∈IC\nCyi +\nX\ni∈Iz\nziyi −\nX\ni∈Iz\nµ⋆(z, y).\n(from y2\ni = 1)\nwhere Iz := {i | λi = zi −µ⋆(z, y)yi ∈(0, C)} with cardinality | Iz | and IC := {i | λi =\nzi −µ⋆(z, y)yi ≥C}. We further solve for µ⋆, which yields\nµ⋆(z, y) =\n1\n| Iz |\n X\ni∈IC\nCyi +\nX\ni∈Iz\nziyi\n!\n.\nThis equation implicitly defines µ⋆(z, y), which represents the basis for the fixed point iteration\nintroduced in Algorithm 2.\nThis equation will also be the basis for computing sensitivities, i.e. quantifying how λ⋆and µ⋆change\nwhen altering z or λ. We first compute ∂λ⋆\n∂z . For a data point i, the following can be stated:\n∂λ⋆\ni\n∂z =\n(\neT\ni −∂µ⋆(z,y)\n∂z\nyi,\nif zi −µ⋆(z, y)yi ∈(0, C)\n0,\nelse,\n(22)\nwhere ei is the ith standard basis vector. Differentiating the constraint (17) yields\n0 = ∂(yTλ⋆)\n∂z\n=\nn\nX\ni=1\n∂λ⋆\ni\n∂z yi\n=\nX\ni∈Iz\n\u0012\neT\ni yi −∂µ⋆(z, y)\n∂z\ny2\ni\n\u0013\n.\nSubstituting y2\ni = 1 into the previous equation yields\n∂µ⋆(z, y)\n∂z\n=\nP\ni∈Iz\neT\ni yi\n| Iz |\n,\n(23)\nwhere Iz with cardinality | Iz | is defined previously. From (22) and (23), we have\n∂λ⋆\ni\n∂z =\n\n\n\neT\ni −\nP\nj∈Iz\neT\nj yj\n|Iz|\nyi,\nif i ∈Iz\n0,\nif i /∈Iz.\nTherefore, we conclude that\n\r\r\r\r\n∂λ⋆\ni\n∂z\n\r\r\r\r\n∞\n≤1, ∀i ∈[n],\n(24)\nwhere we have exploited the fact that yi ∈{±1}.\nWe further note that in the situation Iz = ∅, λ⋆∈{0, C}, a change in z or y will not affect λ⋆unless\nz = µ⋆y or z = C + µ⋆y. As a result, we have for Iz = ∅, ∂λ⋆\n∂z = ∂λ⋆\n∂y = 0 (a.e.).\nWe now compute ∂λ⋆\n∂y . For a data point i, the following holds:\n∂λ⋆\ni\n∂y =\n(\n−∂µ⋆(z,y)\n∂y\nyi −eT\ni µ⋆(z, y),\nif i ∈Iz\n0,\nif i /∈Iz.\n(25)\n17\n\nPublished as a conference paper at ICLR 2025\nDifferentiating the constraint (17) with respect to y yields\n0 = ∂(yTλ⋆)\n∂y\n= λ⋆T +\nn\nX\ni=1\n∂λ⋆\ni\n∂y yi\n= λ⋆T +\nX\ni∈Iz\n\u0012\n−∂µ⋆(z, y)\n∂y\n|yi|2 −yiµ⋆(z, y)eT\ni\n\u0013\n.\nIt follows from |yi|2 = 1 that\n∂µ⋆(z, y)\n∂y\n=\nλ⋆T −µ⋆(z, y) P\ni∈Iz\nyieT\ni\n| Iz |\n.\n(26)\nFrom (25) and (26), we obtain the following.\n∂λ⋆\ni\n∂y =\n\n\n\n\n\n−yiλ⋆T\n|Iz| + µ⋆(z, y)\n \nyi\nP\nj∈Iz\neT\nj yj\n|Iz|\n−eT\ni\n!\n,\nif i ∈Iz\n0,\nif i /∈Iz.\nAs a result, we conclude using Lemma 3 that the following bound holds ∀i ∈[n]\n\r\r\r\r\n∂λ⋆\ni\n∂y\n\r\r\r\r\n∞\n≤∥λ∥∞\n| Iz | + | µ⋆(z, y) |≤\nC\n| Iz |+ | µ⋆|\n|\n{z\n}\nκy\n,\n(27)\nwhere κy is a constant that only depends on C and the features of the dataset.\nFrom (24) and (27), we conclude that\n∥λt −ˆλ∥∞= ∥λ⋆(zt, yt) −λ⋆(zt, ˆy(ˆλ)) + λ⋆(zt, ˆy(ˆλ)) −λ⋆(ˆz, ˆy(ˆλ))∥∞\n≤κy∥yt −ˆy(ˆλ)∥∞+ ∥zt −ˆz∥∞.\nA.2\nPROOF OF LEMMA 2\nOur objective is to prove that the distance of the non-projected updates of Algorithm 1 from the\nStackelberg equilibrium (ˆλ, ˆy(ˆλ)), specifically zt −ˆz, is bounded.\nWe begin by recalling the update rule at round t, λt := PROXS(yt)(zt) = PROXS(yt)(λt−1 −\nη∇λf(λt−1, yt)), where yt = ˆy(λt−1), S(yt) is the feasible set defined by constraints (12), using\nthe labels at round t. We further recall the Stackelberg equilibrium (ˆλ, ˆy(ˆλ)), i.e.,\nˆλ := PROXS(ˆy(ˆλ))(ˆz) = PROXS(ˆy(ˆλ))(ˆλ −η∇λf(ˆλ, ˆy(ˆλ)))\nˆy(ˆλ) := LFLIP(ˆλ),\nwhere the operator LFLIP : X × Y →Y defines the label poisoning attack formulated in (7-9). We\nconclude the following:\nzt = λt−1 −η∇λf(λt−1, yt)\nˆz = ˆλ −η∇λf(ˆλ, ˆy(ˆλ))\nzt −ˆz = λt−1 −ˆλ −η\n\u0010\n∇λf(λt−1, yt) −∇λf(ˆλ, ˆy(ˆλ))\n\u0011\n.\nWe apply the mean value theorem for functions with multiple variables to the previous expression\nwhich allows us to rewrite zt −ˆz as\n= λt−1 −ˆλ −η\n\u0010\n∇λf(λt−1, yt) −∇λf(ˆλ, yt) + ∇λf(ˆλ, yt) −∇λf(ˆλ, ˆy(ˆλ))\n\u0011\n= λt−1 −ˆλ −η\n\u0010\n∇2\nλf(ξλ, yt)(λt−1 −ˆλ) + ∇2\nλyf(ˆλ, ξy)(yt −ˆy(ˆλ))\n\u0011\n,\nwhere ξλ ∈(ˆλ, λt−1) and ξy ∈(ˆy(ˆλ), yt). The last equation can be restated as:\nzt −ˆz = (I −η∇2\nλf(ξλ, yt))(λt−1 −ˆλ) −η∇2\nλyf(ˆλ, ξy)(yt −ˆy(ˆλ)),\n(28)\n18\n\nPublished as a conference paper at ICLR 2025\nwhere I denotes the identity matrix. We have defined the gradient of the objective in (10) as\n∇λf(λ, y) = ˜Qλ −1, where ˜Q is the matrix with entries ˜Qij = yiyjKij, ∀i, j ∈[n], using the\nsimplified notation. We express the second-order partial derivatives as:\n∇2\nλf(λ; y) = K ⊙yyT,\n(29)\n∇2\nλyf(λ; y) = K ⊙yλT + I ⊙(K(λ ⊙y)1T),\n(30)\nwhere K is the Gram matrix, I is the n × n identity matrix, 1 is the all-one vector and ⊙denotes the\nHadamard product. From (28), (29) and (30), we obtain\nzt −ˆz = (I −η\n\u0000(K ⊙ytyT\nt )\n\u0001\n(λt−1 −ˆλ)\n−η\n\u0010\nK ⊙ξyˆλT + I ⊙(K(ˆλ ⊙ξy)1T)\n\u0011\n(yt −ˆy(ˆλ)).\nWe take the infinity norm and conclude:\n∥zt −ˆz∥∞= ∥(I −η\n\u0000K ⊙ytyT\nt\n\u0001\n)(λt−1 −ˆλ)\n−η\n\u0010\nK ⊙ξyˆλT + I ⊙(K(ˆλ ⊙ξy)1T)\n\u0011\n(yt −ˆy(ˆλ))∥∞\n≤∥(I −η\n\u0000K ⊙ytyT\nt\n\u0001\n)(λt−1 −ˆλ)∥∞\n+ ∥−η\n\u0010\nK ⊙ξyˆλT + I ⊙(K(ˆλ ⊙ξy)1T)\n\u0011\n(yt −ˆy(ˆλ))∥∞\n(triangle inequality)\n= ∥(I −η\n\u0000K ⊙ytyT\nt\n\u0001\n)(λt−1 −ˆλ)∥∞\n+ ∥η\n\u0010\nK ⊙ξyˆλT + I ⊙(K(ˆλ ⊙ξy)1T)\n\u0011\n(yt −ˆy(ˆλ))∥∞\n(homogeneity)\n≤∥(I −η\n\u0000K ⊙ytyT\nt\n\u0001\n)∥∞\n|\n{z\n}\nκλ\n∥λt−1 −ˆλ∥∞\n+ ∥η\n\u0010\nK ⊙ξyˆλT + I ⊙(K(ˆλ ⊙ξy)1T)\n\u0011\n∥∞\n|\n{z\n}\nκ′y\n∥yt −ˆy(ˆλ)∥∞.\n(homogeneity)\nThis implies that\n∥zt −ˆz∥∞≤κλ∥λt−1 −ˆλ∥∞+ κ′\ny∥yt −ˆy(ˆλ)∥∞.\nWe note that κλ ≤1 if the learning rate η is chosen small enough.\nA.3\nPROOF OF THEOREM 3.1\nLet (ˆλ, ˆy(ˆλ)) denote the Stackelberg equilibrium, i.e.,\nˆλ := PROXS(ˆy(ˆλ))(ˆz) = PROXS(ˆy(ˆλ))(ˆλ −η∇λf(ˆλ, ˆy(ˆλ)))\nˆy(ˆλ) := LFLIP(ˆλ),\nwhere the operator LFLIP : X × Y →Y defines the label poisoning attack formulated in (7-9). We\nfurther assume that the LFLIP operator returns a unique set of adversarial labels at the Stackelberg\nequilibrium (ˆλ, ˆy(ˆλ)), which implies that there are no ties with respect to ˆλ values. As a result, there\nexists a small enough constant δ′ > 0 such that for any λ0 with ∥λ0 −ˆλ∥∞< δ′, the corresponding\nˆy(λ0) satisfies ˆy(λ0) = ˆy(ˆλ). (Indeed, as long as δ′ is small enough, such that the top-k entries\nbetween ˆλ and λ0 agree, ˆy(λ0) = ˆy(ˆλ) will be satisfied.)\nBy combining Lemma 1 and Lemma 2 we conclude\n∥λ1 −ˆλ∥∞≤κy∥ˆy(λ0) −ˆy(ˆλ)∥∞+ ∥z1 −ˆz∥∞≤∥z1 −ˆz∥∞≤κλ∥λ0 −ˆλ∥∞< κλδ′,\nwhere we used the fact that ˆy(λ0) = ˆy(ˆλ). The learning rate η is chosen small enough, such that\nκλ < 1 and therefore ∥λ1 −ˆλ∥∞< κλδ′ < δ′. We therefore conclude by induction on t that\n∥λt −ˆλ∥∞< κt\nλδ′ for all t > 0. This readily implies λt →ˆλ. Moreover, choosing δ = min{ϵ, δ′}\nconcludes ∥λt −ˆλ∥∞< ϵ and concludes the proof.\n19\n\nPublished as a conference paper at ICLR 2025\nA.4\nGLOBAL CONVERGENCE RESULT\nThe previous section provides the proof of Theorem 3.1, which provides a local stability and\nconvergence result. Under additional assumptions on the constants κy and κ′\ny that capture the\nsensitivity of the iterates λt with respect to changes in the labels, one can derive a global convergence\nresult, as summarized by the following proposition:\nProposition 1. Let (ˆλ, ˆy(ˆλ)) denote the Stackelberg equilibrium as before and let δ′ = (ˆλ{k} −\nˆλ{k+1})/2 > 0, where ˆλ{1} denotes the largest entry of ˆλ, ˆλ{2} the second larges entry of ˆλ, etc.\nProvided that\n2(κy + κ′\ny)k\n1 −κλ\n< δ′\nholds and that the step-size η is chosen to be small enough, the iterates {λt} of FLORAL are\nguaranteed to converge to ˆλ from any initial condition λ0.\nProof. As a result of Lemma 1 and Lemma 2 we conclude that\n∥λt −ˆλ∥∞≤κy∥ˆy(λt−1) −ˆy(ˆλ)∥∞+ ∥zt−1 −ˆz∥∞\n≤(κy + κ′\ny)∥ˆy(λt−1) −ˆy(ˆλ)∥∞+ κλ∥λt−1 −λ0∥∞.\nWe further take advantage of the fact that ∥ˆy(λ) −ˆy(ˆλ)∥∞≤2k for any λ (at most k labels are\nflipped), which implies\n∥λt −ˆλ∥∞≤κλ∥λt−1 −λ0∥∞+ 2(κy + κ′\ny)k.\nThe previous inequality is satisfied for all t, and can be used to conclude that\n∥λt −ˆλ∥∞≤κt\nλ∥λ0 −ˆλ∥∞+ 2(κy + κ′\ny)k\n1 −κλ\n(31)\nholds for all t (this can be verified by an induction argument). As a result, there exists an integer\nt′ > 0 such that ∥λt−ˆλ∥∞< δ′ for all t > t′. This implies, due to the choice of δ′, that ˆy(λt) = ˆy(ˆλ)\nfor all t > t′. We therefore conclude that for all t > t′ + 1\n∥λt −ˆλ∥∞≤κλ∥λt−1 −ˆλ∥∞.\nThis readily implies λt →ˆλ, due to the fact that κλ < 1, and implies the desired result.\nB\nTHE GRADIENT OF THE OBJECTIVE (4)\nWe begin by recalling the kernel SVM dual formulation (Boser et al., 1992; Hearst et al., 1998):\nD(fλ; D) : min\nλ∈Rn\n1\n2\nn\nX\ni=1\nn\nX\nj=1\nλiλjyiyjKij −\nn\nX\ni=1\nλi\nsubject to\nn\nX\ni=1\nλiyi = 0\n0 ≤λi ≤C, ∀i ∈[n],\nwhere K represents the Gram matrix with entries Kij = k(xi, xj), ∀i, j ∈[n] := {1, . . . , n},\nderived from a kernel function k. We consider the pth data point and apply differentiation of a double\nsummation to the objective, which yields\n∂(D(fλ; D))\n∂λp\n= 1\n2\n\n\nn\nX\ni=1\nλiyiypKip +\nn\nX\nj=1\nλjypyjKpj\n\n−1\n= yp\nn\nX\ni=1\nλiyiKip −1.\n(from the symmetry of the kernel function)\nIn compact form, we obtain the following.\n∇λD(fλ; D) = Qλ −1,\nwhere Q is the matrix with entries Qij = yiyjKij, ∀i, j ∈[n] and 1 is the vector of all ones.\n20\n\nPublished as a conference paper at ICLR 2025\nC\nEXPERIMENT DETAILS\nFor our experiments, we set the hyperparameter values as given in Table 3. We provide the experiment\ndetails as follows.\n• We initialize the model fλ0 with parameters set to 0. In FLORAL, however, the attacker uses a\nrandomized top-k rule to identify the B most influential support vectors based on the λ values.\nDue to the 0 initialization of λ, a warm-up period is required, which we set to 1 round for all\nSVM-related methods.\n• To train kernel SVM classifiers for all SVM-related methods other than FLORAL, we use our PGD-\nbased Algorithm 1 with a dummy attack, that is, we eliminate the adversarial dataset generation\nstep and employ vanilla PGD training.\n• For large datasets such as IMDB, we implement projection via fixed point iteration as given in\nAlgorithm 2 in Section 3.2 instead of constructing a quadratic program as defined in (11-12).\nTable 3: Hyperparameter values.\nSymbol\nHyperparameter\nValue\nn\nThe size of the training dataset\nMoon: 500, IMDB: 20000\nT\nThe number of training rounds\nMoon: 500, IMDB: 1000\nTproj\nThe number of projection via fixed point iteration rounds\n1000\nB\nThe attacker budget\nMoon: {10, 20, 50, 100, 250}, IMDB: {500, 2500, 5000, 12500}\nk\nThe number of labels to poison\nMoon: {5, 10, 25, 50, 125}, IMDB {250, 1250, 2500, 6250}\nC\nRegularization parameter for soft-margin SVM\nMoon: {10, 100}, IMDB: 10\nγ\nRBF kernel parameter\nMoon: {0.5, 1, 10}, IMDB: 0.005\nϵ\nError rate for projection via fixed point iteration\n1e −21\nη\nLearning rate\nOptimized over the set {0.0001, 0.0003, 0.0005, 0.0007}, for RoBERTa: 2e −05\nLearning rate scheduler\nMoon: a decay rate of 0.1 at every {100, 200} rounds (optimized), for RoBERTa: linear scheduler\nThe model architecture for NN and NN-PGD\nFully connected MLP with 2 hidden layers with 32 units each\nBatch size\n32\nNN-PGD perturbation amount\n8/255\nNN-PGD step size\n2/255\nSGD optimizer momentum value\n0.9\nC.1\nDATASETS\n• Moon is a benchmark dataset for binary classification tasks, generated directly using the\nscikit-learn library (Pedregosa et al., 2011). It contains two-dimensional input examples\nwith each feature taking value in the range [−2.5, 2.5]. We generate its adversarial versions by\nflipping the labels of farthest points from the decision boundary of a linear classifier trained on the\nclean dataset, using label poisoning levels (%) of {5, 10, 15, 20, 25}. We provide the visualizations\nof the Moon training dataset with clean and adversarial labels in Figure 5.\n• IMDB review sentiment analysis benchmark dataset (Maas et al., 2011) contains train and test\ndatasets, each containing 25, 000 examples. We used randomly selected 20, 000 points from the\ntraining set as training examples, and the rest as validation examples. We fine-tuned the RoBERTa-\nbase model 1 (Liu et al., 2019) on this dataset and extracted features (768-dimensional embeddings)\nto train SVM-related models on this dataset. We generated adversarially labelled datasets using\nthe fine-tuned RoBERTa-base model on the clean dataset. Specifically, we identified the most\ninfluential training points based on the gradient of loss with respect to the inputs and flipped their\nlabels under various poisoning levels (%) of {10, 25, 30, 35, 40}.\n(a) Clean.\n(b) Dadv = 5%.\n(c) Dadv = 10%.\n(d) Dadv = 25%.\nFigure 5: Illustrations of the Moon training sets from an example replication, using clean and\nadversarial labels with poisoning levels: 5%, 10%, 25%.\n1The pre-trained RoBERTa-base model can be found in https://huggingface.co/FacebookAI/\nroberta-base.\n21\n\nPublished as a conference paper at ICLR 2025\nC.2\nBASELINES\nIn our main experiments, we compared FLORAL against the baseline methods by carefully selecting\ntheir hyperparameters using the domain knowledge, which we detail below.\n• LN-SVM (Biggio et al., 2011) applies a heuristic-based kernel matrix correction by assuming that\nevery label in the training set is independently flipped with the same probability. It requires a\npredefined noise parameter µ, which we set to µ ∈{0.05, 0.1, 0.15, 0.2, 0.25} by leveraging the\ndomain label poisoning knowledge, i.e. using the poisoning levels of the adversarial datasets.\n• For\nCurie\n(Laishram\n&\nPhoha,\n2016),\nwe\nset\nthe\nconfidence\nparameter\nto\n{0.95, 0.90, 0.85, 0.8, 0.75}.\nTo compute the average distance, we considered k\n=\n20\nneighbors in the same cluster for the Moon dataset and k = 1000 neighbors for the IMDB dataset\nexperiments.\n• For LS-SVM (Paudice et al., 2018), we use the relabeling confidence threshold from\n{0.95, 0.90, 0.85, 0.8, 0.75}, again aligning with the poisoning level of the adversarial datasets. For\nits k-NN step, we considered k = 20 and k = 1000 neighbors for the Moon and IMDB datasets,\nrespectively.\n• NN baseline is a fully connected multi-layer perceptron with two hidden layers with 32 units each,\ntrained using the SGD optimizer with 0.9 momentum and binary cross-entropy loss. For additional\nexperiments on the MNIST dataset, a similar architecture with two hidden layers having {32, 10}\nunits is employed.\n• NN-PGD is based on the same NN architecture as above, trained with PGD-AT (Madry et al.,\n2017) using a standard perturbation budget of 8/255 and a step size of 2/255.\nC.3\nROBERTA EXPERIMENT DETAILS\nWe fine-tune the RoBERTa-base model1 on the IMDB review sentiment analysis dataset2. We fine-\ntune the model for three epochs with no warm-up steps, using the AdamW optimizer, weight decay\n0.01, batch size 16, and learning rate 2e−05 with a linear scheduler, using a single NVIDIA A100\n40GB GPU. We extract the last layer embeddings of the trained model for experiments with FLORAL\nintegration.\nD\nEFFECTIVENESS ANALYSIS OF FLORAL DEFENSE\nAs explained in Section 3, FLORAL takes a proactive defense when the initial training data is clean,\niteratively adjusting the model to reduce sensitivity to potential label poisoning attacks by exposing\nit to adversarial decision boundary configurations through adversarial training. Conversely, when\nthe training data is already contaminated with adversarial labels, FLORAL mitigates their effect by\nimplicitly sanitizing the corrupted labels.\nTo demonstrate how FLORAL defenses under already poisoned training data, we further analyze\nthe efficacy of FLORAL by measuring its \"recovery\" rate of poisoned labels. That is, we quantify\nFLORAL’s rate of disrupting the initial attack (%) on the adversarially labelled training sets, averaged\nover replications.\nAs reported in Figure 6a on the adversarial Moon datasets, FLORAL is able to disrupt the initial label\nattack (already inherited in the training set), at a 25 −35% rate. This contributes to the success of\nthe FLORAL in achieving higher robust accuracy in training with adversarial datasets. Moreover, we\nprovide example illustrations (Figures 6b-6d) that show which poisoned data points are recovered by\nFLORAL under the randomized top-k attack.\n2The IMDB review sentiment dataset can be found in https://huggingface.co/datasets/\nstanfordnlp/imdb.\n22\n\nPublished as a conference paper at ICLR 2025\nC=10, gamma=1\nC=10, gamma=0.5\nC=100, gamma=10\nSetting\n0\n5\n10\n15\n20\n25\n30\n35\n% Identified\nDadv_5\nDadv_5\nDadv_5\nDadv_10\nDadv_10\nDadv_10\nDadv_25\nDadv_25\nDadv_25\n% Identified Poisoned Labels\n(a) Average (%) poisoned labels recovered by FLORAL.\n2\n1\n0\n1\n2\nFeature 0\n2\n1\n0\n1\n2\nFeature 1\nNegative -1\nPositive +1\nPoisoned label\nLabel Identified by FLORAL\n(b) A trace for recovered points on Dadv = 5%.\n2\n1\n0\n1\n2\nFeature 0\n2\n1\n0\n1\n2\nFeature 1\nNegative -1\nPositive +1\nPoisoned label\nLabel Identified by FLORAL\n(c) A trace for recovered points on Dadv = 10%.\n2\n1\n0\n1\n2\nFeature 0\n2\n1\n0\n1\n2\nFeature 1\nNegative -1\nPositive +1\nPoisoned label\nLabel Identified by FLORAL\n(d) A trace for recovered points on Dadv = 25%.\nFigure 6: The average percentage of \"recovered\" poisoned labels by FLORAL over the adversarial\nMoon datasets containing {5, 10, 25} (%) poisoned labels. As shown in (a), FLORAL is able to\nrecover, on average, 25 −35% of the poisoned labels. The plots (b)-(d) illustrate example traces,\nshowing which poisoned data points are recovered by FLORAL.\n23\n\nPublished as a conference paper at ICLR 2025\nE\nADDITIONAL EXPERIMENTAL RESULTS\nWe provide additional experimental results under various hyperparameter settings for the Moon\ndataset in Appendix E.1. In Appendix E.2, we first report a comprehensive comparison of FLORAL\nagainst other baselines on the IMDB dataset, followed by an analysis of how FLORAL shifts the most\ninfluential training points for RoBERTa’s predictions on the IMDB dataset. In Appendix E.3, we\nprovide experiments on the MNIST (Deng, 2012) dataset. Finally, we present a sensitivity analysis\nwith respect to the attacker’s budget in Appendix E.4.\nE.1\nMOON\nWe report the clean and robust test accuracy of methods under different (non-optimal) kernel hyper-\nparameter choices and considering label poisoning levels {5, 10, 25}(%) in Figure 7 and Table 4.\nWhen the kernel hyperparameters are not optimally chosen, NN-PGD shows superior performance\nin less adversarial scenarios compared to SVM-based methods. However, it also demonstrates\nsignificant sensitivity to label attacks in 25% adversarial settings, against all other baselines. FLORAL\nparticularly advances by maintaining a higher robust accuracy in more adversarial settings.\n0\n100\n200\n300\n400\n500\nRounds\n0.800\n0.825\n0.850\n0.875\n0.900\n0.925\n0.950\n0.975\n1.000\nAccuracy\nTest Accuracy\n(a) Clean.\n0\n100\n200\n300\n400\n500\nRounds\n0.800\n0.825\n0.850\n0.875\n0.900\n0.925\n0.950\n0.975\n1.000\nAccuracy\nTest Accuracy\n(b) Dadv = 5%.\n0\n100\n200\n300\n400\n500\nRounds\n0.750\n0.775\n0.800\n0.825\n0.850\n0.875\n0.900\n0.925\n0.950\nAccuracy\nTest Accuracy\n(c) Dadv = 10%.\n0\n100\n200\n300\n400\n500\nRounds\n0.600\n0.625\n0.650\n0.675\n0.700\n0.725\n0.750\n0.775\n0.800\nAccuracy\nTest Accuracy\n(d) Dadv = 25%.\n0\n100\n200\n300\n400\n500\nRounds\n0.800\n0.825\n0.850\n0.875\n0.900\n0.925\n0.950\n0.975\n1.000\nAccuracy\nTest Accuracy\n(e) Clean.\n0\n100\n200\n300\n400\n500\nRounds\n0.800\n0.825\n0.850\n0.875\n0.900\n0.925\n0.950\n0.975\n1.000\nAccuracy\nTest Accuracy\n(f) Dadv = 5%.\n0\n100\n200\n300\n400\n500\nRounds\n0.750\n0.775\n0.800\n0.825\n0.850\n0.875\n0.900\n0.925\n0.950\nAccuracy\nTest Accuracy\n(g) Dadv = 10%.\n0\n100\n200\n300\n400\n500\nRounds\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nAccuracy\nTest Accuracy\n(h) Dadv = 25%.\nFLORAL\nVanilla SVM\nNN\nNN-PGD\nLN-SVM\nCurie\nLS-SVM\nK-LID\nFigure 7: Comparison of clean and robust test accuracy of methods trained on the Moon dataset under\ndifferent kernel hyperparameter choices. For all SVM-related models, the first row corresponds to the\nsetting (C = 10, γ = 0.5), whereas the second row shows the setting (C = 100, γ = 10). As the\nlevel of label poisoning increases, the accuracy of models trained on adversarial datasets generally\ndeclines. When the kernel parameters are not optimally chosen, FLORAL demonstrates superior\nperformance, particularly under the 25% attack.\nE.2\nIMDB\nWe report the test accuracy and loss performance of FLORAL against RoBERTa on the IMDB dataset\nin Figure 8 and Table 2. As demonstrated, FLORAL consistently exhibits superior accuracy and a\nsmaller loss in more adversarial problem instances, without sacrificing the clean performance. This\nshows the effectiveness of FLORAL in achieving robust classifiers when integrated with foundation\nmodels such as RoBERTa.\n24\n\nPublished as a conference paper at ICLR 2025\nTable 4: Test accuracies of methods trained on the Moon dataset. Each entry shows an average of\nfive runs. Highlighted values indicate the best performance in the \"Best\" (peak accuracy during\ntraining) and \"Last\" (final accuracy after training) columns.\nSetting\nMethod\nFLORAL\nSVM\nNN\nNN-PGD\nLN-SVM\nCurie\nLS-SVM\nK-LID\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nClean\nC = 10, γ = 0.5\n0.954\n0.950\n0.952\n0.952\n0.960\n0.960\n0.966\n0.964\n0.933\n0.933\n0.924\n0.924\n0.952\n0.952\n0.947\n0.947\nDadv = 5%\nC = 10, γ = 0.5\n0.941\n0.941\n0.938\n0.938\n0.926\n0.926\n0.964\n0.937\n0.933\n0.933\n0.892\n0.892\n0.938\n0.938\n0.933\n0.933\nDadv = 10%\nC = 10, γ = 0.5\n0.915\n0.874\n0.878\n0.878\n0.859\n0.855\n0.927\n0.853\n0.868\n0.868\n0.889\n0.889\n0.874\n0.874\n0.868\n0.868\nDadv = 25%\nC = 10, γ = 0.5\n0.769\n0.738\n0.717\n0.651\n0.693\n0.647\n0.740\n0.655\n0.717\n0.653\n0.731\n0.661\n0.696\n0.656\n0.717\n0.653\nClean\nC = 10, γ = 1\n0.968\n0.966\n0.968\n0.968\n0.960\n0.960\n0.966\n0.964\n0.940\n0.940\n0.941\n0.941\n0.881\n0.881\n0.966\n0.966\nDadv = 5%\nC = 10, γ = 1\n0.966\n0.966\n0.965\n0.957\n0.926\n0.926\n0.964\n0.937\n0.940\n0.940\n0.903\n0.903\n0.881\n0.881\n0.964\n0.964\nDadv = 10%\nC = 10, γ = 1\n0.924\n0.907\n0.912\n0.900\n0.859\n0.855\n0.927\n0.853\n0.869\n0.868\n0.907\n0.907\n0.894\n0.894\n0.908\n0.907\nDadv = 25%\nC = 10, γ = 1\n0.801\n0.768\n0.753\n0.717\n0.693\n0.647\n0.740\n0.655\n0.754\n0.693\n0.779\n0.697\n0.766\n0.721\n0.747\n0.690\nClean\nC = 100, γ = 10\n0.965\n0.964\n0.966\n0.964\n0.960\n0.960\n0.966\n0.964\n0.950\n0.949\n0.932\n0.931\n0.964\n0.964\n0.966\n0.964\nDadv = 5%\nC = 100, γ = 10\n0.955\n0.940\n0.951\n0.937\n0.926\n0.926\n0.964\n0.937\n0.951\n0.940\n0.888\n0.888\n0.947\n0.945\n0.951\n0.940\nDadv = 10%\nC = 100, γ = 10\n0.910\n0.877\n0.895\n0.874\n0.859\n0.855\n0.927\n0.853\n0.894\n0.888\n0.889\n0.881\n0.896\n0.875\n0.895\n0.876\nDadv = 25%\nC = 100, γ = 10\n0.740\n0.720\n0.697\n0.693\n0.693\n0.647\n0.740\n0.655\n0.697\n0.694\n0.760\n0.744\n0.701\n0.687\n0.697\n0.693\nTable 5: Test accuracy and loss of methods trained on the IMDB dataset. Each entry shows an average\nof five replications. Highlighted values indicate the best performance in the \"Best\" (peak accuracy\nduring training) and \"Last\" (final accuracy after training) columns. FLORAL demonstrates superior\nrobust accuracy and lower test loss compared to RoBERTa, particularly in more adversarial scenarios.\nSetting\nAccuracy\nLoss\nFLORAL\nRoBERTa\nFLORAL\nRoBERTa\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nClean\n0.911\n0.911\n0.911\n0.911\n0.196\n0.216\n0.229\n0.282\nDadv = 10%\n0.903\n0.903\n0.904\n0.903\n0.234\n0.259\n0.227\n0.231\nDadv = 25%\n0.889\n0.889\n0.882\n0.861\n0.310\n0.333\n0.337\n0.365\nDadv = 30%\n0.880\n0.880\n0.867\n0.835\n0.353\n0.366\n0.428\n0.428\nDadv = 35%\n0.871\n0.871\n0.827\n0.805\n0.381\n0.395\n0.496\n0.496\nDadv = 40%\n0.863\n0.863\n0.779\n0.771\n0.428\n0.439\n0.551\n0.551\n0\n1000\n2000\n3000\n4000\nSteps\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nAccuracy\nTest Accuracy\n(a) RoBERTa.\n0\n200\n400\n600\n800\n1000\nSteps\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nAccuracy\n0\n200\n400\n600\n800\n1000\n0.82\n0.84\n0.86\n0.88\n0.90\n0.92\nTest Accuracy\n(b) FLORAL.\n0\n1000\n2000\n3000\n4000\nSteps\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nLoss\nTest Loss\n(c) RoBERTa.\n0\n200\n400\n600\n800\n1000\nSteps\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nLoss\nTest Loss\n(d) FLORAL.\nRoBERTa_Clean\nAdv_10%\nAdv_25%\nAdv_30%\nAdv_35%\nAdv_40%\nFLORAL_Clean\nAdv_10%\nAdv_25%\nAdv_30%\nAdv_35%\nAdv_40%\nFigure 8: Test accuracy ((a)-(b)) and test loss ((c)-(d)) of methods on the IMDB dataset. FLORAL\nintegration outperforms fine-tuned RoBERTa in maintaining better test accuracy and converging\nfaster to lower loss, even when trained on extracted embeddings with heavily adversarial labels.\n25\n\nPublished as a conference paper at ICLR 2025\nAnalysis on influential training points.\nWe further analyze how the influential training points\n(affecting the model’s predictions) identified by FLORAL and RoBERTa change.\nTo illustrate, Figure 10 shows an example from a replication where both models are trained on a\ndataset with 40% adversarially labelled examples. We also provide the result for RoBERTa fine-tuned\non the clean dataset. For FLORAL, the most influential points are selected from the most important\nsupport vectors, while for RoBERTa, these are the points yielding the largest loss gradient with respect\nto the input. The example clearly demonstrates that FLORAL, implemented on RoBERTa-extracted\nembeddings, shifts the most important training point for the model’s decision boundary. FLORAL\nidentified a more descriptive point compared to others as given in Figure 10, however, further analysis\nis required to determine whether FLORAL consistently identifies such training points across all cases.\nAdditionally, we investigate the overlap in influential training points between the two methods. To this\nend, for each method, we extract the 25% most influential training points (for the model predictions)\namong the training dataset, and measure how much overlap between these two sets. In Figure 9, we\nreport the percentage of \"common\" influential points identified from the IMDB dataset, averaged over\nreplications, with error bars denoting the standard deviation. The left figure shows the percentage\noverlap between FLORAL trained on the IMDB dataset with different poison levels and RoBERTa\nfine-tuned on clean labels. Whereas, the right plot shows the overlap between both models trained on\nthe dataset with different poison levels. On the clean dataset, although there are some differences,\nboth methods almost identify the same set of influential points. However, as adversarial labels\nincrease, the overlap decreases. This shows that FLORAL extracts more critical points that enhance\nthe model’s robustness in adversarial settings, as supported by its superior robust accuracy, already\nshown in Figure 8 in Section 4.1.\nClean\nAdv_0.1\nAdv_0.25\nAdv_0.3\nAdv_0.35\nAdv_0.4\nDataset\n0\n10\n20\n30\n40\n50\n60\n70\n(%) common points\nCommon Influential Training Points (%): \n Identified by FLORAL and RoBERTa\n(a) w.r.t. RoBERTa (clean).\nClean\nAdv_0.1\nAdv_0.25\nAdv_0.3\nAdv_0.35\nAdv_0.4\nDataset\n0\n10\n20\n30\n40\n50\n60\n70\n80\n(%) common points\nCommon Influential Training Points (%): \n Identified by FLORAL and RoBERTa\n(b) w.r.t. RoBERTa (adversarial).\nFigure 9: The percentage of \"common\" influential points identified by FLORAL and RoBERTa from\nthe IMDB dataset, averaged over replications, with error bars denoting the standard deviation. \"Clean\"\nshows the dataset with clean labels, whereas adversarial datasets contain {10, 25, 30, 35, 40}(%)\npoisoned labels. Even when both methods are fine-tuned on the clean dataset, slight differences\nemerge in the identified influential training points. The discrepancy increases as the dataset becomes\nmore adversarial, highlighting that FLORAL adjusts the influential training points affecting the\nmodel’s predictions.\n26\n\nPublished as a conference paper at ICLR 2025\nSitting down to watch the 14th season of the Bachelor (\"On the Wings of Love\"), I knew I would be in for an \"interesting\" time. I had watched some of the\nprevious seasons of the Bachelor in passing; watching an episode or two and missing the next three or so. I find that the Bachelor is often appealing and\nintriguing, though its quality and morality are often lacking.<br /><br />\"On the Wings of Love\" details the journey taken by Jake, a 31 year old\ncommercial pilot from Dallas, Texas, to find true love, as true a love as one can find in a season-long reality-drama dating show. Jake meets 25\nbeautiful girls from all over the country. He begins to get to know them a bit, but it is mostly superficial; how well can you get to know someone in a\nfew 5 minute conversations? Jake tries to make his true intentions known from the very beginning, at least to the audience. He noted that he doesn't just\nwant love or a good time, but he wants a fiancé or wife. We can only assume that he has made this clear to the women in the competition as well. If that\nis the case, it might explain, to a degree, some of the women's actions. The women are super competitive. While they don't even know Jake at all yet,\nthey are still in it to win it no matter what the cost.<br /><br />Not only were the women competitive, but they were also confident and catty. Threats,\nbackstabbing, and warnings of \"Watch out!\" all show that these women weren't there for a good time either. Jake noted that he was not just looking for\nsex appeal, but looking for \"a connection.\" However, the girls pulled out all the stops to try to impress Jake with said sex appeal. They arrived at the\nmansion in skimpy dresses – either low-cut or short.<br /><br />While some girls seemed to maintain their sense of decorum, others missed that memo\naltogether. One girl, Channy, noted that Jake was a \"good guy\" to whom she could be a \"naughty girl.\" She went on to say that Jake could land on her\n\"runway anytime.\" She got flack from the other girls for her provocative statement which showed their take on these situations.<br /><br />So, a reality\ndating show couldn't be that bad, could it? Besides the obvious issue of sex-driven attraction, there are other issues that mar this seemingly harmless\nshow. Is this the right way to find a future mate; vying for someone's attention by flaunting oneself to extreme proportions? Unfortunately, however,\nthat is what America has reduced dating to these days: pleasure and sex without commitment and a little happiness on the side.<br /><br />Another problem\nis the premature emotional attachment by which many of the girls bound themselves to Jake. A few girls in particular seemed to be overly attached. One\ngirl said \"If I don't get that first impression rose it will kill me!\" As mentioned before, they don't even know him yet and she was talking about a\nspecific rose, not just one of the 15 roses to keep from being eliminated.<br /><br />Michelle, in particular, seemed to have some issues with attachment\nto Jake. The other girls noticed it too. After one particular Michelle outburst, Vienna asserted that Michelle had a \"mental breakdown and we've only\nbeen here an hour.\" Michelle got the last rose of the evening on the first show –narrowly missing elimination –and was extremely emotional about it. The\nother girls thought it was simply ridiculous. Another girl also cried, but because she was eliminated.<br /><br />It began with Survivor, and from there\nit just took off –reality TV. It shows our entertainment interests as a country; if we weren't watching the shows and giving them good ratings, the\nnetworks would not continue to run them. The only logical conclusion that can be drawn is that enough of America is hooked. One thing is clear: America\n(in general) loves reality TV and its ensuing trappings.<br /><br />This begs me to question: why is it that we even like reality TV? What is it about it\nthat draws us to it? Is it because we see the similarities to our own lives, or is it because we want to be sure that we are more stable and less\npathetic than others? Whatever it is that draws us to it, we should be careful of the media and entertainment that we allow to fill our minds. I'm not\nsaying that all reality TV shows are bad; however, I am saying that we need to evaluate each one.<br /><br />Episodes used for critique: Season Premier\nand Episode 2.\n(negative)\nFLORAL(Dadv=40%):\nThis documentary on schlockmeister William Castle takes a few cheap shots at the naive '50s-'60s environment in which he did his most characteristic\nwork--look at the funny, silly people with the ghost-glasses--but it's also affectionate and lively, with particularly bright commentary from John\nWaters, who was absolutely the target audience for such things at the time, and from Castle's daughter, who adored her dad and also is pretty perceptive\nabout how he plied his craft. (We never find out what became of the other Castle offspring.) The movies were not very good, it makes clear, but his\nmarketing of them was brilliant, and he appears to have been a sweet, hardworking family man. Fun people keep popping up, like \"Straight Jacket\"'s Diane\nBaker, who looks great, and Anne Helm, whom she replaced at the instigation of star Joan Crawford. Darryl Hickman all but explodes into giggles at the\nhappy memory of working with Castle on \"The Tingler,\" and there's enough footage to give us an idea of the level of Castle's talent--not very high, but\nvery energetic. A pleasant look at a time when audiences were more easily pleased, and it does make you nostalgic for simpler movie-going days.\n(positive)\nSomeone actually gave this movie 2 stars. There's a very high chance they need immediate professional help as anyone who doesn't spend 30 seconds to see\nif you can award no stars is quite literally scary.<br /><br />This film is ... well ... I guess it's pretty much some kind of attempt at a horrible porn\n/ snuff movie with no porn or no real horrible bits (apart from the acting, plot, story, sets, dialogue and sound). I wrongly assumed it was about\nzombies. <br /><br />Watching it is actually quite scary in fairness; you're terrified someone will come over and you'll never be able to describe what\nit is and they'll go away thinking you're a freak that watches home-made amateur torture videos or something along those lines. <br /><br />I'm so taken\naback I'm writing this review on my mobile so I don't forget to attempt to bring the rating down further than the current 1.6 to save others from the\nsame horrible fate that I just suffered. <br /><br />I worst film I've ever seen and I can say (with hand on heart) it will never, never be topped.\n(negative)\nRoBERTa(fine-tuned on clean):\nRoBERTa(fine-tuned on Dadv=40%):\nFigure 10: Note: Figure might contain offensive content. The most influential training point for\nthe model’s predictions, identified by FLORAL and RoBERTa from the IMDB dataset. FLORAL\nimplemented on RoBERTa extracted-embeddings changes the most important training point for the\nmodel’s decision boundary.\n27\n\nPublished as a conference paper at ICLR 2025\nE.3\nMNIST\nTo demonstrate the generalizability of FLORAL across diverse datasets, we provide additional\nexperiments on the MNIST dataset (Deng, 2012). Similar to (Rosenfeld et al., 2020), we consider\nclasses 1 and 7 which leads to a dataset of D = {(xi, yi)}13007\ni=1\nwhere xi ∈R784 and yi ∈{±1},\nwith 784 pixel values for each image.\nWe perform the randomized top-k label poisoning attack described in Section 3 and report the clean\nand robust test accuracy performance of methods in Figure 11 and Table 6. The results show that\nFLORAL maintains a higher robust accuracy compared to most of the baselines. While Curie behaves\nalmost on par, FLORAL achieves higher robust accuracy. Although NN baselines perform better\non clean and 5% adversarially labelled datasets, they show a significant accuracy decrease when\nthe training dataset gets more adversarial.\n0\n100\n200\n300\n400\n500\nRounds\n0.90\n0.92\n0.94\n0.96\n0.98\n1.00\nAccuracy\nTest Accuracy\n(a) Clean.\n0\n100\n200\n300\n400\n500\nRounds\n0.90\n0.92\n0.94\n0.96\n0.98\n1.00\nAccuracy\nTest Accuracy\n(b) Dadv = 5%.\n0\n100\n200\n300\n400\n500\nRounds\n0.86\n0.88\n0.90\n0.92\n0.94\n0.96\n0.98\n1.00\nAccuracy\nTest Accuracy\n(c) Dadv = 10%.\n0\n100\n200\n300\n400\n500\nRounds\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nTest Accuracy\n(d) Dadv = 25%.\nFLORAL\nVanilla SVM\nNN\nNN-PGD\nLN-SVM\nCurie\nLS-SVM\nK-LID\nFigure 11: Clean and robust test accuracy of methods trained on the MNIST-1vs7 dataset. \"Clean\"\nrefers to the dataset with clean labels, while the adversarial datasets contain {5, 10, 25} (%) poisoned\nlabels. For all SVM-related models, the setting C = 5, γ = 0.005 is used. As the level of\nlabel poisoning increases, models trained on adversarial datasets generally demonstrate a decline\nin accuracy. However, FLORAL maintains a higher robust accuracy level compared to most of the\nbaselines and behaving on par with the Curie method.\nTable 6: Test accuracies of methods trained over the MNIST-1vs7 dataset. Each entry shows\nthe average of five replications with different train/test splits. Highlighted values indicate the best\nperformance in the \"Best\" (peak accuracy during training) and \"Last\" (final accuracy after training)\ncolumns.\nSetting\nMethod\nFLORAL\nSVM\nNN\nNN-PGD\nLN-SVM\nCurie\nLS-SVM\nK-LID\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nClean\nC = 5, γ = 0.005\n0.992\n0.991\n0.992\n0.992\n0.993\n0.993\n0.995\n0.994\n0.987\n0.987\n0.990\n0.990\n0.978\n0.977\n0.987\n0.987\nDadv = 5%\nC = 5, γ = 0.005\n0.988\n0.984\n0.980\n0.974\n0.989\n0.982\n0.989\n0.949\n0.979\n0.979\n0.984\n0.979\n0.978\n0.977\n0.979\n0.979\nDadv = 10%\nC = 5, γ = 0.005\n0.984\n0.978\n0.964\n0.920\n0.982\n0.930\n0.982\n0.894\n0.965\n0.940\n0.974\n0.974\n0.978\n0.977\n0.966\n0.945\nDadv = 25%\nC = 5, γ = 0.005\n0.853\n0.830\n0.741\n0.741\n0.738\n0.738\n0.763\n0.750\n0.712\n0.712\n0.887\n0.822\n0.796\n0.795\n0.712\n0.712\nE.4\nSENSITIVITY ANALYSIS\nIn our experiments with the Moon dataset under varying label poisoning levels, we consider attacker\nbudgets B = 2k under varying k values, and report the best performing setting in Figure 3 in\nSection 4.1.\nHowever, we further investigate the sensitivity of FLORAL to the attacker’s budget B, by considering\nlevels B ∈{5, 10, 25, 50, 125}, with results presented in Figure 12. As demonstrated, FLORAL\nshows superior performance under a constrained attacker budget in the clean label scenario, as\nexpected, since an increasing number of adversarially labelled examples during training degrades\nclean test accuracy. In contrast, baseline methods operate on a fixed dataset. However, as the dataset\ngets more adversarial, FLORAL outperforms under higher attacker budgets.\n28\n\nPublished as a conference paper at ICLR 2025\n0\n100\n200\n300\n400\n500\nRounds\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nTest Accuracy\n(a) Clean.\n0\n100\n200\n300\n400\n500\nRounds\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nTest Accuracy\n(b) Dadv = 5%.\n0\n100\n200\n300\n400\n500\nRounds\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nTest Accuracy\n(c) Dadv = 10%.\n0\n100\n200\n300\n400\n500\nRounds\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nAccuracy\nTest Accuracy\n(d) Dadv = 25%.\nFLORAL_B=5\nFLORAL_B=10\nFLORAL_B=25\nFLORAL_B=50\nFLORAL_B=125\nFigure 12: The sensitivity of FLORAL to the attacker’s budget B. \"Clean\" refers to the dataset with\nclean labels, while the adversarial datasets contain {5, 10, 25}(%) poisoned labels. The performance\nunder setting (C = 10, γ = 1) is presented. As the level of label poisoning increases, FLORAL\nperforms better under higher attacker budget settings.\nF\nEXTENSION TO MULTI-CLASS CLASSIFICATION\nWe extend our algorithm to multi-class classification tasks, as detailed in Algorithm 3. The primary\nmodification involves adopting a one-vs-all approach (Hsu & Lin, 2002) by employing kernel\nSVM model f m\nλ0 for each class m ∈M and associating multiple attackers am, m ∈M for the\ncorresponding classifiers. In each round t, the attackers identify the Bm most influential data points\nwith respect to λm\nt values of the corresponding models under their constrained budgets Bm, and\ngather them into a set Bt. Among the points in Bt, the labels of top-k influential data points are\npoisoned according to a predefined label poisoning distribution q. The dataset with adversarial labels\nis then shared with each kernel SVM model and local training is applied via PGD training step.\nAlgorithm 3 FLORAL-MultiClass\n1: Input:\nInitial kernel SVM models f m\nλ0 for each class m\n∈\nM, training dataset\nD0 = {(xi, yi)}n\ni=1, xi ∈Rd, yi ∈{±1}, attackers’ budgets Bm, parameter k, where\nk ≪min{Bm}m∈M, learning rate η, a pre-defined label flip distribution q.\n2: for round t = 1, . . . , T do\n3:\nInitialize Bt ←∅.\n4:\nfor m ∈M do\n5:\nBm\nt ←Identify top-Bm support vectors w.r.t. λm\nt−1 values by solving (7-9).\n6:\nend for\n7:\nBt ←S\nm∈M Bm\nt .\n8:\n˜yt ←randomized top-k : Randomly select k points among Bt and poison their labels w.r.t. q.\n9:\nDt ←{(xi, ˜yt\ni)}n\ni=1\n*/ Adversarial dataset with selected k poisoned labels\n10:\nfor m ∈M do\n11:\nCompute gradient of the objective (4), ∇λD(f m\nλt−1; Dt), based on λm\nt−1, Dt as given in (10).\n12:\nTake a PGD step λm\nt ←PROX S(˜yt)(λm\nt−1 −η∇λD(fλm\nt−1; Dt)). */ Adversarial training\n13:\nend for\n14: end for\n15: return {f m\nλT }m∈M\nG\nCOMPARISON AGAINST ADDITIONAL METHODS\nWe additionally compare FLORAL against least squares classifier using randomized smoothing (RS)\n(Rosenfeld et al., 2020), and regularized synthetic reduced nearest neighbor (RSRNN) (Tavallali\net al., 2022) methods on the Moon and MNIST-1vs7 datasets. RS provides a robustness certification\nfor a linear classifier under label-flipping attacks. Whereas, RSRNN, conceptually similar to Curie\n(Laishram & Phoha, 2016), provides a filtering-out defense based on clustering.\n29\n\nPublished as a conference paper at ICLR 2025\nWe evaluated the performance under different noise (q) and l2 regularization (λ) hyperparameter\nvalues for the RS method suggested in (Rosenfeld et al., 2020), whereas we considered varying\nnumber of centroids K, penalty coefficient λ, cost complexity coefficient α, for the RSRNN method,\nusing referenced values in the study (Tavallali et al., 2022) for the MNIST dataset.\nThe results presented in Tables 7-10 demonstrate that FLORAL consistently outperforms both RS\nand RSRNN across all datasets and experimental settings. While RSRNN achieves comparable\nperformance on the MNIST dataset, it still falls short of FLORAL. The performance of the RS method,\nwhich employs a linear classifier with a pointwise robustness certificate, aligns with expectations, as\nits simpler classifier limits its ability to capture complex patterns. In contrast, FLORAL utilizes kernel\nSVMs, enabling it to effectively model intricate patterns within the data and achieve superior results.\nTable 7: Test accuracies of FLORAL against randomized smoothing (RS) method (Rosenfeld et al.,\n2020) on the Moon dataset. Each entry shows an average of five runs. We evaluated the performance\nunder different noise (q) values for RS.\nSetting\nMethod\nFLORAL\nRS (q = 0.1, λ = 0.01)\nRS (q = 0.3, λ = 0.01)\nRS (q = 0.4, λ = 0.01)\nClean\nC = 10, γ = 1\n0.968\n0.557\n0.509\n0.509\nDadv = 5%\nC = 10, γ = 1\n0.963\n0.552\n0.509\n0.509\nDadv = 10%\nC = 10, γ = 1\n0.954\n0.540\n0.509\n0.509\nDadv = 25%\nC = 10, γ = 1\n0.776\n0.520\n0.505\n0.505\nTable 8: Test accuracies of FLORAL against randomized smoothing (RS) method (Rosenfeld et al.,\n2020) on the MNIST-1vs7 dataset. Each entry shows an average of five runs. We evaluated the\nperformance under different noise (q) and l2 regularization (λ) hyperparameter values for RS, as\nsuggested in (Rosenfeld et al., 2020).\nSetting\nMethod\nFLORAL\nRS (q = 0.1, λ = 0.01)\nRS (q = 0.3, λ = 0.01)\nRS (q = 0.4, λ = 0.01)\nRS (q = 0.1, λ = 12291)\nRS (q = 0.3, λ = 12291)\nRS (q = 0.4, λ = 13237)\nClean\nC = 5, γ = 0.005\n0.991\n0.973\n0.921\n0.836\n0.940\n0.846\n0.732\nDadv = 5%\nC = 5, γ = 0.005\n0.984\n0.921\n0.876\n0.800\n0.895\n0.802\n0.701\nDadv = 10%\nC = 5, γ = 0.005\n0.978\n0.868\n0.831\n0.768\n0.830\n0.745\n0.673\nDadv = 25%\nC = 5, γ = 0.005\n0.830\n0.706\n0.693\n0.669\n0.548\n0.594\n0.595\nTable 9: Test accuracies of FLORAL against regularized synthetic reduced nearest neighbor (RSRNN)\n(Tavallali et al., 2022) trained on the Moon dataset. Each entry shows an average of five runs. We\nevaluated the performance under different hyperparameter values (number of centroids K, penalty\ncoefficient λ, cost complexity coefficient α) for the RSRNN method.\nSetting\nMethod\nFLORAL\nRSRNN (K = 2, α = 0.01, λ = 0.1)\nRSRNN (K = 10, α = 0.01, λ = 0.1)\nRSRNN (K = 10, α = 0.1, λ = 1)\nRSRNN (K = 20, α = 0.01, λ = 0.1)\nClean\nC = 10, γ = 1\n0.968\n0.505\n0.629\n0.688\n0.617\nDadv = 5%\nC = 10, γ = 1\n0.963\n0.502\n0.547\n0.603\n0.512\nDadv = 10%\nC = 10, γ = 1\n0.954\n0.502\n0.532\n0.566\n0.482\nDadv = 25%\nC = 10, γ = 1\n0.776\n0.494\n0.434\n0.476\n0.439\nTable 10: Test accuracies of FLORAL against regularized synthetic reduced nearest neighbor (RSRNN)\n(Tavallali et al., 2022) trained on the MNIST-1vs7 dataset. Each entry shows an average of five runs.\nWe evaluated the performance under different cost complexity coefficient (α) values for the RSRNN\nmethod.\nSetting\nMethod\nFLORAL\nRSRNN (K = 10, α = 0.1, λ = 1.0)\nRSRNN (K = 10, α = 1.0, λ = 1.0)\nClean\nC = 5, γ = 0.005\n0.991\n0.619\n0.692\nDadv = 5%\nC = 5, γ = 0.005\n0.984\n0.599\n0.441\nDadv = 10%\nC = 5, γ = 0.005\n0.978\n0.432\n0.408\nDadv = 25%\nC = 5, γ = 0.005\n0.830\n0.403\n0.408\nH\nEXPERIMENTS UNDER DIFFERENT LABEL ATTACKS\nTo show the generalizability of our approach in the presence of otherlabel poisoning attack types,\nwe further compare FLORAL against baselines on adversarial datasets generated using the alfa,\nalfa-tilt (Xiao et al., 2015) and LFA attacks (Paudice et al., 2018).\n30\n\nPublished as a conference paper at ICLR 2025\nH.1\nEXPERIMENTS WITH THE ALF A-T I L T ATTACK\nWe further evaluate FLORAL’s performance in the presence of alfa-tilt attack (Xiao et al., 2015)\non the Moon and MNIST-1vs7 datasets. We report the results on the Moon datasets in Table 11,\nwhereas we present the results for MNIST-1vs7 dataset in Figure 13 and Table 12.\nAs shown with the results on the Moon dataset, FLORAL is able to achieve a higher \"Best\" robust accu-\nracy level throughout the training process. Furthermore, FLORAL’s effectiveness under alfa-tilt\nattack is best shown on the MNIST dataset. As reported in Figure 13 and Table 12, FLORAL achieves\nan outperforming robust accuracy level compared to baseline methods on all adversarial settings.\nThis demonstrates the potential of FLORAL defense against other label poisoning attacks.\nTable 11: Test accuracies of methods trained over the Moon dataset with adversarial labels generated\nby the alfa-tilt (Xiao et al., 2015) attack. Each entry shows the average of five replications.\nHighlighted values indicate the best performance in the \"Best\" (peak accuracy during training) and\n\"Last\" (final accuracy after training) columns.\nSetting\nMethod\nFLORAL\nSVM\nNN\nNN-PGD\nLN-SVM\nCurie\nLS-SVM\nK-LID\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nClean\nC = 10, γ = 1\n0.968\n0.966\n0.968\n0.968\n0.960\n0.960\n0.966\n0.964\n0.940\n0.940\n0.941\n0.941\n0.881\n0.881\n0.966\n0.966\nDadv = 5%\nC = 10, γ = 1\n0.972\n0.957\n0.944\n0.939\n0.948\n0.948\n0.962\n0.943\n0.956\n0.956\n0.940\n0.939\n0.898\n0.896\n0.937\n0.936\nDadv = 10%\nC = 10, γ = 1\n0.971\n0.928\n0.910\n0.886\n0.915\n0.914\n0.940\n0.906\n0.930\n0.930\n0.920\n0.902\n0.898\n0.896\n0.926\n0.926\nDadv = 25%\nC = 10, γ = 1\n0.893\n0.824\n0.787\n0.722\n0.837\n0.750\n0.837\n0.720\n0.786\n0.723\n0.792\n0.759\n0.792\n0.791\n0.770\n0.708\nTable 12: Test accuracies of methods trained on the MNIST-1vs7 dataset under alfa-tilt poi-\nsoning attack (Xiao et al., 2015). Each entry shows the average of five replications with different\ntrain/test splits. Highlighted values indicate the best performance in the \"Best\" (peak accuracy\nduring training) and \"Last\" (final accuracy after training) columns.\nSetting\nMethod\nFLORAL\nSVM\nNN\nNN-PGD\nLN-SVM\nCurie\nLS-SVM\nK-LID\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nClean\nC = 5, γ = 0.005\n0.992\n0.991\n0.992\n0.992\n0.993\n0.993\n0.995\n0.994\n0.987\n0.987\n0.990\n0.990\n0.978\n0.977\n0.987\n0.987\nDadv = 5%\nC = 5, γ = 0.005\n0.991\n0.990\n0.980\n0.980\n0.991\n0.958\n0.988\n0.955\n0.979\n0.979\n0.987\n0.987\n0.980\n0.979\n0.978\n0.978\nDadv = 10%\nC = 5, γ = 0.005\n0.984\n0.982\n0.970\n0.970\n0.986\n0.917\n0.988\n0.909\n0.966\n0.966\n0.974\n0.974\n0.979\n0.978\n0.965\n0.965\nDadv = 25%\nC = 5, γ = 0.005\n0.811\n0.788\n0.713\n0.713\n0.795\n0.739\n0.824\n0.754\n0.703\n0.701\n0.734\n0.734\n0.526\n0.526\n0.707\n0.705\n0\n100\n200\n300\n400\n500\nRounds\n0.90\n0.92\n0.94\n0.96\n0.98\n1.00\nAccuracy\nTest Accuracy\n(a) Clean.\n0\n100\n200\n300\n400\n500\nRounds\n0.90\n0.92\n0.94\n0.96\n0.98\n1.00\nAccuracy\nTest Accuracy\n(b) Dadv = 5%.\n0\n100\n200\n300\n400\n500\nRounds\n0.88\n0.90\n0.92\n0.94\n0.96\n0.98\n1.00\nAccuracy\nTest Accuracy\n(c) Dadv = 10%.\n0\n100\n200\n300\n400\n500\nRounds\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nAccuracy\nTest Accuracy\n(d) Dadv = 25%.\nFLORAL\nVanilla SVM\nNN\nNN-PGD\nLN-SVM\nCurie\nLS-SVM\nK-LID\nFigure 13: Clean and robust test accuracy of methods trained on the MNIST-1vs7 dataset under\nalfa-tilt poisoning attack (Xiao et al., 2015). \"Clean\" refers to the dataset with clean labels,\nwhile the adversarial datasets contain {5, 10, 25} (%) poisoned labels. For all SVM-related models,\nthe setting C = 5, γ = 0.005 is used. FLORAL achieves outperforming robust accuracy level\ncompared to baseline methods on all adversarial settings, clearly demonstrating the potential of\nFLORAL as a defense against other types of label poisoning attacks.\n31\n\nPublished as a conference paper at ICLR 2025\nH.2\nEXPERIMENTS WITH THE ALF A ATTACK\nThe alfa attack is generated under the assumption that the attacker can maliciously alter the training\nlabels to maximize the empirical loss of the original classifier on the tainted dataset. From this,\nthe attacker’s objective is formulated as maximizing the difference between the empirical risk for\nclassifiers under tainted and untainted label sets.\nWe experimented on the Moon dataset and considered label poisoning levels (%) of {5, 10, 25}.\nThe illustrations of the Moon dataset with clean and alfa-attacked adversarial labels are given\nFigure 14. We report the results under alfa attack in Figure 15 and Table 13. As shown, FLORAL\nis especially effective against vanilla SVM in maintaining higher robust accuracy in adversarial\nsettings. NN-based methods again fail drastically as the dataset becomes more adversarial under\nalfa attack. Although LS-SVM shows premise in moderate adversarial scenarios, it fails to be\neffective considering clean and most adversarial performance. Furthermore, FLORAL demonstrates\nsuperior performance compared to all baselines in the most adversarial setting (with 25% poisoned\nlabels).\n(a) Clean.\n(b) Dadv = 5%.\n(c) Dadv = 10%.\n(d) Dadv = 25%.\nFigure 14: Illustrations of the Moon training sets from an example replication, using clean and\nalfa-attacked adversarial labels with poisoning levels: 5%, 10%, 25%.\nTable 13: Test accuracies of methods trained over the Moon dataset with adversarial labels generated\nby the alfa attack. Each entry shows the average of five replications with different train/test splits.\nHighlighted values indicate the best performance in the \"Best\" (peak accuracy during training) and\n\"Last\" (final accuracy after training) columns.\nSetting\nMethod\nFLORAL\nSVM\nNN\nNN-PGD\nLN-SVM\nCurie\nLS-SVM\nK-LID\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nClean\nC = 10, γ = 1\n0.968\n0.966\n0.968\n0.968\n0.960\n0.960\n0.966\n0.964\n0.940\n0.940\n0.941\n0.941\n0.881\n0.881\n0.966\n0.966\nDadv = 5%\nC = 10, γ = 1\n0.963\n0.950\n0.954\n0.946\n0.875\n0.875\n0.963\n0.958\n0.942\n0.942\n0.934\n0.933\n0.964\n0.964\n0.942\n0.942\nDadv = 10%\nC = 10, γ = 1\n0.954\n0.902\n0.914\n0.893\n0.836\n0.816\n0.918\n0.895\n0.914\n0.907\n0.915\n0.914\n0.955\n0.954\n0.914\n0.907\nDadv = 25%\nC = 10, γ = 1\n0.776\n0.763\n0.750\n0.750\n0.693\n0.658\n0.693\n0.645\n0.729\n0.729\n0.741\n0.741\n0.740\n0.740\n0.729\n0.729\n0\n100\n200\n300\n400\n500\nRounds\n0.800\n0.825\n0.850\n0.875\n0.900\n0.925\n0.950\n0.975\n1.000\nAccuracy\nTest Accuracy\n(a) Clean.\n0\n100\n200\n300\n400\n500\nRounds\n0.800\n0.825\n0.850\n0.875\n0.900\n0.925\n0.950\n0.975\n1.000\nAccuracy\nTest Accuracy\n(b) Dadv = 5%.\n0\n100\n200\n300\n400\n500\nRounds\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nAccuracy\nTest Accuracy\n(c) Dadv = 10%.\n0\n100\n200\n300\n400\n500\nRounds\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nAccuracy\nTest Accuracy\n(d) Dadv = 25%.\nFLORAL\nVanilla SVM\nNN\nNN-PGD\nLN-SVM\nCurie\nLS-SVM\nK-LID\nFigure 15: Clean and robust test accuracy of methods trained on the Moon dataset under alfa\npoisoning attack. \"Clean\" refers to the dataset with clean labels, while the adversarial datasets\ncontain {5, 10, 25}(%) poisoned labels. As the level of label poisoning increases, models trained on\nadversarial datasets generally demonstrate a decline in accuracy. However, FLORAL demonstrates a\ngradually improving robust accuracy performance, particularly when the attack intensity increases to\n25%.\n32\n\nPublished as a conference paper at ICLR 2025\n0\n100\n200\n300\n400\n500\nRounds\n0.800\n0.825\n0.850\n0.875\n0.900\n0.925\n0.950\n0.975\n1.000\nAccuracy\nTest Accuracy\n(a) Clean.\n0\n100\n200\n300\n400\n500\nRounds\n0.800\n0.825\n0.850\n0.875\n0.900\n0.925\n0.950\n0.975\n1.000\nAccuracy\nTest Accuracy\n(b) Dadv = 5%.\n0\n100\n200\n300\n400\n500\nRounds\n0.800\n0.825\n0.850\n0.875\n0.900\n0.925\n0.950\n0.975\n1.000\nAccuracy\nTest Accuracy\n(c) Dadv = 10%.\n0\n100\n200\n300\n400\n500\nRounds\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nTest Accuracy\n(d) Dadv = 25%.\nFLORAL\nVanilla SVM\nNN\nNN-PGD\nLN-SVM\nCurie\nLS-SVM\nK-LID\nFigure 16: Clean and robust test accuracy of methods trained on the Moon dataset under LFA\npoisoning attack (Paudice et al., 2018). \"Clean\" refers to the dataset with clean labels, while the\nadversarial datasets contain {5, 10, 25} (%) poisoned labels. For all SVM-related models, the setting\nC = 1, γ = 1.0 is used. As the level of label poisoning increases, models trained on adversarial\ndatasets generally demonstrate a decline in accuracy. However, FLORAL demonstrates a gradually\nimproving robust accuracy performance, particularly when the attack level is 10% or 25%.\nTable 14: Test accuracies of methods trained over the Moon dataset with adversarial labels gener-\nated by the LFA (Paudice et al., 2018) attack. Each entry shows the average of five replications.\nHighlighted values indicate the best performance in the \"Best\" (peak accuracy during training) and\n\"Last\" (final accuracy after training) columns.\nSetting\nMethod\nFLORAL\nSVM\nNN\nNN-PGD\nLN-SVM\nCurie\nLS-SVM\nK-LID\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nBest\nLast\nClean\nC = 10, γ = 1\n0.968\n0.966\n0.968\n0.968\n0.960\n0.960\n0.966\n0.964\n0.940\n0.940\n0.941\n0.941\n0.881\n0.881\n0.966\n0.966\nDadv = 5%\nC = 10, γ = 1\n0.957\n0.954\n0.967\n0.967\n0.906\n0.906\n0.955\n0.930\n0.948\n0.948\n0.940\n0.940\n0.880\n0.880\n0.943\n0.943\nDadv = 10%\nC = 10, γ = 1\n0.943\n0.937\n0.919\n0.918\n0.903\n0.903\n0.917\n0.872\n0.938\n0.938\n0.931\n0.931\n0.933\n0.932\n0.900\n0.900\nDadv = 25%\nC = 10, γ = 1\n0.922\n0.903\n0.822\n0.822\n0.695\n0.695\n0.757\n0.753\n0.853\n0.853\n0.892\n0.846\n0.907\n0.906\n0.900\n0.900\nH.3\nEXPERIMENTS WITH THE LFA ATTACK\nWe additionally evaluate FLORAL’s effectiveness compared to baselines in the presence of LFA attack\n(Paudice et al., 2018) on the Moon dataset. As results are shown in Figure 16 and Table 14, FLORAL\ndemonstrates significant performance when the label poisoning attack level is high, i.e., 10% or 25%.\nHowever, under those settings, LS-SVM (Paudice et al., 2018) baseline shows faster convergence,\nwhich is expected as the LS-SVM (Paudice et al., 2018) method is specifically crafted against the LFA\nattack. Considering that LS-SVM fails in clean test performance, it is clear that FLORAL provides a\ngeneralizable defense.\n33\n\nPublished as a conference paper at ICLR 2025\nI\nINTEGRATION WITH NEURAL NETWORKS\nAs demonstrated with the IMDB experiments in Section 4.1, FLORAL can be integrated with complex\nmodel architectures, e.g. a transformer-based language model such as RoBERTa, serving as a robust\nclassifier head that enhances model robustness on classification tasks.\nSimilarly, FLORAL can be directly incorporated into neural networks by utilizing the last-layer\nembeddings (the xi’s in Algorithm 1) as inputs. These extracted representations can then be trained\nusing FLORAL, resulting in more robust feature representations. Notably, our theoretical analysis\nremains valid under this integration, ensuring the approach’s soundness.\nTo demonstrate this further, we performed additional experiments on the Moon and MNIST-1vs7\n(Deng, 2012) datasets, by integrating FLORAL with a neural network.\nFrom Figure 17, we can conclude that FLORAL integration achieves a higher robust accuracy level\ncompared to plain neural network training.\n0\n100\n200\n300\n400\n500\nRounds\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nTest Accuracy\n(a) Clean.\n0\n100\n200\n300\n400\n500\nRounds\n0.5\n0.6\n0.7\n0.8\n0.9\nAccuracy\nTest Accuracy\n(b) Dadv = 5%.\n0\n100\n200\n300\n400\n500\nRounds\n0.5\n0.6\n0.7\n0.8\n0.9\nAccuracy\nTest Accuracy\n(c) Dadv = 10%.\n0\n100\n200\n300\n400\n500\nRounds\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nAccuracy\nTest Accuracy\n(d) Dadv = 25%.\n0\n100\n200\n300\n400\n500\nRounds\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nTest Accuracy\n(e) Clean.\n0\n100\n200\n300\n400\n500\nRounds\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nTest Accuracy\n(f) Dadv = 5%.\n0\n100\n200\n300\n400\n500\nRounds\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nTest Accuracy\n(g) Dadv = 10%.\n0\n100\n200\n300\n400\n500\nRounds\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nAccuracy\nTest Accuracy\n(h) Dadv = 25%.\nFLORALM\nNN\nFigure 17: Clean and robust test accuracy performance of neural network vs FLORAL-integrated\nneural network trained on the Moon (the first row) and MNIST-1vs7 (the second row) datasets.\nThe results demonstrate that FLORAL integration helps to achieve a higher robust accuracy level.\n34\n\nPublished as a conference paper at ICLR 2025\n(a) FLORAL (Clean).\n(b) Dadv = 5%.\n(c) Dadv = 10%.\nDadv = 25%.\n(d) SVM (Clean).\n(e) Dadv = 5%.\n(f) Dadv = 10%.\n(g) Dadv = 25%.\n(h) NN (Clean).\n(i) Dadv = 5%.\n(j) Dadv = 10%.\n(k) Dadv = 25%.\n(l) NN-PGD (Clean).\n(m) Dadv = 5%.\n(n) Dadv = 10%.\n(o) Dadv = 25%.\n(p) LN-SVM (Clean).\nDadv = 5%.\nDadv = 10%.\nDadv = 25%.\n(q) Curie (Clean).\nDadv = 5%.\nDadv = 10%.\nDadv = 25%.\n(r) LS-SVM (Clean).\nDadv = 5%.\nDadv = 10%.\nDadv = 25%.\n(s) K-LID (Clean).\nDadv = 5%.\nDadv = 10%.\nDadv = 25%.\nFigure 18: The decision boundaries on the Moon test dataset with various label poisoning levels. SVM-\nrelated models use an RBF kernel with C = 10 and γ = 0.5. FLORAL generates a relatively smooth\ndecision boundary compared to baseline methods, particularly in 25% adversarial setting, where\nbaselines show drastic changes in their decision boundaries as a result of adversarial manipulations.\n35\n",
  "metadata": {
    "source_path": "papers/arxiv/Adversarial_Training_for_Defense_Against_Label_Poisoning_Attacks_5149cbf5da4671bb.pdf",
    "content_hash": "5149cbf5da4671bb7cabb24564422e4d41a401b51b47c26ab31aec084536967c",
    "arxiv_id": null,
    "title": "Adversarial_Training_for_Defense_Against_Label_Poisoning_Attacks_5149cbf5da4671bb",
    "author": "",
    "creation_date": "D:20250225024709Z",
    "published": "2025-02-25T02:47:09",
    "pages": 35,
    "size": 8236809,
    "file_mtime": 1740470182.3469331
  }
}