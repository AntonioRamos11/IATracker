{
  "text": "1\nGenerative Models in Decision Making: A Survey\nYinchuan Li, Xinyu Shao, Jianping Zhang, Haozhi Wang, Leo Maxime Brunswic,Kaiwen Zhou,\nJiqian Dong, Kaiyang Guo, Xiu Li, Zhitang Chen, Jun Wang, Jianye Hao\nAbstract—In recent years, the exceptional performance of generative models in generative tasks has sparked significant interest in\ntheir integration into decision-making processes. Due to their ability to handle complex data distributions and their strong model\ncapacity, generative models can be effectively incorporated into decision-making systems by generating trajectories that guide agents\ntoward high-reward state-action regions or intermediate sub-goals. This paper presents a comprehensive review of the application of\ngenerative models in decision-making tasks. We classify seven fundamental types of generative models: energy-based models,\ngenerative adversarial networks, variational autoencoders, normalizing flows, diffusion models, generative flow networks, and\nautoregressive models. Regarding their applications, we categorize their functions into three main roles: controllers, modelers and\noptimizers, and discuss how each role contributes to decision-making. Furthermore, we examine the deployment of these models\nacross five critical real-world decision-making scenarios. Finally, we summarize the strengths and limitations of current approaches and\npropose three key directions for advancing next-generation generative directive models: high-performance algorithms, large-scale\ngeneralized decision-making models, and self-evolving and adaptive models.\nIndex Terms—Generative Models, Decision Making, Generative Decision Making\n✦\n1\nINTRODUCTION\nG\nENERATIVE models have become a hot topic in both\nacademia and industry, primarily due to their ability\nto generate large quantities of synthetic data with high\nquality and diversity. From early systems like DALL-E [1]\n(for image generation) and GPT-3 [2] (for text generation) to\nmore recent advancements such as DALL-E3 [3], ChatGPT\nand GPT-4 [4], generative models have rapidly advanced in\nboth quality and scale of their outputs.\nContent generation aims to create coherent material\nthat mimics training examples, while decision-making fo-\ncuses on producing action sequences for optimal outcomes.\nUnlike content generation, decision-making involves com-\nplex, dynamic environments and long-term decisions. Thus,\ndespite generative models’ success in content generation,\napplying them to decision-making poses challenges. These\nchallenges include: 1) how to learn policies through interac-\ntion with the environment, rather than simply mimicking\nexpert behavior. 2) how to generate new policies based\non learned behaviors, transitioning from policy learning to\npolicy generation. 3) how to establish a robust fundamental\ndecision generating model that can adapt to various en-\nvironments with minimal tuning efforts. 4) how to build\nmulti-step reasoning and long-term evolution capabilities\nof strategies. These challenges emphasize the need for gen-\nerative models to go beyond mere data generation.\nIn practice, decision-making is often referred to as se-\nquential decision making, where a decision maker makes\nYinchuan\nLi,\nHaozhi\nWang,\nLeo\nMaxime\nBrunswic,\nKaiwen\nZhou,\nJiqian Dong, Kaiyang Guo, Zhitang Chen and Jianye Hao are with the\nHuawei Noah’s Ark Lab, China (e-mail: liyinchuan@huawei.com; hao-\njianye@huawei.com). (Corresponding author: Jianye Hao)\nXinyu Shao is with Tsinghua Shenzhen International Graduate School, Ts-\ninghua University, Shenzhen 518055, China, and is also with the Huawei\nNoah’s Ark Lab, Shenzhen 518129, China.\nJianping Zhang is with The Chinese University of Hong Kong, Hong Kong.\nXiu Li is with Tsinghua Shenzhen International Graduate School, Tsinghua\nUniversity, Shenzhen 518055, China.\nJun Wang is with Department of Computer Science, University College\nLondon, WC1E 6BT London, UK.\na series of observations over time, and each decision in-\nfluencs subsequent choices. The goal is to identify a policy\nthat optimizes expected rewards or minimizes costs across\nsequential actions. Classical algorithms such as Dynamic\nProgramming (DP) and Reinforcement Learning (RL) are\nwidely used to solve problems modeled as Markov Deci-\nsion Processes (MDPs). These methods optimize decision-\nmaking by updating policies based on observed rewards\nand state transitions rather than generating new ones. De-\nspite their many successful applications, these traditional\napproaches often rely on trial-and-error or predefined states\nand transitions, limiting exploration and potentially missing\nbetter solutions. Moreover, they require substantial compu-\ntation and optimization, which can be impractical for high-\ndimensional or large-scale problems. Traditional methods\nalso need significant reconfiguration or retraining to new\nenvironment, reducing flexibility.\nOn the other hand, generative models are designed to\nmodel data distributions, rather than simply fitting labels.\nOnce trained, they can generate new samples that resem-\nble the original data, enabling the exploration of diverse\nscenarios and outcomes. This ability enables the discovery\nof novel strategies that may not be immediately evident\nwith traditional methods. In complex or poorly labeled\ndata scenarios, generative models offer a richer under-\nstanding of possible decision paths, sometimes leading to\nstrategies that better align with high rewards or desired\ngoals. However, traditional methods like optimization or\nreinforcement learning remain effective in simpler, well-\ndefined environments where the decision space is clearer,\nand goals are more straightforward. The choice between\nthese approaches depends on the task complexity and the\nenvironment characteristics.\nRecognizing these advantages, recent years have seen\nsubstantial research efforts aimed at developing new gener-\native models and applying them to decision making. Fig. 1\nillustrates the research trends in generative models and their\narXiv:2502.17100v1  [cs.LG]  24 Feb 2025\n\n2\nFig. 1: Research trends in generative models and their appli-\ncations in decision-making (2000-2024). The bars represent\nthe rough average annual number of papers, sourced from\nGoogle Scholar. The search included titles associated with\nseven classic types of generative models and their applica-\ntions in five real-world scenarios we detailed in Fig 20.\napplications in decision making, further emphasizing the\nsignificance of these methods in addressing such challenges.\nHowever, there is a lack of comprehensive reviews that\nsummarize past work and pave the way for new research\navenues. This gap motivates us to provide this review paper.\nThe survey highlights three major contributions: 1) A\ncomprehensive taxonomy is proposed for classifying current\ngenerative decision-making methods. We identify seven\ntypes of generative models used in decision-making and\ncategorize their functions into three key roles: controller,\nmodeler, and optimizer. 2) We review the diverse practical\nuses of generative models in decision-making, focusing on\nrobot control, structural generation, games, autonomous\ndriving and optimization tasks. 3) Finally, we summarize\nthe advantages and limitations of existing work and discuss\nfuture perspectives for developing high-performance gener-\native models in decision-making tasks.\nThe organization of the rest of this survey is as follows\n(refer to Fig. 2 for a general outline): Section 2 serves as\na preliminary by introducing sequential decision-making\nformulation and provides the basics of all the examined\nmethods. Specifically, we offer a detailed introduction to\nseven types of generative models and compare their perfor-\nmance with traditional approaches. Section 3 presents the\nproposed taxonomy for categorizing generative decision-\nmaking methods. In Section 4, we review and analyze exist-\ning literature according to the introduced taxonomy. Section\n5 showcases practical applications of generative models\nin decision-making. Finally, Section 6 discusses the future\ndirections of generative models in decision-making, and we\nconclude the paper in Section 7 with an overall summary.\n2\nPRELIMINARIES\n2.1\nSequential Decision Making\nSequential decision-making involves making step-by-step\ndecisions, where each choice depends on previous out-\ncomes. At each step, the agent typically observes the current\nstate, selects an optimal action based on its policy, and the\nenvironment updates its state while providing new rewards.\nThe ultimate goal is to maximize the accumulated rewards.\nThe Sequential decision process is categorized accord-\ning to the number of agents and the observation to the\nenvironment. In this paper, we only focus on the single\nagent. If the agent is able to fully observe the environment,\nit is considered a Markov Decision Process (MDP) [5]. An\nMDP is a framework for modeling discrete-time decision-\nmaking process, where outcomes depend on both random\nfactors and the agent’s decisions [5]. Formally, an MDP is\ndefined as a tuple M = (S, A, R,P,ρ0,γ,H) to represent\nMDP, where S is the state space, and s ∈S contains all the\ninformation perceived by the agent from the environment.\nA represents the action space, encompassing all possible\nactions a that the agent can execute when interacting with\nthe environment. r ∈R ∶S × A × S →R denotes the reward\nfunction, which corresponds to the transition pair (s,a,s′).\nP ∶S × A →∆(S) refers to the transition function. When\nthe action a is applied to the state s, P(s′∣s,a) generates\na distribution over states. ρ0 ∈∆(S) represents the initial\nstate distribution. The discount factor γ ∈[0,1] quantifies\nthe long-term value of the current action, while H denotes\nthe horizon.\nIf observations are limited, we can define the Partially\nObserved Markov Decision Process (POMDP) [6] as a tuple\nM = (S, A, O, R,P,ρ0,E,γ,H). Here, O denotes the obser-\nvation space, and each observation ot ∈O is obtained by the\nemission function E(ot∣st).\nThe goal of sequential decision making is to learn a\npolicy π(a∣s) or π(a∣o) by optimizing the expected reward:\nJ(π) = Eτ∼pπ(τ) [\nH\n∑\nt=0\nγtr (st,at)],\n(1)\nwhere pπ(τ) is the distribution over trajectories τ induced\nby policy π:\npπ(τ) = ρ0 (s0)\nH\n∏\nt=0\nπ (at∣st)P (st+1∣st,at).\n(2)\n2.2\nRelated Methods\nWe introduce several traditional methods for solving se-\nquential decision making problems in this section.\n2.2.1\nImitation Learning\nImitation learning aims to optimize a policy πθ(s) such that\nthe action distribution is close to that of an expert policy π∗\nfor any state s, the objective function is given by\nθ = arg min\nθ\nEs∼p(s∣πθ)L (π∗(s) −πθ(s)),\n(3)\nwhere p(s∣πθ) denotes the state distribution induced by πθ,\nand L is a metric function.\n2.2.2\nSearch based Methods\nPolicy search focuses on exploring the policy space directly,\nrather than computing the value function explicitly. For a\ngiven policy π, the expected discounted return is given by\nU(π) = ∑\ns\nρ0(s)Uπ(s).\n(4)\nWith large state space, U(π) could be approximated by\nsampling trajectories consists of (S, A, R) pairs and U(π)\ncan be reformulated as [7]:\nU(π) = Eτ[R(τ)] = ∫pπ(τ)R(τ)dτ,\n(5)\n\n3\nSurvey Pipeline\nIntroduction\nPreliminaries\nGenerative \nmodels\nRelated methods\nDifference \nFormulation\nMethodology\nModeler\nController\nTaxonomy\nOptimizer\nApplication\nPerspective\nConclusion\nimitation \nlearning\nsearch based \nmethod\nplanning &\noptimization\nreinforcement \nlearning\nvalue-based \nmethod\npolicy \ngradient\nactor-critic\nmethod\nmodel-based\nRL\nenergy based\nmodel\ngenerative \nadversarial\nnetwork \nvariantional\nautoencoder\nnormalizing fLow\ngflownet/\ncflownet\ndiffusion model\nautoregressive \nmodel\nfamily\nfunction\nstructure\nexpertise\napplication\nrobot control\nstructure \ngeneration\ngames\nautomoous driving\noptimization\nhigh performance\nalgorithm\nlarge-scale \ngeneralized \ndecision making \nmodel \nself-evolve\nand adaptive \ngenerative  model\nFig. 2: An overview of the survey. Specific sections are distinguished by different colors. Best viewed in color.\nwhere R(τ) denotes the discounted return related to τ.\nMonte Carlo policy evaluation entails estimating the ex-\npected utility of the policy π by performing numerous\nrollouts starting from s0 ∼ρ0(s). Local search starts with\na feasible solution and incrementally moves to the neighbor\nwith better utility over the search space until convergence\noccurs, e.g. Hooke-Jeeves method [8]. Simulated anneal-\ning [9] allows for occasional moves to feasible solutions\nwith worse utility to approximate the global optimum.\nGenetic algorithms evaluate different simulations concur-\nrently based on the objective, recombine them and guide\nthe population toward a global optimum.\n2.2.3\nPlanning & Optimization\nPlanning is an optimization approach that relies on a pre-\ndefined model to guide the search process. Let U(s) denote\nthe action space for each state s, which represents the set of\nall actions that could be applied from s. For distinct s,s′ ∈S,\nU(s) and U(s′) are not necessarily disjoint. As part of the\nplanning problem, a set SG ⊂S of goal states and the initial\nstate sI are defined.\nLet πK denote a K-step plan, which is a sequence of\n(a1,a2, ⋯,aK) of K actions. Let F denote the final stage\nwhere F = K + 1. The cost functional is defined as\nL(πK) =\nN\n∑\nk=1\nl(sk,ak) + lF (sF ),\n(6)\nwhere l(sk,ak) is the cost term yielding a real value for\nevery sk ∈S and ak ∈U(sk). The final term is defined\nlF (sF ) = 0 if sf ∈SG and lF (sF ) = ∞otherwise.\n2.2.4\nReinforcement Learning\nThe field of RL contains various methods that cater to dif-\nferent decision-making tasks, but all standard RL algorithms\ngenerally follow the same learning principles. In particular,\nthe agent interacts the current MDP M using some sort of\nbehavior policy, which can be the present policy π(a∣s) or\nmix it with a random policy. Then the agent can receive\nthe subsequent state st+1 along with the reward function rt.\nAfter repeating this process for many steps, the agent can\nuse the collected samples {s,a,r,s′} to update the policy.\nValue-based Methods:\nOne classic approach is to directly estimate the value\nfunction of the state of state-action so that a near-optimal\npolicy can be obtained. Define the state-action value as\nQπ (st,at) = Eτ∼pπ(τ∣st,at) [\nH\n∑\nt′=t\nγt′−tr (st,at)]\n(7)\nand the state value V π(st) as\nV π (st) = Eat∼π(at∣st) [Qπ (st,at)].\n(8)\nThen the recursive representation of Qπ(st,at) can be de-\nrived as [10]:\nQπ (st,at) = r (st,at) + γEst+1∼P (st,at) [V π (st+1)],\n(9)\nwhich can also be expressed as the Bellman operator Bπ,\ni.e., Qπ = BπQπ. Bπ has the unique fixed point obtained\nby repeating iterations Qπ\nk+1 = BπQπ\nk [11]. Based on this\nproperty, we can derive the modern value iteration (VI)\nalgorithms. Q-learning is a common VI method that ex-\npresses the policy as π(at∣st) = δ(at = arg maxat Q(st,at))\nwith Dirac function δ(⋅). The optimal Q-function can be\n\n4\napproximated by substituting this policy into (9). To derive\na learning algorithm, we can define a parametric Q-function\nestimator Qϕ(s,a), which can be optimized by minimizing\nthe difference between LHS and RHS in Bellman equation,\nsuch as fitted Q-learning [12]. With the help of neural\nnetworks, existing deep Q-learning methods can achieve\nmore accurate value-function estimation by minimizing the\nBellman error objective [13].\nPolicy Gradients: Another classic approach is to directly\nestimate the gradient of (1). We define policy πθ(a∣s) param-\neterized by θ, which can be a neural network and output the\nlogits of action a. Then, We can then write the gradient of\n(1) with respect to θ as\n∇θJ (πθ) = Eτ∼pπθ (τ) [\nH\n∑\nt=0\nγt∇θ log πθ (at∣st) ˆA(st,at)],\nwhere ˆA(st,at) is the return estimator, written by\nˆA(st,at) =\nH\n∑\nt′=t\nγt′−tr (st′,at′) −b (st),\n(10)\nwhich can be calculated with Monte Carlo samples [14].\nb(st) denotes the baseline, which can be approximated as\nthe mean return of the collected trajectories or by the value\nfunction V (st) [15].\nBy introducing KL regularization and clipped surrogate\nobjective, True Region Policy Optimization (TRPO) [16] and\nProximal Policy Optimization (PPO) [17] constrain policy\nsize updated at each iteration to stable the training pro-\ncess. These methods and their variants have achieved great\nsuccess in areas like robot control [18], games [19] and\ngenerative model training [20].\nActor-Critic Methods:\nTaking advantage of both policy gradient and value-\nbased approaches, Actor-critic algorithms use a parameter-\nized policy and value function. And the value function can\nprovide a better estimation of ˆA(st,at) for policy gradients,\nwith calculating the average return over the sampled trajec-\ntories.\nUnlike Q-learning, the actor-critic method seeks to op-\ntimize the Q-function associated with the current policy πθ\nrather than learning the optimal Q-function directly. Many\nvariants are proposed based on this principle, such as on-\npolicy and off-policy algorithms [21]. Deep Deterministic\nPolicy Gradient (DDPG) introduces a deterministic policy\nunder the Actor-Critic architecture, which can solve contin-\nuous control problems [21]. Soft actor-critic introduce the\nentropy regularization into the policy optimization process,\nso that the agent can make better exploration [22].\nModel-based RL: For model-based RL, the first step is\nto learn the environment model P(⋅∣s,a) based on historical\ntrajectories. According to the probabilistic transition model,\na common approach is to reduce the KL divergence between\nthe learned model ˆP(⋅∣s,a) and the true dynamics P(⋅∣s,a)\nas [23]:\nmin\nθ\nE(s,a)∼ρπ [DKL (P(⋅∣s,a), ˆPθ(⋅∣s,a))],\n(11)\nwhere θ denotes the learned model’s parameters. The model\nlearning process can be transformed as a supervised learn-\ning task, which can be solved effectively by some supervised\nlearning technique. To reduce the error of model learning,\nseveral scholars propose the Lipschitz continuity constraints\n[24], distribution matching [25], and robust model learning\nmethods [26] for further precise estimation.\nWhen the model is ready, the agent develops plans to\nimprove its strategy to interact with the modeled world.\nTraditional methods for integrating planning into MBRL\nmainly include model predictive control, Monte Carlo tree\nsearch and Dyna-style methods [27]. Dyna-style methods\nutilize the learned model to generate more experience and\nthen perform RL on the model-augmented dataset, which\nhas been the mainstream method [28]. Similarly, these meth-\nods employ value function estimation and policy gradient to\nimprove the policy.\n2.3\nGenerative Model\nGenerative Model [29], [30], [31], [32], [33], [34], [35], [36]\nis an important subdivision of artificial intelligence that\nemploys techniques and models specifically designed for\nsampling unseen data from the underlying distribution of\nthe existing dataset.\nIn this section, we explore generative models through\nthree crucial dimensions: sample quality, sample diversity,\nand computational efficiency [37], [38]. These dimensions\nare essential for understanding how generative models per-\nform in decision-making, as they directly impact the accu-\nracy, robustness, and practical applicability of the generated\noutputs. Sample quality measures how well the generated\nsamples align with the real data distribution, reflecting\nthe realism and reliability of the outputs. High sample\nquality ensures that decisions based on the generated data\nare both accurate and trustworthy in real-world contexts.\nSample diversity, on the other hand, evaluates how well\nthe model can generate a broad range of distinct samples,\ncapturing the full spectrum of potential outcomes. This is\nvital in decision-making tasks, where diverse options are\nnecessary to adapt to varying conditions and avoid narrow,\noverfitted solutions. Finally, computational efficiency refers\nto the computational resources required for training and\ninference, making it a key consideration for the deployment\nof generative models in real-world applications. In con-\ntexts such as autonomous driving or robotic control, where\ndecisions must be made in real time and under resource\nconstraints, computational efficiency is critical to ensure\nthat the model remains practical and scalable. Together,\nthese dimensions provide a comprehensive framework for\nevaluating generative models, guiding their application in\ncomplex, resource-sensitive decision-making environments.\nWe will examine each of the seven generative models\nthrough these three crucial dimensions, identifying their\nstrengths and weaknesses. Generative models face chal-\nlenges in balancing sample quality, sample diversity, and\ncomputational efficiency. For example, Diffusion Models\nand Normalizing Flows offer strong sample diversity and\nstability but require high computational resources, limiting\ntheir suitability for real-time decision-making applications\n[39], [40], [41]. In contrast, models like VAEs and GANs\nprovide faster training and better efficiency but may strug-\ngle with maintaining sample diversity, potentially leading\nto overly similar or overfitted outputs [34], [42], [43], [44].\nBased on the references [?], [43], [45], [46], [47], [48], [49],\n\n5\nDM\nGFN\nAM\nVAE\nGAN\nEBM\nSample  Quality\nSample Diversity\nNF\n    \nbubble size\ncomputational efficiency\nFig. 3: Comparison of seven generative models in decision-\nmaking: training stability, generation diversity, and com-\nputational efficiency. Larger bubbles represent higher com-\nputational efficiency, with different models indicated by\ndistinct colors. Best viewed in color.\n[50], [51], [52], we compare the performance of these seven\ngenerative models across sample quality, diversity, and effi-\nciency, as shown in Fig. 3.\nEnergy Based Models (EBMs). An EBM [53] assigns an\nenergy function Eθ(x) to each data x [54], which is the\nunnormalized log probability of the data to measure the\ncompatibility of x with the model. The density of x is\npθ(x) = 1\nZθ\nexp(−Eθ(x)),\n(12)\nwhere Zθ denotes the normalizing constant, which also\nknown as partition function:\nZθ = ∫exp(−Eθ(x))dx.\n(13)\nThe model is trained to reduce the energy of data point\nfrom the training set while increasing the energy of other,\npossibly unrealistic, data points [27]. The earliest EBM is the\nBoltzmann machine [33], which is a stochastic version of the\nHopfield network [55]. Since the energy function has no re-\nstrictions, it can be parameterized freely and can model any\nhigh-dimensional complex data [56], [57] like images [58]\nand natural languages [59]. However, the intractable nor-\nmalizing constant poses training and sampling challenges.\nTypical solutions include 1) MCMC sampling [60]; 2) score\nmatching [61] and 3) noise contrastive estimation [62].\nGenerative Adversarial Networks (GANs). GANs [34]\nare well-known due to their distinctive architecturewhich\ninvolves a generator G that creates synthetic samples and\na discriminator D that evaluates them by distinguishing\nbetween real and generated data. The target loss of a typical\nGAN is given by\nL(θ,ϕ) = min\nθ max\nϕ\nEx∼pdata[log D(x;ϕ)]\n+ Eˆx∼G(θ)[log(1 −D(ˆx;ϕ))].\n(14)\nWhile GAN can generate realistic contents in image syn-\nthesis [34], image style transfer [35], and behavior [63], it\nsuffers from training instability and mode collapse. Several\nvariants of GANs have be proposed to solve the problem,\nlike Wasserstein GANs [64], [65], CycleGANs [35], and\nProgressive GANs [66], which use different loss functions,\narchitectures, or training techniques.\nVariational Autoencoders (VAEs). VAEs\n[43], known\nfor their stable learning process and efficient computation,\nattract significant attention in recent years. VAEs [67] are\nprobabilistic models that represent the input data in a com-\npressed form, referred to as the latent space. New samples\nare generated by sampling from this latent space and then\ndecoding them back into the original data domain. The goal\nof a VAE, presented in (15), aims to maximize the evidence\nlower bound (ELBO):\nL(ϕ,θ) = Ez∼p(z∣x;ϕ)[log p(x ∣z;θ)] −KL(q(z)∥p(z)). (15)\nIn (15), the first term represents the likelihood of the ob-\nserved data x given latent variable z, and the expectation is\ntaken over the distribution of z. The KL term measures the\ndivergence between the variational distribution q(z), which\napproximates the posterior distribution of z, and the prior\ndistribution p(z).\nVAEs are deployed to generate new images or videos\n[68], and augment images, such as image inpainting [69]\nand image colorization [70]. VAEs are used to compress\ndata [71], [72] or provide a good way to detect anomalies\nor outliers in data [73]. However, VAEs may suffer from\nlow generation quality and the independence assumption\nbetween the latent variables [71]. To address these limita-\ntions, several extensions of VAEs have been proposed, e.g.,\nconditional VAE [74] and Gaussian mixture VAE [75].\nResearchers propose to combine VAEs with other tech-\nniques, including adversarial training [76], [77] and normal-\nizing flows [78] for improving disentanglement. Besides,\n[79] and\n[80] study the continual learning of VAE rep-\nresentations and\n[81] trains a response function of the\nhyperparameter β in β−V AE to trade off the reconstruction\nerror and the KL divergence.\nNormalizing Flow. Flow-based models [82] learn to map\na simple random variable, such as a Gaussian distribution,\nto a more complex distribution capable of representing\ndata. Normalizing Flow\n[83], [84] applys a specific type\nof invertible transformation to the simple distribution to\npreserve the tractability of the density function, as shown\nbelow:\nx = fK (fK−1 (...f0 (z0)))...)) with z0 ∼N(0,I),\nfθ(z) = z + uh (wT z + b) .\n(16)\nA primary strength of normalizing flows is their precision in\ncapturing complex probability. Normalizing flows can also\nbe used to generate new samples and to perform density es-\ntimation, among other tasks. Despite their success, normal-\nizing flows still face some challenges, such as the restricted\nnetwork structure and the rendered limited expressiveness.\nNevertheless, Normalizing flows remain a dynamic field of\nstudy, with continuous advancements being made.\nFrom limited pre-neural attempts [85], [86], [87], spe-\ncific flow transformations [88] are proposed to reduce the\ncomputational cost, such as NICE [89], Real-NVP [90], and\nGlow [41]. Researchers also use autoregressive flows [91],\n[92], [93] to accelerate the sampling process. Many Normal-\nizing Flow variants also leverage the techniques of Neural\nODE [94], [95], [96] and optimal transport [97], [98].\n\n6\nDiffusion Model. Diffusion models, introduced by Sohl-\nDickstein et al. in 2015 [36], learn to generate samples\nthrough a diffusion process. This process involves gradually\nintroducing noise to the data, followed by the reversal of\nthis noise addition during sampling to recover the original\ndata (as illustrated in Fig. 4). The key strength of diffusion\nmodels lies in their ability to generate high-quality, realistic,\nand diverse outputs, surpassing earlier generative models\nsuch as GANs in various image synthesis tasks [99]. This\ncapability has sparked significant interest in their appli-\ncation to decision-making scenarios, particularly in high-\ndimensional environments.\nFig. 4: A diagrammatic depiction of diffusion models and\none denoising step is illustrated [100].\nThe key components of a diffusion model are the for-\nward and the reverse process. In the forward phase, Gaus-\nsian noise is progressively introduced to the data. At time\nstep t, the data point xt−1 is transformed into a noisy version\nxt according to Equation 17:\nq (xt ∣xt−1) = N (xt;\n√\n1 −βtxt−1,βtI).\n(17)\nHere, βt denotes a schedule of noise variance, which\ngoverns how quickly the noise is introduced at step t.\nThe forward process effectively destroys the original data\nstructure, making it increasingly difficult to recover the\noriginal input as the process progresses.\nThe sampling process works by reversing the diffusion\n(or noise addition) steps. The model iteratively refines a\nnoisy sample to recover the original data by learning a\nreverse diffusion process:\npθ (xt−1 ∣xt)\nfor\nt = T,T −1,...,1.\n(18)\nThe reverse process can be computationally expensive,\nas it involves many time steps, each requiring careful sam-\npling to accurately reconstruct the data. Despite this chal-\nlenge, diffusion models can generate high-quality samples,\nparticularly in fields such as image generation, where they\nhave outperformed GANs in terms of visual fidelity and\ndiversity.\nIn the context of decision-making, diffusion models of-\nfer several advantages. They can model complex, high-\ndimensional decision spaces and generate solutions that\nexplore a wide range of potential outcomes, making them\nsuitable for applications in reinforcement learning (RL),\nrobotics, and automated planning.\nFor instance, in robotic control, diffusion models can\ngenerate motion sequences that not only mimic the de-\nsired behavior but also explore alternative strategies that\ncould lead to better performance in uncertain or dynamic\nenvironments. Similarly, in autonomous driving, they can\ngenerate realistic trajectories and responses to dynamic driv-\ning scenarios, taking into account the uncertainty of the\nenvironment.\nMoreover, diffusion models can facilitate the exploration\nof decision spaces in complex environments where tradi-\ntional models might struggle to account for the full range of\npossibilities. This capability is particularly useful in multi-\nobjective decision making, where a model needs to balance\ntrade-offs between conflicting goals.\nDiffusion models demonstrate remarkable potential for\nproducing high-quality, realistic, and novel data, and they\ncontinue to be an important tool in the field of genera-\ntive modeling. Nevertheless, diffusion models have a slow\nsampling process, which will limit its scalability to high-\ndimensional cases. Consistency models [101] are a potential\nsolution that support fast one-step generation.\nGFlowNets/CFlowNets GFlowNets is a new type of\ngenerative model, which [51], [102] are initially defined\non finite directed acyclic graphs G = (S,E) with given\ninitial and final states s0,sf ∈S and given reward function\nR(s →sf); an edge flow is simply a map F ∶E →R+. Fig.5\nshows the illustration of the structure of GFlowNets. From\nsuch an edge flow, one can define a Markov chain (pt) on G\nsetting P(pt+1 = s′∣pt = s) =\nF (s→s′)\n∑s→s′′ F (s→s′′). The edge flow F\nis trained so that for all state s ∈S ∖{s0,sf}\n∑\ns′→s\nF(s′ →s) = ∑\ns→s′ F(s →s′),\nF(s →sf) = R(s →sf).\nThe first equality is the flow-matching constraint, the second\nis the reward constraint. Assuming the reward R is non-\nnegative and not identically zero, those two constraints en-\nsures that the Markov chains reaches sf at some finite time\nτ and that the last non-terminal position pτ−1 of the Markov\nchain follows the distribution obtained by normalizing R\ni.e. P(pτ−1 = s) =\nR(s→sf )\n∑s′∈S R(s′→sf ). The framework has been\nextended beyond graphs and acyclicity [103], [104], [105],\nwith variation on losses and regularizations [104], [106],\n[107].\ns0\ns1\ns2\ns3\ns4\ns5\ns6\ns7\ns8\ns9\ns10\nF(s0 →s2)\nsf\nsf\nsf\nsf\nsf\nsf\ns0\ns1\ns2\ns3\ns4\ns5\ns6\ns7\ns8\ns9\ns10\nsf\nsf\nsf\nsf\nsf\nsf\nR(s9)\nR(s4)\nTerminating state\ns ∈Sf\nInitial state\nTerminal\nstate\nFig. 5: Illustration of the structure of a Generative Flow\nNetwork, as a pointed DAG over states s, with particles\nflowing along edges to represent the flow function [51].\nAutoregressive\nModels. Autoregressive models are\ncommonly used in natural language processing and genera-\ntive modeling tasks. They are a key component of sequence-\nto-sequence models, such as the Transformer model [108],\nand are responsible for generating output sequences based\non an input sequence or a given context.\nAutoregressive models operate by predicting each out-\nput element sequentially conditioned on the preceding ones.\nThis means that the generation process is sequential, with\nthe model generating the elements in a left-to-right fashion.\nAt each step, the model takes into account the previously\ngenerated elements and the context information to make\npredictions.\nA popular method for autoregressive generation is using\nan autoregressive language model, such as GPT [109], which\n\n7\nhas proven successful in NLP tasks like text generation,\nmachine translation, and dialogue systems.\nIn autoregressive models, the input to each step is typ-\nically a combination of the previously generated elements\nand a context vector that captures the overall context or\nrepresentation of the input sequence. The context vector\ncan be obtained from an encoder network that processes\nthe input sequence and produces a fixed-size representation,\nwhich is then used as input to the decoder.\nDuring training, autoregressive models are typically\noptimized using Maximum Likelihood Estimation (MLE)\n[110]. The model aims to optimize the likelihood of gen-\nerating target output sequence given the input sequence or\ncontext. This involves calculating the probability of each tar-\nget element based on the preceding elements and updating\nparameters to improve its predictions [111].\nOverall, autoregressive models are a powerful tool for\nsequence generation tasks, particularly in natural language\nprocessing [112]. They have contributed to significant ad-\nvancements in generative modeling and continue to be an\nactive area of research in the field.\n2.4\nDifference with previous approaches\nReinforcement Learning (RL) and generative models are two\ndistinct branches of machine learning that serve different\npurposes and tackle different challenges. The key difference\nlies in their primary objectives and the nature of the learning\nprocess [113], as show in Fig. 6.\nThe primary objective of RL is for an agent to learn an\noptimal policy that maximizes cumulative rewards through\ntrial-and-error with the environment. The agent learns by\ntaking actions, observing the resulting states, then adjusting\ncurrent strategy according to the returns. On the other\nhand, generative models aims to understand and represent\nthe latent data distribution in a given dataset [114]. These\nmodels are used to generate new samples resembling the\ntraining data, capturing inherent patterns and structures in\nthe data.\nThe second one is the learning process. In RL, the agent\nlearns through trial and error by exploring the environment,\ntaking actions, and receiving rewards. As it gains experi-\nence, the agent refines its policy to make more effective\ndecisions, ultimately maximizing its rewards. Meanwhile,\ngenerative models learn from a given dataset and analyze\nthe data, attempting to estimate the probability distribution\nthat generates the data, such as VAEs and GANs.\nLast but not least, they differ in how they interact with\nenvironment [115]. The RL agent interacts with the envi-\nronment iteratively, learning from feedback and adjusting\nits behavior without generating any data. In contrast, the\ngenerative model, once trained, is capable of producing\nentirely new data points resembling the original ones [116],\nthus actively generating rather than merely responding.\nIn summary, reinforcement learning focuses on learning\noptimal actions to achieve specific goals [117], whereas gen-\nerative models aim to model data distributions and generate\nnew, similar samples. Both are crucial in machine learning\nbut address different challenges.\nFig. 6: A comparative framework: traditional and generative\ndecision-making (MDP-based vs. data-driven).\n3\nTAXONOMY\nIn this section, we state our taxonomy to group the gen-\nerative approaches for solving sequential decision making\nproblems. We categorize the methodologies into five key\ndimensions, outlined as follows. This taxonomy is further\nillustrated in Table 1.\nFamily. The first dimension is the family of the gener-\native models. We represent the family of the approach by\nthe acronym of the category of the generative models. To\nbe more specific, Energy Based Models (EBMs), Generative\nAdversarial Networks (GANs), Variational Autoencoders\n(VAEs), Normalizing Flow (NFs), Diffusion Models (DMs),\nGFlowNets (GFNs), and Autoregressive Models (AMs).\nFunction. The second dimension functions on the role of\ngenerative models in sequential decision making. Because\nthey can model a diverse set of distributions, generative\nmodels can serve various roles in addressing sequential\ndecision-making problems. We basically classify the func-\ntions into three categories.\n●\nController. Generative models can serve as policy\nmodels to guide the decision-making process by\nmodeling high-reward trajectories or observation-\naction pairs. They generate candidate actions based\non current observations, refining these candidates to\nachieve optimal decisions. In this context, sampling\nfrom learned distributions is a key step that informs\nthe decision-making process.\n●\nModeler. Generative models are capable of capturing\nthe underlying patterns of the dataset and generating\nnew data resembling the original’s distribution. By\nproducing samples that capture the data’s structure,\nthey support tasks like data augmentation, privacy\npreservation, and simulation, providing rich inputs\nfor subsequent optimization or decision-making.\n●\nOptimizer. Generative models can explore high-\ndimensional spaces and optimize solutions by iter-\natively refining sampled candidates. They generate\n\n8\nModel\nFamily\nFunction\nStructure\nExpertise\nApplication\nDu & Mordatch [118]\nEBM\nController\nEBM\nImitation Learning\nRobot Control\nEBIL [119]\nEBM\nController\nEBM\nImitation Learning\nRobot Control\nBM [33]\nEBM\nModeler\nEBM\nGeneration\nOptimization / Structural Generation\nDEBMs [120]\nEBM\nModeler\nEBM\nOnline RL\nRobot Control / Optimization\nSGMs [99]\nEBM\nModeler\nEBM\nGeneration\nStructural Generation\nSO-EBM [121]\nEBM\nOptimizer\nEBM\nOthers\nOptimization\npcEBM [122]\nEBM\nOptimizer\nEBM\nGeneration\nStructural Generation / Optimization\nCF-EBM [123]\nEBM\nOptimizer\nEBM\nGeneration\nStructural Generation\nGAIL [63]\nGAN\nController\nGAN\nImitation Learning\nRobot Control\nInfoGAIL [124]\nGAN\nController\nWGAN\nImitation Learning\nRobot Control\nMGAIL [125]\nGAN\nController\nGAN\nImitation Learning\nRobot Control\nFAGIL [126]\nGAN\nController\nWGAN\nImitation Learning\nRobot Control\nWGAIL [127]\nGAN\nController\nGAN\nImitation Learning\nRobot Control\nIC-GAIL [128]\nGAN\nController\nGAN\nImitation Learning\nRobot Control\nAIRL [129]\nGAN\nController\nGAN\nImitation Learning\nRobot Control\nAugAIRL [130]\nGAN\nController\nGAN\nImitation Learning\nRobot Control\nWAIL [131]\nGAN\nController\nWGAN\nImitation Learning\nRobot Control\nMAIRL [132]\nGAN\nController\nGAN\nImitation Learning\nRobot Control\nEGAN [133]\nGAN\nModeler\nGAN\nOnline RL\nStructural Generation\nS2P [134]\nGAN\nModeler\nGAN\nOffline RL\nRobot Control\nHe et al. [135]\nGAN\nOptimizer\nGAN\nGeneration\nOptimization / Games\nDCGANs [136]\nGAN\nOptimizer\nDCGAN\nGeneration\nOptimization\nC-GANs [137]\nGAN\nOptimizer\nC-GAN\nGeneration\nOptimization\nGTI [138]\nVAE\nController\nCVAE\nImitation Learning\nRobot Control\nHULC [139]\nVAE\nController\nseq2seq CVAE\nImitation Learning\nRobot Control\nPlay-LMP [140]\nVAE\nController\nseq2seq CVAE\nImitation Learning\nRobot Control\nTACO-RL [141]\nVAE\nController\nseq2seq CVAE\nImitation Learning\nRobot Control\nOPAL [142]\nVAE\nController\nβ VAE\nImitation Learning\nStructural Generation\nMaskDP [143]\nVAE\nController\nMAE\nOffline RL\nRobot Control\nHan & Kim [144]\nVAE\nModeler\nVAE\nOffline RL\nStructural Generation\nCageBO [145]\nVAE\nOptimizer\nCVAE\nOthers\nOptimization\nCVAE-Opt [146]\nVAE\nOptimizer\nCVAE\nOthers\nOptimization\nNF Policy [147]\nNF\nController\nCoupling Flow\nOffline RL\nRobot Control\nCNF [148]\nNF\nController\nAutoregressive Flow\nOffline RL\nAutonomous Driving\nGuided Flows [149]\nNF\nController\nContinuous Flow\nOffline RL / Generation\nOptimization / Games\nNICE [89]\nNF\nModeler\nCoupling Flow\nGeneration\nOptimization\nRezende et al [82]\nNF\nModeler\nNF\nGeneration\nOptimization\nGabri´e et al. [150]\nNF\nOptimizer\nNF\nGeneration\nOptimization\nPearce et al. [151]\nDM\nController\nDDPM\nImitation Learning\nRobot Control / Games\nDiffuser [152]\nDM\nController\nDDPM\nOffline RL\nStructural Generation\nDecision Diffuser [153]\nDM\nController\nDDPM\nOffline RL\nStructural Generation\nDecision Stacks [154]\nDM\nController\nDDPM\nOffline RL\nStructural Generation\nDiffusion-QL [155]\nDM\nController\nDDPM\nOffline RL\nRobot Control\nDiffusion Policy [156]\nDM\nController\nDDPM\nRobotics\nRobot Control\nSfBC [157]\nDM\nController\nDDPM\nOffline RL\nStructural Generation\nUniPi [158]\nDM\nController\nDDPM\nOffline RL\nRobot Control\nAdaptDiffuser [159]\nDM\nController\nDDPM\nRobotics\nRobot Control\nDIPO [160]\nDM\nController\nDDPM\nOnline RL\nRobot Control\nMTDiff [161]\nDM\nModeler\nDDPM\nOffline RL\nRobot Control\nGenAug [162]\nDM\nModeler\nLDM\nRobotics\nRobot Control\nSynthER [163]\nDM\nModeler\nEDM\nOffline & Online RL\nRobot Control\nDDOM [164]\nDM\nOptimizer\nDDPM\nOthers\nOptimization\nLi et. al. [165]\nDM\nOptimizer\nDDIM\nOthers\nOptimization\nDiffOPT [166]\nDM\nOptimizer\nDDIM\nOthers\nOptimization\nGFlowNets [51], [102]\nGFN\nController\nGFN\nOffline & Online RL\nStructural Generation\nPan et al. [167]\nGFN\nController\nGFN\nOffline RL\nStructural Generation\nGAFlowNets [168]\nGFN\nController\nGFN\nOffline RL\nStructural Generation\nAFlowNets [169]\nGFN\nController\nGFN\nOffline RL\nStructural Generation / Games\nBrunswic et al. [170]\nGFN\nController\nGFN\nOffline RL\nStructural Generation\nCFlowNets [103]\nCFN\nController\nCFN\nOnline RL\nStructural Generation\nZhang et al. [171]\nGFN\nModeler\nGFN\nGeneration\nOptimization\nZhang et al. [172]\nGFN\nModeler\nGFN\nGeneration\nOptimization\nMOGFNs [173]\nGFN\nOptimizer\nCFN\nGeneration\nOptimization\nGFACS [174]\nGFN\nOptimizer\nGFN\nGeneration\nOptimization\nDecision Transformer [175]\nAM\nController\nDecoder Only\nOffline RL\nRobot Control\nTrajectory Transformer [176]\nAM\nController\nDecoder Only\nOffline RL\nRobot Control\nOnline DT [177]\nAM\nController\nDecoder Only\nOnline RL\nAutonomous Driving\nGATO [178]\nAM\nController\nDecoder Only\nOffline RL\nRobot Control / Games\nMulti-Game DT [179]\nAM\nController\nDecoder Only\nOffline RL\nGames\nPEDA [180]\nAM\nController\nDecoder Only\nOffline RL\nRobot Control\nBooT [181]\nAM\nController\nDecoder Only\nOffline RL\nStructural Generation\nPixelCNN [84]\nAM\nModeler\nDecoder Only\nGeneration\nStructural Generation\nPixelRNN [182]\nAM\nModeler\nDecoder Only\nGeneration\nStructural Generation\nBONET [183]\nAM\nOptimizer\nDecoder Only\nOthers\nOptimization\nTNP [184]\nAM\nOptimizer\nEncoder Decoder\nOthers\nOptimization\nTABLE 1: Generative models in decision making\n\n9\nand evaluate samples to navigate towards better\nsolutions, helping to optimize decision-making pro-\ncesses or high-reward outcomes.\nStructure. The third dimension illustrates the basic\nmodel structure of generative models. Below is the structure\nfor each category of generative models.\n●\nEBM: EBM\n●\nGAN: GAN / WGAN / DCGAN / C-GAN.\n●\nVAE: VAE / CVAE / seq2seq CVAE / β VAE / MAE.\n●\nNF: NF / Coupling Flow / Autoregressive Flow /\nContinuous Flow.\n●\nDM: DDPM / DDIM / LDM / EDM.\n●\nGFN: GFN / CFN.\n●\nAM: Decoder Only / Encoder Decoder.\nExpertise. The fourth dimension represents the expertise\nareas of the generative models in the sequential decision\nmaking. The expertise area includes Imitation Learning,\nonline RL, offline RL, Robotics, Generation and Others.\nApplication. The last dimension highlights several rep-\nresentative applications where generative models have\nshown particular promise, including robot control, au-\ntonomous driving, gaming, structural generation, and op-\ntimization.\n4\nSURVEY: METHODOLOGY\nThis section delves into the three core functions of genera-\ntive models and organizes the existing literature based on\nthe methodology taxonomy we propose.\n4.1\nGenerative Models as Controller\nA controller aims to find a policy (a set of instructions or\nrules) that determines how to modify the system’s inputs\nto attain the desired behavior [185], [186].The policy is\ntypically generated through a process called control system\ndesign, which involves analyzing the system’s behavior,\ndefining the desired outcomes, and then designing a con-\ntroller to achieve those goals. The method used to generate\nthe policy according to the system’s specific requirements\nand the design process’s objective. Generative models are\nable to model the decision-making process and produce\ndiverse decision paths, making them well suited for gen-\nerating policies in control systems.\nGenerative models are adept at capturing complex data\ndistributions, making them particularly effective for learn-\ning from expert trajectories in decision-making tasks. When\napplied as controllers, these models can leverage offline data\ncontaining trajectories associated with high-reward values\nto extract and replicate optimal policies. In general, gen-\nerative models facilitate the generation of control policies\nthrough two primary approaches, which we detail below.\n(1) Generative models learn the policy from the state-\naction or observation-action pairs, as shown in (19):\nθ = arg min\nθ\nE(si,ai)∼DL (π∗(si) −πθ(si)),\n(19)\nwhere L is a metric function.\n(2) Generative models learn the policy from the decision\ntrajectories D = (s0,a0,s1,a1, ⋯) as shown in (20):\nθ = arg min\nθ\nE(s0,a0,s1,a1,...)∼DL (π∗(s) −πθ(s)).\n(20)\nThe detailed explanations are as follows.\n4.1.1\nEBMs as Controller\nEBMs serve as controllers by solving the Inverse Rein-\nforcement Learning (IRL) problems, where the goal is to\nextract a control policy based on expert demonstrations.\nIn this context, the energy-based model is used to learn\nthe underlying reward structure of the expert’s behavior.\nDu & Mordatch [118] take a pioneering approach by using\nMarkov Chain Monte Carlo (MCMC)-based EBM training to\ngenerate robotic hand trajectories, applying it to continuous\nneural networks. Building on this, EBIL [119] further refines\nthe process by introducing a more straightforward solution\nto the IRL problems. Specifically, EBIL estimates the expert’s\nenergy through score matching, which serves as a surrogate\nreward function. This reward function is then utilized to\nlearn the policy through RL algorithms.\n4.1.2\nGANs as Controller\nGenerative Adversarial Imitation Learning (GAIL) [63] com-\nbines imitation learning with GANs to directly extract the\npolicy from data. It seeks to find a saddle point (π,D), rep-\nresenting a pair consisting of a policy π and a discriminator\nD, by optimizing the following expression:\nE\nπ[log(D(s,a))] + E\nπE\n[log(1 −D(s,a))] −λH(π),\n(21)\nwhere (s,a) denotes the state-action pair, π is the policy, πE\nrepresents the expert policy and H is a regularizer for the\npolicy. To overcome some of GAIL’s limitations, several ex-\ntensions have been proposed. For example, InfoGAIL [124],\nwhich builds upon GAIL, aims to calculate the potential\nstructure of expert demonstrations in an unsupervised man-\nner. This method can both imitate complex behaviors and\nextract meaningful, interpretable representations from intri-\ncate behavioral data. Similarly, MGAIL [125] incorporates\na forward model into GAIL, making the calculation differ-\nentiable and enabling policy training with fewer samples\nand interactions with the environment. In contrast, Fail-Safe\nAdversarial Generative Imitation Learning (FAGIL) [126],\nan extension of GAIL, introduces a safety layer to ensure\nthe generated policy is safe, while also facilitating closed-\nform computation of probability densities and gradients.\nThis method integrates end-to-end generative adversarial\ntraining with worst-case safety guarantees, making it well-\nsuited for scenarios requiring both safety and performance.\nOn the other hand, WGAIL [127] and IC-GAIL [128] address\nthe issue of imperfect demonstrations in GAIL. WGAIL\nfocuses on learning the weights of imperfect demonstrations\nwithout requiring substantial prior information, while IC-\nGAIL uses confidence levels to assess the quality of these\ndemonstrations, improving learning efficiency in the pres-\nence of noisy or suboptimal expert data.\nMoreover, there are other related approaches such as\nAdversarial Inverse Reinforcement Learning (AIRL) [129],\nwhich formulates policy learning as an adversarial reward\nlearning problem, allowing for acquisition of policies in\ndiverse environments. AugAIRL [130] enhances AIRL by\nadopting semantic rewards in the learning framework (see\nFig. 7 for an overview), while WAIL [131] bridges inverse\nreinforcement learning with optimal transport, using regu-\nlarized optimal transport for large-scale applications. Lastly,\n\n10\nModel-based AIRL (MAIRL) [132] improves policy opti-\nmization by leveraging a self-attention dynamics model to\nmake the computation graph end-to-end differentiable, and\nSC-AIRL [187] further reduces the need for extensive ex-\nploration by decomposing long-horizon tasks into subtasks,\nusing shared rewards and critics across different subtasks.\nFig. 7: The augmented AIRL learning framework uses the\ndiscriminator’s weights as the reward function [130].\n4.1.3\nVAEs as Controller\nVariational Autoencoders (VAEs) serve as controllers by\nlearning compact, structured latent spaces that encode high-\ndimensional environments and decision-making processes,\nwhich are directly used to generate control actions.\nFor\nexample,\nPlay-LMP\n[140]\nintroduces\na\nself-\nsupervised approach to learn control from play behaviors,\nenabling the system to autonomously generate control poli-\ncies. Building on this idea, TACO-RL [141] extends Play-\nLMP by adopting a hierarchical model, which allows for\nlearning long-horizon policies from high-dimensional cam-\nera observations in agnostic tasks . OPAL [142] utilizes VAE\nfor offline learning of primitive actions in imitation learn-\ning tasks. Similarly, Generalization Through Imitation (GTI)\n[138] develops a two-stage algorithm that leverages VAE for\npolicy learning through imitation, enhancing generalization\nin complex environments. Finally, HULC [139] attempts to\nimprove performance by integrating hierarchical decompo-\nsition in robot-controlled learning, utilizing a multimodal\ntransformer encoder, leveraging discrete latent plans, and\napplying a self-supervised contrastive loss.\nAdditionally, MaskDP [143] introduces a masked au-\ntoencoding technique that enhances decision-making mod-\nels by learning latent spaces in a scalable and general-\nizable manner, thereby improving performance in high-\ndimensional tasks (see Fig.8). Similar to VAEs, this ap-\nproach focuses on generating control policies by learning\nefficient latent representations, while additionally incorpo-\nrating masking mechanisms to improve the model’s ability\nto generalize across tasks.\n4.1.4\nNormalizing Flows as Controller\nNormalizing flows have garnered interest recently due to\ntheir capacity to learn complex probability distributions\nand perform efficient density estimation. While they are\nprimarily used in generative modeling tasks, they can also\nbe applied to decision-making problems control [74], [90].\nIn this context, normalizing flows can be leveraged to model\nMasked Decision Prediction\nDownstream Tasks\nGoal Reaching\nOffline RL\nCausal Actor\n......\n......\nCausal Critic\n......\n......\ngoal\nBidirectional Transformer\nmask\nmask\n......\nmask\nmask\nmask\nmask\nmask\n......\nSkill Prompting\nBidirectional Transformer\nmask\n...\n......\nmask\nmask\nmask\n......\nmask\nmask\nmask\nmask\nBidirectional Decoder\nBidirectional Encoder\nmask\n......\nmask\nmask\n......\n......\nPre-Training\nFig. 8: Illustration of MaskDP. In the pretraining phase, the\nmodel is trained to predict masked tokens. After pretrain-\ning, it can be applied to a variety of downstream tasks by\nusing different masking patterns [143].\nand approximate the underlying distribution of the decision\nspace [88], [188].\nThe natural use of Normalizing flows for controllers\nis behavioral cloning, where the behavior to reproduce is\nrepresented by the target distribution µ. However, since the\ntraining only requires µ to be samplable and ν to have a\ndefined density, one can train a ”reversed Normalization\nflows”, where ν represents a complex distribution defined\nby its density and µ is a simpler distribution, such as a\nnormal distribution.\nNormalizing Flow Policy (NF Policy) [147] integrates\nNormalizing Flow policies into the Soft Actor-Critic (SAC)\nframework, enabling the learning of more expressive poli-\ncies. Fig. 9 illustrates the process of sampling actions from\nNF policies. Conservative Normalizing Flows (CNF) [148]\nmakes use of Normalizing Flows as a conservative action\nencoder to learn policies in latent action spaces. First, the\naction encoder undergoes supervised pre-training on an\noffline dataset, and then a reinforcement learning-based\npolicy model (serving as a controller in the latent space)\nis trained. Guided Flows [149] generates plan in the of-\nfline reinforcement learning setting by training Continuous\nNormalizing Flows based on regressing vector fields via\nintegrating classifier free guidance.\nf1(at0)\n…\nat0\nat1\nat0 ∼p0(at0)\n= 𝒩(μ, σ)\ntanh(at0)\natN−1\natN\nat\nat1 ∼p1(at1)\natN−1 ∼pN−1(atN−1)\natN ∼pN(atN)\nst\nθ\n𝒩\n+\nState Encoder\nFig. 9: A sampled action from NF policies [147].\n4.1.5\nDiffusion Models as Controller\nDiffusion models, characterized by their iterative refinement\nand controlled noise perturbation processes, have proven\nto be powerful frameworks for sequential decision-making\ntasks. These models excel in scenarios requiring the progres-\n\n11\nsive transformation of noisy inputs into structured outputs,\nenabling them to serve as effective controllers.\nThere are generally two diffusion-based frameworks\nfor decision making. The first approach utilizes diffusion\nmodels to generate subsequent actions based on historical\nobservations, effectively capturing temporal dependencies\nin sequential tasks [189]. On the other hand, the second\none employs diffusion models to learn the trajectory of\nthe decision-making process, modeling the progression of\ndecisions over time and capturing the broader decision path.\nDiffusion\nmodels\ncan\nabstract\nthe\ndistribution\nof\nobservation-action pairs as a policy. For example, diffusion\nQ-learning (Diffusion-QL) [155] employs a conditional dif-\nfusion model as its policy. This model learns an action-value\nfunction and seeks to optimize actions that align closely\nwith the behavior policy by optimizing the loss function.\nThe policy is trained by reducing the loss below:\nL(θ) = Ei,ϵ,(s,a)∼D[∣∣ϵ −ϵθ(a,s,i)∣∣2].\n(22)\nSfBC [157] decomposes the policy into two components:\nan expressive generative behavior model for capturing the\ndata distribution and an action evaluation model for assess-\ning action quality. DIPO [160] utilizes a diffusion policy to\nperform model-free online reinforcement learning, enabling\nefficient decision-making in complex environments.\nDiffusion models also provide an effective way to de-\nscribe the joint distribution of trajectories. Diffuser [152]\ntreats trajectory optimization as a modeling problem, which\nis solved by iteratively refining trajectories using a diffusion\nprobabilistic model. In this framework, a trajectory is repre-\nsented as τ = [s0\ns1\n⋯\nsT\na0\na1\n⋯\naT ], where T denotes the time\nhorizon. During training, Diffuser minimizes a diffusion\nloss to optimize the trajectory generation process:\nL(θ) = Ei,ϵ,τ 0[∣∣ϵ −ϵθ(τ i,i)∣∣2].\n(23)\nDiffuser leverages classifier-guided sampling and im-\nage inpainting to reinterpret planning strategies. Deci-\nsion Diffuser [153] models decision-making through condi-\ntional generative modeling, treating the policy as a return-\nconditioned diffusion model (see Fig. 10 for an overview).\nThis approach allows Decision Diffuser to incorporate addi-\ntional variables such as constraints and skills, enabling the\ngeneration of behaviors that meet multiple constraints si-\nmultaneously or demonstrate a combination of skills during\ntest-time. AdaptDiffuser [159] creates rich synthetic expert\ndata for goal-conditioned tasks by guiding the process with\nreward gradients and selecting high-quality data through a\ndiscriminator to finetune the diffusion model.\nDiffusion models offer diverse applications in decision-\nmaking.\nDecision\nStacks\n[154]\ndeconstructs\na\ngoal-\nconditioned policy agent into three generative modules that\nuse independent models trained in parallel with teacher\nforcing to simulate the temporal evolution of observa-\ntions, rewards, and actions. Diffusion Policy [156] simulates\nrobotic policy by converting a visuomotor policy into a\ndenoising diffusion process conditioned on specific inputs.\nUniPi [158] casts sequential decision-making as a video\ngeneration task conditioned on text. The planner generates\na sequence of future frames depicting planned actions based\non a text-encoded goal, from which control actions are de-\nrived. Pearce et al. [151] attempts to imitate human behavior\nusing diffusion models, abstracting a rich distribution over\nthe joint action space.\nst+h\nat+h\nreturns\nconstraints\nskills\nst+2\nat+1\nst+1\nat\nst+h\nat+h\nst+2\nat+1\nst+1\nat\nst+h\nat+h\nst+2\nat+1\nst+1\nst\nFig. 10: Planning with decision diffuser via generative mod-\neling and inverse dynamics [153].\n4.1.6\nGFlowNets as Controller\nGFlowNet (GFN) is a powerful tool for sequential sampling,\nparticularly in Markov decision processes (MDPs). In such\na process, where a reward function R(⋅) is defined on\neach terminal state sf, GFlowNet can be trained to learn a\nstochastic policy that determines the probability of any ter-\nminal state sf is proportional to R(sf). The most basic form\nof GFlowNet is applied to discrete, acyclic MDP (DAGs),\nwith both the state and action space are finite, and the states\nand their transitions form a directed acyclic graph (DAG). In\nthis context, we define four key components of a GFlowNet:\n1)\nAn edge flow function F(s,s′) which denotes the\nflow on the edge s →s′.\n2)\nA vertex flow function F(s):\nF(s) =\n∑\ns′∈child(s)\nF(s,s′).\n(24)\n3)\nForward probability PF (s,s′) = F (s,s′)\nF (s) .\n4)\nBackward probability PB(s,s′) = F (s′,s)\nF (s′) .\nThe induced policy of a GFlowNet is given by PF ,i.e.,\nπf(s,a)=s′(s,a) = PF (s,s′).\nGFlowNet [51], [102] is well-suited for complex scenarios\nwhere multiple trajectories may lead to the same terminal\nstate. It transforms the set of trajectories into a flow and\nformulates the flow consistency equations as the learning\nobjective, similar to how the Bellman equations are cast\ninto Temporal Difference (TD) methods. GFlowNet demon-\nstrates that any global minimum of the proposed objective\nresults in a policy that samples from the desired distribu-\ntion, achieving improved performance and diversity.\nCFlowNet\n...\n...\n...\n...\n...\n...\n...\n...\nCFlowNet\nContinuous Flow \nMatching Loss\nRandomly Sample Continuous Action\nRandomly Sample Continuous Action\nInflows\nInflows\nApproximation\nOutflows\nApproximation\n...\n...\n...\n...\nProbability\nDistribution\nAction Probability\nBuffer     \nCFlowNet\nForward\nPropagation\nEnvironment\nInteraction\nTraining\nFlow Sampling\nOutflows / Reward\nFig. 11: Overall framework of CFlowNets [103]: environ-\nment interaction, flow estimation, and training.\n\n12\nStochastic GFlowNet [167], an extension of GFlowNet,\nis tailored for stochastic environments. It decomposes state\ntransitions into isolated environmental stochasticity, with\na dynamics model trained to capture this stochasticity.\nGAFlowNets [168] introduces an intermediate reward as\nintrinsic motivation to address the exploration problem in\nsparse reward environments. This model combines edge-\nbased and state-based intrinsic rewards to guide explo-\nration effectively. Expected Flow Networks (EFlowNets)\n[169] adapts GFlowNets to stochastic environments as well,\nextending their applicability. Brunswic et al. [170] further\ngeneralizes GFlowNets to measurable spaces, including\ncontinuous state spaces, and extends the concept to handle\ncycles in this broader context.\nCFlowNet [103] represents a significant advancement\nin the GFlowNet community by extending its capabilities\nto continuous control tasks (see Fig. 11). This extension is\ncrucial as it bridges the gap between simplified simulation\nenvironments and complex real-world scenarios, expanding\nGFlowNet’s applicability to a broader range of practical\napplications. CFlowNet introduces a novel approach for\nsolving continuous control tasks using GFlowNets. The\nauthors present a theoretical formulation of CFlowNets\nand propose a unified framework that combines the ac-\ntion selection strategy, flow approximation method, and\noptimization objectives. Experimental results demonstrate\nthat CFlowNets outperform many reinforcement learning\nmethods, particularly in terms of exploration ability. A\nsubsequent work, Continuous GFlowNet [105], develops a\nmathematical framework for continuous GFlowNet theory,\nfurther advancing the field.\n4.1.7\nAutoregressive Models as Controller\nAutoregressive models excel in decision-making tasks by\nsequentially generating outputs conditioned on past ob-\nservations. Decision Transformers (DTs) [175] advance this\napproach by defining reinforcement learning as conditional\nsequence modeling. It attempts to reformulate reinforce-\nment learning as a conditional sequence modeling prob-\nlem. It employs a causally masked Transformer to generate\noptimal actions aimed at maximizing the expected reward,\nconditioned on the past states, actions and the reward. The\nOnline Decision Transformer [177] pretrains in the offline\nmode and fine-tunes online, unifying both within a frame-\nwork that leverages sequence-level entropy regularizers and\nautoregressive modeling objectives to guide the learning\nprocess. DADT [190] improves the generalization ability by\nincorporating representation learning, which predicts the\nnext state.\nR\ns\na\na\ns\na\na\ncausal transformer\nemb. + pos. enc.\nlinear decoder\n. . .\n21\nreturn\nstate\naction\n. . .\n^\nR\n^\nFig. 12: Decision transformer architecture [175].\nPrompt-DT [191] extends DTs by introducing task-\nspecific prompts as Fig. 13, enabling few-shot policy gen-\neralization and quick adaptation to new tasks, while main-\ntaining the autoregressive decision-making framework.\nCausal Decision Transformer for Recommender Systems\n(CDT4Rec) [192] applies DTs to recommendation systems by\nincorporating causal inference, predicting recommendations\nbased on user history to maximize long-term reward. It\nserves as a controller, generating actions conditioned on past\nuser interactions and system states.\nFig. 13: Prompt-DT for few-shot policy generalization [191].\nTrajectory Transformer [176] also extends this perspec-\ntive by framing RL as sequence modeling over full trajecto-\nries. It models distributions over trajectories using a Trans-\nformer architecture and employs beam search for planning,\nenabling the generation of high-reward action sequences.\nBooT [181] further refines this approach by leveraging\nbootstrapping techniques and generating additional self-\nsupervised offline data to enhance sequence model training.\nTrajectory Transformer\nFig. 14: The trajectory transformer trains on autoregressively\ndiscretized state, action, and reward sequences, using a\nplanning process akin to language model sampling [176].\nPareto-Efficient Decision Agents (PEDA) [180] extends\nDecision Transformers [175] and RvS [193] for multi-\nobjective reinforcement learning. It introduces a preference-\nand return-conditioned policy to handle multiple objectives\nefficiently in offline settings. With the advancement of large-\nscale language models [2], Transformer-based controllers\nhave emerged as versatile solutions capable of handling a\ndiverse set of tasks within a single unified model. GATO\n[178] introduces a generalist agent capable of handling\nmulti-modal, multi-task, and multi-embodiment scenarios.\nThis agent is versatile in executing a wide range of func-\ntions, including chatting, controlling robotic arms, and de-\ntermining outputs such as text, joint torques, button presses,\nor other actions based on contextual input. Based on this\nwork, RT-1 [194] develops a scalable model trained on a\nlarge-scale dataset of real-world tasks performed by real\nrobots. It shows performance improvements that scale with\ndata size, model capacity, and the diversity of tasks in-\nvolved.\n4.2\nGenerative Models as Modeler\nUnlike previous method S4RL [195] or RAD [196], which\naugments the states or the input spaces with slight per-\nturbation to ensure the consistency, generative models take\n\n13\na different approach. Instead of merely modifying existing\ninputs, generative models focus on learning the underlying\npatterns within a dataset, thereby generating new data that\nclosely resembles the original distribution [197].\nIn other words, a generative model processes a given set\nof input data and analyzes it to uncover the inherent pat-\nterns and structures of the data points [198]. In this context,\ngenerative models can be viewed as powerful ”modelers”,\ncapable of capturing complex relationships within data and\ncreating new and similar data that reflect these intrinsic\nrelationships. This means that they can be employed to\nextract data representations for downstream tasks, such as\nclassification or regression.\nFor example, generative models are capable of generat-\ning new images that resemble the images in the training\ndataset [199]. The model analyzes the features and patterns\nof the original images and uses this information to produce\nnew images that mirror the style and content of the origi-\nnals, just like DALL·E [1] does. Similarly, generative models\ncan be used to create new text similar in style and content to\nthe training corpus. Beyond image and text generation, they\nare useful for creating synthetic data for purposes like data\naugmentation, privacy protection, and simulations.\nOverall, a generative model can be viewed as a ”mod-\neler” because it learns the underlying patterns from a\ndataset [200] and generates new data that closely resembles\nthe original data.\n4.2.1\nEBMs as Modeler\nAs ”models”, EBMs model data through an energy function\nthat captures the underlying structure. Unlike other genera-\ntive models, EBMs do not rely on explicit probabilities dis-\ntributions but instead use an energy landscape to describe\nthe data. Once trained, EBMs can generate new data by\nminimizing the energy, ensuring that the generated samples\nalign with the learned patterns of the original dataset.\nBoltzmann Machines (BM) [33] uses contrastive diver-\ngence techniques to generate samples that match the train-\ning data’s distribution. Similarly, DEBMs [120] extend this\napproach by using energy-based models to learn a distribu-\ntion over state-action pairs, enabling reinforcement learning\nagents to generate and select optimal actions through con-\ntrastive divergence. Score-Based Generative Models (SGMs)\n[99] use score matching techniques to learn the gradients of\nthe data distribution, enabling the generation of samples by\nrefining noisy data towards the target distribution.\n4.2.2\nGANs as Modeler\nGANs serve as models by learning the underlying patterns\nthrough their unique adversarial framework, which consists\nof two parts: the generator and the discriminator. Synthetic\ndata is generated by the generator, with the discriminator\nassessing its realism, which in turn drives the generator to\nrefine its output.\nEGAN [133] serves as a modeler by leveraging the rela-\ntionship between states and actions to learn the underlying\npatterns of the environment, which enables the pre-training\nof the agent and accelerates the learning process. Similarly,\nState2Pixel (S2P) [134] synthesizes raw pixel images from\nthe agent’s state, effectively bridging the gap between the\nstate space and the image domain in reinforcement learning\n(RL) algorithms. Additionally, S2P facilitates virtual explo-\nration of the latent image distribution through model-based\ntransitions in the state space.\n4.2.3\nVAEs as Modeler\nVAEs are well-suited as modelers due to their capacity in\nlearning a probabilistic mapping from the data space to the\nlatent space, capturing the underlying variations in the data.\nHan & Kim [144] represent the subspaces of the dataset\nby using the variational autoencoder. Subsequently, the VAE\ndecoder is used to extract new data from the latent space\nand maps it back to the original one.\n4.2.4\nNormalizing Flows as Modeler\nNormalizing Flows (NF) serve as powerful tools for mod-\neling complex, high-dimensional data distributions by ap-\nplying a series of invertible transformations. This flexibility\nenables NF to capture intricate data structures, making them\nparticularly useful for tasks such as density estimation,\nvariational inference, and posterior distribution modeling.\nFor instance, Non-linear Independent Component Es-\ntimation (NICE) [89] applied NF to model complex dis-\ntributions by applying a series of invertible transforma-\ntions, allowing it to represent high-dimensional data dis-\ntributions flexibly. Rezende and Mohamed [82] further ex-\npanded the use of NF by applying them in variational\ninference to model complex posterior distributions. By ap-\nplying NF to parameterize the posterior, they were able\nto construct flexible, tractable distributions that could be\nused for efficient Bayesian inference. This method helped\novercome challenges associated with approximating pos-\nterior distributions, offering a scalable approach to varia-\ntional inference in high-dimensional spaces. M¨uller et al.\n[201] enhanced NF’s flexibility by introducing piecewise-\npolynomial coupling transformations, which enhanced NF’s\nability to model more complex distributions. By applying\nNF to parameterize the posterior, they were able to con-\nstruct flexible, tractable distributions that could be used for\nefficient Bayesian inference. This method helped overcome\nchallenges associated with approximating posterior distri-\nbutions, offering a scalable approach to variational inference\nin high-dimensional spaces.\n4.2.5\nDiffusion Models as Modeler\nDiffusion models are based on a network architecture that\niteratively denoises samples, starting from random noise\nand gradually transforming it into structured data. This\nprocess enables them to capture complex data distributions\nand generate high-quality data by learning to reverse the\nnoise-corruption process, making them highly effective as\nmodelers.\n𝐴\nMulti-task actions\n𝐴\n𝐴\n𝐴\n𝐴\n𝑅\n𝑆\n𝐴\n𝑆\n𝑅\nMulti-task transitions\nOptimal actions\nDiffusion Model\nConditional Generative Process\n𝑆\n𝑅\nR\n𝐴\n𝑆\n𝐴\nHigh fidelity synthetic transitions\nAct\nAugment\nMulti-Task Dataset\n(a) Planning\n(b) Synthesizing\n𝑆prev, 𝑅𝜏\n𝐴\n𝑅\n𝑆\n𝐴\n𝑆\n𝑅\n𝑆\n𝑅\nR\n𝐴\n𝑆\n𝐴\n𝐴\n𝐴\n𝐴\n𝐴\nPrompt\nEnvironment\nOriginal Data\nFig. 15: Overall architecture of MTDIF F . Different colors\nrepresent different tasks [161].\n\n14\nROSIE [202] introduces text-to-image diffusion models\nto acquire valid data for robot to learn without additional\nrobot data. Specifically, this method takes advantage of the\nart text-to-image diffusion. ROSIE inpaints diverse invisi-\nble objects for manipulation, backgrounds, and distractors\nwith the guidance from the text. In robotics manipulation\ntasks like tabletop pick-and-place, GenAug [162] utilizes\na small image-action demonstration dataset and produces\nan augmented dataset of image observations to enhance\nthe real-world demonstration data. SynthER [163] flexibly\nupsamples an agent’s collected experience to train RL agents\noffline and online, in both proprioceptive and pixel-based\nenvironments. MTDiff [161] introduces a single diffusion\nmodel to establish large-scale multi-task offline data model,\nwhich combines Transformer backbones and prompt learn-\ning for data synthesis in multitask offline scenarios.\n4.2.6\nGFlowNets as Modeler\nGenerative Flow Networks (GFlowNets) model complex\ndata distributions by learning to sample from reward-\ndefined distributions, making them effective for tasks like\ndata augmentation and simulation.\nIn [171], GFlowNets treat sampling as a decision-making\nprocess, using Markovian trajectories to model data distri-\nbutions. This unifies various generative models, enabling\nefficient training and inference algorithms.\n[172] extends this idea with energy-based GFlowNets\n(EB-GFNs), combining GFlowNets with energy functions\nto generate samples from energy-defined distributions. This\napproach improves probabilistic modeling, particularly for\nhigh-dimensional discrete data.\nIn [203], GFlowNets are applied to text-to-image gen-\neration. The Diffusion Alignment with GFlowNet (DAG)\nalgorithm post-trains diffusion models by generating high-\nreward images with high probability, addressing the chal-\nlenges of alignment in generative models.\n4.2.7\nAutoregressive Models as Modeler\nAutoregressive models, such as PixelCNN [84] and Pix-\nelRNN [182], generate data by modeling the conditional\ndependencies between data points, predicting each com-\nponent sequentially based on previous ones. This structure\nallows them to capture complex patterns in data, making\nthem highly effective ”modelers”, especially in tasks like\nimage generation, where they predict pixel distributions to\ngenerate coherent, high-quality images.\n4.3\nGenerative Models as Optimizer\nRecent research has increasingly approached optimization\nproblems as sampling tasks, with generative models being\nused to tackle these challenges. This shift allows generative\nmodels to learn data representations that can be directly\nused to optimize specific objective functions, making them\npowerful tools for solving complex optimization problems\n[204]. These models can capture intricate data distributions\nand generate solutions that are both efficient and scalable\n[205]. In this context, we explore how generative models,\nwhen applied as optimizers, offer new perspectives on\nimproving optimization strategies, surpassing traditional\nmethods in several domains.\n4.3.1\nEBMs as Optimizer\nEnergy-Based Models (EBMs) can also function as powerful\noptimizers by refining candidate solutions toward optimal\nconfigurations in the solution space. The key mechanism\ninvolves learning an energy function that not only evaluates\nthe quality of solutions but also guides the optimization\nprocess. EBMs optimize the energy landscape by employing\ntechniques like contrastive divergence [206] or stochastic\ngradient descent. Rather than generating random samples,\nthe optimization process is driven by minimizing the energy\nfunction, which allows EBMs to select the most optimal so-\nlutions according to the learned energy landscape. Methods\nlike MCMC or Langevin dynamics are often employed to\niteratively refine candidate solutions, effectively navigating\nthe solution space to converge toward the global or local\nminimum.\nEBMs have shown their effectiveness in various op-\ntimization tasks. One approach, SO-EBM [121], lever-\nages EBMs to directly parameterize optimization problems\nthrough differentiable energy functions. Unlike traditional\nmethods that rely on KKT conditions to implicitly induce an\noptimization layer, SO-EBM explicitly defines the optimiza-\ntion process by using an energy-based model, allowing for\nmore accurate capture of the optimization landscape. This\napproach enables more efficient and refined optimization,\nparticularly in complex, high-dimensional tasks.\nAdditionally,\nthe\nPareto-compositional\nenergy-based\nmodel (pcEBM) [122] is designed for protein sequences\noptimization and sampling. In this approach, pcEBM in-\ntegrates multiple objectives into a single energy function\nthrough a compositional structure that balances competing\nperformance criteria. The model learns a multi-objective\nenergy function that simultaneously accounts for factors\nsuch as structural stability and sequence diversity, which\nare critical in protein design. By optimizing this compo-\nsitional energy function, pcEBM efficiently navigates the\nsolution space to identify sequences that achieve a Pareto-\noptimal balance. This method enhances the optimization\nprocess, making it particularly valuable for tasks requiring\nthe simultaneous optimization of multiple criteria, such as\nin protein engineering and drug design. Similarly, a multi-\nstage coarse-to-fine expanding and sampling strategy (CF-\nEBM) [123] also utilizes sampling as a key mechanism for\noptimization. While pcEBM optimizes multi-objective tasks\nby refining protein sequences, the coarse-to-fine method\nenhances sample quality by progressively refining solutions.\nBoth methods optimize the energy landscape through sam-\npling, highlighting the versatility of EBMs in addressing a\nrange of optimization challenges, from multi-objective tasks\nto high-quality generative sampling.\n4.3.2\nGANs as Optimizer\nGenerative Adversarial Networks (GANs) have emerged\nas powerful optimizers, addressing challenges in high-\ndimensional and complex optimization problems.\nHe et al. [135] demonstrated how GANs can generate\nhigh-quality offspring solutions in multi-objective evolu-\ntionary algorithms, improving both solution diversity and\noptimality. This method alleviated the curse of dimensional-\nity, improving optimization performance with limited data.\n\n15\nSimilarly, Sim et al. [136] utilized GANs and DCGANs for\ntopology optimization, generating valid data for structural\ndesign optimization. Their method utilized K-means clus-\ntering analysis to select optimized solutions, showing how\nGANs can be leveraged for structural design optimization,\nefficiently navigating complex design spaces and producing\nfeasible solutions that meet specific criteria. This highlights\nGANs’ flexibility in optimization tasks beyond traditional\nsettings, extending their application to engineering and\ndesign optimization.\nFurthermore, Kalehbasti et al. [137] explored the combi-\nnation of Conditional GANs (C-GANs) with genetic algo-\nrithms for solving high-dimensional nonlinear optimization\nproblems. By augmenting solutions generated by a genetic\nalgorithm with C-GANs, they enhanced the search for opti-\nmal solutions, demonstrating that the integration of GANs\nwith evolutionary algorithms significantly accelerates the\noptimization process, even in the absence of vast amounts\nof training data. This development underscores the growing\ncapability of GANs to serve as powerful optimizers in\ndiverse optimization problems.\n4.3.3\nVAEs as Optimizer\nVAEs are effective optimizers due to their ability to learn\ncompact probabilistic representations, which facilitate ef-\nficient exploration of large and complex decision spaces.\nBy mapping complex high-dimensional inputs to a simpler,\nlower-dimensional latent space, VAEs enable more efficient\nexploration of the decision space for optimization tasks.\nFor instance, CageBO [145] employs a conditional VAE\nto model the distribution of viable decisions in optimization\ntasks. By establishing a bidirectional mapping between the\ninitial decision space and a reduced, unconstrained latent\nspace, CageBO is able to optimize decision-making pro-\ncesses more efficiently. In particular, the model uses the la-\ntent representation to navigate the decision space, enabling\nmore effective and scalable exploration of possible solutions.\nThis approach improves optimization performance by gen-\nerating high-quality candidate solutions and reducing the\ncomputational burden typically associated with exhaustive\nsearch methods.\nSimilarly,\nConditional\nVariational\nAutoencoders\n(CVAEs) are also used in optimization tasks where the\ndecision space is complex, such as routing problems.\nIn [146], researchers employ a CVAE to learn a latent\nrepresentation of the solution space, optimizing the search\nfor feasible solutions (see Fig.16). By conditioning the\ngenerative process on problem-specific constraints, CVAE\nrefines the optimization process to generate high-quality\nsolutions within the reduced latent space.\nFig. 16: The overall training process of CVAE-Opt (Left) and\nthe iterative search process (Right) [146].\n4.3.4\nNormalizing Flows as Optimizer\nNormalizing Flows (NF) can also serve as powerful op-\ntimizers. The core idea behind using Normalizing Flows\n(NF) as an optimizer is their ability to transform simple\ndistributions (like Gaussian or uniform) into complex ones\nthat match the desired solution distribution. This enables the\ngeneration of high-quality solutions efficiently, with fewer\niterations than traditional methods. Because NF are differ-\nentiable, they can be easily integrated with gradient-based\noptimization, allowing for smooth, end-to-end optimization\nthrough backpropagation.\nGabri´e et al. [150] proposed an adaptive MCMC method\nthat enhances traditional MCMC sampling by incorporating\nnonlocal transition kernels parameterized by normalized\nflows. This method enhances optimization by using NF\nto guide the search more efficiently. By combining local\nMCMC updates with NF-driven transitions, it accelerates\nconvergence to optimal solutions, improving both efficiency\nand performance.\n4.3.5\nDiffusion Models as Optimizer\nDiffusion models have shown increasing potential as opti-\nmizers. The core advantage of diffusion models lies in their\niterative denoising process, which allows them to refine\ncandidate solutions over time, making them particularly ef-\nfective for black-box optimization tasks where the objective\nfunction is unknown or difficult to compute directly.\nIn traditional optimization problems, gradient-based\nmethods often struggle with high-dimensional spaces, espe-\ncially when the objective function is highly complex or non-\nconvex. In contrast, diffusion models approach optimization\nthrough a process of gradually refining solutions, effectively\nexploring the solution space and generating promising can-\ndidates.\nFor instance, Denoising Diffusion Optimization Models\n(DDOM) [164] introduce a novel inverse approach for solv-\ning offline black-box optimization problems by leveraging\ndiffusion models. This method models complex objective\nfunctions through a reverse diffusion process, offering an\nefficient way to explore and optimize in high-dimensional,\npoorly understood spaces. By iterating through multiple\ndiffusion steps, DDOM can refine candidate solutions to\nfind near-optimal results even when the objective function\nis opaque or difficult to compute directly.\nSimilarly, Li et al. [165] reformulate optimization as a\nconditional sampling task and propose a reward-directed\nconditional diffusion model as Fig.17. This model is trained\nusing mixed data to sample near-optimal solutions con-\nditioned on predicted rewards, guiding the optimization\nprocess in a direction that maximizes the objective function’s\nexpected value. This reward-directed approach allows the\nmodel to focus on the most promising areas of the solution\nspace, further enhancing its efficiency in optimization tasks.\nDunlabel\nDlabel\n…\nReward Model\nLabel\nPseudo Label\n˜D\nX \n0\nX \nT\nX \nt\nX \nt0\nConditional Score\nReward-Conditioned Diﬀusion\ns(feature, label, t)\nTarget Reward\nDnew\nHigh-reward Generated Samples\nStep 1: Reward Learning\nStep 2: Pseudo Labeling\nStep 3: Conditional Diﬀusion Model Training\nStep 4: Guided Generation\nFig. 17: Black-box optimization with reward-directed condi-\ntional diffusion models [165].\n\n16\nAnother approach, DiffOPT [166], reframes the optimiza-\ntion problem by drawing samples from a joint Boltzmann\ndistribution determined by both the objective function and\nthe learned data distribution. This method incorporates a\ntwo-stage framework: the first stage uses a guided diffusion\nprocess to initialize the optimization, while the second stage\napplies Langevin dynamics to further refine the solution.\nThe combination of these two stages allows for both effi-\ncient exploration and precise refinement, making DiffOPT\nparticularly effective for complex optimization tasks.\n4.3.6\nGFlowNets as Optimizer\nGenerative Flow Networks (GFlowNets) have emerged as\npowerful optimizers, effectively navigating complex solu-\ntion spaces through a unique generative flow framework.\nBy constructing solutions step by step according to a learned\nflow, GFlowNets can efficiently explore and exploit the\nunderlying structure of optimization problems. The flow-\nbased nature of GFlowNets enables them to generate di-\nverse candidate solutions while maintaining high sample\nefficiency, which is crucial in scenarios where the evaluation\nof solutions is computationally expensive. Their ability to\nlearn from both on-policy and off-policy data further en-\nhances their adaptability and performance across a wide\nrange of optimization tasks.\n[207] explores the application of GFlowNets to combina-\ntorial optimization (CO) problems, which are often NP-hard\nand challenging for exact algorithms. The researchers design\na MDP tailored to various combinatorial problems and train\nconditional GFlowNets to sample from the solution space.\nExtensive experiments on diverse CO tasks with both syn-\nthetic and realistic data have demonstrated that GFlowNet\npolicies can effectively find high-quality solutions.\nBuilding on this foundation, GFlowNets were extended\nto multi-objective optimization, addressing the challenge of\nbalancing multiple conflicting objectives. MOGFNs [173], a\nnovel method based on GFLowNets, can generate diverse\nPareto-optimal solutions by learning to navigate the trade-\noffs between different objectives. This development signifi-\ncantly broadened the scope of GFlowNets’ applicability in\noptimization.\nAdditionally, Generative Flow Ant Colony Sampler\n(GFACS) [174] hierarchically combines amortized inference\nand parallel stochastic search. The solution sampling pro-\ncess is shown in Fig. ??. This hybrid approach leverages the\nstrengths of both methods: the efficient sampling and flow-\nbased learning of GFlowNets, and the heuristic search capa-\nbilities of ACO. GFACS demonstrated significant improve-\nments in solving combinatorial optimization problems by\nincorporating off-policy training and local search strategies.\nFig. 18: Solution sampling mechanism of GFACS [165]. The\nGNN, trained with a GFlowNet loss, acts as an expert guide\nto select the next step in constructing a solution, like a tour\nin the TSP.\n4.3.7\nAutoregressive Models as Optimizer\nAutoregressive models, particularly due to their sequen-\ntial structure, are effective optimizers because they capture\ntemporal or sequential dependencies between data points,\nallowing for iterative refinement of solutions. This structure\nenables the models to handle uncertainty and adaptively\nimprove the optimization performance across various tasks.\nThe sequential nature of autoregressive models, which pre-\ndicts the next step based on previous steps, allows them to\ngenerate or adjust solutions gradually, making them highly\neffective in settings where the solution space is complex and\nhigh-dimensional.\nFor instance, Transformer Neural Processes (TNPs)\n[184] introduce a meta-learning framework that employs\na transformer-based autoregressive model for optimizing\nperformance under uncertainty in tasks that involve vari-\nable inputs or distributions. By leveraging the strengths of\ntransformers in sequence modeling and combining them\nwith Neural Processes for better uncertainty estimation,\nTNPs optimize a model’s performance on various tasks by\niteratively refining predictions and learning optimal strate-\ngies.\nSimilarly, BONET [183] proposes a generative frame-\nwork that uses autoregressive models for pretraining op-\ntimization on offline datasets. By training a transformer-\nbased model in an unsupervised manner, BONET learns to\noptimize task performance in real-world settings without\ndirect task-specific supervision. This enables it to adapt to\na wide variety of optimization problems, from machine\nlearning to more general generative optimization tasks.\nFig. 19: Illustration of the TNP-A architecture [184].\n5\nAPPLICATIONS\nThere are lots of applications of generative models in deci-\nsion making scenarios. We consider five typical applications\nincluding, robot control, autonomous driving, games, struc-\ntural generation, and optimization.\n5.1\nRobot Control\nRobot control refers to the process of commanding a robot\nto execute specific tasks or actions [208], [209], [210], [211],\n[212]. This can be achieved through a variety of meth-\nods, including manual control by a human operator, pre-\nprogrammed instructions, or autonomous decision-making\nusing sensors and machine learning algorithms.\nPolicy generation in robotics refers to creating the\ndecision-making framework that guides a robot’s actions\n[213], [214], [215]. It entails defining objectives or goals for\nthe robot while incorporating constraints and limitations to\nensure safe and efficient behavior.\n\n17\nEffective policy generation for robots requires consid-\neration of various factors, such as the robot’s capabilities,\nlimitations, operating environment, and task requirements.\nThis often involves designing sophisticated algorithms and\ndecision-making models capable of real-time adaptation to\ndynamic conditions.\nRobot control generally encompasses several key as-\npects, including robot manipulation [216], trajectory gen-\neration [54], [106], [217], and locomotion [218]. Generative\nmodels play a pivotal role in either directly controlling\nrobots or generating synthetic data to enhance the training\nof more effective control policies.\n5.2\nStructural Generation\nGenerative models are increasingly applied to various\ngraph-based tasks, such as graph generation [219], graph\ncompletion [220], and graph classification [221]. In graph\ngeneration, these models learn the underlying structures\nof training graphs and use this knowledge to generate\nnew graphs with similar characteristics. This capability\nhas wide-ranging applications, including molecule design\nin chemistry, protein interaction modeling in biology, and\narchitecture optimization in engineering.\nA notable example of applying generative models in\nstructure generation is through GFlowNets. Jain et al. [222]\nproposed an active learning framework that integrates epis-\ntemic uncertainty estimation with GFlowNets to generate\ndiverse and informative candidate solutions for drug dis-\ncovery and other problems. This method selects a batch\nof promising candidates after each learning round. To im-\nprove the exploration efficiency, Kim et al. [223] introduced\na local search strategy within GFlowNets, targeting high-\nreward sample spaces while preventing over-exploration.\nThis approach combines forward and backward policies\nwith backtracking and reconstruction techniques, further\nrefining the generated solutions.\nRecent works [224], [225], [226] have made notable ad-\nvancements in integrating reinforcement learning (RL) with\nnatural language processing (NLP). Xin et al. [224] propose\nPrompt-Based Reinforcement Learning (PRL), which uses\nhistorical data and state-reward pairs as prompts to train\nRL-based recommendation agents. Deng et al. [225] intro-\nduce RLPROMPT, an efficient method for discrete prompt\noptimization that leverages RL to discover optimal prompts\nfor pre-trained language models (LMs). This approach is\napplicable across different types of LMs and tasks, including\nboth classification and generation. Zhang et al. [226] present\nTEMPERA, which designs optimal prompts for large LMs in\nzero-shot or few-shot learning settings by leveraging prior\nknowledge and enabling flexible prompt edits.\nThese works illustrate how RL-optimized generative\napproaches, like prompt-based methods, improve decision-\nmaking in language models. By structuring inputs and\noutputs flexibly, they facilitate context-aware solutions for\ncomplex tasks, such as multi-step reasoning and dynamic\nadaptation, with potential applications beyond NLP in\nfields like robotics and planning.\n5.3\nGames\nGames artificial intelligence (AI) is a prominent research\narea focused on developing AI techniques that achieve\nhuman-level performance in gameplay [227]. Games offer\nintriguing and complex problems for AI agents to solve\nwithin safe and controllable environments. They also pro-\nvide diverse challenges, from strategic planning to real-\ntime decision-making, serving as benchmarks for testing\nand advancing AI algorithms.\nSeveral generative models have been developed for\ngameplay. Adversarial Flow Networks (AFlowNets) [169]\nextend EFlowNets to adversarial environments, specifically\nfor two-player zero-sum games. GATO [178] is a versatile\ngeneralist agent capable of playing multiple games. Simi-\nlarly, the Multi-Game Decision Transformer [179] adapts a\nsingle transformer-based model to handle diverse gaming\nscenarios.\n5.4\nAutonomous Driving\nGenerative models have been used in autonomous driving\nfor a variety of tasks, including driving control [130], [228],\n[229], image and video processing [229], object detection\n[230] and scene understanding [231].\nHuang et al. [232] introduced a hybrid framework that\nintegrates neural decision-making into the classical modu-\nlar pipeline using end-to-end imitation learning via repa-\nrameterized generative adversarial learning. This approach\nretains the advantages of the classical pipeline, such as\nstrict adherence to physical and logical constraints, while\nenabling the framework to learn complex driving decisions\ndirectly from data.\nAnother application of generative model in autonomous\ndriving is in the generation of synthetic data for training\ndeep learning models [233]. Synthetic data helps augment\nreal-world datasets, which are often constrained in size and\ndiversity. These models can produce synthetic images and\nvideos resembling real-world scenarios, thereby enhancing\nthe performance of deep learning models in autonomous\ndriving tasks. For example, TrafficGen [234] acts as an au-\ntoregressive generative model based on an encoder-decoder\nstructure. It is trained on fragmented human driving data\ngathered from real-world environments to create lifelike\ntraffic scenarios. By employing an attention mechanism,\nTrafficGen encodes the current traffic context. subsequently,\nit decodes the initial state of the vehicle and generates a\nlong-term trajectory.\nGenerative models are valuable for processing sensor\ndata, such as LiDAR and camera inputs, in applications\nlike object detection and tracking [235]. These models can\ngenerate 3D representations of the environment from sensor\ndata, which serve as a foundation for more effective object\ndetection and tracking.\nGenerative model can also be used for scene understand-\ning in autonomous driving [95]. For example, it can be used\nto generate semantic maps of the environment, which can be\nused for navigation and path planning. Generative model\ncan also be used to generate predictions of future vehicle\nand pedestrian movements, which can be used for collision\navoidance and planning.\nPlanCP [236] leverages Conformal Prediction (CP) to\nquantify the uncertainty in diffusion dynamics models. By\nutilizing a finite set of exchangeable expert trajectory ex-\namples, PlanCP enhances robustness across diverse train-\n\n18\nFig. 20: The bar chart illustrates the continuous growth of generative models in decision-making across various applications,\nas indicated in the legend. The methods highlighted in the figure represent the seminal works of each period.\ning scenarios, ensuring more reliable performance in au-\ntonomous driving applications.\nOverall, generative models hold significant potential\nfor enhancing the performance and safety of autonomous\ndriving systems [237]. They are versatile tools applicable to\na range of tasks, including data augmentation and scene\nunderstanding, and have shown promising results in both\nresearch studies and practical applications.\n5.5\nOptimization\nWe consider three optimization settings: black-box opti-\nmization, neural architecture search, and scheduling. These\nsettings represent critical challenges in decision-making\ntasks, where generative models have demonstrated signif-\nicant potential.\nIn the black-box optimization setting, an agent learns\nto optimize an unknown function through pointwise eval-\nuations [238]. For example, Transformer Neural Process\n[184] reframes meta learning with uncertainty awareness\nin the context of sequence modeling, solving it with an\nautoregressive likelihood-based objective, utilizing a novel\ntransformer-based architecture.\nGenerative models such as GFlowNets have also been\napplied to combinatorial optimization. Zhang et al. [239]\napplied Markov decision processes (MDPs) to various\ncombinatorial problems, training conditional GFlowNets to\nsample from solution spaces. DIFUSCO [240] introduced\na graph-based diffusion framework to solve NP-complete\nproblems by casting them as binary vector optimization\ntasks and leveraging graph-based denoising diffusion mod-\nels to create superior methods.\nNeural Combinatorial Optimization (NCO) approaches\nfocus on leveraging neural networks to learn feasible solu-\ntion distributions for combinatorial optimization problems\n[241], [242]. Autoregressive construction solvers [146], [243],\n[244], [245], [246] are built upon the success of transformer-\nbased [108] architectures in sequential generation tasks.\nNon-autoregressive construction solvers [247], [248], [249],\n[250], [251], [252] have also been proposed to learn high\nquality solution distributions.\nFurthermore, prompt tuning and reinforcement learning\n(RL)-based prompt optimization methods are practical ap-\nplications of black-box optimization. Prompt tuning [253],\n[254], [255], [256] optimizes task-specific embeddings to\nguide model behavior efficiently, while RL-based methods\nlike RLPROMPT [225] and TEMPERA [226] refine prompt\ndesign to improve performance in tasks like classification\nand few-shot learning. These approaches demonstrate how\ngenerative models can navigate vast solution spaces effec-\ntively, balancing exploration and exploitation.\nIn the neural architecture search (NAS) setting. NAS\nfocuses on optimizing neural network designs to maximize\ntask performance. GFlowOut [257] leverages GFlowNets to\nlearn the posterior distribution of dropout masks, providing\nan innovative method for optimizing network architecture.\nBy utilizing generative models, GFlowOut highlights the\npotential for improving architecture efficiency while main-\ntaining high performance.\nIn the scheduling setting, optimization involves design-\ning strategies to allocate resources or order tasks efficiently.\nGFlowNets have been deployed in compiler scheduling\n[258], where they control the balance between the diver-\nsity and quality of the proposed methods. This application\nunderscores the ability of generative models to manage\ncompeting objectives in complex, high-stakes environments,\nsuch as industrial and computational settings.\nThese three optimization settings illustrate the versatility\nof generative models in tackling diverse and complex chal-\nlenges, offering promising avenues for future exploration\nand development.\n6\nPERSPECTIVE: NEXT GENERATION GENERA-\nTIVE DECISION MAKING MODELS\nDespite numerous successful examples, advancing next-\ngeneration generative decision-making models remains a\n\n19\ncritical challenge. In this section, we highlight three pivotal\nperspectives to enhance the capabilities of generative mod-\nels: high-performance algorithms, large-scale generalized\ndecision-making Models, and self-evolving and adaptive\ngenerative models.\n6.1\nHigh performance Algorithm\nThe growing integration of generative models in decision-\nmaking\nhighlights\nan\nincreasing\ndemand\nfor\nhigh-\nperformance algorithms capable of addressing the com-\nplexities of real-world applications. Future research should\nfocus on enhancing their scalability, efficiency, and adapt-\nability to dynamic environments. Normalizing flows and\nGFlowNets represent two promising directions for ad-\nvancing generative decision-making algorithms, each with\nunique strengths and challenges.\nNormalizing flows have demonstrated potential in mod-\neling uncertainties and supporting principled decision-\nmaking in complex, high-dimensional environments. How-\never, several critical challenges remain. These include\ndesigning more effective objective functions tailored to\ndecision-making tasks, improving their efficiency in han-\ndling high-dimensional decision spaces, and reducing the\ncomputational costs of training. Future work should explore\nnovel architectures and optimization strategies to address\nthese limitations, enabling their broader application in dy-\nnamic, real-world scenarios.\nSimilarly, GFlowNets present a novel probabilistic ap-\nproach to modeling unnormalized probability distributions,\nmaking them particularly effective for generating structured\nor discrete objects such as graphs. Unlike VAEs and GANs,\nwhose goals are to approximate data distributions or gen-\nerate realistic samples, GFlowNets are specifically designed\nto sample x in proportion to a predefined reward function\nR(x). The objective ensures that the generated samples\napproximately follow the desired reward distribution.\nBy leveraging their strengths and addressing current\nlimitations, these frameworks have the potential to reshape\ngenerative decision-making, enabling more efficient, scal-\nable, and adaptable systems for complex, dynamic real-\nworld applications.\n6.2\nLarge-scale Generalized Decision Making Models\nAs artificial intelligence continues to advance towards\nlarge-scale models, developing generative large models for\ndecision-making reasoning becomes increasingly essential.\nThese models have the potential to handle complex rea-\nsoning tasks, integrate diverse knowledge, and offer robust\nsolutions in dynamic and uncertain environments.\nAdditionally, current generative models are often tai-\nlored to predefined tasks or scenarios, limiting their general-\nization capabilities. To address real-world challenges, there\nis a pressing need for large-scale, generalized generative\ndecision models capable of adapting to diverse and dynamic\nenvironments.\nFor instance, web-scale diffusion models represent a\nsignificant step toward achieving this generalization. These\nmodels leverage massive datasets and scalable architectures\nto learn intricate patterns across diverse domains, making\nthem suitable for generating solutions in complex decision-\nmaking tasks. Similarly, large language models (LLMs),\nsuch as GPT [20] or PaLM [259], showcase the ability to pro-\ncess diverse inputs and perform multi-step reasoning, high-\nlighting their potential as general-purpose decision-making\ntools. Moreover, vision-language-action (VLA) models push\nthis generalization further by integrating multimodal data,\nenabling them to interpret, reason, and act across diverse\nscenarios. These examples underline the transformative po-\ntential of generative models in building adaptive, large-scale\ndecision-making frameworks.\nBesides, generative models should be capable of produc-\ning diverse solutions, enabling them to navigate various\npossibilities and adapt to the complexities in real-world\ndecision-making scenarios. This diversity is particularly\nimportant in handling ambiguous or multi-objective tasks,\nwhere a single solution may not suffice. By generating\nvaried yet high-quality outputs, these models can better\naccount for uncertainties, provide alternative strategies, and\nsupport robust decision-making processes. Such capabilities\nare critical for applications in dynamic and multi-faceted\ndomains, ranging from autonomous systems to large-scale\noptimization tasks.\n6.3\nSelf-Evolving and Adaptive Generative Models\nThe third perspective emphasizes the importance of self-\nevolving capabilities in generative models to address dy-\nnamic and complex decision-making environments. While\ncurrent generative models are often built with static\nassumptions and trained on fixed datasets, real-world\ndecision-making scenarios require models that can adapt\ncontinuously, learn in real time, and transfer knowledge\nacross different domains.\nA key component of adaptive generative models is\ncontinual learning, which allows them to evolve as new\ndata becomes available. This ability allows them to update\ntheir knowledge without forgetting previously learned in-\nformation, a challenge known as catastrophic forgetting. By\nleveraging incremental training, memory replay, and regu-\nlarization techniques, these models can respond to evolving\ndata distributions and changing environments, ensuring\ntheir decision-making remains accurate and relevant. This\nadaptability is crucial for applications where data patterns\nshift frequently and models need to stay up-to-date without\nretraining from scratch.\nFurthermore, the integration of real-time feedback mech-\nanisms allows generative models to quickly adjust their\ndecision-making strategies based on immediate outcomes\nor user interactions. Real-time performance monitoring can\ntrigger dynamic updates, enabling the model to refine its\napproach as it receives feedback from the environment. This\nis essential for tasks that require quick reactions, such as\nautonomous navigation or adaptive recommendation sys-\ntems, where rapid changes in input or context need to be\naddressed effectively to maintain high performance.\nCross-domain adaptability is another vital aspect, as\nreal-world decision-making often spans multiple domains\nwith distinct challenges and data characteristics. Generative\nmodels that can transfer knowledge learned from one do-\nmain to another can enhance their versatility and expand\n\n20\ntheir applicability. Approaches like meta-learning and trans-\nfer learning enable models to recognize and apply shared\nstructures and features across different tasks, making them\ncapable of tackling new and unseen problems with minimal\nadditional training. This cross-domain capability ensures\nthat models are not limited to a narrow set of conditions\nbut are instead able to operate effectively in diverse and\nmulti-faceted environments.\nIncorporating adaptive features like continual learning,\nreal-time feedback, and cross-domain adaptability allows\ngenerative models to better align with real-world chal-\nlenges. This approach paves the way for more robust,\nversatile, and future-proof decision-making systems that\ncan thrive in complex and ever-changing environments,\nextending the potential of generative models far beyond\ntheir current scope and applications.\n7\nCONCLUSION\nThis survey systematically examines the applications of\ngenerative models in decision-making tasks. We begin by\nrevisiting the problem formulation and comparing tradi-\ntional and generative model approaches. We then present\na taxonomy classifying generative models into seven cate-\ngories based on their model families, functionalities, struc-\ntures, and advantages. Next, we explore their core principles\nand real-world applications. We also outline promising fu-\nture directions, emphasizing the desired characteristics of\nnext-generation decision-making models. Leveraging their\nability to model complex data distributions and generate\ninnovative strategies, we argue that generative models hold\ntransformative potential in decision-making. By synthesiz-\ning recent advancements and highlighting key research\ntrajectories, this survey aims to bridge gaps and lay the\ngroundwork for the continued evolution of generative mod-\nels in decision-making.\nREFERENCES\n[1]\nA. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford,\nM. Chen, and I. Sutskever, “Zero-shot text-to-image generation,”\nin International conference on machine learning.\nPmlr, 2021, pp.\n8821–8831.\n[2]\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhari-\nwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al.,\n“Language models are few-shot learners,” Advances in neural\ninformation processing systems, vol. 33, pp. 1877–1901, 2020.\n[3]\nJ. Betker, G. Goh, L. Jing, T. Brooks, J. Wang, L. Li, L. Ouyang,\nJ. Zhuang, J. Lee, Y. Guo et al., “Improving image genera-\ntion with better captions,” Computer Science. https://cdn. openai.\ncom/papers/dall-e-3. pdf, vol. 2, no. 3, p. 8, 2023.\n[4]\nR. OpenAI, “Gpt-4 technical report. arxiv 2303.08774,” View in\nArticle, vol. 2, no. 5, 2023.\n[5]\nR. Bellman, “A markovian decision process,” Journal of mathemat-\nics and mechanics, pp. 679–684, 1957.\n[6]\nV. Krishnamurthy, Partially observed Markov decision processes.\nCambridge university press, 2016.\n[7]\nM. J. Kochenderfer, T. A. Wheeler, and K. H. Wray, Algorithms for\ndecision making.\nMIT press, 2022.\n[8]\nR. Hooke and T. A. Jeeves, ““direct search”solution of numerical\nand statistical problems,” Journal of the ACM (JACM), vol. 8, no. 2,\npp. 212–229, 1961.\n[9]\nS. Kirkpatrick, C. D. Gelatt Jr, and M. P. Vecchi, “Optimization\nby simulated annealing,” science, vol. 220, no. 4598, pp. 671–680,\n1983.\n[10]\nF. Tao, “Data-driven decision-making based on noisy data\nsamples—studies in the machine learning applications,” Ph.D.\ndissertation, The University of Texas at San Antonio, 2021.\n[11]\nR. S. Sutton, A. G. Barto et al., Introduction to reinforcement learning.\nMIT press Cambridge, 1998, vol. 135.\n[12]\nD. Ernst, P. Geurts, and L. Wehenkel, “Tree-based batch mode\nreinforcement learning,” Journal of Machine Learning Research,\nvol. 6, 2005.\n[13]\nJ. Fu, A. Kumar, M. Soh, and S. Levine, “Diagnosing bottlenecks\nin deep q-learning algorithms,” in International Conference on\nMachine Learning.\nPMLR, 2019, pp. 2021–2030.\n[14]\nR. S. Sutton, D. McAllester, S. Singh, and Y. Mansour, “Policy\ngradient methods for reinforcement learning with function ap-\nproximation,” Advances in neural information processing systems,\nvol. 12, 1999.\n[15]\nT. Degris, P. M. Pilarski, and R. S. Sutton, “Model-free rein-\nforcement learning with continuous action in practice,” in 2012\nAmerican Control Conference (ACC).\nIEEE, 2012, pp. 2177–2182.\n[16]\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust\nregion policy optimization,” in International conference on machine\nlearning.\nPMLR, 2015, pp. 1889–1897.\n[17]\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,\n“Proximal\npolicy\noptimization\nalgorithms,”\narXiv\npreprint\narXiv:1707.06347, 2017.\n[18]\nJ. G. Kuba, R. Chen, M. Wen, Y. Wen, F. Sun, J. Wang, and Y. Yang,\n“Trust region policy optimisation in multi-agent reinforcement\nlearning,” arXiv preprint arXiv:2109.11251, 2021.\n[19]\nC. Yu, A. Velu, E. Vinitsky, J. Gao, Y. Wang, A. Bayen, and Y. Wu,\n“The surprising effectiveness of ppo in cooperative multi-agent\ngames,” Advances in Neural Information Processing Systems, vol. 35,\npp. 24 611–24 624, 2022.\n[20]\nA. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al.,\n“Improving language understanding by generative pre-training,”\n2018.\n[21]\nT. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,\nD. Silver, and D. Wierstra, “Continuous control with deep rein-\nforcement learning,” arXiv preprint arXiv:1509.02971, 2015.\n[22]\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic:\nOff-policy maximum entropy deep reinforcement learning with\na stochastic actor,” in International conference on machine learning.\nPMLR, 2018, pp. 1861–1870.\n[23]\nK. Chua, R. Calandra, R. McAllister, and S. Levine, “Deep rein-\nforcement learning in a handful of trials using probabilistic dy-\nnamics models,” Advances in neural information processing systems,\nvol. 31, 2018.\n[24]\nK. Asadi, D. Misra, and M. Littman, “Lipschitz continuity in\nmodel-based reinforcement learning,” in International Conference\non Machine Learning.\nPMLR, 2018, pp. 264–273.\n[25]\nT. Xu, Z. Li, and Y. Yu, “Error bounds of imitating policies and\nenvironments,” Advances in Neural Information Processing Systems,\nvol. 33, pp. 15 737–15 749, 2020.\n[26]\nW. Zhang, Z. Yang, J. Shen, M. Liu, Y. Huang, X. Zhang, R. Tang,\nand Z. Li, “Learning to build high-fidelity and robust envi-\nronment models,” in Machine Learning and Knowledge Discovery\nin Databases. Research Track: European Conference, ECML PKDD\n2021, Bilbao, Spain, September 13–17, 2021, Proceedings, Part I 21.\nSpringer, 2021, pp. 104–121.\n[27]\nC. Sutton, A. McCallum et al., “An introduction to conditional\nrandom fields,” Foundations and Trends® in Machine Learning,\nvol. 4, no. 4, pp. 267–373, 2012.\n[28]\nV. Feinberg, A. Wan, I. Stoica, M. I. Jordan, J. E. Gonzalez, and\nS. Levine, “Model-based value estimation for efficient model-free\nreinforcement learning,” arXiv preprint arXiv:1803.00101, 2018.\n[29]\nR. Gozalo-Brizuela and E. C. Garrido-Merch´an, “A survey of\ngenerative ai applications,” arXiv preprint arXiv:2306.02781, 2023.\n[30]\nR. Gozalo-Brizuela and E. C. Garrido-Merchan, “Chatgpt is not\nall you need. a state of the art review of large generative ai\nmodels,” arXiv preprint arXiv:2301.04655, 2023.\n[31]\nA. Jo, “The promise and peril of generative ai,” Nature, vol. 614,\nno. 1, pp. 214–216, 2023.\n[32]\nE. Brynjolfsson, D. Li, and L. R. Raymond, “Generative ai at\nwork,” National Bureau of Economic Research, Tech. Rep., 2023.\n[33]\nD. H. Ackley, G. E. Hinton, and T. J. Sejnowski, “A learning\nalgorithm for boltzmann machines,” Cognitive science, vol. 9,\nno. 1, pp. 147–169, 1985.\n[34]\nI. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-\nFarley, S. Ozair, A. Courville, and Y. Bengio, “Generative adver-\nsarial networks,” Communications of the ACM, vol. 63, no. 11, pp.\n139–144, 2020.\n\n21\n[35]\nJ.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-\nimage translation using cycle-consistent adversarial networks,”\nin Proceedings of the IEEE international conference on computer vision,\n2017, pp. 2223–2232.\n[36]\nJ. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli,\n“Deep unsupervised learning using nonequilibrium thermody-\nnamics,” in International Conference on Machine Learning.\nPMLR,\n2015, pp. 2256–2265.\n[37]\nA. Verine, “Quality and diversity in generative models through\nthe lens of f-divergences,” Ph.D. dissertation, Universit´e Paris\nsciences et lettres, 2024.\n[38]\nM. L. Li and S. Zhu, “Balancing optimality and diversity: Human-\ncentered decision making through generative curation,” arXiv\npreprint arXiv:2409.11535, 2024.\n[39]\nJ. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic\nmodels,” Advances in neural information processing systems, vol. 33,\npp. 6840–6851, 2020.\n[40]\nT. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford,\nand X. Chen, “Improved techniques for training gans,” Advances\nin neural information processing systems, vol. 29, 2016.\n[41]\nD. P. Kingma and P. Dhariwal, “Glow: Generative flow with\ninvertible 1x1 convolutions,” Advances in neural information pro-\ncessing systems, vol. 31, 2018.\n[42]\nA. A. Alemi, I. Fischer, J. V. Dillon, and K. Murphy, “Deep vari-\national information bottleneck,” arXiv preprint arXiv:1612.00410,\n2016.\n[43]\nD. P. Kingma and M. Welling, “Auto-encoding variational bayes,”\narXiv preprint arXiv:1312.6114, 2013.\n[44]\nA. Radford, L. Metz, and S. Chintala, “Unsupervised represen-\ntation learning with deep convolutional generative adversarial\nnetworks,” arXiv preprint arXiv:1511.06434, 2015.\n[45]\nA. Oussidi and A. Elhassouny, “Deep generative models: Sur-\nvey,” in 2018 International conference on intelligent systems and\ncomputer vision (ISCV).\nIEEE, 2018, pp. 1–8.\n[46]\nP. Dhariwal and A. Nichol, “Diffusion models beat gans on im-\nage synthesis,” Advances in neural information processing systems,\nvol. 34, pp. 8780–8794, 2021.\n[47]\nT. Salimans, A. Karpathy, X. Chen, and D. P. Kingma, “Pix-\nelcnn++:\nImproving\nthe\npixelcnn\nwith\ndiscretized\nlogistic\nmixture likelihood and other modifications,” arXiv preprint\narXiv:1701.05517, 2017.\n[48]\nL. Girin, S. Leglaive, X. Bie, J. Diard, T. Hueber, and X. Alameda-\nPineda, “Dynamical variational autoencoders: A comprehensive\nreview,” arXiv preprint arXiv:2008.12595, 2020.\n[49]\nC. Finn, P. Christiano, P. Abbeel, and S. Levine, “A connec-\ntion between generative adversarial networks, inverse rein-\nforcement learning, and energy-based models,” arXiv preprint\narXiv:1611.03852, 2016.\n[50]\nM. Abdollahzadeh, T. Malekzadeh, C. T. Teo, K. Chandrasegaran,\nG. Liu, and N.-M. Cheung, “A survey on generative modeling\nwith limited data, few shots, and zero shot,” arXiv preprint\narXiv:2307.14397, 2023.\n[51]\nY. Bengio, S. Lahlou, T. Deleu, E. J. Hu, M. Tiwari, and E. Bengio,\n“Gflownet foundations,” arXiv preprint arXiv:2111.09266, 2021.\n[52]\nE. Betzalel, C. Penso, A. Navon, and E. Fetaya, “A study on the\nevaluation of generative models,” arXiv preprint arXiv:2206.10935,\n2022.\n[53]\nD. Belanger, “Deep energy-based models for structured predic-\ntion,” 2017.\n[54]\nY. Xu, T. Zhao, C. Baker, Y. Zhao, and Y. N. Wu, “Learning\ntrajectory prediction with continuous inverse optimal control\nvia langevin sampling of energy-based models,” arXiv preprint\narXiv:1904.05453, 2019.\n[55]\nJ. J. Hopfield, “Hopfield network,” Scholarpedia, vol. 2, no. 5, p.\n1977, 2007.\n[56]\nM. Suhail, A. Mittal, B. Siddiquie, C. Broaddus, J. Eledath,\nG. Medioni, and L. Sigal, “Energy-based learning for scene graph\ngeneration,” in Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, 2021, pp. 13 936–13 945.\n[57]\nR. B. Lyngsø and C. N. Pedersen, “Rna pseudoknot prediction in\nenergy-based models,” Journal of computational biology, vol. 7, no.\n3-4, pp. 409–427, 2000.\n[58]\nJ. Ngiam, Z. Chen, P. W. Koh, and A. Y. Ng, “Learning deep\nenergy models,” in Proceedings of the 28th international conference\non machine learning (ICML-11), 2011, pp. 1105–1112.\n[59]\nT. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean,\n“Distributed representations of words and phrases and their\ncompositionality,” Advances in neural information processing sys-\ntems, vol. 26, 2013.\n[60]\nG. Parisi, “Correlation functions and computer simulations,”\nNuclear Physics B, vol. 180, no. 3, pp. 378–384, 1981.\n[61]\nA. Hyv¨arinen and P. Dayan, “Estimation of non-normalized\nstatistical models by score matching.” Journal of Machine Learning\nResearch, vol. 6, no. 4, 2005.\n[62]\nM. Gutmann and A. Hyv¨arinen, “Noise-contrastive estimation:\nA new estimation principle for unnormalized statistical models,”\nin Proceedings of the thirteenth international conference on artificial\nintelligence and statistics.\nJMLR Workshop and Conference Pro-\nceedings, 2010, pp. 297–304.\n[63]\nJ. Ho and S. Ermon, “Generative adversarial imitation learning,”\nAdvances in neural information processing systems, vol. 29, 2016.\n[64]\nM. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein generative\nadversarial networks,” in International conference on machine learn-\ning.\nPMLR, 2017, pp. 214–223.\n[65]\nI. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C.\nCourville, “Improved training of wasserstein gans,” Advances in\nneural information processing systems, vol. 30, 2017.\n[66]\nT. Karras, T. Aila, S. Laine, and J. Lehtinen, “Progressive growing\nof gans for improved quality, stability, and variation,” arXiv\npreprint arXiv:1710.10196, 2017.\n[67]\nC. Doersch, “Tutorial on variational autoencoders,” arXiv preprint\narXiv:1606.05908, 2016.\n[68]\nT. D. Kulkarni, W. F. Whitney, P. Kohli, and J. Tenenbaum, “Deep\nconvolutional inverse graphics network,” Advances in neural in-\nformation processing systems, vol. 28, 2015.\n[69]\nG. Liu, F. A. Reda, K. J. Shih, T.-C. Wang, A. Tao, and B. Catan-\nzaro, “Image inpainting for irregular holes using partial convolu-\ntions,” in Proceedings of the European conference on computer vision\n(ECCV), 2018, pp. 85–100.\n[70]\nA. Deshpande, J. Lu, M.-C. Yeh, M. Jin Chong, and D. Forsyth,\n“Learning diverse image colorization,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2017, pp.\n6837–6845.\n[71]\nI. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick,\nS. Mohamed, and A. Lerchner, “beta-vae: Learning basic visual\nconcepts with a constrained variational framework,” in Interna-\ntional conference on learning representations, 2016.\n[72]\nJ. Hu, J. Whitman, and H. Choset, “Glso: Grammar-guided latent\nspace optimization for sample-efficient robot design automa-\ntion,” in Conference on Robot Learning.\nPMLR, 2023, pp. 1321–\n1331.\n[73]\nJ. An and S. Cho, “Variational autoencoder based anomaly detec-\ntion using reconstruction probability,” Special lecture on IE, vol. 2,\nno. 1, pp. 1–18, 2015.\n[74]\nK. Sohn, H. Lee, and X. Yan, “Learning structured output repre-\nsentation using deep conditional generative models,” Advances in\nneural information processing systems, vol. 28, 2015.\n[75]\nN. Dilokthanakul, P. A. Mediano, M. Garnelo, M. C. Lee,\nH. Salimbeni, K. Arulkumaran, and M. Shanahan, “Deep un-\nsupervised clustering with gaussian mixture variational autoen-\ncoders,” arXiv preprint arXiv:1611.02648, 2016.\n[76]\nL. Mescheder, S. Nowozin, and A. Geiger, “Adversarial varia-\ntional bayes: Unifying variational autoencoders and generative\nadversarial networks,” in International conference on machine learn-\ning.\nPMLR, 2017, pp. 2391–2400.\n[77]\nJ. Kim, J. Kong, and J. Son, “Conditional variational autoencoder\nwith adversarial learning for end-to-end text-to-speech,” in Inter-\nnational Conference on Machine Learning.\nPMLR, 2021, pp. 5530–\n5540.\n[78]\nG. Silvestri, D. Roos, and L. Ambrogioni, “Deterministic train-\ning of generative autoencoders using invertible layers,” in The\nEleventh International Conference on Learning Representations, 2022.\n[79]\nF. Ye and A. G. Bors, “Continual variational autoencoder learning\nvia online cooperative memorization,” in European Conference on\nComputer Vision.\nSpringer, 2022, pp. 531–549.\n[80]\nZ. Li, X. Jiang, R. Missel, P. K. Gyawali, N. Kumar, and L. Wang,\n“Continual unsupervised disentangling of self-organizing repre-\nsentations,” in The Eleventh International Conference on Learning\nRepresentations, 2022.\n[81]\nJ. Bae, M. R. Zhang, M. Ruan, E. Wang, S. Hasegawa, J. Ba, and\nR. Grosse, “Multi-rate vae: Train once, get the full rate-distortion\ncurve,” arXiv preprint arXiv:2212.03905, 2022.\n\n22\n[82]\nD. Rezende and S. Mohamed, “Variational inference with nor-\nmalizing flows,” in International conference on machine learning.\nPMLR, 2015, pp. 1530–1538.\n[83]\nA. Oord, Y. Li, I. Babuschkin, K. Simonyan, O. Vinyals,\nK. Kavukcuoglu, G. Driessche, E. Lockhart, L. Cobo, F. Stimberg\net al., “Parallel wavenet: Fast high-fidelity speech synthesis,” in\nInternational conference on machine learning.\nPMLR, 2018, pp.\n3918–3926.\n[84]\nA. Van den Oord, N. Kalchbrenner, L. Espeholt, O. Vinyals,\nA. Graves et al., “Conditional image generation with pixelcnn de-\ncoders,” Advances in neural information processing systems, vol. 29,\n2016.\n[85]\nS. Chen and R. Gopinath, “Gaussianization,” in Advances in\nNeural Information Processing Systems, T. Leen, T. Dietterich,\nand\nV.\nTresp,\nEds.,\nvol.\n13.\nMIT\nPress,\n2000.\n[Online].\nAvailable:\nhttps://proceedings.neurips.cc/paper/\n2000/file/3c947bc2f7ff007b86a9428b74654de5-Paper.pdf\n[86]\nE. G. Tabak and E. Vanden-Eijnden, “Density estimation by dual\nascent of the log-likelihood,” Communications in Mathematical\nSciences, vol. 8, no. 1, pp. 217–233, 2010.\n[87]\nE. G. Tabak and C. V. Turner, “A family of nonparametric den-\nsity estimation algorithms,” Communications on Pure and Applied\nMathematics, vol. 66, no. 2, pp. 145–164, 2013.\n[88]\nI. Kobyzev, S. J. Prince, and M. A. Brubaker, “Normalizing flows:\nAn introduction and review of current methods,” IEEE transac-\ntions on pattern analysis and machine intelligence, vol. 43, no. 11, pp.\n3964–3979, 2020.\n[89]\nL. Dinh, D. Krueger, and Y. Bengio, “Nice: Non-linear inde-\npendent components estimation,” arXiv preprint arXiv:1410.8516,\n2014.\n[90]\nL. Dinh, J. Sohl-Dickstein, and S. Bengio, “Density estimation\nusing real nvp,” arXiv preprint arXiv:1605.08803, 2016.\n[91]\nD. P. Kingma, T. Salimans, R. Jozefowicz, X. Chen, I. Sutskever,\nand M. Welling, “Improved variational inference with inverse\nautoregressive flow,” Advances in neural information processing\nsystems, vol. 29, 2016.\n[92]\nG. Papamakarios, T. Pavlakou, and I. Murray, “Masked autore-\ngressive flow for density estimation,” Advances in neural informa-\ntion processing systems, vol. 30, 2017.\n[93]\nC.-W. Huang, D. Krueger, A. Lacoste, and A. Courville, “Neu-\nral autoregressive flows,” in International Conference on Machine\nLearning.\nPMLR, 2018, pp. 2078–2087.\n[94]\nY. Lu, A. Zhong, Q. Li, and B. Dong, “Beyond finite layer neural\nnetworks: Bridging deep architectures and numerical differen-\ntial equations,” in International Conference on Machine Learning.\nPMLR, 2018, pp. 3276–3285.\n[95]\nR. T. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud,\n“Neural ordinary differential equations,” Advances in neural infor-\nmation processing systems, vol. 31, 2018.\n[96]\nW. Grathwohl, R. T. Chen, J. Bettencourt, I. Sutskever, and D. Du-\nvenaud, “Ffjord: Free-form continuous dynamics for scalable\nreversible generative models,” arXiv preprint arXiv:1810.01367,\n2018.\n[97]\nC. Finlay, J.-H. Jacobsen, L. Nurbekyan, and A. Oberman, “How\nto train your neural ode: the world of jacobian and kinetic regu-\nlarization,” in International conference on machine learning.\nPMLR,\n2020, pp. 3154–3164.\n[98]\nN. Rozen, A. Grover, M. Nickel, and Y. Lipman, “Moser flow:\nDivergence-based generative modeling on manifolds,” Advances\nin Neural Information Processing Systems, vol. 34, pp. 17 669–17 680,\n2021.\n[99]\nY. Song and S. Ermon, “Generative modeling by estimating\ngradients of the data distribution,” Advances in neural information\nprocessing systems, vol. 32, 2019.\n[100] L. Yang, Z. Zhang, Y. Song, S. Hong, R. Xu, Y. Zhao, W. Zhang,\nB. Cui, and M.-H. Yang, “Diffusion models: A comprehensive\nsurvey of methods and applications,” ACM Computing Surveys,\nvol. 56, no. 4, pp. 1–39, 2023.\n[101] Y. Song, P. Dhariwal, M. Chen, and I. Sutskever, “Consistency\nmodels,” 2023.\n[102] E. Bengio, M. Jain, M. Korablyov, D. Precup, and Y. Bengio,\n“Flow network based generative models for non-iterative diverse\ncandidate generation,” Advances in Neural Information Processing\nSystems, vol. 34, pp. 27 381–27 394, 2021.\n[103] Y. Li, S. Luo, H. Wang, and J. Hao, “Cflownets: Continu-\nous control with generative flow networks,” arXiv preprint\narXiv:2303.02430, 2023.\n[104] L. M. Brunswic, Y. Xu, Y. Feng, M. Lizhuang, and S. Jui, “Non-\nacyclic gflownets,” arXiv, 2023.\n[105] S. Lahlou, T. Deleu, P. Lemos, D. Zhang, A. Volokhova,\nA. Hern´andez-Garc´ıa, L. N. Ezzine, Y. Bengio, and N. Malkin, “A\ntheory of continuous generative flow networks,” arXiv preprint\narXiv:2301.12594, 2023.\n[106] N. Malkin, M. Jain, E. Bengio, C. Sun, and Y. Bengio, “Trajectory\nbalance: Improved credit assignment in gflownets,” arXiv preprint\narXiv:2201.13259, 2022.\n[107] L. Pan, N. Malkin, D. Zhang, and Y. Bengio, “Better training of\ngflownets with local credit and incomplete trajectories,” arXiv\npreprint arXiv:2302.01687, 2023.\n[108] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\nAdvances in neural information processing systems, vol. 30, 2017.\n[109] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever\net al., “Language models are unsupervised multitask learners,”\nOpenAI blog, vol. 1, no. 8, p. 9, 2019.\n[110] R. Child, S. Gray, A. Radford, and I. Sutskever, “Generat-\ning long sequences with sparse transformers,” arXiv preprint\narXiv:1904.10509, 2019.\n[111] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and\nQ. V. Le, “Xlnet: Generalized autoregressive pretraining for lan-\nguage understanding,” Advances in neural information processing\nsystems, vol. 32, 2019.\n[112] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding,” arXiv preprint arXiv:1810.04805, 2018.\n[113] A. Agarwal, S. Kakade, and L. F. Yang, “Model-based reinforce-\nment learning with a generative model is minimax optimal,” in\nConference on Learning Theory.\nPMLR, 2020, pp. 67–83.\n[114] A. Tirinzoni, R. Poiani, and M. Restelli, “Sequential transfer in\nreinforcement learning with a generative model,” in International\nConference on Machine Learning.\nPMLR, 2020, pp. 9481–9492.\n[115] G. Li, Y. Wei, Y. Chi, Y. Gu, and Y. Chen, “Breaking the sample size\nbarrier in model-based reinforcement learning with a generative\nmodel,” Advances in neural information processing systems, vol. 33,\npp. 12 861–12 872, 2020.\n[116] D. Wang, A. Sundaram, R. Kothari, A. Kapoor, and M. Roetteler,\n“Quantum algorithms for reinforcement learning with a gen-\nerative model,” in International Conference on Machine Learning.\nPMLR, 2021, pp. 10 916–10 926.\n[117] M. G. Azar, R. Munos, and B. Kappen, “On the sample complex-\nity of reinforcement learning with a generative model,” arXiv\npreprint arXiv:1206.6461, 2012.\n[118] Y. Du and I. Mordatch, “Implicit generation and modeling with\nenergy based models,” Advances in Neural Information Processing\nSystems, vol. 32, 2019.\n[119] M. Liu, T. He, M. Xu, and W. Zhang, “Energy-based imitation\nlearning,” in Proceedings of the 20th International Conference on\nAutonomous Agents and MultiAgent Systems, 2021, pp. 809–817.\n[120] T. Haarnoja, H. Tang, P. Abbeel, and S. Levine, “Reinforcement\nlearning with deep energy-based policies,” in International confer-\nence on machine learning.\nPMLR, 2017, pp. 1352–1361.\n[121] L. Kong, J. Cui, Y. Zhuang, R. Feng, B. A. Prakash, and C. Zhang,\n“End-to-end stochastic optimization with energy-based model,”\nAdvances in Neural Information Processing Systems, vol. 35, pp.\n11 341–11 354, 2022.\n[122] N. Tagasovska, N. C. Frey, A. Loukas, I. H¨otzel, J. Lafrance-\nVanasse, R. L. Kelly, Y. Wu, A. Rajpal, R. Bonneau, K. Cho\net al., “A pareto-optimal compositional energy-based model for\nsampling and optimization of protein sequences,” arXiv preprint\narXiv:2210.10838, 2022.\n[123] Y. Zhao, J. Xie, and P. Li, “Learning energy-based generative\nmodels via coarse-to-fine expanding and sampling,” in Interna-\ntional Conference on Learning Representations, 2020.\n[124] Y. Li, J. Song, and S. Ermon, “Infogail: Interpretable imitation\nlearning from visual demonstrations,” Advances in neural informa-\ntion processing systems, vol. 30, 2017.\n[125] N. Baram, O. Anschel, I. Caspi, and S. Mannor, “End-to-end dif-\nferentiable adversarial imitation learning,” in International Con-\nference on Machine Learning.\nPMLR, 2017, pp. 390–399.\n[126] P. Geiger and C.-N. Straehle, “Fail-safe adversarial generative\nimitation learning,” Transactions on Machine Learning Research,\n2022.\n\n23\n[127] Y. Wang, C. Xu, B. Du, and H. Lee, “Learning to weight imperfect\ndemonstrations,” in International Conference on Machine Learning.\nPMLR, 2021, pp. 10 961–10 970.\n[128] Y.-H. Wu, N. Charoenphakdee, H. Bao, V. Tangkaratt, and\nM. Sugiyama, “Imitation learning from imperfect demonstra-\ntion,” in International Conference on Machine Learning.\nPMLR,\n2019, pp. 6818–6827.\n[129] J. Fu, K. Luo, and S. Levine, “Learning robust rewards with\nadverserial inverse reinforcement learning,” in International Con-\nference on Learning Representations, 2018.\n[130] P. Wang, D. Liu, J. Chen, H. Li, and C.-Y. Chan, “Decision mak-\ning for autonomous driving via augmented adversarial inverse\nreinforcement learning,” in 2021 IEEE International Conference on\nRobotics and Automation (ICRA).\nIEEE, 2021, pp. 1036–1042.\n[131] H. Xiao, M. Herman, J. Wagner, S. Ziesche, J. Etesami, and T. H.\nLinh, “Wasserstein adversarial imitation learning,” arXiv preprint\narXiv:1906.08113, 2019.\n[132] J. Sun, L. Yu, P. Dong, B. Lu, and B. Zhou, “Adversarial inverse re-\ninforcement learning with self-attention dynamics model,” IEEE\nRobotics and Automation Letters, vol. 6, no. 2, pp. 1880–1886, 2021.\n[133] V. Huang, T. Ley, M. Vlachou-Konchylaki, and W. Hu, “Enhanced\nexperience replay generation for efficient reinforcement learn-\ning,” arXiv preprint arXiv:1705.08245, 2017.\n[134] D. Cho, D. Shim, and H. J. Kim, “S2p: State-conditioned image\nsynthesis for data augmentation in offline reinforcement learn-\ning,” Advances in Neural Information Processing Systems, vol. 35,\npp. 11 534–11 546, 2022.\n[135] C. He, S. Huang, R. Cheng, K. C. Tan, and Y. Jin, “Evolutionary\nmultiobjective optimization driven by generative adversarial net-\nworks (gans),” IEEE transactions on cybernetics, vol. 51, no. 6, pp.\n3129–3142, 2020.\n[136] E.-A. Sim, S. Lee, J. Oh, and J. Lee, “Gans and dcgans for\ngeneration of topology optimization validation curve through\nclustering analysis,” Advances in Engineering Software, vol. 152,\np. 102957, 2021.\n[137] P. R. Kalehbasti, M. D. Lepech, and S. S. Pandher, “Augmenting\nhigh-dimensional nonlinear optimization with conditional gans,”\nin Proceedings of the Genetic and Evolutionary Computation Confer-\nence Companion, 2021, pp. 1879–1880.\n[138] A. Mandlekar, D. Xu, R. Mart´ın-Mart´ın, S. Savarese, and L. Fei-\nFei, “Learning to generalize across long-horizon tasks from hu-\nman demonstrations,” arXiv preprint arXiv:2003.06085, 2020.\n[139] O. Mees, L. Hermann, and W. Burgard, “What matters in lan-\nguage conditioned robotic imitation learning over unstructured\ndata,” IEEE Robotics and Automation Letters, vol. 7, no. 4, pp.\n11 205–11 212, 2022.\n[140] C. Lynch, M. Khansari, T. Xiao, V. Kumar, J. Tompson, S. Levine,\nand P. Sermanet, “Learning latent plans from play,” in Conference\non robot learning.\nPMLR, 2020, pp. 1113–1132.\n[141] E. Rosete-Beas, O. Mees, G. Kalweit, J. Boedecker, and W. Bur-\ngard, “Latent plans for task-agnostic offline reinforcement learn-\ning,” in Conference on Robot Learning. PMLR, 2023, pp. 1838–1849.\n[142] A. Ajay, A. Kumar, P. Agrawal, S. Levine, and O. Nachum, “Opal:\nOffline primitive discovery for accelerating offline reinforcement\nlearning,” in International Conference on Learning Representations,\n2020.\n[143] F. Liu, H. Liu, A. Grover, and P. Abbeel, “Masked autoencoding\nfor scalable and generalizable decision making,” Advances in\nNeural Information Processing Systems, vol. 35, pp. 12 608–12 618,\n2022.\n[144] J. Han and J. Kim, “Selective data augmentation for improving\nthe performance of offline reinforcement learning,” in 2022 22nd\nInternational Conference on Control, Automation and Systems (IC-\nCAS).\nIEEE, 2022, pp. 222–226.\n[145] W. Xing, J. Lee, C. Liu, and S. Zhu, “Bayesian optimization with\nhidden constraints via latent decision models,” arXiv preprint\narXiv:2310.18449, 2023.\n[146] A. Hottung, B. Bhandari, and K. Tierney, “Learning a latent\nsearch space for routing problems using variational autoen-\ncoders,” in International Conference on Learning Representations,\n2021.\n[147] P. N. Ward, A. Smofsky, and A. J. Bose, “Improving exploration\nin soft-actor-critic with normalizing flows policies,” arXiv preprint\narXiv:1906.02771, 2019.\n[148] D.\nAkimov,\nV.\nKurenkov,\nA.\nNikulin,\nD.\nTarasov,\nand\nS. Kolesnikov, “Let offline rl flow: Training conservative agents in\nthe latent space of normalizing flows,” in 3rd Offline RL Workshop:\nOffline RL as a”Launchpad”, 2022.\n[149] Q. Zheng, M. Le, N. Shaul, Y. Lipman, A. Grover, and R. T. Chen,\n“Guided flows for generative modeling and decision making,”\narXiv preprint arXiv:2311.13443, 2023.\n[150] M. Gabri´e, G. M. Rotskoff, and E. Vanden-Eijnden, “Adaptive\nmonte carlo augmented with normalizing flows,” Proceedings of\nthe National Academy of Sciences, vol. 119, no. 10, p. e2109420119,\n2022.\n[151] T. Pearce, T. Rashid, A. Kanervisto, D. Bignell, M. Sun,\nR. Georgescu, S. V. Macua, S. Z. Tan, I. Momennejad, K. Hofmann\net al., “Imitating human behaviour with diffusion models,” in The\nEleventh International Conference on Learning Representations, 2022.\n[152] M. Janner, Y. Du, J. Tenenbaum, and S. Levine, “Planning with\ndiffusion for flexible behavior synthesis,” in International Confer-\nence on Machine Learning.\nPMLR, 2022, pp. 9902–9915.\n[153] A. Ajay, Y. Du, A. Gupta, J. B. Tenenbaum, T. S. Jaakkola,\nand\nP.\nAgrawal,\n“Is\nconditional\ngenerative\nmodeling\nall\nyou need for decision making?” in The Eleventh International\nConference on Learning Representations, 2023. [Online]. Available:\nhttps://openreview.net/forum?id=sP1fo2K9DFG\n[154] S. Zhao and A. Grover, “Decision stacks: Flexible reinforcement\nlearning via modular generative models,” Advances in Neural\nInformation Processing Systems, vol. 36, 2024.\n[155] Z. Wang, J. J. Hunt, and M. Zhou, “Diffusion policies as an\nexpressive policy class for offline reinforcement learning,” in\nThe Eleventh International Conference on Learning Representations,\n2023. [Online]. Available: https://openreview.net/forum?id=\nAHvFDPi-FA\n[156] C. Chi, S. Feng, Y. Du, Z. Xu, E. Cousineau, B. Burchfiel, and\nS. Song, “Diffusion policy: Visuomotor policy learning via action\ndiffusion,” arXiv preprint arXiv:2303.04137, 2023.\n[157] H.\nChen,\nC.\nLu,\nC.\nYing,\nH.\nSu,\nand\nJ.\nZhu,\n“Offline\nreinforcement learning via high-fidelity generative behavior\nmodeling,” in The Eleventh International Conference on Learning\nRepresentations, 2023. [Online]. Available: https://openreview.\nnet/forum?id=42zs3qa2kpy\n[158] Y. Du, S. Yang, B. Dai, H. Dai, O. Nachum, J. Tenenbaum,\nD. Schuurmans, and P. Abbeel, “Learning universal policies via\ntext-guided video generation,” Advances in Neural Information\nProcessing Systems, vol. 36, 2024.\n[159] Z. Liang, Y. Mu, M. Ding, F. Ni, M. Tomizuka, and P. Luo, “Adapt-\ndiffuser: Diffusion models as adaptive self-evolving planners,”\nin International Conference on Machine Learning.\nPMLR, 2023, pp.\n20 725–20 745.\n[160] L. Yang, Z. Huang, F. Lei, Y. Zhong, Y. Yang, C. Fang, S. Wen,\nB. Zhou, and Z. Lin, “Policy representation via diffusion\nprobability model for reinforcement learning,” arXiv preprint\narXiv:2305.13122, 2023.\n[161] H. He, C. Bai, K. Xu, Z. Yang, W. Zhang, D. Wang, B. Zhao,\nand X. Li, “Diffusion model is an effective planner and data\nsynthesizer for multi-task reinforcement learning,” Advances in\nneural information processing systems, vol. 36, 2024.\n[162] Z. Chen, S. Kiami, A. Gupta, and V. Kumar, “Genaug: Retargeting\nbehaviors to unseen situations via generative augmentation,”\narXiv preprint arXiv:2302.06671, 2023.\n[163] C. Lu, P. Ball, Y. W. Teh, and J. Parker-Holder, “Synthetic expe-\nrience replay,” Advances in Neural Information Processing Systems,\nvol. 36, 2024.\n[164] S. Krishnamoorthy, S. M. Mashkaria, and A. Grover, “Diffusion\nmodels for black-box optimization,” in International Conference on\nMachine Learning.\nPMLR, 2023, pp. 17 842–17 857.\n[165] Z. Li, H. Yuan, K. Huang, C. Ni, Y. Ye, M. Chen, and M. Wang,\n“Diffusion model for data-driven black-box optimization,” arXiv\npreprint arXiv:2403.13219, 2024.\n[166] L. Kong, Y. Du, W. Mu, K. Neklyudov, V. De Bortol, H. Wang,\nD. Wu, A. Ferber, Y.-A. Ma, C. P. Gomes et al., “Diffusion\nmodels as constrained samplers for optimization with unknown\nconstraints,” arXiv preprint arXiv:2402.18012, 2024.\n[167] L. Pan, D. Zhang, M. Jain, L. Huang, and Y. Bengio, “Stochastic\ngenerative flow networks,” in Uncertainty in Artificial Intelligence.\nPMLR, 2023, pp. 1628–1638.\n[168] L. Pan, D. Zhang, A. Courville, L. Huang, and Y. Bengio, “Gen-\nerative augmented flow networks,” in The Eleventh International\nConference on Learning Representations, 2022.\n[169] M. Jiralerspong, B. Sun, D. Vucetic, T. Zhang, Y. Bengio, G. Gidel,\nand N. Malkin, “Expected flow networks in stochastic environ-\n\n24\nments and two-player zero-sum games,” in The Twelfth Interna-\ntional Conference on Learning Representations, 2023.\n[170] L. Brunswic, Y. Li, Y. Xu, Y. Feng, S. Jui, and L. Ma, “A theory\nof non-acyclic generative flow networks,” in Proceedings of the\nAAAI Conference on Artificial Intelligence, vol. 38, no. 10, 2024, pp.\n11 124–11 131.\n[171] D. Zhang, R. T. Chen, N. Malkin, and Y. Bengio, “Unifying gen-\nerative models with gflownets,” arXiv preprint arXiv:2209.02606,\n2022.\n[172] D. Zhang, N. Malkin, Z. Liu, A. Volokhova, A. Courville, and\nY. Bengio, “Generative flow networks for discrete probabilis-\ntic modeling,” in International Conference on Machine Learning.\nPMLR, 2022, pp. 26 412–26 428.\n[173] M. Jain, S. C. Raparthy, A. Hern´andez-Garcıa, J. Rector-Brooks,\nY. Bengio, S. Miret, and E. Bengio, “Multi-objective gflownets,”\nin International conference on machine learning.\nPMLR, 2023, pp.\n14 631–14 653.\n[174] M. Kim, S. Choi, H. Kim, J. Son, J. Park, and Y. Bengio, “Ant\ncolony sampling with gflownets for combinatorial optimization,”\narXiv preprint arXiv:2403.07041, 2024.\n[175] L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin,\nP. Abbeel, A. Srinivas, and I. Mordatch, “Decision transformer:\nReinforcement learning via sequence modeling,” Advances in\nneural information processing systems, vol. 34, pp. 15 084–15 097,\n2021.\n[176] M. Janner, Q. Li, and S. Levine, “Offline reinforcement learning\nas one big sequence modeling problem,” Advances in neural\ninformation processing systems, vol. 34, pp. 1273–1286, 2021.\n[177] Q. Zheng, A. Zhang, and A. Grover, “Online decision trans-\nformer,” in international conference on machine learning.\nPMLR,\n2022, pp. 27 042–27 059.\n[178] S. Reed, K. Zolna, E. Parisotto, S. G. Colmenarejo, A. Novikov,\nG. Barth-Maron, M. Gimenez, Y. Sulsky, J. Kay, J. T. Springenberg\net al., “A generalist agent,” arXiv preprint arXiv:2205.06175, 2022.\n[179] K.-H. Lee, O. Nachum, S. Yang, L. Lee, C. D. Freeman,\nS. Guadarrama, I. Fischer, W. Xu, E. Jang, H. Michalewski, and\nI. Mordatch, “Multi-game decision transformers,” in Advances\nin Neural Information Processing Systems, A. H. Oh, A. Agarwal,\nD. Belgrave, and K. Cho, Eds., 2022. [Online]. Available:\nhttps://openreview.net/forum?id=0gouO5saq6K\n[180] B. Zhu, M. Dang, and A. Grover, “Scaling pareto-efficient\ndecision making via offline multi-objective RL,” in The Eleventh\nInternational Conference on Learning Representations, 2023. [Online].\nAvailable: https://openreview.net/forum?id=Ki4ocDm364\n[181] K. Wang, H. Zhao, X. Luo, K. Ren, W. Zhang, and D. Li,\n“Bootstrapped transformer for offline reinforcement learning,”\nAdvances in Neural Information Processing Systems, vol. 35, pp.\n34 748–34 761, 2022.\n[182] H. M. So, L. Bose, P. Dudek, and G. Wetzstein, “Pixelrnn: in-pixel\nrecurrent neural networks for end-to-end-optimized perception\nwith neural sensors,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2024, pp. 25 233–25 244.\n[183] S. M. Mashkaria, S. Krishnamoorthy, and A. Grover, “Generative\npretraining for black-box optimization,” in International Confer-\nence on Machine Learning.\nPMLR, 2023, pp. 24 173–24 197.\n[184] T. Nguyen and A. Grover, “Transformer neural processes:\nUncertainty-aware meta learning via sequence modeling,” in\nInternational Conference on Machine Learning.\nPMLR, 2022, pp.\n16 569–16 594.\n[185] E. Hazan, S. Kakade, and K. Singh, “The nonstochastic control\nproblem,” in Algorithmic Learning Theory, 2020.\n[186] X. Chen and E. Hazan, “Black-box control for linear dynamical\nsystems,” in Conference on Learning Theory, 2021.\n[187] G. Xiang, S. Li, F. Shuang, F. Gao, and X. Yuan, “Sc-airl: Share-\ncritic in adversarial inverse reinforcement learning for long-\nhorizon task,” IEEE Robotics and Automation Letters, vol. 9, no. 4,\npp. 3179–3186, 2024.\n[188] G. Papamakarios, E. Nalisnick, D. J. Rezende, S. Mohamed,\nand B. Lakshminarayanan, “Normalizing flows for probabilistic\nmodeling and inference,” The Journal of Machine Learning Research,\nvol. 22, no. 1, pp. 2617–2680, 2021.\n[189] T. Pearce, T. Rashid, A. Kanervisto, D. Bignell, M. Sun,\nR. Georgescu, S. V. Macua, S. Z. Tan, I. Momennejad, K. Hofmann\net al., “Imitating human behaviour with diffusion models,” arXiv\npreprint arXiv:2301.10677, 2023.\n[190] C. Kim, J. Kim, Y. Seo, K. Lee, H. Lee, and J. Shin, “Dynamics-\naugmented decision transformer for offline dynamics generaliza-\ntion,” in 3rd Offline RL Workshop: Offline RL as a”Launchpad”, 2022.\n[191] M. Xu, Y. Shen, S. Zhang, Y. Lu, D. Zhao, J. Tenenbaum, and\nC. Gan, “Prompting decision transformer for few-shot policy\ngeneralization,” in international conference on machine learning.\nPMLR, 2022, pp. 24 631–24 645.\n[192] S. Wang, X. Chen, D. Jannach, and L. Yao, “Causal decision\ntransformer for recommender systems via offline reinforcement\nlearning,” in Proceedings of the 46th International ACM SIGIR\nConference on Research and Development in Information Retrieval,\n2023, pp. 1599–1608.\n[193] S. Emmons, B. Eysenbach, I. Kostrikov, and S. Levine, “Rvs: What\nis essential for offline rl via supervised learning?” in International\nConference on Learning Representations, 2021.\n[194] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn,\nK. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu et al., “Rt-\n1: Robotics transformer for real-world control at scale,” arXiv\npreprint arXiv:2212.06817, 2022.\n[195] S. Sinha, A. Mandlekar, and A. Garg, “S4rl: Surprisingly simple\nself-supervision for offline reinforcement learning in robotics,” in\nConference on Robot Learning.\nPMLR, 2022, pp. 907–917.\n[196] M. Laskin, K. Lee, A. Stooke, L. Pinto, P. Abbeel, and A. Srinivas,\n“Reinforcement learning with augmented data,” Advances in neu-\nral information processing systems, vol. 33, pp. 19 884–19 895, 2020.\n[197] T. Ashuach, M. I. Gabitto, R. V. Koodli, G.-A. Saldi, M. I. Jordan,\nand N. Yosef, “Multivi: deep generative model for the integration\nof multimodal data,” Nature Methods, pp. 1–10, 2023.\n[198] M. Thomas, A. Bender, and C. de Graaf, “Integrating structure-\nbased approaches in generative molecular design,” Current Opin-\nion in Structural Biology, vol. 79, p. 102559, 2023.\n[199] F. Kong, J. Li, B. Jiang, H. Wang, and H. Song, “Integrated gen-\nerative model for industrial anomaly detection via bidirectional\nlstm and attention mechanism,” IEEE Transactions on Industrial\nInformatics, vol. 19, no. 1, pp. 541–550, 2021.\n[200] K. Minoura, K. Abe, H. Nam, H. Nishikawa, and T. Shimamura,\n“A mixture-of-experts deep generative model for integrated anal-\nysis of single-cell multiomics data,” Cell reports methods, vol. 1,\nno. 5, 2021.\n[201] T. M¨uller, B. McWilliams, F. Rousselle, M. Gross, and J. Nov´ak,\n“Neural importance sampling,” ACM Transactions on Graphics\n(ToG), vol. 38, no. 5, pp. 1–19, 2019.\n[202] T. Yu, T. Xiao, A. Stone, J. Tompson, A. Brohan, S. Wang, J. Singh,\nC. Tan, J. Peralta, B. Ichter et al., “Scaling robot learning with se-\nmantically imagined experience,” arXiv preprint arXiv:2302.11550,\n2023.\n[203] D. Zhang, Y. Zhang, J. Gu, R. Zhang, J. Susskind, N. Jaitly,\nand S. Zhai, “Improving gflownets for text-to-image diffusion\nalignment,” arXiv preprint arXiv:2406.00633, 2024.\n[204] X. Qiao, Y. Yang, and H. Li, “Defending neural backdoors via\ngenerative distribution modeling,” Advances in neural information\nprocessing systems, vol. 32, 2019.\n[205] T. Lattimore, C. Szepesvari, and G. Weisz, “Learning with good\nfeature representations in bandits and in rl with a generative\nmodel,” in International Conference on Machine Learning.\nPMLR,\n2020, pp. 5662–5670.\n[206] G. E. Hinton, “Training products of experts by minimizing con-\ntrastive divergence,” Neural computation, vol. 14, no. 8, pp. 1771–\n1800, 2002.\n[207] D. Zhang, H. Dai, N. Malkin, A. C. Courville, Y. Bengio, and\nL. Pan, “Let the flows tell: Solving graph combinatorial prob-\nlems with gflownets,” Advances in Neural Information Processing\nSystems, vol. 36, 2024.\n[208] Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel,\n“Benchmarking deep reinforcement learning for continuous con-\ntrol,” in International conference on machine learning.\nPMLR, 2016,\npp. 1329–1338.\n[209] S. Levine, C. Finn, T. Darrell, and P. Abbeel, “End-to-end training\nof deep visuomotor policies,” The Journal of Machine Learning\nResearch, vol. 17, no. 1, pp. 1334–1373, 2016.\n[210] R. Bailo, M. Bongini, J. A. Carrillo, and D. Kalise, “Optimal con-\nsensus control of the cucker-smale model,” IFAC-PapersOnLine,\nvol. 51, no. 13, pp. 1–6, 2018.\n[211] A. Cartea, S. Jaimungal, and J. Ricci, “Algorithmic trading,\nstochastic control, and mutually exciting processes,” SIAM re-\nview, vol. 60, no. 3, pp. 673–703, 2018.\n\n25\n[212] H. Wang and X. Y. Zhou, “Continuous-time mean–variance port-\nfolio selection: A reinforcement learning framework,” Mathemat-\nical Finance, vol. 30, no. 4, pp. 1273–1308, 2020.\n[213] S. H¨ofer, K. Bekris, A. Handa, J. C. Gamboa, M. Mozifian,\nF. Golemo, C. Atkeson, D. Fox, K. Goldberg, J. Leonard et al.,\n“Sim2real in robotics and automation: Applications and chal-\nlenges,” IEEE transactions on automation science and engineering,\n2021.\n[214] X. Chen, J. Hu, C. Jin, L. Li, and L. Wang, “Understanding domain\nrandomization for sim-to-real transfer,” in International Conference\non Learning Representations, 2022.\n[215] J. Hu, H. Zhong, C. Jin, and L. Wang, “Provable sim-to-real\ntransfer in continuous domain with partial observations,” arXiv\npreprint arXiv:2210.15598, 2022.\n[216] W. Xu, S. Keshmiri, and G. Wang, “Adversarially approximated\nautoencoder for image generation and manipulation,” IEEE\nTransactions on Multimedia, vol. 21, no. 9, pp. 2387–2396, 2019.\n[217] D. Zhang, R. T. Chen, C.-H. Liu, A. Courville, and Y. Bengio,\n“Diffusion generative flow samplers: Improving learning signals\nthrough partial trajectory optimization,” in The Twelfth Interna-\ntional Conference on Learning Representations, 2023.\n[218] G. B. Margolis, G. Yang, K. Paigwar, T. Chen, and P. Agrawal,\n“Rapid locomotion via reinforcement learning,” The International\nJournal of Robotics Research, vol. 43, no. 4, pp. 572–587, 2024.\n[219] Y. Li, L. Zhang, and Z. Liu, “Multi-objective de novo drug design\nwith conditional graph generative model,” Journal of cheminfor-\nmatics, vol. 10, pp. 1–24, 2018.\n[220] A. Grover, A. Zweig, and S. Ermon, “Graphite: Iterative gener-\native modeling of graphs,” in International conference on machine\nlearning.\nPMLR, 2019, pp. 2434–2444.\n[221] S. Zhong and J. Ghosh, “Generative model-based document clus-\ntering: a comparative study,” Knowledge and Information Systems,\nvol. 8, pp. 374–384, 2005.\n[222] M. Jain, E. Bengio, A. Hernandez-Garcia, J. Rector-Brooks, B. F.\nDossou, C. A. Ekbote, J. Fu, T. Zhang, M. Kilgour, D. Zhang et al.,\n“Biological sequence design with gflownets,” in International\nConference on Machine Learning.\nPMLR, 2022, pp. 9786–9801.\n[223] M. Kim, T. Yun, E. Bengio, D. Zhang, Y. Bengio, S. Ahn, and\nJ. Park, “Local search gflownets,” in The Twelfth International\nConference on Learning Representations, 2023.\n[224] X. Xin, T. Pimentel, A. Karatzoglou, P. Ren, K. Christakopoulou,\nand Z. Ren, “Rethinking reinforcement learning for recommenda-\ntion: A prompt perspective,” in Proceedings of the 45th International\nACM SIGIR Conference on Research and Development in Information\nRetrieval, 2022, pp. 1347–1357.\n[225] M. Deng, J. Wang, C.-P. Hsieh, Y. Wang, H. Guo, T. Shu,\nM. Song, E. P. Xing, and Z. Hu, “Rlprompt: Optimizing dis-\ncrete text prompts with reinforcement learning,” arXiv preprint\narXiv:2205.12548, 2022.\n[226] T. Zhang, X. Wang, D. Zhou, D. Schuurmans, and J. E. Gonzalez,\n“Tempera: Test-time prompt editing via reinforcement learning,”\nin The Eleventh International Conference on Learning Representations,\n2023.\n[227] K. Shao, Z. Tang, Y. Zhu, N. Li, and D. Zhao, “A survey of\ndeep reinforcement learning in video games,” arXiv preprint\narXiv:1912.10944, 2019.\n[228] L. Chen, X. Hu, W. Tian, H. Wang, D. Cao, and F.-Y. Wang, “Paral-\nlel planning: A new motion planning framework for autonomous\ndriving,” IEEE/CAA Journal of Automatica Sinica, vol. 6, no. 1, pp.\n236–246, 2018.\n[229] W. Si, T. Wei, and C. Liu, “Agen: Adaptable generative predic-\ntion networks for autonomous driving,” in 2019 IEEE Intelligent\nVehicles Symposium (IV).\nIEEE, 2019, pp. 281–286.\n[230] A. Marathe, D. Ramanan, R. Walambe, and K. Kotecha, “Wedge:\nA multi-weather autonomous driving dataset built from gener-\native vision-language models,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2023, pp.\n3317–3326.\n[231] H. Arnelid, E. L. Zec, and N. Mohammadiha, “Recurrent condi-\ntional generative adversarial networks for autonomous driving\nsensor modelling,” in 2019 IEEE Intelligent transportation systems\nconference (ITSC).\nIEEE, 2019, pp. 1613–1618.\n[232] J. Huang, S. Xie, J. Sun, Q. Ma, C. Liu, D. Lin, and B. Zhou,\n“Learning a decision module by imitating driver’s control be-\nhaviors,” in Conference on Robot Learning.\nPMLR, 2021, pp. 1–10.\n[233] S. Ding, Y. Tian, F. Xu, Q. Li, and S. Zhong, “Trojan attack\non deep generative models in autonomous driving,” in Security\nand Privacy in Communication Networks: 15th EAI International\nConference, SecureComm 2019, Orlando, FL, USA, October 23-25,\n2019, Proceedings, Part I 15.\nSpringer, 2019, pp. 299–318.\n[234] L. Feng, Q. Li, Z. Peng, S. Tan, and B. Zhou, “Trafficgen: Learning\nto generate diverse and realistic traffic scenarios,” in 2023 IEEE\nInternational Conference on Robotics and Automation (ICRA).\nIEEE,\n2023, pp. 3567–3575.\n[235] F. Lateef, M. Kas, and Y. Ruichek, “Saliency heat-map as visual\nattention for autonomous driving using generative adversarial\nnetwork (gan),” IEEE Transactions on Intelligent Transportation\nSystems, vol. 23, no. 6, pp. 5360–5373, 2021.\n[236] J. Sun, Y. Jiang, J. Qiu, P. Nobel, M. J. Kochenderfer, and\nM. Schwager, “Conformal prediction for uncertainty-aware plan-\nning with diffusion dynamics model,” Advances in Neural Infor-\nmation Processing Systems, vol. 36, 2024.\n[237] A. Ghosh, B. Bhattacharya, and S. B. R. Chowdhury, “Sad-\ngan: Synthetic autonomous driving using generative adversarial\nnetworks,” arXiv preprint arXiv:1611.08788, 2016.\n[238] A. Grover, “Generative decision making under uncertainty,” in\nProceedings of the AAAI Conference on Artificial Intelligence, vol. 37,\nno. 13, 2023, pp. 15 440–15 440.\n[239] D. Zhang, H. Dai, N. Malkin, A. Courville, Y. Bengio, and L. Pan,\n“Let the flows tell: Solving graph combinatorial optimization\nproblems with gflownets,” arXiv preprint arXiv:2305.17010, 2023.\n[240] Z. Sun and Y. Yang, “Difusco: Graph-based diffusion solvers\nfor combinatorial optimization,” Advances in Neural Information\nProcessing Systems, vol. 36, 2024.\n[241] Y. Bengio, A. Lodi, and A. Prouvost, “Machine learning for\ncombinatorial optimization: a methodological tour d’horizon,”\nEuropean Journal of Operational Research, vol. 290, no. 2, pp. 405–\n421, 2021.\n[242] J. Zhang, C. Liu, X. Li, H.-L. Zhen, M. Yuan, Y. Li, and J. Yan,\n“A survey for solving mixed integer programming via machine\nlearning,” Neurocomputing, vol. 519, pp. 205–217, 2023.\n[243] E. Khalil, H. Dai, Y. Zhang, B. Dilkina, and L. Song, “Learning\ncombinatorial optimization algorithms over graphs,” Advances in\nneural information processing systems, vol. 30, 2017.\n[244] W. Kool, H. Van Hoof, and M. Welling, “Attention, learn to solve\nrouting problems!” arXiv preprint arXiv:1803.08475, 2018.\n[245] Y.-D. Kwon, J. Choo, B. Kim, I. Yoon, Y. Gwon, and S. Min,\n“Pomo: Policy optimization with multiple optima for reinforce-\nment learning,” Advances in Neural Information Processing Systems,\nvol. 33, pp. 21 188–21 198, 2020.\n[246] M. Kim, J. Park, and J. Park, “Sym-nco: Leveraging symmetricity\nfor neural combinatorial optimization,” Advances in Neural Infor-\nmation Processing Systems, vol. 35, pp. 1936–1949, 2022.\n[247] C. K. Joshi, T. Laurent, and X. Bresson, “An efficient graph convo-\nlutional network technique for the travelling salesman problem,”\narXiv preprint arXiv:1906.01227, 2019.\n[248] Z.-H. Fu, K.-B. Qiu, and H. Zha, “Generalize a small pre-trained\nmodel to arbitrarily large tsp instances,” in Proceedings of the\nAAAI conference on artificial intelligence, vol. 35, no. 8, 2021, pp.\n7474–7482.\n[249] R. Qiu, Z. Sun, and Y. Yang, “Dimes: A differentiable meta solver\nfor combinatorial optimization problems,” Advances in Neural\nInformation Processing Systems, vol. 35, pp. 25 531–25 546, 2022.\n[250] C. Wang, Z. Yu, S. McAleer, T. Yu, and Y. Yang, “Asp: Learn\na universal neural solver!” IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 2024.\n[251] Z. Sun and Y. Yang, “Difusco: Graph-based diffusion solvers\nfor combinatorial optimization,” Advances in Neural Information\nProcessing Systems, vol. 36, pp. 3706–3731, 2023.\n[252] S. Sanokowski, S. Hochreiter, and S. Lehner, “A diffusion model\nframework for unsupervised neural combinatorial optimization,”\narXiv preprint arXiv:2406.01661, 2024.\n[253] B.\nLester,\nR.\nAl-Rfou,\nand\nN.\nConstant,\n“The\npower\nof\nscale for parameter-efficient prompt tuning,” arXiv preprint\narXiv:2104.08691, 2021.\n[254] N. Ding, S. Hu, W. Zhao, Y. Chen, Z. Liu, H.-T. Zheng, and\nM. Sun, “Openprompt: An open-source framework for prompt-\nlearning,” arXiv preprint arXiv:2111.01998, 2021.\n[255] X. Liu, K. Ji, Y. Fu, W. Tam, Z. Du, Z. Yang, and J. Tang, “P-\ntuning: Prompt tuning can be comparable to fine-tuning across\nscales and tasks,” in Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 2: Short Papers),\n2022, pp. 61–68.\n\n26\n[256] T. Vu, B. Lester, N. Constant, R. Al-Rfou, and D. Cer, “Spot: Better\nfrozen model adaptation through soft prompt transfer,” arXiv\npreprint arXiv:2110.07904, 2021.\n[257] D. Liu, M. Jain, B. F. Dossou, Q. Shen, S. Lahlou, A. Goyal,\nN. Malkin, C. C. Emezue, D. Zhang, N. Hassen et al., “Gflowout:\nDropout with generative flow networks,” in International Confer-\nence on Machine Learning.\nPMLR, 2023, pp. 21 715–21 729.\n[258] D. W. Zhang, C. Rainone, M. Peschl, and R. Bondesan, “Ro-\nbust scheduling with gflownets,” arXiv preprint arXiv:2302.05446,\n2023.\n[259] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra,\nA. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann\net al., “Palm: Scaling language modeling with pathways,” Journal\nof Machine Learning Research, vol. 24, no. 240, pp. 1–113, 2023.\n",
  "metadata": {
    "source_path": "papers/arxiv/Generative_Models_in_Decision_Making_A_Survey_0901d60ee2e9675c.pdf",
    "content_hash": "0901d60ee2e9675cdbb8459d457280e1540baa3ab37fe1d3325a4141ae8f88dc",
    "arxiv_id": null,
    "title": "Generative_Models_in_Decision_Making_A_Survey_0901d60ee2e9675c",
    "author": "",
    "creation_date": "D:20250225024441Z",
    "published": "2025-02-25T02:44:41",
    "pages": 26,
    "size": 9261592,
    "file_mtime": 1740470186.887236
  }
}