{
  "text": "Tidiness Score-Guided Monte Carlo Tree Search for\nVisual Tabletop Rearrangement\nHogun Kee, Wooseok Oh, Minjae Kang, Hyemin Ahn, and Songhwai Oh\nAbstract— In this paper, we present the tidiness score-\nguided Monte Carlo tree search (TSMCTS), a novel framework\ndesigned to address the tabletop tidying up problem using\nonly an RGB-D camera. We address two major problems for\ntabletop tidying up problem: (1) the lack of public datasets\nand benchmarks, and (2) the difficulty of specifying the goal\nconfiguration of unseen objects. We address the former by\npresenting the tabletop tidying up (TTU) dataset, a structured\ndataset collected in simulation. Using this dataset, we train a\nvision-based discriminator capable of predicting the tidiness\nscore. This discriminator can consistently evaluate the degree\nof tidiness across unseen configurations, including real-world\nscenes. Addressing the second problem, we employ Monte\nCarlo tree search (MCTS) to find tidying trajectories without\nspecifying explicit goals. Instead of providing specific goals, we\ndemonstrate that our MCTS-based planner can find diverse\ntidied configurations using the tidiness score as a guidance.\nConsequently, we propose TSMCTS, which integrates a tidiness\ndiscriminator with an MCTS-based tidying planner to find\noptimal tidied arrangements. TSMCTS has successfully demon-\nstrated its capability across various environments, including cof-\nfee tables, dining tables, office desks, and bathrooms. The TTU\ndataset is available at: https://github.com/rllab-snu/\nTTU-Dataset.\nI. INTRODUCTION\nIn this paper, we address the tabletop tidying problem,\nwhere an embodied AI agent autonomously organizes ob-\njects on a table based on their composition. As depicted\nin Figure 1, tidying up involves rearranging objects by\ndetermining an appropriate configuration of given objects,\nwithout providing an explicit target configuration. Previous\nresearch has encountered difficulties in defining the tidying\nup problem, primarily due to the lack of public datasets\nand metrics to assess tidiness. To address these issues, we\ncollect a structured dataset for tabletop tidying, and train\na tidiness discriminator and tidying planner to transform a\nmessy table into an organized one through a simple and\neffective framework. We refer to this as the tidiness score-\nguided Monte Carlo tree search (TSMCTS).\nIn previous research on object rearrangement, goal con-\nfigurations are provided either as target positions or as im-\nages of the desired arrangement [1]–[3]. This setup enables\nstraightforward evaluation by comparing the state to the\ngoal. However, using an image as a goal requires objects to\nH. Kee, W. Oh, M. Kang and S. Oh are with the Department\nof\nElectrical\nand\nComputer\nEngineering\nand\nASRI,\nSeoul\nNational\nUniversity,\nSeoul\n08826,\nKorea\n(e-mail: {hogun.kee,\nwooseok.oh, minjae.kang}@rllab.snu.ac.kr,\nsonghwai@snu.ac.kr).\nH. Ahn is with Artificial Intelligence Graduate School (AIGS), Ulsan\nNational Institute of Science and Technology (UNIST), Ulsan, Korea (e-\nmail: hyemin.ahn@unist.ac.kr).\nFig. 1.\nThe hierarchical policy of TSMCTS iteratively finds pick-and-\nplace actions to tidy up objects on a table. The high-level policy finds which\nobject to pick and place according to the current configuration. The low-\nlevel policy finds grasp points and trajectories of the end effector. Details\nof each policy are described in Section V.\nbe pre-arranged for goal generation, limiting flexibility. To\naccommodate a wider variety of goals, recent studies employ\nlanguage labels or descriptions to define more abstract goals\n[4]–[6]. Nonetheless, these language-conditioned rearrange-\nment studies require separate encoding modules to integrate\nlanguage with image or positional inputs. To connect these\ndomains, models like CLIP [7] and BLIP [8] map features\ninto a unified latent space. To connect the language domain\nto the image domain, CLIP [7] and BLIP [8] models focus on\nmapping features from different domains into a unified latent\nspace. While these approaches show promising performance\nin extracting semantic features [9], [10], they still struggle\nwith understanding the spatial relationships among objects.\nThe proposed method, TSMCTS, learns a tidiness score\nfunction and finds an action sequence that generates a tidy\nconfiguration based on object combinations without explicit\ngoals. We collect a tabletop tidying dataset from diverse\nenvironments (e.g., coffee tables, dining tables, office desks,\nbathrooms) and train a tidiness discriminator to measure the\ndegree of tidiness reliably, even with unseen objects and\nreal-world images. Our discriminator is superior to previous\napproaches since it can consistently measure tidiness from\nreal-world images, whereas previous attempts [11], [12]\nprimarily demonstrate success in simulations or toy examples\nrather than real-world scenarios. Finally, we use the tidying-\nup discriminator as a utility function of MCTS [13] to find a\nsequence of pick-and-place actions. The proposed MCTS-\nbased planner employs a tidying policy trained with our\ntabletop tidying-up dataset using an offline reinforcement\nlearning method, specifically Implicit Q-learning (IQL) [14],\nas its tree policy.\narXiv:2502.17235v1  [cs.RO]  24 Feb 2025\n\nThe proposed method, TSMCTS, achieves a tidying suc-\ncess rate of 88.5% in simulation experiments and 85% in real\nrobot experiments. These results experimentally demonstrate\nTSMCTS’s ability to tidy up various combinations of objects\nusing both simulations and real robots. We also perform\na human evaluation, which demonstrates that the proposed\nmethod can tidy up a table as much as humans can perceive\nit well-arranged.\nII. RELATED WORK\nTidying up is an object rearrangement problem occurring\nin situations where the goal is not explicitly provided. In-\nstead of a specific goal, several research approaches involve\nexpressing goals in natural language [4]–[6], or finding\nfunctional arrangements based on user preferences [15], [16].\nAdditionally, there are studies that directly learn the degree\nof tidiness as a score function and plan trajectories to achieve\na tidied scene [11], [12].\nRecent studies such as StructFormer [4] and StructDif-\nfusion [5] find appropriate positions for objects guided by\nnatural language instructions. Both methods take language\ntokens and object point clouds as inputs to find arrangements\nthat satisfy language conditions. Studies such as [15] and\n[16] learn user preferences to find organized arrangements\nwithout explicit goals. For instance, [15] uses scene graphs\nto encode scenes and learns user preference vectors, and [16]\naddresses tasks involving the organization of various items\ninto containers or shelves, learning pairwise preferences of\nobjects. These studies rely more on semantic information\nrather than visual information of the objects. There exist dif-\nfusion based methods to directly generate final arrangement\nimages [17], [18]. These studies rely on the commonsense\nknowledge inherent in large language models (LLMs) and\nvision language models (VLMs) to find arrangements that\nare similar to human intentions.\nSimilar to the current work, studies such as [11] and [12]\nlearn to quantify the degree of tidiness with a score function.\n[11] uses an energy-based model to learn and predict the cost,\nwhich is most relevant to our work. While [11] focused on\nfinding positions for just one missing object, our study plans\nto find the optimal state by moving all movable objects on a\ntable. [12] learns a score function to calculate the likelihood\nwith the target distribution for each task, using this score to\nlearn a policy for rearranging objects. Each task requires a\nseparate target distribution, and the score function is trained\nseparately for each task, whereas our study uses a single\nscore function to tidy up across various environments.\nLanguage-guided Monte-Carlo tree search (LGMCTS) [6]\nuses the MCTS algorithm to find trajectories to obtain\narrangements that satisfy language conditions. LGMCTS\nassumes that explicit spatial conditions can be derived based\non language conditions. They first establish these spatial\nconditions and then find a trajectory that arranges the objects\nto satisfy all these conditions. In this paper, we propose an\nalgorithm that learns a score function to find various tidied\narrangements without the guidance of language.\nIII. PROBLEM FORMULATION\nIn the tabletop tidying up problem, an agent (in our case,\na robot) M is tasked to rearrange a set of movable objects\nO = {o1, o2, . . . , oN} to achieve a tidy arrangement.\nAt each timestep, the agent receives a single top-down\nview RGB-D image from a fixed overhead camera. The\nworkspace is planar and all objects are assumed to be rigid\nbodies. The robot M can interact with objects through pick-\nand-place actions to perform arbitrary translation and rotation\nchanges.\nWe assume there is a tidiness-score function Ψ which\nreturns the degree of tidiness given an image of the tabletop\nwith objects. This function assesses whether objects on the\ntable are visually tidied up, considering the types, shapes, and\nsizes of the objects, and assigns a tidiness score between 0\nand 1, where 0 represents a completely messy scene and 1\nindicates a well-arranged scene.\nFor a given set of objects O, the visual observation\ndepends on the pose of the objects. Therefore, we can\nformulate the tidiness score as ψ = Ψ(O, P), where P\ndenotes the 6-DoF positions of O. Then we can formulate the\nobjective of the table tidying problem as finding the optimal\narrangement P ∗to maximize the tidiness score:\nP ∗= arg max\nP\nΨ(O, P).\n(1)\nIn this paper, we parameterize a tidiness-score function\nwith neural networks θ, as a discriminator Ψθ. Ψθ is trained\nto estimate the tidiness-score of a tabletop arrangement\nimage.\nIV. TABLETOP TIDYING UP DATASET\nWe collect a Tabletop Tidying Up (TTU) dataset which\nincludes both tidied and messy scenes to train a vision-\nbased tidiness discriminator. To cover diverse object arrange-\nments, we define a set of environments E consisting of four\nenvironments: Coffee table, Dining table, Office desk, and\nBathroom. For each environment e ∈E, we define a set\nof objects Oe ⊆Oall belonging to that environment, where\nOall is the entire set of objects. Then, we predefine possible\ncombinations of objects within Oe, each consisting of two\nto nine objects.\nIn this study, we introduce the concept of a template to en-\ncourage automatic collection of well-organized arrangement\ndata. A template is defined as a specific set of spatial relation-\nships between objects, categorized as one of the following:\non, under, left, right, front, behind, left-front, left-behind,\nright-front, and right-behind. Figure 2-a shows examples\nof templates and their corresponding tidied arrangements\nfor a set of objects O = {knife, fork, plate, cup}. An\narrangement where the fork is to the left of the plate and\nthe knife to the right could all be considered as belonging to\ntemplate A. Meanwhile, an arrangement where both the fork\nand knife are neatly placed on the left side of the plate would\nfall under template B. By defining templates in this manner,\nwe can represent all tidied arrangements that a person might\ncreate with specific templates.\n\nFig. 2.\n(a) Different arrangements can be created with the same combina-\ntion of objects. R represents ‘right’, L stands for ‘left’, B for ‘behind’, and F\ndenotes ‘front’ among the spatial relations. Various templates are collected\nto capture as many tidied arrangements as possible for each object set. (b)\nThe TTU dataset consists of state-action sequences for each environment,\nranging from a messy scene (t=1) to a perfectly tidied scene (t=T).\nTABLE I\nDATA COLLECTION ACROSS VARIOUS ENVIRONMENTS\nEnvironment\n# Objects\n# Templates\n# Trajectories\n# Data\nCoffee Table\n93\n120\n14,880\n74,400\nDining Table\n105\n125\n13,245\n66,225\nOffice Desk\n43\n131\n12,865\n64,325\nBathroom\n29\n37\n3,855\n19,275\nTotal\n170\n413\n44,845\n224,225\nWe first collect templates for the given object sets and then\nuse these templates to gather tidied scene data. We configure\nan appropriate combination of objects for each environment\nand design up to 16 templates for each combination of\nobjects. The entire process of finding templates is conducted\nmanually by spawning object models on a table in the\nPyBullet simulation. We use 3D object models from the YCB\ndataset [19] and the HouseCat6D dataset [20].\nTo collect tidying sequence data, we first created ti-\ndied scenes based on templates, then generated untidying\nsequences by scattering the tidied objects one by one.\nAfter sampling a template, we create a tidied scene by\naugmenting the distances between objects, changing objects\nwithin the same category, and modifying the central position\nof the arrangements. This tidied scene becomes the final\nstate sT of the trajectory, where st represents the scene at\ntimestep t, and T denotes the trajectory length. We start\nfrom sT and randomly pick objects to move to random\npositions on the desk, collecting the untidying sequence of\n(sT −1, sT −2, ..., s1). By reversing this sequence, we obtain a\ntidying sequence from a messy to a tidied table. Finally, we\ncollect a dataset D = {τ1, ..., τNtraj}, where each trajectory\nτi = ((s1, ψ1), ..., (sT , ψT )) consists of a sequence of state\nand tidiness score pairs. The tidiness scores are given as\nfollows, proportional to the timestep t, with the final state\nsT receiving a tidiness score of 1:\nψt = t −1\nT −1\n(2)\nIn Figure 2-b, we collect tidying sequences using a trajectory\nlength of T = 5. Table I lists the number of object models\nused in each environment, the number of templates, the num-\nber of trajectories, and the number of scene data. There are\noverlapping objects across the environments, and we utilize\na total of 170 object models. In total, we have collected 413\ntemplates and 224,225 scene data including RGB and depth\nimages, object categories and 6-DoF positions of objects.\nV. PROPOSED METHOD\nThe proposed framework, the Tidiness Score-guided\nMonte Carlo Tree Search (TSMCTS), consists of two com-\nponents: (1) training the tidiness discriminator and tidying\npolicy, and (2) planning the tidying up process using MCTS.\nWe trained a tidiness discriminator and tidying policy\nusing the TTU dataset described in the previous section. The\ntidiness discriminator learns a score function that evaluates\nthe tidiness score of the current state. The tidying policy is\nused as a tree policy in the MCTS algorithm to efficiently\nsample appropriate actions from the entire feasible action\nspace. We trained the tidiness discriminator in a supervised\nmanner and the tidying policy using the Implicit Q-Learning\n(IQL) framework.\nFinally, starting from the initial configuration (O, P), we\niteratively find pick-and-place actions by planning with the\nMCTS algorithm using the tidiness discriminator as a utility\nfunction and the tidying policy as a tree policy, until all the\nobjects on the table are tidied up.\nA. Tidiness Discriminator and Tidying Policy\nThe training process of the tidiness discriminator and the\ntidying policy is illustrated in Figure 3-a. We parameterized\nthe tidiness score function using neural networks Ψθ : S 7→\n[0, 1] and the tidying policy πρ : S 7→A, where S and\nA represent the state space and action space, respectively.\nIn this paper, the state is represented as an RGB image of\nthe table, while the action corresponds to a pick-and-place\noperation, defined by the target object, placement position,\nand rotation angle.\nFrom the TTU dataset, we can obtain sequences of state\nand tidiness score pairs ((s1, ψ1), ..., (sT , ψT )), where T\ndenotes the length of collected trajectories. Here, s1 is the\nmost messy scene and sT is the final tidied up scene. Finally,\nwe train the discriminator using pairs of states and score\nlabels, DDisc = {(s, ψ)i}, employing the mean squared error\nas the loss function:\nL(θ) = E\n\u0002\n(Ψθ(st) −ψt)2\u0003\n.\n(3)\n\nFig. 3.\n(a) We train the tidiness discriminator and tidying policy using the TTU dataset. The tidiness discriminator is trained in a supervised manner\nto predict the tidiness score of the table, while the tidying policy is trained to estimate the action distribution for pick-and-place actions using the IQL\nframework. (b) During inference, MCTS utilizes the tidiness discriminator Ψθ and the tidying policy πρ to find the best pick-and-place actions. (c) From the\ncurrent table image st, the policy networks take the table image I−oi and the object’s patch P(oi) as inputs to generate an action probability distribution.\nThe action is defined by the selected object, its placement position, and its rotation.\nFor policy training, we use the Implicit Q-Learning (IQL)\nmethod. We use a sparse binary reward as below:\nrt =\n(\n1,\nif the episode ends (t = T)\n0,\notherwise\n(4)\nand obtain offline data DRL = {(st, at, st+1, rt)} from TTU\ndataset. In IQL method, we learns Q-function Qϕ, value\nfunction Vφ and the policy πρ simultaneously. The loss\nfunctions for Vφ and Qϕ are computed according to the\nmodified TD learning procedure in IQL,\nLV (φ) = Es,a\n\u0002\nLτ\n2(Q ˆϕ(s, a) −Vφ(s))\n\u0003\n,\n(5)\nwhere Lτ\n2(u) = |τ −1(u < 0)|u2 represents the expectile\nregression loss, with τ = 0.7 used as the default value. Q ˆϕ\nis a target network, which is a lagged version of Qϕ.\nLQ(ϕ) = Es,a,s′,r\n\u0002\n(r + γVφ(s′) −Qϕ(s, a))2\u0003\n.\n(6)\nThen, the policy extraction step can be applied using advan-\ntage weighted regression:\nLπ(ρ) = Es,a\n\u0002\nexp(β(Q ˆϕ(s, a) −Vφ(s))) log πρ(a|s)\n\u0003\n,\n(7)\nwhere β denotes the inverse temperature.\nWe employ a pre-trained ResNet-18 as the backbone of\nthe tidiness discriminator, replacing its final fully connected\nlayer with one that predicts a single tidiness score. The\ntidiness discriminator takes the current table image st as\ninput and outputs the corresponding tidiness score Ψθ(st).\nWe use the Segment Anything Model (SAM) [21] to remove\nthe background to learn a more consistent score function.\nFor the tidying policy, two inputs are used for each\nobject oi, i = 1, . . . , N: (1) the patch image of the object,\nP(oi), and (2) the table image without the object, I−oi. The\npolicy outputs a probability distribution over pixel positions\nand rotations for placing the target object. As shown in\nFigure 3-c, the policy networks extract separate features for\nthe table F(I−oi) and the object F(P(oi)) using ResNet-\n18 networks. The 32-dimensional object feature F(P(oi))\nis then expanded to match the size of the table feature\nand concatenated with it to form the combined feature,\ncat(F(I−oi), F(P(oi))). This combined feature is processed\nthrough fully convolutional networks to generate an H×W ×\nR probability distribution, where R denotes the number of\npossible rotations. The tidying policy repeats this process for\nall N objects in parallel to produce an N ×H×W ×R action\nprobability distribution.\nB. Low-Level Planner\nTo discretize the pick-and-place action, we divide the\nworkspace into an H × W grid map and split the 360◦ro-\ntation into R bins. Then, we define a pick-and-place action\nas a = (o, p), where o denotes the target object and p =\n(x, y, r) represents the placement position. Here, x and y are\nthe selected pixel position, and r is the rotation index. When\npicking up objects, we use Contact-GraspNet [22], which\nprocesses 3D point clouds from RGB-D images to output 6-\nDoF grasping points for the robot arm’s end effector. When\nplacing objects, even if they are placed in the same location,\nthey can appear tidy or completely disordered depending on\nthe rotation of the objects. We observe that objects appear\nmore organized to humans when aligned with the table’s\nx-axis or y-axis. To achieve this, we fit an ellipse to the\nobject’s segmentation mask and use its major axis as the\ndefault rotation axis. This process is illustrated in Figure 4.\nFirst, the agent receives a top-down view RGB image of\nthe table and uses SAM [21] to obtain a segmentation mask\nfor each object. Next, we use the least squares method to find\nan ellipse that fits each object and determine its major and\nminor axes. The robot’s placement action allows the object\nto be aligned with the table’s horizontal or vertical axis.\nC. Tidiness Score-Guided High-Level Planner\nWe utilize Monte Carlo Tree Search (MCTS) as a high-\nlevel planner. MCTS is a search algorithm to solve decision-\nmaking processes for deterministic problems. For the im-\nplementation of MCTS, it is necessary to recognize the\ndynamics from a state st to the next state st+1 after an action\n\nFig. 4.\nGiven a high-level action specifying which object to pick and\nwhere to place, the low-level planner uses the Contact-GraspNet to find\na stable grasping point for the object. To place the object in the desired\norientation, the initial orientation is determined by applying ellipse fitting to\nthe object mask obtained through SAM, followed by calculating the rotation\ntransformation to determine the placement.\nat is performed. Instead of using a separate simulator to\nget the predicted next state ˆst+1, our method generates ˆst+1\nby directly moving each object’s image patch on the initial\nRGB image. Although ˆst+1 obtained through this method\nmay differ from the state achieved by physically performing\na pick-and-place action, we demonstrate that this approach\nachieves an 85% success rate in real world experiments,\nindicating its robustness despite potential errors. Figure 5\nshows the process of next state prediction and the sequence\nof expected and actual states during real world evaluation.\nOur high-level policy leverages the trained tidiness dis-\ncriminator and tidying policy to guide MCTS in finding the\nmost efficient action sequence for tabletop tidying up. The\noverall inference process of TSMCTS is shown in Figure\n3-b. For each timestep t, the agent receives the state which\nconsists of the current RGB-D images. Then TSMCTS builds\na search tree with the root node representing the current\nstate st. Starting from the initial tree, TSMCTS repeats the\nfollowing four steps K times to complete the tree: Selection,\nExpansion, Simulation, and Backpropagation.\nSelection: Starting from the root node, TSMCTS selects\nchild nodes until it reaches a leaf node. At each node s,\nTSMCTS selects an action a based on the UCT function,\ngiven as follows:\nU(s, a) =\nQ(s)\nN(s, a) + c\ns\n2 log N(s)\nN(s, a) ,\n(8)\nwhere c is the exploration term. N(s) denotes the number\nof visits to node s, and N(s, a) denotes the number of times\naction a has been executed at node s. Q(s) denotes the\ncumulative reward of node s, where the reward is assigned\nduring the Backpropagation step.\nExpansion: If TSMCTS reaches a leaf node sleaf, it adds\na child node to the tree. To expand the tree at the leaf\nnode, we use the trained tidying policy πρ to sample actions\nFig. 5.\nThe upper figure illustrates the process of next state prediction\nby directly moving object patches. The lower figure depicts a sequence of\nTSMCTS evaluations in the real world. The top row presents the predicted\nstates ˆst by moving image patches from the previous states. The bottom\nrow displays the observed states st. ψt denotes the tidiness score of each\nstate st.\nfrom the action space, a ∼πρ(·|sleaf). Here, the action a\nrepresents a pick-and-place action, a = (o, p). As mentioned\nabove, we create the new child node snew by moving the\nimage patch of object o to the position p in the RGB image\nof sleaf.\nSimulation: We leverage the trained tidiness discriminator\nto predict the expected value of the expanded node snew\nas V (snew) = Ψθ(snew), where Ψθ denotes the tidiness\ndiscriminator. Additionally, we obtain the outcome z(snew)\nfrom a random rollout by executing πρ until the terminal step\nTrollout. The outcome z(snew) is set to 1 if the final state\nof the rollout is fully tidied up and 0 otherwise.\nBackpropagation: TSMCTS backpropagates Q-value up-\ndates from the newly expanded node snew back to the root\nnode. For each node s and action a along the path, the Q-\nvalue updates are performed as follows:\nN(s) ←N(s) + 1,\nN(s, a) ←N(s, a) + 1,\nQ(s, a) ←Q(s, a) + (1 −λ)V (snew) + λz(snew).\n(9)\nWe use λ = 0.3, where λ denotes the mixing parameter.\nAfter the tree search is completed, TSMCTS selects the\nmost visited child node of the root node as the best action.\nThen, the high-level action is converted into low-level actions\nby the low-level planner to control the robot.\nVI. EXPERIMENTS\nA. Evaluation of Tidiness Discriminator\nWe evaluate whether the trained tidiness discriminator\ngeneralizes well to unseen objects and unseen configurations\nbeyond the training data. We divide the 224,225 tidying data\ninto 162,000 training data and 62,225 validation data. The\nvalidation data contains unseen objects and templates from\nthe training data. To determine whether a scene is fully tidied\nup, we define a tidiness threshold ξ, ranging from 0 to 1.\nDuring the experiments, a task is considered successful if\nthe tidiness score exceeds ξ. The tidiness threshold is crucial\nfor determining success - lowering ξ increases recall by\n\nFig. 6.\nThe left graph shows the recall and precision measured according\nto the tidiness threshold. The orange dashed line represents the tidiness\nthreshold used in the experiments, ξ = 0.85. The right graph shows the\ndistribution of human evaluated ratings according to the tidiness scores. We\ndivide the tidiness score range from 0 to 1 into 40 intervals and average the\nratings of scenes within each interval.\nclassifying more scenes as well-tidied but reduces precision\ndue to more false positives.\nTo determine an appropriate threshold, we analyze the\nclassification performance of the tidiness discriminator\nacross varying tidiness threshold values. The recall and\nprecision measured on the validation set as functions of the\ntidiness threshold are illustrated in Figure 6. Additionally,\nwe conduct a human evaluation to ensure that the tidiness\nthreshold aligns with human perceptions of tidiness. We\npresent 20 randomly selected sequences from 50 tidying\nsequences organized by TSMCTS to 17 participants, asking\nthem to choose the scenes they judge to be tidied up. For\neach sequence, we define the tidiness threshold as the lowest\ntidiness score among the scenes that participants judge to be\ntidied up. The average thresholds for each environment are\npresented in Table II. As a result, people judge that the table\nis tidied up at an average tidiness score of 0.8486. Looking at\nthe environment, the threshold for the Dining table is higher\nat 0.9017 compared to other environments. This appears to\nbe because people consider the arrangement more organized\nwhen the tableware and cutlery are placed according to their\nfunctional uses. Based on these results, we determine that a\nthreshold of 0.85 is an appropriate value. The trained tidiness\ndiscriminator achieve a recall of 71.8% and a precision of\n92.2% on the validation data using this threshold.\nWe conduct another human evaluation to verify how well\nthe trained tidiness score reflects the actual perception of\ntidiness by humans. We sample 10 scene data for each\ntidiness score interval from 0 to 1 at 0.1 intervals, resulting\nin a total of 100 scenes. Then, we ask 17 participants to view\n15 randomly selected scenes from the 100 scenes and rate\nthe degree of tidiness on a scale of 1 to 5. The correlation\nbetween the tidiness score and human ratings is shown in\nFigure 6. We observe a strong positive correlation between\nthe tidiness score and human ratings. We also find a tendency\nfor the variance in human ratings to increase as the tidiness\nscore rises. This is likely because the standards for tidiness\nare highly subjective and vary from person to person.\nB. Simulation Experiments\nWe use the PyBullet simulator for the simulation experi-\nments. In the simulator, a workspace table and a UR5 robot\nare set up. As the initial states, random objects are spawned\nTABLE II\nTIDINESS THRESHOLD MEASURED BY HUMAN EVALUATION\nEnvironment\nTidiness Threshold\nCoffee Table\n0.8223 ± 0.1230\nDining Table\n0.9017 ± 0.0895\nOffice Desk\n0.8310 ± 0.0892\nBathroom\n0.8632 ± 0.0799\nAverage\n0.8486 ± 0.0423\nTABLE III\nTSMCTS EVALUATION IN THE SIMULATION\nEnvironment\nSuccess Rate ↑\nTidiness Score ↑\nLength ↓\nCoffee Table\n79.3%\n0.889\n4.631\nDining Table\n92.7%\n0.904\n5.144\nOffice Desk\n90.7%\n0.905\n4.778\nBathroom\n89.3%\n0.902\n4.626\nMixed\n90.7%\n0.907\n4.620\nAverage\n88.5%\n0.901\n4.760\non the table in random positions and orientations. We use\n3D object models from the YCB and HouseCat6D datasets,\nalong with 10 additional object models and four extra object\ncategories not included in the training set of the TTU dataset.\nWe evaluate TSMCTS in simulation across five environ-\nments by adding, Mixed, a mixed table environment to the\noriginal four: Coffee table, Dining table, Office desk, and\nBathroom. For each environment, we tested 150 scenarios\nwith varying object compositions and initial placements.\nWe use the tidiness threshold 0.85 defined in the previous\nsection for the success criteria. A failure is noted if objects\nare placed outside the workspace, collide and overlap, or\nif tidying is not completed within 10 steps. We measured\nthe tidying success rate, the tidiness score of the final state,\nand the number of steps taken. The experimental results\nare presented in Table III. In the Coffee table environment,\nwhich has a diverse and complex set of objects, a success\nrate of 79.3% and an average tidiness score of 0.889 are\nachieved, while in the Dining Table environment, which has\nmore standardized object templates, a higher success rate\nis observed compared to other settings. Additionally, high\nsuccess rates and tidiness scores are also achieved in mixed\nobject configurations, which are not part of the training\ndata. TSMCTS demonstrates its ability to successfully find\narrangements that meet tidiness conditions across a variety\nof environments and object configurations.\nFor baseline comparisons, we evaluate TSMCTS compar-\ning StructFormer [4] and StructDiffusion [5]. Both Struct-\nFormer and StructDiffusion are algorithms that find ar-\nrangements matching given conditions, based on language\ntokens related to goals. While their setup is different from\nours, both studies include tasks for organizing a dining\ntable, so we conduct comparative experiments exclusively\nin the Dining table setting. Additionally, we perform an\nablation study on the tidiness discriminator by comparing\nTSMCTS with TSMCTS-binary. TSMCTS-binary utilizes a\ntidiness discriminator trained with binary labels from the\nTTU dataset, where completely tidied scenes are labeled as\n1, and all other scenes are labeled as 0.\n\nFig. 7.\nExamples of tidying up across various object sets. Starting from the initial configurations, TSMCTS successfully tidied up the tables. The\ndemonstration includes various objects such as apples, bananas, clocks, razors, and scotch tape, which are not included in the TTU dataset.\nWe conduct a human evaluation to determine how closely\nthe results tidied by each algorithm approximate human-\nperceived tidiness. We ask 17 participants to view 30 ran-\ndomly selected scenes and move objects as much as they\ndesire until they achieve a tidiness that satisfy them. For\nthis purpose, we collect 20 final tidied scenes from each\nalgorithm and include scenes during the tidying process,\npreparing a total of 160 scene data. We obtain a segmentation\nmask for each scene, which allow participants to move\nobject patches using keyboard controls. Movements are set\nto adjust 1 cm per step in any direction, and rotations\ncould be adjusted by 10 degrees clockwise or counterclock-\nwise. We measure the cumulative object movement distance,\ncumulative object rotation angle, and cumulative number\nof keyboard operations for each scene. Additionally, the\nparticipants are asked to fill out a NASA task load index\n(NASA-TLX) [23] form for each scene. NASA-TLX form\nis used to evaluate the workload of a task, which divides the\ntotal workload into six subjective subscales. In this paper,\nwe focus on three subscales that are most relevant to our\ntask: mental demand, own performance, and frustration level.\nThe results of human evaluation are presented in Table IV.\nTSMCTS shows the lowest total object movement distance at\n57.2cm and fewest keyboard operations at 102.9. However,\nthe NASA-TLX score is lowest for TSMCTS-Binary at\n26.47, while StructDiffusion and StructFormer achieve lower\ntotal rotation angles than TSMCTS. This suggests that point\ncloud-based algorithms perform better in finding the appro-\npriate orientation for each object. Low NASA-TLX scores\nfor TSMCTS and TSMCTS-Binary indicate that participants\nfeel less task load when tidying the table, implying less effort\nor stress is required to achieve a tidied arrangement that\nmeets human standards. The minimal movement distance and\nkeyboard operations for TSMCTS suggest it positions objects\nclosest to what people consider a tidy arrangement.\nC. Real Robot Experiments\nWe use a Universal Robots UR5 mounted with a Robotiq\n2F-85 Gripper at the end effector for real robot experiments.\nAn Intel RealSense D435 camera is mounted on the wrist\nof UR5 to capture RGB-D images in 480 × 640 resolution.\nAt each timestep, our agent receives RGB and depth images\nTABLE IV\nHUMAN EVALUATION IN SIMULATION EXPERIMENTS\nMethods\nDistance ↓Rotation ↓Number of NASA-TLX ↓\nOperations\nStructFormer [4]\n88.2cm\n150.3°\n143.1\n37.84\nStructDiffusion [5]\n65.3cm\n101.9°\n116.5\n41.86\nTSMCTS-Binary\n60.3cm\n201.0°\n104.0\n26.47\nTSMCTS\n57.2cm\n158.1°\n102.9\n27.06\nTABLE V\nTSMCTS EVALUATION IN THE REAL WORLD\nEnvironment\nSuccess Rate ↑\nTidiness Score ↑\nLength ↓\nCoffee Table\n100%\n0.894\n4.4\nDining Table\n80%\n0.840\n3.8\nOffice Desk\n80%\n0.945\n7.0\nBathroom\n80%\n0.909\n5.6\nAverage\n85%\n0.897\n5.1\nfrom the mounted camera at the fixed view point.\nWe evaluate TSMCTS across four environments: Coffee\ntable, Dining table, Office desk, and Bathroom. We conduct\ntidying scenarios with five different object configurations\nin each environment and measure the tidiness score of the\nfinal scene for each scenario. If the tidiness score exceeds\nthe tidiness threshold within 10 timesteps, the scenario is\nconsidered a success. However, if tidying is not achieved\nwithin 10 timesteps, if objects become entangled and a\ngrasping point cannot be found, or if an object leaves the\nworkspace, the scenario is considered a failure. The results of\nthe experiments are presented in Table V, and demonstrations\nof the final tidied up tables are shown in Figure 7. TSMCTS\nachieves an average tidiness score of 0.897 and a success rate\nof 85% across a total of 20 scenarios. In real experimental\nscenarios, objects not shown in the TTU dataset are also\nincluded in the configurations. These results demonstrate that\nTSMCTS can robustly tidy up even in scenarios with diverse\nand complex object compositions.\nFor baseline comparisons, we evaluate TSMCTS com-\nparing StructFormer and StructDiffusion. We conduct com-\nparative experiments solely in the Dining table setting,\nsimilar to the simulation experiment. We measure average\nscenario length and average number of collisions for the five\nscenarios. StructFormer and StructDiffusion are considered\nsuccessful if they arrange objects according to the intended\n\nFig. 8.\nExamples of tidying up using various methods in the real world.\nWe evaluated StructFormer, StructDiffusion, and TSMCTS on a Dining table\nsetup, starting from same initial configurations.\nTABLE VI\nREAL WORLD EVALUATION ON THE DINING TABLE ENVIRONMENT\nMethods\nSuccess Rate ↑\nLength ↓\nCollisions ↓\nStructFormer [4]\n40%\n3.6\n0.6\nStructDiffusion [5]\n60%\n3.0\n0.4\nTSMCTS\n80%\n3.4\n0.2\npositions without any collisions. For TSMCTS, the success\ncondition is the same as the previous experiment. The results\nof the experiments are presented in Table VI, and the final\ntidied up tables are shown in Figure 8. In a simple dining\ntable setup with one plate, fork, and knife, StructFormer\nand StructDiffusion demonstrate good tidying performance.\nHowever, in complex configurations with multiple forks or\nknives, StructFormer and StructDiffusion often fail to find\nappropriate positions for all objects, leading to overlaps\nand collisions. TSMCTS, on the other hand, is able to\nfind tidied arrangements even with complex configurations,\ndemonstrating its diverse and robust tidying capabilities.\nVII. CONCLUSION\nIn this paper, we have introduced the TSMCTS framework,\na tidiness score-guided Monte Carlo tree search for tabletop\ntidying up. TSMCTS is a framework that uses a tidiness\ndiscriminator to assess current and future table tidying states,\ngenerates a search tree according to the tidying policy, and\nfinds the optimal arrangement for tidying up. To train the\ntidiness discriminator and tidying policy, we have collected\nthe TTU dataset, a structured dataset that includes tidying\nsequence data across various environments. We have shown\nexperimental results that TSMCTS has robust tidying capa-\nbilities across various object configurations including unseen\nobjects and duplicate objects. In addition, we have success-\nfully transferred TSMCTS to the real world without any\ntransferring efforts. Despite the satisfactory results, there also\nexist limitations in the proposed method. Since TSMCTS\nassumes a 2D arrangement, it cannot perform tidying that\ninvolves stacking objects in layers. Additionally, the tidiness\ndiscriminator relies on visual information, which often leads\nto a lack of consideration for the functional uses of objects.\nIn future work, we plan to leverage large language models\n(LLMs) as guidance to better handle ambiguous cases and\nresolve scenarios where the functional use or arrangement of\nobjects is unclear.\nREFERENCES\n[1] E. Huang, Z. Jia, and M. T. Mason, “Large-scale multi-object rear-\nrangement,” in 2019 IEEE International Conference on Robotics and\nAutomation (ICRA).\nIEEE, 2019.\n[2] A. H. Qureshi, A. Mousavian, C. Paxton, M. C. Yip, and D. Fox,\n“Nerp: Neural rearrangement planning for unknown objects,” in\nRobotics: Science and Systems (RSS), 2021.\n[3] A. Goyal, A. Mousavian, C. Paxton, Y.-W. Chao, B. Okorn, J. Deng,\nand D. Fox, “Ifor: Iterative flow minimization for robotic object\nrearrangement,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2022, pp. 14 787–14 797.\n[4] W. Liu, C. Paxton, T. Hermans, and D. Fox, “Structformer: Learning\nspatial structure for language-guided semantic rearrangement of novel\nobjects,” in 2022 IEEE International Conference on Robotics and\nAutomation (ICRA).\nIEEE, 2022.\n[5] W. Liu, Y. Du, T. Hermans, S. Chernova, and C. Paxton, “Structdif-\nfusion: Language-guided creation of physically-valid structures using\nunseen objects,” in Robotics: Science and Systems (RSS), 2023.\n[6] H. Chang, K. Gao, K. Boyalakuntla, A. Lee, B. Huang, J. Yu,\nand A. Boularias, “Lgmcts: Language-guided monte-carlo tree search\nfor executable semantic object rearrangement,” in 2024 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS),\n2024.\n[7] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,\nG. Sastry, A. Askell, P. M. J. Clark, G. Krueger, and I. Sutskever,\n“Learning transferable visual models from natural language supervi-\nsion,” in International conference on machine learning. PMLR, 2021.\n[8] J. Li, D. Li, C. Xiong, and S. Hoi, “Blip: Bootstrapping language-\nimage pre-training for unified vision-language understanding and\ngeneration,” in International conference on machine learning. PMLR,\n2022.\n[9] M. Shridhar, L. Manuelli, and D. Fox, “Cliport: What and where\npathways for robotic manipulation,” in Conference on Robot Learning\n(CoRL), 2022.\n[10] T. Xiao, H. Chan, P. Sermanet, A. Wahid, A. Brohan, K. Haus-\nman, S. Levine, and J. Tompson, “Robotic skill acquisition via\ninstruction augmentation with vision-language models,” arXiv preprint\narXiv:2211.11736, 2022.\n[11] I. Kapelyukh and E. Johns, “Scenescore: Learning a cost function for\nobject arrangement,” arXiv preprint arXiv:2311.08530, 2023.\n[12] M. Wu, F. Zhong, Y. Xia, and H. Dong, “Targf: Learning target\ngradient field to rearrange objects without explicit goal specification,”\nAdvances in Neural Information Processing Systems, vol. 35, pp.\n31 986–31 999, 2022.\n[13] L. Kocsis and C. Szepesv´ari, “Bandit based monte-carlo planning,”\nin European conference on machine learning.\nSpringer, 2006, pp.\n282–293.\n[14] Kostrikov, Ilya, A. Nair, and S. Levine, “Offline reinforcement learning\nwith implicit q-learning,” arXiv preprint arXiv:2110.06169, 2021.\n[15] I. Kapelyukh and E. Johns, “My house, my rules: Learning tidying\npreferences with graph neural networks,” in Conference on Robot\nLearning (CoRL).\nPMLR, 2022.\n[16] N. Abdo, C. Stachniss, L. Spinello, and W. Burgard, “Robot, organize\nmy shelves! tidying up objects by predicting user preferences,” in 2015\nIEEE International Conference on Robotics and Automation (ICRA).\nIEEE, 2015.\n[17] I. Kapelyukh, V. Vosylius, and E. Johns, “Dall-e-bot: Introducing web-\nscale diffusion models to robotics,” IEEE Robotics and Automation\nLetters, vol. 8, no. 7, pp. 3956–3963, 2023.\n[18] Y. Zeng, M. Wu, L. Yang, J. Zhang, H. Ding, H. Cheng, and H. Dong,\n“Lvdiffusor: Distilling functional rearrangement priors from large\nmodels into diffusor,” IEEE Robotics and Automation Letters, vol. 9,\nno. 10, pp. 8258–8265, 2024.\n[19] B. Calli, A. Walsman, A. Singh, S. Srinivasa, P. Abbeel, and A. M.\nDollar, “Benchmarking in manipulation research: Using the yale-\ncmu-berkeley object and model set,” IEEE Robotics & Automation\nMagazine, vol. 22, no. 3, pp. 36–52, 2015.\n[20] H. Jung, S.-C. Wu, P. Ruhkamp, G. Zhai, H. Schieber, G. Rizzoli,\nP. Wang, H. Zhao, L. Garattoni, S. Meier, D. Roth, N. Navab, and\nB. Busam, “Housecat6d-a large-scale multi-modal category level 6d\nobject perception dataset with household objects in realistic scenarios,”\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2024, pp. 22 498–22 508.\n\n[21] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson,\nT. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, P. Doll´ar, and\nR. Girshick, “Segment anything,” in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, 2023, pp. 4015–4026.\n[22] M. Sundermeyer, A. Mousavian, R. Triebel, and D. Fox, “Contact-\ngraspnet: Efficient 6-dof grasp generation in cluttered scenes,” in 2021\nIEEE International Conference on Robotics and Automation (ICRA).\nIEEE, 2021.\n[23] L. Colligan, H. W. Potts, C. T. Finn, and R. A. Sinkin, “Cognitive\nworkload changes for nurses transitioning from a legacy system\nwith paper documentation to a commercial electronic health record,”\nInternational journal of medical informatics, vol. 84, no. 7, pp. 469–\n476, 2015.\n",
  "metadata": {
    "source_path": "papers/arxiv/Tidiness_Score-Guided_Monte_Carlo_Tree_Search_for_Visual_Tabletop\n__Rearrangement_d44bc281b315e270.pdf",
    "content_hash": "d44bc281b315e2701d16451d067eacf02481587291bfd8c82a3a487ee2fe9721",
    "arxiv_id": null,
    "title": "Tidiness_Score-Guided_Monte_Carlo_Tree_Search_for_Visual_Tabletop\n__Rearrangement_d44bc281b315e270",
    "author": "",
    "creation_date": "D:20250225025415Z",
    "published": "2025-02-25T02:54:15",
    "pages": 9,
    "size": 6540379,
    "file_mtime": 1740470167.1579206
  }
}