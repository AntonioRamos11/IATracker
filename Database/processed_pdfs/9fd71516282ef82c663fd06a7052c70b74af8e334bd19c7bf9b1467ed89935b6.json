{
  "text": "FADE: Why Bad Descriptions Happen to Good Features\nBruno Puri1,2,*, Aakriti Jain1,*, Elena Golimblevskaia1,*,\nPatrick Kahardipraja1, Thomas Wiegand1,2,3, Wojciech Samek1,2,3, Sebastian Lapuschkin1\n1Department of Artificial Intelligence, Fraunhofer Heinrich Hertz Institute, Berlin, Germany\n2Department of Electrical Engineering and Computer Science, Technische Universität Berlin, Berlin, Germany\n3BIFOLD - Berlin Institute for the Foundations of Learning and Data, Berlin, Germany\n*contributed equally\ncorresponding authors: {wojciech.samek,sebastian.lapuschkin}@hhi.fraunhofer.de\nAbstract\nRecent advances in mechanistic interpretabil-\nity have highlighted the potential of automat-\ning interpretability pipelines in analyzing the\nlatent representations within LLMs.\nWhile\nthey may enhance our understanding of in-\nternal mechanisms, the field lacks standard-\nized evaluation methods for assessing the va-\nlidity of discovered features. We attempt to\nbridge this gap by introducing\nFADE: Fea-\nture Alignment to Description Evaluation, a\nscalable model-agnostic framework for evaluat-\ning feature-description alignment. FADE evalu-\nates alignment across four key metrics — Clar-\nity, Responsiveness, Purity, and Faithfulness\n— and systematically quantifies the causes for\nthe misalignment of feature and their descrip-\ntion. We apply\nFADE to analyze existing\nopen-source feature descriptions, and assess\nkey components of automated interpretability\npipelines, aiming to enhance the quality of de-\nscriptions. Our findings highlight fundamental\nchallenges in generating feature descriptions,\nparticularly for SAEs as compared to MLP neu-\nrons, providing insights into the limitations and\nfuture directions of automated interpretability.\nWe release\nFADE as an open-source package\nat: https://github.com/brunibrun/FADE.\n1\nIntroduction\nUnderstanding the latent features of machine\nlearning models and aligning their descriptions\nwith human-comprehensible concepts remains a\ncrucial challenge in AI interpretability research.\nRecent advances have made significant strides\nin this direction,\nby introducing automated\ninterpretability methods (Bills et al., 2023; Bykov\net al., 2024; Choi et al., 2024), that leverage larger\nlanguage-capable models to describe the latent\nrepresentations of smaller models (Bykov et al.,\n2023; Templeton et al., 2024; Dreyer et al., 2025).\nThis facilitates inspection of ML models, enabling\na deeper understanding of models’ behaviour.\nConsequently, this enhances our ability to identify\nor mitigate harmful responses and biases, thus\nimproving model transparency and interpretability\n(Lee et al., 2024; Gandikota et al., 2024). A key\ninsight from these investigations is the highly\npolysemantic nature of individual neurons —\nthey rarely correspond to single, clear concepts.\nThis discovery has led to the development and\nadoption of sparse autoencoders (SAEs) (McGrath\net al., 2024; Bricken et al., 2023; Rajamanoharan\net al., 2024), which are intended to decompose\npolysemantic representations by separating neuron\nactivations into more interpretable components.\nWhile SAEs offer a promising approach for feature\ndecomposition, their reliability remains an open\nquestion.\nRecent research reveals significant\nvariability in the way SAEs capture the underlying\nlearned features (Heap et al., 2025; Paulo and Bel-\nrose, 2025), thus highlighting the need for a holistic\nframework for the evaluation of feature-description\nalignment. To the best of our knowledge, there is\nan absence of widely accepted quantitative metrics\nfor evaluating the quality and effectiveness of\nopen-vocabulary feature descriptions. Different\nmethodologies rely on custom evaluation criteria\nwhich makes it challenging to conduct meaningful,\ngeneralizable comparisons across techniques.\nAdditionally,\nexisting\nevaluation\napproaches\ntypically optimize for a single metric (Bills et al.,\n2023; Choi et al., 2024) which may not capture\nthe full complexity of a feature’s behavior and\nleaves open questions about whether the model\ntruly encodes the hypothesized concept rather than\nsimply correlating with the measured feature. With\nour work, we contribute as follows:\n[1] We present a robust automated evaluation\nframework\ndesigned\nfor\nbroad\napplicability\nacross\nmodel\narchitectures\nand\ntheir\nSAE\nimplementations.\nFADE combines four metrics\nthat allow quantitative analysis of different aspects\narXiv:2502.16994v1  [cs.LG]  24 Feb 2025\n\nFigure 1: Visualization of the\nFADE pipeline for three features and their corresponding feature descriptions.\nof alignment between features and their generated\ndescriptions.1\n[2] Through systematic empirical analysis, we\nprovide insights into how various components of\nthe autointerpretability pipeline — such as the\nnumber of layers, sample sizes, and architectural\nchoices — affect the quality of feature descriptions.\n[3] We release a selected subset of feature de-\nscriptions, presented as a part of this work for\nGemma-2-2b and Gemma Scope SAEs along with\ntheir evaluations.\n2\nRelated Work\nEvaluating the alignment between features and\ntheir descriptions has become increasingly impor-\ntant with the rise of automated interpretability ap-\nproaches. While manually inspecting highly ac-\ntivating examples remains a common method to\nvalidate interpretations and demonstrate automated\ninterpretability techniques (Bills et al., 2023; Tem-\npleton et al., 2024), more scalable tools are needed\nfor thorough quantitative evaluation. Many auto-\nmated or semi-automated approaches have been\nproposed, generally falling into activation-centric\nand output-centric methods.\nActivation-centric methods focus on measuring\nhow well a feature’s activations correspond to its\nassigned description.\nOne prominent approach is a simulation-based\nscoring, where an LLM predicts feature activations\nbased on the description and input data, and the\n1We release\nFADE as an open-source Python package\n(available at https://github.com/brunibrun/FADE) that\nincludes notebooks demonstrating example neurons featured\nin this work.\ncorrelation between predicted and real activations\nof a feature is measured (Bills et al., 2023; Bricken\net al., 2023; Choi et al., 2024). While elegant, this\napproach can be computationally expensive and\ntends to favor broad, high-level explanations.\nA related and conceptually more straightforward\nway to measure how well the description explains a\nfeature’s behavior is to try to directly generate syn-\nthetic samples using the description and compare\nthe resulting activations between concept and non-\nconcept samples (Huang et al., 2023; Kopf et al.,\n2024; Gur-Arieh et al., 2025; Shaham et al., 2025).\nHowever, generated datasets are typically small (on\nthe order of 5–20 samples (Huang et al., 2023; Gur-\nArieh et al., 2025)) and often constrained to rigid\nsyntactic structures or focus only on the occurence\nof particular tokens, making them less effective for\nevaluating abstract or open-ended language con-\ncepts (Huang et al., 2023; Foote et al., 2023).\nAnother strategy is rating individual samples\nfrom a natural dataset for how strongly they express\na concept and compare those ratings to the feature’s\nactivations (Huang et al., 2023; Paulo et al., 2024;\nTempleton et al., 2024).\nA common limitation of activation-centric meth-\nods is that they primarily evaluate positively corre-\nlated activations while ignoring negatively encoded\nneurons, effectively ignoring negatively encoded\nfeatures (Huang et al., 2023; Kopf et al., 2024).\nOutput-centric methods instead assess how fea-\nture activations influence model behavior. Some\napproaches measure the general decrease in perfor-\nmance of the model after ablating the feature (Bills\net al., 2023; Makelov et al., 2024), while others use\nsteering-based interventions, where an increase in\ngenerated outputs containing the concept is used as\na proxy for feature alignment (Paulo et al., 2024;\n\nGur-Arieh et al., 2025).\nThere is a growing need for frameworks that inte-\ngrate multiple perspectives to provide a comprehen-\nsive assessment of feature-to-description alignment\nacross different interpretability methods.\nFor instance, prior work (Bills et al., 2023;\nMenon et al., 2025; Gur-Arieh et al., 2025) has\nshown that while activation-centric and output-\ncentric measures often correlate, they do not neces-\nsarily imply a causal relationship.\nSome studies focus exclusively on SAEs (Mc-\nGrath et al., 2024; Paulo et al., 2024), while others\nanalyze MLP neurons (Bills et al., 2023; Choi et al.,\n2024). Developing an architecture-agnostic frame-\nwork for feature-to-description evaluation is essen-\ntial for enabling robust quantitative comparisons\nacross interpretability approaches.\nAlthough efforts have been made to integrate\nmultiple evaluation perspectives (Paulo et al., 2024;\nGur-Arieh et al., 2025), these remain fragmented\nand are often too narrowly scoped to handle open-\nended language descriptions.\nOur work addresses these limitations by intro-\nducing a more comprehensive evaluation frame-\nwork that combines activation- and output-centric\nmetrics while explicitly considering interpretability\nfor open-ended language descriptions.\n3\nEvaluating Feature Explanations\nOur primary objective is to establish a comprehen-\nsive framework that automatically evaluates feature\ndescriptions across a variety of feature types with-\nout human intervention. Our framework encom-\npasses four distinct metrics: Clarity, Responsive-\nness, Purity, and Faithfulness, which we consider\nnecessary and sufficient for assessing the align-\nment between a feature and its description. In our\nopinion, such a comprehensive evaluation frame-\nwork is necessary to ensure that features encode\nthe ascribed concept in a robust way. As feature\ndescriptions are often generated by optimizing for\na single metric, such as maximizing the activations\nof specific neurons, they do not necessarily gen-\neralize well to other quantifiable aspects, such as\nfaithfulness (Bills et al., 2023; Choi et al., 2024).\nWe base our approach on four key assumptions.\nFirst, we adopt a 1⃝Binary Concept Expression\nmodel, whereby a concept is either present in a\ntext sequence or absent. Second, we assume 2⃝\nConcept Sparsity, i.e. that a given concept appears\nonly rarely in natural datasets, though a sufficiently\nlarge dataset will contain some representative ex-\namples. Third, we assume 3⃝Feature Reactivity,\nmeaning, when a feature encodes a concept, its\nactivations are significantly stronger on samples\nthat express the concept. This will be valid espe-\ncially for SAEs, since by construction, for most\nsamples, their activations are zero. This is a strong\nassumption, as it also implies that a feature should\nactivate strongly only for a single concept. Note,\nhowever, that this does not require strict monose-\nmanticity (Bricken et al., 2023). In our framework\na feature might encode multiple, even entirely unre-\nlated topics, as long as its feature description fully\ndescribes all of them. Unlike traditional monose-\nmanticity, which assumes features should directly\nalign with a single human-interpretable category,\nour framework evaluates interpretability based on\nwhether the feature description accurately reflects\nthe feature’s truly encoded concept, rather than\nenforcing human-aligned conceptual boundaries.\nAssumption\n1⃝and\n3⃝allow us to interpret the\nactivations of a feature as output of a “classifier”\nof the encoded concept, which can then be easily\nevaluated. For our metrics, we expect a feature\nto encode the concept linearly in its activations.\nFinally, we assume 4⃝Causality - a feature is ex-\npected to causally influence the model’s output so\nthat modifying its activation will lead to predictable\nchanges in the generation of concept-related con-\ntent. These four assumptions will not always hold\nbut are necessary simplifications for now.\n3.1\nEvaluation Framework Components\nOur evaluation framework consists of three main\ncomponents: A subject LLM, that contains the fea-\ntures we want to evaluate, a natural dataset, that\nideally should be close to the LLM training data\ndistribution and is sufficiently large to contain all\nthe concepts, of which the descriptions we want\nto evaluate, and an evaluating LLM, an open- or\nclosed-source LLM that is used for automating the\nevaluation process. The evaluating LLM is used\nfor “human-like” inference tasks, such as rating\nthe strength of concept expression in samples and\ncreating synthetic concept data.\n3.2\nEvaluation Metrics\nClarity\nevaluates whether a feature’s description\nis precise enough to generate strongly activating\nsamples. We assess this by prompting the evalu-\nating LLM to generate synthetic samples based\non the feature description (see prompts in Ap-\n\nFigure 2:\nFADE can highlight different problems that arise with description generation. (a) Feature 128 contains\nconcept of the expression “under someone’s belt”. However, the derived concept “belt”, is not clear/ specific enough\nto be useful for generating synthetic data, that would activate the feature. However, the description is still very close\nto the concept, therefore Responsiveness and Purity are high. (b) Feature 1776 strongly reacts to the word “each”,\nbut also to many other general, unrelated words, resulting in lower Clarity and Purity. (c) The description for feature\n7657 catches its main concept, low Purity indicates that this feature is clearly polysemantic. (d) Description for\nfeature 10647 is expressed too broad, resulting in low Responsiveness and Purity.\npendix D.2.1). Unlike Gur-Arieh et al. (2025),\nwhich generates non-concept sequences artificially,\nwe sample them uniformly from the natural dataset\nto avoid unnatural biases (i.e. by asking the evalu-\nating LLM not to think about pink elephants). If a\nfeature is well explained by its description, the syn-\nthetic concept samples should elicit significantly\nstronger activations than non-concept samples. We\nquantify this separability using the absolute Gini\ncoefficient\nGabs(Ac, An) =\n\f\f\f\f\f\f\f\n2 ·\n\n\n\nP\nac∈Ac\nP\nan∈An\n1[ac>an]\n∥Ac∥0 · ∥An∥0\n\n\n−1\n\f\f\f\f\f\f\f\n(1)\nwhere Ac and An are the sets of concept and non-\nconcept activations, respectively. Since this metric\nfocuses on linear separability rather than precision,\nit remains robust even when concept samples oc-\ncasionally appear within the natural dataset. A\nlow clarity score indicates that either the descrip-\ntion is not precise enough to be useful, or might\nsimply be unfitting for the feature, resulting in sim-\nilar activations for both concept and non-concept\nsamples. For example, in Figure 2, feature (d) re-\nsponds to “having something under one’s belt,” yet\nis inaccurately described as “belt”. Conversely, a\nhigh clarity score confirms that we can effectively\ngenerate samples that elicit strong activations in\nthe feature, although it does not guarantee that the\nfeature is monosemantic or causally involved.\nResponsiveness\nevaluates the difference in acti-\nvations between concept and non-concept samples.\nWe select samples from the natural dataset based on\ntheir activation levels, drawing both from the high-\nest activations and from lower percentiles (details\nin Appendix D.1). Following an approach similar\nto Templeton et al. (2024), we prompt the evaluat-\ning LLM to rate each sample on a three-point scale\nto indicate how strongly the concept is present (0 =\nnot expressed, 1 = partially expressed, 2 = clearly\nexpressed). By discarding the ambiguous (partially\nexpressed) cases, we effectively binarize samples\ninto concept and non-concept categories. We com-\npute the responsiveness score again using the abso-\nlute Gini coefficient. A low responsiveness score\nindicates that activations of concept-samples are\nsimilarly strong as non-concept samples, while a\nhigh score indicates that, in natural data, samples\nwith strong activations reliably contain the concept.\nPurity\nis computed using the same set of rated\nnatural samples as responsiveness, but with a dif-\nferent focus: it evaluates whether the strong activa-\ntions are exclusive to the target concept. In contrast\nto (Huang et al., 2023), who measure recall and\nprecision for a single threshold, we measure the\npurity using the Average Precision (AP)\nAP(Ac, An) =\nX\nj\n(rj −rj−1) · pj\n(2)\nwhere rj is the recall and pj is the precision com-\nputed at threshold j, for each possible threshold,\nbased on Ac and An. The AP penalizes instances\nwhere non-concept samples also trigger high acti-\nvations. A purity score near one thus indicates that\nthe feature’s activations are highly specific to the\nconcept, whereas a score near zero suggests that\n\ntop activations occur for other unrelated concepts\nas well. This is, for example, the case in poly-\nsemanticity, where a feature responds to multiple\nunrelated concepts.\nFaithfulness\naddresses the causal relationship be-\ntween a feature and the model’s output. In other\nwords, it tests whether direct manipulation of the\nfeature’s activations can steer the model’s output to-\nward generating more concept-related content. To\nevaluate faithfulness, we take random samples from\nthe natural dataset and have the subject LLM gen-\nerate continuations while applying different mod-\nifications to the feature’s activation. For neurons,\nwe multiply the raw activations by a range of fac-\ntors, including negative values, so that we do not\nimpose a directional bias on how the concept is en-\ncoded. For SAE features, of which the activations\nare more sparse, we first determine the maximum\nactivation observed in the natural dataset (Temple-\nton et al., 2024) and then scale this value by the\ndifferent modification factors. After generating the\nmodified continuations, the evaluating LLM rates\nhow strongly the concept appears in each output.\nWe quantify the strength of this causal influence by\nmeasuring the largest increase we were able to steer\nthe model in producing concept-related outputs\nFaithfulness(R) = max(max(R) −R0, 0)\n1 −R0\n(3)\nwhere R is a vector capturing the proportion of\nconcept-related outputs for each modification fac-\ntor, and R0 denotes the base case in which the\nfeature is “zeroed out” (i.e., multiplied by zero). A\nfaithfulness score of zero implies that manipulat-\ning the feature does not increase the occurrence of\nconcept-related outputs, while a score of one indi-\ncates that for some modification factor the concept\nis produced in every continuation.\n4\nExperiments\nIn this section, we apply\nFADE to assess the\nquality of descriptions generated for various state-\nof-the-art feature descriptions (Choi et al., 2024;\nLieberum et al., 2024). Our goal is to demonstrate\nthat the proposed framework provides a robust,\nmultidimensional measure of feature to feature de-\nscription alignment.\nExperimental Setup\nAs a natural dataset for\nthe evaluations we use samples drawn from the\ntest partition of the Pile dataset (Gao et al.,\n2020), preprocessed as shown in Appendix B.\nAs evaluating LLM we use the OpenAI model\ngpt-4o-mini-2024-07-18 unless stated other-\nwise. Prompts for the evaluating LLM as well\nas details on the hyperparameters can be found\nin Appendix D.2.\nWe run the experiments on\n103 randomly chosen features from a single layer\nof a model:\nlayer 20 for Gemma-2-2b (Riv-\niere et al., 2024), layer 20 of Gemma\nScope\nSAEs (Lieberum et al., 2024), and layer 19 of\nLlama-3.1-8B-Instruct (Dubey et al., 2024)\n(see Appendix C.1 for details). The evaluation\nresults have a high variance, which is caused by\nboth the inherent difficulty of interpreting some\nfeatures as well as the quality of the ascribed fea-\nture descriptions. Therefore the mean values are\ndemonstrated only if they aid the analysis of met-\nrics distributions. For all of the presented tables we\ndemonstrate the full distributions as kernel-density\nestimations with bandwidth adjustment factor 0.3\nin Appendix E.1.\nAutomated interpretability approach\nFeature\ndescriptions, which we refer to as MaxAct*, are\ngenerated based on samples of the train partition of\nthe Pile dataset, that demonstrate maximum activa-\ntion on the feature, similarly to methods utilized in\n(Bills et al., 2023; Paulo et al., 2024; Rajamanoha-\nran et al., 2024), that we refer to as MaxAct. The\nminor differences between MaxAct and MaxAct*\nare prompts, optimized on qualitative analysis pro-\nvided via\nFADE, and preprocessing steps of the\ndataset. The automated interpretability pipeline is\ndescribed in Appendix C.1.\n4.1\nDepth and Reliability of Evaluations\nLimitations of single-metric approaches\nWe\ncompare\nFADE with simulated-activation-based\nmetrics (Bills et al., 2023; Templeton et al., 2024;\nChoi et al., 2024), that, while computationally effi-\ncient, fail to fully capture the feature-to-description\nalignment, potentially overlooking critical issues\nlike polysemanticity. To illustrate this, we analyze\nfeature descriptions of Llama-3.1-8B-Instruct\ngenerated in (Choi et al., 2024).\nAs shown in\nFigure 3, despite high simulated-activation scores,\nFADE identifies many features with low purity.\nMoreover, comparing the average results across all\nsubsampled features with the top 10% of features\nbased on the simulated-activation metric, we find\nonly a marginal gain in clarity and responsiveness,\nwhile the purity worsens. Via MaxAct*, we gen-\n\nFigure 3: Distribution of metrics in\nFADE framework and a simulation-based metric on a uniformly subsampled\nset of features and the top 10% of subsampled features, selected based on the simulated activation metric generated\nin (Choi et al., 2024) (right), and comparison to the proposed descriptions for these features, generated in this work\n(left).\nerate descriptions with slightly lower clarity but\nsignificantly higher responsiveness and purity. We\nattribute this to the explainer model in (Choi et al.,\n2024) being fine-tuned on descriptions optimized\nfor the simulated-activation metric, which aligns\nmore closely with clarity but neglects responsive-\nness and purity.\nBetter models provide better evaluations\nAs\nthe evaluating LLM is one of the most computa-\ntionally expensive components of our framework,\nselecting a model that balances performance and\ncost is critical. Larger models generally achieve\nbetter performance, but at significantly higher com-\nputational costs. To determine a minimal feasi-\nble model size and capability required for effec-\ntive evaluation, we conduct a quantitative analysis\nof concept-expression ratings across various open-\nweight and proprietary models, using GPT-4o as\na baseline due to its superior benchmark perfor-\nmance (OpenAI, 2024). We evaluate models on\nNeuronpedia (Lin, 2023) feature descriptions for\nthe Gemma Scope SAEs, generated via MaxAct\nmethod (Rajamanoharan et al., 2024). By compar-\ning deviations in concept strength ratings between\nGPT-4o and other models, we assess their relative\nperformance (see Table 1).\nOur findings reveal a clear trend: larger, more ca-\npable models consistently yield better evaluations.\nThe\nopen-weight\nLlama-3.3-70B-Instruct\n(AWQ 4-bit quantized) performs comparably\nto the proprietary GPT-4o mini, a widely used\nmodel in autointerpretability research (Choi\net al., 2024; Lin, 2023). While Class 1 (partial\nalignment) is the most error-prone, smaller models,\nsuch as Llama-3.2-3B-Instruct, remain viable\nfor the more critical Class 0 (no alignment)\nModel\nClass 0\nClass 1\nClass 2\nValid\nGPT-4o\n243,233\n24,766\n19,716\n100\nLlama-3.2-1B\n63.8\n22.1\n20.9\n8.8\nLlama-3.2-3B\n77.4\n9.9\n70.3\n72.2\nLlama-3.1-8B\n82.0\n14.6\n85.6\n82.8\nLlama-3.3-70B 4q\n88.7\n31.5\n92.6\n88.3\nGPT-4o mini\n93.4\n44.8\n79.3\n88.6\nTable 1: Concept rating procedure for different eval-\nuating LLMs. The GPT-4o baseline shows the num-\nber of occurrences per class. The other models show\ntheir alignment with the GPT-4o rating in percent. The\n“Valid” column shows the percentage of samples that\nwere correctly classified. Class 0 represents no align-\nment with the concept, class 1 a partial alignment and\nclass 2 means the samples clearly exhibits the concept.\nand Class 2 (strong alignment).\nHowever, for\noptimal\nperformance,\nmodels\nsmaller\nthan\nLlama-3.1-8B-Instruct are likely insufficient.\nGenerating feature descriptions for SAEs is\nmore challenging than for MLP neurons\nTo\ncompare MLP neurons and SAE features, we ana-\nlyze Gemma-2-2b and Gemma Scope SAEs. While\nGemma-2 outperforms Gemma Scope SAEs in av-\nerage clarity (see Table 4), the clarity score dis-\ntribution reveals a left-skewed peak for SAEs, as\ndepicted on Figure 4. Further analysis identifies a\ncluster of features with low clarity but moderate\nto high responsiveness and purity. These features\nhave descriptions that approximate the encoded\nconcept but lack the precision to strongly activate\nthe SAE feature (see (d) in Figure 2). This suggests\nthat despite greater monosemanticity, interpreting\nSAE features remains challenging due to the diffi-\nculty of generating precise descriptions. In contrast,\nresponsiveness and purity are higher on average for\nSAEs, as these metrics are less sensitive to impre-\ncise descriptions and still align with the underlying\n\nFigure 4: Feature description fit for neuron-based features (Gemma-2) and SAE-based features (Gemma Scope).\nSAE Size\nClarity\nRespon-\nsiveness\nPurity\nFaithfulness\nMaxAct* 16K\n0.57\n0.78\n0.69\n0.17\nMaxAct* 65K\n0.46\n0.71\n0.66\n0.15\nNeuronpedia 16K\n0.43\n0.67\n0.60\n0.17\nNeuronpedia 65K\n0.29\n0.64\n0.56\n0.13\nTable 2: Comparison results for SAEs of different sizes,\nsee metrics distributions on Figure 5.\nLayer\nClarity\nRespon-\nsiveness\nPurity\nFaithfulness\n3\n0.60\n0.71\n0.55\n0.009\n12\n0.44\n0.54\n0.43\n0.016\n20\n0.67\n0.74\n0.61\n0.011\n25\n0.59\n0.65\n0.54\n0.025\nTable 3:\nEvaluation results for different layers in\nGemma-2, see metrics distributions on Figure 6.\nconcept. The higher purity in SAEs aligns with\ntheir increased monosemanticity.\nInterpreting larger SAEs is more difficult\nWe\ninvestigate whether SAEs with a higher number\nof features inherently exhibit a better alignment\nwith the feature descriptions. To quantify this, we\ncompare feature descriptions from Gemma Scope\n16K and Gemma Scope 65K. We compare it based\non Neuronpedia feature descriptions, as well as the\nones obtained in this work via MaxAct*. Consis-\ntent with our previous finding, our results indicate\nthat increasing the number of SAE features does\nnot inherently improve the alignment of features\nwith their descriptions, as shown in Table 2. We\nhypothesize that this stems from a finer-grained de-\ncomposition of concepts, making it more challeng-\ning for the explainer LLM to capture and articulate\nthe precise concept.\nInterpretability varies across layers\nTable 3\npresents an evaluation of feature descriptions from\ndifferent layers of Gemma-2-2b. Our analysis iden-\ntifies layer 12 as the most challenging to interpret.\nA manual inspection of 50 randomly sampled fea-\ntures confirms these results: features in layer 12\nModel\nInput\ntype\nClarity\nRespon-\nsiveness\nPurity\nFaithfulness\nGemma-2 delimiters\n0.67\n0.74\n0.61\n0.01\nnumeric\n0.67\n0.76\n0.64\n0.01\nGemma\nScope\ndelimiters\n0.57\n0.78\n0.69\n0.16\nnumeric\n0.59\n0.80\n0.72\n0.17\nTable 4: Comparison results of activations input types\nvia delimiters vs numerical input, see metrics distribu-\ntions on Figure 7 (a).\nexhibit high polysemanticity. The highest scores\nare observed in layer 20, with the exception of the\nfaithfulness metric. However, this may be due to\nthe fine-tuning of the MaxAct* approach on this\nlayer, which introduces a bias that specifically af-\nfects faithfulness. The highest faithfulness score is\nobserved in layer 25, while the lowest is found in\nlayer 3.\n4.2\nEvaluating Autointerpretability: Prompts,\nExamples, and Model Size\nDespite the growing number of methods proposed\nin automated interpretability research, there has\nbeen surprisingly little comprehensive evaluation\nof different approaches. In this section, we present\na series of experiments that assess different com-\nponents of feature generation pipelines and demon-\nstrate how\nFADE can help in fine-tuning inter-\npretability pipelines.\nPrompting with numerical- or delimiter-based\ninput\nPrompt construction can significantly influ-\nence the quality of the generated descriptions. We\ninvestigate two primary approaches: passing (word,\nactivation) pairs and using {{delimiters}} to high-\nlight the most activated tokens (see Appendix C.2\nfor more details). Our experiments indicate that the\nnumerical input performs slightly better than the\ndelimiter-based prompt, which contradicts previous\nresearch (Choi et al., 2024).\n\nNumber of\nshots\nClarity\nRespon-\nsiveness\nPurity\nFaithfulness\n0-shot\n0.53\n0.76\n0.70\n0.17\n1-shot\n0.55\n0.76\n0.68\n0.19\n2-shot\n0.57\n0.78\n0.69\n0.17\n5-shot\n0.60\n0.79\n0.72\n0.16\n10-shot\n0.60\n0.79\n0.72\n0.16\n20-shot\n0.61\n0.81\n0.73\n0.16\nTable 5: Comparison results for different number of\nshots provided to the explainer model with the prompt\nbased on Gemma Scope, see metrics distributions on\nFigure 7 (c).\nModel\n#\nsam-\nples\nClarity\nRespon-\nsiveness\nPurity\nFaithfulness\nGemma2\n5\n0.63\n0.72\n0.59\n0.01\n15\n0.67\n0.74\n0.61\n0.01\n50\n0.69\n0.77\n0.62\n0.01\nGemma\nScope\n5\n0.56\n0.77\n0.67\n0.17\n15\n0.57\n0.78\n0.69\n0.18\n50\n0.60\n0.79\n0.71\n0.17\nTable 6: Comparison results for different number of\nactivating samples provided to the explainer model, see\nmetrics distributions on Figure 7 (b).\nFew-shot prompting improves description qual-\nity\nNext we test how many examples should be\npassed to the explainer model in the prompt. We\ncompare 0-shot (without examples), 1-shot, 2-shot,\n5-shot, 10-shot and 20-shot prompts on Gemma\nScope. In these variations, we use the delimiter-\nbased prompts. The results, provided in Table 5,\ndemonstrate, that a larger number of examples\nbrings steady improvement in clarity, responsive-\nness and purity. Faithfulness shows no clear trend.\nProviding more samples increases evaluation\nscores\nWe test 5, 15 and 50 samples, using the\nsame delimiter-based prompts (see Table 6). The\nresults indicate, that increasing the number of sam-\nples improves description quality, though the gains\nare not substantial for any of the tested number of\nsamples.\nBetter models produce better feature descrip-\ntions\nSimilarly to the experiment, comparing dif-\nferent evaluation models presented in Table 1, we\ncompare different explainer models and demon-\nstrate the results in Table 7. GPT-4o achieves the\nhighest scores, with Llama-3.3-70B-Instruct\n(AWQ 4-bit quantized) and GPT-4o mini as close\nalternatives. The smaller models struggle with\nassigning reasonable feature descriptions, in par-\nticularly Llama-3.2-1B, which frequently fails to\nmaintain a consistent response structure (see Ap-\npendix E.1).\nModel\nClarity\nRespon-\nsiveness\nPurity\nFaithfulness\nLlama-3.2-1B\n0.39\n0.56\n0.35\n0.10\nLlama-3.2-3B\n0.51\n0.73\n0.61\n0.15\nLlama-3.1-8B\n0.50\n0.75\n0.66\n0.17\nLlama-3.3-70B\n0.54\n0.78\n0.70\n0.19\nGPT-4o mini\n0.58\n0.78\n0.70\n0.17\nGPT-4o\n0.61\n0.80\n0.73\n0.17\nTable 7: Explainer models comparison, see metrics dis-\ntributions on Figure 8.\nApproach\nClarity\nRespon-\nsiveness\nPurity\nFaithfulness\nNeuronpedia\n0.43\n0.67\n0.60\n0.21\nTF-IDF\n0.42\n0.72\n0.53\n0.21\nUnembedding\n0.38\n0.65\n0.61\n0.29\nMaxAct*\n0.57\n0.78\n0.69\n0.21\nTable 8: Comparison of the quality of our feature de-\nscriptions to baselines, see metrics distributions on Fig-\nure 9.\nBaselines fail in predictable ways\nTo assess\nthe effectiveness of MaxAct* approach, we com-\npare it against established baseline methods, in-\ncluding the Neuronpedia feature descriptions, a\nTF-IDF (Ramos et al., 2003; Salton and Buck-\nley, 1988) based description approach, and an\nunembedding-based method (Joseph Bloom, 2024)\n(see Appendix C.1 for methodological details). The\nresults are presented in Table 8. MaxAct* consis-\ntently outperforms baselines in clarity, responsive-\nness, and purity. Notably, the unembedding method\nachieves the highest faithfulness score, a result that\naligns with our expectations and related work (Gur-\nArieh et al., 2025). Since this method explicitly\nconsiders the output that a given feature promotes,\nit naturally excels at capturing causal influence of\nthe feature. However, this focus on output consis-\ntency often comes at the expense of clarity, respon-\nsiveness, and purity, as raw unembedding-based\ndescriptions do not incorporate any information\nabout what activates the feature. These findings\nagain highlight the necessity of a holistic evalua-\ntion framework, as different methods optimize for\ndifferent aspects of interpretability.\n5\nConclusion\nIn this work, we presented\nFADE, a new auto-\nmated evaluation framework designed to rigorously\nevaluate the alignment between features and their\nopen-vocabulary feature descriptions. By combin-\ning four complementary metrics Clarity, Respon-\nsiveness, Purity, and Faithfulness, our approach\ngives a comprehensive assessment of how a fea-\nture reacts to instances of the described concept,\n\nan evaluation of the description itself as well as\nthe feature’s causal role in the model’s outputs.\nThrough extensive experiments across different fea-\nture types, layers, and description generation mech-\nanisms, we demonstrated that methods relying on\na single metric (e.g., simulation-based approaches)\noften give incomplete or misleading feature descrip-\ntions. Our framework can be used to highlight both\nthe strengths and weaknesses of existing methods,\nwhile it also helps in debugging and improving\nthese methods. We highlighted multiple results\nfor improving the quality of feature explanations,\nsuch as using larger, more capable LLMs for the ex-\nplainer and including more examples in the prompt.\nWe hope that the open-source implementation of\nFADE will drive further research in automated\ninterpretability and help make language models\nmore transparent and safe to use.\nLimitations\nDespite presenting a comprehensive and robust\nevaluation framework, our work has certain lim-\nitations that we want to highlight here: One key\nlimitation is the potential biases in the LLMs used\nfor both rating and synthetic data generation. These\nbiases can affect the evaluation process and perpet-\nuate biases, especially in automated interpretability.\nFor instance, an LLM might recognize a feature en-\ncoding a concept in English as directly representing\nthat concept, whereas the same feature in another\nlanguage might be classified with the additional\nspecification of the language. This discrepancy\ncould lead to unintended biases when steering mod-\nels based on these interpretations. Similar issues\nmay arise from biases present in the pre-training\ndatasets used in our evaluation procedure. Another\nlimitation is related to the steering behavior in the\nfaithfulness pipeline. Our current implementation\ndoes not explicitly verify whether the generated se-\nquences under modification remain grammatically\ncorrect and semantically meaningful. However,\nminor modifications to the prompt could poten-\ntially address this issue in the future. Finally, our\nfaithfulness measure is not well-suited for handling\ninhibitory neurons. A neuron may causally inhibit\nthe presence of a concept in a model’s output, but\nour metric, by design, does not effectively cap-\nture decreases in the appearance of sparse concepts.\nThis limitation arises both from the definition of\nour faithfulness metric and the inherent challenges\nin measuring such suppression effects for already\nsparse concepts.\nAcknowledgements\nWe sincerely thank Melina Zeeb for her valuable\nassistance in creating the graphics and logo for\nFADE. This work was supported by the Fed-\neral Ministry of Education and Research (BMBF)\nas grant BIFOLD (01IS18025A, 01IS180371I);\nthe European Union’s Horizon Europe research\nand innovation programme (EU Horizon Eu-\nrope) as grants [ACHILLES (101189689), TEMA\n(101093003)]; and the German Research Founda-\ntion (DFG) as research unit DeSBi [KI-FOR 5363]\n(459422098).\nReferences\nSteven Bills, Nick Cammarata, Dan Mossing, Henk\nTillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan\nLeike, Jeff Wu, and William Saunders. 2023. Lan-\nguage models can explain neurons in language mod-\nels.\nSteven Bird, Ewan Klein, and Edward Loper. 2009. Nat-\nural language processing with Python: analyzing text\nwith the natural language toolkit. \" O’Reilly Media,\nInc.\".\nTrenton Bricken, Adly Templeton, Joshua Batson,\nBrian Chen, Adam Jermyn, Tom Conerly, Nick\nTurner, Cem Anil, Carson Denison, Amanda Askell,\nRobert Lasenby, Yifan Wu, Shauna Kravec, Nicholas\nSchiefer, Tim Maxwell, Nicholas Joseph, Zac\nHatfield-Dodds, Alex Tamkin, Karina Nguyen, Bray-\nden McLean, Josiah E Burke, Tristan Hume, Shan\nCarter, Tom Henighan, and Christopher Olah. 2023.\nTowards monosemanticity: Decomposing language\nmodels with dictionary learning. Transformer Cir-\ncuits Thread.\nKirill Bykov, Mayukh Deb, Dennis Grinwald, Klaus-\nRobert M\"uller, and Marina M.-C. H\"ohne. 2023.\nDORA: exploring outlier representations in deep neu-\nral networks. Trans. Mach. Learn. Res., 2023.\nKirill Bykov, Laura Kopf, Shinichi Nakajima, Marius\nKloft, and Marina H\"ohne. 2024. Labeling neural\nrepresentations with inverse recognition. Advances\nin Neural Information Processing Systems, 36.\nDami Choi, Vincent Huang, Kevin Meng, Daniel D\nJohnson, Jacob Steinhardt, and Sarah Schwettmann.\n2024. Scaling automatic neuron description.\nTogether Computer. 2023. Redpajama: An open source\nrecipe to reproduce llama training dataset.\nMaximilian Dreyer, Jim Berend, Tobias Labarta, Jo-\nhanna Vielhaben, Thomas Wiegand, Sebastian La-\npuschkin, and Wojciech Samek. 2025. Mechanistic\nunderstanding and validation of large ai models with\nsemanticlens. Preprint, arXiv:2501.05398.\n\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Let-\nman, Akhil Mathur, Alan Schelten, Amy Yang, An-\ngela Fan, et al. 2024. The llama 3 herd of models.\nPreprint, arXiv:2407.21783.\nAlex Foote, Neel Nanda, Esben Kran, Ionnis Konstas,\nand Fazl Barez. 2023. N2g: A scalable approach for\nquantifying interpretable neuron representations in\nlarge language models. Preprint, arXiv:2304.12918.\nRohit Gandikota, Sheridan Feucht, Samuel Marks, and\nDavid Bau. 2024. Erasing conceptual knowledge\nfrom language models. Preprint, arXiv:2410.02760.\nLeo Gao, Stella Biderman, Sid Black, et al. 2020. The\npile: An 800gb dataset of diverse text for language\nmodeling. Preprint, arXiv:2101.00027.\nYoav Gur-Arieh, Roy Mayan, Chen Agassy, Atticus\nGeiger, and Mor Geva. 2025. Enhancing automated\ninterpretability with output-centric feature descrip-\ntions. Preprint, arXiv:2501.08319.\nThomas Heap, Tim Lawson, Lucy Farnik, and Lau-\nrence Aitchison. 2025. Sparse autoencoders can in-\nterpret randomly initialized transformers. Preprint,\narXiv:2501.17727.\nJing Huang, Atticus Geiger, Karel D’Oosterlinck,\nZhengxuan Wu, and Christopher Potts. 2023. Rig-\norously assessing natural language explanations of\nneurons. In Proceedings of the 6th BlackboxNLP\nWorkshop: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 317–331, Singapore. Associa-\ntion for Computational Linguistics.\nJohnny Lin Joseph Bloom. 2024. Understanding sae\nfeatures with the logit lens.\nLaura Kopf, Philine Lou Bommer, Anna Hedstr\"om, Se-\nbastian Lapuschkin, Marina M.-C. H\"ohne, and Kirill\nBykov. 2024. Cosy: Evaluating textual explanations\nof neurons. In Advances in Neural Information Pro-\ncessing Systems 38: Annual Conference on Neural\nInformation Processing Systems 2024.\nAndrew Lee, Xiaoyan Bai, Itamar Pres, Martin Watten-\nberg, Jonathan K Kummerfeld, and Rada Mihalcea.\n2024. A mechanistic understanding of alignment al-\ngorithms: A case study on dpo and toxicity. Preprint,\narXiv:2401.01967.\nTom Lieberum, Senthooran Rajamanoharan, Arthur\nConmy, Lewis Smith, Nicolas Sonnerat, Vikrant\nVarma, Janos Kramar, Anca Dragan, Rohin Shah,\nand Neel Nanda. 2024. Gemma scope: Open sparse\nautoencoders everywhere all at once on gemma 2.\nIn Proceedings of the 7th BlackboxNLP Workshop:\nAnalyzing and Interpreting Neural Networks for NLP,\npages 278–300, Miami, Florida, US. Association for\nComputational Linguistics.\nJohnny Lin. 2023. Neuronpedia: Interactive reference\nand tooling for analyzing neural networks. Software\navailable from neuronpedia.org.\nAleksandar Makelov, George Lange, and Neel Nanda.\n2024. Towards principled evaluations of sparse au-\ntoencoders for interpretability and control. Preprint,\narXiv:2405.08366.\nTom McGrath, Daniel Balsam, Myra Deng, and Eric\nHo. 2024. Understanding and steering llama 3 with\nsparse autoencoders.\nAbhinav Menon, Manish Shrivastava, David Krueger,\nand Ekdeep Singh Lubana. 2025.\nAnalyzing\n(in)abilities of saes via formal languages. Preprint,\narXiv:2410.11767.\nOpenAI. 2024. Hello GPT-4o.\nGonçalo Paulo and Nora Belrose. 2025. Sparse au-\ntoencoders trained on the same data learn different\nfeatures. Preprint, arXiv:2501.16615.\nGonçalo Paulo, Alex Mallen, Caden Juang, and Nora\nBelrose. 2024. Automatically interpreting millions\nof features in large language models.\nPreprint,\narXiv:2410.13928.\nGuilherme Penedo, Hynek Kydlíˇcek, Loubna Ben al-\nlal, Anton Lozhkov, Margaret Mitchell, Colin Raffel,\nLeandro Von Werra, and Thomas Wolf. 2024. The\nfineweb datasets: Decanting the web for the finest\ntext data at scale. In The Thirty-eight Conference on\nNeural Information Processing Systems Datasets and\nBenchmarks Track.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nSenthooran Rajamanoharan, Tom Lieberum, Nicolas\nSonnerat, Arthur Conmy, Vikrant Varma, János\nKramár, and Neel Nanda. 2024. Jumping ahead: Im-\nproving reconstruction fidelity with jumprelu sparse\nautoencoders. Preprint, arXiv:2407.14435.\nJuan Ramos et al. 2003. Using tf-idf to determine word\nrelevance in document queries. In Proceedings of the\nfirst instructional conference on machine learning,\nvolume 242, pages 29–48. Citeseer.\nMorgane Riviere, Shreya Pathak, Pier Giuseppe\nSessa, Cassidy Hardin, Surya Bhupatiraju, Léonard\nHussenot,\nThomas Mesnard,\nBobak Shahriari,\nAlexandre Ramé, et al. 2024. Gemma 2: Improving\nopen language models at a practical size. Preprint,\narXiv:2408.00118.\nGerard Salton and Christopher Buckley. 1988. Term-\nweighting approaches in automatic text retrieval. In-\nformation processing and management, 24(5):513–\n523.\nTamar Rott Shaham, Sarah Schwettmann, Franklin\nWang, Achyuta Rajaram, Evan Hernandez, Jacob\nAndreas, and Antonio Torralba. 2025.\nA multi-\nmodal automated interpretability agent. Preprint,\narXiv:2404.14394.\n\nAdly Templeton, Tom Conerly, Jonathan Marcus, Jack\nLindsey, Trenton Bricken, Brian Chen, Adam Pearce,\nCraig Citro, Emmanuel Ameisen, Andy Jones, Hoagy\nCunningham, Nicholas L Turner, Callum McDougall,\nMonte MacDiarmid, C. Daniel Freeman, Theodore R.\nSumers, Edward Rees, Joshua Batson, Adam Jermyn,\nShan Carter, Chris Olah, and Tom Henighan. 2024.\nScaling monosemanticity: Extracting interpretable\nfeatures from claude 3 sonnet. Transformer Circuits\nThread.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transform-\ners: State-of-the-art natural language processing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle\nLi, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZhuohan Li, Zi Lin, Eric P Xing, et al. 2023. Lmsys-\nchat-1m: A large-scale real-world llm conversation\ndataset. arXiv preprint arXiv:2309.11998.\n\nA\nExtended related work\nA common approach to automatic interpretability\nincludes selecting data samples that strongly acti-\nvate a given neuron and using these samples, along\nwith their activations as input to a larger LLM, that\nserves as an explainer model and generates feature\ndescriptions (Bills et al., 2023; Bricken et al., 2023;\nChoi et al., 2024; Paulo et al., 2024; Rajamanoha-\nran et al., 2024). Previous research has investigated\nvarious factors influencing this method, including\nprompt engineering, the number of data samples\nused, and the size of the explainer model (see Ap-\npendix A).\nBuilding on this approach,\n(Choi et al.,\n2024) advanced the method by fine-tuning\nLlama-3.1-8B-Instruct on the most accurate\nfeature descriptions,\nas determined by their\nsimulated-activation metric. This fine-tuning aimed\nto improve the performance and accuracy of\ndescription generation, ultimately outperforming\nGPT-4o mini.\nAn output-centric approach was introduced by\n(Gur-Arieh et al., 2025) in an attempt to address\nanother key challenge-the feature descriptions gen-\nerated via the data samples that activate the feature\nthe most, often fail to reflect its influence on the\nmodel’s output. The study demonstrates that a com-\nbined approach, integrating both activation-based\nand output-based data, results in more accurate\nfeature descriptions and improves performance in\ncausality evaluations.\nSeveral studies perform their experiments exclu-\nsively on SAEs (McGrath et al., 2024; Paulo et al.,\n2024; Rajamanoharan et al., 2024; Templeton et al.,\n2024), while others focus on MLP neurons (Bills\net al., 2023; Choi et al., 2024). Although (Temple-\nton et al., 2024) compares the interpretability of\nSAEs to that of neurons and concludes that features\nin SAEs are significantly more interpretable, these\nfindings heavily rely on qualitative analyses.\nPrevious research consistently shows that open-\nsource models are effective for generating expla-\nnations, with advanced models producing better\ndescriptions. For instance, Bills et al. (2023) report\nthat GPT-4 achieves the highest scores, whereas\nClaude 3.5 Sonnet performs best for Paulo et al.\n(2024). The number of data samples used to gen-\nerate feature description also varies across studies.\nBills et al. (2023) use the top five most activating\nsamples, while Choi et al. (2024) select 10-20 of\nthe most activating samples to generate multiple\ndescriptions for evaluation. In contrast, Paulo et al.\n(2024) use 40 samples and suggests that randomly\nsampling from a broader set of activating leads to\ndescriptions that cover a more diverse set of acti-\nvating examples, whereas using only top activating\nexamples often yields more concise descriptions\nwhich fails to capture the entire description.\nThe datasets used in these studies also differs.\nPaulo et al. (2024) utilize the RedPajama 10M\n(Computer, 2023) dataset, Choi et al. (2024) use the\nfull LMSYSChat1M (Zheng et al., 2023) and 10B\ntoken subset of FineWeb (Penedo et al., 2024), and\nBills et al. (2023) — on WebText(Radford et al.,\n2019) and the data used to train GPT-2 (Radford\net al., 2019). Additionally, different delimiter con-\nventions are observed: Choi et al. (2024) use de-\nlimiters, Paulo et al. (2024) use « », and Bills et al.\n(2023) use numerical markers.\nB\nData Preprocessing\nFor our work, we used an uncopyrighted version\nof the Pile dataset, with all copyrighted content\nremoved, available on Hugging Face (Gao et al.,\n2020)\n(https://huggingface.co/datasets/\nmonology/pile-uncopyrighted). This version\ncontains over 345.7 GB of training data from\nvarious sources. From this dataset, we extracted\napproximately 6 GB while preserving the relative\nproportions of the original data sources.\nThe\nextracted portion from the training partition was\nused to collect the most activated samples. For\nevaluations, we utilized the test partition from the\nsame dataset, applying identical preprocessing\nsteps as those used for the training data.\nComponent\nSize (GB)\nProportion (%)\nPile-CC\n1.93\n32.17\nPubMed Central\n1.16\n19.38\nArXiv\n0.70\n11.67\nFreeLaw\n0.55\n9.14\nPubMed Abstracts\n0.32\n5.39\nUSPTO Backgrounds\n0.31\n5.23\nGithub\n0.27\n4.54\nGutenberg (PG-19)\n0.23\n3.85\nWikipedia (en)\n0.15\n2.57\nDM Mathematics\n0.11\n1.87\nHackerNews\n0.07\n1.17\nUbuntu IRC\n0.06\n0.99\nEuroParl\n0.06\n0.95\nPhilPapers\n0.03\n0.58\nNIH ExPorter\n0.03\n0.50\nTotal\n5.99\n100.00\nTable 9: Extracted dataset and proportion of sub compo-\nnents\nOur preprocessing involved several steps to en-\nsure a balanced and informative dataset. First, we\nused the NLTK (Bird et al., 2009) sentence tok-\n\nenizer to break large text chunks into individual\nsentences. We then filtered out sentences in the\nbottom and top fifth percentiles based on length,\nas these were typically out-of-distribution cases\nconsisting of single words or characters or a few\noutliers. This step helped achieve a more balanced\ndistribution. Additionally, we removed sentences\ncontaining only numbers or special characters with\nno meaningful content. Finally, duplicate sentences\nwere deleted.\nTraining\nTest\nNumber of sentences\n88,689,425\n5,443,427\nNumber of tokens\n2,284,636,243\n137,600,815\nNumber of unique tokens\n21,707,092\n2,336,552\nTable 10: Dataset Statistics\nC\nAutomated Interpretability Pipeline\nC.1\nImplementation of Automated\nInterpretability\nOur experiments are based on models and feature\ndescriptions presented in Table 11. We generate\nour feature descriptions as follows. In the imple-\nmentation of the autointerpretability pipeline, we’re\nclosely following (Bills et al., 2023; Paulo et al.,\n2024; Rajamanoharan et al., 2024) and others: we\npass the natural dataset, used for generating de-\nscriptions (see Appendix B) through the model,\nand via forward hooks we access the activations\nof each feature and each layer. This part can also\nbe easily parallelized. With Gemma Scope SAEs, a\nwrapper class is implemented, that is built into the\nmodel as another module. It can be extended easily\nto other SAEs.\nAfter passing the whole dataset through the\nmodel, for each feature we take top 103 data sam-\nples based on the maximum activation of tokens.\nWe only consider the maximum activating token of\na data sample when we do the sorting. Later, we\nuniformly subsample the necessary number of data\nsamples, i.e. 5, 15, ..., 50, that are further passed to\nan explainer LLM.\nThe reason why we subsample from top 103 is\nto avoid outliers in terms of activation. Top 103\nstill represents top 0.001% of the dataset. More\ncomplex sampling strategies can bring better per-\nformance, as described in Appendix A, but their\nimplementation and evaluation is left out for the\nfuture work.\nExperiments are performed on the following\nmodels: Gemma-2-2b layer 20 (Riviere et al., 2024),\nLlama-3.1-8B-Instruct layer 19 (Dubey et al.,\n2024), and SAEs Gemma\nScope 16K and 65K\nlayer 20 (Lieberum et al., 2024). We also gen-\nerate descriptions using baseline methods, namely\nTF-IDF and unembedding matrix projection. Term\nFrequency-Inverse Document Frequency (TF-IDF)\nis a widely used technique in NLP for measuring\nthe importance of a word in a document relative\nto a corpus. It balances word frequency with how\nuniquely the word appears across documents, as-\nsigning higher scores to informative words while\ndown-weighting common ones. We generate these\nvalues using 15 maximally activating samples. On\nthe other hand, the unembedding matrix (WU)\n(Joseph Bloom, 2024) in transformer models maps\nthe residual stream activations to vocabulary log-\nits, determining word probabilities in the output.\nBy analyzing projections onto this unembedding\nmatrix, we gain insight into how learned features\ninfluence token predictions. To generate SAE un-\nembedding descriptions, we generate logit weight\ndistribution across the vocabulary, and then use\nthe top ten words with the highest probabilities as\nfeature description.\nLogit weight distribution = WU ∗Wdec[feature]\nhere, WU is the unembedding matrix of a trans-\nformer model and Wdec are the decoder weights of\nsparse auto encoders.\nThis reveals which words are most associated\nwith a given feature, enabling interpretability of\nsparse autoencoder (SAE) features, as well as MLP\nneurons. Both these methods are cheap baselines\nto compare with different auto-interpretability gen-\nerated descriptions.\nC.2\nPrompts Engineering\nOur feature description pipeline consists of several\nkey components. The Subject Model for which\nthe descriptions are getting generated, while the\nExplainer Model is a larger model used to generate\ndescriptions. The System Prompt provides task-\nspecific instructions to the LLM, detailing what to\nfocus on in the provided samples and how to for-\nmat the output. We append 5, 10, or 50 sentences\nalong with a user_message_ending at the end,\nwhich helps reinforce the expected output structure.\nBefore normalizing activations, we first average\nactivations for tokens belonging to the same word.\nThese values are then normalized between 0 and\n10. For delimiters, we use single curly brackets\n\nModel\nLayers\nHuffingFace\nDescriptions\nGemma-2-2b\n3, 12, 20, 25\ngoogle/gemma-2-2b\n–\nGemma Scope 16K\n20\ngoogle/gemma-scope-2b-pt-res/tree/main/\nlayer_20/width_16k/average_l0_71\nneuronpedia.org/gemma-2-2b/20-gemmascope-res-16k\nGemma Scope 65K\n20\ngoogle/gemma-scope-2b-pt-res/tree/main/\nlayer_20/width_65k/average_l0_114\nneuronpedia.org/gemma-2-2b/20-gemmascope-res-65k\nLlama3.1-8B-Instruct\n19\nmeta-llama/Llama-3.1-8B-Instruct\ngithub.com/TransluceAI/observatory.git\nTable 11: Sources for models, SAEs and feature descriptions, used in this work.\nAvg # of tokens\nCost/103 feature ($)\n0-shot\n1,423\n2.13\n1-shot\n1,498\n2.25\n2-shot\n1,564\n2.35\n10-shot\n2,395\n3.95\n20-shot\n3,393\n5.09\nTable 12: Token usage and cost comparison for different\nshot settings.\n# of sentences\nAvg # of tokens\n5\n333\n15\n964\n50\n2,507\nTable 13: Average number of tokens as the number of\nsentences increases. These values are based on tokenizer\nused for Gemma-Scope-16k and are not the number of\ntokens per requests generated by openai.\nif the activation intensity is below 4 and double\ncurly brackets otherwise. For numerical input, we\nprovide the LLM with dictionary of most activated\ntoken at the end of each sample.\nMain prompt: The main prompt for our we\nuse to generate descriptions is given below:\nYou are a meticulous AI researcher conducting an\nimportant investigation into sparse\nautoencoders of a language model that activates\nin response to specific tokens within text\nexcerpts. Your overall task is to identify and\ndescribe the common features of highlighted\ntokens , focusing exclusively on the tokens that\nactivate and ignoring broader sentence context\nunless absolutely necessary.\nYou will receive a list of sentences in which\nspecific tokens activate the neuron. Tokens\ncausing activation will appear between\ndelimiters like {{ }}. The activation values\nrange from 0-10:\n- If a token activates with an intensity of <4, it\nwill be delimited like {{this }}.\n- If a token activates with an intensity of >4, it\nwill be delimited like {{{{ this }}}}.\nGuidelines:\n1. Focus on the activated tokens: The description\nmust primarily relate to the highlighted tokens\n, not the entire sentence.\n2. Look for patterns in the tokens: If a specific\ntoken or a group of similar tokens repeatedly\nactivates , center your analysis on them.\n3. Sparse Autoencoder Dependency: The activations\ndepend only on the words preceding the\nhighlighted token. Descriptions should avoid\nrelying on words that come after the activated\ntoken.\n4. No coherent presence of concept: Return 'NO\nCONCEPT FOUND ' if there is no coherent theme in\nthe sentences provided , do not force a concept\nand stay grounded.\nOutput Format:\nConcept: [Focus on the common concept tied to the\nhighlighted tokens , described in a concise\nphrase .]\n### Example:\nInput example 1:\nSentence 1: The {{ United States }} will not allow {{\nthreats }} against its people.\nSentence 2: The {{U.S.}}\nemphasizes deterrence in\nforeign policy.\nExample Output:\nConcept: U.S. deterrence policies and moral stance\non global threats.\nInput example 2:\nSentence 1:\nAsh/Brock [Bouldershipping ]\\nI forget\nabout this one {{all}} for one favours.\nSentence 2: I see this attitude brewing {{all}} at\nonce.\nSentence 3: I get emails from people {{all}} over\nthe world.\nExample Output:\nConcept: Presence of the word 'all '.\nuser_message_ending: >\nAnalyze all these sentences as ONE corpus and\nprovide your description in the following\nformat:\nConcept: Your devised concept and it's description\nin a concise manner , and very few words.\nPrompt variation 1: using numbers, instead of\ndelimiters for further experiments as mentioned in\n, we include small variations in the main prompt\nYou are a meticulous AI researcher [...]\nYou analyze a list of most -activated sentences and a\ndictionary of relevant tokens , each with\nassigned activation values , to identify key\nthemes and concepts. Tokens causing activations\nwill be provided in the dictionary after each\nsentence. The activation values range from\n0-10.\nGuidelines :[..]\nOutput Format:\nConcept: [Focus on the common concept tied to the\nhighlighted tokens , described in a concise\nphrase .]\nEXAMPLE 1\nSentence 1: \"The choir 's harmonies resounded\nthroughout the church as the congregation stood\nin awe.\"\nMost relevant tokens: {{\" harmonies \": 9, \"resounded \":\n8, \"church \": 6, \"congregation \": 4}}[..]\nPrompt variation 2: main prompt + no shotsc\nPrompt variation 3: main prompt + one shot\nexample\nPrompt variation 4: main prompt + five shots\n\n# of features\nTime (h)\nCost/103 features ($)\nGemma-2-2b\n239,616\n185\n0.77\nGemma Scope 16K\n16,384\n524\n31.98\nGemma Scope 65K\n65,536\n622\n9.49\nLlama-3.1-8B\n458,752\n361\n0.79\nTable 14: Cost comparison for discovering samples, that\nare maximally activating features.\nC.3\nComputational Costs\nObtaining maximum activating samples for fea-\ntures is performed locally on a cluster with 8\nNVIDIA A100 GPUs with VRAM 40Gb. The pro-\ncess can be easily parallelized, but for cost calcu-\nlation we are using the total consumed time. Such\nGPUs can be rented starting with $0.67/hr, how-\never, an average price on the market exceeds this\nvalue. For simplicity of calculations, we take a\nprice of $1/hr. For convenience, we consider a\nprice per 103 features, the results are presented in\nTable 14.\nDue to the differences in the implementation,\ncalculating maximum activating samples is signifi-\ncantly cheaper for the complete models, since we\nare doing it at the same moment for the whole\nmodel, and the price is divided on the total num-\nber of features that we consider. For SAEs, in the\ncurrent implementation we only do it per one layer,\nwhich is significantly reducing the number of con-\nsidered features. In the future we are planning on\noptimizing this process in a way, that it is possible\nto calculate it at the same time for multiple cho-\nsen SAEs. The size of SAEs is also playing an\nimportant role: in terms of total costs it is more ex-\npensive, but if we consider the cost per 103 features,\nthe larger the SAE - the cheaper it is.\nThe total computational cost for this part of the\nexperiments is estimated at 4,920 GPU hours. This\nincludes all experiments conducted on the models\nreferenced in this paper. While not all experiments\nare explicitly discussed, they contributed to the\nfinal results presented here.\nD\nFADE Evaluation Framework\nOur framework is designed to work with a wide\nvariety of subject models, including most Hugging-\nFace Transformers (Wolf et al., 2020) as well as\nany feature implemented as a named module in\nPyTorch. Moreover, our framework is extensible:\ninterpretability tools such as SAEs or various super-\nvised interpretability techniques can be integrated\nwith minimal effort, provided they implement some\nbasic steering functions.\nIn addition, we offer\nan interface for a diverse set of evaluating LLMs,\nwhether open-weight or proprietary, with support\nfor platforms such as vllm, Ollama, OpenAI, and\nAzure APIs. Our implentation can be found at\nhttps://github.com/brunibrun/FADE.\nD.1\nImplementation and Computational\nEfficiency\nTo compute the purity and responsiveness mea-\nsures, we base our sampling from the natural\ndataset on the activations of the subject LLM. For\neach sequence, we calculate the maximum abso-\nlute activation value across all tokens. Using these\nvalues, we sample sequences by selecting a user-\nconfigurable percentage of those with the strongest\nactivations and for the remainder, drawing an\nequal number of samples from each of the fol-\nlowing percentile ranges: [0%, 50%[, [50%, 75%[,\n[75%, 95%[, and [95%, 100%].\nComputational efficiency is a key consideration\nin our design, as evaluating every neuron in an\nLLM can be prohibitively expensive. The cost\nof evaluations is dynamically adjustable based on\nseveral factors, including the number of samples\ngenerated and rated, the evaluating LLM used, and\nthe natural dataset selection.\nOur method allows users to control the cost by\nsetting the number of synthetic samples (denoted\nn) relative to the full size of the natural dataset\n(N). By pre-computing activations from the natural\ndataset in parallel, we effectively reduce the per-\nrun complexity from O(N ∗M) for M neurons to\nO(n∗M). Given that n is typically in the hundreds\nwhile N is in the millions, this strategy yields big\nefficiency gain.\nAdditionally, we only execute the computa-\ntionally intensive faithfulness evaluation when\nboth clarity and responsiveness exceed a user-\nconfigurable threshold. This conditional execution\nensures that unnecessary computations are avoided\nfor features that do not meet our interpretability\ncriteria.\nD.2\nDetails on the Experiment Setup\nIn our experiments we send 15 requests to the evalu-\nation LLM for generating synthetic samples. We re-\nmove duplicates and use these as concept-samples.\nWe use the whole evaluation dataset as control sam-\nples. For rating we draw 500 samples from the\nnatural dataset, where we take 50 from the top\nactivated and 450 from the lower percentiles, ac-\ncording to the sampling strategy outlined above in\n\nAppendix D.1. If we obtain fewer than 15 concept-\nsamples in this first rating we again sample 500 new\nsamples with the same sampling approach. We rate\n15 samples at once, and if one of the calls fails, for\nexample due to formatting errors of the evaluating\nLLM, we retry the failed samples once. For the\nFaithfulness experiments we use the modification\nfactors [−50, −10, −1, 0, 1, 10, 50]. We draw 50\nsamples from the natural dataset and let the subject\nLLM continue them for 30 tokens. We then rate\nonly these continuations for concept strength by the\nevaluating LLM, again retrying once, if the rating\nfails. We only execute the Faithfulness experiment,\nif both the Clarity and Responsiveness of a feature\nis larger or equal to 0.5.\nLicenses Gemma-2-2b is released under a cus-\ntom Gemma Terms of Use. Gemma Scope SAEs\nare released under Creative Commons Attribution\n4.0 International. Llama3.1-8B-Instruct is re-\nleased under a custom Llama 3.1 Community Li-\ncense. Transluce feature descriptions, Pile Uncopy-\nrighted dataset and LangChain are released under\nMIT License. vLLM is released under Apache 2.0\nLicense.\nD.2.1\nPrompts for the Evaluating LLM\nGenerating Synthetic Data Prompt\nYou are tasked with building a database of sequences\nthat best represent a specific concept.\nTo create this , you will generate sequences that\nvary in style , tone , context , length , and\nstructure , while maintaining a clear connection\nto the concept.\nThe concept does not need to be explicitly stated in\neach sequence , but each should relate\nmeaningfully to it. Be creative and explore\ndifferent ways to express the concept.\nHere are examples of how different concepts might be\nexpressed:\nConcept: \"German language\" - Sequences might include\nGerman phrases , or sentences.\nConcept: \"Start of a Java Function\" - Sequences\nmight include Java code snippets defining a\nfunction.\nConcept: \"Irony\" - Sequences might include ironic\nstatements or expressions.\nProvide your sequences as strings in a Python List\nformat.\nExample: [\"This is a first example sequence.\", \"\nSecond example sequence but it is much longer\nalso there are somy typos in it. wjo told you\nthat I can type ?\"]\nOutput only the Python List object , without any\nadditional comments , symbols , or extraneous\ncontent.\nRating Natural Data Prompt\nYou are tasked with building a database of sequences\nthat best represent a specific concept.\nTo create this , you will review a dataset of varying\nsequences and rate each one according to how\nmuch the concept is expressed.\nFor each sequence , assign a rating based on this\nscale:\n0: The concept is not expressed.\n1: The concept is vaguely or partially expressed.\n2: The concept is clearly and unambiguously present.\nUse conservative ratings. If uncertain , choose a\nlower rating to avoid including irrelevant\nsequences in your database.\nIf no sequence expresses the concept , rate all\nsequences as 0.\nEach sequence is identified by a unique ID. Provide\nyour ratings as a Python dictionary with\nsequence IDs as keys and their ratings as\nvalues.\nExample Output: {{\"14\": 0, \"15\": 2, \"20\": 1, \"27\":\n0}}\nOutput only the dictionary - no additional text ,\ncomments , or symbols .\"\nD.2.2\nAssociated Cost\nThe activation generation for the subject models\nfor the results in section 4.1 was run locally on a\ncluster with NVIDIA A100 GPUs with 40GB of\nVRAM. Similar to section C.3 we assume a price\nof $1/hr. Since activations can be cached in par-\nallel for the whole model, only a single pass over\nthe evaluation dataset was needed per model. We\nestimate a needed time of 24 hours on one GPU\nfor the activation generation, resulting in a cost of\n24$ per model. During the evaluations, only the\nactivations of synthetic samples need to be com-\nputed. Due to a suboptimal configuration of the\nevaluation and subject LLM in our experiments we\nassume an average time of one minute per neuron\nevaluation, leading to an average cost of 0.024$\nper evaluated feature. The estimated cost for the\nevaluating LLM consists of the cost for the gener-\nation of synthetic samples as well as the cost for\nrating natural data. In the configuration used for the\nexperiments, unless stated otherwise, we estimate\nthe evaluating LLM creates about 2,000 tokens per\nfeature evaluation, which corresponds to a cost of\nabout 0.0012$ per feature at an output token cost of\n0.6$ per million tokens. For the rating part the eval-\nuating LLM recieves about 20,000 tokens as input,\nwhich corresponds to a cost of 0.003$ per feature\nat an input token cost of 0.15$ per million tokens.\nCorresponding results are presented in Table 15.\nE\nExtended Results\nE.1\nQuantitative Analysis\nThis section provides a more in-depth analysis of\nthe experimental results presented in Section 4.1.\n\nFigure 5: Feature descriptions fit for SAEs of different sizes.\nFigure 6: Feature descriptions fit for different layers of Gemma-2.\n# of features\nevaluated\nGPU\nTime (h)\nCost/ 103 features($)\nGemma-2-2b\n9,000\n216 + 24\n26.67\nGemma Scope 16K\n25,000\n600 + 24\n24.96\nGemma Scope 65K\n2,000\n48 + 24\n36\nLlama-3.1-8B\n2,000\n48 + 24\n36\nTable 15: Computational cost comparison for running\nthe evaluation experiments on GPU.\nSAEs concept narrowness results in lower in-\nterpretability. This might seem counterintuitive at\nfirst, but again it is important to stress that we are\nnot evaluating the features by themselves, but in-\nstead the adequacy of the proposed feature descrip-\ntion to these features. One potential explanation\nis that larger SAEs distribute concepts more finely\nacross features. As a result, a slightly inaccurate\ndescription that might have still activated a feature\nin a smaller SAE may fail to activate the corre-\nsponding feature in a larger SAE, where concepts\nare encoded even more sparsely. The consistency\nof this result is demonstrated by using different\nfeature descriptions — MaxAct*, produced in this\nwork, and the ones available on Neuronpedia.\nInterestingly, we observe a small left-skewed\npeak in the responsiveness distribution for 65K\nSAEs labelled via MaxAct*, a pattern not seen in\nany other experiment (see Figure 5). A qualitative\nanalysis suggests that this is primarily caused by\nfeatures representing out-of-distribution concepts\nrelative to the dataset used for evaluation, such\nas, e.g., “new line” feature, which due to the pre-\nprocessing steps is not present in the dataset used\nfor descriptions generation (see Figure 12). Due\nto the lower quality of feature descriptions, the\npurity distribution for Neuronpedia descriptions\nof Gemma Scope 65K exhibits a bimodal pattern.\nHigh-quality descriptions tend to have high purity,\nreflecting the greater monosemanticity of the fea-\ntures. However, a substantial number of inaccurate\ndescriptions lead to very low purity values.\nSome Gemma-2 layers look almost not in-\nterpretable. The feature distribution in layer 12\ndiffers significantly from other layers.\nUnlike\nother layers, layer 12 lacks the characteristic right-\nward elevation in clarity, responsiveness, and purity\nscores, as presented on Figure 6. Manual analysis\nof the heatmaps supported the theory, that layer\n12 demonstrates a high level of polysemanticity.\nInterestingly, similar result was demonstrated on\nLlama-3.1-8B-Instruct by (Choi et al., 2024).\nEvaluating several complete models with\nFADE\nwould provide more insights into interpretability of\ndifferent models components.\nNumeric input shows marginally better per-\n\nFigure 7: Analysis of the main prompt’s components: (a) numeric input vs delimeter-based; (b) number of activating\nsamples, provided to the explainer LLM; (c) number of shots used in the prompt.\nformance. As illustrated in Figure 7 (a), the per-\nformance difference between numeric input-based\nprompts and delimiter-based highlighting is not\nstatistically significant for both MLP neurons and\nSAEs, though the mean score is slightly higher\nfor numeric input. A more comprehensive eval-\nuation across multiple layers and a larger feature\nset is needed to determine the optimal approach.\nNonetheless, these findings challenge prior asser-\ntions that highlighting the most activating tokens\nis superior due to LLMs’ assumed difficulty in pro-\ncessing numerical inputs (Choi et al., 2024).\nIncreasing sample count improves perfor-\nmance. A clear trend emerges: providing more\nsamples to the explainer LLM enhances its perfor-\nmance. However, this also increases the computa-\ntional cost of feature generation due to the higher\ntoken count. While 15 samples yield strong re-\nsults, 50 samples perform even better, as shown in\nFigure 7 (b).\nIncreasing examples improves performance\nbut raises costs. As shown in Figure 7 (c), in-\ncreasing the number of shots consistently enhances\nperformance on activation-based metrics without\nsignificantly affecting faithfulness, contradicting\nfindings in (Choi et al., 2024). This discrepancy\nmay stem from differences in provided examples\nor feature types, as our analysis focuses on Gemma\nScope SAEs, whereas (Choi et al., 2024) examined\nLlama-3.1-8B-Instruct neurons. Additionally,\n\nFigure 8: Performance of feature descriptions generation via different explainer LLMs.\nFigure 9: MaxAct* comparison to feature descriptions, generated via other methods on Gemma Scope 16K and\nLlama3.1-8B-Instruct.\na higher shot count increases computational costs\ndue to the larger token input (see Table 12). In this\nstudy, we primarily use 2-shot prompts, balancing\nperformance and cost efficiency.\nStronger explainer LLMs yield better fea-\nture descriptions.\nMore capable models con-\nsistently achieve higher performance across\nnearly all metrics, as presented in Figure 8.\nLlama-3.1-70B-Instruct (quantized) performs\ncomparably to GPT-4o mini, aligning with find-\nings from the evaluation of LLMs (see Table 1).\nPerformance generally declines with model size,\nexcept for Llama-3.2-1B-Instruct, which fre-\nquently fails to adhere to the required output for-\nmat, leading to significantly poorer results across\nall metrics.\nMaxAct*\ndemonstrates\nsuperior\nperfor-\nmance. Our findings highlight the importance of\nconsidering all four\nFADE metrics when opti-\nmizing automated interpretability approaches. On\nGemma Scope 16K, MaxAct* outperforms all base-\nlines across all metrics except faithfulness (see Fig-\nure 9). We show that our generated descriptions\nsignificantly surpass those currently available on\nNeuronpedia. Additionally, we compare MaxAct*\nto TF-IDF and unembedding-based baselines (see\nAppendix C.1). While the unembedding method\nunderperforms in activation-based metrics such as\nclarity, responsiveness, and purity, it achieves no-\ntably higher faithfulness by explicitly considering\nthe feature’s output behavior. This underscores\nthat faithfulness depends on both the feature type\n(e.g., SAEs vs. MLP neurons) and the generated\ndescription.\nOur results align with (Gur-Arieh et al., 2025),\nwhich demonstrates that combining MaxAct-like\napproaches with output-based methods enhances\noverall feature description quality. However, the\ninput-centric metric used in that work does not fully\ncapture failure modes that clarity, responsiveness,\nand purity account for.\nThis becomes particularly evident when com-\nparing MaxAct* to feature descriptions gener-\nated for Llama-3.1-8B-Instruct in (Choi et al.,\n2024). While clarity scores are comparable—albeit\nslightly lower for MaxAct* — responsiveness and\npurity show significant improvements. This differ-\nence may partially stem from dataset construction\nvariations. Notably, the purity distributions of the\ntwo approaches are strikingly different, even op-\nposing: MaxAct* exhibits a right-skewed peak,\nwhereas Transluce’s feature descriptions perform\n\nFigure 10: Examples of feature descriptions, obtained via MaxAct*, demonstrating low clarity, but medium or high\nresponsiveness and purity. Feature 475: “Frequent presence of token ’self’ indicating object oriented programming\nconcept”; feature 653: “Word ’make’ in different forms, expressions and concepts”; feature 821: “Expressions\nindicating lists or explanations”.\nFigure 11: Heatmap and descriptions evaluation result\nfor feature 5183 of Llama3.1-8B-Instruct layer 19.\npoorly overall. Faithfulness differences are minor\nbut still favor MaxAct*, likely due to the generation\nof higher-quality feature descriptions.\nE.2\nQualitative Analysis\nFine-tuning automated interpretability requires\nconsideration of all metrics in\nFADE. The com-\nparison of metric distributions against the simulated\nactivation-based metric in Figure 3 highlights that\nrelying solely on this metric is insufficient for accu-\nrately assessing the quality of feature descriptions.\nIf an automated interpretability framework is fine-\ntuned exclusively on such a metric, it may generate\nsuboptimal descriptions.\nFor instance, evaluations indicate that the\ndescription for feature 5183 in layer 19 of\nMethod\nLabel\nTransluce\nactivation on names with specific formatting, including\n\"Bryson,\" \"Brioung,\" \"Bryony,\" and \"Brianna\"\nMaxAct*\nPresence and significance of the word \"is\"\nTable 16: Descriptions for Llama3.1-8B-Instruct fea-\nture 5183 layer 19.\nLlama3.1-8B-Instruct, as generated in (Choi\net al., 2024), performs well in terms of clarity\nand responsiveness, yet achieves a near-zero purity\nscore (see Figure 11). Conversely, a description\nproduced using MaxAct* (see Table 16) exhibits\nlower clarity and responsiveness but significantly\nhigher purity.\nThe heatmap of activations during description\ngeneration suggests that the description from (Choi\net al., 2024) strongly activates this feature. While\nthe heatmap does not show all the names listed\nin Transluce’s description, this may be due to the\nsampling method, which selects random sentences\nfrom the top 1000 to mitigate outliers. However,\nactivation is observed on similar names, such as\n“Bernstein” and “Brittney.” More importantly, this\nclearly polysemantic feature responds to multiple\ndistinct concepts, including the word “is” in spe-\ncific contexts (included into a description generated\nvia MaxAct*), as well as certain coding patterns.\nAs a result, despite the relatively high metric\nscore of 0.77 in Transluce’s evaluation, the de-\nscription has very low purity. This underscores\nthe importance of considering not only how well\na concept activates a feature but also other inter-\npretability factors measurable with\nFADE. In\nthis case, although the feature is inherently difficult\nto interpret, we argue that the MaxAct* descrip-\ntion provides a more accurate representation, as\nit better captures the feature’s activating pattern,\nand\nFADE is clearly demonstrating the feature’s\n\nFigure 12: Feature 3,286 of Gemma Scope 65K SAE. MaxAct* description: “Mathematical expressions and\nsignificant numerical values”. Neuronpedia description: “function definitions in a programming context”.\npolysemanticity.\nSAEs descriptions often fail to accurately cap-\nture the concept. Figure 10 presents the heatmap\nfor feature 475, whose description emphasizes the\noccurrence of the token “self.” However, the fea-\nture does not activate on all instances of “self”\n(highlighted in yellow), indicating that a crucial\naspect of the concept is missing or remains un-\nclear. Additionally, the feature activates on other\ntokens, further suggesting that the description is\nincomplete.\nThis is reflected in the evaluation metrics: low\nclarity indicates that the concept is not expressed\nprecisely enough for the evaluating LLM to gener-\nate synthetic data that reliably activates the feature.\nA responsiveness score of 0.93 suggests that the fea-\nture does activate on natural data aligned with the\nconcept, while a purity score of 0.81 reveals that al-\nthough the feature is primarily associated with the\ndescribed concept, it also responds to other inputs.\nSimilar issues arise in features 653 and 821,\nwhere the underlying concepts appear highly spe-\ncific – activating on a particular token within a\nspecific context. However, their descriptions are\noverly broad, making it difficult to generate syn-\nthetic data that reliably triggers the feature. These\nand many other similar features contribute to the\nleft-skewed peak in the clarity distribution, which\nbecomes more pronounced for narrower concepts\nrepresented by features in Gemma Scope 65K.\nMethod\nLabel\nNeuronpedia\nThe presence of JavaScript code segments or functions\nTF-IDF\nasdfasleilse asdkhadsj easy file jpds just mean span think\nUnembedding\nf, <eos>, fd, wer, sdf, df, b, jd, hs, ks\nMaxAct*\nPresence of nonsensical or random alphanumeric strings\nTable 17: Descriptions for Gemma Scope feature 9295\nlayer 20.\nOut-of-Distribution Features in Gemma Scope\n65K SAEs. The dataset used for automatic inter-\npretability omitted certain concepts, such as \"new\nline,\" leading to gaps in feature descriptions. These\nFigure 13: Heatmap and descriptions evaluation result\nfor feature 9295 of Gemma Scope layer 20.\nomissions contribute to the small left-side peak in\nresponsiveness distribution in Figure 5. Several\nfeatures, including 3315, 3858, and 4337, lack ac-\ntivation heatmaps under the MaxAct* approach,\nas the dataset does not represent their concepts.\nConsequently, the explainer model, relying on un-\nrelated sentences, generates incorrect descriptions\n(see Figure 12). Heatmaps from Neuronpedia2 re-\nveal what would activate these features, highlight-\ning limitations of the dataset, used in this work,\nand broader issues in the automated interpretability\npipeline. For example, despite obtaining and visual-\nizing correct results, feature descriptions available\non Neuronpedia are also not representing a correct\nconcept. Similar results have been obtained for the\n<bos> token and indentation in text and code.\nReliable Evaluation —\nFADE Identifies\nthe Best Description.\nDifferent automated in-\nterpretability methods prioritize either activation-\n2https://www.neuronpedia.org/gemma-2-2b/20-\ngemmascope-res-65k/3286\n\nFigure 14: Heatmap and descriptions evaluation result\nfor feature 1139 of Gemma Scope layer 20.\nMethod\nLabel\nNeuronpedia\nreferences to problematic situations or conflicts that cause\ntrouble\nTF-IDF\ntrouble\nUnembedding\ntroubles, difficulties, problems, troublesome, mischief, ...\nMaxAct*\nActivation of \"into\" and \"trouble\" indicating situations\nleading to problem\nTable 18: Descriptions for Gemma Scope feature 1139\nlayer 20.\nbased metrics or faithfulness-based measures, lead-\ning to descriptions that may be overly broad or\ninaccurate.\nIn some cases, even manual inspection of\nheatmaps fails to fully capture the underlying con-\ncept represented by a feature. Therefore, a compre-\nhensive evaluation must consider all four metrics.\nTable 17 presents feature descriptions generated\nby various methods. Based on the heatmap analy-\nsis, the MaxAct* description most accurately repre-\nsents the concept. The unembedding method, while\nincorporating specific tokens promoted by the fea-\nture, also demonstrates strong alignment with the\nconcept, as reflected in the corresponding metrics.\nHowever, it is not descriptive enough, which is\nresulting in lower responsiveness and purity.\nSometimes baseline methods may outperform\nmore complex approaches, particularly on specific\nmetrics. For instance, TF-IDF and unembedding\nbaselines exhibit significantly higher faithfulness\ncompared to Neuronpedia or MaxAct* for certain\nfeatures (see Figure 14).\nFeature 1139, for example, influences the output\nof tokens related to the concept of “trouble”. De-\nscriptions that explicitly capture this aspect tend\nto achieve higher faithfulness (see Table 18). The\nMaxAct* description, in contrast, emphasizes the\nbroader meaning and the most activating expres-\nsion, “into trouble”, leading to higher clarity.\n",
  "metadata": {
    "source_path": "papers/arxiv/FADE_Why_Bad_Descriptions_Happen_to_Good_Features_9fd71516282ef82c.pdf",
    "content_hash": "9fd71516282ef82c663fd06a7052c70b74af8e334bd19c7bf9b1467ed89935b6",
    "arxiv_id": null,
    "title": "FADE_Why_Bad_Descriptions_Happen_to_Good_Features_9fd71516282ef82c",
    "author": "",
    "creation_date": "D:20250225023550Z",
    "published": "2025-02-25T02:35:50",
    "pages": 22,
    "size": 5651468,
    "file_mtime": 1740470199.68409
  }
}