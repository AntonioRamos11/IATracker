{
  "text": "Evaluating Expert Contributions in a MoE LLM for Quiz-Based Tasks\nAndrei Chernov 1,\n1Independent Researcher,\nCorrespondence: chernov.andrey.998@gmail.com\nAbstract\nRecently, Large Language Models (LLMs)\nwith Mixture of Experts (MoE) layers have\ngained significant attention. Currently, state-\nof-the-art LLMs utilize this architecture. There\nis a substantial amount of research on how to\ntrain such models and how to select hyperpa-\nrameters for this architecture. However, there\nis a lack of studies focusing on post-evaluation\nanalysis of MoE layer properties. In this pa-\nper, we take a first step toward closing this gap\nby evaluating expert contributions on the quiz-\nbased MMLU benchmark. We show that most\nexperts were never activated during inference\non this benchmark. Additionally, the output dis-\ntribution of gating networks is much closer to\nuniform than sparse. Finally, we demonstrate\nthat the average performance of some experts\nwithin the same layer varies significantly.\n1\nIntroduction\nRecently, Large Language Models (LLMs) with\nMixture of Experts (MoE) layers, instead of fully\ndense layers, have gained popularity (Du et al.,\n2022; Wan et al., 2023). Currently, one of the\nbest-performing models utilizes this architecture\n(Liu et al., 2024). The main reason MoE models\nare preferred over dense models is that they tend to\nachieve similar performance while activating signif-\nicantly fewer parameters, thereby reducing training\ntime compared to dense LLMs (Muennighoff et al.,\n2024).\nMost research on MoE in the natural language\nprocessing (NLP) domain has focused on ei-\nther modifying the architecture to speed up in-\nference—such as the Top-K gating mechanism\n(Shazeer et al., 2017), which selects only the top-K\nexperts with the highest probabilities—or adjust-\ning the training loss to prevent the gating networks\nfrom always activating only a small subset of ex-\nperts (Shazeer et al., 2017; Shen et al., 2024).\nIn this paper, we focus on post-evaluation anal-\nysis of expert contributions to final predictions.\nSpecifically, we evaluate the pretrained OLMoE\nmodel1 (Muennighoff et al., 2024) on the quiz-\nbased MMLU benchmark (Hendrycks et al., 2020)\nto address the following questions:\n• How many experts were activated at least once\nduring inference on this benchmark?\n• What does the distribution of gating network\noutputs look like? Does it tend to be sharp or\ncloser to uniform?\n• Do all experts perform equally in terms of\naccuracy?\n2\nExperimental Setup\nIn this paper, we investigate the contribution of\neach expert in the OLMoE model during inference\non the MMLU benchmark. MMLU is a quiz-based\nbenchmark that evaluates the knowledge and rea-\nsoning abilities of large language models (LLMs).\nIt consists of 57 datasets covering various domains,\nsuch as humanities, STEM, social sciences, and\nother fields.\nWe did not observe a significant difference in ex-\npert contributions across different domains. There-\nfore, in the results section (Section 3), we present\nresults aggregated over all datasets, comprising a\ntotal of 14, 042 questions.\nFor each question, the benchmark requires a\nmodel to select the correct answer from four possi-\nble choices: A, B, C, and D. Thus, the model needs\nto generate only one token corresponding to an\nanswer. To assess the contribution of experts, we\nstore the probabilities (alphas) from the gating net-\nwork for each MoE layer when the model predicts\nthe token corresponding to the correct answer.\n1https://huggingface.co/allenai/\nOLMoE-1B-7B-0125-Instruct\n1\narXiv:2502.17187v1  [cs.CL]  24 Feb 2025\n\nThe OLMoE model consists of 16 MoE layers,\neach containing 64 experts. For every question, we\nstore an array of alphas with the following dimen-\nsions: 16 × 64. Note that only the top 8 experts\nwith the highest probabilities contribute to the final\noutput.\nTo run the experiment, we utilized a V100 GPU\nwith 16 GB of memory. We used a batch size of\n2, and the evaluation of the MMLU dataset took\napproximately 5 hours.\n3\nResults\n3.1\nDistribution of Activated Experts\nIn this section, we analyze how many experts were\nactivated2 during inference, as well as the normal-\nized distribution of activated experts for each data-\npoint. Tables 1 and 2 report the number of experts\nthat were activated for at least one datapoint.\nConsidering that the total number of experts is\n64, we observe that more than 60% of the experts\nwere never activated for the entire MMLU dataset.\nAdditionally, we report the mean and standard de-\nviation of natural entropy (Conrad, 2004), defined\nas:\nE = −\nX\ni∈top 8\npi log pi\n(1)\nwhere pi represents the normalized distribution\nover the highest gating probabilities, i.e.,\npi =\nαi\nP\ni∈top 8 αi\n,\n(2)\nwhere αi is the output from the gating network.\nWe use natural entropy as a measure of uncertainty.\nIt converges to zero when one expert has a prob-\nability close to 1, meaning that only this expert\ncontributes to the result. Conversely, when the dis-\ntribution is uniform, entropy reaches its maximum\nvalue. Specifically, for a discrete distribution with\n8 outcomes, the highest entropy value is 2.0794.\nBased on the reported entropy in the tables, we\nconclude that the distribution for each expert is\nfar from sparse and instead tends to be closer to\nuniform. We believe this behavior is likely caused\nby auxiliary losses during the training procedure,\nwhich force the model to activate each expert ap-\nproximately the same number of times. This pre-\nvents the model from converging to a small sub-\nset of preferred experts, thereby ensuring that all\n2To be activated, the corresponding probability for this\nexpert from the gating function must be among the top 8.\nexperts remain utilized. However, as our results\nsuggest, this may lead to a gating probability distri-\nbution that is close to uniform, which might not be\ndesirable. These results also hold for the distribu-\ntion across all 64 experts (see Appendix A).\nA hypothesis that we believe is worth validating\nin future work is whether this uniform-like behav-\nior negatively impacts the model’s robustness. The\nprimary concern is that the Top-K activation ap-\nproach is not smooth. If the gating outputs follow a\nnearly uniform distribution, small changes in input\nmay lead to significant differences in output due\nto a different set of experts being activated. Even\nif only the last expert in the top K differs, this\ncould still cause noticeable variations. As shown\nin the Table 3, the weight of the eighth expert is\nsignificant, averaging 8.74%. This observation mo-\ntivated us to investigate the average accuracy of\neach expert (see Section 3.2).\nAdditionally, an unexpected result for us is that\nentropy tends to increase from the first to the last\nlayer. The first layer has the lowest entropy, while\nthe last layer has one of the highest entropy. In-\ntuitively, we expected the opposite: the last layer\nshould be more confident in its predictions. One\npossible explanation is that some benchmark ques-\ntions are too complex for the model, leading to less\nconfident predictions. However, the standard devi-\nation of entropy is low, indicating that the distribu-\ntion remains stable across all questions, regardless\nof their complexity.\n3.2\nAccuracy of each Expert\nIn Section 3.1, we showed that the output distribu-\ntion from the gating function is closer to uniform\nrather than sparse. This means that the contribution\nof each expert among the top 8 is significant to\nthe final outcome. In this section, we investigate\nwhether all experts have similar accuracy or not.\nTo achieve this, we compute the accuracy of each\nexpert over all test data points where the expert\nwas activated. Since an expert may contribute to\ndifferent questions with varying weights, we also\nreport the accuracy weighted by the probability\nassigned to each expert. Specifically, the weighted\naccuracy for expert j is defined as:\nPn\ni=1 αij · 1(ˆyi = yi)\nPn\ni=1 αij\n,\n(3)\nwhere αij represents the probability assigned\nto expert j for datapoint i, and 1(ˆyi = yi) is an\n2\n\nLayer\n1\n2\n3\n4\n5\n6\n7\n8\nMean Entropy (top 8)\n1.8516\n1.9375\n1.9297\n1.9531\n1.8516\n1.9219\n2.0156\n1.8984\nStd Entropy (top 8)\n0.0084\n0.0096\n0.0080\n0.0104\n0.0220\n0.0165\n0.0120\n0.0288\nNumber of Activated Experts\n20\n14\n14\n10\n16\n14\n19\n15\nTable 1: Statistical data per layer (Layers 1 to 8). Entropy calculated across the top 8 normalized experts.\nLayer\n9\n10\n11\n12\n13\n14\n15\n16\nMean Entropy (top 8)\n2.0000\n1.9297\n2.0469\n1.9688\n2.0156\n1.9063\n2.0156\n2.0313\nStd of Entropy (top 8)\n0.0092\n0.0245\n0.0092\n0.0233\n0.0135\n0.0354\n0.0151\n0.0133\nNumber of Activated Experts\n19\n12\n11\n14\n15\n29\n24\n25\nTable 2: Statistical data per layer (Layers 9 to 16). Entropy calculated across the top 8 normalized experts.\nindicator function that equals one when the final\nprediction is correct and zero otherwise.\nAdditionally, we report the average contribution\nweight, computed as 100 · pi from Equation 2, for\neach expert when it was activated. Results are\npresented for the first MoE layer (Table 4) and\nthe last MoE layer (Table 5). In these tables, we\ninclude only experts that were activated in at least\n1% of the data (column: \"Appearances\"). There\nare 12 such experts in the first MoE layer and 17 in\nthe last one.\nFor the first MoE layer, 7 experts were activated\nin nearly all cases, meaning they appeared in more\nthan 95% of the data. The top eight experts were\nmainly chosen from three experts with indices:3 19,\n26, and 52. However, the accuracy of these experts\nvaries significantly.\nFor the last MoE layer, only 3 experts were ac-\ntivated in more than 95% of the cases, providing\nthe gating network with more flexibility in select-\ning different experts. In terms of accuracy, we\nobserve a similar pattern to the first MoE layer:\nsome experts achieve significantly higher accuracy\nthan average (e.g., expert 12), while others perform\nconsiderably worse (e.g., experts 34 and 30).\nThese findings suggest that a potential direction\nfor future research could be adjusting the gating\noutput probabilities by increasing the probability\nfor high-accuracy experts and/or decreasing it for\nunderperforming experts. This is particularly rele-\nvant given that the gating probability distribution\nis nearly uniform (see Section 3.1). This unifor-\nmity implies that the probability difference between\nhigh-accuracy experts and the top eight expert is\nrelatively small. For instance, in the last MoE layer,\nthe average gating function output for expert 12,\n3The expert number refers to the index of an expert in an\nMoE layer, ranging from 0 to 63 inclusively.\nwhich performs significantly better than the aver-\nage, is 0.0291, while the average unnormalized\nprobability for the top eight experts is 0.0317.\n4\nConclusion\nIn this paper, we evaluated the contribution of ex-\nperts in an LLM MoE model to the final output on\na quiz-based benchmark. Our key findings are:\n• More than 60% of experts were never acti-\nvated during prediction. This implies that for\nquiz-based tasks, inactive experts can be re-\nmoved, making the model smaller without\nany loss in performance. Additionally, this\ncan significantly reduce training time during\nfine-tuning.\n• The distribution of gating outputs is not sharp\nbut rather nearly uniform across all MoE lay-\ners. Moreover, entropy does not decrease from\nthe first layer to the last. Given that most LLM\nMoE models use a Top-K gating mechanism,\nwhich is a non-continuous gating method, this\nbehavior may negatively impact the robust-\nness of the models.\n• Some experts perform better on average than\nothers, suggesting that adjusting the gating\noutput to prioritize high-accuracy experts\ncould lead to performance improvements.\n3\n\nTop\n1\n2\n3\n4\n5\n6\n7\n8\nMean Probability\n0.19297\n0.17575\n0.13789\n0.11344\n0.10380\n0.09713\n0.09181\n0.08721\nStd Probability\n0.01119\n0.01010\n0.01657\n0.00995\n0.00724\n0.00598\n0.00569\n0.00570\nTable 3: Mean and standard deviation of top 8 normalized probabilities from a gating network from the last MoE\nlayer.\nExpert Number\nAppearances (%)\nAccuracy (%)\nWeighted Accuracy (%)\nMean Weight of the Expert (%)\n19\n30.93\n60.12\n60.17\n5.04\n26\n38.38\n54.83\n54.72\n4.83\n2\n99.70\n52.56\n52.52\n5.43\n36\n99.82\n52.54\n52.48\n5.53\n31\n100.00\n52.52\n52.49\n32.04\n33\n100.00\n52.52\n52.56\n10.53\n56\n100.00\n52.52\n52.49\n19.87\n48\n100.00\n52.52\n52.60\n15.67\n61\n98.72\n52.32\n52.48\n5.90\n49\n2.20\n46.60\n46.44\n4.81\n52\n26.36\n43.00\n43.35\n5.33\n1\n1.69\n33.76\n33.63\n4.86\nTable 4: Statistical data of experts in the first layer.\nExpert Number\nAppearances (%)\nAccuracy (%)\nWeighted Accuracy (%)\nMean Weight of the Expert (%)\n53\n1.44\n79.70\n80.16\n9.21\n12\n27.30\n61.58\n62.56\n9.84\n38\n10.68\n61.40\n62.17\n10.11\n59\n5.60\n60.74\n62.23\n9.55\n9\n46.00\n55.84\n55.82\n9.83\n8\n81.80\n53.77\n55.44\n9.84\n3\n85.50\n53.07\n54.09\n10.73\n17\n100.00\n52.52\n51.45\n18.58\n58\n100.00\n52.52\n52.49\n18.23\n52\n99.00\n52.31\n51.23\n13.24\n60\n73.10\n51.14\n51.24\n9.72\n24\n50.64\n50.92\n51.73\n9.92\n42\n1.17\n50.61\n51.04\n9.35\n34\n62.38\n48.91\n48.98\n10.49\n26\n5.07\n47.47\n47.98\n9.05\n30\n47.51\n46.30\n46.62\n9.22\n51\n1.18\n45.45\n47.13\n9.39\nTable 5: Statistical data of experts in the 16th layer.\n4\n\nLimitations\nThe main limitation of this short paper is that the\nexperiment was conducted on only one model and\none benchmark. Our primary focus was on quiz-\nbased datasets, and we believe that the MMLU\nbenchmark represents this category well. There-\nfore, the use of a single benchmark is not a major\nlimitation. However, a more significant limitation\nis that we evaluated only one LLM MoE model. We\nacknowledge that these results may not generalize\nto other LLM MoE models.\nThe primary reason for using only one LLM\nMoE model is that most other models have a sig-\nnificantly larger number of parameters and require\nsubstantially more computational resources for in-\nference, which we currently do not have.\nReferences\nKeith Conrad. 2004. Probability distributions and maxi-\nmum entropy. Entropy, 6(452):10.\nNan Du, Yanping Huang, Andrew M Dai, Simon Tong,\nDmitry Lepikhin, Yuanzhong Xu, Maxim Krikun,\nYanqi Zhou, Adams Wei Yu, Orhan Firat, et al. 2022.\nGlam: Efficient scaling of language models with\nmixture-of-experts. In International Conference on\nMachine Learning, pages 5547–5569. PMLR.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2020. Measuring massive multitask language under-\nstanding. arXiv preprint arXiv:2009.03300.\nAixin Liu, Bei Feng, Bing Xue, Bingxuan Wang,\nBochao Wu, Chengda Lu, Chenggang Zhao, Chengqi\nDeng, Chenyu Zhang, Chong Ruan, et al. 2024.\nDeepseek-v3 technical report.\narXiv preprint\narXiv:2412.19437.\nNiklas Muennighoff, Luca Soldaini, Dirk Groeneveld,\nKyle Lo, Jacob Morrison, Sewon Min, Weijia Shi,\nPete Walsh, Oyvind Tafjord, Nathan Lambert, et al.\n2024. Olmoe: Open mixture-of-experts language\nmodels. arXiv preprint arXiv:2409.02060.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,\nAndy Davis, Quoc Le, Geoffrey Hinton, and Jeff\nDean. 2017. Outrageously large neural networks:\nThe sparsely-gated mixture-of-experts layer. arXiv\npreprint arXiv:1701.06538.\nYikang Shen, Zhen Guo, Tianle Cai, and Zengyi Qin.\n2024. Jetmoe: Reaching llama2 performance with\n0.1 m dollars. arXiv preprint arXiv:2404.07413.\nZhongwei Wan, Xin Wang, Che Liu, Samiul Alam,\nYu Zheng, Jiachen Liu, Zhongnan Qu, Shen Yan,\nYi Zhu, Quanlu Zhang, et al. 2023.\nEfficient\nlarge language models: A survey. arXiv preprint\narXiv:2312.03863.\nA\nEntropy of distribution across all\nExperts\nIn Table 6 and Table 7, we show that all statements\nregarding entropy across the top 8 experts in Sec-\ntion 3.1 also hold for entropy across the probabili-\nties of all 64 experts given by the gating networks.\nNote that entropy generally increases with the num-\nber of possible outcomes, and for 64 possible out-\ncomes, the upper bound is 4.1589.\n5\n\n1\n2\n3\n4\n5\n6\n7\n8\nMean Entropy\n3.78125\n3.89063\n3.85938\n3.85938\n3.75000\n3.75000\n3.87500\n3.75000\nStd Entropy\n0.01245\n0.01447\n0.01056\n0.01453\n0.02759\n0.03113\n0.02478\n0.02783\nTable 6: Mean and standard deviation of entropy across all gating outputs (Layers 1 to 8).\n9\n10\n11\n12\n13\n14\n15\n16\nMean Entropy\n3.79688\n3.51563\n3.62500\n3.46875\n3.50000\n3.51563\n3.64063\n3.82813\nStd Entropy\n0.03467\n0.08057\n0.06152\n0.05884\n0.07666\n0.07422\n0.05835\n0.03809\nTable 7: Mean and standard deviation of entropy across all gating outputs (Layers 9 to 16).\n6\n",
  "metadata": {
    "source_path": "papers/arxiv/Evaluating_Expert_Contributions_in_a_MoE_LLM_for_Quiz-Based_Tasks_46cf75e9323d5006.pdf",
    "content_hash": "46cf75e9323d50067468fad839da593440469a447fd01e4b102d78e52682fe64",
    "arxiv_id": null,
    "title": "Evaluating_Expert_Contributions_in_a_MoE_LLM_for_Quiz-Based_Tasks_46cf75e9323d5006",
    "author": "",
    "creation_date": "D:20250225025103Z",
    "published": "2025-02-25T02:51:03",
    "pages": 6,
    "size": 196632,
    "file_mtime": 1740470173.362334
  }
}