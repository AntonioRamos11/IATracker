{
  "text": "AnyTop: Character Animation Diffusion with Any Topology\nInbar Gatâˆ—,\nSigal Raabâˆ—,\nGuy Tevet,\nYuval Reshef,\nAmit H. Bermano,\nDaniel Cohen-Or\nTel Aviv University, Israel\nGatinbar2344@gmail.com\nFig. 1. AnyTop generates motions for diverse characters with distinct motion dynamics, using only their skeletal structure as input.\nGenerating motion for arbitrary skeletons is a longstanding challenge in\ncomputer graphics, remaining largely unexplored due to the scarcity of di-\nverse datasets and the irregular nature of the data. In this work, we introduce\nAnyTop, a diffusion model that generates motions for diverse characters\nwith distinct motion dynamics, using only their skeletal structure as in-\nput. Our work features a transformer-based denoising network, tailored\nfor arbitrary skeleton learning, integrating topology information into the\ntraditional attention mechanism. Additionally, by incorporating textual joint\ndescriptions into the latent feature representation, AnyTop learns semantic\ncorrespondences between joints across diverse skeletons. Our evaluation\ndemonstrates that AnyTop generalizes well, even with as few as three train-\ning examples per topology, and can produce motions for unseen skeletons as\nwell. Furthermore, our modelâ€™s latent space is highly informative, enabling\ndownstream tasks such as joint correspondence, temporal segmentation and\nmotion editing. Our webpage, https://anytop2025.github.io/Anytop-page,\nincludes links to videos and code.\n1\nINTRODUCTION\nCharacter animation is a fundamental task in computer animation,\nplaying a crucial role in industries such as film, gaming, and virtual\nreality. Animating 3D characters is a complex and time-consuming\ntask that requires manual high-skill effort. Typically, animation\npipelines involve a unique skeleton for each character, defining its\nmotion span, over which the animation is carefully crafted.\nIn recent years, neural network-based approaches have simplified\nthe animation process, showing impressive results in tasks such\nas motion generation and editing [Dabral et al. 2023; Holden et al.\n2016; Tevet et al. 2023; Zhang et al. 2024a]. However, most existing\nmethods cannot handle different skeletons and focus on a single\ntopology [Kapon et al. 2023; Shafir et al. 2024], target skeletons that\ndiffer only in bone proportions [Tripathi et al. 2025; Yang et al. 2023],\nor rely on skeletal homeomorphism [Aberman et al. 2020].\nWhile effective within their scopes, these methods overlook the\nbroader opportunities presented by diverse character animation,\nwhich require handling a wide variety of skeletal topologies. Con-\nversely, methods designed to handle multiple skeletons often lack\nâˆ—Equal contribution.\nscalability, relying on topology-specific adjustments such as addi-\ntive functional blocks for each skeleton [Li et al. 2024] or entirely\ndistinct instances of the model [Li et al. 2022; Raab et al. 2024].\nThere are two main reasons keeping arbitrary skeleton anima-\ntion generation largely under-explored. First, the irregular nature\nof the data, with skeletons varying in the number of joints and their\nconnectivity, challenges standard methods for processing and anal-\nysis. Second, the lack of datasets encompassing diverse topologies\npresents significant challenges for data-driven approaches.\nIn this work, we introduce AnyTop, a diffusion framework de-\nsigned to generate motions for arbitrary skeletal structures, as illus-\ntrated in fig. 1. AnyTop is carefully designed to handle any skeleton\nin a general manner with no need for topology-specific adjustments.\nAnyTop is based on a transformer encoder, specifically adapted\nfor graph learning. While many works embed an entire pose in\none tensor [Han et al. 2024; Xie et al. 2023], we embed each joint\nindependently at each frame [Aberman et al. 2020; Agrawal et al.\n2024], enabling capturing both joint interactions within the skele-\nton and universal joint behaviors across diverse skeletal structures.\nAnyTop applies attention along both the temporal and skeletal axes.\nNotably, the skeletal attention is between all joints. This is in con-\ntrast to previous art, and is made possible thanks to our topological\nconditioning scheme; we integrate graph characteristics [Park et al.\n2022; Ying et al. 2021a], such as joint parent-child relations, into the\nattention maps. Consequently, each joint has access to information\nfrom all skeletal parts while also being able to prioritize topologi-\ncally closer joints. Furthermore, to bridge the gap between similarly\nbehaved parts in different skeletons, AnyTop incorporates textual\ndescriptions of joints into the latent feature representation.\nAnyTop is trained on Truebones Zoo dataset [Truebones Motions\nAnimation Studios 2022], which includes motion captures of diverse\nskeletal structures. We contribute a processed version, aligned with\nthe popular HumanML3D [Guo et al. 2022] representation, which\nwill be made publicly available. Using quantitative and qualitative\nevaluations, we show that AnyTop outperforms current art.\n1\narXiv:2502.17327v1  [cs.GR]  24 Feb 2025\n\nGat, I. et al.\nOur model demonstrates three forms of generalization in its gen-\nerations: In-skeleton Generalization allows for new motion variants\nthat preserve the characterâ€™s original motion motifs; Cross-skeleton\ngeneralization facilitates generating motions that adapt motifs from\nseveral characters; and Unseen-skeleton generalization enables mo-\ntion generation for skeletons not encountered during training. Be-\nyond its generative capabilities, AnyTopâ€™s highly informative Diffu-\nsion Features (DIFT) [Tang et al. 2023] enable various downstream\napplications, including unsupervised correlation, temporal segmen-\ntation, and motion editing.\nThe approach presented here, and its ability to share information\nacross characters, opens doors for more flexible generation, better\nequipped to learn and operate on more complex characters and\nscenarios, that better fit the real-world needs of 3D content creators.\n2\nRELATED WORK\nSkeletal variability in generative motion models . We refer to\nfour types of skeletal variability (table 1). The naming draws from\nterminology in the graph domain, hence we interchangeably use\nthe terms joint and vertex, as well as edge and bone. A single skele-\nton type refers to identical skeletons â€” that is, skeletons with the\nsame vertices, connectivity, and edge lengths. Isomorphic skeletons\ncorrespond to isomorphic graphs, sharing vertices and edges but po-\ntentially differing in edge proportions. Homeomorphic skeletons may\nvary in structure, yet correspond to homeomorphic graphs, i.e., use\ntopologies obtained from the same primal graph by subdivision of\nedges. Specifically, homeomorphic skeletons share the same number\nof kinematic chains and end-effectors. Finally, non-homeomorphic\nskeletons vary in their structure and have no common primal graph.\nMost motion generative methods focus on a single skeletal struc-\nture [Karunratanakul et al. 2023; Petrovich et al. 2021; Raab et al.\n2023]. Others train on isomorphic skeletons [Villegas et al. 2021;\nZhang et al. 2023b], including works that use the SMPL [Loper et al.\n2015] body model [Jang et al. 2024; Petrovich et al. 2022a; Tripathi\net al. 2025] and SMAL [Zuffi et al. 2017] body model or its derivatives\n[Rueegg et al. 2023; Yang et al. 2023]. A smaller portion of genera-\ntive works support homeomorphic skeletons [Cao and Yang 2024;\nLee et al. 2023; Ponton et al. 2024; Studer et al. 2024; Zhang et al.\n2024d]. Among these works, some [Aberman et al. 2020] require\na designated encoder and decoder per skeleton, and some [Zhang\net al. 2024b] offer a unified framework for all skeletons.\nOnly a handful of works can handle non-homeomorphic skeletons.\nMartinelli et al. [2024] performs motion retargeting by learning\nTable 1. Skeletal Variability. Character skeletons can vary in edge length,\nkinematic chain complexity, or overall topology. Each level of variation\nintroduces greater challenges for motion synthesis. AnyTop can generate\nmotions for dozens of non-homeomorphic skeletons using a single model.\nSkeleton\nVariability type\nEdge lengths\nvariations\nKinematic chains\nvariations\nPrimal skeleton\nvariations\nSingle\nâœ—\nâœ—\nâœ—\nIsomorphic\nâœ“\nâœ—\nâœ—\nHomeomorphic\nâœ“\nâœ“\nâœ—\nNon-homeomorphic\nâœ“\nâœ“\nâœ“\na shared manifold for all skeletons, and decoding it to motions\nusing learned skeleton-specific tokens. The learned tokens capture\nthe skeletal information of characters in the dataset, limiting the\nmodelâ€™s ability to generalize to skeletons unseen during training. Its\nresults are shown exclusively on bipeds, leaving the applicability to\nother character families (e.g., quadrupeds, millipedes) unexplored.\nWalkTheDog [Li et al. 2024] uses a latent space that encodes motion\nphases and accommodates non-generative motion matching.\nA different class of generative models bypasses the handling of the\nskeletal structure by generating motion directly from point clouds\n[Mo et al. 2025], shape-handles [Zhang et al. 2023a] or meshes\n[Muralikrishnan et al. 2024; Song et al. 2023; Ye et al. 2024; Zhang\net al. 2024c]. These works demonstrate great flexibility in target\ncharacter structure, but overlook the advantage of skeletons, which\nare more compact and semantically meaningful, easier to manipulate\nvia rig-based animation, and compatible with physics engines [Tevet\net al. 2024] and inverse kinematics systems. Some works [Wang et al.\n2024] perform automatic rigging after the generation, but automatic\nrigging often necessitates manual adjustments.\nFinally, methods that support arbitrary skeletons [Li et al. 2022;\nRaab et al. 2024] involve a separate training process for each skeleton,\nexhibiting scaling issues and lacking Cross-skeleton generalization.\nAnyTop addresses training on non-homeomorphic skeletons and\nis the only skeletal-based approach capable of generating natural,\nsmooth motions on a diverse range of characters, including bipeds\n(e.g., raptor, bird), quadrupeds (e.g., dog, bear), multi-legged arthro-\npods (e.g., spider, centipede), and limbless creatures (e.g., snakes).\nTo the best of our knowledge, our work is the only one capable of\naccepting an input topology, including unseen ones, and generating\nmotions based on that topology.\nTransformer-based Graph Learning. Early versions of deep net-\nworks on graphs relied on convolutional architectures [Kipf and\nWelling 2016]. The emergence of transformers has sparked a new av-\nenue of research, integrating graphs and transformers. GAT [VeliÄkoviÄ‡\net al. 2018] replace the graph-convolution operation with a self-\nattention module, where attention is restricted to neighboring nodes.\nRong et al. [2020] iteratively stack self-attention layers alongside\ngraph convolutional ones to account for long-range interactions\nbetween nodes. Unlike transformers in the language and imaging\ndomains, and due to the irregular structure of graphs, these earlier\nworks do not use positional encoding.\nSubsequent works [Dwivedi and Bresson 2021; Kreuzer et al. 2021]\nlinearize the graphs into an array of nodes and add absolute posi-\ntional encoding to each node. However, linearization is unnatural\nto the graph structure, requiring a reconsideration of the approach.\nEncoding relative positional information has been explored to\nmaintain positional precision while adhering to the graphâ€™s struc-\nture. Works using it [Park et al. 2022; Shaw et al. 2018; Ying et al.\n2021b] integrate relative positional encoding into the attention map\nbased on relative measures, such as shortest path distance between\nnodes or edge type.\nThe aforementioned approaches are discriminative, applied to\ntasks such as regression and segmentation. AnyTop leverages the\nrelative positional encoding approach for generative tasks and tailors\nit to the motion domain. In particular, our work redefines edge types\n2\n\nAnyTop: Character Animation Diffusion with Any Topology\nSkeletal Temporal Transformer\nBlock\nt\nN+1\nL\nx\nF\nJ\nx0\n Skeletal \nAttention\nJ\nF\nN\nF\nJ\nHips\nNeck\nLeft Wing\nHead\n...\nJoints Names\n Temporal \nAttention\nFF\nLinear\nN+1\nF\nJ\nEnrichment Block\nxt\nT5\n1\nF\nJ\n1\nLinear\nLinear\nLinear\nSkeleton\n,\nFig. 2. Overview. The input to AnyTop is a noised motion ğ‘‹ğ‘¡and the skeleton S = {PS, RS, DS, NS }, where PS refers to the rest-pose, RS denotes jointsâ€™\nrelations, DS defines topological distances between each pair of joints and NS denotes joint names. The Enrichment Block incorporates the skeletal features\ninto the noised motion by concatenating the embedded PS to the sequence as an additional temporal token, and adding a T5-embedded name to each joint.\nThe enriched motion is then passed through a stack of L Skeletal Temporal Transformer layers. We apply skeletal attention along the joint axis to capture\ninteractions between all joints, and incorporate topology information RS and DS to attention maps. Next, we apply temporal attention along the frame axis.\nFinally, the output is projected back to the motion features dimension, facilitating the reconstruction of the motion sequence.\nto capture joint relations within skeletal structures and considers a\ntemporal axis, which is not present in the graph domain.\n3\nMETHOD\nAnyTop is a diffusion model synthesizing motions for multiple dif-\nferent characters with arbitrary skeletons. Given a skeletal structure\nfor input, it generates a natural motion sequence with high fidelity to\nground-truth characters. AnyTop is based on a transformer encoder,\nspecifically adapted for graph learning, as depicted in fig. 2.\n3.1\nPreliminaries\nMotion Representation. We represent motion as a 3D tensor ğ‘‹âˆˆ\nRğ‘Ã—ğ½Ã—ğ·, where ğ‘and ğ½are the maximum number of frames and\njoints across all motions in the dataset, and D is the number of\nmotion features per joint. As motions vary in duration and skele-\ntal structure, we pad the original number of frames and joints of\neach motion to match the maximum values ğ‘and ğ½, respectively.\nWe adopt a redundant representation, where each joint ğ‘—(except\nthe root) consists of its root-relative position ğ‘ğ‘—âˆˆR3, 6D joint\nrotation ğ‘Ÿğ‘—âˆˆR6 [Zhou et al. 2018], linear velocity ğ‘£ğ‘—âˆˆR3, and\nfoot contact label ğ‘“ğ‘ğ‘—âˆˆ{0, 1}. Altogether a joint is represented by\n{ğ‘ğ‘—,ğ‘Ÿğ‘—, ğ‘£ğ‘—, ğ‘“ğ‘ğ‘—} âˆˆR13, hence ğ·= 13. For the root joint, features\ninclude its rotational velocity, linear velocity and height, which\nare concatenated and zero-padded to match the size ğ·. Our repre-\nsentation is inspired by Guo et al. [2022]; however, our approach\nmaintains features at the joint level by representing each joint as\na separate tensor, resulting in ğ½tokens per frame. In contrast, Guo\net al. concatenate features from all joints into one tensor, resulting\nin a single token per frame.\nSkeletal structure Representation. In the context of 3D motion,\ntopology is a directed, acyclic, and connected graph (DAG). Adding\ngeometric information to this graph makes it a skeleton. We use\nthe terms â€œtopology\" and â€œskeleton\" interchangeably throughout\nthis work, clarifying any distinction when necessary. A rest-pose is\nthe characterâ€™s natural pose, represented by (G,ğ‘‚), where G is a\nDAG defining the topological hierarchy and ğ‘‚âˆˆRğ½Ã—3 is a set of 3D\noffsets, specifying each jointâ€™s parent-relative position. In our work,\nwe represent a skeleton by S = {PS, RS, DS, NS}. The first term,\nPS âˆˆRğ½Ã—ğ·, is the rest-pose, converted to the format of individual\nposes in the motion sequence. The second term, RS âˆˆNğ½Ã—ğ½\n0\n, is the\njoints relations, where RS [ğ‘–, ğ‘—] holds the relation type between ğ‘–\nand ğ‘—. We allow six types of relations, which are child, parent, sibling,\nno-relation, self and end-effector. Self and end-effector are valid only\nin case ğ‘–= ğ‘—, and end-effector specifies if the joint is a leaf in GS.\nThe third term, DS âˆˆNğ½Ã—ğ½\n0\n, represents the graph distances, where\nDS [ğ‘–, ğ‘—] holds the topological distance between ğ‘–and ğ‘—in GS, up to\na maximal distance ğ‘‘ğ‘šğ‘ğ‘¥. The topological conditions, RS and DS,\nare illustrated in fig. 3. Finally, NS is the jointsâ€™ textual descriptions,\nwhich are typically included in 3D asset formats (e.g., bvh, fbx).\n3.2\nArchitecture\nAnyTop is a generative Denoising Diffusion Probabilistic Model\n(DDPM) [Ho et al. 2020]. At each denoising step ğ‘¡âˆˆ[1,ğ‘‡] it gets\na noisy motion ğ‘‹ğ‘¡and a skeleton S = {PS, RS, DS, NS} as input,\nand predicts the clean motion Ë†ğ‘‹0 [Tevet et al. 2023] rather than the\nnoise ğœ–ğ‘¡.\nAnyTop consists of two primary components, illustrated in fig. 2.\nThe first is an Enrichment Block, which integrates skeleton-specific\ninformation into the noised motion. The second is a Skeletal Tempo-\nral Transformer Block, which employs attention across both skeletal\nand temporal axes while embedding topological information into\nthe skeletal attention maps.\n3\n\nGat, I. et al.\nSelf Node\nParent Node\nChild Nodes\nSibling Nodes\nSelf Node\nDistance 1\nDistance 2\nDistance 3\nDistance â‰¥4\nFig. 3. Topological Conditions. Joint relations RS (top) and graph dis-\ntances DS (bottom), visualized for a specific joint marked in red. Different\ncolors indicate different values in the row corresponding to the visualized\njoint in the RS, DS matrices.\nEnrichment block. This block incorporates semantic information\nfrom the rest-pose PS and the joint descriptions NS, into the noised\nsample ğ‘‹ğ‘¡. It projects PS to feature lentgh ğ¹and concatenates it\nwith ğ‘‹ğ‘¡along the temporal axis, effectively making it frame 0. The\njoint descriptions NS are encoded by a T5 model, projected to length\nğ¹, and added to their corresponding joint features across all frames.\nFinally, the block outputs enhanced data of shape R(ğ‘+1)Ã—ğ½Ã—ğ¹.\nSkeletal Temporal Transformer block. The inputs to this block\nare the embedded tokens of ğ‘‹ğ‘¡emitted from the Enrichment Block,\nthe diffusion step ğ‘¡, and the precomputed values DS, RS. The block\ncomprises a stack of ğ¿identical Skeletal Temporal Transformer (STT)\nencoder layers, each consisting of three parts. The first component\nis a Skeletal Attention, which performs spatial self-attention across\njoints within the same frame. Unlike concurrent approaches that\nlimit attention or convolution to adjacent joints within the skeletal\nhierarchy, our method enables each joint to attend to all others,\ncapturing long-range relations. To regain the local joint knowledge,\nwe incorporate topology information RS and DS into the attention\nmaps. This allows each joint to access information from all skeletal\nparts while also prioritizing topologically closer joints.\nThe second component is a Temporal Attention, which applies\nself-attention along the temporal axis for each joint independently,\nobserving its motion over time. To enhance efficiency and mitigate\noverfitting, the temporal attention is applied within a temporal\nwindow of length ğ‘Š. The third component is a feed-forward block.\nFinally, the output is projected to the original motion dimension,\nenabling motion reconstruction.\nTopological Conditioning Scheme. We extend transformers for\ngraph-based learning by incorporating both graph topology and\nnode interaction information through our Skeletal Attention mecha-\nnism. Inspired by discriminative works in the graphs domain [Ying\net al. 2021b], AnyTop introduces a novel method for generative\ntasks, specifically tailored to the motion domain. We integrate graph\nproperties directly into attention maps, enabling the structural char-\nacteristics of the graph to influence the learning process. Our work\nuses two types of node affinity, the topological distance, DS, and\nrelations, RS, as detailed in section 3.1. We incorporate the graph\ninformation into the attention maps [Park et al. 2022], by learn-\ning distinct query and key embeddings for distances, denoted by\nğ¸D\nğ‘, ğ¸D\nğ‘˜âˆˆRğ‘‘ğ‘šğ‘ğ‘¥Ã—ğ¹, and embeddings for relation, denoted by ğ¸R\nğ‘,\nğ¸R\nğ‘˜âˆˆR6Ã—ğ¹, where ğ¸(Â·)\nğ‘\nand ğ¸(Â·)\nğ‘˜\ndenote embeddings that relate to\nqueries and keys, respectively, and ğ¹is the latent feature size. These\nembeddings are used to form two new attention maps, ğ‘D and ğ‘R\ndefined for a given pair of joints ğ‘–, ğ‘—âˆˆ[ğ½]:\nğ‘D\nğ‘–ğ‘—= ğ‘ğ‘–Â· ğ¸D\nğ‘[Dğ‘–ğ‘—] + ğ‘˜ğ‘—Â· ğ¸D\nğ‘˜[Dğ‘–ğ‘—],\n(1)\nğ‘R\nğ‘–ğ‘—= ğ‘ğ‘–Â· ğ¸R\nğ‘[Rğ‘–ğ‘—] + ğ‘˜ğ‘—Â· ğ¸R\nğ‘˜[Rğ‘–ğ‘—],\n(2)\nwhere ğ‘ğ‘–, ğ‘˜ğ‘—denote the ğ‘–â€™th joint query and ğ‘—â€™th joint key, respec-\ntively, and [Â·] denotes an index in the embedding matrix. Finally, we\nincorporate graph information by adding the two attention maps to\nthe standard attention map and scaling their sum:\nğ‘ğ‘–ğ‘—=\nğ‘ğ‘–Â· ğ‘˜ğ‘—+ ğ‘D\nğ‘–ğ‘—+ ğ‘R\nğ‘–ğ‘—\nâˆš\nğ¹\n.\n(3)\nThe final attention score is computed by applying the standard\nrow-wise softmax to ğ‘ğ‘–ğ‘—.\n3.3\nTraining\nData Sampling and Augmentations. We train AnyTop using\nminibatches sampled with a Balancing Sampler to address the imbal-\nanced nature of the data (described in section 6.1) and mitigate the\ndominance of specific skeletons. To further enhance generalization,\nwe apply skeletal augmentations to the data samples, including ran-\ndomly removing 10% to 30% of the joints and adding new joints at\nthe midpoint of existing edges. Further details on our data augmen-\ntation are provided in Appendix B .\nTraining Objectives. Given a motion ğ‘‹0 of skeleton S, its noised\ncounterpart ğ‘‹ğ‘¡, with diffusion step ğ‘¡âˆ¼[1,ğ‘‡], our model predicts the\nclean motion, Ë†ğ‘‹0 = ğ´ğ‘›ğ‘¦ğ‘‡ğ‘œğ‘(ğ‘‹ğ‘¡,ğ‘¡, S). Our main objective is defined\nby the simple formulation [Ho et al. 2020], namely,\nLğ‘ ğ‘–ğ‘šğ‘ğ‘™ğ‘’= ğ¸ğ‘¡âˆ¼[1,ğ‘‡]\n\r\r Ë†ğ‘‹0 âˆ’ğ‘‹0\n\r\r2\n2 .\n(4)\nThe Mean Squared Error (MSE) over rotations does not directly\ncorrelate to their distance in the rotation space, hence we apply\na geodesic loss [Huang et al. 2017; Tripathi et al. 2025] over the\nlearned rotations. Let ğ‘Ÿ, Ë†ğ‘ŸâˆˆRğ‘Ã—ğ½Ã—6 denote the 6D rotations of ğ‘‹0\nand Ë†ğ‘‹0 respectively. The geodesic loss is defined as follows:\nLğ‘Ÿğ‘œğ‘¡=\nğ‘\nâˆ‘ï¸\nğ‘›=1\nğ½âˆ‘ï¸\nğ‘—=1\narccos ğ‘‡ğ‘Ÿ(ğºğ‘†(ğ‘Ÿğ‘›,ğ‘—)(ğºğ‘†(Ë†ğ‘Ÿğ‘›,ğ‘—)ğ‘‡) âˆ’1\n2\n,\n(5)\nwhereğºğ‘†is the Gram-Schmidt process, used to convert 6D rotations\nto rotation matrices [Zhou et al. 2019], and ğ‘‡ğ‘Ÿis the matrix Trace\noperation. Overall, the final training objective is\nL = Lğ‘ ğ‘–ğ‘šğ‘ğ‘™ğ‘’+ ğœ†ğ‘Ÿğ‘œğ‘¡Lğ‘Ÿğ‘œğ‘¡.\n(6)\n4\nANALYSIS\n4.1\nLatent Space Analysis\nIn this section, we examine AnyTopâ€™s latent space and show that\nit features a unified manifold for joints across all skeletons. We\nuse DIFT [Tang et al. 2023], a framework designed for detecting\n4\n\nAnyTop: Character Animation Diffusion with Any Topology\nFig. 4. Spatial Correspondence. Monkey (top left) depicts the reference\nskeleton, while the fox, scorpion, and bird depict different target skeletons.\nTarget skeleton joints are color-coded to match their corresponding joints\nin the reference. For better visualization, we color the bones to match their\nadjacent joints. Note the correspondence in limbs, spine, and tail.\ncorrespondence in the latent space of models undergoing diffusion.\nDIFT features are intermediate activations from layerğ‘™ğ‘ğ‘œğ‘Ÿğ‘Ÿ, extracted\nduring a single denoising pass on a sample that has been noised\ndirectly to diffusion step ğ‘¡ğ‘ğ‘œğ‘Ÿğ‘Ÿ. These features serve as effective\nsemantic descriptors for predicting correspondence. Note that the\nvalues we choose for ğ‘™ğ‘ğ‘œğ‘Ÿğ‘Ÿand ğ‘¡ğ‘ğ‘œğ‘Ÿğ‘Ÿalign with those used in the\noriginal DIFT work. Let ğ‘‹ğ‘Ÿğ‘’ğ‘“denote a reference motion, and let\nğ‘‹ğ‘¡ğ‘”ğ‘¡denote a motion in which we search for corresponding parts.\nLet ğ‘†ğ‘Ÿğ‘’ğ‘“,ğ‘†ğ‘¡ğ‘”ğ‘¡denote their skeletons, respectively.\nOur spatial and temporal correspondence results are illustrated\nin figs. 4 and 5 respectively, and in the supplementary video.\nSpatial Correspondence. We show that manifold features of se-\nmantically similar skeletal joints across different characters are close\nto each other. Our objective is to find the most similar joint in Sğ‘Ÿğ‘’ğ‘“\nfor each joint in Sğ‘¡ğ‘”ğ‘¡. To achieve this, we extract DIFT features\nfor both motions ğ‘‹ğ‘Ÿğ‘’ğ‘“,ğ‘‹ğ‘¡ğ‘”ğ‘¡at diffusion step ğ‘¡ğ‘ğ‘œğ‘Ÿğ‘Ÿ= 2 and layer\nğ‘™ğ‘ğ‘œğ‘Ÿğ‘Ÿ= 0, average them along the temporal axis, and obtain a single\nfeature vector per joint. Using cosine similarity, we detect the closest\ncounterpart for each joint in ğ‘†ğ‘¡ğ‘”ğ‘¡.\nTemporal Correspondence. We show that AnyTop can recognize\npose-level similarities and identify analogous actions across different\nskeletons. This time, our objective is to find the most similar frame\nin ğ‘‹ğ‘Ÿğ‘’ğ‘“for each frame in ğ‘‹ğ‘¡ğ‘”ğ‘¡. To accomplish this goal, we extract\nDIFT features at diffusion step ğ‘¡ğ‘ğ‘œğ‘Ÿğ‘Ÿ= 3 and layer ğ‘™ğ‘ğ‘œğ‘Ÿğ‘Ÿ= 1, and\naverage them along the skeletal axis, resulting in a single feature\nvector per frame. We use cosine similarity to detect the closest\ncounterpart for each frame in ğ‘‹ğ‘¡ğ‘”ğ‘¡.\n4.2\nGeneralization Forms\nWe identify three forms of generalization in our generated motions.\nIn-skeleton Generalization. dubbed in-gen, refers to generaliza-\ntion within a specific skeleton, featured as both temporal composition\nâ€“ combining motion segments from dataset instances, and spatial\ntime\nFig. 5. Temporal Correspondence. Monkey (top row) features the refer-\nence motion, while the Crab and Lynx represent two target motions. The\nframes of the targets are color-coded to align with their corresponding\nreference frames. Note the correspondence: aggressive motion segments\nare pink, idle frames blue, and transitional frames green.\nAnyTop Generation (ours)\nGround Truth Motion A\nGround Truth Motion B\nSinMDM Generation\ntime\ntime\nFig. 6. In-skeleton Generalization. The top row depicts two ground truth\nchicken motions: pecking (left) and walking (right). The bottom row presents\nsynthesized motions of an adapted SinMDM (left) and AnyTop (right). The\nemphasized frames in AnyTop demonstrate spatial composition of walking\nand pecking, introducing novel poses not present in the ground truth. Sin-\nMDM embeds entire poses, hence cannot spatially-compose joints.\ncomposition â€“ introducing novel poses by combining skeletal parts\nof ground truth poses. Notably, spatial composition is enabled by our\nper-joint encoding, which provides the flexibility required for such\ndiversity. In fig. 6 and in our supp. video, we showcase AnyTopâ€™s\nin-gen and highlight how other methods, which embed the entire\npose, fail to achieve a comparable variety.\nCross-skeleton generalization. dubbed cross-gen, is expressed through\nshared motion motifs across different characters. This form of gen-\neralization enables the adaptation of motion behaviors originally\nperformed by other skeletons, as shown in fig. 7 and our supp.\nvideo. When motions must strictly align with typical behaviors, the\ntraining dataset can be restricted accordingly.\nUnseen-skeleton generalization. extends to skeletons not encoun-\ntered during training, and illustrated in fig. 8 and the video.\n5\nAPPLICATIONS\nAnyTop enables various downstream tasks; we demonstrate two.\n5\n\nGat, I. et al.\nCross-skeleton Generalization\nNearest Ground Truth\ntime\ntime\nFig. 7. Cross-skeleton generalization. Right: a generated motion featur-\ning an action not in the performing skeletonâ€™s ground truth. Left: notably,\nthe nearest ground truth originates from a different character.\nTemporal Segmentation. Temporal segmentation is the task of\npartitioning a temporal sequence into disjoint groups, where frames\nsharing similar characteristics are grouped. For a clean sample ğ‘‹0,\neither generated or given, and skeleton S, we extract DIFT features\nat diffusion step ğ‘¡ğ‘ ğ‘’ğ‘”=3 and layer ğ‘™ğ‘ ğ‘’ğ‘”=1. The features are averaged\nalong the joint dimension to produce a single feature vector per\nframe. We apply PCA for dimensionality reduction and then use\nK-means to cluster the frames into ğ‘˜=3 categories. Our results are\nvisualized in fig. 9 and in the supp. video. This application reinforces\nsection 4, showing that AnyTopâ€™s latent features are effective frame\ndescriptors. However, in section 4, frames are grouped by similarity\nto ğ‘‹ğ‘Ÿğ‘’ğ‘“, while here they are grouped by similarity to each other.\nEditing. We demonstrate our methodâ€™s versatility through two mo-\ntion editing applications: in-betweening for temporal manipulation\nand body-part editing for spatial modifications, both leveraging the\nsame underlying approach. For in-betweening, the prefix and suffix\nof the motion are fixed, allowing the model to generate the middle.\nFor body-part editing, we fix some of the joints and let the model\ngenerate the rest. Given a fixed subset (temporal or spatial) of the\nmotion sequence tokens, we override the denoised Ë†ğ‘¥0 at each sam-\npling iteration with the fixed motion part. This approach ensures\nfidelity to the fixed input while synthesizing the missing elements\nof the motion. Our results, in fig. 10 and the supp. video, show a\nsmooth and natural transition between the given and the synthe-\nsized parts, and demonstrate that our model successfully generalizes\ntechniques previously limited to human skeletons [Tevet et al. 2023]\nto accommodate diverse skeletal structures.\n6\nEXPERIMENTS\n6.1\nDataset and Preprocessing\nThe Truebones Zoo [Truebones Motions Animation Studios 2022]\ndataset comprises motion captures featuring 70 diverse skeletons,\nincluding mammals, birds, insects, dinosaurs, fish, and snakes. The\nnumber of motions per skeleton ranges from 3 to 40, adding up\nto 1219 motions and 147,178 frames in total. The dataset includes\nvariations in orientation, root definition, and scale. Additionally, the\nskeletons vary in joint order, naming conventions, and connectivity\nstandards. To address these variations, we have performed compre-\nhensive preprocessing of the data, including aligning all motions to\nthe same orientation and average bone length, centering the first\nAdapted MDM\nAnyTop (Ours)\nFig. 8. Unseen-skeleton generalization Zero-shot inference of the cat\n(left) and komodo dragon (right) using AnyTop (top) and adapted MDM\nbaseline (bottom). AnyTopâ€™s generated motions maintain natural appearance\nwhile MDMâ€™s generated motions are static and jittery.\nframe at the origin, and ensuring it is located on the ground. This\nprocess is described in details in Appendix B and the processed\ndata will be made available.\nSkeletal Subsets. In addition to experimenting with the full dataset,\nwe categorize the skeletons into four groups based on their mo-\ntion dynamics and train AnyTop on these subsets, alongside a\nmodel trained on the entire dataset. The four skeletal categories are\nQuadrupeds, Bipeds, Flying, and Insects. These subsets allow us to\nconstrain cross-gen to characters with similar behavior. Our visu-\nalizations illustrate generations from models trained on the entire\ndataset or sub-datasets, depending on the context (e.g., fig. 8).\n6.2\nImplementation details\nWe use ğ‘‡= 100 diffusion steps, ğ¿= 4 STT layers, and latent dimen-\nsion ğ¹= 128. We train the model using a single NVIDIA RTX A6000\nGPU for 24 hours. Inference runs on an NVIDIA GeForce RTX 2080\nTi GPU. More implementation details can be found in Appendix A .\n6.3\nEvaluation\nBenchmark. To evaluate AnyTop, we introduce a benchmark com-\nprising 30 skeletons randomly selected from those with cumulative\nframe counts ranging between 600 and 1200. The benchmark in-\ncludes 43% Quadrupeds, 17% Bipeds, 23% Flying, and 17% Insects,\nreflecting the relative proportions of these categories in the dataset.\nMetrics. We report four metrics that measure different aspects of\nthe generated motions, following Li et al. [2022]; Raab et al. [2024].\nThe metrics are calculated separately for each skeleton, and the\nmean and standard deviation across all tested skeletons are reported\nin the form meanÂ±std. For each skeleton, we evaluate a number of\nsamples proportional to its sample count in the dataset. Let ğ‘€,ğº\ndenote the group of ground truth (GT) and generated motions of\nthe assessed skeleton, respectively. The metrics that we use are\n(a) coverage, which is the rate of temporal windows in ğ‘€, that are\nreproduced in ğº, (b) local diversity, which is the average distance\nbetween windows in ğºand their nearest neighbors in ğ‘€, and (c)\ninter diversity, the diversity between synthesized motions. We define\nintra diversity to be the diversity between sub-windows internal to\na motion and define (d) intra diversity diff, which is the difference\nbetween the intra diversity of ğºand that of ğ‘€. Metrics (a) and (d)\nevaluate fidelity to the GT, while metrics (b) and (c) assess diversity.\n6\n\nAnyTop: Character Animation Diffusion with Any Topology\nStand\nFall\nOn the ground\nIdle\nStretch\nGrowl\nIdle\nFall\nOn the ground\ntime\nFig. 9. Temporal Segmentation. Temporal clustering on a tyrannosaurus,\na cat, and an anaconda snake, using K-means on PCA-reduced DIFT features.\nAn ideal score features both high fidelity and high diversity. High\nfidelity with low diversity suggests overfitting, while low fidelity\nwith high diversity indicates divergence and noise.\n6.4\nBaselines\nTo the best of our knowledge, no current works address such a\ndiverse range of skeletal structures within a single model. Hence,\nwe compare AnyTop to adaptations of two baselines. The first is\nMDM [Tevet et al. 2023], originally designed for a single humanoid\nskeleton. MDM uses per-frame embedding, so to match its represen-\ntation format, we concatenate all joint features for each character,\nand pad them to a length of ğ½Ã— ğ·. For fairness, we also concate-\nnate the vectorized rest-pose embedding PS along the temporal\naxis as frame 0. Since MDM accepts textual conditions, we use the\nskeletonâ€™s name (e.g., Cat, Dragon) as the input text. Additionally,\nsince MDMâ€™s original configuration was designed for a dataset 14\ntimes larger than ours [Petrovich et al. 2022b], we reduced its latent\ndimension size to mitigate overfit.\nThe second baseline is SinMDM [Raab et al. 2024], designed to be\ntrained on a single motion sequence. We modify it to enable training\non multiple sequences of the same character, resulting in a separate\nmodel for each skeleton.\n6.5\nQuantitative Results\nTable 2 shows a quantitative comparison of AnyTop and the base-\nlines. AnyTop outperforms MDM in all categories and SinMDM in\nall but coverage, which is expected since SinMDM is trained sepa-\nrately for each skeleton. Note the significant gap in diversity metrics,\nwhere the table shows AnyTop generalizes well, while the others\nTable 2. Comparison with baselines. Our model clearly outperforms the\nbaselines. Bold and underline denote best and second best, respectively. âˆ—\nindicates the work was adapted to align with the terms of our experiment.\nModel\nCoverage â†‘\nLocal\nDiv.\nâ†‘\nInter\nDiv.\nâ†‘\nIntra Div.\nDiff.\nâ†“\n#Param.\n(M)\nâ†“\nMDMâˆ—[2023]\n71.3Â±31\n0.168Â±0.12\n0.139Â±0.13\n0.177Â±0.08\n5.96\nSinMDMâˆ—[2024]\n89.3Â±15\n0.080Â±0.13\n0.280Â±0.13\n0.144Â±0.09\n176.1 (5.87 Ã— 30)\nAnyTop (Ours)\n80.5Â±20\n0.252Â±0.14\n0.312Â±0.17\n0.118Â±0.07\n2.28\nGiven\nGenerated\nFig. 10. Editing. Top: In-betweening. Given motion prefix and suffix, AnyTop\ncan generate the middle frames. Bottom: Body part editing. Given the\nmotion of the lower body, AnyTop can generate its complement for the\nupper body. Both editing strategies produce smooth and natural transition\nbetween the given and the synthesized parts.\ntime\ntime\nAnyTop Generation (ours)\nMDM Generation\nFig. 11. Comparison with MDM Baseline. AnyTop (right) generates nat-\nural motions, while MDM (left) produces static, jittery motions.\nstruggle to do so. We also report the modelsâ€™ parameter count, show-\ning ours uses fewer parameters, enabling lower computation and\nfaster inference. In Appendix C , we provide a comparison with the\nbaselines on the data subsets, demonstrating our modelâ€™s superiority\non these as well.\n6.6\nQualitative Results\nOur supp. video reflects the quality of our results. It presents gener-\nated motions for various skeletons and comparisons to baselines.\nIn fig. 11, we show that AnyTop produces natural and lively\nmotions while MDM produces static, jittery motions. Moreover,\nMDMâ€™s results in our video show jittery transitions and unnatural\nposes. Figure 6 and our supp. video show that AnyTop can generate\nnovel poses by effectively combining joints from different ground\ntruth poses. In contrast, SinMDM is limited to temporal in-skeleton\ngeneralization and cannot handle spatial composition, due to its\nreliance on per-frame features. Moreover, since SinMDM trains a\nseparate model per skeleton, it cannot feature cross-skeleton or\nunseen-skeleton generalization. As accurate foot contact is one of\nthe major factors of motion quality, we follow Li et al. [2022]; Raab\net al. [2023] and use an IK post-process to ensure proper contact.\nUnseen skeleton. We present two unseen skeleton motions. One is\na komodo dragon, generated by the Bipeds model. The second is a\n7\n\nGat, I. et al.\nCat, generated by a model trained on Quadrupeds, excluding the cat.\nFigure 8 and our supp. video demonstrate AnyTop generalizes well\nto unseen skeletons, while adapted MDM under the same settings\ngenerates static and jittery motions.\n6.7\nAblation\nIn table 3, we explore three key components of AnyTopâ€™s architec-\nture. First, the results confirm that without access to topological\ninformation, the model struggles to prioritize joints based on their\nhierarchical relations. Omitting the incorporation of D and R leads\nto degradation in all metrics. Next, excluding the rest pose PS pro-\nduces inferior results, reinforcing the idea that PS encodes vital\ninformation about joint offsets and bone lengths. Lastly, we examine\ncross-skeletal prior sharing via the addition of joint name embed-\ndings. While cross-gen improves motion diversity, it introduces a\ntradeoff, as generated motions may exhibit motifs absent in the\nskeletonâ€™s ground truth, reducing coverage. Results show that re-\nmoving joint name embeddings increases coverage but severely\nsacrifices diversity and cross-skeleton generalization.\n7\nCONCLUSION, LIMITATIONS AND FUTURE WORK\nWe have presented AnyTop, a generative model that synthesizes\ndiverse characters with distinct motion dynamics using a skeletal\nstructure as input. It uses a transformer-based denoising network,\nintegrating graph information at key points in the pipeline. Our\nevaluation shows a highly informative latent space and notable\ngeneralization, even for characters with few or no training samples.\nOne limitation of our method stems from imperfections in the\ninput data. Despite our cleaning procedure, certain data artifacts\nremain unresolved. Another limitation is that our data augmentation\nprocess is computationally expensive with ğ‘‚(ğ½2) complexity.\nIn the future, we plan to use AnyTop for skeletal retargeting,\nmulti-character interaction, editing, and various control modalities\nsuch as text-based and music-driven animation. Another potential\ndirection is editing animations by simply modifying joint labels\nin the text descriptions. Finally, future work could further explore\nDIFT features in the motion domain.\n8\nACKNOWLEDGEMENTS\nThis research was supported in part by the Israel Science Foundation\n(grants no. 2492/20 and 3441/21), Len Blavatnik and the Blavatnik\nfamily foundation, and the Tel Aviv University Innovation Labora-\ntories (TILabs).\nTable 3. Ablation. Removing architectural choices leads to a degradation\nin AnyTopâ€™s performance.\nModel\nCoverage â†‘\nLocal\nDiv.\nâ†‘\nInter\nDiv.\nâ†‘\nIntra Div.\nDiff.\nâ†“\nw/o graph property\nembedding (D, R)\n76.8Â±23\n0.249Â±0.14\n0.303Â±0.17\n0.127Â±0.11\nw/o rest-pose token\n77.2Â±25\n0.250Â±0.14\n0.292Â±0.18\n0.130Â±0.09\nw/o joint name\nembedding\n82.3Â±17\n0.218Â±0.12\n0.276Â±0.15\n0.113Â±0.06\nAnyTop (Ours)\n80.5Â±20\n0.252Â±0.14\n0.312Â±0.17\n0.118Â±0.07\nREFERENCES\nKfir Aberman, Peizhuo Li, Dani Lischinski, Olga Sorkine-Hornung, Daniel Cohen-Or,\nand Baoquan Chen. 2020. Skeleton-aware networks for deep motion retargeting.\nACM Transactions on Graphics (TOG) 39, 4 (2020), 62â€“1.\nDhruv Agrawal, Jakob Buhmann, Dominik Borer, Robert W Sumner, and Martin Guay.\n2024. SKEL-Betweener: a Neural Motion Rig for Interactive Motion Authoring. ACM\nTransactions on Graphics (TOG) 43, 6 (2024), 1â€“11.\nYu Cao and MingHui Yang. 2024. CAR: Collision-Avoidance Retargeting for Varied\nSkeletal Architectures. In SIGGRAPH Asia 2024 Technical Communications. ACM,\nNew York, NY, USA, 1â€“5.\nRishabh Dabral, Muhammad Hamza Mughal, Vladislav Golyanik, and Christian\nTheobalt. 2023. MoFusion: A Framework for Denoising-Diffusion-based Motion Syn-\nthesis. In Computer Vision and Pattern Recognition (CVPR). IEEE Computer Society,\nWashington, DC, USA.\nVijay Prakash Dwivedi and Xavier Bresson. 2021. A Generalization of Transformer\nNetworks to Graphs. In AAAI Workshop on Deep Learning on Graphs: Methods and\nApplications. AAAI Press, Washington, DC, USA.\nChuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. 2022.\nGenerating Diverse and Natural 3D Human Motions From Text. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE Computer\nSociety, Washington, DC, USA, 5152â€“5161.\nBo Han, Hao Peng, Minjing Dong, Yi Ren, Yixuan Shen, and Chang Xu. 2024. AMD:\nAutoregressive Motion Diffusion. In Proceedings of the AAAI Conference on Artificial\nIntelligence, Vol. 38. AAAI Press, Washington, DC, USA, 2022â€“2030.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic\nmodels. Advances in Neural Information Processing Systems 33 (2020), 6840â€“6851.\nDaniel Holden, Jun Saito, and Taku Komura. 2016. A deep learning framework for\ncharacter motion synthesis and editing. ACM Transactions on Graphics (TOG) 35, 4\n(2016), 1â€“11.\nZhiwu Huang, Chengde Wan, Thomas Probst, and Luc Van Gool. 2017. Deep Learning\non Lie Groups for Skeleton-Based Action Recognition. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (CVPR). IEEE Computer\nSociety, Washington, DC, USA.\nInseo Jang, Soojin Choi, Seokhyeon Hong, Chaelin Kim, and Junyong Noh. 2024.\nGeometry-Aware Retargeting for Two-Skinned Characters Interaction. ACM Trans-\nactions on Graphics (TOG) 43, 6 (2024), 1â€“17.\nRoy Kapon, Guy Tevet, Daniel Cohen-Or, and Amit H. Bermano. 2023.\nMAS:\nMulti-view Ancestral Sampling for 3D motion generation using 2D diffusion.\narXiv:2310.14729 [cs.CV]\nKorrawe Karunratanakul, Konpat Preechakul, Supasorn Suwajanakorn, and Siyu Tang.\n2023.\nGuided Motion Diffusion for Controllable Human Motion Synthesis. In\nProceedings of the IEEE/CVF International Conference on Computer Vision. IEEE\nComputer Society, Washington, DC, USA, 2151â€“2162.\nThomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph\nconvolutional networks.\nDevin Kreuzer, Dominique Beaini, Will Hamilton, Vincent LÃ©tourneau, and Prudencio\nTossou. 2021. Rethinking graph transformers with spectral attention. Advances in\nNeural Information Processing Systems 34 (2021), 21618â€“21629.\nSunmin Lee, Taeho Kang, Jungnam Park, Jehee Lee, and Jungdam Won. 2023. Same:\nSkeleton-agnostic motion embedding for character animation. In SIGGRAPH Asia\n2023 Conference Papers. ACM, New York, NY, USA, 1â€“11.\nPeizhuo Li, Kfir Aberman, Zihan Zhang, Rana Hanocka, and Olga Sorkine-Hornung.\n2022. GANimator: Neural Motion Synthesis from a Single Sequence. ACM Transac-\ntions on Graphics (TOG) 41, 4 (2022), 138.\nPeizhuo Li, Sebastian Starke, Yuting Ye, and Olga Sorkine-Hornung. 2024. WalkTheDog:\nCross-Morphology Motion Alignment via Phase Manifolds. In ACM SIGGRAPH 2024\nConference Papers. ACM, New York, NY, USA, 1â€“10.\nMatthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J\nBlack. 2015. SMPL: A skinned multi-person linear model. ACM transactions on\ngraphics (TOG) 34, 6 (2015), 1â€“16.\nGiulia Martinelli, Nicola Garau, NiccolÃ³ Bisagno, and Nicola Conci. 2024. MoMa:\nSkinned motion retargeting using masked pose modeling. Computer Vision and\nImage Understanding 249 (2024), 104141.\nClinton Mo, Kun Hu, Chengjiang Long, Dong Yuan, and Zhiyong Wang. 2025. Motion\nKeyframe Interpolation for Any Human Skeleton via Temporally Consistent Point\nCloud Sampling and Reconstruction. In European Conference on Computer Vision.\nSpringer, Springer International Publishing, Berlin/Heidelberg, Germany, 159â€“175.\nSanjeev Muralikrishnan, Niladri Dutt, Siddhartha Chaudhuri, Noam Aigerman, Vladimir\nKim, Matthew Fisher, and Niloy J. Mitra. 2024. Temporal Residual Jacobians for Rig-\nFree Motion Transfer. In Computer Vision â€“ ECCV 2024: 18th European Conference,\nMilan, Italy, September 29â€“October 4, 2024, Proceedings, Part LVIII (Milan, Italy).\nSpringer-Verlag, Berlin, Heidelberg, 93â€“109.\nhttps://doi.org/10.1007/978-3-031-\n73636-0_6\nWonpyo Park, Woong-Gi Chang, Donggeon Lee, Juntae Kim, and Seungwon Hwang.\n2022. GRPE: Relative Positional Encoding for Graph Transformer. In ICLR2022\nMachine Learning for Drug Discovery. OpenReview.net, OpenReview.net.\n8\n\nAnyTop: Character Animation Diffusion with Any Topology\nMathis Petrovich, Michael J. Black, and GÃ¼l Varol. 2021. Action-Conditioned 3D Human\nMotion Synthesis with Transformer VAE. In International Conference on Computer\nVision (ICCV). IEEE Computer Society, Washington, DC, USA, 10985â€“10995.\nMathis Petrovich, Michael J. Black, and GÃ¼l Varol. 2022a. TEMOS: Generating diverse\nhuman motions from textual descriptions. In European Conference on Computer\nVision (ECCV). Springer International Publishing, Berlin/Heidelberg, Germany.\nMathis Petrovich, Michael J. Black, and GÃ¼l Varol. 2022b. TEMOS: Generating diverse\nhuman motions from textual descriptions. arXiv:2204.14109 [cs.CV] https://arxiv.\norg/abs/2204.14109\nJose Luis Ponton, Eduard Pujol, Andreas Aristidou, Carlos Andujar, and Nuria Pelechano.\n2024. Dragposer: Motion reconstruction from variable sparse tracking signals via\nlatent space optimization.\nSigal Raab, Inbal Leibovitch, Peizhuo Li, Kfir Aberman, Olga Sorkine-Hornung, and\nDaniel Cohen-Or. 2023. MoDi: Unconditional Motion Synthesis from Diverse Data.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR). IEEE Computer Society, Washington, DC, USA, 13873â€“13883.\nSigal Raab, Inbal Leibovitch, Guy Tevet, Moab Arar, Amit H Bermano, and Daniel\nCohen-Or. 2024. Single Motion Diffusion. In The Twelfth International Conference\non Learning Representations (ICLR). OpenReview.net, OpenReview.net.\nhttps://\nopenreview.net/pdf?id=DrhZneqz4n\nYu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and\nJunzhou Huang. 2020. Self-supervised graph transformer on large-scale molecular\ndata. Advances in neural information processing systems 33 (2020), 12559â€“12571.\nNadine Rueegg, Silvia Zuffi, Konrad Schindler, and Michael J Black. 2023. Barc: Breed-\naugmented regression using classification for 3d dog reconstruction from images.\nInternational Journal of Computer Vision 131, 8 (2023), 1964â€“1979.\nYoni Shafir, Guy Tevet, Roy Kapon, and Amit Haim Bermano. 2024. Human Motion\nDiffusion as a Generative Prior. In The Twelfth International Conference on Learning\nRepresentations. OpenReview.net, OpenReview.net.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-Attention with Relative\nPosition Representations. In Proceedings of NAACL-HLT. Association for Computa-\ntional Linguistics, Pennsylvania, USA, 464â€“468.\nChaoyue Song, Jiacheng Wei, Ruibo Li, Fayao Liu, and Guosheng Lin. 2023. Unsu-\npervised 3d pose transfer with cross consistency and dual reconstruction. IEEE\nTransactions on Pattern Analysis and Machine Intelligence 45, 8 (2023), 10488â€“10499.\nJustin Studer, Dhruv Agrawal, Dominik Borer, Seyedmorteza Sadat, Robert W Sumner,\nMartin Guay, and Jakob Buhmann. 2024. Factorized Motion Diffusion for Precise and\nCharacter-Agnostic Motion Inbetweening. In Proceedings of the 17th ACM SIGGRAPH\nConference on Motion, Interaction, and Games. ACM, New York, NY, USA, 1â€“10.\nLuming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan.\n2023. Emergent Correspondence from Image Diffusion.\nGuy Tevet, Sigal Raab, Setareh Cohan, Daniele Reda, Zhengyi Luo, Xue Bin Peng,\nAmit H Bermano, and Michiel van de Panne. 2024. CLoSD: Closing the Loop\nbetween Simulation and Diffusion for multi-task character control.\nGuy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and Amit Haim\nBermano. 2023. Human Motion Diffusion Model. In The Eleventh International\nConference on Learning Representations (ICLR). OpenReview.net, OpenReview.net.\nhttps://openreview.net/forum?id=SJ1kSyO2jwu\nShashank Tripathi, Omid Taheri, Christoph Lassner, Michael J. Black, Daniel Holden,\nand Carsten Stoll. 2025. HUMOS: Human Motion Model Conditioned on Body\nShape. In European Conference on Computer Vision. Springer, Springer International\nPublishing, Berlin/Heidelberg, Germany, 133â€“152.\nTruebones Motions Animation Studios. 2022. Truebones. https://truebones.gumroad.\ncom/ Accessed: 2022-1-15.\nPetar VeliÄkoviÄ‡, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro LiÃ²,\nand Yoshua Bengio. 2018. Graph Attention Networks. In International Conference\non Learning Representations. OpenReview.net, OpenReview.net. https://openreview.\nnet/forum?id=rJXMpikCZ\nRuben Villegas, Duygu Ceylan, Aaron Hertzmann, Jimei Yang, and Jun Saito. 2021.\nContact-Aware Retargeting of Skinned Motion. In 2021 IEEE/CVF International\nConference on Computer Vision (ICCV). IEEE Computer Society, Washington, DC,\nUSA, 9700â€“9709. https://api.semanticscholar.org/CorpusID:237513547\nHaoyu Wang, Shaoli Huang, Fang Zhao, and Chun Yuan. 2024. MMR: Multi-scale\nMotion Retargeting between Skeleton-agnostic Characters. In 2024 International\nJoint Conference on Neural Networks (IJCNN). IEEE, IEEE, Washington, DC, USA,\n1â€“8.\nYiming Xie, Varun Jampani, Lei Zhong, Deqing Sun, and Huaizu Jiang. 2023. Omni-\nControl: Control Any Joint at Any Time for Human Motion Generation. In The\nTwelfth International Conference on Learning Representations. OpenReview.net, Open-\nReview.net.\nZhangsihao Yang, Mingyuan Zhou, Mengyi Shan, Bingbing Wen, Ziwei Xuan, Mitch\nHill, Junjie Bai, Guo-Jun Qi, and Yalin Wang. 2023. OmniMotionGPT: Animal Motion\nGeneration with Limited Data. arXiv:2311.18303 [cs.CV]\nZijie Ye, Jia-Wei Liu, Jia Jia, Shikun Sun, and Mike Zheng Shou. 2024. Skinned Motion\nRetargeting with Dense Geometric Interaction Perception.\nChengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming\nShen, and Tie-Yan Liu. 2021a. Do Transformers Really Perform Badly for Graph\nRepresentation?. In Thirty-Fifth Conference on Neural Information Processing Systems.\nCurran Associates Inc., NY, USA. https://openreview.net/forum?id=OeWooOxFwDa\nChengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming\nShen, and Tie-Yan Liu. 2021b. Do transformers really perform badly for graph\nrepresentation? Advances in neural information processing systems 34 (2021), 28877â€“\n28888.\nHaodong Zhang, Zhike Chen, Haocheng Xu, Lei Hao, Xiaofei Wu, Songcen Xu, Rong\nXiong, and Yue Wang. 2024b. Unified Cross-Structural Motion Retargeting for\nHumanoid Characters. IEEE Transactions on Visualization and Computer Graphics 1\n(2024).\nJiaxu Zhang, Shaoli Huang, Zhigang Tu, Xin Chen, Xiaohang Zhan, Gang Yu, and Ying\nShan. 2023a. TapMo: Shape-aware Motion Generation of Skeleton-free Characters.\nJiaxu Zhang, Junwu Weng, Di Kang, Fang Zhao, Shaoli Huang, Xuefei Zhe, Linchao\nBao, Ying Shan, Jue Wang, and Zhigang Tu. 2023b. Skinned motion retargeting with\nresidual perception of motion semantics & geometry. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. IEEE Computer Society,\nWashington, DC, USA, 13864â€“13872.\nJia-Qi Zhang, Miao Wang, Fu-Cheng Zhang, and Fang-Lue Zhang. 2024d. Skinned\nMotion Retargeting with Preservation of Body Part Relationships. IEEE Transactions\non Visualization and Computer Graphics 1 (2024).\nMingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang,\nand Ziwei Liu. 2024a. MotionDiffuse: Text-driven human motion generation with\ndiffusion model. IEEE Transactions on Pattern Analysis and Machine Intelligence 1\n(2024).\nXinyi Zhang, Naiqi Li, and Angela Dai. 2024c. DNF: Unconditional 4D Generation with\nDictionary-based Neural Fields.\nYi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li. 2018. On the Continuity\nof Rotation Representations in Neural Networks. CoRR abs/1812.07035 (2018).\narXiv:1812.07035 http://arxiv.org/abs/1812.07035\nYi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li. 2019. On the continuity\nof rotation representations in neural networks. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. IEEE Computer Society,\nWashington, DC, USA, 5745â€“5753.\nSilvia Zuffi, Angjoo Kanazawa, David W Jacobs, and Michael J Black. 2017.\n3D\nmenagerie: Modeling the 3D shape and pose of animals. In Proceedings of the IEEE\nconference on computer vision and pattern recognition. IEEE Computer Society, Wash-\nington, DC, USA, 6365â€“6373.\nAPPENDIX\nThis Appendix provides additional details to complement the in-\nformation presented in the main paper. While the main paper is\nself-contained, the details provided here offer further insights and\nclarifications.\nIn Appendix A, we provide implementation details of AnyTop, and\nin Appendix B, we elaborate on our preprocessing and augmentation\npipelines. Finally, in Appendix C and present additional quantitative\nresults beyond those in the main paper.\nA\nIMPLEMENTATION DETAILS\nThe maximum topological distance we allow in D is ğ‘‘ğ‘šğ‘ğ‘¥= 6,\nand our Temporal Attention is applied on temporal windows of\nlength ğ‘Š= 31. For our model inputs, we allow maximum number\nof joints ğ½= 143. During training, we use cropped sequences of\nğ‘= 40 frames. To enable our model handle higher frame positions\nand generate longer sequences, we incorporate positional encoding\nrelative to the cropping index. For training, we used batch size of\n16 when training on the entire dataset, and a batch size of 8 to train\nin the data subsets.\nB\nDATA\nTruebones Zoo dataset. In addition to the data misalignment issues\ndiscussed in the main paper, the dataset also contains vulnerabilities\nsuch as excessive dummy joints, qualitative artifacts like foot sliding\nand floating, and 20% of the frames involve skeletons connected\n9\n\nGat, I. et al.\nto the origin via an additional bone, resulting in artefacts such as\nwalking or running in place. We address some of these issues as\npart of our data processing pipeline, which is detailed in the main\npaper and further extended in the following paragraph.\nData Preprocessing. In this section, we provide further details on\nthe preprocessing steps mentioned in the main paper, as well as\ndescribe additional refinements applied to the dataset. As part of\nthe alignment process, we ensure that all skeletons are properly\ngrounded. This is achieved by using the textual descriptions of the\njoints to identify the foot joints of each skeleton. Based on their\nheight in the rest pose, we determine the ground height for each\nskeleton and subtract it from the corresponding root height in the\nmotion data. For skeletons that do not interact with the ground, such\nas flying birds or swimming fish, the ground height is determined by\nthe position of the lowest joint in the rest pose. Another important\npreprocessing step is ensuring that the rest poses of all skeletons are\nnatural. This is essential for two key reasons. First, many animals\nfeature a similar span of rotation angles in organs that have similar\nfunctionality, e.g., the forearm. We would like this span of rotations\nto constitute a manifold representing multiple animals. Once all\nrotation angles are defined relative to a characterâ€™s rest-pose, we\nhave a common representation basis, hence the desired manifold\ncan be obtained.\nSecond, the rest pose is encoded as a single frame within the\nmotion sequence. To maintain consistency with the other frames,\nwhich represent natural poses, the rest pose must also exhibit a\nnatural configuration. To accomplish this, we transform all motion\nrotations so that they are relative to a natural rest pose, which can\neither be provided as an additional motion capture (mocap) file or\nselected from the skeletonâ€™s idle motion.\nIn addition to the alignment procedure, we also extract relevant\ninformation from the skeletons and motion data. First, we use foot\njoints labels to generate foot-contact indicators for each frame,\nwhich are concatenated with the motion features. Next, we compute\nthe mean and standard deviation for each skeletonâ€™s frames and use\nthese statistics to normalize the motions before feeding them into\nthe model during training.\nInput Preprocessing. The input to our model is a skeleton S =\nPS, RS, DS, NS, derived from the raw rest pose of the character,\nrepresented as (GS,ğ‘‚S), along with the corresponding joint names.\nBoth (GS,ğ‘‚S) and the joint names can be obtained from standard\nmotion capture formats (e.g., bvh, fbx).\nThe skeletal features S = {PS, RS, DS, NS} are computed as\nfollows: First, to compute PS, we apply forward kinematics with\nzero rotations on (GS,ğ‘‚S) obtaining the global joint positions in\nthe rest pose. These positions are then converted to root-relative co-\nordinates. To align the rest pose with the format of individual frames\nin a motion sequence, we append to each root-relative position a\n6D representation of zero rotation, zero velocity, and foot contact\nindicators. The topological conditions, RS and DS, are derived\nthrough a traversal of the skeletal hierarchy GS. Finally, the joint\nnames NS are extracted from the motion capture data and undergo\ntext-preprocessing, which includes the removal of digits, symbols,\nirrelevant words, and redundant prefixes. Additionally, side indica-\ntors such as â€™L/Râ€™ are replaced with â€™Left/Rightâ€™, non-English joint\nnames are translated, and similar actions are standardized.\nData augmentation. Skeletal Augmentation exposes our model\nto a wider variety of skeletons, as described in the Method section\nof the main paper. Next, we further elaborate about this process.\nThe first augmentation we apply is joint removal, which randomly\nremoves up to 30% of the joints from the skeleton, where feet joints\nare never removed to maintain physical correctness. For efficiency\nconsideration, we exclude joints with more than a single child from\nthe removal procedure. The second augmentation is joint addition,\nwhich introduces a new joint at the midpoint of a randomly selected\nedge. After removing or adding joints to the skeleton, we update RS,\nDS and NS accordingly. Note that updating DS is computationally\nexpensive with a complexity of 0(ğ½2), as it requires recomputing\nthe path between each pair of joints in the DAG.\nC\nCOMPARISON WITH BASELINES ON SUBSET\nMODELS\nWe provide a quantitative evaluation of AnyTop trained on the\ndata subsets defined in the Experiments section of the main paper.\nTo maintain fairness in comparison, we train the adapted MDM\nbaseline separately for each subset. Since SinMDM is independently\ntrained for each skeleton, no additional adjustments are needed.\nEach model is evaluated using the corresponding skeletons from our\nbenchmark that match the relevant data subset. The results, shown\nin table 4, indicate that AnyTop achieves the optimal coverage-\ndiversity tradeoff compared to all other baselines presented.\nTable 4. Comparison on Data Subsets. Quantitative results of AnyTop\ntrained on different data subsets, compared to the baselines trained under\nequivalent settings. âˆ—indicates the work has been adjusted to our experi-\nmental terms and â€  indicates that a specific skeleton (Scorpion) has been\nremoved from the SinMDM evaluation set, as SinMDM fails to converge\non this skeleton. This exclusion ensures that its impact does not skew the\noverall score.\nSubset\nModel\nCoverage â†‘\nLocal\nDiv.\nâ†‘\nInter\nDiv.\nâ†‘\nIntra Div.\nDiff.\nâ†“\nQuadrupeds\nMDMâˆ—\n83.3Â±23\n0.103Â±0.14\n0.112Â±0.07\n0.160Â±0.03\nSinMDMâˆ—\n94.0Â±06\n0.050Â±0.04\n0.230Â±0.12\n0.151Â±0.08\nAnyTop\n89.2Â±09\n0.215Â±0.08\n0.291Â±0.17\n0.114Â±0.06\nBipeds\nMDMâˆ—\n87.9Â±13\n0.034Â±0.01\n0.081Â±0.03\n0.108Â±0.05\nSinMDMâˆ—\n95.0Â±05\n0.040Â±0.02\n0.251Â±0.12\n0.090Â±0.03\nAnyTop\n93.5Â±05\n0.191Â±0.09\n0.288Â±0.19\n0.120Â±0.06\nFlying\nMDMâˆ—\n63.7Â±31\n0.219Â±0.25\n0.193Â±0.18\n0.154Â±0.08\nSinMDMâˆ—\n78.9Â±18\n0.071Â±0.04\n0.320Â±0.13\n0.095Â±0.03\nAnyTop\n72.6Â±18\n0.289Â±0.13\n0.410Â±0.19\n0.166Â±0.07\nInsects\nMDMâˆ—\n88.4Â±07\n0.063Â±0.03\n0.185Â±0.10\n0.117Â±0.05\nSinMDM âˆ—\n77.8Â±04\n0.235Â±0.29\n0.419Â±0.08\n0.152Â±0.05\nSinMDM âˆ—â€ \n92.9Â±03\n0.061Â±0.015\n0.348Â±0.10\n0.136Â±0.06\nAnyTop\n90.6Â±09\n0.189Â±0.07\n0.317Â±0.117\n0.127Â±0.05\n10\n",
  "metadata": {
    "source_path": "papers/arxiv/AnyTop_Character_Animation_Diffusion_with_Any_Topology_9eda4fdea6a63c05.pdf",
    "content_hash": "9eda4fdea6a63c05af7855e5fab47f69d38d45463a09dda8e4058fcfa2073b2c",
    "arxiv_id": null,
    "title": "AnyTop: Character Animation Diffusion with Any Topology",
    "author": "",
    "creation_date": "D:20250225030133Z",
    "published": "2025-02-25T03:01:33",
    "pages": 10,
    "size": 8927711,
    "file_mtime": 1740470161.0655146
  }
}