{
  "text": "Neural Attention: A Novel Mechanism for Enhanced\nExpressive Power in Transformer Models\nAndrew J. DiGiugno\n∗Department of Computer Science\nUniversity of Bridgeport\nBridgeport, CT 06604\nadigiugn@my.bridgeport.edu\nAusif Mahmood\nDepartment of Computer Science\nUniversity of Bridgeport\nBridgeport, CT 06604\nmahmood@bridgeport.edu\nAbstract\nTransformer models typically calculate attention matrices using dot products, which\nhave limitations when capturing nonlinear relationships between embedding vec-\ntors. We propose Neural Attention, a technique that replaces dot products with\nfeed-forward networks, enabling a more expressive representation of relationships\nbetween tokens. This approach modifies only the attention matrix calculation\nwhile preserving the matrix dimensions, making it easily adaptable to existing\ntransformer-based architectures. We provide a detailed mathematical justification\nfor why Neural Attention increases representational capacity and conduct con-\ntrolled experiments to validate this claim. When comparing Neural Attention and\nDot-Product Attention, NLP experiments on WikiText-103 show a reduction in\nperplexity of over 5 percent. Similarly, experiments on CIFAR-10 and CIFAR-100\nshow comparable improvements for image classification tasks. While Neural Atten-\ntion introduces higher computational demands, we develop techniques to mitigate\nthese challenges, ensuring practical usability without sacrificing the increased ex-\npressivity it provides. This work establishes Neural Attention as an effective means\nof enhancing the predictive capabilities of transformer models across a variety of\napplications.\n1\nIntroduction\nTransformers have revolutionized artificial intelligence, delivering groundbreaking advances in Nat-\nural Language Processing (NLP) and computer vision tasks. At the core of these models lies the\nattention mechanism, which captures relationships between embedding vectors that represent data\nwith spatial dependencies. The most widely used implementation, Dot-Product Attention, is founda-\ntional to the success of transformers [9]. However, it operates within a constrained representational\nspace, limiting its ability to model nonlinear relationships, and reducing the expressivity of the model.\nWe introduce Neural Attention, a novel technique designed to enhance the expressive capacity of\ntransformers. By replacing dot products with learnable weight matrices in feed-forward networks,\nNeural Attention enables transformers to better capture intricate, nonlinear dependencies between\nembedding vectors. Unlike most current research, which is focused on improving the computational\nefficiency of attention mechanisms, our method prioritizes enhancing their representational power,\nwhich is a critical factor in advancing the foundational capabilities of transformers. In this work,\nwe clearly demonstrate this added expressivity on both NLP and vision tasks. While Neural Atten-\ntion exhibits higher computational and memory requirements, we develop an implementation that\nsignificantly reduces this additional overhead, as detailed in Section 3.\n∗Printprint. This work has not been peer-reviewed.\n1\narXiv:2502.17206v1  [cs.LG]  24 Feb 2025\n\n1.1\nTransformer Basics\nA key challenge in modeling sequential data is that the meaning of each data point is dependent on\nthe data that surrounds it. For example, in text, the meaning of each word depends on the context\nprovided by surrounding words. In an image, the meaning of each pixel depends on the surrounding\npixels. Transformers address this challenge by converting data sequences of length n, into embedding\nvectors of length d. These embedding vectors become contextually aware representations of each\nelement in the sequence and are arranged into three learnable matrices: Query (Q), Key (K), and\nValue (V ), each with size Rn×d. Each matrix can be viewed as a different representation of the\nsame input sequence. An \"attention\" operation is performed between Q and K, which creates the\nattention matrix A ∈Rn×n. This is a matrix of \"attention scores\", which quantifies the importance\nof each token in the sequence with respect to the others [9]. This attention matrix is then used to\nweight the V matrix, producing a new representation of the sequence as: softmax (A/\n√\nd)V ∈Rn×d.\nThis process is repeated through multiple layers, with each layer refining the embeddings to capture\nincreasingly complex relationships. An output layer is then used to make a prediction. Traditionally,\nthe attention scores in matrix A are computed using the matrix product between Q and K ⊤, as shown\nin Equation 1. The key contribution of this work is to explore a different method for calculating this\nmatrix, which allows for a better contextual understanding of the data.\nScaled Dot-Product Attention(Q, K, V ) = Softmax\n\u0012QK⊤\n√\nd\n\u0013\nV\n(1)\n1.2\nTheoretical Justification of Neural Attention\nIn Equation 2, we demonstrate a vector ⃗q ∈Rd×1 from the Q matrix, and a vector ⃗k ∈Rd×1 from\nthe K matrix, producing an attention score through a dot product operation. Note that due to the\nconstraint i = j, it is only possible for elements of the same index to interact when mapping two\nvectors to a scalar in this way.\nAttention Score =\nd\nX\ni,j;i=j\n⃗qi · ⃗kj\n(2)\nBy using the dot product, we are creating an attention score that measures global, linear, dependencies\nbetween embedding vectors. This potentially misses relationships between individual elements,\nespecially those where i ̸= j. With Dot-Product Attention, transformers rely solely on their ability\nto learn embeddings capable of expressing all relevant cross-positional dependencies through a dot\nproduct.\nNeural Attention allows the model to learn a unique function that maps two vectors to a scalar,\ncapable of modeling nonlinear dependencies that a dot product cannot capture. It does this by first\nconcatenating them into the vector ⃗qkconcat ∈R2d×1. We then pass ⃗qkconcat into a hidden layer using\na learnable weight matrix Wh ∈Rh×2d and bias⃗bh ∈Rh×1, where h is is number of neurons in the\nhidden layer. The vector ⃗h ∈Rh×1 encodes information about dependencies between ⃗q and ⃗k. This\ncan be seen in Equation 3, where σ represents a nonlinear activation function.\n⃗h = σ\n\u0010\nWh · concat(⃗q,⃗k) +⃗bh\n\u0011\n(3)\nThe vector ⃗h is then projected to a scalar using a learnable vector of weights ⃗w⊤\na ∈R1×h and bias ba.\nThis produces an attention score that allows the model to account for local, nonlinear dependencies\nbetween embedding vectors, as shown in Equation 4. A detailed visual comparison of Dot-Product\nAttention and Neural Attention can be seen in Appendix A.\nAttention Score = ⃗w⊤\na σ\n\u0010\nWh · concat(⃗q,⃗k) +⃗bh\n\u0011\n+ ba\n(4)\nWe illustrate the geometric differences between Dot-Product Attention and Neural Attention by\nconsidering an example with an embedding dimension of 2, where ⃗q = [q1, q2] and ⃗k = [k1, k2]. In\nthis case, (⃗q,⃗k) ∈R2 × R2, and the scalar output z ∈R. For f(⃗q,⃗k) = z, this defines the mapping:\nf : R2 × R2 →R\n2\n\nWhen including both the input embeddings and the scalar output, we consider the 5-dimensional\nspace R5, where each point is represented as (q1, q2, k1, k2, z). By restricting the embeddings to the\ncase q1 = k1 = x and q2 = k2 = y, we can visualize the attention as a 3D manifold embedded in\nR5, as seen in Figure 1. In this case, Dot-Product Attention is given by z = ⃗q · ⃗k = x2 + y2, which\ncorresponds to a smooth, continuous 3D manifold, embedded in R5. Specifically, it is a paraboloid.\nIn contrast, Neural Attention introduces a nonlinear mapping:\nz = ⃗w⊤\na σ\n\u0010\nWh · concat(⃗q,⃗k) +⃗bh\n\u0011\n+ ba = ⃗w⊤\na σ\n\n\nWh ·\n\n\nx\ny\nx\ny\n\n+⃗bh\n\n\n+ ba\nThis enables z to exist on a far more flexible 3D manifold in R5, defined by the learned parameters\nWh, ⃗w⊤\na ,⃗bh, and ba. Unlike the paraboloid produced by Dot-Product Attention, this manifold can\ncapture intricate, nonlinear relationships, including sharp transitions and complex geometries. In\nFigure 1, the left plot shows the smooth paraboloid surface modeled by Dot-Product Attention and\nthe right plot illustrates the potential complexity of a manifold modeled by Neural Attention.\nz = x2 + y2\nx axis\n4\n2\n0\n2\n4\ny axis\n4\n2\n0\n2\n4\nz axis\n10\n20\n30\n40\n50\nz = ⃗w⊤\na σ\n\n\nWh ·\n\n\nx\ny\nx\ny\n\n+⃗bh\n\n\n+ ba\nx axis\n4\n2\n0\n2\n4\ny axis\n4\n2\n0\n2\n4\nz axis\n5\n0\n5\n10\nFigure 1: A smooth parabolic surface (left) versus a complex surface with sharp edges (right).\n2\nRelated Work\nThe attention mechanism was first introduced within the context of machine translation by Bahdamau\net al. [1], when it was used for dynamic alignment of words between input and output sequences.\nThis work introduced the concept of \"additive attention\", in which two vectors are independently\nprocessed by their own feed-forward networks to produce scalars that are summed to calculate\nthe final attention score. With Neural Attention, instead of processing each vector separately, we\nconcatenate the two vectors and perform a nonlinear operation on the combined representation,\nprojecting them jointly into a space that encodes their relationship. This approach captures richer\ndependencies between vectors, whereas additive attention ultimately relies on a linear combination of\nindependently projected representations.\nBuilding upon this foundation, Vaswani et al. [9] proposed the transformer architecture, eliminating\nthe need for recurrence while still allowing flexibility of input sequence length. This innovation\nsignificantly improved the ability of sequence processing to be parallelized, addressing some of\nthe bottlenecks in recurrent models. In this work, they adopted Dot-Product Attention as the\n3\n\nmechanism for calculating attention scores, citing its practical advantages over additive attention\nwhen considering computational and memory efficiency. However, their work did not explore whether\nadditive attention could offer advantages in expressivity, leaving the representational limitations\nof Dot-Product Attention largely unexamined. Neural Attention addresses this gap by prioritizing\nenhanced expressivity, even as we tackle the associated complexity challenges, as explained in\nsection 3.\nWhile these foundational innovations laid the groundwork for modern transformers, much of the\nsubsequent research has shifted toward improving computational efficiency, rather than addressing\nrepresentational limitations. Fastformer [11] explored additive attention to achieve faster processing,\nwhile SwiftFormer [7] revisited additive attention for real-time applications in mobile vision tasks.\nSimilarly, Xu et al. [12] proposed additive and convolution-based attention mechanisms tailored for\nvision transformers, focusing on practical implementation benefits. Linformer [10] addressed the\nO(n2) complexity of self-attention by employing low-rank approximations to reduce computational\nand memory costs. More recently, Mahmood and Huang [5] introduced a segmented attention\napproach, computing attention only on pairs of consecutive overlapping segments to further optimize\ncomputational efficiency. While these efforts underscore the ongoing interest in rethinking attention\nmechanisms, they largely prioritize efficiency improvements over representational power. Neural\nAttention distinguishes itself by directly addressing the expressivity limitations of standard attention\nmechanisms, providing a novel approach to modeling complex, nonlinear relationships.\nIn the current literature, there are not many examples of work solely focused on improving the\nexpressive power of transformers by exploring alternative approaches to the attention mechanism.\nHowever, RoFormer [8] improved the capability of transformers to capture long-term dependencies\nby introducing a novel technique for preserving relative position relationships in self-attention calcu-\nlations. gMLP [4] replaced the attention mechanism entirely by using multi-layer perceptions with\ngating. This work relates to ours in that it uses feed-forward networks to learn spatial dependencies.\nThis provides a justification for our work, however, it is a fundamentally different approach and\ncannot be easily adapted into existing attention-based transformer architectures, as ours can be.\nMoreover, their implementation does not allow for nor was it tested on autoregressive tasks. The\nwork most similar to ours is Möbius Attention [3] which sought to improve the expressive power of\nattention mechanisms by adding a nonlinear Möbius transformation and calculating attention scores\nin a complex space. This differs from our work in that they only apply the nonlinear transformation to\nthe embedding vectors in the Q matrix, creating a richer representation prior to attention calculation.\nUnlike Neural Attention, which adds a nonlinear transformation into the attention calculation itself.\n3\nMethodology\n3.1\nImplementation of Neural Attention\nWe begin with the Query (Q) and Key (K) matrices, commonly used in transformer models. We\ndefine these matrices to have a sequence length n and an embedding dimension d. A linear down-\nprojection is performed on both matrices along the embedding dimension to make subsequent steps\nless resource-intensive. We define the resulting matrices as Q′ and K ′, and the reduced embedding\ndimension as d′. This dimensionality reduction is shown below in equations 5 and 6.\nQ′ = QWq,\nQ ∈Rn×d,\nWq ∈Rd×d′,\nQ′ ∈Rn×d′\n(5)\nK ′ = KWk,\nK ∈Rn×d,\nWk ∈Rd×d′,\nK ′ ∈Rn×d′\n(6)\nWe then reshape Q′, making it a tensor by adding a singleton dimension and giving it the form\nQ′ ∈Rn×1×d′. We reshape K ′, making it a tensor by adding a singleton dimension and giving it the\nform K′ ∈R1×n×d′. By broadcasting along the singleton dimensions, we get the resulting forms:\nQ′ ∈Rn×n×d′ and K′ ∈Rn×n×d′. Below, this process is written step by step:\nQ′ ∈Rn×d′\nreshape\n−−−−→\nQ′ ∈Rn×1×d′\nbroadcast\n−−−−−→\nQ′ ∈Rn×n×d′\nK ′ ∈Rn×d′\nreshape\n−−−−→\nK′ ∈R1×n×d′\nbroadcast\n−−−−−→\nK′ ∈Rn×n×d′\nIn Figure 2, we demonstrate matrices Q′ (shown in orange) and K ′ (shown in blue) undergoing the\nreshape and broadcast process and becoming tensors. These matrices are defined to have n = 3 and\n4\n\nd′ = 2. The red arrows signify what data is contained in each dimension. Indices are included for\nclarity. Note that broadcasting along perpendicular axes can be seen as analogous to transposing the\nK ′ matrix in Dot-Product Attention implementations. This is represented by the \"copies\" axis.\nQ11\nQ21\nQ12\nQ22\nQ31\nQ32\nK11\nK21\nK12\nK22\nK31\nK32\nd′\nn\nn\nd′\nFigure 2: Reshape and broadcast process of matrices Q′ and K ′ into tensors Q′ and K′. Prime\nsymbols are left out for better readability.\nAfter the reshape and broadcast steps, we then create a tensor C by concatenating the Q′ and K′\ntensors along their embedding dimension. The tensor resulting from applying Equation 7 to the Q′\nand K′ tensors in Figure 2, is shown in Figure 3. The axes i, j, and k are added for clarity, as they\nwill be used in subsequent equations to index this tensor.\nC = concat(Q′, K′, axis = −1),\nC ∈Rn×n×2d′\n(7)\nFigure 3: Tensor C, created after concatenating tensors Q′ and K′ from Figure 2 along their\nembedding dimension.\nWe define two learnable weight tensors, Wh ∈Rn×n×h×2d′ and Wa ∈Rn×n×1×h, which we will\nuse to calculate the final attention matrix. For every (i, j)-th slice of tensor C along the k axis,\nC[i, j, :], there is a corresponding matrix of h × 2d′ learnable weights located at Wh[i, j, :, :], and a\ncorresponding vector of 1 × h learnable weights located at Wa[i, j, :, :]. After processing tensor C\nusing Equation 8, we define the resulting matrix of attention scores as A. A visual example using this\nequation is given in Appendix B. Note that bias terms are left out of the methodology for simplicity,\nbut are used in the implementation.\nA ←AttentionScoreij = Wa[i, j] · σ(Wh[i, j] · C[i, j]),\nA ∈Rn×n\n(8)\nThe final calculation for Neural Attention is given by Equation 9. Note that the only difference\nbetween this and the Scaled Dot-Product Attention, seen in Equation 1, is the numerator in the\nsoftmax function. Also note that A can be derived from Q and K. Both of these equations produce a\nmatrix of size n × n, making Neural Attention easy to implement in any architecture currently using\nDot-Product Attention.\n5\n\nNeural Attention(Q, K, V ) = Softmax\n\u0012 A\n√\nd\n\u0013\nV\n(9)\nTo provide further clarity, the below pseudocode shows how the attention matrix is calculated in our\nimplementation of Neural Attention.\nAlgorithm 1 Calculating Attention Scores\nRequire: Query matrix Q ∈Rbatch×heads×seq_length×embedding_dim\nRequire: Key matrix K ∈Rbatch×heads×seq_length×embedding_dim\nEnsure: Attention scores A ∈Rbatch×heads×seq_length×seq_length\n1: Perform linear down-projection of Q and K along the embedding dimension:\nQ′ = QWq,\nWq ∈Rembedding_dim×reduced_dim,\nQ′ ∈Rbatch×heads×seq_length×reduced_dim\nK′ = KWk,\nWk ∈Rembedding_dim×reduced_dim,\nK′ ∈Rbatch×heads×seq_length×reduced_dim\n2: Reshape Q′ and K′: Add singleton dimensions and repeat along sequence length\nQ′\nreshape\n−−−−→Rbatch×heads×seq_length×1×reduced_dim\nbroadcast\n−−−−−→Rbatch×heads×seq_length×seq_length×reduced_dim\nK′\nreshape\n−−−−→Rbatch×heads×1×seq_length×reduced_dim\nbroadcast\n−−−−−→Rbatch×heads×seq_length×seq_length×reduced_dim\n3: Concatenate Q′ and K′ along the embedding dimension (axis=-1):\nconcat(Q′, K′, axis = −1) →C ∈Rbatch×heads×seq_length×seq_length×2·reduced_dim\n4: Pass all vectors in the final dimension of C through their own feed-forward networks:\nHidden Layer: Apply a linear transformation and nonlinear activation function\nOutput Layer: Apply a linear transformation to reduce the final dimension to 1\n5: Squeeze the final dimension to output attention scores A:\nA ∈Rbatch×heads×seq_length×seq_length×1\nsqueeze\n−−−−→A ∈Rbatch×heads×seq_length×seq_length\n3.2\nOvercoming Complexity Challenges\nBoth Dot-Product Attention and Neural Attention have identical time complexities of O(n2) with\nrespect to the sequence length. However, Neural Attention has a much worse memory requirement\ndue to the large intermediate tensor C, seen in Equation 7. To overcome this challenge, Neural\nAttention is used only in the first layer of our transformer model, with all subsequent layers using\nDot-Product Attention. This approach is used in combination with down-projection, as seen in\nEquations 5 and 6. In our testing, we find that even when projecting to an embedding dimension\nas low as d′ = 2, Neural Attention produces better results than when no down-projection is used at\nall. This implies that Neural Attention is low-rank, able to make meaningful contributions to the\nmodel with a small embedding dimension. For these reasons, it is possible to take advantage of the\nincreased expressive power of this technique in a scalable manner, as evidenced in Section 4.3.\n4\nExperiments and Results\nThe effectiveness of Neural Attention is tested in both a generative NLP context and an image\nclassification context. This is beneficial because it provides a diversity of data types and training\nmodalities. The former requires attention to be used in an autoregressive context, while the latter\ndoes not. In every test presented in this section, all training, architecture, and data are identical for\nboth Neural Attention and Dot-Product Attention. The only difference being that Neural Attention\nis applied in the first layer only, with subsequent layers using Dot-Product Attention, as detailed in\nSection 3.2. None of the models use pretraining, all are trained from scratch. In these results, the\nfocus is not on raw performance indicators (perplexity for NLP and accuracy for image classification),\nbut rather on the relative improvement seen when using Neural Attention vs Dot-Product Attention.\n6\n\n4.1\nGenerative NLP Testing\nThe tests in this section are performed using a decoder-only, multi-headed, causal masked self-\nattention transformer model, similar to that described by Radford et al. [6]. Training is performed\nin an autoregressive manner and utilizes sinusoidal positional encoding. All tests use 8 transformer\nlayers, each one having 8 attention heads. A sequence length of 1024 and a batch size of 16 is used.\nGenerative performance is benchmarked using perplexity measurements every 10,000 iterations while\ntraining on the WikiText-103 dataset for 1,000,000 iterations in total. An embedding dimension of\n512 is used for tokens and an embedding dimension of d = 64 per head is used in the Q, K, and V\nmatrices. Neural Attention is tested without the down-projection described by Equations 5 and 6, as\nwell as with a value of d′ = 16 and a value d′ = 2. Note that when no down-projection is used, we\ndo not apply Equations 5 and 6 with a value of d′ = d, but instead skip this step entirely, substituting\nout Q′ and K′ for Q and K. Training graphs for all tests can be found in Appendix C.\nTable 1: Comparison of perplexity results after 1 million iterations.\nMethod\nModel Size (params)\nDown-Projection (d′)\nLowest Perplexity\nDot-Product Attention\n46.528M\nNone (d = 64)\n26.03\nNeural Attention\n47.052M\n2\n24.71\nNeural Attention\n47.055M\n16\n24.55\nNeural Attention\n47.062M\nNone (d = 64)\n24.77\nTable 2: Percent improvement in perplexity over Dot-Product Attention for various Neural Attention\nconfigurations. Derived from the data in Table 1.\nDown-Projection (d′)\nPercent Improvement Over Dot-Product Attention (%)\n2\n5.07%\n16\n5.69%\nNone (d = 64)\n4.84%\nTable 2 shows the use of Neural Attention leads to a significant improvement in perplexity, with the\nbest result being a reduction of 5.69%. One should pay particular attention to the fact that down-\nprojecting the Q and K matrices has very minimal impact on performance. Reducing from d′ = 16\nall the way down to d′ = 2 only decreased the improvement in performance by 0.62 percentage\npoints. Interestingly, the worst-performing test case was the one that used no down-projection at all.\nThis could potentially be explained by the additional linear layer and learnable parameters introduced\nby the projection or by a beneficial dropout effect.\n4.2\nImage Classification Testing\nThese tests are performed using a vision transformer similar to that described by Donsovitsky et al.\n[2], with a patch size of 8, an embedding dimension of 768, a sequence length of 785, and a batch size\nof 32. The model has 12 layers, each with 8 attention heads, utilizing self-attention with sinusoidal\npositional encoding and no masking. The accuracy of image classification is benchmarked using the\nCIFAR-10 and CIFAR-100 datasets. Note that CIFAR-10 creates a 10-class classification problem,\nwhile CIFAR-100 creates a 100-class classification problem.\nTable 3: Validation accuracy after 350 training epochs on CIFAR-10.\nMethod\nModel Size (params)\nDown-Projection (d′)\nValidation Accuracy (%)\nDot-Product Attention\n47.402M\nNone (d = 64)\n83.70\nNeural Attention\n47.472M\n2\n86.70\nTable 4: Validation accuracy after 350 training epochs on CIFAR-100.\nMethod\nModel Size (params)\nDown-Projection (d′)\nValidation Accuracy (%)\nDot-Product Attention\n47.402M\nNone (d = 64)\n55.63\nNeural Attention\n47.472M\n2\n59.89\n7\n\nTable 5: Improvement in validation accuracy with Neural Attention (d′ = 2) compared to Dot-Product\nAttention.\nDataset\nPercentage Point Improvement over Dot-Product Attention\nCIFAR-10\n3.0 %\nCIFAR-100\n4.26 %\n4.3\nComputational and Memory Comparison\nMemory requirements and inference speeds during training for all experiments are in the below tables.\nThese metrics are included to provide a comprehensive picture of Neural Attention’s implementation,\nbut the primary takeaway remains its superior expressivity, as evidenced by the results in Sections 4.1\nand 4.2. Techniques to address the complexity challenges associated with Neural Attention, discussed\nin section 3.2, have achieved a reasonable memory requirement and inference speed when using\na value of d′ = 2, making Neural Attention scalable. Note that with Neural Attention only being\napplied to the first layer, scaling the model by adding more layers would be no different than scaling\na model built entirely with Dot-Product Attention. As such, the deeper the model becomes, the less\nrelevant the increased overhead due to Neural Attention will be.\nIn Table 6, the memory requirements decrease sharply as the down-projection dimension is reduced.\nThis is advantageous when considering the results in Table 2, which show down-projection has little\nto no effect on perplexity improvements.\nTable 6: Memory usage and inference speed during training on Wikitext-103\nAttention Type\nDown-Projection (d′)\nMemory Required (GB/sample)\nInference Speed (ms/sample)\nDot-Product Attention\nNone (d = 64)\n1.0\n5.7\nNeural Attention\n2\n1.4\n7.8\nNeural Attention\n16\n4.9\n15.8\nNeural Attention\nNone (d = 64)\n8.7\n48.1\nTables 7 and 8 show similar results during image classification training to those seen during NLP\ntraining.\nTable 7: Memory usage and inference speed during training on CIFAR-10\nAttention Type\nDown-Projection (d′)\nMemory Required (GB/sample)\nInference Speed (ms/sample)\nDot-Product Attention\nNone (d = 64)\n0.7\n5.4\nNeural Attention\n2\n1.1\n6.6\nTable 8: Memory usage and inference speed during training on CIFAR-100\nAttention Type\nDown-Projection (d′)\nMemory Required (GB/sample)\nInference Speed (ms/sample)\nDot-Product Attention\nNone (d = 64)\n0.7\n5.4\nNeural Attention\n2\n1.1\n6.6\n5\nConclusion\nIn this work, we have presented Neural Attention, a novel, modular, and easy-to-implement technique,\nproven to enhance the predictive capabilities of transformer models. By replacing dot products\nwith feed-forward networks, Neural Attention captures local, nonlinear relationships, creating more\nexpressive representations than standard methods can produce. We have demonstrated this enhanced\nexpressivity through extensive experimentation in both NLP and vision contexts. Text generation tests\non Wikitext-103 showed a reduction in perplexity of up to 5.69%, while vision tests on CIFAR-10\nand CIFAR-100 led to relative accuracy improvements of 3.0% and 4.26%, respectively. Notably, the\nadditional overhead added by this technique was mitigated by our implementation, proving it to be\nboth practical and scalable.\nIn future work, several research directions could further expand the impact of Neural Attention. When\napplied during the pretraining phase of large-scale conversational AI models, Neural Attention’s\n8\n\nability to enhance contextual understanding could lead to more coherent and nuanced responses\nin chat-based applications. Neural Attention may also find applications in edge computing, where\nits ability to increase performance with minimal loss of efficiency would be very valuable. Lastly,\nthere is a potential to further explore hybrid attention architectures, where Neural Attention is\nselectively applied in critical layers, exploring the optimal trade-off between computational efficiency\nand representational power. Given its modular design, strong empirical performance, and ease\nof integration into existing architectures, Neural Attention offers a promising new direction for\nadvancing transformer-based AI models across diverse applications.\nReferences\n[1] D. Bahdanau. Neural machine translation by jointly learning to align and translate. arXiv\npreprint arXiv:1409.0473, 2014.\n[2] A. Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale.\narXiv preprint arXiv:2010.11929, 2020.\n[3] A.-M. Halacheva, M. Nayyeri, and S. Staab. Expanding expressivity in transformer models\nwith möbius attention. arXiv preprint arXiv:2409.12175, 2024.\n[4] H. Liu, Z. Dai, D. So, and Q. V. Le. Pay attention to mlps. Advances in neural information\nprocessing systems, 34:9204–9215, 2021.\n[5] K. Mahmood and S. Huang. Enhanced computationally efficient long lora inspired perceiver\narchitectures for auto-regressive language modeling. arXiv preprint arXiv:2412.06106, 2024.\n[6] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever.\nImproving language under-\nstanding by generative pre-training.\nTechnical Report OpenAI Report, OpenAI, 2018.\nAvailable at https://cdn.openai.com/research-covers/language-unsupervised/\nlanguage_understanding_paper.pdf.\n[7] A. Shaker, M. Maaz, H. Rasheed, S. Khan, M.-H. Yang, and F. S. Khan. Swiftformer: Efficient\nadditive attention for transformer-based real-time mobile vision applications. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision, pages 17425–17436, 2023.\n[8] J. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu. Roformer: Enhanced transformer with\nrotary position embedding. Neurocomputing, 568:127063, 2024.\n[9] A. Vaswani. Attention is all you need. Advances in Neural Information Processing Systems,\n2017.\n[10] S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma. Linformer: Self-attention with linear\ncomplexity. arXiv preprint arXiv:2006.04768, 2020.\n[11] C. Wu, F. Wu, T. Qi, Y. Huang, and X. Xie. Fastformer: Additive attention can be all you need.\narXiv preprint arXiv:2108.09084, 2021.\n[12] T. Zhang, L. Li, Y. Zhou, W. Liu, C. Qian, and X. Ji. Cas-vit: Convolutional additive self-\nattention vision transformers for efficient mobile applications. arXiv preprint arXiv:2408.03703,\n2024.\nAppendix\nA\nIllustrating Dot Product vs. Neural Attention\nFigure 4 shows two embedding vectors ⃗q and ⃗k, each with an embedding dimension of d = 2. We\ndraw ⃗q as a column vector and ⃗k as a row vector to maintain familiar conventions when working with\ntransformers.\n9\n\nq1\nq2\nk1\nk2\n𝒒 \n𝒌  \nFigure 4: Embedding vectors ⃗q (shown in orange) and ⃗k (shown in blue).\nFigure 5 demonstrates ⃗q and ⃗k producing an attention score through a dot product operation. Red\nboxes highlight how values in the embedding dimension interact.\nk1\nk2\nq1\nq2\nk1\nq1\nk2\nq2\nATTENTION\nSCORE\nFigure 5: Dot product calculation between embedding vectors ⃗q (shown in orange) and ⃗k (shown in\nblue).\nFigure 6 demonstrates ⃗q and ⃗k producing an attention score through Neural Attention. The vectors\nare first concatenated before being transformed by a learnable weight matrix Wh, processed by an\nactivation function, and then further transformed to a scaler value by a learnable weight vector ⃗w⊤\na .\nNote that every attention score is calculated using its own unique set of learnable parameters.\nAttention Score = ⃗w⊤\na σ\n\u0010\nWh · concat(⃗q,⃗k) +⃗bh\n\u0011\n+ ba\nq1\nq2\nk1\nk2\n|\nATTENTION\nSCORE\nk1\nk2\nq1\nq2\n1\n2\n3\n4\n𝝈 (activation function)\n11\n12\n13\n14\n21\n22\n23\n24\n31\n32\n33\n34\n41\n42\n43\n44\n𝑾𝒉\nConcatenation\nFeed-Forward Network\n𝒘𝒂𝑻\n3\n4\n1\n2\n1\n+\n+\n𝒒𝒌concat\n𝒃𝒉\n𝒃𝒂\n𝒒𝒌concat\nFigure 6: Neural Attention calculation between the two embedding vectors ⃗q (shown in orange) and\n⃗k (shown in blue). Weights are shown in green, bias terms are shown in red, and a dashed box is used\nto represent the activation function\nB\nExample Neural Attention Calculation\nTo give a specific example, consider applying Equation 8 to vector C[1, 1, :] from the tensor in\nFigure 3. The result can be seen below, along with a visual depiction in Figure 7.\n10\n\nAttentionScore11 = Wa[1, 1] · σ(Wh[1, 1] · C[1, 1])\nA11\nk1\nk2\nq1\nq2\n11\n12\n13\n14\n11\n12\n13\n14\n21\n22\n23\n24\n31\n32\n33\n34\n41\n42\n43\n44\n𝐖𝐡[1,1]\n𝐖𝐚[1,1]\nC[1,1]\n𝝈 (activation function)\nFigure 7: Visual representation of the calculation for AttentionScore11, applying Equation 8 to\nC[1, 1, :] of the tensor in Figure 3.\nIf we extend what is depicted in Figure 7 to every (ij)-th vector in tensor C shown in Figure 3, we\ncan create the attention matrix A, shown in Figure 8. Note that each attention score is calculated\nusing its own unique set of learnable parameters and that bias terms are left out for simplicity.\nA11\nA21\nA12\nA22\nA31\nA32\nA13\nA23\nA33\nA\nFigure 8: Matrix A created by applying Equation 8 to the tensor C shown in Figure 3.\nC\nTraining Graphs\n22\n24\n26\n28\n30\n32\n34\n36\n38\n40\n42\n0\n200000\n400000\n600000\n800000\n1000000\nPerplexity\nIterations\nDot Product Attention\nNeural Attention (d'=2)\nNeural Attention (d'=16)\nNeural Attention (Full Embedding)\nFigure 9: Perplexity comparison of Dot-Product Attention and Neural Attention across training\niterations on WikiText-103.\n11\n\n20\n30\n40\n50\n60\n70\n80\n90\n0\n50\n100\n150\n200\n250\n300\n350\nAccuracy (%)\nEpochs\nDot Product Attention\nNeural Attention (d'=2)\nFigure 10: Accuracy comparison of Dot-Product Attention and Neural Attention across training\nepochs on CIFAR-10.\n0\n10\n20\n30\n40\n50\n60\n70\n0\n50\n100\n150\n200\n250\n300\n350\nAccuracy (%)\nEpochs\nDot Product Attention\nNeural Attention (d'=2)\nFigure 11: Accuracy comparison of Dot-Product Attention and Neural Attention across training\nepochs on CIFAR-100.\n12\n",
  "metadata": {
    "source_path": "papers/arxiv/Neural_Attention_A_Novel_Mechanism_for_Enhanced_Expressive_Power_in\n__Transformer_Models_177cd724bdf21ba8.pdf",
    "content_hash": "177cd724bdf21ba864cfc26950ce30b5188a43dddc1f3bec24c91facb6cbe98d",
    "arxiv_id": null,
    "title": "Neural_Attention_A_Novel_Mechanism_for_Enhanced_Expressive_Power_in\n__Transformer_Models_177cd724bdf21ba8",
    "author": "",
    "creation_date": "D:20250225025215Z",
    "published": "2025-02-25T02:52:15",
    "pages": 12,
    "size": 1056154,
    "file_mtime": 1740470168.6900227
  }
}