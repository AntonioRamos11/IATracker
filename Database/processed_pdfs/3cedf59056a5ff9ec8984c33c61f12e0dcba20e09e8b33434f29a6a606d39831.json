{
  "text": "Big-Math: A Large-Scale, High-Quality Math\nDataset for Reinforcement Learning in\nLanguage Models\nAlon Albalak1, Duy Phung1, Nathan Lile1, Rafael Rafailov†2, Kanishk Gandhi2, Louis Castricato1, Anikait\nSingh2, Chase Blagden1, Violet Xiang2, Dakota Mahan1 and Nick Haber2\n1SynthLabs, 2Stanford University\nIncreasing interest in reasoning models has led math to become a prominent testing ground for algorithmic and\nmethodological improvements. However, existing open math datasets either contain a small collection of high-\nquality, human-written problems or a large corpus of machine-generated problems of uncertain quality, forcing\nresearchers to choose between quality and quantity. In this work, we present Big-Math, a dataset of over\n250,000 high-quality math questions with verifiable answers, purposefully made for reinforcement learning (RL).\nTo create Big-Math, we rigorously filter, clean, and curate openly available datasets, extracting questions that\nsatisfy our three desiderata: (1) problems with uniquely verifiable solutions, (2) problems that are open-ended,\n(3) and problems with a closed-form solution. To ensure the quality of Big-Math, we manually verify each\nstep in our filtering process and iteratively improve our filters over multiple rounds. Based on the findings from\nour filtering process, we introduce 47,000 new questions with verified answers, Big-Math-Reformulated:\nclosed-ended questions (i.e. multiple choice questions) that have been reformulated as open-ended questions\nthrough a systematic reformulation algorithm. Compared to the most commonly used existing open-source\ndatasets for math reasoning, GSM8k and MATH, Big-Math is an order of magnitude larger (250,000\nquestions vs. 8,000 questions in GSM8k and 12,000 in MATH), while our rigorous filtering ensures that we\nmaintain the questions most suitable for RL. We also provide a rigorous analysis of the dataset, finding that\nBig-Math contains a high degree of diversity across problem domains, and incorporates a wide range of\nproblem difficulties, enabling a wide range of downstream uses for models of varying capabilities and training\nrequirements. In conclusion, this work presents our new dataset, Big-Math, the largest open dataset of math\nproblems suitable for RL training. By bridging the gap between data quality and quantity, Big-Math establish\na robust foundation for advancing reasoning in LLMs. Big-Math and Big-Math-Reformulated are\navailable at https://huggingface.co/datasets/SynthLabsAI/Big-Math-RL-Verified.\n1. Introduction\nIn the past few years, mathematics has emerged as a critical testing ground for the development\nand evaluation of advanced reasoning techniques used in Large Language Models (LLMs) [Cobbe\net al., 2021, Wei et al., 2023, Schaeffer et al., 2023, Mirzadeh et al., 2024, OpenAI et al., 2024,\nDeepSeek-AI et al., 2025, inter alia]. Following the release of strong reasoning models, such as\nOpenAI’s o1 [OpenAI et al., 2024] and DeepSeek’s R1 [DeepSeek-AI et al., 2025], a plethora of\nsupervised fine-tuning (SFT) datasets have been released with the aim to distill reasoning capabilities\ninto other models, or to bootstrap a model prior to RL [Liu et al., 2024, Li et al., 2025, AI, 2025, Labs,\n2025, Muennighoff et al., 2025]. However, knowledge distillation has its limitations. For example,\nresearch has shown that while SFT distillation enables models to memorize reasoning patterns and\nsolutions, the resultant models do not generalize well [Chu et al., 2025]. On the other hand, RL\nFiltering and reformulation code available at https://github.com/SynthLabsAI/Big-Math.\nPrompts for all filters and reformulation can be found in Appendix D.\n†Work done while at SynthLabs.\narXiv:2502.17387v1  [cs.LG]  24 Feb 2025\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\ntraining algorithms have been shown to yield models with better generalization to novel problems\nby emphasizing the exploration and refinement of reasoning strategies. Furthermore, [Wang et al.,\n2025] find that current reasoning models (across model sizes and families) frequently achieve the\ncorrect answer with incorrect reasoning steps, suggesting that distilled reasoning models will also\nsuffer from incorrect reasoning. Additionally, OpenAI et al. [2025] show that scaling RL training is\nvery effective in improving reasoning and other capabilities. Therefore, while distillation through SFT\ndatasets is clearly a helpful method for improving model performance on some evaluations, it does not\naddress the greater needs at the forefront of reasoning capabilities which can be addressed by RL: learning\nhow to reason [Xiang et al., 2025, Yeo et al., 2025, Kim et al., 2025].\nA significant bottleneck in RL-based reasoning research is the lack of high-quality datasets tailored\nfor reinforcement learning. RL training methods all assume access to a dataset with verifiable answers\n[DeepSeek-AI et al., 2025, Team et al., 2025]; either to train a reward model [Lightman et al.,\n2023b, Havrilla et al., 2024b, Zhang et al., 2024, Mahan et al., 2024] or to directly evaluate the\ncorrectness of a generated solution and, as has been demonstrated through the course of deep\nlearning’s existence, scaling up data quantity and quality is a crucial step towards success [Havrilla\net al., 2024a]. However, a central issue with the existing math datasets is that they are either (1)\nhuman written, but limited in quantity, or (2) machine generated and large, but of unknown quality,\nforcing researchers to choose datasets with either quality or quantity. For example, prior works have\nmostly used the GSM8k [Cobbe et al., 2021] and MATH [Hendrycks et al., 2021] datasets. While they\ncontain human-written questions and answers, both datasets are quite limited in quantity, with 8,000\nand 12,000 problems, respectively. On the other hand, large-scale datasets, such as NuminaMath [Li\net al., 2024b], exhibit quality issues, including many duplicate problems and incomplete solutions,\nhindering their utility in RL training. Furthermore, many existing math datasets contain problems\nwhich are not well-suited for training a reasoning model with RL. For instance, they sometimes have\na high proportion of multiple choice questions. While multiple choice questions may be useful for\nSFT, they are less effective for RL-based training. Recent works [Xiang et al., 2025], suggest that\nthe goal of RL for reasoning is not just to train the model to answer correctly, but more importantly,\nto reason correctly. With this in mind, questions in multiple choice formats are problematic for RL;\neven though the correct answers may be difficult to deduce, the model can simply guess the correct\nanswer without performing the correct reasoning. Problems like these highlight the biggest issues\nwith curating a dataset for RL.\nTo address these challenges, we present Big-Math, a dataset of over 250,000 high-quality math\nproblems and solutions, curated with three core desiderata:\n1. Uniquely verifiable solutions: problems must admit a single correct answer that can be reliably\nverified;\n2. Open-ended problem formulations: problems that cannot be easily solved by guessing (as\nmight occur in multiple choice formats) and instead require nontrivial reasoning steps; and\n3. Closed-form solutions: answers must be expressible in a closed form (e.g., a scalar or formula,\nnot a proof), thereby enabling automated evaluation.\nWith these desiderata in mind, we design and develop a process of cleaning and curating datasets by\napplying a strict set of filters. To ensure the quality of our filtration process, we apply a human-in-\nthe-loop algorithm, iteratively improving each filter through multiple rounds of manual verification\nuntil the filter achieves suitable levels of precision and recall (minimum of 90%). Then, we apply our\nfiltration process to three openly available datasets, extracting a high-quality and large-scale subset of\nover 200,000 uniquely verifiable, open-ended problems with closed-form solutions. Additionally, we\nintroduce Big-Math-Reformulated, a novel subset of 47,000 problems derived by reformulating\nmultiple-choice questions into an open-ended format, while preserving their integrity and complexity\n2\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\nOrca-Math\ncn_k12\nBig-MATH-Reformulated\nolympiads\nMATH\naops_forum\nGSM8K\nHARP\nOmni-MATH\namc_aime\nMath\nWord\nProblems\nAlgebra\nGeometry\nStatistics\nCalculus\nNumber\nTheory\nIntermediate\nAlgebra\nPrecalculus\nDiscrete\nMathematics\nPrealgebra\nLinear\nAlgebra\nOther\nAbstract\nAlgebra\nDifferential\nEquations\nFigure 1: The sources (left) and domains (right) of the Big-Math dataset, scaled by size.\nand aligning with RL training requirements. In combination, our filtered datasets and reformulated\nproblems create Big-Math, a dataset of over 250,000 questions with extracted answers, specifically\ndesigned for RL training.\nIn summary, this work provides:\n• Big-Math, the largest open source dataset of math problems, designed specifically for RL,\nwith over 250,000 problems.\n• A rigorous filtering and cleaning pipeline, involving humans-in-the-loop, and achieving over\n90% precision and recall.\n• Big-Math-Reformulated, a dataset of 47,000 novel problems, reformulated from existing\nmultiple choice problems, and the full reformulation pipeline.\n• Empirical analyses highlighting the diversity, difficulty, and suitability of Big-Math for\nadvancing reasoning in LLMs.\n2. Big-Math\nIn this section, we describe the technical details of our data collection, cleaning, filtering, and\nreformulation processes.\n2.1. Dataset Collection\nWe considered a number of openly available math datasets and selected 3 well established math-\nematical problem datasets that are commonly used in recent literature: HARP [Yue et al., 2024],\nOmni-MATH [Gao et al., 2024], and NuminaMath [Li et al., 2024b]. The goal of this work is to find a\nlarge set of high-quality math problems that satisfy our three desiderata. Human-written problems\nare generally associated as being high-quality, and the datasets that we select provide a large quantity\nof human-written problems to start with (> 75%). Additionally, we choose to incorporate synthetically\ngenerated data, but limit ourselves to a single, well established source Table 1 shows a breakdown of\nthe sources, including the original quantity of data and the amount after filtering.\n3\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\nData Source\nOriginal\nBig-Math\nOrca-Math [Mitra et al., 2024]\n153,314\n83,215\ncn_k12 [Li et al., 2024b]\n276,554\n63,609\nolympiads [Li et al., 2024b]\n150,563\n33,485\nMATH [Hendrycks et al., 2021]\n12,000\n9,257\naops_forum [Li et al., 2024b]\n30,192\n5,740\nGSM8k [Cobbe et al., 2021]\n7,473\n3,254\nHARP [Yue et al., 2024]\n4,780\n2,996\nOmni-MATH [Gao et al., 2024]\n4,428\n2,478\namc_aime [Li et al., 2024b]\n4,070\n78\nBig-Math-Reformulated (Section 2.3)\n–\n47,010\nTotal\n643,374\n251,122\nTable 1: Comparison of problems by data source with the original quantity and the quantity\nincluded in Big-Math. Big-Math-Reformulated is a new problem set introduced in this\nwork and does not have an “original” size.\nHARP\nFirst, we include the Human Annotated Reasoning Problems (HARP) dataset [Yue et al.,\n2024], which contains 4,780 short answer problems collected from U.S. national math competitions.\nWe specifically take only the “short answer” subset, which should not contain any questions requiring\nproofs, or multiple choice questions.\nOmni-MATH\nNext, we incorporate the Omni-MATH dataset [Gao et al., 2024], contributing almost\n4,500 olympiad-level problems curated from 39 different competition websites. Notably, this dataset\nemploys professional annotators and verifiers to maintain solution-answer quality.\nNuminaMath\nFinally, we use the NuminaMath [Li et al., 2024b] dataset, which is composed of\nroughly 860,000 problems divided into 9 sources (6 of which we retain). First, there are three sources\nthat we do not use from Numina: the synthetic_math, synthetic_amc, and the MATH subsets. We\nchoose not to include either of the synthetic data subsets as they was never evaluated independently\nfrom the remainder of the dataset, so we cannot know their quality. Additionally, while NuminaMath\nuses the original split of the MATH dataset (7,000 training problems, 5,500 test problems), we choose\nto use version with 12,000 training problems and 500 test problems, as proposed by [Lightman\net al., 2023a]. The largest subset from Numina is the cn_k12 subset of data, composed of ∼275, 000\nmath problems scraped from chinese math exams. The next largest subset from NuminaMath is\nthe Orca-Math [Mitra et al., 2024] subset (∼150, 000 problems), which is a synthetically generated\ndataset of grade school math problems. Although this data was synthetically generated, Mitra et al.\n[2024] demonstrate that as the lone source of SFT training data, this subset leads to impressive math\nperformance, even for small language models. Next, we include the olympiads subset, consisting of\n150,000 problems collected from online sources of international-level math competitions. Next, Li\net al. [2024b] scrape the Art of Problem Solving forum for additional math competition problems,\nselecting to keep only problems with high quantities of LaTeX and at least one \\boxed or ■symbol,\nleading to an additional 30,000 problems. Additionally, we use the GSM8k [Cobbe et al., 2021]\nsubset, providing another ∼8, 000 problems. Finally, we also include the amc_aime subset, consisting\nof 4,000 more math competition problems, with solutions scraped from online sources.\n4\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\n2.2. Dataset Cleaning and Filtering\nThe collection of all the above datasets leads us to a combined dataset of over 640,000 problems.\nHowever, this dataset likely has many duplicated questions, undesirable content, and data that does\nnot satisfy our desiderata. To achieve a dataset of the highest quality, and appropriate for RL training,\nwe next clean and filter the data from each source using a combination of bespoke and common\nstrategies [Albalak et al., 2024, Soldaini et al., 2024, Li et al., 2024a]. We ensure the quality of our\nfilters by applying human-in-the-loop methodology, iteratively refining the filters through human\nverification and annotation of positive and negative examples. By the end of the iterative process\nour filters achieve over 90% F1 score, oftentimes reaching much higher than 90%. After filtering,\nthe data should contain only problems that closely follow our three desired properties: open-ended,\nverifiable, closed-form problem-solution pairs.\n2.2.1. Source-specific Filtering and Cleaning\nThe first step in our cleaning and filtering process is to observe a sampling of data from each\ndataset, and to design bespoke filters to be utilized on each source separately based on their unique\nidiosyncrasies.\nHARP\nFor the HARP dataset [Yue et al., 2024], we find many problems that contain figures in the\nAsymptote1 vector graphics language, which we identify by string matching for “[asy]”. We err on\nthe side of caution and remove 625 such problems (13% of the dataset), assuming that the model\nwould need to see the rendered image to solve the question.\nOmni-MATH\nWhen exploring Omni-MATH [Gao et al., 2024], we found a number of problems\ncontaining author attributions (e.g. a person’s name in parenthesis, or in the format “[i] Name [/i]”)\nand manually revise each problem to remove the attribution (45 in total). Next, we found that some\nproblems contain information about their scores in the competition and remove this text from the\nproblem. Specifically, we found the phrase “If the correct answer is X and your answer is Y” in a\nnumber of problems. Additionally, we found and removed 2 problems with the following solution:\n“The problem provided does not contain a solution. Therefore, no final answer can be extracted.”,\nlikely a parsing error when extracting problem-solution pairs from online sources.\nNuminaMath\nWithin NuminaMath [Li et al., 2024b], we find some unique characteristics of each\nsubset. First, because some of the subsets are quite large, we deduplicate problems within each subset\nwith a MinHashLSH filter. We use 128 hashing functions, and through a few rounds of experiments,\nempirically determine that a similarity threshold of 0.6 or 0.7 (depending on the subset) is the\nstrictest threshold we can set without the filter quality degrading. Next, the NuminaMath dataset\ndoes not explicitly contain answers to each problem, so we extract answers by searching for boxed\nsolutions (“\\boxed{}” in LaTeX). Any problem whose solution does not contain exactly 1 boxed answer\nis filtered out. Finally, for the aops_forum subset, we find 2535 problems containing unnecessary\ninformation, including problem attribution (e.g. “proposed by”), year of submission, point scoring\n(e.g “(1 point)”) and remove these strings with regular expressions.\nAt the end of source-specific filtering, we find that we have 463,426 remaining problems. However,\nthese problems are not yet fit for use, as this data still contains problems not suitable for RL training.\n1https://asymptote.sourceforge.io/\n5\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\nMultiple\nTrue/False\nYes/No\nMulti-Part\nProof\nSemantic\nTotal\nSubset\nChoice\nDuplicates\nMatching\nOrca-Math\n1,277\n5\n12\n18,375\n5\n8,286\n26,487\ncn_k12\n96,804\n420\n384\n38,625\n5,233\n2,098\n123,809\nolympiads\n12,688\n311\n4,615\n22,116\n30,357\n1,362\n53,102\nMATH\n1,124\n0\n3\n2,885\n31\n7,694\n9,714\naops_forum\n1,665\n85\n126\n3,162\n4,608\n262\n7,853\nGSM8k\n46\n0\n0\n792\n0\n7,664\n7,999\nHARP\n303\n2\n7\n288\n2\n155\n695\nOmni-MATH\n149\n126\n206\n467\n207\n378\n1,182\namc_aime\n2,962\n0\n22\n1,274\n66\n53\n3,066\nTotal\n117,018\n949\n5,375\n87,984\n40,509\n27,952\n233,907\nMatching\nTable 2: The quantity of data matched by each filter. Some data matches multiple filters, so column\ntotals are a summation, but row totals are not. We see some interesting trends here: the cn_k12\nsubset contains a significant amount more multiple choice questions than any other subset, olympiads\ncontains many more proof questions, but also yes/no questions than other subsets. Interestingly,\nbut unsurprisingly, we also see that nearly the entire MATH and GSM8k datasets contain semantic\nduplicates with other subsets.\n2.2.2. Source-agnostic Filtering\nAfter running each of the described filters over the individual subsets, we perform 11 filtering\noperations across the full collection. These filters are specially designed to convert the raw dataset of\nquestion-answer pairs into a dataset that is suitable for training a math reasoning model with RL.\nJust as for the source-specific filters, we also manually verify the results of each source-agnostic filter,\nand iteratively improve their performance until they achieve suitable results (where appropriate).\nThe high-level results of our filtering process are found in Table 2, with more detailed results in\nAppendix A. Prompts for model-based filters are found in Appendix D.1.\nDeduplication and Decontamination\nTo prevent a model from unknowingly being trained on\nthe same problem too frequently (and risk overfitting), we need to remove duplicates. To handle\nthis, we run a very simple and efficient deduplication step, calculating duplicate problem by string\nmatching (not including whitespace), removing all but one copy of the duplicated problem. Next,\nwe also want to ensure that the final dataset has a diverse set of problems, without focusing on any\nspecific problem types. To do so, we remove semantic duplicates (problems with similar meaning,\ne.g. the same problem with numbers changed) with the SemDeDup algorithm [Abbas et al., 2023].\nTo embed the problems, we use the model at sentence-transformers/all-MiniLM-L6-v2 and remove\nproblems with a cosine similarity over 0.5. For semantic deduplication, we tested thresholds between\n0.95 and 0.2, finding that even the strictest thresholds can cluster dissimilar problems (likely caused\nby short problems). We want to ensure that no duplicated data passes through, so we purposefully\nselect a threshold where our manual verification determines that no duplicates remain. Finally, the\nMATH [Hendrycks et al., 2021] and Omni-MATH [Gao et al., 2024] test sets are prime candidates for\nevaluating a model trained on our dataset, so we need to ensure that the problems in those test sets\ndo not exist in our training set. To decontaminate our dataset with any of the 500 MATH test set\nproblems and the 500 Omni-MATH test set problems we find and remove all contaminated data by a\nstring matching algorithm on the problems, the same as for deduplication. We were surprised to find\nminimal duplication (< 1%), removing only 4,229 problems, with the majority of duplicates being\n6\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\nbetween the Orca-Math (3,916), GSM8k (3,875), and cn_k12 (405) datasets.\nEnsuring that problems are solvable\nFor a number of reasons, there are problems in existing math\ndatasets which are not solvable.\n(Language Filter) First, in this work, we focus on English-only models and require English math\nproblems. So, we use a FastText language identifier [Joulin et al., 2016b,a, Grave et al., 2018] and\nremove any problems where English is not the primary language (only 101 non-English problems\nwere detected).2 Through our iterative improvement process, we found that it was important to\nremove LaTeX, along with most special characters (e.g. numbers, math symbols, “()[]{}!@#$%ˆ&”,\netc.) in order to achieve a high level of accuracy with the language detection model. Additionally,\nproblems which were very short (< 10 characters) were often classified as non-English (even if they\nwere entirely numbers), so we simply include all problems with fewer than 10 non-LaTeX, non-special\ncharacters.\n(Hyperlink Detection) Next, we remove problems containing a hyperlink using a simple regular\nexpression, as the existence of hyperlinks suggests that a model may not have the full resources\nrequired to solve the problem (e.g. hyperlinks that point to a website containing a theorem). While\nthis likely removes problems that are solvable (hyperlinks may also link to the website source of the\nproblem), we prefer to err on the side of caution and remove all problems containing hyperlinks. This\nover-filtering can be addressed in the future through more complex filters.\n(Model Solve Rate) Finally, while it is not feasible to manually ensure the correctness of each\nproblem-answer pair, we develop a heuristic for correctness using language models. For each problem,\nwe generate 64 solutions from Llama-3.1-8B (∼30, 000, 000 rollouts) and 5-8 solutions from Llama-\n3.1-405B (∼1, 100, 000 rollouts, generated on a pre-filtered subset) [Dubey et al., 2024]. If either\nmodel answers the question with the ground truth answer, then we determine that the question-answer\npair may be valid. We do not apply this filter to HARP, Omni-Math, MATH, or GSM8k as these datasets\ninclude pre-parsed answers.\nOf course, this method does not guarantee that the given answer is correct, as it is possible that\nthe answers fall under a commonly made mistake, or that the models have seen the data during\npre- or post-training. Furthermore, this filter does not guarantee that removed data has an incorrect\nanswer, as it is very likely that the models we use cannot solve the most difficult math problems. One\nmethod for improving this filter would be to use a stronger, math-specific model.\nEnsuring that problems are open-ended\nAn important aspect of reinforcement learning is that the\ntraining signal should appropriately attribute the good actions with high rewards and poor actions\nwith low rewards. Therefore, problems with multiple choice answers pose a problem: the model can\ninadvertently respond with the correct answer option (generally between a 25-50% chance of guessing\ncorrectly) without providing the correct intermediate reasoning steps, leading to a poor learning\nsignal. For this reason, we choose to remove any problems that are multiple choice, True/False, and\nYes/No. To detect and remove all three types of questions, we develop both a regular expression-based\nfilter, and a model-based filter.\n(Regular Expression Filters) For multiple choice questions, we use a simple regular expression\nfilter that searches for either alphabetic options (A, B, C, D) or numerical options (1, 2, 3, 4), occurring\nin order. To ensure that we do not incidentally remove questions referring to shapes (e.g. “rectangle\nABCD...”) or numbers (e.g. 1234), we first remove those strings from the question, prior to the\nregular expression search. Next, for the True/False questions, we search for either “true” or “false”\nin the answer or, when available, in the final line of the solution. Then, for Yes/No questions, we\nperform the same check as True/False questions, searching in the answer or solution for exact phrases.\n2We use the model at https://huggingface.co/facebook/fasttext-language-identification.\n7\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\nAdditionally, we search the final line of the question for specific phrases that imply a Yes/No question:\n“is”, “are”, “do”, “does”, and “can”. While these regular expression filters are guaranteed to remove all\ndata of this exact form, they are not flexible enough to catch many exceptions.\n(Model-based Filters) Therefore, for each question type, we design a model-based filter by\niteratively developing a prompt to use with Llama-3.1-70B [Dubey et al., 2024]. For example, to\ndevelop the prompt for multiple choice questions, we first manually find examples of multiple choice\nproblems and open-ended problems and include these as in-context examples. Next, we run our filter\nover the dataset and inspect 100 problems classified as positive examples (multiple choice) and 100\nproblems classified as negative examples (open-ended). We iteratively add the difficult incorrectly\nclassified problems into the prompt, mostly selecting problems following a previously unseen pattern.\nFor these filters we prioritize a high recall, ensuring that we remove as much undesirable data as\npossible. In this iterative process of filter development, we continue until achieving over 98% recall,\nrequiring between 5-8 rounds of manual verification.\nEnsuring that problems are uniquely verifiable\nA critical aspect of training reasoning models\nwith reinforcement learning is the existence of verifiable answers, generally in the form of a ground\ntruth to be compared against the model response.\n(Answer Filter) As a simple first step, we remove all examples where the final answer did not\npreviously exist, or could not be extracted from the solution (e.g. if there was no “\\boxed{}” element\nin the solution).\n(Multi-part Question Filter) Next, we find a large proportion of questions with multiple parts\nrequiring multiple corresponding answers. There are surely methods with which to handle partial\ncorrectness in multi-part questions, but this is still an open research question requiring further study,\nso we leave these problems for future, more difficult, versions of the dataset. To develop our multi-\npart question filter, we follow a similar pattern to the multiple choice questions filter: a dual-filter\napproach including a regular expression filter and a model-based filter. For the regular expressions,\nwe iteratively improve the filter through manual search of the data and improvements to the regular\nexpressions. The final version of our filter searches for commonly found signals: ordered roman\nnumerals (e.g. I, II), multiple numbered parts in parentheses (e.g. (1) ... (2)), multiple numbered\nparts with a period (e.g. 1. ... 2.), as well as numbered special characters (e.g. 1O ...\n2O). For the\nmodel-based filter, we use the same iterative process as for the multiple choice question, using a\nLlama-3.1-70B [Dubey et al., 2024], inspecting 100 examples of positively and negatively classified\nexamples at each iteration, ending only once we achieve over 98% recall.\n(Proof Filter) While proofs can be verified, there can be many correct variations, and how to\nquickly verify proofs written in natural language is unclear at the moment (other than converting\nto a theorem proving language, which can incur additional parsing errors). Therefore we also elect\nto remove proofs from our dataset, leaving them for a future version which is more difficult. We\ndevelop our proof filter with the same dual-method approach previously discussed. The regular\nexpression filter simply searches for either of the following phrases in the problem: “prove that” or “a\nproof”. We again implement the model-based filter through an iterative improvement process with\nLlama-3.1-70B, inspecting 100 examples of predicted positive and negative examples, completing the\nprocess only once 98% recall is achieved.\n2.3. Big-Math-Reformulated\nBased on findings from the previous sections we introduce a new subset of 47,000 questions and\nanswers, Big-Math-Reformulated, which we describe here.\nDuring the development of our filters, we found a staggering number of multiple choice questions\n8\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\nKey Information Extraction\ncore_mathematical_concept: \"Number\ntheory - prime factorization and sum of\nprime factors\"\nkey_information_extraction: [\"Find the\nsum of prime factors of 2010\", \"Prime\nfactorization required\"]\nproblem_structure_analysis: \"Direct calculation problem\"\nmultiple_choice_removal_strategy: [\"Remove all\nanswer choices\", \"Ask for direct calculation of\nsum of prime factors\"]\nrephrasing_approach: [\"Request the sum of prime\nfactors directly\", \"No comparative aspect needed\"]\nproblem_integrity_preservation: [\"Maintain the original\nnumber (2010)\", \"Require prime factorization\"]\nanswer_format_specification: [\"Answer should be a\nsingle integer\", \"No units or decimal places required\"]\nWhat is the sum of the \nprime factors of 2010?\n(A) 67\n(B) 75\n(C) 77\n(D) 201\n(E) 210\nReformulated Problem\nFind the sum of the prime factors of 2010. Express \nyour answer as a single integer.\nJudgement\nThe reformulated problem is a direct\nand clear request to find the sum of\nthe prime factors of 2010. It does not\nprovide any multiple-choice options, nor does it \nlimit the answer choices in any way\n…\nThe original problem and the reformulated \nproblem are mathematically equivalent.\n...\nOverall, the reformulated problem is a well-posed \nand mathematically sound problem that requires \nthe solver to apply mathematical concepts and \ntechniques to find the solution.\nVerification\nOriginal Problem\nFigure 2: Our reformulation strategy first extracts key information, then reformulates the problem\nas open-ended, followed by a judgement of the reformulation and a final verification. The full process\nis detailed in Section 2.3.1, with successful and failed examples of reformulation in Appendix C.\n(> 117, 000), which are removed during the filtering process for Big-Math. In particular, we are\nconcerned with the loss of significant quantities of data from human-written and high-quality sources:\nolympiads, amc_aime, and aops_forum. The inherent structure of multiple choice questions presents\na challenge for RL algorithms. Specifically, the multiple-choice format increases the probability of\nanswering correctly regardless of the correctness of a reasoning chain. This can lead the algorithm to\nassign high rewards to incorrect reasoning, reducing the utility of such questions in an RL environment.\nTo address this issue, we propose a novel approach to reintroduce these valuable questions by\nreformulating the multiple choice questions into open-ended questions with a series of carefully\ndevised and detailed steps enacted by large language models. Our reformulation process is developed\nto be systematic and rigorous, in order to ensure that the integrity and complexity of the original\nquestions are maintained, while making them amenable for RL. We include examples of a successful\nreformulation and a failed reformulation in Figure 9.\n2.3.1. Reformulation strategy\nSource\nNumber of\nProblems\ncn_k12\n35,211\nolympiads\n9,019\namc_aime\n1,878\naops_forum\n853\nMATH\n48\nOrca-Math\n1\nTable\n3:\nComposition\nof\nproblems\nin\nBig-Math-\nReformulated, reformulated\nfrom\nproblems\nin\nthe\nsource\ncolumn.\nWe develop a 4-step process to reformulate multiple choice\nproblems as open-ended problems. All prompts can be found\nin Appendix D.2, and we use Llama-3.1-405B [Dubey et al.,\n2024] to create Big-Math-Reformulated.\n(Key Information Extraction) We begin the process by\nidentifying a few core pieces of information about each ques-\ntion. We determine whether the question is in the multiple\nchoice format, extract the core mathematical concepts (e.g.\ngeometry), and identify key problem details (e.g. distances,\ngoal of the question). In the first step, we also ask the language\nmodel to develop a strategy to convert the question into an\nopen-ended format, as well as strategies for rephrasing and\nensuring that the integrity of the original problem is main-\ntained. Finally, we extract a plan for what the format of the\nfinal answer should be (e.g. the answer should be expressed\nin 𝑐𝑚3).\n(Reformulation) Next we reformulate the multiple choice question into an open-ended question\nby conditioning on the key extracted information extracted.\n9\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\n(Judgement) Following the reformulation, a critical evaluation along multiple dimensions is\nconducted to evaluate whether the reformulation succeeded. This includes determining whether the\nnew question is multiple choice, or if it even has a limited number of possible answers which would\nstill be considered multiple choice. Additionally, we evaluate whether the reformulated problem is\nfree from any information distortion: no existing information was changed, no critical information\nwas omitted, and no new information was added. Finally, we determine whether the expected answer\nformat is clear and if the problem can stand alone, without reference to the original options.\n(Verification) Finally, we perform a last verification that the entire process succeeded by checking\nthat the expected information is present (e.g. expected answer format, rephrasing strategy, etc.).\n2.3.2. Reformulation Post-Processing\nThe outcome of our reformulation is 88,983 questions that have passed the judgement and verification\nsteps. However, we still need to ensure that the questions in Big-Math satisfy the same criteria:\nuniquely verifiable, open-ended problems with closed-form solutions. Therefore, we next examine the\nsolvability of each reformulated problem by evaluating Llama-3.1-8B (8 rollouts) and Llama-3.1-405B\n(3 rollouts) on them. We filter the 88,983 problems down to only 48,698 by keeping only problems\nthat were solved at least once by either model, but not solved 100% by Llama-3.1-8B (to remove\nquestions that may be too simple or obvious). In instances where neither model produces a solution\nthat matches the reformulated answer, it is difficult to determine whether the issue lies with the\nmodel’s performance, or with the reformulated answer. To mitigate such uncertainty, we exclude\nthese problems from the dataset.\nFinally, Big-Math-Reformulated undergoes the same comprehensive filters as the rest of\nthe datasets, as outlined in Section 2.2.2, yielding a final set of 47,010 reformulated and filtered\nproblems, with the composition of source data found in Table 3. We find that this process has\nsuccessfully reintroduced high-quality questions that were previously removed. Specifically, 72.7%\nof the amc_aime subset was found to be multiple choice, but the reformulation process successfully\nreintroduces 63.4% of the multiple choice problems.\n3. Analysis and Discussion\nIn this section we discuss and analyze the Big-Math dataset. We consider the dataset difficulty,\ndiversity, and outcomes of the filters that we proposed. Throughout the analysis, we include discussion\npoints to aid the downstream uses of Big-Math.\n3.1. Dataset Difficulty\nWe calculate the difficulty of problems in Big-Math based on rollouts from the Llama-3.1-8B\nmodel [Dubey et al., 2024], which provide a benchmark for understanding problem complexity. For\neach problem in the dataset we generate 64 rollouts and calculate the success rate per problem. The\ndistribution of success rates, split by source and domain is found in Figures 3 and 4.\nFirst, Figure 3 shows that the majority of the easiest data (highest solve rate) comes from the\nOrca-Math, cnk_12, and MATH datasets, while the most difficult data is divided more evenly across\nthe datasets. In particular, we find that nearly all of Omni-MATH and HARP are unsolveable by\nLlama-3.1-8B. Thus, in order to apply RL for Llama-3.1-8B on these difficult subsets, this particular\nmodel would need to be either be supervised fine-tuned on these datasets, or using an RL training\nalgorithm that makes use of a process reward model. For example, Reinforcement Learning with\nVerifiable Rewards (RLVR) [Lambert et al., 2024] would be unlikely to work effectively on Omni-Math\n10\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSolve Rate\n0\n2500\n5000\n7500\n10000\n12500\n15000\n17500\nFrequency\nSource\ncn_k12\nOrca-Math\nolympiads\naops_forum\nGSM8K\namc_aime\nMATH\nOmni-MATH\nHARP\nBig-MATH-Reformulated\nFigure 3: Distribution of solve rates on each subset of Big-Math, calculated with Llama-3.1-8B.\nand HARP as the models responses would produce no training signal.\nNext, we group problems into difficulty quintiles, with the hardest quintile being problems that\nhave a success rate less than 20% and the easiest quintile with a success rate over 80%. We find that,\nfrom easiest to hardest, the quintiles have 71,926 (28.64%), 30,533 (12.16%), 25,763 (10.26%),\n31,249 (12.44%), and 91,647 problems (36.50% of the total problems). An obvious question now\nis: how should practitioners use these dataset difficulties for their own purposes? In general,\nthose training less capable, or smaller, models may want to remove the most difficult problems as it is\nunlikely that model rollouts will lead to a correct answer. This leads to inefficiency in the learning\nprocess because most RL methods used for LLMs (except those with a process reward model) will\nhave 0 signal if the model can never come to the correct answer. On the other hand, for those training\na larger, or math-specific, model will find many of the easy questions redundant, and training on such\ndata will be inefficient. Therefore, for practitioners training strong models it would be sensible to\nkeep only the harder problems. Supposing that the hardest two quintiles of data are retained, there\nis still > 120, 000 problems, 10 times more problems than the next closest RL-suitable dataset.\nNext, we look at the difficulty of our novel Big-Math-Reformulated subset. We see\nthat our subset follows a similar solve rate distribution as the rest of the dataset; it has slightly\nmore density around the low- and high-ends of the difficulty distribution. However, Big-Math-\nReformulated is skewed towards more difficult problems. Specifically, we find that 34.44% of\nBig-Math-Reformulated is in the hardest quintile, with an additional 16.42% in the second\nhardest quintile, combining to greater than 50% of the new data.\nFinally, we look into the distribution of solve rates by each problem domain, shown in Figure 4. We\nfind that the most difficult problems come from the differential equations, discrete mathematics, and\nabstract algebra domains, while the prealgebra domain is the easiest by a wide margin. Interestingly,\nthe remaining domains have a very wide distribution of difficulties, suggesting that within each\ndomain there are likely problems requiring varying levels of expertise. Surprisingly, linear algebra\nwas found to be one of the easier domains, while geometry was one of the most difficult domains,\nhowever, this may either be an artifact of the domain classification process, or of the specific training\ndata for Llama-3.1-8B.\n11\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSolve Rate\nDifferential Equations\nDiscrete Mathematics\nAbstract Algebra\nGeometry\nNumber Theory\nOther\nCalculus\nPrecalculus\nAlgebra\nStatistics\nLinear Algebra\nIntermediate Algebra\nMath Word Problems\nPrealgebra\nFigure 4: Distribution of solve rates by domain, calculated with Llama-3.1-8B.\n3.2. Dataset Diversity\nTo better understand our dataset and the diversity of problems it provides, we next study the domain\ncomposition and distribution of our dataset.\nWe consider the distribution of problems sorted into mathematics domains according to two\nontologies. The first ontology we consider is that proposed by Gao et al. [2024], and the second\nis the ontology defined by the American Mathematical Society in their 2020 Mathematics Subject\nClassification (MSC3). We use the same prompt and procedure as Gao et al. [2024] to classify questions\ninto their domains. Similar to Muennighoff et al. [2025], we classify problems into one of the 63\ndomains defined by the MSC using GPT-4o-mini. The domains of Gao et al. [2024] contain high-level\ntopics ranging from grade-school to college-level, while the MSC domains are more fine-grained\nand include more detailed domains on math-adjacent topics. For the domains defined by Gao et al.\n[2024], we present examples in Figure 6 and a chart with the number of problems per domain in\nFigure 5, as well as a detailed breakdown of domains by source in Section B in the Appendix. For the\nMSC domains, Figure 5 shows the distribution of all domains with 10 or more problems, and Figure 8\nshows example problems for the top-5 most common domains.\nWe see in Figures 5 and 7 that the distributions for both ontologies have long tails. The largest\ndomain from Gao et al. [2024] is the Math Word Problems, which we find to come disproportionately\nfrom Orca-Math, which contains > 66, 000 such problems. Surprisingly, we find that the largest MSC\ndomain was operations research, and we inspected the data under this category to better understand\nwhere such a large quantity came from. We found that these problems are generally an application of\nalgebra, geometry, or statistics to real world domains and could just as easily have been classified\ninto another category. We found that the distribution of MSC domains follows a nearly log-linear\nrelation, with large quantities of problems in college-level topics, including ∼10, 000 problems on\nordinary differential equations, field theory, and optimal control. We also found smaller quantities of\napplications in the sciences in the dataset, with problems in electromagnetic theory, thermodynamics,\nand fluid mechanics, showing that the collected dataset contains problems from a wide variety of\nmathematics domains.\n3https://mathscinet.ams.org/mathscinet/msc/msc2020.html\n12\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\nMath Word Problems\nAlgebra\nGeometry\nStatistics\nCalculus\nNumber Theory\nIntermediate Algebra\nPrecalculus\nDiscrete Mathematics\nPrealgebra\nLinear Algebra\nOther\nAbstract Algebra\nDifferential Equations\n102\n103\n104\n105\nCount\n85555\n54133\n36973\n17554\n10996\n9974\n9712\n9235\n8134\n5617\n3643\n3497\n159\n109\nFigure 5: Number of problems per domain, as classified by gpt-4o-mini using math domains as\ndefined by Gao et al. [2024]. Note that the y-axis is log-scale.\nExample Problem: Math Word Problem\nQuestion: A shopper buys a 100 dollar coat on sale for 20% off. An additional 5 dollars are taken off the sale\nprice by using a discount coupon. A sales tax of 8% is paid on the final selling price. Calculate the total amount\nthe shopper pays for the coat. Express your answer in dollars, rounded to two decimal places.\nAnswer: 81.00\nExample Problem: Algebra\nQuestion: Real numbers 𝑥and 𝑦satisfy 4𝑥2 −5𝑥𝑦+ 4𝑦2 = 5. If 𝑠= 𝑥2 + 𝑦2, then\n1\n𝑠max +\n1\n𝑠min =?\nAnswer:\n8\n5\nExample Problem: Geometry\nQuestion: Let 𝐴𝐵𝐶𝐷be a cyclic quadrilateral, and suppose that 𝐵𝐶= 𝐶𝐷= 2 . Let 𝐼be the incenter of\ntriangle 𝐴𝐵𝐷. If 𝐴𝐼= 2 as well, find the minimum value of the length of diagonal 𝐵𝐷.\nAnswer: 2\n√\n3\nExample Problem: Statistics\nQuestion: A three-digit natural number 𝑎𝑏𝑐is termed a \"convex number\" if and only if the digits 𝑎, 𝑏, and 𝑐\n(representing the hundreds, tens, and units place, respectively) satisfy 𝑎< 𝑏and 𝑐< 𝑏. Given that 𝑎, 𝑏, and 𝑐\nbelong to the set {5, 6, 7, 8, 9} and are distinct from one another, find the probability that a randomly chosen\nthree-digit number 𝑎𝑏𝑐is a \"convex number\".\nAnswer:\n1\n3\nExample Problem: Calculus\nQuestion: Find the sum\n1\n2! + 2\n3! + 3\n4! + . . . +\n𝑛\n(𝑛+1)! and compute its limit as 𝑛→∞.\nAnswer: 1\nFigure 6: Example problems from the top-5 most common domains defined by Gao et al. [2024].\n13\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\nOperations research\nGeometry\nProbability theory\nCombinatorics\nNumber theory\nODE\nGeneral\nReal functions\nField theory\nOptimal control\nGame theory\nAlgebraic systems\nSequences and series\nComplex functions\nLinear algebra\nLogic\nParticle mechanics\nMathematics education\nMeasure and integration\nBiology\nDifference equations\nStatistics\nAlgebraic geometry\nConvex and discrete geometry\nHistory\nSpecial functions\nComputer science\nFunctional analysis\nDifferential geometry\nGroup theory\nOrdered algebraic structures\nDynamical systems\nGeneral topology\nInformation and communication\nIntegral transforms\nSystems theory\nOptics, electromagnetic theory\nThermodynamics\nIntegral equations\nAbstract harmonic analysis\nNumerical analysis\nOperator theory\nApproximations and expansions\nCommutative algebra\nAnalytic spaces\nFluid mechanics\nPotential theory\n101\n102\n103\n104\ncount\nFigure 7: Number of problems per domain, as classified by gpt-4o-mini using math domains as\ndefined by the 2020 Mathematics Subject Classification. Note that the y-axis is log-scale.\nExample Problem: Operations Research\nQuestion: A book was sold at a profit of 12%. Had it been sold for $18 more, a profit of 18% would have been\ngained. What is the cost price of the book?\nAnswer: $300\nExample Problem: Geometry\nQuestion: Find the angles at which the parabola 𝑦= 𝑥2 −𝑥intersects the x-axis.\nAnswer:\n3𝜋\n4 and 𝜋\n4\nExample Problem: Probability Theory\nQuestion: Ash and Gary independently come up with their own lineups of 15 fire, grass, and water monsters.\nThen, the first monster of both lineups will fight, with fire beating grass, grass beating water, and water beating\nfire. The defeated monster is then substituted with the next one from their team’s lineup; if there is a draw,\nboth monsters get defeated. Gary completes his lineup randomly, with each monster being equally likely to be\nany of the three types. Without seeing Gary’s lineup, Ash chooses a lineup that maximizes the probability 𝑝\nthat his monsters are the last ones standing. Compute 𝑝.\nAnswer: 1 −215\n315\nExample Problem: Combinatorics\nQuestion: Let 𝑥1 ≤𝑥2 ≤· · · ≤𝑥100 be real numbers such that |𝑥1|+|𝑥2|+· · ·+|𝑥100| = 1 and 𝑥1+𝑥2+· · ·+𝑥100 =\n0. Among all such 100-tuples of numbers, the greatest value that 𝑥76 −𝑥16 can achieve is 𝑚\n𝑛, where 𝑚and 𝑛\nare relatively prime positive integers. Find 𝑚+ 𝑛.\nAnswer: 841\nExample Problem: Number Theory\nQuestion: Suppose that (𝑢𝑛) is a sequence of real numbers satisfying 𝑢𝑛+2 = 2𝑢𝑛+1 + 𝑢𝑛and that 𝑢3 = 9 and\n𝑢6 = 128. What is 𝑢5?\nAnswer: 53\nFigure 8: Example problems from the top-5 most common domains defined by the 2020 Mathe-\nmatics Subject Classification.\n14\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\n3.3. Filter Analysis\nIn this section we provide an analysis of the filtering process we designed to curate Big-Math. See\nTable 2 for the main results and Appendix A for detailed results. Before looking closely at the filter\noutputs, it is important to note that our filtering process is intentionally designed to be more strict\nthan is necessary. This conservative approach ensures that Big-Math contains only the types of data\nthat we desire. However, this leaves room for improvements to our filtering process, and expansion to\nthe Big-Math dataset. The filtering set-up proposed in this work serves only as a first step, allowing\nus to identify areas for improvement to maximize the utility of the source data and develop more\nprecise filters.\nIn order to best identify where to spend future efforts, we start by identifying the filters which\nremove the most data: multiple-choice, multi-part, and proof filters.\n3.3.1. Multiple Choice Filter\nThe largest portion of removed data falls under the multiple choice filter (∼18%), with over 80% of\nthe multiple choice questions being sourced from the cn_k12 subset. Since this filter affects such a\nlarge number of problems, we see this as promising area for improvement in the future, where the\nrecall of filters can be improved to reintroduce some of the open-ended questions that are mistakenly\nclassified as multiple choice. Additionally, we find large discrepancies in the classifications of the\nregular expression filter compared with the model-based filter, especially for the olympiad and MATH\nsubsets, where the regular expression-based filter removes significantly more data than the model-\nbased filter. A closer look at these data, which are classified as positive by one filter but negative by\nthe other filter, are a prime area for further inspection. Exploration in this area can further improve\nthe recall of the whole filtering system, and may lead to 1,000s of reintroduced problems.\n3.3.2. Multi-Part Filter\nSimilarly, the multi-part filter shows potential for further refinement. Again, we see that the regular\nexpression-based filter consistently removes a larger portion of data than the model-based filter.\nSpecifically, we find that the regular expression-based filter removes 14,000 more questions from\nOrca-Math, 6,500 more from Olympiads, and 2,700 more from the MATH subset, compared with the\nmodel-based filter. Given these large differences, there is a clear opportunity to further improve this\nfiltering step.\n3.3.3. Proof Filter\nInterestingly, the proof filter has the opposite outcome from the multiple choice and multi-part filters:\nthe model-based filter removes 10,000 more problems than the regular expression-based filter. This is\nagain an area for further investigation to determine how many of those problems can be reintroduced\nto the dataset, while still fulfilling our desiderata.\n3.3.4. Asymptote Filter\nOne area that would be interesting to study further is a model’s ability to use the Asymptote vector\ngraphics language. In our work, we found 625 problems from the HARP subset using this language,\nand removed them all. However, this is not an insignificant portion of human-written problems, and\nmay be beneficial to include these problems. In contrast to our work, Muennighoff et al. [2025]\nevaluate a model on AIME24 [of America, 2024] which contains images in the asymptote language.\nPresumably, training on such problems would be beneficial for this setting, however, they do not\n15\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\ntrain a model on any asymptote images and do not analyze the performance of their model on such\nquestions. Therefore, whether including such problems in the training set is beneficial is still unclear.\n4. Future Directions\nWhile this work primarily focuses on the creation and curation of the Big-Mathand Big-Math-\nReformulateddatasets, this only represents the starting point for further research. Below we\noutline several promising directions for exploration.\nUsing this dataset:\n• Scaling - Investigate data scaling laws on RL training. In particular, there are a number of\ntraining algorithms, and new methods continually developing, and the efficiency of each is\nunknown [Team et al., 2025]. For example, in very recent studies, Hou et al. [2025] demonstrate\nthat training data and inference time compute scale in a complementary way (increasing\ninference is most useful when also scaling training data), and Setlur et al. [2025] demonstrate\nthat the gap between methods trained with verifier-based methods and verifier-free methods\nincreases as the quantity of training data increases.\n• Balancing - Future studies can analyze how to define and balance the quality, diversity, and\ncomplexity of a dataset [Havrilla et al., 2024a].\n• Leveraging difficulty - Our difficulty classifications can be used studies on subset selection\nfor RL, similar to the method proposed by Muennighoff et al. [2025]. In particular, it will be\ninteresting to explore how these difficulty scores translate to other models or model families.\nAdditionally, a potentially fruitful avenue of research is to explore if difficulty scores can be\nused to form an effective curriculum, where curriculum learning has been effectively applied to\nRL in the past [Dennis et al., 2021, Jiang et al., 2021, inter alia], but has only successfully been\napplied to training language models recently [Albalak et al., 2023, Wang et al., 2024]\n• Distillation - As has been done with other datasets [Liu et al., 2024, Guan et al., 2025, inter\nalia], Big-Math an also be used in concert with a strong reasoning model to generate a\ndataset for SFT distillation.\nImproving future datasets:\n• Filter and verifier improvements - As described throughout the paper, our filters are overly\nstrict. This allows for further improvements on our filters, enabling the inclusion of more\ncomplex problems. Additionally, throughout this paper, we have assumed the use of a very\nsimple verifier and only retained data that can be extracted based on simple string matching.\nHowever, the use of improved answer extraction and verifiers would allow for a much broader\ncoverage of data, including proofs and multi-part questions.\n• Model-based filtering - Refine and improve the process of designing model-based filters (e.g.\nprompting approaches). Integrating methods such as LLM-as-a-judge, or similar quality checks,\nto the model-based filtering can reduce the reliance on human oversight. Overall, the goal\nwould be to develop advanced filters with fewer human-in-the-loop requirements. For instance,\ncan RL training be used to train a filtering model, which in turn improves the RL training for a\nreasoning model? Probably! There exists huge quantities of positive and negative examples\nfor the filters that we use here. The question then becomes: can the RL training teach a model\nto filter for new requirements? For example, training the model to identify data where the\nanswer is not consistent with the model’s solution. With methods such as Meta-RL or continual\nlearning, this is a very exciting direction.\n16\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\n• Domain expansion - Some of the methodology in this work is specific to mathematics (e.g. proof\nfilter), but much of the work is applicable to other domains (e.g. multiple choice and multi-part\nquestion filters). Extending the methodologies developed here to other domains is a very\nmeaningful direction of study. A very recent example of generating verifiable question-answer\npairs is that from Yuan et al. [2025], which can be used as inspiration.\n5. Conclusion\nThere has been an incredible rise in the number of proposed methodologies on RL to train reasoning\nmodels [OpenAI et al., 2024, DeepSeek-AI et al., 2025, Liu et al., 2024, Yuan et al., 2024, Guan\net al., 2025], where each method requires a large-scale corpus of high-quality, diverse prompts with\nverifiable answers. While these recent works have proposed differing methodologies, many of them\nachieve similar results, suggesting that algorithms may not be the limiting factor in performance but,\nas has been proposed, data is the limiting factor on performance [Xiang et al., 2025, Yeo et al., 2025].\nThis work aims to address these issues by developing a large-scale corpus of high-quality, diverse\nmath problems, with verifiable answers, Big-Math. Through our human-in-the-loop filtering\nprocess, we bridge the gap between data quality and quantity, leading to a dataset of over 250,000\nproblems satisfying our three key desiderata: uniquely verifiable solutions, open-ended formulations,\nand closed-form answers. Furthermore, we introduce Big-Math-Reformulated, demonstrating\nthe potential to reclaim valuable data lost during filtering, contributing over 47,000 high quality\nproblems to the dataset. Through an in-depth analysis, we explored the diversity and difficulty of\nBig-Math, highlighting it’s utility in advancing the state of the art in LLM reasoning capabilities,\nand pointing out promising directions for the future development of filters.\nBy releasing Big-Math to the research community, we aim to catalyze progress in RL method-\nologies and establish a shared benchmark for future advancements in mathematical reasoning with\nLLMs. This dataset not only supports the development of more generalizable reasoning models but\nalso lays the groundwork for exploring scaling laws, training dynamics, and new RL algorithms in the\ncontext of mathematical problem-solving.\n17\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\nReferences\nAmro Abbas, Kushal Tirumala, Dániel Simig, Surya Ganguli, and Ari S. Morcos. Semdedup: Data-\nefficient learning at web-scale through semantic deduplication, 2023. URL https://arxiv.org/\nabs/2303.09540.\nServiceNow AI. Slam lab - r1-distill-sft dataset, 2025. URL https://hf.co/ServiceNow-AI/\nR1-Distill-SFT. Accessed: 2025-02-04.\nAlon Albalak, Liangming Pan, Colin Raffel, and William Yang Wang. Efficient online data mixing for\nlanguage model pre-training, 2023. URL https://arxiv.org/abs/2312.02406.\nAlon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas\nMuennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, Colin Raffel, Shiyu Chang, Tatsunori\nHashimoto, and William Yang Wang. A survey on data selection for language models, 2024. URL\nhttps://arxiv.org/abs/2402.16827.\nTianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V.\nLe, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: A comparative study of foundation\nmodel post-training, 2025. URL https://arxiv.org/abs/2501.17161.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.\nTraining verifiers to solve math word problems, 2021. URL https://arxiv.org/abs/2110.\n14168.\nDeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao\nZhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou,\nZhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng,\nChengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen,\nDongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei\nLi, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao,\nHui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie\nQiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan,\nKexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu,\nLeyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming\nLi, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge,\nRuisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan\nZhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S.\nLi, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding\nZeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao,\nWei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie,\nXingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin\nShen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia\nShan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun,\nYaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang,\nYixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng\nZou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu,\nYanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun\nZha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan\nZhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li,\nZiwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang.\n18\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\nDeepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL\nhttps://arxiv.org/abs/2501.12948.\nMichael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre Bayen, Stuart Russell, Andrew Critch,\nand Sergey Levine. Emergent complexity and zero-shot transfer via unsupervised environment\ndesign, 2021. URL https://arxiv.org/abs/2012.02096.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha\nLetman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn,\nAobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston\nZhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh\nTang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell,\nChristian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus\nNikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv\nChoudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin,\nEhab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang,\nGabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan\nPang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,\nImanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan\nGeffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer\nBillock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang,\nJiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe,\nJunteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield,\nKevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren\nRantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin,\nLovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi,\nMahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu\nRita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan,\nNaman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier\nDuchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng,\nPrajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao\nDong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic,\nRoberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly,\nRoss Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh,\nSean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy,\nSheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra,\nSpencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar\nHerman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher,\nTodor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan,\nViktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan\nXiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng\nXie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen\nSong, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen,\nZoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya\nGangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex\nVaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu,\nAndres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit\nRamchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin\nBharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie\nHuang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock,\n19\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\nBram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly\nBurton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang\nChu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer,\nDaniel Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana\nLiskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil,\nElaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute,\nEvan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni,\nFrancisco Guzmán, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada\nBadeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang,\nGuna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun\nHabeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Ibrahim Damlaj, Igor\nMolybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli,\nJaphet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy\nReizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon\nShepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U,\nKaran Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik\nVeeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle\nHuang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng\nGuo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish\nBhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso,\nMaxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko,\nMichelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike\nWang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini\nSanthanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas\nUsunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia\nHart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro\nRittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani,\nPritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu\nNayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari,\nRuss Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon,\nSasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay,\nShaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang,\nShuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max,\nStephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin\nCho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser,\nTamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy\nChou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish\nKumar, Vishal Mangla, Vítor Albiero, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir\nIvanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang,\nXiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu,\nYe Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao,\nYundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and\nZhiwei Zhao. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783.\nBofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma,\nLiang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran Quan,\nGe Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, and Baobao Chang. Omni-\nmath: A universal olympiad level mathematic benchmark for large language models, 2024. URL\nhttps://arxiv.org/abs/2410.07985.\n20\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\nEdouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, and Tomas Mikolov. Learning word\nvectors for 157 languages. In Proceedings of the International Conference on Language Resources and\nEvaluation (LREC 2018), 2018.\nXinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang.\nrstar-math: Small llms can master math reasoning with self-evolved deep thinking, 2025. URL\nhttps://arxiv.org/abs/2501.04519.\nAlex Havrilla, Andrew Dai, Laura O’Mahony, Koen Oostermeijer, Vera Zisler, Alon Albalak, Fab-\nrizio Milo, Sharath Chandra Raparthy, Kanishk Gandhi, Baber Abbasi, Duy Phung, Maia Iyer,\nDakota Mahan, Chase Blagden, Srishti Gureja, Mohammed Hamdy, Wen-Ding Li, Giovanni\nPaolini, Pawan Sasanka Ammanamanchi, and Elliot Meyerson. Surveying the effects of qual-\nity, diversity, and complexity in synthetic data from large language models, 2024a.\nURL\nhttps://arxiv.org/abs/2412.02980.\nAlex Havrilla, Sharath Raparthy, Christoforus Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi,\nEric Hambro, and Roberta Raileanu. Glore: When, where, and how to improve llm reasoning via\nglobal and local refinements, 2024b. URL https://arxiv.org/abs/2402.10963.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021. URL\nhttps://arxiv.org/abs/2103.03874.\nZhenyu Hou, Xin Lv, Rui Lu, Jiajie Zhang, Yujiang Li, Zijun Yao, Juanzi Li, Jie Tang, and Yuxiao Dong.\nAdvancing language model reasoning through reinforcement learning and inference scaling, 2025.\nURL https://arxiv.org/abs/2501.11651.\nMinqi Jiang, Edward Grefenstette, and Tim Rocktäschel. Prioritized level replay, 2021. URL https:\n//arxiv.org/abs/2010.03934.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, H’erve J’egou, and Tomas Mikolov.\nFasttext.zip: Compressing text classification models. arXiv preprint arXiv:1612.03651, 2016a.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efficient text\nclassification. arXiv preprint arXiv:1607.01759, 2016b.\nJuno Kim, Denny Wu, Jason Lee, and Taiji Suzuki. Metastable dynamics of chain-of-thought reasoning:\nProvable benefits of search, rl and distillation, 2025. URL https://arxiv.org/abs/2502.\n01694.\nBespoke Labs. Bespoke-stratos: The unreasonable effectiveness of reasoning distillation, 2025. URL\nhttps://hf.co/bespokelabs/Bespoke-Stratos-32B. Accessed: 2025-01-22.\nNathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman,\nLester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf,\nJena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini,\nNoah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. Tulu 3: Pushing frontiers\nin open language model post-training, 2024. URL https://arxiv.org/abs/2411.15124.\nDacheng Li, Shiyi Cao, Tyler Griggs, Shu Liu, Xiangxi Mo, Shishir G. Patil, Matei Zaharia, Joseph E.\nGonzalez, and Ion Stoica. Llms can easily learn to reason from demonstrations structure, not\ncontent, is what matters!, 2025. URL https://arxiv.org/abs/2502.07374.\n21\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\nJeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash\nGuha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Reinhard Heckel,\nJean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton,\nMarianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej Kilian,\nHanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani\nMarathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu, Thao Nguyen, Igor Vasiljevic, Sham\nKakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo,\nAlaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca\nSoldaini, Pang Wei Koh, Jenia Jitsev, Thomas Kollar, Alexandros G. Dimakis, Yair Carmon, Achal\nDave, Ludwig Schmidt, and Vaishaal Shankar. Datacomp-lm: In search of the next generation of\ntraining sets for language models, 2024a. URL https://arxiv.org/abs/2406.11794.\nJia Li,\nEdward Beeching,\nLewis Tunstall,\nBen Lipkin,\nRoman Soletskyi,\nShengyi Costa\nHuang,\nKashif Rasul,\nLonghui Yu,\nAlbert Jiang,\nZiju Shen,\nZihan Qin,\nBin Dong,\nLi Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu.\nNuminamath.\n[https:\n//huggingface.co/AI-MO/NuminaMath-CoT](https://github.com/project-numina/\naimo-progress-prize/blob/main/report/numina_dataset.pdf), 2024b.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan\nLeike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. arXiv preprint\narXiv:2305.20050, 2023a.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike,\nJohn Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step, 2023b. URL https:\n//arxiv.org/abs/2305.20050.\nZihan Liu, Yang Chen, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Acemath: Advancing\nfrontier math reasoning with post-training and reward modeling, 2024. URL https://arxiv.\norg/abs/2412.15084.\nDakota Mahan, Duy Van Phung, Rafael Rafailov, Chase Blagden, Nathan Lile, Louis Castricato,\nJan-Philipp Fränken, Chelsea Finn, and Alon Albalak. Generative reward models, 2024. URL\nhttps://arxiv.org/abs/2410.12832.\nIman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad\nFarajtabar. Gsm-symbolic: Understanding the limitations of mathematical reasoning in large\nlanguage models, 2024. URL https://arxiv.org/abs/2410.05229.\nArindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah. Orca-math: Unlocking the\npotential of slms in grade school math, 2024. URL https://arxiv.org/abs/2402.14830.\nNiklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke\nZettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time\nscaling, 2025. URL https://arxiv.org/abs/2501.19393.\nMathematical Association of America. Aime, February 2024. URL https://artofproblemsolving.\ncom/wiki/index.php/AIME_Problems_and_Solutions/.\nOpenAI, :, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low,\nAlec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard\nPassos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya\nKumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko,\nAndy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin\n22\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\nSokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton,\nBrandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming Li,\nCharles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger, Christopher\nHesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam,\nDavid Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li, Dragos Oprica,\nEben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric\nWallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni,\nFoivos Tsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista\nParascandolo, Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman,\nHaiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman,\nHyung Won Chung, Ian Kivlichan, Ian O’Connell, Ian Osband, Ignasi Clavera Gilaberte, Ilge Akkaya,\nIlya Kostrikov, Ilya Sutskever, Irina Kofman, Jakub Pachocki, James Lennon, Jason Wei, Jean\nHarb, Jerry Twore, Jiacheng Feng, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joaquin Quiñonero\nCandela, Joe Palermo, Joel Parish, Johannes Heidecke, John Hallman, John Rizzo, Jonathan Gordon,\nJonathan Uesato, Jonathan Ward, Joost Huizinga, Julie Wang, Kai Chen, Kai Xiao, Karan Singhal,\nKarina Nguyen, Karl Cobbe, Katy Shi, Kayla Wood, Kendra Rimbach, Keren Gu-Lemberg, Kevin Liu,\nKevin Lu, Kevin Stone, Kevin Yu, Lama Ahmad, Lauren Yang, Leo Liu, Leon Maksin, Leyton Ho, Liam\nFedus, Lilian Weng, Linden Li, Lindsay McCallum, Lindsey Held, Lorenz Kuhn, Lukas Kondraciuk,\nLukasz Kaiser, Luke Metz, Madelaine Boyd, Maja Trebacz, Manas Joglekar, Mark Chen, Marko\nTintor, Mason Meyer, Matt Jones, Matt Kaufer, Max Schwarzer, Meghan Shah, Mehmet Yatbaz,\nMelody Y. Guan, Mengyuan Xu, Mengyuan Yan, Mia Glaese, Mianna Chen, Michael Lampe, Michael\nMalek, Michele Wang, Michelle Fradin, Mike McClay, Mikhail Pavlov, Miles Wang, Mingxuan Wang,\nMira Murati, Mo Bavarian, Mostafa Rohaninejad, Nat McAleese, Neil Chowdhury, Neil Chowdhury,\nNick Ryder, Nikolas Tezak, Noam Brown, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivia Watkins,\nPatrick Chao, Paul Ashbourne, Pavel Izmailov, Peter Zhokhov, Rachel Dias, Rahul Arora, Randall\nLin, Rapha Gontijo Lopes, Raz Gaon, Reah Miyara, Reimar Leike, Renny Hwang, Rhythm Garg,\nRobin Brown, Roshan James, Rui Shu, Ryan Cheu, Ryan Greene, Saachi Jain, Sam Altman, Sam\nToizer, Sam Toyer, Samuel Miserendino, Sandhini Agarwal, Santiago Hernandez, Sasha Baker, Scott\nMcKinney, Scottie Yan, Shengjia Zhao, Shengli Hu, Shibani Santurkar, Shraman Ray Chaudhuri,\nShuyuan Zhang, Siyuan Fu, Spencer Papay, Steph Lin, Suchir Balaji, Suvansh Sanjeev, Szymon\nSidor, Tal Broda, Aidan Clark, Tao Wang, Taylor Gordon, Ted Sanders, Tejal Patwardhan, Thibault\nSottiaux, Thomas Degry, Thomas Dimson, Tianhao Zheng, Timur Garipov, Tom Stasi, Trapit Bansal,\nTrevor Creech, Troy Peterson, Tyna Eloundou, Valerie Qi, Vineet Kosaraju, Vinnie Monaco, Vitchyr\nPong, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wes McCabe, Wojciech Zaremba, Yann Dubois,\nYinghai Lu, Yining Chen, Young Cha, Yu Bai, Yuchen He, Yuchen Zhang, Yunyun Wang, Zheng Shao,\nand Zhuohan Li. Openai o1 system card, 2024. URL https://arxiv.org/abs/2412.16720.\nOpenAI, :, Ahmed El-Kishky, Alexander Wei, Andre Saraiva, Borys Minaev, Daniel Selsam, David\nDohan, Francis Song, Hunter Lightman, Ignasi Clavera, Jakub Pachocki, Jerry Tworek, Lorenz Kuhn,\nLukasz Kaiser, Mark Chen, Max Schwarzer, Mostafa Rohaninejad, Nat McAleese, o3 contributors,\nOleg Mürk, Rhythm Garg, Rui Shu, Szymon Sidor, Vineet Kosaraju, and Wenda Zhou. Competitive\nprogramming with large reasoning models, 2025. URL https://arxiv.org/abs/2502.06807.\nRylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities of large language models\na mirage?, 2023. URL https://arxiv.org/abs/2304.15004.\nAmrith Setlur, Nived Rajaraman, Sergey Levine, and Aviral Kumar. Scaling test-time compute without\nverification or rl is suboptimal, 2025. URL https://arxiv.org/abs/2502.12118.\nLuca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur,\nBen Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh\n23\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\nJha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas\nMuennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle\nRichardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke\nZettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and\nKyle Lo. Dolma: an open corpus of three trillion tokens for language model pretraining research,\n2024. URL https://arxiv.org/abs/2402.00159.\nKimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao,\nChenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan,\nEnzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao\nDing, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen\nYu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou\nWang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu,\nMengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma,\nShupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao\nHuang, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing\nZu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie\nWang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang,\nZaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi\nHuang, Zihao Huang, Ziyao Xu, and Zonghan Yang. Kimi k1.5: Scaling reinforcement learning\nwith llms, 2025. URL https://arxiv.org/abs/2501.12599.\nJiachen (Tianhao) Wang, Tong Wu, Dawn Song, Prateek Mittal, and Ruoxi Jia.\nGreats: On-\nline selection of high-quality data for llm training in every iteration.\nIn A. Globerson,\nL. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in\nNeural Information Processing Systems, volume 37, pages 131197–131223. Curran Associates,\nInc., 2024.\nURL https://proceedings.neurips.cc/paper_files/paper/2024/file/\ned165f2ff227cf36c7e3ef88957dadd9-Paper-Conference.pdf.\nYu Wang, Nan Yang, Liang Wang, and Furu Wei. Examining false positives under inference scaling for\nmathematical reasoning, 2025. URL https://arxiv.org/abs/2502.06217.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le,\nand Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023.\nURL https://arxiv.org/abs/2201.11903.\nViolet Xiang, Charlie Snell, Kanishk Gandhi, Alon Albalak, Anikait Singh, Chase Blagden, Duy Phung,\nRafael Rafailov, Nathan Lile, Dakota Mahan, Louis Castricato, Jan-Philipp Franken, Nick Haber,\nand Chelsea Finn. Towards system 2 reasoning in llms: Learning how to think with meta chain-of-\nthought, 2025. URL https://arxiv.org/abs/2501.04682.\nEdward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chain-of-\nthought reasoning in llms, 2025. URL https://arxiv.org/abs/2502.03373.\nLifan Yuan, Wendi Li, Huayu Chen, Ganqu Cui, Ning Ding, Kaiyan Zhang, Bowen Zhou, Zhiyuan Liu,\nand Hao Peng. Free process rewards without process labels, 2024. URL https://arxiv.org/\nabs/2412.01981.\nWeizhe Yuan, Jane Yu, Song Jiang, Karthik Padthe, Yang Li, Dong Wang, Ilia Kulikov, Kyunghyun\nCho, Yuandong Tian, Jason E Weston, and Xian Li. Naturalreasoning: Reasoning in the wild with\n2.8m challenging questions, 2025. URL https://arxiv.org/abs/2502.13124.\n24\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\nAlbert S. Yue, Lovish Madaan, Ted Moskovitz, DJ Strouse, and Aaditya K. Singh. HARP: A chal-\nlenging human-annotated math reasoning benchmark, 2024.\nURL https://github.com/\naadityasingh/HARP.\nLunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal.\nGenerative verifiers: Reward modeling as next-token prediction, 2024. URL https://arxiv.\norg/abs/2408.15240.\n25\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\nA. Filter Statistics\nMultiple Choice\nTrue/False\nYes/No\nMulti-Part\nProof\nSemantic\nTotal\nSubset\n(R)\n(L)\n(R)\n(L)\n(R)\n(L)\n(R)\n(L)\n(R)\n(L)\nDeduplication\nOrca-Math\n1174\n111\n5\n0\n12\n0\n16345\n2298\n2\n5\n8286\n26,487\ncn_k12\n94888\n94932\n420\n0\n384\n0\n20358\n19428\n3969\n5185\n2098\n123,809\nolympiads\n12123\n9507\n311\n0\n4615\n0\n13509\n6981\n23210\n30315\n1362\n53,102\nMATH\n1046\n386\n0\n0\n0\n3\n2734\n50\n0\n31\n7694\n9,714\naops_forum\n1588\n1162\n85\n0\n126\n0\n1508\n1488\n2282\n4605\n262\n7,853\nGSM8k\n45\n1\n0\n0\n0\n0\n782\n3\n0\n0\n7664\n7,999\nHARP\n92\n215\n2\n0\n7\n0\n253\n44\n0\n2\n155\n695\nOmni-MATH\n125\n29\n7\n122\n191\n169\n283\n212\n3\n207\n378\n1,182\namc_aime\n2955\n2938\n0\n0\n22\n0\n1018\n7\n36\n63\n53\n3,066\nTotal\n114,036\n109,281\n830\n122\n5,357\n172\n49,932\n30,511\n29,502\n40,413\n27,952\n233,907\nStatistics of the individual filters used to create Big-Math. (R) columns are regular expression\nfilters, while (L) columns are the result of filtering with Llama-3.1-70B.\n26\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\nB. Domains By Source\nDomain\nAbstract\nAlgebra\nCalculus\nDifferential\nDiscrete\nGeometry\nIntermediate\nLinear\nMath Word\nNumber\nOther\nPrealgebra\nPrecalculus\nStatistics\nSource\nAlgebra\nEquations\nMathematics\nAlgebra\nAlgebra\nProblems\nTheory\namc_aime\n0\n12\n0\n0\n12\n18\n2\n0\n14\n8\n3\n0\n0\n10\naops_forum\n42\n1263\n212\n3\n848\n1218\n27\n19\n495\n1065\n174\n4\n39\n458\nBig-Math-Reformulated\n17\n14362\n2725\n2\n1499\n8207\n3971\n1600\n4534\n1219\n939\n1059\n2629\n5186\ncn_k12\n17\n23282\n6169\n2\n1757\n12813\n4324\n1563\n3033\n774\n1039\n970\n5022\n4998\nGSM8k\n0\n17\n0\n0\n0\n12\n0\n0\n1810\n0\n0\n11\n0\n15\nHARP\n1\n677\n21\n0\n123\n690\n101\n4\n716\n250\n36\n50\n28\n340\nMATH\n12\n2752\n175\n0\n102\n1247\n532\n282\n992\n1145\n113\n258\n431\n1018\nolympiads\n59\n6458\n1603\n101\n3347\n9117\n456\n156\n5744\n2859\n780\n116\n1029\n2324\nOmni-MATH\n11\n547\n59\n0\n334\n479\n38\n7\n259\n355\n78\n42\n24\n301\nOrca-Math\n0\n4689\n29\n1\n111\n3102\n241\n3\n66563\n2285\n333\n3091\n11\n2843\nBreakdown of problems by source and domains (as defined by Gao et al. [2024]).\n27\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\nC. Reformulation Examples\n28\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\nSuccessful Reformulation\nOriginal Problem: What is the sum of the prime factors of 2010?\n(A) 67\n(B) 75\n(C) 77\n(D) 201\n(E) 210\nKey Information Extraction:\ncore_mathematical_concept: \"Number theory - prime factorization and sum of prime factors\"\nkey_information_extraction: [\"Find the sum of prime factors of 2010\", \"Prime factorization required\"]\nproblem_structure_analysis: \"Direct calculation problem\"\nmultiple_choice_removal_strategy: [\"Remove all answer choices\", \"Ask for direct calculation of sum of prime\nfactors\"]\nrephrasing_approach: [\"Request the sum of prime factors directly\", \"No comparative aspect needed\"]\nproblem_integrity_preservation: [\"Maintain the original number (2010)\", \"Require prime factorization\"]\nanswer_format_specification: [\"Answer should be a single integer\", \"No units or decimal places required\"]\nis_multiple_choice: true\nReformulation: Find the sum of the prime factors of 2010. Express your answer as a single integer.\nJudgement: The reformulated problem is a direct and clear request to find the sum of the prime factors of\n2010. It does not provide any multiple-choice options, nor does it limit the answer choices in any way. The\nproblem requires the solver to find the prime factorization of 2010 and then sum the prime factors, which is a\nwell-defined mathematical task.\nThe original problem and the reformulated problem are mathematically equivalent, and the solution to the\noriginal problem is still applicable to the reformulated problem. The reformulated problem does not introduce\nany new mathematical concepts or difficulties, and it does not provide any additional information that would\nmake the problem easier to solve.\nThe answer format specification is clear and unambiguous, requiring the solver to express the answer as a\nsingle integer. This is a suitable format for the problem, as the sum of the prime factors is a well-defined\ninteger value.\nOverall, the reformulated problem is a well-posed and mathematically sound problem that requires the solver\nto apply mathematical concepts and techniques to find the solution.\nFailed Reformulation\nOriginal Problem: Let 𝑥1, 𝑥2 be distinct positive real numbers and 𝑎be a real number in the interval (0, 1).\nDefine 𝑦1 =\n𝑥1\n1+𝑎+ 𝑎𝑥2\n1+𝑎and 𝑦2 =\n𝑥2\n1+𝑎+ 𝑎𝑥1\n1+𝑎. Determine the relationship between 𝑥1𝑥2 and 𝑦1𝑦2:\n(A) 𝑥1 · 𝑥2 > 𝑦1𝑦2\n(B) 𝑥1𝑥2 = 𝑦1𝑦2\n(C) 𝑥1𝑥2 < 𝑦1𝑦2\n(D) Cannot be determined, it depends on the value of 𝑎\nKey Information Extraction:\ncore_mathematical_concept: \"Inequalities involving real numbers and their products\"\nkey_information_extraction: [\"Distinct positive real numbers x1, x2\", \"Real number a in the interval (0,1)\",\n\"Expressions for y1 and y2 in terms of x1, x2, and a\", \"Need to compare the product x1*x2 with y1*y2\"]\nproblem_structure_analysis: \"Comparison problem requiring algebraic manipulation and inequality analysis\"\nmultiple_choice_removal_strategy: [\"Remove all comparisons and answer choices\", \"Ask for direct determina-\ntion of the relationship between x1*x2 and y1*y2\"]\nrephrasing_approach:\n[\"Keep the expressions for y1 and y2 intact\", \"Request the determination of the\nrelationship between x1*x2 and y1*y2\", \"Specify the possible relationships (>, <, =)\"]\nproblem_integrity_preservation:\n[\"Maintain all original expressions and conditions\", \"Remove comparative\naspect entirely\"]\nanswer_format_specification: [\"Answer should be expressed as one of the following: >, <, or =\", \"Include the\nrelationship between x1*x2 and y1*y2 in the answer\"]\nis_multiple_choice: true\nReformulation: Given distinct positive real numbers 𝑥1, 𝑥2 and a real number 𝑎in the interval (0,1), define\n𝑦1 = 𝑥1/(1 + 𝑎) + 𝑎* 𝑥2/(1 + 𝑎) and 𝑦2 = 𝑥2/(1 + 𝑎) + 𝑎* 𝑥1/(1 + 𝑎). Determine the relationship between\n𝑥1 * 𝑥2 and 𝑦1 * 𝑦2. Express your answer as one of the following: >, <, or =.\nJudgement: The reformulated problem still implies a multiple-choice format with the options >, <, or =,\nwhich may limit the answer choices and does not fully open the problem to exploration.\nFigure 9: Examples of the reformulation process starting from the original problem, we extract key\ninformation, and then use that information to reformulate the problem as an open-ended question.\nFinally, a critical evaluation of the reformulation determines whether the reformulation succeeded.\nWe include an example of both a successful reformulation (top) and failed reformulation (bottom).\n29\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\nD. Prompts\nD.1. Llama-3.1-70B Filter prompts\nMultiple Choice\nGiven this question: {problem}\nIs this a multiple choice question (a question that provides specific options to choose from, typically labeled as\nA, B, C, D or 1, 2, 3, 4)? Return only \"yes\" or \"no\" without any additional explanation.\nD.2. Reformulation Prompts\n30\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\nProof\nGiven this question: {problem}\nIs this a mathematical proof question (a question that asks to prove a statement, theorem, or property...)?\nExamples of proof indicators:\n- \"Prove that...\"\n- \"Show that...\"\n- \"Demonstrate why...\"\n- \"Justify your answer...\"\n- \"Explain why...\" Here are examples of proof questions:\nExample 1: Given positive integers 𝑎and 𝑏such that 𝑏> 𝑎> 1, and 𝑎does not divide 𝑏, and a given sequence\nof positive integers {𝑏𝑛}∞\n𝑛=1 satisfying 𝑏𝑛+1 ≥2𝑏𝑛for all positive integers 𝑛. Does there always exist a sequence\nof positive integers {𝑎𝑛}∞\n𝑛=1 such that for all positive integers 𝑛, 𝑎𝑛+1 −𝑎𝑛∈{𝑎, 𝑏}, and for all positive integers\n𝑚and 𝑙(which can be the same), 𝑎𝑚+ 𝑎𝑙/∈{𝑏𝑛} for all 𝑛?\nExample 2: Let 𝑓(𝑥) = 𝑥𝑛, 𝑥∈𝐷, 𝑛∈N+. Determine whether 𝑓(𝑥) is a solution to the functional inequality\n𝑓(𝑥) + 𝑓(1 −𝑥) > 1\nIf so, find the domain 𝐷; if not, provide an explanation.\nExample 3: In a right angled-triangle 𝐴𝐵𝐶, ∠𝐴𝐶𝐵= 90𝑜.\nIts incircle 𝑂meets 𝐵𝐶, 𝐴𝐶, 𝐴𝐵at 𝐷,𝐸,𝐹\nrespectively. 𝐴𝐷cuts 𝑂at 𝑃. If ∠𝐵𝑃𝐶= 90𝑜, prove 𝐴𝐸+ 𝐴𝑃= 𝑃𝐷.\nExample 4: A(x,y), B(x,y), and C(x,y) are three homogeneous real-coefficient polynomials of x and y with\ndegree 2, 3, and 4 respectively. we know that there is a real-coefficient polinimial R(x,y) such that 𝐵(𝑥, 𝑦)2 −\n4𝐴(𝑥, 𝑦)𝐶(𝑥, 𝑦) = −𝑅(𝑥, 𝑦)2. Show that there exist 2 polynomials F(x,y,z) and G(x,y,z) such that 𝐹(𝑥, 𝑦, 𝑧)2 +\n𝐺(𝑥, 𝑦, 𝑧)2 = 𝐴(𝑥, 𝑦)𝑧2 + 𝐵(𝑥, 𝑦)𝑧+ 𝐶(𝑥, 𝑦) if for any x, y, z real numbers 𝐴(𝑥, 𝑦)𝑧2 + 𝐵(𝑥, 𝑦)𝑧+ 𝐶(𝑥, 𝑦) ≥0\nExample 5: Prove\n1\ncos 0∘cos 1∘+\n1\ncos 1∘cos 2∘+ · · · +\n1\ncos 88∘cos 89∘= cos 1∘\nsin2 1∘.\nHere are examples of non-proof questions:\nExample 1: In a 100×25 rectangular table, each cell is filled with a non-negative real number. The number in the\n𝑖-th row and 𝑗-th column is denoted by 𝑥𝑖,𝑗(𝑖= 1, 2, . . . , 100; 𝑗= 1, 2, . . . , 25) (Table 1). The numbers in each\ncolumn of Table 1 are then reordered in descending order to create Table 2 such that 𝑥′\n1,𝑗≥𝑥′\n2,𝑗≥· · · ≥𝑥′\n100,𝑗\n(𝑗= 1, 2, . . . , 25). Find the smallest natural number 𝑘such that if the numbers in Table 1 satisfy ∑︀25\n𝑗=1 𝑥𝑖,𝑗≤1\n(𝑖= 1, 2, . . . , 100), then for 𝑖≥𝑘, Table 2 satisfies ∑︀25\n𝑗=1 𝑥′\n𝑖,𝑗≤1 (𝑖= 1, 2, . . . , 100).\nExample 2: We are given 2𝑛natural numbers\n1, 1, 2, 2, 3, 3, . . . , 𝑛−1, 𝑛−1, 𝑛, 𝑛.\nFind all 𝑛for which these numbers can be arranged in a row such that for each 𝑘≤𝑛, there are exactly 𝑘\nnumbers between the two numbers 𝑘.\nExample 3: Determine all positive integers 𝑛, 𝑛≥2, such that the following statement is true: If (𝑎1, 𝑎2, ..., 𝑎𝑛)\nis a sequence of positive integers with 𝑎1 +𝑎2 +· · ·+𝑎𝑛= 2𝑛−1, then there is block of (at least two) consecutive\nterms in the sequence with their (arithmetic) mean being an integer.\nExample 4: Turbo the snail sits on a point on a circle with circumference 1. Given an infinite sequence of\npositive real numbers 𝑐1, 𝑐2, 𝑐3, . . . , Turbo successively crawls distances 𝑐1, 𝑐2, 𝑐3, . . . around the circle, each\ntime choosing to crawl either clockwise or counterclockwise. Determine the largest constant 𝐶> 0 with the\nfollowing property: for every sequence of positive real numbers 𝑐1, 𝑐2, 𝑐3, . . . with 𝑐𝑖< 𝐶for all 𝑖, Turbo can\n(after studying the sequence) ensure that there is some point on the circle that it will never visit or crawl across.\nExample 5: For an even integer positive integer 𝑛Kevin has a tape of length 4𝑛with marks at −2𝑛, −2𝑛+\n1, . . . , 2𝑛−1, 2𝑛. He then randomly picks 𝑛points in the set −𝑛, −𝑛+1, −𝑛+2, . . . , 𝑛−1, 𝑛, and places a stone\non each of these points. We call a stone ’stuck’ if it is on 2𝑛or −2𝑛, or either all the points to the right, or all\nthe points to the left, all contain stones. Then, every minute, Kevin shifts the unstuck stones in the following\nmanner: He picks an unstuck stone uniformly at random and then flips a fair coin. If the coin came up heads,\nhe then moves that stone and every stone in the largest contiguous set containing that stone one point to the\nleft. If the coin came up tails, he moves every stone in that set one point right instead. He repeats until all the\nstones are stuck. Let 𝑝𝑘be the probability that at the end of the process there are exactly 𝑘stones in the right\nhalf. Evaluate\n𝑝𝑛−1−𝑝𝑛−2+𝑝𝑛−3−...+𝑝3−𝑝2+𝑝1\n𝑝𝑛−1+𝑝𝑛−2+𝑝𝑛−3+...+𝑝3+𝑝2+𝑝1 in terms of 𝑛.\nExample 6: A 0-1 sequence of length 2𝑘is given. Alice can pick a member from the sequence, and reveal it\n(its place and its value) to Bob. Find the largest number 𝑠for which Bob can always pick 𝑠members of the\nsequence, and guess all their values correctly. Alice and Bob can discuss a strategy before the game with the\naim of maximizing the number of correct guesses of Bob. The only information Bob has is the length of the\nsequence and the member of the sequence picked by Alice.\nReturn only \"yes\" or \"no\" without any additional explanation.\n31\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\nYes/No\nGiven this question: {problem} Is this a yes/no question (a question that asks to choose between two options,\ntypically labeled as yes or no)? Return only \"yes\" or \"no\" without any additional explanation.\nTrue/False\nGiven this question: {problem} Is this a true/false question (a question that asks to choose between two options,\ntypically labeled as true or false)? Return only \"true\" or \"false\" without any additional explanation.\nMultiple Part\nYour task is to determine if the given question contains multiple sub-questions, sub-parts, or sub-tasks. A multi-\npart question requires separate answers for different components, rather than a single comprehensive answer.\nBesides that, if the question is multiple choice and only requires to select one option, it is not a multi-part\nquestion.\nHere are examples of multi-part questions that require multiple distinct answers:\nExample 1: Given the set 𝑀= {0, 1}, 𝐴= {(𝑥, 𝑦)|𝑥∈𝑀, 𝑦∈𝑀}, 𝐵= {(𝑥, 𝑦)|𝑦= −𝑥+ 1}. 1. Please list the\nelements of set 𝐴. 2. Find 𝐴∩𝐵and list all subsets of 𝐴∩𝐵.\nExample 2: In the Cartesian coordinate system 𝑥𝑂𝑦, the parametric equation of curve 𝐶1 is\n{︂𝑥= cos 𝜃\n𝑦= 1 + sin 𝜃\n(where 𝜃is the parameter), and the equation of curve 𝐶2 is 𝑥2\n1 + 𝑦2\n2 = 1. With 𝑂as the pole and the non-negative\nhalf-axis of 𝑥as the polar axis, a polar coordinate system is established with the same unit of length as the\nCartesian coordinate system 𝑥𝑂𝑦. (1) Find the polar equations of curves 𝐶1 and 𝐶2; (2) The ray 𝜃= 𝜋\n3 (𝜌> 0)\nintersects curve 𝐶1 at point 𝐴(other than the pole) and intersects curve 𝐶2 at point 𝐵. Find |𝐴𝐵|.\nExample 3: Given the function 𝑓(𝑥) = |𝑥+2|−|2𝑥−𝑎|, (𝑎∈R). (I) When 𝑎= 3, solve the inequality 𝑓(𝑥) > 0;\n(II) When 𝑥∈[0, +∞), 𝑓(𝑥) < 3 always holds, find the range of 𝑎.\nExample 4: Given an ellipse 𝐶1:\n𝑥2\n𝑎2 + 𝑦2\n𝑏2 = 1 (𝑎> 𝑏> 0) with a major axis length of 4 and an eccentricity\nof 1\n2, where 𝐹1 and 𝐹2 are its left and right foci, respectively. A moving circle passes through point 𝐹2 and is\ntangent to the line 𝑥= −1. (I) (i) Find the equation of the ellipse 𝐶1; (ii) Find the equation of the trajectory\nof the center 𝐶of the moving circle; (II) On the curve 𝐶, there are two points 𝑀and 𝑁, and on the ellipse\n𝐶1, there are two points 𝑃and 𝑄, satisfying that 𝑀𝐹2 and −−→\n𝑁𝐹2 are collinear, −−→\n𝑃𝐹2 and −−→\n𝑄𝐹2 are collinear, and\n−−→\n𝑃𝐹2 · −−−→\n𝑀𝐹2 = 0, find the minimum value of the area of quadrilateral 𝑃𝑀𝑄𝑁.\nExample 5: In the rectangular coordinate system 𝑥𝑂𝑦, a polar coordinate system is established with the\ncoordinate origin as the pole and the positive semi-axis of the 𝑥-axis as the polar axis. The polar coordinate\nequation of circle 𝐶is 𝜌2 −2𝑚𝜌cos 𝜃+ 4𝜌sin 𝜃= 1 −2𝑚. (1) Find the rectangular coordinate equation of 𝐶\nand its radius. (2) When the radius of 𝐶is the smallest, the curve 𝑦=\n√\n3|𝑥−1| −2 intersects 𝐶at points 𝐴\nand 𝐵, and point 𝑀(1, −4). Find the area of △𝑀𝐴𝐵.\nHere are examples of single-part questions that require only one answer:\nExample 1: Ancient Greek mathematicians from the Pythagorean school studied various polygonal numbers,\nsuch as triangular numbers 1, 3, 6, 10, ..., with the 𝑛-th triangular number being 𝑛(𝑛+1)\n2\n= 1\n2𝑛2 + 1\n2𝑛. Let the\n𝑛-th 𝑘-sided polygon number be denoted as 𝑁(𝑛, 𝑘) (𝑘≥3). Below are the expressions for the 𝑛-th number of\nsome 𝑘-sided polygon numbers:\nTriangular numbers 𝑁(𝑛, 3) = 1\n2𝑛2 + 1\n2𝑛\nSquare numbers 𝑁(𝑛, 4) = 𝑛2\nPentagonal numbers 𝑁(𝑛, 5) = 3\n2𝑛2 −1\n2𝑛\nHexagonal numbers 𝑁(𝑛, 6) = 2𝑛2 −𝑛\n...\nFrom this, we can deduce the expression for 𝑁(𝑛, 𝑘) and calculate 𝑁(8, 12) = ______.’\nExample 2: Let x be the dividend, y be the divisor, z be the quotient, and r be the remainder.\nIf 𝑦=\n3(𝑦1 + 𝑦2) + 4, 𝑧= 2𝑧2\n1 −𝑧2, 𝑟= 3𝑟1 + 2, and 𝑥= 2𝑥1𝑦1 −𝑥2 + 10, find the values of 𝑥, 𝑦, 𝑧, and 𝑟, given that\n𝑦1 = 2, 𝑦2 = 3, 𝑧1 = 3, 𝑧2 = 5, 𝑟1 = 1, 𝑥1 = 4, and 𝑥2 = 6.\nExample 3: If 𝑥1, 𝑥2, . . . , 𝑥𝑛are positive real numbers with 𝑥2\n1 + 𝑥2\n2 + . . . + 𝑥2\n𝑛= 1 , find the minimum value of\n∑︀𝑛\n𝑖=1\n𝑥5\n𝑖\n𝑥1+𝑥2+...+𝑥𝑛−𝑥𝑖.\nExample 4: Given that the value of the function 𝑓(𝑥) =\n1\n(𝑥−2)2 −2𝑥+ cos 2𝜃−3 sin 𝜃+ 2 is always positive for\n𝑥∈(−∞, 2), determine the range of the parameter 𝜃within the interval (0, 𝜋).\nExample 5: A transparent, sealed cubic container is exactly half filled with water. When this cube is rotated\narbitrarily, the shape of the water surface inside the container can be: (1) triangle; (2) rectangle; (3) square;\n(4) regular hexagon. Among these, the correct conclusions are __________.\n32\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\nLlama-3.1-8B Rollout Prompt\nYou are a math expert. Given the following math problem, provide your solution in Latex format. Always\nformat your final answer in perfect LaTeX \\boxed{final_answer} format.\nLlama-3.1-405B Rollout Prompt (part 1)\nFrom this moment forward, adopt the persona of a tenured mathematics professor. You receive math problems\nand you solve them, step by step, following the formatting instructions below.\n# Mathematical Solution Formatting Guidelines\n## Mathematical Notation Rules\n**Use $$ $$ for:**\n- Standalone equations\n- Complex mathematical expressions\n- Multi-line equations\n- Final answers\n**Use single $ $ for:**\n- Simple inline expressions\n- Variables\n- Numbers with mathematical meaning\n- Parts of expressions being discussed\n**For equations with multiple lines:**\n‘latex\n$$\\\\begin{aligned}\nequation1 &= expression1 \\\\\\\\\nequation2 &= expression2\n\\\\end{aligned}$$\n‘\n**Place the final answer inside \\boxed{answer_here}**\nAfter solving and verifying your full solution, write the final answer in \\\\boxed{answer} notation.\n## Thought Structure\nEach thought must:\n1. Begin with active voice (\"I\" statements or direct observations)\n2. Express exactly ONE logical step\n3. Integrate mathematical notation naturally within sentences\n4. Use appropriate notation based on context\n5. No more than 1 sentence + interleaved math notation long\n### Example: \"I can see that when 𝑥= 2, the expression 𝑦= 𝑥2 + 3𝑥+ 1 becomes 𝑦= 4 + 6 + 1 = 11.\"\n## Common Mathematical Patterns\n- Use $$ $$ for equations being solved\n- Use $ $ for discussing components: \"where $m$ is the slope\"\n- Keep expressions inline when discussing specific parts\n- Use block notation ($$) for key steps and results\n## Visual Formatting\n- Two blank lines between thoughts (\\n \\n)\n- No bullet points or numbered lists\n- No section headers within the solution\n- Mathematical expressions should flow naturally within sentences\n- No explicit step labeling\n- Human-style reasoning\n—\n33\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\nLlama-3.1-405B Rollout Prompt (part 2)\n## Examples\n### Problem 1:\nA standard die is rolled six times. What is the probability that the product of all six rolls is odd? Express\nyour answer as a common fraction.\n### Solution 1:\nI need to find the probability by dividing favorable outcomes by total outcomes.\nI know that the total number of possible outcomes when rolling a die six times is 66.\nFor the product to be odd, each individual roll must be odd.\nLooking at a standard die, the odd numbers are 1, 3, and 5.\nTherefore, for each roll, I have only 3 choices to maintain an odd product.\nThe total number of favorable outcomes is 36 since I have 3 choices for each of the 6 rolls.\nThe probability is thus 36\n66 .\nThis simplifies to\n(︀1\n2\n)︀6 = \\\\boxed{ 1\n64}.\n### Problem 2:\nSolve over the integers: 2𝑎+ 4𝑏+ 8𝑐= 328\n### Solution 2:\nI notice that all terms are powers of 2, so I can rewrite everything with base 2.\nI know that 4 = 22 and 8 = 23, so the equation becomes 2𝑎+ 22𝑏+ 23𝑐= 328.\nTo solve this, I can decompose 328 into powers of 2.\nThe largest power of 2 less than 328 is 256 (28).\nSubtracting 256 from 328 leaves 72.\nThe largest power of 2 less than 72 is 64 (26).\nSubtracting 64 leaves 8 (23).\nTherefore, 328 = 28 + 26 + 23.\nLlama-3.1-405B Rollout Prompt (part 3)\nComparing terms: 2𝑎= 28, 22𝑏= 26, and 23𝑐= 23.\nSolving these equations: 𝑎= 8, 𝑏= 3, and 𝑐= 1.\nMy final answer is \\\\boxed{(a, b, c) = (8, 3, 1)}\n### Problem 3:\nFind the equation of the circle which passes through (2, 3) and (4, 5) and whose center lies on the straight line\n𝑦−4𝑥+ 3 = 0.\n### Solution 3:\nI know that the center lies on the line, so I can write its coordinates as (𝑥, 4𝑥−3).\nThe radius can be found using the distance from the center to (2, 3).\nUsing the distance formula, I get 𝑟2 = (𝑥−2)2 + (4𝑥−3 −3)2 = (𝑥−2)2 + (4𝑥−6)2.\nExpanding and simplifying: 𝑟2 = 17𝑥2 −52𝑥+ 40.\nSince the circle also passes through (4, 5), I can write 𝑟2 = (𝑥−4)2 + (4𝑥−3 −5)2.\nThis simplifies to 𝑟2 = 17𝑥2 −72𝑥+ 80.\nSince both expressions equal 𝑟2, I can write 17𝑥2 −52𝑥+ 40 = 17𝑥2 −72𝑥+ 80.\n34\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\nLlama-3.1-405B Rollout Prompt (part 4)\nI can now simplify: 20𝑥= 40, so 𝑥= 2.\nThe center is therefore (2, 4(2) −3) = (2, 5).\nThe radius squared is 𝑟2 = 17(2)2 −52(2) + 40 = 16, so 𝑟= 4.\nThe equation of the circle is (𝑥−2)2 + (𝑦−5)2 = 16.\nThe final answer is the full equation of the circle, so the correct final answer is \\\\boxed{(𝑥−2)2 +(𝑦−5)2 = 16}\n35\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\nMultiple Choice Reformulation (Part 1)\nYou are an AI assistant specializing in transforming multiple-choice math problems into open-ended, solvable\nquestions suitable for an automatic grading system that relies on regex pattern matching within ‘ ‘. Your task\nis to reformulate the given problem while adhering to specific guidelines.\n**Rules:**\n- Never turn problems into proofs, all problems should have an answer that is machine-verifiable\n- Reformulated problems should have the same final answer as the original problem (just not in the multiple-\nchoice format)\n- It must be reasonably clear from the problem what the student is expected to write inside of 𝑓𝑖𝑛𝑎𝑙_𝑎𝑛𝑠𝑤𝑒𝑟.\nE.g.\n\"What are its key properties and characteristics?\" is almost always a bad reformulation because it is\nambiguous what the student should write inside of the ‘ ‘. \"Express your answer in centimeters.\" is a good\nanswer format specification because it does NOT give away the answer but does specify a clear format.\n- Do NOT reformulate problems that are not actually multiple choice or are impossible to reformulate\n- If a problem is not multiple choice, return \"N/A\" for the reformulated problem and other \"N/A\" fields\n**Follow these steps:**\n1. **Analyze the original problem:**\n- Identify the core mathematical concept.\n- Note any crucial information, including numerical values, equations, and key terms.\n- Determine the structure of the problem (e.g., scenario-based, direct question, multi-step).\n- Preserve all mathematical notations, symbols, and formatting as they appear in the original problem.\n2. **Remove multiple-choice options:**\n- Eliminate all answer choices and their labels (A, B, C, etc.).\n- If any options contain information essential to solving the problem, incorporate that information into the\nmain problem statement.\n3. **Rephrase the question:**\n- Transform the question into an open-ended format that requires a direct numerical or algebraic answer.\n- Ensure the rephrased question is clear, unambiguous, and uses language appropriate for the student’s level.\n4. **Maintain problem integrity:**\n- Keep all original numerical values, equations, figures, and key terms intact.\n- Preserve any scenarios, dialogues, or conditional information crucial for solving the problem.\n- Do not introduce new information, alter units of measurement, or change the mathematical intent of the\nproblem.\n- If the problem references diagrams or figures, ensure that any necessary descriptions are included.\n5. **Specify the answer format:**\n- Instruct the student to provide their answer using the ‘ ‘ format.\n- Do not include placeholders like ‘ 𝑁‘; instead, guide the student to input their calculated answer within the\nboxed format.\n- For example, \"Provide your answer in the form ‘ your answer here ‘.\"\n6. **Final check:**\n- Ensure the reformulated problem contains all necessary information for independent solving.\n- Verify that the problem hasn’t become easier or harder than the original.\n- Check for any common errors, such as unit inconsistencies or typographical mistakes.\n- Confirm that no hints, solution methods, or additional explanations have been inadvertently added.\n7. **Is actually multiple choice:**\n- Some problems are not actually multiple choice and do NOT actually need to be reformulated.\n- If the problem is **NOT** multiple choice, do NOT reformulate it! Note here if it is not multiple choice and\nreturn \"N/A\" for the reformulated problem.\n**Before providing the final reformulated problem, please create a ‘reformulation_process‘ dictionary (for\ninternal use; do not include this dictionary or its content in the final problem). The dictionary should have the\nfollowing exact keys:**\n- ‘\"core_mathematical_concept\"‘: Summarize the core mathematical concept.\n- ‘\"key_information_extraction\"‘: List key information (numerical values, equations, terms).\n- ‘\"problem_structure_analysis\"‘: Identify the problem structure.\n- ‘\"multiple_choice_removal_strategy\"‘: Plan how to remove multiple-choice options.\n- ‘\"rephrasing_approach\"‘: Outline the rephrasing strategy.\n36\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\nMultiple Choice Reformulation (Part 2)\n- ‘\"problem_integrity_preservation\"‘: Note how to maintain problem integrity.\n- ‘\"answer_format_specification\"‘: Specify the answer format.\n- ‘\"is_multiple_choice\"‘: Whether the problem is actually multiple choice.\n**This structured approach will help ensure accuracy and completeness in your final output.**\n**After your reformulation process, present the reformulated problem as plain text, making sure it meets all\nthe criteria outlined above. Do not include the ‘<reformulation_process>‘ tags or any of their content in the\nfinal problem presented to the student.**\n**Examples:**\n“‘\n<example>\n<problem>\nThree squares with the same center and corresponding parallel sides are drawn. The distance between the sides\nof successive squares is 3 units, and the side length of the largest square is 22 units. What is the perimeter of\nthe smallest square?\n(A) 40\n(B) 64\n(C) 100\n(D) 10\n(E) 20\n</problem>\n<reformulation_process>\n\"core_mathematical_concept\": \"Geometry - concentric squares and their dimensions\",\n\"key_information_extraction\": [\n\"Three concentric squares\",\n\"Distance between sides of successive squares: 3 units\",\n\"Side length of the largest square: 22 units\",\n\"Need to find the perimeter of the smallest square\"\n],\n\"problem_structure_analysis\": \"Direct question based on given information\",\n\"multiple_choice_removal_strategy\": [\n\"Eliminate all answer choices and their labels\",\n\"No essential information is lost by removing the options\"\n],\n\"rephrasing_approach\": [\n\"Keep the description of the squares intact\",\n\"Rephrase the question to ask for the perimeter in an open-ended format\",\n\"Specify units and number format requirements\"\n],\n\"problem_integrity_preservation\": [\n\"Preserve all original numerical values and key terms\", \"Do not add any new information or hints\"\n],\n\"answer_format_specification\": [\n\"Answer should be expressed in units\",\n\"Answer should be given as a whole number\",\n\"No decimal places required based on original options\"\n],\n\"is_multiple_choice\": true\n</reformulation_process>\n<reasoning>\nLet me think about this step by step:\n1. Analyzing the multiple choice aspect:\n- The options are all whole numbers: 40, 64, 100, 10, 20\n- No units are specified in the options but units are used in the problem\n- The options suggest an exact answer is expected, not an approximation\n2. Looking at the essential information:\n- The geometric relationships are clearly stated\n- All measurements are given in units\n- No complex transformations or unit conversions needed\n37\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\nMultiple Choice Reformulation (Part 3)\n3. What needs to be preserved:\n- The concentric squares description\n- The 3-unit distance between squares\n- The 22-unit largest square side length\n- The focus on perimeter rather than area\n4. What needs to be clarified:\n- The answer should include units\n- A whole number is expected\n- No decimal places needed\n5. How to make it open-ended:\n- Remove all multiple choice options\n- Keep the geometric scenario exactly as is\n- Add specific answer format requirements\n- Maintain the same level of difficulty\n6. Final considerations:\n- The reformulated question should be solvable without reference to the original options\n- The specifications should ensure an unambiguous answer format\n- The mathematical challenge should remain unchanged\n</reasoning>\n<reformulated_problem>\nThree squares with the same center and corresponding parallel sides are drawn. The distance between the sides\nof each successive square is 3 units, and the side length of the largest square is 22 units. Find the perimeter of\nthe smallest square. Express your answer in units as a whole number.\n</reformulated_problem>\n</example>\n“‘\n“‘\n<example>\n<problem>\nThe teacher picked 9 out of 13 cards labeled with numbers 1 to 13 and stuck them on the foreheads of 9\nstudents. Each student can see the numbers on the other 8 students’ foreheads but not their own. The teacher\nasked, \"Those who know the number of divisors of their own number, please raise your hand.\" Two students\nraised their hands. After they put their hands down, three students had the following conversation.\n- A: \"I know my number now.\"\n- B: \"I still don’t know what my number is, but I have figured out whether it is odd or even.\"\n- C: \"My number is 2 less than B’s number and 1 more than A’s number.\"\nWhat is the sum of the numbers on the four cards that were not picked?\n</problem>\n<reformulation_process>\n\"core_mathematical_concept\": \"Number theory and logical deduction with combinatorics\",\n\"key_information_extraction\": [\n\"13 cards numbered from 1 to 13\",\n\"9 cards picked and assigned to students\",\n\"Students can see others’ numbers but not their own\",\n\"Two students know their number based on number of divisors\",\n\"Conversation between students A, B, and C revealing relationships\",\n\"Need to find sum of the 4 unpicked cards\"\n],\n\"problem_structure_analysis\": \"Complex logical deduction problem combining number theory with information\nfrom multiple stages\",\n\"multiple_choice_removal_strategy\": \"Not applicable as this is not a multiple choice question\",\n\"rephrasing_approach\": \"Not needed as problem is already in appropriate format\",\n\"problem_integrity_preservation\": \"No modifications needed\",\n\"answer_format_specification\": \"Answer should be expressed as a single integer (sum of the unpicked cards)\",\n\"is_multiple_choice\": false\n</reformulation_process>\n38\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\nMultiple Choice Reformulation (Part 4)\n<reasoning>\nLet me think about this step by step:\n1. First, assess if reformulation is needed:\n- This is not a multiple choice question\n- It already asks for a specific numerical answer\n- The format is clear and unambiguous\n2. Analyzing the problem structure:\n- It’s a complex logic puzzle\n- All information provided is necessary\n- The sequence of events matters\n- The dialogue provides crucial information\n3. Consider what might need clarification:\n- Answer format is implicitly clear (a sum)\n- No unit specification needed\n- No decimal places involved (working with integers 1-13)\n- No ambiguity in what constitutes an answer\n4. Check for potential improvements:\n- Current format is optimal for the problem type\n- Adding structure might make it harder to follow\n- The narrative flow is important for understanding\n5. Final decision:\n- No reformulation needed\n- Problem is already in ideal open-ended format\n- Answer requirements are clear (single integer sum)\n</reasoning>\n<reformulated_problem>\nN/A\n</reformulated_problem>\n</example>\n“‘\n“‘\n<example>\n<problem>\nIn the cube shown, 𝐿and 𝐾are midpoints of adjacent edges 𝐴𝐷and 𝐴𝐵, respectively. The perpendicular\ndistance from vertex 𝐹to the line segment 𝐿𝐾is 10 units. Which of the following measurements is closest to\nthe volume of the cube after subtracting 3 units?\n(A) 200 in3\n(B) 500 cm3\n(C) 0.01039 m3\n(D) Tie\n</problem>\n<reformulation_process>\n{\n\"core_mathematical_concept\": \"3D geometry involving cube volume calculation\",\n\"key_information_extraction\": [\n\"Points L and K are midpoints of adjacent edges AD and AB\",\n\"Perpendicular distance from F to LK is 10 units\",\n\"Need to subtract 3 units from final volume\"\n],\n\"problem_structure_analysis\": \"Geometry problem requiring volume calculation\",\n\"multiple_choice_removal_strategy\": [\n\"Remove all comparisons\",\n\"Ask for direct volume calculation\"\n],\n39\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\nMultiple Choice Reformulation (Part 5)\n\"rephrasing_approach\": [\n\"Keep geometric relationships intact\",\n\"Request exact volume calculation\",\n\"Specify units and precision\"\n],\n\"problem_integrity_preservation\": [\n\"Maintain all numerical values and geometric relationships\",\n\"Remove comparative aspect entirely\"\n],\n\"answer_format_specification\": [\n\"Answer should be expressed in cubic meters (m3)\",\n\"Round to 5 decimal places\",\n\"Include units in answer\"\n],\n\"is_multiple_choice\": true\n}\n</reformulation_process>\n<reasoning>\nLet me think about this step by step:\n1. First, I need to identify what makes this multiple choice:\n- The core task is actually calculating a specific volume\n- The multiple choice aspect is just about comparing to different unit measurements\n- The comparison isn’t fundamental to the mathematical concept being tested\n2. Looking at the original answer choices:\n- They use different units (in3, cm3, m3)\n- The m3 value has 5 decimal places (0.01039)\n- This suggests m3 with 5 decimal places is an appropriate precision level\n3. How to remove the multiple choice aspect:\n- Instead of asking which measurement is closest\n- Ask for the exact calculation\n- Need to specify one consistent unit to avoid ambiguity\n4. What to preserve:\n- All geometric relationships\n- The 3-unit subtraction\n- The original scenario and values\n5. What needs to be added:\n- Specific unit requirement (m3)\n- Decimal place requirement (5 places)\n- Clear instruction about including units\n6. The reformulated question should:\n- Be completely open-ended\n- Have a unique, calculable answer\n- Remove all traces of comparison\n- Maintain the same difficulty level\n</reasoning>\n<reformulated_problem>\nIn the cube shown, 𝐿and 𝐾are midpoints of adjacent edges 𝐴𝐷and 𝐴𝐵, respectively. The perpendicular\ndistance from vertex 𝐹to the line segment 𝐿𝐾is 10 units. Calculate the volume of the cube after subtracting\n3 cubic units. Express your answer in cubic meters (m3), rounded to 5 decimal places.\n</reformulated_problem>\n</example>\n“‘\n40\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\nMultiple Choice Reformulation Judge (Part 1)\nYou are a IMO judge, specialized in evaluating the quality of IMO math problem reformulations. Your task is\nto critically analyze whether a reformulated problem truly meets the criteria for being open-ended, challenging,\nand maintains the original problem’s integrity.\n**Important:**\n- Problems you review are being transformed from multiple choice problems into open-ended, solvable questions\nsuitable for an automatic grading system that relies on regex pattern matching within ‘ ‘.\nIt is VERY\nimportant that even though the problem is no longer multiple choice, it still has a single, definite answer.\n- It must be reasonably clear from the problem what the student is expected to write inside of 𝑓𝑖𝑛𝑎𝑙_𝑎𝑛𝑠𝑤𝑒𝑟.\nE.g.\n\"What are its key properties and characteristics?\" is almost always a bad reformulation because it is\nambiguous what the student should write inside of the ‘ ‘. \"Express your answer in centimeters.\" is a good\nanswer format specification because it does NOT give away the answer but does specify a clear format.\nKey aspects to evaluate:\n1. Hidden Multiple Choice\n- Check if the reformulation still effectively presents multiple choice options by:\n- Embedding a limited/fixed set of choices (e.g. 4 or 5 options) within the problem text that limits the answer\nchoices to one of those options\n- Asking to compare with specific values\n- Limiting answers to specific options\n- Flag any reformulations that are just disguised multiple choice questions\nExample:\n<problem>\nFor real numbers 𝑡̸= 0, the point\n(𝑥, 𝑦) =\n(︂𝑡+ 1\n𝑡\n, 𝑡−1\n𝑡\n)︂\nis plotted. All the plotted points lie on what kind of curve? (A) Line (B) Circle (C) Parabola (D) Ellipse (E)\nHyperbola Enter the letter of the correct option.\n</problem>\nReformulated failed example (all multiple choice aspect still present):\n<bad_reformulated_problem>\nFor real numbers 𝑡̸= 0, the point\n(𝑥, 𝑦) =\n(︂𝑡+ 1\n𝑡\n, 𝑡−1\n𝑡\n)︂\nis plotted. What type of curve do all the plotted points lie on? Provide your answer as a specific curve type\n(e.g., line, circle, parabola, ellipse, hyperbola).\n</bad_reformulated_problem>\nReformulated successful example (multiple choice aspect removed):\n<reformulated_problem>\nFor real numbers 𝑡̸= 0, the point\n(𝑥, 𝑦) =\n(︂𝑡+ 1\n𝑡\n, 𝑡−1\n𝑡\n)︂\nis plotted. Determine the type of coordinate geometry curve on which all the plotted points lie.\n</reformulated_problem>\n2. Mathematical Integrity\n- Verify that the mathematical difficulty remains unchanged\n- Ensure no accidental hints or simplifications were introduced\n- Check that all necessary information was preserved\n- Confirm no extraneous information was added\nExample 1:\nThis problem was not actually multiple choice, but it does imply options that are clearly missing/omitted.\n<problem>\nA resident wants to renovate their house and buys several strips of wood, each with a length of 0.7 meters and\n0.8 meters. By connecting some of these wood strips, many different lengths of wood can be obtained. For\nexample, 0.7 + 0.7 = 1.4 meters, 0.7 + 0.8 = 1.5 meters, etc. From the options below, what length of wood strip\ncannot be obtained by connecting these wood strips?\n</problem>\n41\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\nMultiple Choice Reformulation Judge (Part 2)\nExample 2:\n<problem>\n△𝐴𝐵𝐶is inscribed in a semicircle of radius 𝑟so that its base 𝐴𝐵coincides with diameter 𝐴𝐵.\nPoint\n𝐶does not coincide with either 𝐴or 𝐵.\nLet 𝑠= 𝐴𝐶+ 𝐵𝐶.\nThen, for all permissible positions of 𝐶:\n(A) 𝑠2 ≤8𝑟2\n(B) 𝑠2 = 8𝑟2\n(C) 𝑠2 ≥8𝑟2\n(D) 𝑠2 ≤4𝑟2\n(E) 𝑠2 = 4𝑟2\n</problem>\nThe reformulation is flawed because it prematurely focuses on a single inequality (𝑠2 ≤8𝑟2) rather than\ninviting exploration of the entire range of 𝑠2. It biases the solver, limits generality, and reduces the problem’s\nopen-ended nature.\n<bad_reformulated_problem>\n△𝐴𝐵𝐶is inscribed in a semicircle of radius 𝑟so that its base 𝐴𝐵coincides with diameter 𝐴𝐵.\nPoint 𝐶\ndoes not coincide with either 𝐴or 𝐵. Let 𝑠= 𝐴𝐶+ 𝐵𝐶. Prove or disprove the inequality 𝑠2 ≤8𝑟2 for all\npermissible positions of 𝐶.\n</bad_reformulated_problem>\nAn open-ended revision that requires the same analysis and leads to the same conclusion\n<reformulated_problem>\nLet triangle 𝐴𝐵𝐶be inscribed in a semicircle of radius 𝑟, with its base 𝐴𝐵coinciding with the diameter 𝐴𝐵.\nPoint 𝐶lies on the semicircle but does not coincide with 𝐴or 𝐵. Let 𝑠= 𝐴𝐶+ 𝐵𝐶. Determine the maximum\npossible value of 𝑠2 in terms of 𝑟, and prove that 𝑠2 ≤8𝑟2 for all permissible positions of 𝐶.\n</reformulated_problem>\nExample 3:\n<problem>\nFor real numbers 𝑡, the point\n(𝑥, 𝑦) =\n(︂1 −𝑡2\n1 + 𝑡2 ,\n2𝑡\n1 + 𝑡2\n)︂\nis plotted. All the plotted points lie on what kind of curve? (A) Line (B) Circle (C) Parabola (D) Ellipse (E)\nHyperbola Enter the letter of the correct option.\n</problem>\nAlthough it removes the multiple-choice format, it does not truly open the problem to exploration. The answer\nasks for a description, which introduces too much variability in responses <bad_reformulated_problem>\nFor real numbers 𝑡, the point\n(𝑥, 𝑦) =\n(︂1 −𝑡2\n1 + 𝑡2 ,\n2𝑡\n1 + 𝑡2\n)︂\nis plotted. What kind of curve do all the plotted points lie on? Provide a brief description of the curve.\n</bad_reformulated_problem>\nThis reformulation is clear, concise, and ensures consistent answers while remaining open-ended for exploration.\n<reformulated_problem>\nFor real numbers 𝑡, consider the point\n(𝑥, 𝑦) =\n(︂1 −𝑡2\n1 + 𝑡2 ,\n2𝑡\n1 + 𝑡2\n)︂\n.\nDetermine the type of curve on which all such points (𝑥, 𝑦) lie. State your answer as the name of the curve.\n</reformulated_problem>\nExample 4:\n<problem>\nDefine * as an operation on ordered pairs of real numbers, such that (𝑎, 𝑏) * (𝑐, 𝑑) = (𝑎𝑐+ 𝑏𝑑, 𝑎𝑑+ 𝑏𝑐). If\n(𝑎, 𝑏) * (𝑥, 𝑦) = (𝑎, 𝑏), then which of the following is (𝑥, 𝑦)? (A) (0, 0). (B) (0, 1). (C) (1, 0). (D) (1, 1). (E)\n(1, −1).\n</problem>\nAmbiguous about whether the equation holds for all (𝑎, 𝑏) or specific values, lacks context to ensure a unique\nsolution, and removes guidance provided by the multiple-choice format. Becomes unbounded.\n<bad_reformulated_problem>\nDefine * as an operation on ordered pairs of real numbers, such that (𝑎, 𝑏) * (𝑐, 𝑑) = (𝑎𝑐+ 𝑏𝑑, 𝑎𝑑+ 𝑏𝑐). If\n(𝑎, 𝑏) * (𝑥, 𝑦) = (𝑎, 𝑏), then calculate the ordered pair (𝑥, 𝑦). Express your answer in the format (𝑥, 𝑦), including\nparentheses and a comma.\n</bad_reformulated_problem>\n42\n\nBig-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models\nMultiple Choice Reformulation Judge (Part 3)\nIdeal because it clarifies the scope (applies to all (𝑎, 𝑏)), ensures a unique solution, and balances open-ended\nexploration with clear grading criteria\n<reformulated_problem>\nDefine * as an operation on ordered pairs of real numbers, such that\n(𝑎, 𝑏) * (𝑐, 𝑑) = (𝑎𝑐+ 𝑏𝑑, 𝑎𝑑+ 𝑏𝑐).\nFind the ordered pair (𝑥, 𝑦) that satisfies\n(𝑎, 𝑏) * (𝑥, 𝑦) = (𝑎, 𝑏)\nfor **all** real numbers 𝑎and 𝑏. Express your answer as (𝑥, 𝑦).\n</reformulated_problem>\n3. Answer Format Clarity\n- Evaluate if the answer format specification is:\n- Clear and unambiguous\n- Appropriate for the mathematical concept\n- Not overly unbounded or restrictive in a way that creates an open-ended problem out of it\n- Check if unit/precision requirements make sense for the problem\n4. Problem Independence\n- Verify the reformulated problem can stand alone\n- Ensure it doesn’t rely on knowledge of the original options\n- Check that answer requirements aren’t derived solely from original choices\nExample:\n<problem>\nWhich of the following is a root of the equation 𝑥2 −𝑥−6 = 0? (A) −3 (B) −2 (C) 2 (D) 3 (E) 6\n</problem>\nRelies on original options, failing to stand independently and limiting exploration.\n<bad_reformulated_problem>\nFind a root of the equation 𝑥2 −𝑥−6 = 0. Your answer must be one of the following: −3, −2, 2, 3, 6.\n</bad_reformulated_problem>\nIdeal because it ensures the solver identifies all roots without being constrained by the original options.\n<reformulated_problem>\nSolve the quadratic equation 𝑥2 −𝑥−6 = 0 and find all real roots. Provide your answers in increasing order.\n</reformulated_problem>\nRemember: A truly open-ended reformulation should allow for calculation and expression of the answer without\nany reference to or knowledge of the original multiple choice options.\n43\n",
  "metadata": {
    "source_path": "papers/arxiv/Big-Math_A_Large-Scale_High-Quality_Math_Dataset_for_Reinforcement\n__Learning_in_Language_Models_3cedf59056a5ff9e.pdf",
    "content_hash": "3cedf59056a5ff9ec8984c33c61f12e0dcba20e09e8b33434f29a6a606d39831",
    "arxiv_id": null,
    "title": "Big-Math_A_Large-Scale_High-Quality_Math_Dataset_for_Reinforcement\n__Learning_in_Language_Models_3cedf59056a5ff9e",
    "author": "",
    "creation_date": "D:20250225030616Z",
    "published": "2025-02-25T03:06:16",
    "pages": 43,
    "size": 2022737,
    "file_mtime": 1740470153.7210252
  }
}