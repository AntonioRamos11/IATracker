{
  "text": "Hotter and Colder: A New Approach to Annotating Sentiment, Emotions,\nand Bias in Icelandic Blog Comments\nSteinunn Rut Friðriksdóttir\nUniversity of Iceland\nsrf2@hi.is\nDan Saattrup Nielsen\nThe Alexandra Institute\ndan.nielsen@alexandra.dk\nHafsteinn Einarsson\nUniversity of Iceland\nhafsteinne@hi.is\nAbstract\nThis paper presents Hotter and Colder, a\ndataset designed to analyze various types\nof online behavior in Icelandic blog com-\nments.\nBuilding on previous work, we\nused GPT-4o mini to annotate approxi-\nmately 800,000 comments for 25 tasks, in-\ncluding sentiment analysis, emotion detec-\ntion, hate speech, and group generaliza-\ntions. Each comment was automatically\nlabeled on a 5-point Likert scale. In a sec-\nond annotation stage, comments with high\nor low probabilities of containing each ex-\namined behavior were subjected to man-\nual revision. By leveraging crowdworkers\nto refine these automatically labeled com-\nments, we ensure the quality and accuracy\nof our dataset resulting in 12,232 uniquely\nannotated comments and 19,301 annota-\ntions. Hotter and Colder provides an es-\nsential resource for advancing research in\ncontent moderation and automatically de-\ntectiong harmful online behaviors in Ice-\nlandic. We release both the dataset1 and\nannotation interface2.\n1\nIntroduction\nThe rapid growth of online communication plat-\nforms has led to an increase in harmful behav-\niors and, subsequently, an increased need for con-\ntent moderation (Mathew et al., 2019). Inappro-\npriate comments targeted at specific individuals or\ngroups of people can even go so far as qualifying\nas hate speech, but more subtle ways of spread-\ning these prejudiced ideas may, for instance, in-\nclude fear speech, where attempts are made to in-\ncite fear about a target community (Saha et al.,\n1https://repository.clarin.is/\nrepository/xmlui/handle/20.500.12537/352\n2https://github.com/icelandic-lt/\nannotation_if_sentiment\n2023).\nRecent work has focused on detecting\nthese toxic behaviors automatically, thereby less-\nening the cost and workload for human moderators\n(see Dehghan and Yanikoglu (2024), Nagar et al.\n(2023) and Mittal (2023) for instance).\nThis paper addresses limitations in previous\nwork on sentiment analysis in Icelandic (Friðriks-\ndóttir et al., 2024), using a new methodology to\nimprove class imbalance and low annotator agree-\nment in some tasks. Our approach first uses GPT-\n4o mini to analyze approximately 800,000 Ice-\nlandic blog comments across 25 tasks, including\nsentiment analysis, emotion detection, hate speech\ndetection, and group generalizations.\nFor most\ntasks, we employ focused binary annotation, tar-\ngeting only the extreme cases (highly likely or\nhighly unlikely to exhibit the behavior), rather\nthan using rating scales which have been shown\nto present challenges in maintaining consistent\nannotation quality (Kiritchenko and Mohammad,\n2017). The exception is sentiment analysis, where\nwe maintain the standard negative, neutral, and\npositive categories.\nThis targeted approach allows us to efficiently\nidentify rare but important cases (the proverbial\nneedles-in-a-haystack) such as hate speech com-\nments, which would be resource-intensive to lo-\ncate through random sampling as used in previous\nwork. To ensure dataset quality, we then employ\ncrowd workers to manually verify the model’s\npredictions, focusing particularly on comments\nflagged as highly likely or highly unlikely to con-\ntain problematic content. This human verification\nstep is crucial for maintaining accuracy and creat-\ning a high-consensus dataset.\nOur contributions are as follows:\n• We present Hotter and Colder, a dataset of\n12,232 Icelandic blog comments annotated\nfor 25 tasks including sentiment, emotions,\nhate speech, and group generalizations\narXiv:2502.16987v1  [cs.CL]  24 Feb 2025\n\n• We\nintroduce\na\ntwo-phase\nannotation\nmethodology combining GPT-4o mini silver\nlabels\nwith\ntargeted\nhuman\nverification\nto address class imbalance and improve\nannotation agreement\n• We release both the annotated dataset and an-\nnotation platform to support research in con-\ntent moderation for low-resource languages3\n2\nMethodology\nOur approach combines AI and human efforts in a\ntwo-phase annotation process designed to create a\nhigh-quality dataset for tasks where the phenom-\nena of interest are often rare. This scarcity poses\na significant challenge for dataset creation - ran-\ndom sampling would require extensive human an-\nnotation effort to find sufficient positive examples\nwhile focusing only on suspected positive cases\ncould bias the dataset. Our methodology aims to\nbalance these concerns by using AI to efficiently\nidentify potential cases across the full spectrum,\nfollowed by targeted human verification.\nIn the first phase (silver labeling), an LLM an-\nalyzes a large dataset of comments. For this ini-\ntial screening, we use GPT-4o mini with a prompt\ndesigned for structured output (see Section 2.1).\nWhile the model was instructed to consider it-\nself an expert in Icelandic blog analysis to main-\ntain consistent task framing across annotations,\nwe acknowledge this is a common but debatable\nprompting practice that warrants further investiga-\ntion. For all tasks except sentiment analysis, the\nLLM uses a 5-point scale for labeling to capture\nnuanced assessments.\nIn the second phase (gold labeling), human an-\nnotators review selected comments, focusing pri-\nmarily on those the LLM rated at the extremes\nof the scale (1 or 5). This design choice reflects\nour priority of establishing a foundational dataset\nwith clear, agreed-upon examples of each phe-\nnomenon. While this approach may not capture\nall nuanced edge cases, it serves several impor-\ntant purposes: (1) it enables efficient identification\nof clear positive examples for rare phenomena,\n(2) it helps establish reliable baseline annotations\nfor model evaluation, and (3) it aligns with find-\nings that human annotators achieve higher agree-\nment on clear cases (Kiritchenko and Mohammad,\n2017). We acknowledge this as a limitation - fu-\n3[links redacted]\nture work should explicitly target borderline cases\nto improve model robustness.\nHuman annotators perform binary (yes/no) an-\nnotations4 for a single task at a time to reduce task\nswitching fatigue. The simplified binary choice\nfor humans, compared to the LLM’s 5-point scale,\nreflects our focus on identifying clear instances\nwhile acknowledging that intermediate cases may\nrequire more nuanced future investigation.\nThis method of using a language model to iden-\ntify potential candidates for gold labeling builds\non established practices. For instance, when com-\npiling their GoEmotions dataset, Demszky et al.\n(2020) used a BERT-based model to filter out\ncomments that contained high levels of neutrality,\nleaving the more emotional comments for humans\nto annotate.\n2.1\nSilver Labeling Phase\nTo automate the initial labeling process, we cre-\nated a prompt for the AI model that instructed the\nmodel to perform all of the 25 annotation tasks on\na given blog comment in Icelandic5. The prompt\nincluded a JSON schema that instructed the model\non how to label a given comment. The context pro-\nvided to the model also included the previous com-\nments and the beginning of the blog post on which\nthe comments were posted. We used strictly struc-\ntured outputs to guarantee that the GPT-4o mini\nmodel always labeled each comment for each of\nthe 25 tasks and to make sure that it could only\noutput values that aligned with the Likert scale6.\n2.2\nData Selection\nFollowing the previous work of Friðriksdóttir\net al. (2024), the blog comments used in this\nwork all derive from the Icelandic blog platform\nblog.is. As one of the oldest and still active\nblogging platforms in Iceland, this website of-\nfers a valuable collection of online communica-\ntion, generating a wide range of debates between\npeople with different perspectives, which is partic-\nularly useful for our purposes. However, it should\nbe noted that the gender distribution of the site’s\nusers appears to be quite skewed. Blog.is has\nno obvious demographics accessible for users. In\n4Hick’s law states that increasing the number of choices\nwill increase the time it takes a person to make a decision\nlogarithmically (Hick, 1952).\n5https://gist.github.com/Haffi112/\n8813b738637fc9a678f524fdf9b5a5d9\n6See information on OpenAI’s website here.\n\nhis master thesis, however, Ásmundsson (2024)\nused a heuristic approach to determine the gen-\nder of the users based on their patronyms (tra-\nditionally, women’s last names in Icelandic end\nwith dóttir (e. daugther) and men’s last names\nend with son). Similarly, we observed that out of\n24,193 unique author names, 2,374 ended in “dót-\ntir”, 7,539 ended in “son” and 14,280 user names\ndid not match these endings.\n2.3\nTask Overview\nThe LLM was provided with the context of the\nblog post, previous comments, and the specific\ncomment to be analyzed. The system prompt for\nthe model was “You are an expert at analyzing Ice-\nlandic blog comments. Analyze the last comment\nshown and provide insights based on the given\nschema.”\nFor a given input, the model gener-\nated its analysis according to a predefined JSON\nschema, ensuring consistency across all evaluated\ncomments.\nThe analysis began with an overall sentiment\nclassification (positive, negative, or neutral) of\neach comment. The LLM then evaluated a wide\nrange of attributes, including toxicity, politeness,\nhate speech, social acceptability in various con-\ntexts, emotional content, sarcasm, constructive-\nness, encouragement, sympathy, trolling behavior,\nmansplaining, and group generalizations. For hate\nspeech, the model identified specific target groups\nand aggression levels when present. The analysis\nof group generalizations included assessing senti-\nment, factual validity, and whether the mentioned\ngroups were marginalized.\nMost attributes were rated on a 5-point Lik-\nert scale, where 1 indicated strong disagreement\nand 5 indicated strong agreement with the pres-\nence or intensity of the attribute7. For some at-\ntributes, such as sentiment (“positive”, “neutral”,\n“negative”) and gender (“male”, “female”, “non-\nbinary”, “n/a”), predefined categories were used\ninstead.\nWe selected our emotion categories based on\nthe foundational work of Ekman (1992); Ekman\nand Heider (1988), who identified seven basic\nemotions that appear to be universal across cul-\ntures: fear, happiness, sadness, surprise, disgust,\nanger, and contempt. To this set, we added in-\ndignation as it represents a distinct social emotion\n7Rubric: 1 - Strongly Disagree, 2 - Disagree, 3 - Neither\nAgree nor Disagree, 4 - Agree, 5 - Strongly Agree\nparticularly relevant to online discourse and con-\ntent moderation. Social acceptability was assessed\nacross various contexts, including conversations\nwith strangers, acquaintances, and close friends,\nin educational settings with different age groups,\nand in parliamentary speeches.\nThe LLM also inferred the author’s gender and\nwe further performed a majority vote over all an-\nnotations of a given username to assign a gender to\nthe author’s name. We note that gender inference\nin online spaces presents significant challenges.\nWhile traditional Icelandic naming conventions\ncan provide gender cues through patronymic suf-\nfixes (-son/-dóttir), we acknowledge several im-\nportant limitations in our approach to gender in-\nference:\n1. Users may choose pseudonyms that do not\nreflect their actual gender, particularly given\ndocumented patterns of gender-based harass-\nment online.\n2. The relationship between usernames and ac-\ntual gender identity is complex and cannot be\nreliably determined through automated anal-\nysis.\n3. Some users may intentionally obscure their\ngender or choose gender-neutral identifiers.\nWe emphasize that the inferred gender labels\nshould be treated as approximations of perceived\nrather than actual gender, particularly in analyses\nof gendered interaction patterns like mansplain-\ning. Future work should explore alternative ap-\nproaches to studying gendered communication\npatterns that do not rely on automated gender in-\nference.\n2.4\nHuman Annotation Process\nTo evaluate Icelandic blog comments, we devel-\noped a comprehensive annotation scheme cov-\nering various aspects of online discourse.\nHu-\nman annotators were provided with detailed in-\nstructions in Icelandic, emphasizing that their per-\nsonal judgment was crucial and that there were no\nstrictly right or wrong answers. Annotators were\ninstructed to base their decisions on the content of\nthe comments rather than the authors’ names, of\nwhich only initials and inferred gender were pro-\nvided.\nFor most tasks, annotators were asked to make\nbinary decisions (yes/no) about whether a com-\n\nment exhibited specific characteristics. The excep-\ntion was sentiment analysis, which used a three-\nway classification. Annotators could view preced-\ning comments and the original blog post for con-\ntext, although some images were no longer avail-\nable. They were also given the option to skip an-\nnotation for comments containing minimal infor-\nmation or those in languages other than Icelandic.\n2.4.1\nSentiment Analysis\nFollowing the approach of Wankhade et al. (2022),\nwe conducted sentiment analysis at the comment\nlevel. Annotators classified each comment as pos-\nitive, negative, or neutral based on their personal\ninterpretation.\nPositive sentiment was defined\nas expressing approval, happiness, satisfaction,\nor optimism.\nNegative sentiment indicated dis-\nsatisfaction, criticism, anger, or disappointment.\nNeutral sentiment was characterized by a lack of\nstrong emotion or a balanced view, often seen in\ninformational or factual statements.\n2.4.2\nToxicity\nWe adopted the definition of toxicity in online\ndiscussions from Klein and Majdoubi (2024), de-\nscribing it as behavior that is rude, disrespectful,\nor unreasonable, potentially making users feel un-\nwelcome or discouraged from participating in the\ndiscussion. Annotators were instructed to identify\ncomments containing insults, aggressive language,\nor content likely to incite conflict. This approach\nacknowledges the potential of toxic comments to\ndisrupt constructive dialogue and decrease user\nengagement, as observed in studies of online fo-\nrums (Young Reusser et al., 2024).\n2.4.3\nHate Speech\nOur hate speech annotation scheme was based on\nBasile et al. (2019) and aligned with Article 233\n(a) of the Icelandic penal code, an approach also\nused by Friðriksdóttir et al. (2024). Annotators\nidentified comments containing threats, defama-\ntion, or denigration based on protected character-\nistics such as nationality, color, race, religion, sex-\nual orientation, disabilities, or gender identity.\n2.4.4\nSocial Acceptance\nTo gauge social acceptability, annotators evalu-\nated whether it would be appropriate to make the\ncomment in question in various real-life contexts.\nThese included interactions with strangers, ac-\nquaintances, and close friends, as well as in ed-\nucational settings (for both young children and\nteenagers) and in parliamentary speeches.\nThis\nmulti-context approach allowed for a nuanced un-\nderstanding of perceived social norms across dif-\nferent situations.\n2.4.5\nEmotion Detection\nOur emotion detection task was inspired by the\nwork of Friðriksdóttir et al. (2024) and Demszky\net al. (2020). We simplified the task by asking an-\nnotators to detect the presence of a single emotion\nat a time in a binary fashion. In other words, to\nanswer whether or not a comment contained the\ngiven emotion. The emotions included were based\non basic emotions identified by Ekman (1992) and\nEkman and Heider (1988): fear, happiness, sad-\nness, surprise, disgust, anger, and contempt. We\nalso included indignation.\n2.4.6\nSarcasm\nFollowing the approach of Ptáˇcek et al. (2014), we\nasked the annotators to label whether a given com-\nment was sarcastic or ironic. In Icelandic, there is\na tendency to lump these two meanings together in\none (ice. kaldhæðni).\n2.4.7\nConstructiveness\nWe employed a simplified version of the annota-\ntion scheme from Kolhatkar et al. (2020), asking\nannotators to determine whether comments were\nconstructive. This binary classification focused on\nidentifying comments that provided useful feed-\nback or contributed positively to the discussion.\n2.4.8\nEncouragement and Sympathy\nInspired by Sosea and Caragea (2022), we asked\nannotators to identify encouragement and sympa-\nthy in comments in a binary fashion. Encourage-\nment was defined as inspirational words or support\nand sympathy was defined to be compassion, pity,\nor understanding of the situation of another per-\nson.\n2.4.9\nAdditional Annotations\nWe included several other classification tasks to\ncapture various aspects of online discourse:\nPoliteness: Annotators assessed whether com-\nments were polite, providing a measure of civility\nin online interactions.\nTrolling:\nFollowing the definition used by\nFriðriksdóttir et al. (2024), we asked annota-\ntors to identify comments that were intentionally\n\nprovocative, offensive, or off-topic, aimed at elic-\niting strong emotional responses or disrupting nor-\nmal discussion.\nMansplaining: The term has been defined by\nBridges (2017) as “a man explaining something to\na woman in a tone perceived as condescending,”\nbut has since been expanded to cover a broader\nrange of communicative behaviors (Smith et al.,\n2022).\nAnnotators were instructed to identify\ninstances where comments exhibited unsolicited,\npatronizing explanations based on the assumption\nthat the recipient is ignorant. Key characteristics\nof mansplaining include:\n• Persistence even when the recipient demon-\nstrates expertise.\n• Maintenance of an oversimplified approach.\n• Unwarranted confidence, sometimes even\nwhen factually incorrect.\nWhile mansplaining can occur between individu-\nals of any gender, annotators were instructed to use\nthe label only for male-to-female interactions. The\ngendered term highlights the frequency of this dy-\nnamic in male-female conversations, particularly\nin fields where women may have equal or superior\nexpertise. This annotation task aimed to reveal on-\ngoing societal assumptions about gender, knowl-\nedge, and competence, illustrating how gender-\nbased power dynamics continue to shape interper-\nsonal and professional communications.\nGroup Generalizations:\nAnnotators were\nasked to identify comments containing broad, of-\nten oversimplified statements about entire groups\nof people. These generalizations could be based\non characteristics such as race, gender, nationality,\nor political views. Importantly, annotators were\ninstructed to note that these generalizations could\nbe positive, negative, or neutral in nature. This\ntask aimed to capture instances where comments\nreflected biases, stereotypes, or assumptions about\ngroups, providing insight into how these general-\nizations manifest in online discourse.\n2.5\nAgreement Measures\nTo evaluate annotation quality and reliability, we\nemployed multiple agreement metrics. For tasks\nwith two or more annotations per comment, we\ncalculated pairwise agreement (PA) as the propor-\ntion of agreeing annotation pairs across all pos-\nsible pairs.\nFor assessing inter-annotator relia-\nbility, we utilized Krippendorff’s alpha (K’s α),\nwhich accounts for chance agreement and can\nhandle missing data — a common occurrence in\ncrowdsourced annotations. To evaluate the GPT-\n4o mini’s performance against human judgments,\nwe computed Cohen’s kappa (C’s κ) between the\nmodel’s predictions and the human consensus la-\nbels that were computed through a majority vote\n(examples with ties were dropped). For the senti-\nment analysis task, which involved three-way clas-\nsification, we adapted these measures to account\nfor the additional category whilst maintaining the\nsame computational framework.\n2.6\nAnnotation Interface\nThe annotation interface was designed to facili-\ntate efficient and accurate labeling of blog com-\nments while providing contextual information to\nannotators. The interface presents one comment\nat a time, along with metadata such as the au-\nthor’s initials, inferred gender, and timestamp. To\nenhance context, annotators can optionally view\nthe full blog post and previous comments in the\nthread where the same type of metadata is shown\nfor each author. Tasks are presented sequentially,\nwith clear instructions and the option to skip com-\nments when necessary. To maintain engagement\nand provide feedback, the interface incorporates\ngamification elements such as progress tracking\nand achievement badges.\nTo ensure data quality, the interface implements\nseveral key features. First, it allows annotators to\nreview task-specific guidelines at any point dur-\ning the annotation process. Second, the interface\noffers an optional real-time feedback mechanism\nthat compares human annotations to predictions\nfrom GPT-4o-mini, though annotators are explic-\nitly instructed to rely on their own judgment rather\nthan attempting to match the model’s output. This\ndesign balances the need for comprehensive con-\ntextual information with the goal of maintaining\nannotator focus and efficiency throughout the task.\n3\nResults\n3.1\nDistribution of AI labels\nBefore selecting comments for human annota-\ntions, we labeled all comments in the 25 different\ntasks using the GPT-4o mini model. The distri-\nbution of labels for each task that was labeled ac-\ncording to a Likert scale is shown in Figure 2 and\nthe distribution of labels in the sentiment task is\nshown in Figure 3. For sentiment analysis, we ob-\n\nFigure 1: Key components of the annotation platform: (left) The landing page introducing the project and\nits importance; (middle) The task overview dashboard displaying user progress and available annotation\ntasks; (right) An example of a specific annotation task (politeness assessment) showing the comment to\nbe annotated, contextual information, and annotation options.\nserve a somewhat balanced distribution of labels\nwith over 180,000 labels in each sentiment cate-\ngory. For tasks that were rated on a Likert scale,\nwe see great variability in the label distributions.\nSome tasks, such as toxicity, social acceptability\n(teacher to young children in an educational en-\nvironment, parliament speeches), emotion (anger,\ncontempt, indignation), and constructiveness have\na somewhat balanced distribution with a signifi-\ncant number of comments in each label category.\nTasks such as politeness and social acceptability\n(strangers, acquaintances, close friends, teacher\nto teenagers in an educational environment) are\nskewed to the right and have few comments rated\nas not having the property of the task. Other tasks\nare skewed to the left with few comments having\nthe property. For example, 6,672 comments were\nlabeled as having hate speech with strong agree-\nment. The most problematic tasks were “surprise”\nand “fear” with only 27 and 668 comments respec-\ntively labeled as having the properties with strong\nagreement.\nOur sampling strategy balanced the need for\ncross-task analysis with the goal of maximiz-\ning dataset diversity.\nWe began by creating a\nshared evaluation set of 100 comments selected\nuniformly at random from the full corpus. These\ncomments were set as annotation candidates for all\n25 tasks, providing a consistent benchmark for an-\nalyzing relationships between different aspects of\nonline discourse, such as how toxicity relates to\nemotion or constructiveness.\nFor each task, we then selected an additional\n1,100 comments that showed strong signals for\nthat specific behavior based on the LLM’s ratings\n(600 comments rated \"5\" and 500 rated \"1\"). To\nmaximize dataset diversity and reduce annotator\nfatigue, we excluded these task-specific comments\nfrom the selection in other tasks. This decision re-\nflects the distinct nature of our annotation tasks –\na comment exhibiting strong hate speech, for in-\nstance, might be uninformative for tasks like en-\ncouragement or constructiveness. By presenting\nannotators with fresh content for each task, we\naimed to maintain their engagement and avoid po-\ntential biases from repeated exposure to the same\ncomments. Additionally, since we focus on ex-\ntreme cases, reusing comments across tasks could\nlead to redundancy, as comments rated extreme in\none dimension often represent neutral or irrelevant\ncases for other dimensions.\nThe resulting dataset of comment candidates8\nfor human evaluation contains 1,200 comments\nper task (100 shared + 1,100 task-specific). While\nthis design limits comprehensive cross-task analy-\nsis to the shared set of 100 comments, it provides\nrich, focused data for developing robust classifiers\nfor each individual task. Future work could ex-\nplore the possibility of annotating a larger shared\nset of comments across all tasks, which would\nenable more comprehensive analysis of task rela-\ntionships while potentially sacrificing some task-\nspecific coverage.\n3.2\nAnnotator Statistics\nThe dataset comprises annotations from 170\nunique annotators with an average age of 37.61\nyears. The educational background of the annota-\ntors is diverse, with the majority holding advanced\ndegrees: 36.5% have a master’s degree, 22.9%\nhave a bachelor’s degree, and 5.9% have a PhD.\nThe gender distribution is nearly balanced, with\n47.6% male and 49.4% female annotators, while a\nsmall percentage identify as other (2.4%) or pre-\n8Note that not all comments were fully annotated in all\ntask categories.\n\nFigure 2: Distribution of AI labels on tasks that\nwere rated from 1 to 5 on a Likert scale.\nFigure 3: Distribution of AI labels for the senti-\nment analysis task.\nfer not to say (0.6%). In terms of participation,\nthere is a notable disparity between the average\nand median number of annotations per user (113.7\nand 27.5 respectively), suggesting that while some\nannotators contributed extensively, the typical an-\nnotator provided a more modest number of anno-\ntations.\nThe recruitment and motivation of crowdwork-\ners for annotation tasks can be a challenge. Most\nof our participants were recruited through targeted\nFacebook groups, with advertisements highlight-\ning the potential societal benefits of training mod-\nels to detect hate speech and toxic online behav-\nior. This framing likely contributed to the rela-\ntively high number of annotations in these cate-\ngories. However, task participation decreased for\ntasks presented later in the annotation sequence,\nleading to an uneven number of annotations across\ntasks and a potential annotator bias in those that\nhad a lower number of total annotations. This sug-\ngests that fatigue or prioritization may have influ-\nenced the workers’ engagement with certain tasks,\nparticularly those positioned further down the task\nlist. In future work, this issue could be mitigated\nby randomizing the order in which tasks are pre-\nsented to each crowd worker, thereby ensuring a\nmore balanced distribution of participation across\ntasks.\n3.3\nAgreement\nTable 1 presents an overview of the annotation\nstatistics and agreement measures for each task in\nour study. We report several metrics to provide a\ncomprehensive view of the annotation quality and\nthe performance of our AI model compared to hu-\nman annotators.\nTo assess the reliability of the annotations,\nwe calculated Krippendorff’s alpha (Krippendorff,\n2018, K’s α) for inter-annotator agreement. The\nresults varied considerably across tasks, with some\nshowing strong agreement (e.g., disgust:\n0.92,\nsympathy:\n0.83) and others showing weaker\nagreement (e.g., mansplaining: 0.07, fear: 0.24).\nThis variability suggests that some concepts were\n\nmore challenging to annotate consistently than\nothers. It may be noted that the instructions for\nmansplaining were more specific for the human\nannotators than for GPT-4o mini as they explic-\nitly mentioned that the comment should be from\na man to a woman. However, that is often an im-\nplicit understanding of the word.\nTo evaluate the performance of our AI model\nagainst human consensus, we computed Cohen’s\nkappa (Cohen, 1960, C’s κ) between the AI pre-\ndictions and the aggregated human labels.\nThe\nAI model showed moderate to substantial agree-\nment with human annotators on several tasks, in-\ncluding politeness (0.82), social acceptability in\neducational settings (0.74), and emotion detec-\ntion for anger and joy (both 0.68). However, the\nmodel struggled with more nuanced tasks such as\nmansplaining (0.17) and sarcasm detection (0.23).\nInterestingly, some tasks exhibited a discrep-\nancy between human inter-annotator agreement\nand AI-human agreement. For instance, the sym-\npathy task had high human agreement (K’s α =\n0.83) but low AI-human agreement (C’s κ = 0.24),\nsuggesting that while humans consistently identi-\nfied sympathy, the AI model had difficulty captur-\ning this concept accurately. However, it should\nbe noted that while certainly a valid translation\nfor “sympathy”, the Icelandic term “samúð” has\na tendency to be linked exclusively to condolences\nmade on the occasion of the death of a person’s rel-\native or friend. It is therefore conceivable that our\nhuman annotators have a more narrow understand-\ning of the word than that used by the AI model.\nThe sentiment analysis task, which involved a\nthree-way classification, showed moderate agree-\nment both among human annotators (K’s α = 0.64)\nand between the AI and human consensus (C’s κ\n= 0.59).\nThe results highlight the varying degrees of\ndifficulty in annotating different aspects of on-\nline discourse.\nWhile some tasks, particularly\nthose related to basic emotions and clearly defined\nconcepts, showed high agreement, others involv-\ning more nuanced or context-dependent judgments\nproved more challenging for both human annota-\ntors and our AI model. Most of the time, if a task\nhas low inter-annotator agreement, the human-AI\nagreement will also be low, indicating that con-\ncepts like sarcasm and trolling are simply diffi-\ncult to detect in text. It is, however, interesting\nto note the cases where inter-annotator agreement\nis high but human-AI agreement is low. For in-\nstance, GPT-4o mini does not seem to have a good\ngrasp of the emotions disgust and surprise.\n4\nDiscussion\nThe gold standard, human annotated Hotter and\nColder dataset is relatively small. While its main\npurpose is to serve as validation for the AI-labeled\nsilver dataset, it can also be used as training\ndata for few-shot learning models.\nThe silver\ndataset offers considerable flexibility, supporting\nthe training of models for individual tasks, such as\nthe automated detection of hate speech. However,\nthe utility of both datasets extends beyond single-\ntask applications.\nMulti-Task Learning (MTL)\nallows a model to tackle multiple tasks simulta-\nneously, drawing on shared representations and\ninsights across tasks to improve overall perfor-\nmance.\nIn sentiment analysis, for example, an\nMTL framework enables a more nuanced under-\nstanding of human communication.\nTan et al.\n(2023) demonstrate how sarcasm detection can\nsignificantly enhance the performance of senti-\nment analysis models, particularly in identifying\nnegative sentiment in sarcastic contexts. Our re-\nsults indicate that sarcasm detection remains a\nchallenge, likely contributing to the suboptimal\nperformance of the model in the sentiment anal-\nysis task. Given that Icelandic humor often relies\non sarcasm, this cultural factor may explain some\nof the difficulties the model encounters in this task.\nConsequently, it is plausible that an Icelandic sen-\ntiment analysis model would benefit from an MTL\napproach, particularly one that integrates sarcasm\ndetection as a complementary task.\nWhen working with multilingual LLMs, cul-\ntural norms exhibited by the model might not\nalways match those of the country in ques-\ntion (Meadows et al., 2024). Rather, these mod-\nels reflect the cultural, legal, and ideological val-\nues of their creators.\nTao et al. (2024) show-\ncased that GPT-4o mini generally mirrors values\nthat are commonly found in English-speaking and\nProtestant European countries. While this cultural\nbias may not be inherently problematic for our\npurposes, it could lead to reduced agreement be-\ntween human annotators and AI models in culture-\nspecific annotations. For instance, ethical align-\nment performed during model training may influ-\nence the model’s ability to judge appropriateness\nin social contexts.\nA model might consistently\n\nTask\nCount\nA ≥2\nAAPC\nK’s α\nC’s κ\nEmotion disgust\n355\n32\n1.09\n0.86\n0.53\nSocial acceptability acquaintances\n395\n44\n1.11\n0.77\n0.71\nEmotion contempt\n342\n37\n1.11\n0.77\n0.63\nEmotion surprise\n359\n23\n1.07\n0.75\n0.35\nEncouragement presence\n448\n69\n1.17\n0.74\n0.66\nEmotion joy\n525\n127\n1.28\n0.69\n0.69\nEmotion sadness\n381\n49\n1.13\n0.68\n0.50\nEmotion anger\n547\n106\n1.22\n0.67\n0.72\nPoliteness\n749\n286\n1.49\n0.67\n0.80\nSocial acceptability educational young\n448\n68\n1.16\n0.65\n0.73\nGroup generalization presence\n585\n156\n1.32\n0.64\n0.62\nSocial acceptability strangers\n526\n129\n1.29\n0.62\n0.76\nHate speech presence\n877\n429\n1.70\n0.61\n0.60\nSentiment\n1099\n837\n2.64\n0.61\n0.61\nSocial acceptability educational older\n390\n51\n1.14\n0.58\n0.75\nConstructiveness\n464\n71\n1.16\n0.53\n0.53\nSympathy\n460\n63\n1.15\n0.53\n0.25\nToxicity\n981\n585\n2.01\n0.52\n0.65\nSocial acceptability close friend\n381\n33\n1.09\n0.44\n0.36\nEmotion fear\n384\n48\n1.13\n0.43\n0.60\nSocial acceptability parliament\n404\n58\n1.15\n0.39\n0.51\nTrolling behavior\n511\n111\n1.25\n0.38\n0.47\nEmotion indignation\n354\n26\n1.08\n0.33\n0.56\nSarcasm\n507\n89\n1.19\n0.29\n0.26\nMansplaining\n572\n136\n1.29\n0.28\n0.21\nAverage\n521.76\n146.52\n1.30\n0.58\n0.56\nTable 1: Overview of the annotations by task. The count column represents the number of comments\nannotated for each task. The A ≥2 represents the number of comments with two or more annotations.\nAAPC represents the average number of non-skipped annotations per comment. K’s α corresponds to\nKrippendorff’s α amongst the human annotators in the task. Finally, C’s κ refers to Cohen’s κ between\nthe AI model and a human consensus label. The last row shows the total for the first two numerical\ncolumns and a macro average for the other columns.\nclassify toxic or hateful comments as unaccept-\nable, even when human annotators might tolerate\nsuch comments in specific contexts, such as in pri-\nvate conversations among friends or informal par-\nliamentary discourse. These nuances in cultural\nand ethical standards may hinder the model’s per-\nformance in tasks requiring a deep understanding\nof social norms and context.\nOn the flip side of the coin, Hotter and Colder\nadditionally offers invaluable insight into the soci-\nolinguistic patterns of a small online community.\nFuture research will i.a. include an analysis of how\ndiscourse changes in liaison with current events,\nwhich communities are most affected by toxic be-\nhaviors and hate speech, and the characteristics of\ntoxic users.\n5\nConclusion\nThis study presents Hotter and Colder, a dataset\nannotated for 25 tasks that examine various types\nof online behaviors. By leveraging both AI-based\nsilver labeling and human-in-the-loop gold label-\ning, we ensure a comprehensive approach to anno-\ntating toxic behaviors, emotions, sentiments, and\nmore in Icelandic blog comments.\nThis dual-\nphase annotation methodology enabled the iden-\ntification of rare but critical instances of harmful\nspeech while maintaining high annotator agree-\nment across a variety of tasks.\nThe introduction of a Multi-Task Learning\nframework as a future direction holds promise for\nimproving the detection of complex phenomena,\nsuch as sarcasm, which remains a challenge for\nboth AI models and human annotators, particu-\nlarly in culturally specific contexts. By integrat-\ning tasks such as sarcasm detection with sentiment\nanalysis, future models may achieve greater accu-\nracy and nuanced understanding in detecting vari-\nous forms of harmful and toxic speech.\nHotter and Colder lays the foundation for fu-\nture work on mitigating bias and improving ethical\nalignment in AI models for Icelandic, hopefully\nfostering safer and more inclusive online environ-\nments.\n6\nEthical Considerations\nIn our efforts to recruit crowd workers, we ap-\npealed mostly to their desire to fight against toxic\nonline behavior and to help aid in the eventual\ncreation of an automatic content moderation tool.\n\nRecruiting crowd workers without offering com-\npensation for their work can be considered prob-\nlematic.\nWe acknowledge that this fact is the\nlikely cause for the relatively unbalanced annota-\ntions across tasks. In our case, participants were\ninformed during the recruitment process that a ran-\ndom participant would receive a prize. However,\nwith sufficient financing, it would be more sustain-\nable and fair towards the participants to pay each\nannotator based on their contributions.\nFurthermore, the content in question is inher-\nently problematic in nature. We instructed users\nto only participate in tasks they were comfortable\nwith and warned them about potential triggers in\nthe content. One user pointed out to us that only\nbeing able to label one task at a time for each\ncomment can be unpleasant. For instance, a com-\nment can both have a positive sentiment and ex-\nhibit hate speech at the same time. Furthermore,\nseveral of the comments will likely be of mixed\nvalence but the annotators were only able to label\nthe comments on either a binary or a 3-class label-\ning scheme. We acknowledge this limitation.\nWe also acknowledge that we studied gender\nfrom a binary perspective. We decided to go for\nthat approach since non-binary gender identities\ncan be significantly harder to infer based on user-\nnames.\nWe encourage future researchers to be\nmore inclusive in their research.\nWe acknowledge the significant computational\nresources and associated carbon footprint involved\nin using GPT-4o mini to analyze 800,000 com-\nments, especially given the final dataset size of ap-\nproximately 12,000 annotated comments. While\nthis approach may appear computationally ineffi-\ncient at first glance, it served a crucial method-\nological purpose: identifying rare but important\ncases of problematic content that would have been\nextremely resource-intensive to locate through\nrandom sampling alone. Traditional approaches\nrequiring human annotators to sift through hun-\ndreds of thousands of comments to find relatively\nrare instances of hate speech or other harmful\ncontent would have been prohibitively expensive\nand potentially more damaging to annotator well-\nbeing through extended exposure to toxic con-\ntent. Future work should explore more environ-\nmentally sustainable approaches, such as using\nsmaller, task-specific models for initial filtering or\ndeveloping more efficient sampling strategies that\ncould achieve similar results with less computa-\ntional overhead.\n7\nEthics Approval\nRunning this study as a crowdsourcing project was\napproved by the ethics board of the University of\nIceland (SHV2024-080).\n8\nAcknowledgements\nThis work was supported by The Ludvig Storr\nTrust no. LSTORR2023-93030 and The Icelandic\nLanguage Technology Programme.\nReferences\nAtli Snær Ásmundsson. 2024. Analyzing Social Be-\nhavior in Icelandic Blogging Communities. Senti-\nment Analysis and Related Tasks with IceBERT.\nMaster’s thesis, University of Iceland.\nValerio Basile, Cristina Bosco, Elisabetta Fersini,\nDebora Nozza, Viviana Patti, Francisco Manuel\nRangel Pardo, Paolo Rosso, and Manuela San-\nguinetti. 2019.\nSemEval-2019 task 5: Multilin-\ngual detection of hate speech against immigrants and\nwomen in Twitter. In Proceedings of the 13th Inter-\nnational Workshop on Semantic Evaluation, pages\n54–63, Minneapolis, Minnesota, USA. Association\nfor Computational Linguistics.\nJudith Bridges. 2017.\nGendering metapragmatics\nin online discourse:\n“mansplaining man gonna\nmansplain...”.\nDiscourse, Context & Media,\n20:94–102.\nJacob Cohen. 1960.\nA coefficient of agreement for\nnominal scales.\nEducational and Psychological\nMeasurement, 20(1):37–46.\nSomaiyeh Dehghan and Berrin Yanikoglu. 2024. Eval-\nuating chatgpt’s ability to detect hate speech in turk-\nish tweets.\nIn Proceedings of the 7th Workshop\non Challenges and Applications of Automated Ex-\ntraction of Socio-political Events from Text (CASE\n2024), pages 54–59.\nDorottya Demszky, Dana Movshovitz-Attias, Jeong-\nwoo Ko, Alan Cowen, Gaurav Nemade, and Su-\njith Ravi. 2020.\nGoEmotions: A dataset of fine-\ngrained emotions. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 4040–4054, Online. Association\nfor Computational Linguistics.\nPaul Ekman. 1992. An argument for basic emotions.\nCognition & Emotion, 6(3-4):169–200.\nPaul Ekman and Karl G Heider. 1988. The universality\nof a contempt expression: A replication. Motivation\nand Emotion, 12(3):303–308.\n\nSteinunn\nRut\nFriðriksdóttir,\nAnnika\nSimonsen,\nAtli Snær Ásmundsson, Guðrún Lilja Friðjónsdóttir,\nAnton Karl Ingason, Vésteinn Snæbjarnarson, and\nHafsteinn Einarsson. 2024. Ice and fire: Dataset on\nsentiment, emotions, toxicity, sarcasm, hate speech,\nsympathy and more in Icelandic blog comments. In\nProceedings of the Fourth Workshop on Threat, Ag-\ngression & Cyberbullying @ LREC-COLING-2024,\npages 73–84, Torino, Italia. ELRA and ICCL.\nWilliam E Hick. 1952. On the rate of gain of informa-\ntion. Quarterly Journal of Experimental Psychol-\nogy, 4(1):11–26.\nSvetlana Kiritchenko and Saif Mohammad. 2017.\nBest-worst scaling more reliable than rating scales:\nA case study on sentiment intensity annotation. In\nProceedings of the 55th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), pages 465–470, Vancouver, Canada.\nAssociation for Computational Linguistics.\nMark Klein and Nouhayla Majdoubi. 2024.\nThe\nmedium is the message: toxicity declines in struc-\ntured vs unstructured online deliberations.\nWorld\nWide Web, 27(3):31.\nVarada Kolhatkar, Nithum Thain, Jeffrey Sorensen,\nLucas Dixon, and Maite Taboada. 2020.\nClas-\nsifying constructive comments.\narXiv preprint\narXiv:2004.05476.\nKlaus Krippendorff. 2018. Content analysis: An intro-\nduction to its methodology. Sage Publications.\nBinny Mathew, Ritam Dutt, Pawan Goyal, and Ani-\nmesh Mukherjee. 2019. Spread of hate speech in on-\nline social media. In Proceedings of the 10th ACM\nconference on web science, pages 173–182.\nGwenyth Isobel Meadows, Nicholas Wai Long Lau,\nEva Adelina Susanto, Chi Lok Yu, and Aditya Paul.\n2024. Localvaluebench: A collaboratively built and\nextensible benchmark for evaluating localized value\nalignment and ethical safety in large language mod-\nels. arXiv preprint arXiv:2408.01460.\nUtkarsh Mittal. 2023. Detecting hate speech utilizing\ndeep convolutional network and transformer mod-\nels.\nIn 2023 International Conference on Elec-\ntrical, Electronics, Communication and Computers\n(ELEXCOM), pages 1–4. IEEE.\nSeema Nagar, Ferdous Ahmed Barbhuiya, and Kuntal\nDey. 2023.\nTowards more robust hate speech de-\ntection: using social context and user data. Social\nNetwork Analysis and Mining, 13(1):47.\nTomáš Ptáˇcek, Ivan Habernal, and Jun Hong. 2014.\nSarcasm detection on Czech and English Twitter.\nIn Proceedings of COLING 2014, the 25th Inter-\nnational Conference on Computational Linguistics:\nTechnical Papers, pages 213–223, Dublin, Ireland.\nDublin City University and Association for Compu-\ntational Linguistics.\nPunyajoy Saha, Kiran Garimella, Narla Komal Kalyan,\nSaurabh Kumar Pandey, Pauras Mangesh Meher,\nBinny Mathew, and Animesh Mukherjee. 2023. On\nthe rise of fear speech in online social media.\nProceedings of the National Academy of Sciences,\n120(11):e2212270120.\nChelsie J Smith, Linda Schweitzer, Katarina Lauch,\nand Ashlyn Bird. 2022. ‘Well, actually’: investigat-\ning mansplaining in the modern workplace. Journal\nof Management & Organization, pages 1–19.\nTiberiu Sosea and Cornelia Caragea. 2022. EnsyNet:\nA dataset for encouragement and sympathy detec-\ntion.\nIn Proceedings of the Thirteenth Language\nResources and Evaluation Conference, pages 5444–\n5449, Marseille, France. European Language Re-\nsources Association.\nYik Yang Tan, Chee-Onn Chow, Jeevan Kanesan,\nJoon Huang Chuah, and YongLiang Lim. 2023.\nSentiment analysis and sarcasm detection using deep\nmulti-task learning. Wireless Personal Communica-\ntions, 129(3):2213–2237.\nYan Tao, Olga Viberg, Ryan S Baker, and René F Kizil-\ncec. 2024. Cultural bias and cultural alignment of\nlarge language models. PNAS Nexus, 3(9):pgae346.\nMayur Wankhade, Annavarapu Chandra Sekhara Rao,\nand Chaitanya Kulkarni. 2022.\nA survey on sen-\ntiment analysis methods, applications, and chal-\nlenges. Artificial Intelligence Review, 55(7):5731–\n5780.\nAlison I Young Reusser, Kristian M Veit, Elizabeth A\nGassin, and Jonathan P Case. 2024.\nResponding\nto online toxicity: Which strategies make others\nfeel freer to contribute, believe that toxicity will de-\ncrease, and believe that justice has been restored?\nCollabra: Psychology, 10(1).\n",
  "metadata": {
    "source_path": "papers/arxiv/Hotter_and_Colder_A_New_Approach_to_Annotating_Sentiment_Emotions_and\n__Bias_in_Icelandic_Blog_Comments_72e702a883aeff38.pdf",
    "content_hash": "72e702a883aeff38d9a3fd90dd141748b75ea2a196a68cd0beafb83e1be106a4",
    "arxiv_id": null,
    "title": "Hotter_and_Colder_A_New_Approach_to_Annotating_Sentiment_Emotions_and\n__Bias_in_Icelandic_Blog_Comments_72e702a883aeff38",
    "author": "",
    "creation_date": "D:20250225023518Z",
    "published": "2025-02-25T02:35:18",
    "pages": 11,
    "size": 1097332,
    "file_mtime": 1740470200.0961173
  }
}