{
  "text": "Under Review at the ICLR 2025 Workshop on Reasoning and Planning for LLMs\nMAKING LLMS REASON?\nTHE INTERMEDIATE LANGUAGE PROBLEM IN\nNEUROSYMBOLIC APPROACHES\nAlexander G. Beiser, David penz\nTUWien, Vienna, Austria\n{alexander.beiser,david.penz}@tuwien.ac.at\nABSTRACT\nLogical reasoning tasks manifest themselves as a challenge to Large Language\nModels (LLMs). Neurosymbolic approaches use LLMs to translate logical rea-\nsoning problems formulated in natural language into a formal intermediate lan-\nguage.\nSubsequently, the usage of symbolic reasoners yields reliable solving\nthereof. However, LLMs often fail in translation due to poorly chosen intermedi-\nate languages.\nWe introduce the intermediate language problem, which is the problem of choos-\ning a suitable formal language representation for neurosymbolic approaches. The-\noretically, we argue that its origins lie in the inability of LLMs to distinguish\nsyntax from semantics and the relative independence of the problem from its rep-\nresentation. We showcase its existence experimentally by contrasting two inter-\nmediate languages, Answer Set Programming and the Python Knowledge Engine.\nIn addition, we demonstrate the effects of varying degrees of supplementary con-\ntext information. Our results show a maximum difference in overall-accuracy of\n53.20% and 49.26% in execution-accuracy. When using the GPT4o-mini LLM we\nbeat the state-of-the-art in overall-accuracy on the ProntoQA dataset by 21.20%\nand by 50.50% on the ProofWriter dataset.\n1\nINTRODUCTION\nLarge Language Models (LLMs) perform surprisingly well on logical reasoning tasks (Saparov &\nHe, 2023). Actually, they perform better than humans on some datasets while still falling prey to the\nsame fallacies as humans do (Lampinen et al., 2024). Take, for example, the following two syllogism\nchains and determine whether they are correct or false: (1) All cats are mammals, all mammals are\nanimals, all cats are animals, and (2) all tumpus are wumpus, all wumpus are vumpus, all tumpus\nare vumpus. Research indicates that both humans and LLMs perform better on reasoning chains of\ntype (1) (real ontologies) than on type (2) (fictional ones) (Lampinen et al., 2024). This is interesting,\nas in logic, there is no distinction between (1) and (2) (both are correct), as there is a separation of\nsemantics from its representation (syntax).\nImproving performance of LLMs on logical reasoning tasks (datasets) must involve getting LLMs\nto reason more abstractly, thereby better separating semantics from syntax. One such attempt is\nChain of Thought (CoT) (Wei et al., 2022) prompting. However, LLM’s reasoning chains are non-\nfaithful in general (Lyu et al., 2023). Faithful reasoning chains are obtained by Neurosymbolic AI\nby using LLMs to translate a logical reasoning problem (posed in a natural language) into a formal\n(symbolic) language. This translation is subsequently (faithfully) solved by a symbolic reasoner.\nFinally, its output is translated back into natural language. One such approach is Logic-LM (Pan\net al., 2023).\nHowever, these neurosymbolic approaches fall short of acknowledging the impact of the intermedi-\nate language1 on translation. This paper investigates the current state-of-the-art (SOTA) approach\nLogic-LM and demonstrates that the choice of representation language matters. For this task, we\n1The formal language used between the LLM and the symbolic solver.\n1\narXiv:2502.17216v1  [cs.AI]  24 Feb 2025\n\nUnder Review at the ICLR 2025 Workshop on Reasoning and Planning for LLMs\ncompare the representation languages of the original Logic-LM paper to Answer Set Programming\nand the Python Knowledge Engine. We show differences of up to 49.26% in in execution-accuracy\nwhen different levels of additional information are given to the LLM. Further, we beat the current\nSOTA by 50.50% on the ProofWriter dataset.\nWe start with a discussion of related work (Section 2), followed by preliminary information (Sec-\ntion 3). In Section 4 we introduce the intermediate language problem, and in Section 5 we show our\nexperiments. We discuss our findings in Section 6, and close with a discussion of our limitations\n(Section 7).\n2\nRELATED WORK\nThe seminal Attention is All You Need (Vaswani et al., 2017) paper laid the foundations for LLMs\nsuch as GPT-4 (Achiam et al., 2024), Gemini (Georgiev et al., 2024), or Llama (Grattafiori et al.,\n2024). Surprisingly, LLMs perform decent on reasoning tasks, especially if prompted via a Chain-\nof-Thought (CoT) approach (Wei et al., 2022). This behavior is part of an emergent property of\nLLMs named in-context-learning or few-shot-learning (Shanahan, 2024). Although CoT achieves\nastonishing results on reasoning benchmarks, it is not faithful2 (Lyu et al., 2023). Further, it is argued\nthat not only is the reasoning not faithful but also, that LLMs “remain limited in their capabilities\nto performing probabilistic retrieval” and, therefore, that “pure statistical learning can not cope with\nthe combinatorial explosion inherent in many common-sense reasoning tasks” (Panas et al., 2024).\nRelated results show that LLMs do not acquire systematic problem solving skills (Dziri et al., 2023).\nThe logical reasoning capability of LLMs is measured with datasets such as the ProntoQA (Saparov\n& He, 2023), the ProofWriter (Tafjord et al., 2021), or the FOLIO (Han et al., 2024) dataset. Im-\nproving LLM’s reasoning capability was approached by different angles. Geva et al. (2020) try\nto improve numerical capabilities by injecting additional numerical data in the pre-training phase\nand further fine-tune the model. Other approaches focus on fine-tuning (Yang et al., 2022). How-\never, it was argued that these approaches fail to address the inherent inability of LLMs to reason\nmathematically (Panas et al., 2024).\nNeurosymbolic AI (Garcez & Lamb, 2023) approaches offer an alternative to the pure sub-symbolic\napproaches.\nExamples include differentiable logic (Badreddine et al., 2022), designing neural\nnetworks that act as Turing machines (Siegelmann & Sontag, 1995), or visual question answer-\ning with logic-programming and deep learning (Eiter et al., 2023). For LLM logical reasoning\ntasks, Logic-LM (Pan et al., 2023) is a neurosymbolic method that combines LLMs with symbolic\nsolvers. The studied solvers include a Prolog (K¨orner et al., 2022), First-Order-Logic (FOL) (En-\nderton, 1972), Constraint-Satisfaction-Problems (Kumar, 1992), and a Satisfiability-Problem (Cook,\n1971) solver. Implementation-wise, Logic-LM uses Python libraries for these solvers. For Pro-\nlog they use Pyke (Frederiksen, 2008), for SMT solving (SAT) they use Z3 (de Moura & Bjorner,\n2008), for FOL they use Prover9 (McCune, 2010), and for constraint solving they use the Python-\nconstraint (Niemeyer et al., 2024) library. Logic-LM++ (Kirtania et al., 2024) claims to improve on\nLogic-LM by adding an improved self-refinement module that takes more solver information into\naccount. Lam et al. (2024) acknowledge performance differences between solvers but fail to iden-\ntify that these stem from the chosen intermediate language. For knowledge based systems previous\nresearch shows that different query languages have an impact on LLM understanding (Liu et al.,\n2024).\nDiffering from these approaches, we study the impact of the used syntax inherent to the intermediate\nlanguage of neurosymbolic logical reasoners. In particular, this paper contrasts the syntax used by\nLogic-LM, to Pyke’s and Answer Set Programming’s (ASP) syntax. Answer Set Programming\n(ASP) (Gelfond & Leone, 2002) is a declarative problem-solving paradigm. As our ASP solver we\nuse Clingo (Kaminski et al., 2023) due to its readily available Python support.\n2Faithful means that the reasoning chain corresponds to how the model arrives at the answer (Lyu et al.,\n2023).\n2\n\nUnder Review at the ICLR 2025 Workshop on Reasoning and Planning for LLMs\n3\nPRELIMINARIES\nWe consider LLMs as black-box next-token predictor machines. This means that given the token\nvector (array) ⃗t, they select the token t in token-space T with the maximum predicted value:\nf(⃗t) = arg max\nt∈T\np(t|⃗t)\nTake for example the token-space Tc = {dead, alive} (reduced for this example), and the to-\nkens ⃗t\n=\n(Schr¨odinger’s, cat, is).\nThen, provided the LLM has the following p values :\n(p(alive) = 0.51, p(dead) = 0.49), we obtain f(⃗t) = alive.\n3.1\nCHAIN-OF-THOUGHT (COT) PROMPTING\nChain-of-Thought (CoT) prompting is an in-context-learning technique that has applications ranging\nfrom helping LLMs to express their uncertainty (Xiong et al., 2024), to improving the reasoning\ncapabilities of LLMs on reasoning datasets (Wei et al., 2022). CoT nudges the LLM to mimic a\nhuman reasoning chain, where we show an example adapted from the ProntoQA dataset (Saparov\n& He, 2023) as used for Logic-LM (Pan et al., 2023):\n1 The following example showcases the line of reasoning you have to follow:\n2 ---- Question ----\n3 Each cat is a carnivore. Fae is a cat.\n4 True or false: Fae is a carnivore\n5 ---- Reasoning ----\n6 Fae is a cat. Each cat is a carnivore. So Fae is a carnivore.\nWe show a full CoT example in the Appendix. Reasoning chains are faithful whenever the result\nfollows from the individual steps in the reasoning chain. However, LLM’s reasoning chains are\nnon-faithful in general (Lyu et al., 2023).\n3.2\nLOGIC PROGRAMMING\nAs intermediate languages between LLMs and symbolic reasoning, we consider Logic Programming\nlanguages. In more detail, we consider Answer Set Programming (ASP) and the intermediate lan-\nguage used for the Python knowledge engine (Pyke). We start with introducing ASP, and continue\nto introduce Pyke.\nASP is a declarative rule-based paradigm commonly used for modeling and solving complex plan-\nning or scheduling problems (Abels et al., 2021). We provide a brief summary of the main concepts\nof ASP. For details, we refer to (Eiter et al., 2009). An ASP program Π consists of rules r ∈Π of\nthe form :\np1(X1) ∨. . . ∨pl(Xl) :- pl+1(Xl+1), . . . , pm(Xm), ¬pm+1(Xm+1), . . . , ¬pn(Xn)\nWe call pi a literal, and Xi its term vector. Both stand for abstract concepts with an arbitrary naming.\nFor the exact syntax definition, see (Calimeri et al., 2020). For our purposes, we consider term\nvectors consisting of variables, integers, and lower- and upper-case letters. Let pi(Xi) for 1 ≤i ≤l\nbe the rule’s head Hr, pi(Xi) for l + 1 ≤i ≤m be the positive body B+\nr , and pi(Xi) for m + 1 ≤\ni ≤n be the negative body B−\nr . Semantically, a rule fires (meaning (one h ∈Hr) Hr holds),\nwhenever its body is true. A body is true whenever all literals of B+\nr hold, but no literal of B−\nr\nholds. Grounding replaces variables by their concrete domain values. Grounding is computationally\nvery expensive and a topic of current research (Beiser et al., 2024). The solutions to a grounded\nprogram are called answer sets.\nPyke defines rules and facts. Rules in Pyke express if-then statements. Their syntax is Python like.\nSemantically Pyke works with forward, or backward chaining algorithms.\n3.3\nNEUROSYMBOLIC LLM REASONING\nLogic-LM (Pan et al., 2023) is a neurosymbolic approach that integrates symbolic reasoners into\nthe reasoning steps of LLMs. It translates a natural language posed problem into its formal sym-\nbolic reasoner acceptable formulation. Subsequently, the symbolic reasoner solves the problem by\n3\n\nUnder Review at the ICLR 2025 Workshop on Reasoning and Planning for LLMs\nLarge Language\nModel\nProblem Formulation\nSymbolic \nReasoner\nTranslated Problem\nSolution\nOutput\nNatural Language\nFormal (Intermediate) Language\nSolution Human Readable\nInstruction:\n\"[Your] task is to parse the problem description [..]\ninto a [formal language]. [..] A correctly parsed\nexample is given below. [..]\"\nCorrectly Parsed Example:\n\"Fae is a cat, all cats are mammals, [..]\nFacts: cat(fae). \nRules: mammal(X) :- cat(X). [..]\"\nIn-context-learning\nFigure 1: The Neurosymbolic approach to solving logical reasoning problems. Provided a natural\nlanguage reasoning problem formulation, an LLM translates this into a formal intermediate lan-\nguage. A symbolic reasoner subsequently computes a solution to the problem. Finally, the LLM\nre-translates the symbolic solution into a human readable format (output), or if the solution is already\nhuman readable (dotted-line), it is directly returned. In-context-learning pre-prompts the LLM with\nsuitable instructions and a correctly parsed example to increase translation performance.\nobtaining a solution, which can be either re-translated into natural language or directly outputted.\nLogic-LM consists of three main modules, the problem formulator, the symbolic reasoner, and the\nresult interpreter. The Problem Formulator takes the problem in natural language and translates\nit to a formal formulation. Technically, they prompt the LLM by utilizing the in-context learn-\ning technique. The Symbolic Reasoner uses the formal language of the problem formulator as an\ninput to obtain a solution. Logic-LM uses four different symbolic solvers: Pyke, Prover9, Python-\nconstraint, and Z3. The result interpreter is called whenever a solution is produced by Logic-LM. It\nre-translates the symbolic output back into human-readable language. They implemented the result\ninterpreter as a rule-based program. However, they note that also an LLM can be used. A further\nsub-module is the self-refiner, which takes solver syntax error messages into account.\nWe adapt this setup by changing the solver accepted input language (or encoding) to a (largely)\narbitrary formal intermediate language. By doing that, we can adapt the syntax of the intermediate\nlanguage to tweak LLM reasoning performance. Figure 1 depicts the high-level schematics of this\napproach.\n4\nTHE INTERMEDIATE LANGUAGE PROBLEM\nWe introduce the intermediate language problem, which is the problem of choosing a suitable in-\ntermediate language for Neurosymbolic reasoning tasks, using LLMs as a translation tool between\nnatural and formal (intermediate) languages. Intuitively, the intermediate language problem stems\nfrom two observations: (i) LLMs are unable to separate syntax from semantics, and (ii) the actual\nformal intermediate language is (largely) irrelevant for formal problems. We start by justifying our\nclaim for the intertwined syntax and semantics of LLMs, which we follow by an argument for the\nirrelevancy of the choice of the formal intermediate language.\n4.1\nLLMS CANNOT SEPARATE SYNTAX FROM SEMANTICS\nWe base our claim LLMs cannot separate syntax from semantics on both, experimental observations\nand theoretical considerations. First observe that in logics, the representation (syntax) does not have\nan effect on semantics. Take for example the syllogism chain in Equation (1).\n(∀x : A(x) →B(X) ∧∀x : B(X) →C(X)) →∀x : A(X) →C(X)\n(1)\nThis statement is a valid syllogism chain, where A, B, and C stand for abstract concepts. Therefore,\nlogically, it does not matter whether A is a cat or a wumpus. This leads us to the following obser-\n4\n\nUnder Review at the ICLR 2025 Workshop on Reasoning and Planning for LLMs\nvation: If LLMs are capable of separating syntax from semantics, then changing the representation\nin a logical reasoning task does not affect performance (accuracy). In the following, we argue by\ncontraposition3 that LLMs are not able to separate syntax from semantics.\nIn Saparov & He (2023) they showed experiments that compare realistic to fictional ontologies on\nLLMs. Take for example, the two ontologies all cats are carnivores (realistic), and all tumpus\nare wumpus (fictional). The results show that LLMs achieve better results on realistic ontologies\nthan on fictional ontologies. This is in line with the research conducted in Lampinen et al. (2024),\nwhich discusses LLMs and human reasoning abilities. Their conclusion is that as humans, LLMs\nare biased by the semantic context, and as humans, LLMs perform better in realistic settings, than\nin fictional settings. To demonstrate this, we show an example which adheres to the law of syllogism\n(transitive reasoning, Equation (1)), which is similar to the research shown in (Lampinen et al.,\n2024). (1) Provided all cats are mammals, and all mammals are animals, then all cats are animals,\nand (2) assumed all tumpus are wumpus, and all wumpus are vumpus, then all tumpus are vumpus.\nSyllogisms of type (1) are more often considered as true than syllogisms of type (2) by both humans\nand LLMs. However, logically both syllogism chains are the same (valid). Further, our results from\nSection 5 are in line with these results. These observations lead us to confirm that LLMs are unable\nto separate syntax from semantics.\nThe possible underlying reasons for this problem are briefly discussed: By taking a more mechanis-\ntic interpretability like perspective, we speculate that the issue can be traced back to the current form\nof (word) embeddings. It is known from research that an embedding vector has an assigned mean-\ning (Mikolov et al., 2013). Observe that due to the architecture of LLMs this holds true in LLMs as\nwell (Bricken et al., 2023). Conversely, we speculate that any embedding approach that respects a\nseparation of syntax and semantics should be able to detach meaning from the representation.\n4.2\nINTERMEDIATE LANGUAGE IS INDEPENDENT OF THE SOLVER\nWe take a computational complexity theory point of view, for arguing that we can effectively choose\nan arbitrary intermediate language. Therefore, we briefly introduce some necessary concepts of\ncomplexity theory, where a detailed account of this field is given in (Papadimitriou, 1994). A first\ncrucial observation to our discussion is the distinction between a formal problem, and a solver. A\nformal problem P is an instance I, with an attached question Q. On the other hand, a solver takes\na problem P, encoded in a solver specific intermediate (representation) language, and produces a\nsolution S. Take for example the famous boolean satisfiability problem (SAT). Its instance consists\nof a propositional (≈boolean) formula I, such as (a ∧(b ∨c) ∧¬a), and its attached question Q\nis: Does there exist a satisfying assignment for I4? Solvers on the other hand take the instance\nI encoded in a specific way, be it the DIMACS format5, in a logic programming representation,\nor hypothetically in the unicode representation from above. This is our first observation for our\nargument. Note that for SAT highly efficient solvers are available, such as Z3 (de Moura & Bjorner,\n2008).\nTaking a complexity theoretic perspective we note that SAT is in NP (Actually NP-complete (Cook,\n1971)).\nRemind yourself that NP means (intuitively) that the problem is decidable by a non-\ndeterministic Turing machine in polynomial time. This definition gives rise to the concept of a\ncomplexity class C, which intuitively encodes a set of problems that all can be solved by a certain\ntype of turing machine, under certain resource constraints. The last crucial concept we need to in-\ntroduce for our argument is the concept of a reduction6. Intuitively, a reduction takes a problem P,\nand efficiently translates it into another problem P′, s.t. the solutions exactly match.\nSuch a reduction can be of high practical use. Assume that a problem P has no efficient specialized\nsolver, but there exists an efficient reduction to SAT. Then by reducing P to SAT, P can be solved\nefficiently. Therefore, every solver suitable for solving SAT problems, can solve problems that can\nbe reduced to SAT. So, coming back to our hypothesis that for many formal problems the choice of\nformal intermediate language is irrelevant, the concept of the reduction is our second argument.\n3By contraposition we follow, that if changing the representation has an effect on performance (accuracy),\nthen LLMs are not able to separate syntax from semantics.\n4For our example the answer is no, due to a ∧. . . ∧¬a.\n5See the Appendix for an example.\n6Polynomial time, many-one reduction.\n5\n\nUnder Review at the ICLR 2025 Workshop on Reasoning and Planning for LLMs\nWe summarize that for a formal problem, one efficient solver for a complexity class7 is sufficient\n(second argument), and this solver’s input (representation) is implementation specific. For LLMs\nwe follow that for a logical reasoning task, we can pick an intermediate language that is beneficial\nfor LLM translation. In more detail, we are only required to pick a language that is strong enough\nto represent the complexity theoretic problem class of our reasoning problem.\n4.3\nCHOOSING A SUITABLE INTERMEDIATE LANGUAGE\nFrom this discussion we follow two crucial observations: First, one should pick a representation\nlanguage that is inherently beneficial for LLM translation tasks. So, as an example, we think it is\nmore suitable to encode the problem: “If it rains, then the street is wet.” into a propositional logic\nformulation (Rains →WetStreet) than into a bit-representation. Second, the actual encoding in the\nrepresentation language should provide supplementary information to the LLM. Going back to the\nexample, an encoding such as Rains →WetStreet, provides more supplementary information to the\nLLM than p1 →p2. In our experimental evaluation we demonstrate that both observations hold.\n5\nEXPERIMENTS\nWe performed a set of experiments to empirically verify our intermediate language problem hypoth-\nesis, i.e., that the intermediate language has an effect on solving performance. In the following, we\nwill briefly go over our benchmark scenarios of different intermediate languages and our benchmark\nsetup before coming to our results.\n5.1\nINTERMEDIATE LANGUAGES AND ADDITIONAL CONTEXT\nWe conducted experiments on 11 scenarios. The scenarios differ by the used in-context-learning\ntechniques, mainly in which intermediate language is used and what additional context is provided.\nOur baselines are the scenarios used in Logic-LM. Namely the standard scenario, which is direct\nprompting of the LLM, the CoT scenario, which is the chain-of-thought prompting of the LLM,\nand the Logic-LM scenario without self-refiner, which uses a self-defined intermediate language to\ncommunicate with the Pyke solver. Our other 8 scenarios can be split into two groups: Those with\nPyke (4 scenarios), and those with ASP (4 scenarios). The differences in the scenarios of each\ngroup manifest themselves by different levels of additional supplementary information provided to\nthe LLM.\nTo showcase how the neurosymbolic scenarios differ, we show snippets of example prompts. Note\nthat we used the in-context-learning approach for these scenarios. All prompts include high-level in-\nformation about what the LLM should do, which is followed by an example problem translation into\nthe desired solver language. Consider for this the following example (snippet) from the ProntoQA,\nwhich we use as our running example to demonstrate the different example translations:\n1 Problem: Wren is a tumpus. Each tumpus is a wumpus.\n2 Wumpuses are brown. Each wumpus is not metallic.\n3 Question:\n4 Is the following statement true or false? Wren is not metallic.\nWhen translating the above-stated problem by hand to ASP, one has to define suitable predicates,\nterms, rules, and a query. Suitable predicates are wumpus(X), brown(X), and metallic(X). Terms\nare the variable X and wren, and rules encode the relationships, such as Each tumpus is a wumpus\nby wumpus(X) :- tumpus(X) in ASP. Therefore, for the ASP scenario Text we use the following\nin-context-learning example translation:\n1 Facts:\n2 tumpus(wren).\n3 Rules:\n4 wumpus(X):-tumpus(X). brown(X):-wumpus(X). -metallic(X):-wumpus(X).\n5 Query:\n6 -metallic(wren).\n7In more detail for the C-complete problems. We refer to the Appendix for this definition.\n6\n\nUnder Review at the ICLR 2025 Workshop on Reasoning and Planning for LLMs\nNo-Contex (No-C.)\np2(X) :- p1(X).\nText\nwumpus(X) :- tumpus(X).\nProgram (Prog.)\n```wumpus(X) :- tumpus(X).```\nComment (Comm.)\n``` % Each tumpus is a wumpus.\nwumpus(X) :- tumpus(X).```\nFigure 2:\nExperiment scenarios with increasing level of supplementary context information for\nin-context-learning. No-Context obfuscates the predicate names, text is a direct-translation of the\nproblem, program encapsulates the code in markdown code syntax, and comment adds the relevant\ninformation from the problem formulation. Examples shown in ASP syntax.\nObserve that the Text scenario keeps the names of the predicates, but does not explicitly state that\nparts are code. Scenario Prog. (Programming) wraps the code snippets into markdown code snippets\n(“‘). Scenario No-C. (No-Context) changes Text, by changing the predicate names to p1 (wumpus),\np2 (brown), and p3 (metallic). On the other hand, Comm. (Comment) changes Prog., by additionally\nadding comments with supplementary context information from the problem formulation. We show\nin Figure 2 examples for these described scenarios.\nIn the translation to Pyke, the general steps are the same as for the translation to ASP. However,\nas Pyke’s format is lengthier than ASP’s, we show in the following listing the translation of Each\ntumpus is a wumpus to Pyke’s format, while we show example encodings for each scenario in the\nAppendix. The next listing depicts the Text scenario:\n1 fact1\n2\nforeach\n3\nfacts.Tumpus($x, True)\n4\nassert\n5\nfacts.Wumpus($x, True)\nAs before, the Prog. scenario encapsulates the rules, facts and queries in markdown code snip-\npets, the No-C. scenario removes associations by enumerating the predicate names, and the Comm.\nscenario adds additional supplementary comments to the problem formulation.\n5.2\nBENCHMARK SETUP\nWe used the ProntoQA and ProofWriter datasets, as their problems contain fragments of first-order\nlogic that are solvable by ASP. The datasets were used in the Logic-LM configuration, where the\nProntoQA dataset was used in the fictional characters variant in the 5 hops version with 500 sam-\nples and the ProofWriter dataset with 600 samples and 5 hops reasoning depth. Our experiments\nwere conducted on an adapted Logic-LM implementation that features an ASP symbolic solver and\nresult interpreter based on Clingo, a new8 Pyke solver and interpreter, and different levels of ad-\nditional context according to the previous section. As we do not compare the understandability of\nthe syntax errors messages of different solvers, we disabled the self-refinement module. For the\nLLM we used GPT-4o-mini without any modifications and temperature 0. We measured the num-\nber of correct syntactical- (#EXEC) and the number of correctly solved instances (#TRUE). An\ninstance is syntactically correct, whenever the produced output of the LLM adheres to the solver\ninput language. Similarly, we consider a problem as correctly solved whenever the solver input\nis syntactically correct, and the solver produces the correct answer9. From these we compute the\nexecution-rate, which is the fraction of correct syntactical outputs (Exec-Rate, #EXEC\n#D ), execution-\naccuracy, which is the fraction of correctly solved instances of all syntactically correct ones (Exec-\nAcc, #TRUE\n#EXEC), and overall-accuracy, which is the fraction of correctly solved instances over the\nentire dataset (Overall-Acc, #TRUE\n##D ).\n8The original Logic-LM implementation uses a Pyke solver and interpreter with a self-defined intermediate\nlanguage. We changed it to the actual Pyke syntax, while we use the original one as the Logic-LM baseline.\n9Our measurement is stricter than that of Logic-LM, as Logic-LM performs a random guess on not syntac-\ntically correct inputs.\n7\n\nUnder Review at the ICLR 2025 Workshop on Reasoning and Planning for LLMs\nMethod\nProntoQA\nProofWriter\nOverall-Acc\nExec-Rate\nExec-Acc\nOverall-Acc\nExec-Rate\nExec-Acc\nBaseline\nStandard\n58.60\n/\n58.60\n19.33\n/\n19.33\nCoT\n72.80\n/\n72.80\n19.00\n/\n19.00\nLogic-LM\n74.40\n100.00\n74.40\n00.00\n00.00\n00.00\nPyke\nNo-C.\n41.60\n82.00\n50.73\n42.67\n81.50\n52.35\nText\n75.40\n99.00\n76.16\n47.33\n65.17\n72.63\nProg.\n93.80\n99.60\n94.12\n55.17\n78.67\n70.13\nComm.\n91.00\n99.80\n91.18\n58.00\n78.33\n74.04\nASP\nNo-C.\n42.40\n86.00\n49.30\n49.17\n95.33\n51.57\nText\n70.40\n90.40\n77.88\n54.83\n82.17\n66.73\nProg.\n81.20\n91.80\n88.45\n53.83\n79.33\n67.86\nComm.\n95.60\n97.00\n98.56\n69.83\n90.17\n77.45\nTable 1: Performance is dependent on the intermediate language. Additional context information\nin the intermediate language yields performance improvements. Detailed results of the experiments,\ncontrasting our baselines, to Pyke and ASP intermediate languages as described in Section 5.1. All\nexperiments were conducted on GPT-4o-mini with temperature 0. We measure overall-accuracy,\nexecution-rate, and execution-accuracy on the ProntoQA and ProofWriter datasets. All values are\nshown in %.\n5.3\nRESULTS\nWe show in Table 1 the results of our experiments10.\nAs discussed above, the main aim of\nour experiments is to empirically verify our intermediate language problem hypothesis from\nSection 4.\nSecondary, we analyze which techniques help in obtaining better results.\nTo\nverify our hypothesis, observe the pairs of Pyke and ASP for {No-C., Text, Prog., Comm.}.\nObserve the difference in execution-rate for ProntoQA: which is {82.00, 99.00, 99.60, 99.80}\nfor Pyke, and {86.0, 90.40, 91.80, 97.00} for ASP. However, not only the execution-rate dif-\nfers, but also the execution-accuracy, which is {50.73, 76.16, 94.12, 91.18} for Pyke, and\n{49.30, 77.88, 88.45, 98.56} for ASP. Further, observe the differences for ProofWriter in execution-\nrate, which is {81.50, 65.17, 78.67, 78.33} for Pyke and {95.33, 82.17, 79.33, 90.17} for ASP\nand the difference in execution-accuracy for Pyke, which is {52.35, 72.63, 70.13, 74.04} and\n{51.57, 66.73, 67.86, 77.45} for ASP. Also the overall-accuracy measures differ.\nNext we show the impact of the increase in additional context (Figure 2). We observe a monotone\nincrease in overall-accuracy for the ProntoQA dataset when using ASP from 42.40% (No-C.) to\n95.60% (Comm.), which is a difference of 53.20%. The execution-rate difference between No-C.\nand Comm. is 49.26%. For Pyke the difference is still striking, as it goes from 41.60% (No-C.) to\n91.00% (Comm.), however it is not monotone. The ProofWriter dataset shows a smaller increase\nfrom 49.17% (No-C.) to 69.83% for ASP and 42.67% to 58.00% for Pyke.\nComparing the baseline measures to the other experiments we observe an increase in overall-\naccuracy for both the ProntoQA and ProofWriter datasets. For ProntoQA the increase from Baseline\n(Logic-LM) to ASP (Comm.) is 21.20% in overall-accuracy and 24.16% in execution-accuracy,\nwhereas for ProofWriter the increase from Baseline (Standard) to ASP (Comm.) is 50.50% in\noverall-accuracy and 58.12% execution-accuracy.\n5.4\nINTERPRETATION\nWe can confirm our intermediate language hypothesis, as we were able to show differences in\noverall- and execution-accuracy between different intermediate languages. In addition to that, our\nresults suggest that an increase in context information leads to an increase in accuracy. This can be\nseen by the increase in accuracy when going from the no-context scenario to the comments scenario.\nThe execution-rate remains relatively unaffected by different intermediate languages.\n10Supplementary material available under:\nhttps://osf.io/xnr49/?view_only=f24c3db4f48545fab49daedb07a683e5\n8\n\nUnder Review at the ICLR 2025 Workshop on Reasoning and Planning for LLMs\n6\nDISCUSSION\nIn this paper we discussed the impact of the representation language for solving logical reasoning\nproblems in the neurosymbolic approach for LLMs. Using the neurosymbolic approach one uses\nLLMs for translating a natural-language-posed problem into an intermediate formal language, which\nis subsequently solved by a symbolic solver. This is enabled by the in-context-learning capability\nof LLMs. Previous results showed that with this approach the logical reasoning capability of LLMs\ncan be greatly enhanced (Pan et al., 2023).\nAlthough these approaches discuss the impact of different solvers, they fail to recognize that the\nintermediate formal language is the main factor that impacts performance in terms of overall- and\nexecution-accuracy. This stems from two facts: (i) LLMs suffer from concept entanglement, as\ndo humans (Lampinen et al., 2024). Intuitively, this means that when logical reasoning problems\nare posed in a counter-intuitive way, humans (and LLMs) perform worse (on average). (ii) Formal\nproblems (like a logical reasoning task) are independent of their solver, and consequently of their\nrepresentation language.\nFrom this analysis we introduced the intermediate language problem, which is the problem of choos-\ning a suitable formal language for the symbolic solver. We demonstrated its existence through ex-\nperiments and additionally showed that additional context in the in-context learning tends to yield\nbetter results. By using our highest additional context configuration, we were able to beat the current\nstate-of-the-art.\n7\nLIMITATIONS\nAlthough we are confident in our observations about the intermediate language problem and the gen-\neral tendency that adding more context information to the in-context learning technique will improve\nlogical reasoning capabilities, we are also aware of our experimental and inherent limitations.\nExperiment-wise, we conducted experiments on the GPT-4o-mini model for economic and environ-\nmental reasons. We expect a significant increase in all scores when other, more advanced or bigger,\nmodels are used, such as GPT-4o, GPT-o1, GPT-o3, or Gemini 2. The increase in performance can\nrender the accuracy differences on the ProntoQA and ProofWriter datasets insignificant. We are still\nconfident that when moving to harder datasets, such as FOLIO, or when using different intermediate\nlanguages, the effects can be reproduced.\nInherent to the neurosymbolic approach is the inclusion of a separate symbolic reasoning system.\nHowever, in an ideal integration of a symbolic solver into an LLM, the symbolic solver’s details are\nhidden from the end-user. We mean by that the hiding of the symbolic solver from the end-user in the\nchat and in the API and the automatic decision of the LLM, when the usage of the symbolic solver\nis possible and beneficial. Nonetheless, the symbiosis of the LLM with the symbolic solver into\none coherent system that automatically detects when the usage of the symbolic solver is beneficial,\nmight pose a major challenge.\n9\n\nUnder Review at the ICLR 2025 Workshop on Reasoning and Planning for LLMs\nREFERENCES\nDirk Abels, Julian Jordi, Max Ostrowski, Torsten Schaub, Ambra Toletti, and Philipp Wanko. Train\nScheduling with Hybrid Answer Set Programming. TPLP, 21(3):317–347, 2021. doi: 10.1017/\nS1471068420000046.\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-\nman, Diogo Almeida, Janko Altenschmidt, Sam Altman, and et. al. GPT-4 Technical Report,\n2024. URL http://arxiv.org/abs/2303.08774.\nSamy Badreddine, Artur d’Avila Garcez, Luciano Serafini, and Michael Spranger. Logic Tensor\nNetworks. AI, 303:103649, 2022. doi: 10.1016/j.artint.2021.103649.\nAlexander G. Beiser, Markus Hecher, Kaan Unalan, and Stefan Woltran. Bypassing the ASP Bot-\ntleneck: Hybrid Grounding by Splitting and Rewriting. In IJCAI24, pp. 3250–3258, 2024. doi:\n10.24963/ijcai.2024/360.\nTrenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick\nTurner, Cem Anil, Carson Denison, Amanda Askell, and others. Towards monosemanticity: De-\ncomposing language models with dictionary learning. Transformer Circuits Thread, 2, 2023.\nURL https://transformer-circuits.pub/2023/monosemantic-features.\nFrancesco Calimeri, Wolfgang Faber, Martin Gebser, Giovambattista Ianni, Roland Kamin-\nski, Thomas Krennwallner, Nicola Leone, Marco Maratea, Francesco Ricca, and Torsten\nSchaub.\nASP-Core-2 Input Language Format.\nTPLP, 20(2):294–309, 2020.\ndoi: 10.1017/\nS1471068419000450.\nStephen A. Cook. The complexity of theorem-proving procedures. In STOC71, pp. 151–158, 1971.\ndoi: 10.1145/800157.805047.\nLeonardo de Moura and Nikolaj Bjorner. Z3: An Efficient SMT Solver. TACAS08, pp. 337–340,\n2008. doi: doi.org/10.1007/978-3-540-78800-3 2.\nNouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Sean\nWelleck, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal,\nXiang Ren, Allyson Ettinger, Zaid Harchaoui, and Yejin Choi. Faith and Fate: Limits of Trans-\nformers on Compositionality. In NeurIPS23, pp. 70293–70332, 2023.\nThomas Eiter, Giovambattista Ianni, and Thomas Krennwallner.\nAnswer Set Programming: A\nPrimer. In LNCS, volume 5689, pp. 40–110. 2009. doi: 10.1007/978-3-642-03754-2 2.\nThomas Eiter, Tobias Geibinger, Nelson Higuera, and Johannes Oetsch. A logic-based approach to\ncontrastive explainability for neurosymbolic visual question answering. In IJCAI23, 2023. doi:\n10.24963/ijcai.2023/408.\nHerbert Enderton. A Mathematical Introduction to Logic. Academic Press, New York,, 1972.\nBruce Frederiksen. Applying Expert System Technology to Code Reuse with Pyke, 2008. URL\nhttps://pyke.sourceforge.net/PyCon2008-paper.html.\nArtur d’Avila Garcez and Lu´ıs C. Lamb. Neurosymbolic AI: the 3rd wave. Artif Intell Rev, 56(11):\n12387–12406, 2023. doi: 10.1007/s10462-023-10448-w.\nMichael Gelfond and Nicola Leone. Logic programming and knowledge representation—The A-\nProlog perspective. AI, 138(1):3–38, 2002. doi: 10.1016/S0004-3702(02)00207-2.\nPetko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien\nVincent, Zhufeng Pan, Shibo Wang, and et. al. Gemini 1.5: Unlocking multimodal understanding\nacross millions of tokens of context, 2024. URL http://arxiv.org/abs/2403.05530.\nMor Geva, Ankit Gupta, and Jonathan Berant. Injecting Numerical Reasoning Skills into Language\nModels. In ACL20, pp. 946–958, 2020. doi: 10.18653/v1/2020.acl-main.89.\n10\n\nUnder Review at the ICLR 2025 Workshop on Reasoning and Planning for LLMs\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad\nAl-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, and et. al. The Llama 3\nHerd of Models, 2024. URL http://arxiv.org/abs/2407.21783.\nSimeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Wenfei Zhou, James\nCoady, David Peng, Yujie Qiao, Luke Benson, and et. al. FOLIO: Natural Language Reason-\ning with First-Order Logic. In EMNLP24, pp. 22017–22031, 2024. doi: 10.18653/v1/2024.\nemnlp-main.1229.\nRoland Kaminski, Javier Romero, Torsten Schaub, and Philipp Wanko. How to Build Your Own\nASP-based System? TPLP, 23(1):299–361, 2023. ISSN 1471-0684, 1475-3081. doi: 10.1017/\nS1471068421000508.\nShashank Kirtania, Priyanshu Gupta, and Arjun Radhakrishna. LOGIC-LM++: Multi-Step Refine-\nment for Symbolic Formulations. In ACL24, pp. 56–63. Association for Computational Linguis-\ntics, 2024. doi: 10.18653/v1/2024.nlrse-1.6.\nVipin Kumar. Algorithms for Constraint-Satisfaction Problems: A Survey. AI Mag., 13(1):32, 1992.\ndoi: 10.1609/aimag.v13i1.976.\nPhilipp K¨orner, Michael Leuschel, Jo˜ao Barbosa, V´ıtor Santos Costa, Ver´onica Dahl, Manuel V.\nHermenegildo, Jose F. Morales, Jan Wielemaker, Daniel Diaz, Salvador Abreu, and Giovanni\nCiatto.\nFifty Years of Prolog and Beyond.\nTPLP, 22(6):776–858, 2022.\ndoi: 10.1017/\nS1471068422000102.\nLong Hei Matthew Lam, Ramya Keerthy Thatikonda, and Ehsan Shareghi. A Closer Look at Logical\nReasoning with LLMs: The Choice of Tool Matters, 2024. URL http://arxiv.org/abs/\n2406.00284.\nAndrew K Lampinen, Ishita Dasgupta, Stephanie C Y Chan, Hannah R Sheahan, Antonia Creswell,\nDharshan Kumaran, James L McClelland, and Felix Hill. Language models, like humans, show\ncontent effects on reasoning tasks. PNAS Nexus, 3(7):pgae233, 2024. doi: 10.1093/pnasnexus/\npgae233.\nJinxi Liu, Shulin Cao, Jiaxin Shi, Tingjian Zhang, Lei Hou, and Juanzi Li. How Proficient Are Large\nLanguage Models in Formal Languages? An In-Depth Insight for Knowledge Base Question\nAnswering. In ACL24, 2024.\nQing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki,\nand Chris Callison-Burch. Faithful Chain-of-Thought Reasoning. In IJCNLP23, pp. 305–329,\n2023. doi: 10.18653/v1/2023.ijcnlp-main.20.\nWilliam McCune. Prover9 and Mace4, 2010. URL http://www.cs.unm.edu/˜mccune/\nProver9.\nTomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic Regularities in Continuous Space\nWord Representations. In NAACL13, pp. 746–751, 2013.\nGustavo Niemeyer, Sebastien Celles, and Floris-Jan Willemsen. python-constraint, 2024. URL\nhttps://github.com/python-constraint/python-constraint.\nLiangming Pan, Alon Albalak, Xinyi Wang, and William Wang. Logic-LM: Empowering Large\nLanguage Models with Symbolic Solvers for Faithful Logical Reasoning.\nIn EMNLP23, pp.\n3806–3824, 2023. doi: 10.18653/v1/2023.findings-emnlp.248.\nDagmara Panas, Sohan Seth, and Vaishak Belle. Can Large Language Models Put 2 and 2 Together?\nProbing for Entailed Arithmetical Relationships. In NeSy24, pp. 258–276. Cham, 2024. doi:\n10.1007/978-3-031-71170-1 21.\nChristos H. Papadimitriou. Computational complexity. Addison-Wesley, 1994.\nAbulhair Saparov and He He. Language Models Are Greedy Reasoners: A Systematic Formal\nAnalysis of Chain-of-Thought. ICLR23, 2023.\n11\n\nUnder Review at the ICLR 2025 Workshop on Reasoning and Planning for LLMs\nMurray Shanahan. Talking about Large Language Models. Com. ACM, 67(2):68–79, 2024. doi:\n10.1145/3624724.\nHava T Siegelmann and Eduardo D Sontag. On the Computational Power of Neural Nets. JCSS,\n1995.\nOyvind Tafjord, Bhavana Dalvi, and Peter Clark. ProofWriter: Generating Implications, Proofs,\nand Abductive Statements over Natural Language. In IJCNLP21, pp. 3621–3634, 2021. doi:\n10.18653/v1/2021.findings-acl.317.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is All you Need. NeurIPS17, 30, 2017.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc\nLe, and Denny Zhou. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.\nIn NeurIPS22, pp. 24824–24837, 2022.\nRoland Wolf and Shan-Hwei Nienhuys-Cheng. SLD-resolution. In Foundations of Inductive Logic\nProgramming, pp. 105–126. 1997. ISBN 978-3-540-69049-8. doi: 10.1007/3-540-62927-0 7.\nMiao Xiong, Zhiyuan Hu, Xinyang Lu, YIFEI LI, Jie Fu, Junxian He, and Bryan Hooi. Can LLMs\nExpress Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs. In\nICLR24, 2024.\nKaiyu Yang, Jia Deng, and Danqi Chen. Generating Natural Language Proofs with Verifier-Guided\nSearch. In EMNLP22, pp. 89–105, 2022. doi: 10.18653/v1/2022.emnlp-main.7.\n12\n\nUnder Review at the ICLR 2025 Workshop on Reasoning and Planning for LLMs\n0\n100\n200\n300\n400\n500\nTrial Index\n0\n2\n4\n6\n8\n10\nNumber of Occurrences\nFalse Data\nError Data\nFigure 3: There is no apparent pattern which observations are inherently hard, or easy. No datapoint\nfails for all methods, but there are some datapoints that work for all methods. Error histogram of the\nProntoQA dataset. X-axis represents the observation number of the dataset, the y-axis the number of\nerrors across all experiment methods (11 methods, maximum of 11). Red represents a syntax error\nin the neurosymbolic approach, while blue represents a reasoning error.\n0\n100\n200\n300\n400\nExamples\nASP (Comm.)\nASP (Prog.)\nASP (Text)\nASP (No-C.)\nPyke (Comm.)\nPyke (Prog.)\nPyke (Text)\nPyke (No-C.)\nLogic-LM (Pyke)\nCoT\nStandard\nMethods\nFigure 4:\nThere is no apparent pattern where the methods struggle, except for the general obser-\nvations already discussed in the main-part. Error plot for the ProntoQA dataset. X-axis represents\nthe observation number of the dataset. The Y-axis represents the 11 different methods. A blue tick\n(line) represents a failed instance due to a reasoning error, a red tick a syntax error. .\nA\nAPPENDIX\nA.1\nADDITIONAL EXPERIMENTAL DETAILS\nWe show in Figures 3–6 our analysis of the distribution of errors. No distribution is apparent. There\nis no example, where all methods fail. However, there are some where all methods succeed.\n13\n\nUnder Review at the ICLR 2025 Workshop on Reasoning and Planning for LLMs\n0\n100\n200\n300\n400\n500\n600\nTrial Index\n0\n2\n4\n6\n8\n10\nNumber of Occurrences\nFalse Data\nError Data\nFigure 5:\nThere is no apparent pattern which observations are inherently hard, or easy. Some\ndatapoints fail for all methods, and there are some datapoints that work for all methods. Error\nhistogram of the ProofWriter dataset. X-axis represents the observation number of the dataset, the\ny-axis the number of errors across all experiment methods (11 methods, maximum of 11). Red\nrepresents a syntax error in the neurosymbolic approach, while blue represents a reasoning error.\n0\n100\n200\n300\n400\n500\nExamples\nASP (Comm.)\nASP (Prog.)\nASP (Text)\nASP (No-C.)\nPyke (Comm.)\nPyke (Prog.)\nPyke (Text)\nPyke (No-C.)\nLogic-LM (Pyke)\nCoT\nStandard\nMethods\nFigure 6: There is no apparent pattern where the methods struggle, except for the general observa-\ntions already discussed in the main-part. Error plot for the ProofWriter dataset. X-axis represents\nthe observation number of the dataset. The Y-axis represents the 11 different methods. A blue tick\n(line) represents a failed instance due to a reasoning error, a red tick a syntax error. .\nA.1.1\nWHY DO PARSING ERRORS OCCUR?\nA qualitative analysis of the errors results in an observation that the (strong) negation in our ASP\nencoding seems to be hard. Take for example ProntoQA example number 94 (ASP with Comments).\nThere ChatGPT produces the erroneous results:\n1 \"Facts:\\n‘‘‘\\n% Max is [..] -not(spicy(X)) :- vumpus(X). [..]\nIn ASP a statement like -not is not allowed, as it mixes default with strong negation.\nFor Pyke, we take example ProntoQA example 197 (Prog.). There ChatGPT translated the fact to a\nrule:\n14\n\nUnder Review at the ICLR 2025 Workshop on Reasoning and Planning for LLMs\n1 \"Facts:\\n‘‘‘\\nJompus($x, True)\\nassert\\n\nKind($x, True)\\n\\nfact1\\\nnforeach [..]\nA.2\nFURTHER DETAILS ON LOGIC-LM\nIn addition to the already discussed concepts in the main part, we now present the self-refiner: If the\ninput contains syntax errors, the symbolic reasoner produces an error with a suitable error message.\nWhenever Self-Refinement is activated, the system tries to correct the syntax error, by re-prompting\nthe problem formulator with the error message from the symbolic reasoner.\nA.2.1\nRUNNING EXAMPLE\nLogic-LM uses five different datasets to compare their results, of four different symbolic reasoners.\nWe explain the workings of Logic-LM along the ProntoQA dataset, with their logic programming11\nformulation using the Pyke solver. The first part of the example shows the problem, the second one\nthe question. The goal is to decide the question, which is a true or false question, based on the\ninferences and facts in the problem:\n1 Problem: Wren is a tumpus. Each tumpus is a wumpus.\n2 Wumpuses are brown. Each wumpus is not metallic.\n3 Question:\n4 Is the following statement true or false? Wren is not metallic.\nThis example needs two inference steps, (i) deduce that Wren is a wumpus, (ii) that wren is not\nmetallic. The number of necessary inference steps for ProntoQA can be set as a parameter in the\ndataset generation phase. Further, the ProntoQA dataset includes misleading statements, such as\nWumpuses are brown, which do not help in the inference.\nA.2.2\nPROBLEM FORMULATOR\nFor translating the natural language problem into the formal language, Logic-LM leverages on the\nfew-shot learning technique. Thereby, in addition to the problem specification, detailed instruction\non how to convert the natural language to the formal language are given. Furthermore, they provide\none example to the LLM, including problem specification, formal language translation, and result.\nWe show an example for the in-context learning for the ProntoQA Saparov & He (2023) dataset\nprompt. The LLM translates a single ProntoQA problem into a formal language that can be parsed\nwith a rule based parser to the Pyke Frederiksen (2008) Prolog style. The ProntoQA data consists of\na general task description, followed by an example problem, including an example translation. We\nstart with the general task description:\n1 Task\nD e s c r i p t i o n : You are\ngiven\na problem\nd e s c r i p t i o n\nand a\nq u e s t i o n .\n2 The\nt a s k\ni s\nto :\n1)\nd e f i n e\na l l\nthe\np r e d i c a t e s\nin\nthe\nproblem\n3 2)\nparse\nthe\nproblem\ni n t o\nl o g i c\nr u l e s\nbased on\nthe\nd e f i n e d\np r e d i c a t e s\n4 3)\nw r i t e\na l l\nthe\nf a c t s\nmentioned\nin\nthe\nproblem\n5 4)\nparse\nthe\nq u e s t i o n\ni n t o\nthe\nl o g i c\nform\nThis is followed by an example problem. In the following we show a snippet of the actual example\nproblem.\n1 Problem\n[ Snippet ] :\nAlex\ni s\na tumpus .\nTumpuses\nare\nvumpuses .\nEach vumpus\ni s\na\n2 yumpus .\nYumpuses\nare\nnumpuses .\nEach numpus\ni s\na dumpus .\nEvery dumpus\ni s\nnot\nshy .\n3 Question :\nTrue\nor\nf a l s e :\nAlex\ni s\nnot\nshy .\nFinally, Logic-LM provides an example translation into their intermediate language format. First\nthe predicate specification is shown, followed by the facts, and finally by the rules.\n11Logic-LM introduces rules in Prolog as clauses, which they define as: F1 ∧. . .∧Fm →Fm+1 ∧. . .∧Fn.\nIn clause form this rule is (¬F1 ∨. . . ∨¬Fm ∨(Fm+1 ∧. . . ∧Fn)), so unless (Fm+1 ∧. . . ∧Fn) is treated\nas a single literal, their definition is not a clause.\n15\n\nUnder Review at the ICLR 2025 Workshop on Reasoning and Planning for LLMs\n1 P r e d i c a t e s :\n2 Tumpuses ( $x ,\nbool )\n: : :\nDoes x belong\nto\nTumpuses ?\n3 Vumpuses ( $x ,\nbool )\n: : :\nDoes x belong\nto\nVumpuses?\n4\n[ . . . ]\n5 Facts :\n6 Tumpuses ( Alex ,\nTrue )\n7 Rules :\n8 Tumpuses ( $x ,\nTrue ) >>> Vumpuses ( $x ,\nTrue )\n9\n[ . . . ]\n10 Query :\n11 Shy ( Alex ,\nFalse )\nThe question query is added after these prefix-prompts. The full prompt can be found in the Ap-\npendix in Section A.4. The output of the LLM is then passed on to the symbolic reasoner.\nA.2.3\nSYMBOLIC REASONER, INTERPRETER AND SELF-REFINER\nLogic-LM takes the output of the LLM, parses it into Pyke’s format, and then calls and interprets\nPyke. The parsing is done entirely in Python, which splits the output into a fact, and a rule knowledge\nbase. From this knowledge base, Pyke uses its backward and forward chaining mechanisms to obtain\nan answer. For ProntoQA, Logic-LM uses a rule-based mechanism to interpret the answer.\nBelow, we show how the first rule “Each tumpus is a wumpus” of our running example (which we\nassume is correctly translated) gets parsed into Pyke’s format. First we show the translated rule:\n1 Tumpuses($x, True) >>> Wumpus($x, True)\nThe symbolic reasoner parses the translated rule to the following Pyke rule:\n1 foreach\n2\nfacts.Tumpus($x, True)\n3 assert\n4\nfacts.Wumpus($x, True)\nIf the symbolic reasoner is unable to execute the program (for example, due to a syntax error), then\nthe (optional) self-refiner takes the error message of the symbolic solver into account. Ideally, the\nthen-generated is correct.\nA.2.4\nDISCUSSION OF LOGIC-LM\nLogic-LM claims that their method shows an improvement of 39.20% over standard prompting,\nand 18.40% over CoT prompting for GPT-3.5. However, as they are using two different GPT-\n3.5 versions (gpt-3.5-turbo and text-davinci-003), and further not show how they compute these\nimprovement values, we were not able to verify their claim. Although we acknowledge that both,\nthe relative improvement for gpt-3.5-turbo, and for text-davinci-003, is approximately in this range\n(see Appendix Section A.2.5).\nAlthough the idea of self-refinement is promising, the current self-refinement mode is less significant\nthan choosing a better representation language. Their maximum increase in execution-accuracy is\n2.50% points (ours 49.82% points), while their maximum execution-rate increase is 17.60% points\n(ours 17.80% points). Lastly, as they are randomly guessing solutions when they encounter a syntax\nerror, their results for overall-accuracy is shifted slightly in a positive way.\nA.2.5\nCHECKING IMPROVEMENT VALUES\nLet the relative improvement be\nNew-Acc\nOld-Acc , and let D be the set of the experiments (ProntoQA,\nProofWriter, FOLIO, LogicalDeduction, and AR-LSAT). Then we define the average12 improve-\nment to be 1\n5Σd∈D\nNew-Accd\nOld-Accd .\nComparing standard vs. Logic-LM, Logic-LM achieves a relative improvement of 45.23% for gpt-\n3.5-turbo, of 47.47% for text-davinci-003, and of 24.98% for gpt-4. Regarding CoT vs. Logic-LM,\n12We acknowledge that a weighted average, by the number of samples per dataset, is another feasible option.\n16\n\nUnder Review at the ICLR 2025 Workshop on Reasoning and Planning for LLMs\nLogic-LM achieves a relative improvement of 25.14% for gpt-3.5-turbo, of 19.59% for text-davinci-\n003, and of 10.44% for gpt-4.\nTherefore, although we could not verify their claims of an improvement of 39.2% (standard vs.\nLogic-LM), or 18.4% (CoT vs. Logic-LM), we conclude that their reported numbers are in the range\nof our computed improvements, and therefore likely due to a different weighting of the average.\nA.3\nCHAIN-OF-THOUGHT (COT) PROMPT\nWe show a full example of a CoT prompt for the question:\n1 Tumpuses are rompuses. Rompuses are not luminous. Stella is a tumpus.\n2 True or false: Stella is not luminous.\nThe prompt is shown in the listing below:\n1 Given a problem statement, the goal is to answer a true/false question.\n2 The following example showcases the line of reasoning you have to follow:\n3 ---- Question ----\n4 Each cat is a carnivore. Every carnivore is not herbivorous. Fae is a cat\n.\n5 True or false: Fae is not herbivorous.\n6 ---- Reasoning ----\n7 Fae is a cat. Each cat is a carnivore. So Fae is a carnivore.\n8 Every carnivore is not herbivorous. So Fae is not herbivorous.\n9 ---- Answer ----\n10 True\n11 ----\n12 Now consider the following question:\n13 ---- Question ----\n14 Tumpuses are rompuses. Rompuses are not luminous. Stella is a tumpus.\n15 True or false: Stella is not luminous.\n16 ---- Reasoning ----\nIf the LLM answers correctly (and faithfully), we get:\n1 Stella is a tumpus. Tumpuses are rompuses. So Stella is a rompus.\n2 Rompuses are not luminous. So Stella is not luminous.\n3 ---- Answer ----\n4 True\nA.4\nLOGIC-LM: EXAMPLE PROMPT\nThe following prompt is the full prompt for the example shown in the main part of the paper, for the\nLogic-LM Pyke reasoner (Baseline (Logic-LM)).\n1 Task Description: You are given a problem description and a question. The\ntask is to:\n2 1) define all the predicates in the problem\n3 2) parse the problem into logic rules based on the defined predicates\n4 3) write all the facts mentioned in the problem\n5 4) parse the question into the logic form\n6 ------\n7 Problem:\n8 Alex is a tumpus. Tumpuses are vumpuses. Each vumpus is a\n9 yumpus. Yumpuses are numpuses. Each numpus is a dumpus. Every dumpus is\nnot shy.\n10 Question:\n11 True or false: Alex is not shy.\n12 ###\n13 Predicates:\n14 Tumpuses($x, bool) ::: Does x belong to Tumpuses?\n15 Vumpuses($x, bool) ::: Does x belong to Vumpuses?\n16 Yumpus($x, bool) ::: Does x belong to Yumpus?\n17\n\nUnder Review at the ICLR 2025 Workshop on Reasoning and Planning for LLMs\n17 Numpus($x, bool) ::: Does x belong to Numpus?\n18 Dumpus($x, bool) ::: Does x belong to Dumpus?\n19 Shy($x, bool) ::: Is x shy?\n20 Liquid($x, bool) ::: Is x liquid?\n21 Zumpus($x, bool) ::: Does x belong to Zumpus?\n22 Facts:\n23 Tumpuses(Alex, True)\n24 Rules:\n25 Tumpuses($x, True) >>> Vumpuses($x, True)\n26 Vumpuses($x, True) >>> Yumpus($x, True)\n27 Yumpus($x, True) >>> Numpus($x, True)\n28 Numpus($x, True) >>> Dumpus($x, True)\n29 Dumpus($x, True) >>> Shy($x, False)\n30 Query:\n31 Shy(Alex, False)\n32 ------\n33 Problem:\n34 Wren is a tumpus. Each tumpus is a wumpus.\n35 Wumpuses are brown. Each wumpus is not metallic.\n36 Question:\n37 Is the following statement true or false? Wren is not metallic.\n38 ###\nA.5\nPYKE (TEXT) PROMPT\nThe following shows our full in-context learning prompt for Pyke (Text).\n1 Task Description: You are given a problem description and a question.\n2 In general, the task is to parse the problem description and question\ninto a a Pyke (Python Knowledge Engine) readable format.\n3 In more detail:\n4 1) Define the facts.\n5 2) Define the rules.\n6 3) Define the \"query\". The query has to be defined according to the\nfollowing example: Given the question: \"True or false: Alex is not\nshy\".\n7 Then you should define this as \"Shy(alex, false)\".\n8 The program must by syntactically correct. A correctly parsed example is\ngiven below. The output should be given in a Pyke readable format.\nTherefore, be sure not to use any \"bullet points\", or \"numberings\"\nwhen printing the output. Further, no special characters like (\"#\")\nmust occur.\n9 ------\n10 Problem:\n11 Each jompus is fruity. Every jompus is a wumpus. Every wumpus is not\ntransparent. Wumpuses are tumpuses. Tumpuses are mean. Tumpuses are\nvumpuses. Every vumpus is cold. Each vumpus is a yumpus. Yumpuses are\norange. Yumpuses are numpuses.\n12 Numpuses are dull. Each numpus is a dumpus. Every dumpus is not shy.\nImpuses are shy. Dumpuses are rompuses. Each rompus is liquid.\nRompuses are zumpuses. Alex is a tumpus.\n13 Question:\n14 True or false: Alex is not shy.\n15 ###\n16 Facts:\n17 Tumpus(Alex, True)\n18 Rules:\n19 fact1\n20\nforeach\n21\nfacts.Jompus($x, True)\n22\nassert\n23\nfacts.Fruity($x, True)\n24\n25 fact2\n18\n\nUnder Review at the ICLR 2025 Workshop on Reasoning and Planning for LLMs\n26\nforeach\n27\nfacts.Jompus($x, True)\n28\nassert\n29\nfacts.Wumpus($x, True)\n30 [...]\n31 Query:\n32 Shy(Alex,False)\n33 [...]\nA.6\nPYKE (NO-C.) PROMPT\nThe following shows our full in-context learning prompt for Pyke (No-C.).\n1 [...]\n2 ###\n3 Facts:\n4 P1(Alex, True)\n5 Rules:\n6 fact1\n7\nforeach\n8\nfacts.P2($x, True)\n9\nassert\n10\nfacts.P3($x, True)\n11\n12 fact2\n13\nforeach\n14\nfacts.P2($x, True)\n15\nassert\n16\nfacts.P4($x, True)\n17 [...]\n18 Query:\n19 P13(Alex,False)\n20 [...]\nA.7\nPYKE (PROG.) PROMPT\nThe following shows our full in-context learning prompt for Pyke (Prog).\n1 [...]\n2 ###\n3 Facts:\n4 ‘‘‘\n5 Tumpus(Alex, True)\n6 ‘‘‘\n7 Rules:\n8 ‘‘‘\n9 fact1\n10\nforeach\n11\nfacts.Jompus($x, True)\n12\nassert\n13\nfacts.Fruity($x, True)\n14\n15 fact2\n16\nforeach\n17\nfacts.Jompus($x, True)\n18\nassert\n19\nfacts.Wumpus($x, True)\n20 [...]\n21 ‘‘‘\n22 Query:\n23 ‘‘‘\n24 Shy(Alex,False)\n25 ‘‘‘\n26 [...]\n19\n\nUnder Review at the ICLR 2025 Workshop on Reasoning and Planning for LLMs\nA.8\nPYKE (COMM.) PROMPT\nThe following shows our full in-context learning prompt for Pyke (Comm.).\n1 [...]\n2 ###\n3 Facts:\n4 ‘‘‘\n5 # Alex is a tumpus.\n6 Tumpus(Alex, True)\n7 ‘‘‘\n8 Rules:\n9 ‘‘‘\n10 # Each jompus is fruity.\n11 fact1\n12\nforeach\n13\nfacts.Jompus($x, True)\n14\nassert\n15\nfacts.Fruity($x, True)\n16 # Every jompus is a wumpus.\n17 fact2\n18\nforeach\n19\nfacts.Jompus($x, True)\n20\nassert\n21\nfacts.Wumpus($x, True)\n22 [...]\n23 ‘‘‘\n24 Query:\n25 ‘‘‘\n26 # True or false: Alex is not shy.\n27 Shy(Alex,False)\n28 ‘‘‘\n29 [...]\nA.9\nASP (TEXT) PROMPT\nThe following shows our full in-context learning prompt for ASP (Text).\n1 [...]\n2 ###\n3 Facts:\n4 tumpus(alex).\n5 Rules:\n6 fruity(X) :- jompus(X).\n7 wumpus(X) :- jompus(X).\n8 [...]\n9 Query:\n10 -shy(alex).\n11 [...]\nA.10\nASP (NO-C.) PROMPT\nThe following shows our full in-context learning prompt for ASP (No-C.).\n1 [...]\n2 Facts:\n3 p1(alex).\n4 Rules:\n5 p2(X) :- p3(X).\n6 p4(X) :- p3(X).\n7 [...]\n8 Query:\n9 -p15(alex).\n10 [...]\n20\n\nUnder Review at the ICLR 2025 Workshop on Reasoning and Planning for LLMs\nA.11\nASP (PROG.) PROMPT\nThe following shows our full in-context learning prompt for ASP (Prog.).\n1 [...]\n2 Facts:\n3 ‘‘‘\n4 tumpus(alex).\n5 ‘‘‘\n6 Rules:\n7 ‘‘‘\n8 fruity(X) :- jompus(X).\n9 wumpus(X) :- jompus(X).\n10 [...]\n11 ‘‘‘\n12 Query:\n13 ‘‘‘\n14 -shy(alex).\n15 ‘‘‘\n16 [...]\nA.12\nASP (COMM.) PROMPT\nThe following shows our full in-context learning prompt for ASP (Comm.).\n1 [...]\n2 Facts:\n3 ‘‘‘\n4 % Alex is a tumpus.\n5 tumpus(alex).\n6 ‘‘‘\n7 Rules:\n8 ‘‘‘\n9 % Each jompus is fruity.\n10 fruity(X) :- jompus(X).\n11 % Every jompus is a wumpus.\n12 wumpus(X) :- jompus(X).\n13 [...]\n14 ‘‘‘\n15 Query:\n16 ‘‘‘\n17 % True or false: Alex is not shy.\n18 -shy(alex).\n19 ‘‘‘\n20 [...]\nA.13\nHARDNESS AND COMPLETENESS\nFor a formal problem P, and a complexity class C, if all problems P′ can be reduced to P, then P\nis C-hard. If additionally, P ∈C, then it is said that P is C-complete.\nA.14\nDIMACS EXAMPLE\nThe DIMACS format is a popular format for representing SAT problems, encoded as a conjunctive-\nnormal-form. Our example (a ∧(b ∨c) ∧¬a) translates to:\n1 p cnf 3 3\n2 1\n3 2 3\n4 -1\n21\n\nUnder Review at the ICLR 2025 Workshop on Reasoning and Planning for LLMs\nA.15\nANSWER SET PROGRAMMING SEMANTICS\nLet I be a set of atoms (an interpretation). Then I is an answer set, iff all rules r ∈Π are satisfied,\nand all atoms a ∈I are founded. r ∈Π is satisfied iff (Hr ∪B−\nr ) ∩I ̸= ∅, or B+\nr \\ I ̸= ∅. An\natom a ∈I is founded if there exists r ∈Π, s.t. a ∈Hr, B+\nr ⊆I, I ∩(B−\nr ∪(Hr \\ {a})) = ∅,\nand for each b ∈B+\nr\n: ψ(b) < ψ(a), where ψ is a level-mapping function. Another (popular)\ncharacterization of answer sets is the Gelfond-Lifschitz-Reduct.\nIntuitively this means that for each rule it must hold that whenever the positive body can be derived,\nand no literal of the negative body holds, then (one literal of) the head must hold as well. An\nexcellent overview about ASP is provided by Eiter et al. (2009).\nSyntactically, Prolog and ASP are similar. In terms of expressiveness however, Prolog is in general\nTuring-complete, which does not hold for ASP. Prolog programs are evaluated by using backtrack-\ning, and SLD-Resolution Wolf & Nienhuys-Cheng (1997), for a given query. However, Pyke’s Fred-\neriksen (2008) syntax significantly diverges from the standard (ISO) Prolog. In Prolog the statement\nEach cat is a carnivore can be expressed with “cat(X) :- carnivore(X).” which is in contrast to\nPyke’s syntax: “foreach facts.cat($x, True) assert facts.carnivore ($x, True)”\n22\n",
  "metadata": {
    "source_path": "papers/arxiv/Making_LLMs_Reason_The_Intermediate_Language_Problem_in_Neurosymbolic\n__Approaches_4af8bd6b11d08c19.pdf",
    "content_hash": "4af8bd6b11d08c19308fd298bacfff3af6d02456e2655092b3b659f2b3ac5817",
    "arxiv_id": null,
    "title": "Making_LLMs_Reason_The_Intermediate_Language_Problem_in_Neurosymbolic\n__Approaches_4af8bd6b11d08c19",
    "author": "",
    "creation_date": "D:20250225025258Z",
    "published": "2025-02-25T02:52:58",
    "pages": 22,
    "size": 867817,
    "file_mtime": 1740470167.6659544
  }
}