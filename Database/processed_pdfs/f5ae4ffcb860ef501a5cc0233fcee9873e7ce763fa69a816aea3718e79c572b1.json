{
  "text": "Improving the Diffusability of Autoencoders\nIvan Skorokhodov 1 Sharath Girish 1 Benran Hu 1 2 Willi Menapace 1 Yanyu Li 1 Rameen Abdal 1\nSergey Tulyakov 1 Aliaksandr Siarohin 1\nAbstract\nLatent diffusion models have emerged as the lead-\ning approach for generating high-quality images\nand videos, utilizing compressed latent repre-\nsentations to reduce the computational burden\nof the diffusion process. While recent advance-\nments have primarily focused on scaling diffu-\nsion backbones and improving autoencoder recon-\nstruction quality, the interaction between these\ncomponents has received comparatively less atten-\ntion. In this work, we perform a spectral analysis\nof modern autoencoders and identify inordinate\nhigh-frequency components in their latent spaces,\nwhich are especially pronounced in the autoen-\ncoders with a large bottleneck channel size. We\nhypothesize that this high-frequency component\ninterferes with the coarse-to-fine nature of the\ndiffusion synthesis process and hinders the gener-\nation quality. To mitigate the issue, we propose\nscale equivariance: a simple regularization strat-\negy that aligns latent and RGB spaces across fre-\nquencies by enforcing scale equivariance in the de-\ncoder. It requires minimal code changes and only\nup to 20K autoencoder fine-tuning steps, yet sig-\nnificantly improves generation quality, reducing\nFID by 19% for image generation on ImageNet-\n1K 2562 and FVD by at least 44% for video gen-\neration on Kinetics-700 17 × 2562.\n1. Introduction\nIn recent years, diffusion models (DMs) have emerged as\nthe dominant generative modeling paradigm in computer\nvision. However, the high dimensionality of visual data\nposes a significant challenge, making the direct application\nof diffusion models impractical. Latent diffusion models\n(LDMs) (Vahdat et al., 2021; Rombach et al., 2022) have\nbecome the main approach in mitigating this issue, demon-\nstrating remarkable success in generating high-resolution\nimages (Black Forest Labs, 2023; Betker et al., 2023; Esser\n1Snap Inc.\n2Carnegie Mellon University. Correspondence\nto: Ivan Skorokhodov <iskorokhodov@gmail.com>, Aliaksandr\nSiarohin <aliaksandr.siarohin@gmail.com>.\n50000 100000 150000 200000 250000 300000 350000 400000\nIteration\n10\n15\n20\n25\n30\n35\nFID\nDiT-XL/2 + FluxAE (vanilla)\nDiT-XL/2 + FluxAE + SE\nDiT-XL/2 + CogVideoX AE (vanilla)\nDiT-XL/2 + CogVideoX AE + SE\n0\n200\n400\n600\n800\n1000\n1200\nFVD\nFigure 1. Convergence speed of DiT-XL/2 on top of vanilla\nFluxAE vs FluxAE fine-tuned for 10K steps with scale equiv-\nariance (SE) regularization on ImageNet-1K-2562; and on top\nof CogVideoX-AE vs CogVideoX-AE with SE on Kinetics-700-\n17 × 2562. Our regularization improves the performance of image\nand video LDMs by refinng the frequency profile of their autoen-\ncoders’ latent spaces.\net al., 2024) and videos (Brooks et al., 2024; Yang et al.,\n2024; Kong et al., 2024). A typical LDM consists of two\nmain components: an autoencoder and a diffusion backbone.\nMost recent breakthroughs have been driven by scaling up\ndiffusion backbones (Peebles & Xie, 2022), while autoen-\ncoders (AEs) have received comparatively less attention.\nRecently, the research community has begun focusing more\non improving the autoencoders, recognizing their crucial\nimpact on overall performance, but most effort has been con-\ncentrated on enhancing reconstruction quality (Black Forest\nLabs, 2023; Hong et al., 2022; Agarwal et al., 2025; HaCo-\nhen et al., 2024; Chen et al., 2025) and achieving higher\ncompression ratios (Agarwal et al., 2025; HaCohen et al.,\n2024; Chen et al., 2025) to accelerate the diffusion process.\nHowever, we argue that a critical yet under-explored aspect,\nwhich we refer to as diffusability1, also plays a key role in\ndetermining the utility of autoencoders. Indeed, all three\nfactors—reconstruction quality, compression efficiency, and\ndiffusability—are essential for the practical effectiveness of\nLDMs. Specifically, inaccurate reconstruction sets an upper\n1Diffusability describes how easily a distribution can be mod-\neled by a diffusion process: high diffusability indicates that the\ndistribution is easy to fit, whereas low diffusability makes the\nprocess more complex.\n1\narXiv:2502.14831v1  [cs.CV]  20 Feb 2025\n\nImproving the Diffusability of Autoencoders\nbound on generation fidelity, low compression efficiency\nleads to slow and costly generation, and poor diffusability\nnecessitates the use of heavier, more expensive, and sophis-\nticated diffusion backbones, further limiting LDM quality.\nDiffusion models possess a unique property of being coarse-\nto-fine in nature (Dieleman, 2024; Ning et al., 2024): in\nthe denoising process, they synthesize low-frequency signal\ncomponents first and add high-frequency ones on top of\nthem later. It is a beneficial trait since it allows to defer error\naccumulation to higher frequency parts of the spectrum,\nwhich aligns well with how humans perceive quality: we\nare sensitive to the image structure and composition, but\noblivious of its fine-grained textural details. However, when\napplying the diffusion process in the latent spaces of pre-\ntrained autoencoders, the correspondence between latent\nlow-frequency components and their RGB counterparts may\nbe lost, hindering the spectral autoregression property.\nIn this work, we identify a correlation between the spectral\nproperties of the latent space and its diffusability. We an-\nalyze the spectral characteristics of latent representations\nacross several widely used image and video autoencoders.\nOur investigation reveals a prominent high-frequency com-\nponent in these latent spaces, deviating significantly from\nthe spectral distribution of RGB signals. This component be-\ncomes even more pronounced as the channel size increases,\nwhich the recent autoencoders use to improve reconstruction.\nWe hypothesize that the flat spectral distribution induced by\nthe strong high-frequency component harms the spectral au-\ntoregression property. Moreover, we demonstrate that these\nhigh-frequency components substantially influence the final\nRGB result, and their inaccurate modeling can introduce\nnoticeable visual artifacts. Finally, we show that standard\nKL regularization is insufficient to address spectrum defects\nand, in some cases, may even amplify the issue.\nTo mitigate these spurious high-frequency components in\nlatent representations, we propose a simple and effective\nregularization strategy. Our approach involves aligning the\nlatent space and the RGB space at different frequencies.\nThis is achieved by enforcing scale equivariance in the de-\ncoder — ensuring that downsampled latents correspond to\ndownsampled RGB representations. Our method requires\nminimal modifications and few additional autoencoder fine-\ntuning steps, yet significantly enhances diffusability across\nvarious architectures, ultimately improving the quality of\ngenerated samples. We validate our approach on both im-\nage and video autoencoders, including FluxAE (Black For-\nest Labs, 2023), CosmosTokenizer (Agarwal et al., 2025),\nCogVideoX-AE (Hong et al., 2022), and LTX-AE (HaCo-\nhen et al., 2024), consistently demonstrating improved LDM\nperformance on ImageNet-1K (Deng et al., 2009) 2562, re-\nducing FID by 19% for DiT-XL, and Kinetics-700 (Carreira\net al., 2019) 17 × 2562, reducing FVD by at least 44%.\n2. Related Work\nDiffusion models. Diffusion models (Vahdat et al., 2021;\nRombach et al., 2022; Song et al., 2021; Ho et al., 2020;\nNichol & Dhariwal, 2021; Karras et al., 2022) have emerged\nas the dominant framework for generative modeling, sur-\npassing traditional approaches like GANs (Goodfellow et al.,\n2014; Karras et al., 2019) and VAEs (Kingma & Welling,\n2013). Diffusion models express generation as a denoising\nprocess producing the generated content by progressively\ndenoising an initial noise sample. Owing to their efficiency\nand scalability, foundational generative models (Saharia\net al., 2022; Ho et al., 2022; Yang et al., 2024; Podell et al.,\n2023; Blattmann et al., 2023; Polyak et al., 2024) have made\nsignificant strides in synthesizing visually stunning and se-\nmantically aligned images and videos.\nInitially applied to low-resolution visual content in the\npixel space (Vahdat et al., 2021; Ho et al., 2020; Nichol\n& Dhariwal, 2021; Karras et al., 2022), they have soon\nbeen extended to higher resolutions. In Latent Diffusion\nModels (LDMs) (Vahdat et al., 2021; Rombach et al.,\n2022) high-resolution visual content is modeled in the com-\npact latent space produced by a variational autoencoder\n(VAE) (Kingma & Welling, 2013) within a two-stage frame-\nwork. Latent Flow Models (LFMs) (Dao et al., 2023; Liu\net al., 2024), follow the same approach but leverage Recti-\nfied Flows (RFs) to enable faster and more stable sampling.\nRecent work attributes the success of diffusion models to\na form of implicit spectral autoregression (Rissanen et al.,\n2023; Ning et al., 2024) implied by the progressive removal\nof noise during sampling, resulting in the generation of vi-\nsual content in a coarse-to-fine manner. Such result holds\nin the pixel-space of natural images, based on its pattern\nof decreasing spectral power (Ruderman, 1997). We show\nthat popular autoencoders (Black Forest Labs, 2023; Agar-\nwal et al., 2025; HaCohen et al., 2024; Yang et al., 2024)\nhave a less pronounced pattern of decreasing spectral power,\ninhibiting implicit spectral autoregressive generation. Build-\ning on this observation, this work proposes a regularization\nscheme that re-establishes this property, consistently show-\ning improved LDM performance and avoiding the need for\nexplicit coarse-to-fine generation.\nImage and video autoencoders. Due to the success of\nLDMs, a lot of effort has been devoted to the development\nof better AEs. Image LDMs (Rombach et al., 2022) and\nearly video diffusion models (Blattmann et al., 2023; Guo\net al., 2023) employ a spatial AE with a compression ra-\ntio of 1×8×8. The rapid advancement of video diffusion\nmodels poses the demand for 3D AEs that jointly compress\nspatial and temporal dimensions to further improve effi-\nciency (Hong et al., 2022; Zhou et al., 2024; Kong et al.,\n2024). Among them, Open-Sora (Zheng et al., 2024) in-\nherits the 1×8×8 spatial AE and trains a decoupled 4×\n2\n\nImproving the Diffusability of Autoencoders\ntemporal AE on top of its latent space, while others tend\nto build a hierarchical spatio-temporal AE with 3D causal\nconvolutions (Xing et al., 2024; Wu et al., 2024; Chen et al.,\n2024a; Zhao et al., 2024; Sadat et al., 2024; Hansen-Estruch\net al., 2025). To improve the reconstruction quality, Open-\nSora-Plan (Lin et al., 2024) and CosmosTokenizer (Agarwal\net al., 2025) propose to employ wavelet transforms to en-\nhance high-frequency details. Another popular trend is to\nfurther increase the compression ratio to reduce the number\nof tokens in the latent space (Xie et al., 2024; Tian et al.,\n2024; HaCohen et al., 2024), thus enabling a more efficient\ndenoising process. In addition to the continuous AEs ex-\nplored in this work, multiple discrete AEs (Wang et al.,\n2024; Tang et al., 2024; Agarwal et al., 2025) are proposed\nto aid autoregressive tasks. Esteves et al. (2024) leverages\na wavelet transform to produce latents corresponding to\ndifferent frequency components.\nAEs for compression. Many works train neural-based AEs\nfor image compression (Ball´e et al., 2016; 2018; Minnen\net al., 2018; Cheng et al., 2020), typically with 16×16 down-\nsampled latents which are discrete and entropy constrained.\nVideo compression AEs involve autoregressive AEs (Li\net al., 2021; 2023; Sheng et al., 2022) with explicit frame-\nwise formulations that utilize motion vectors or implicit\nmodeling (Mentzer et al., 2022). These approaches tar-\nget high-quality reconstruction with low bitrates and adopt\ncomplex designs for learnable entropy models. They typi-\ncally employ a larger number of latent bottleneck channels\n(96−192), which is not generally suited for the generation\ntask, thus we do not consider them in this work.\nConcurrent works.\nIndependently from us,\nEQ-\nVAE (Kouzelis et al., 2025) proposes scale equivariance\nregularization for autoencoders, but with a different motiva-\ntion of improving their equivariance to spatial transforms.\n3. Improving Diffusability\nWe begin this section by discussing the spectral decompo-\nsition of 2D signals and providing some background on\ndiscrete cosine transform in Section 3.1. In Section 3.2, we\nanalyze the spectral properties of latent spaces across dif-\nferent autoencoders and compare them to those of the RGB\nspace. Our main insight is that the frequency profile of the\nlatent space includes large-magnitude high-frequency com-\nponents. We also show that as the channel size increases, the\nhigh-frequency components become more pronounced. Ad-\nditionally, we demonstrate that the widely adopted KL regu-\nlarization only increases the strength of these components.\nFinally, Section 3.3 presents a straightforward method to\nimprove the diffusability of a latent space of an autoencoder\nby enhancing its spectral properties.\n0\n10\n20\n30\n40\n50\n60\nZigzag Frequency Index\n10−3\n10−2\n10−1\n100\n101\nNormalized Amplitude\nFluxAE-ch4\nFluxAE-ch8\nFluxAE-ch16\nFluxAE-ch24\nFluxAE-ch32\nFluxAE-ch48\nFluxAE-ch64\nRGB\nFigure 2. Comparison between the reconstructions from a family\nof FluxAE autoencoders with different bottleneck sizes trained in\nidentical conditions (with different KL regularization strengths),\nand also RGB. One can note two things: 1) the latent space of an\nautoencoder, in general, exhibits a different power profile from\nRGB; and 2) the coefficient magnitudes of high frequencies in the\nspectrum increase with the channel sizes.\n3.1. Background: Blockwise 2D DCT\nThe discrete cosine transform (DCT) (Ahmed et al., 2006)\nover a 2D signal is a transformation converting the signal’s\nrepresentation between the spatial and frequency domains.\nDCT, in particular, represents the original input signal as\ncoefficients for a set of horizontal and vertical cosine basis\noscillating with different frequencies. More formally, given\na 2D signal block A ∈RB×B whose values Axy denote\nthe pixel intensity at position (x, y), the two-dimensional\ntype-II DCT yields a frequency-domain block D ∈RB×B\nwhere Duv captures the coefficient for the corresponding\nhorizontal and vertical cosine bases:\nDuv = α(u)α(v)\nB−1\nX\nx=0\nB−1\nX\ny=0\nAxyf(x, u)f(y, v),\nwhere\nα(u) =\n\u001ap\n1/B,\nu = 0,\np\n2/B,\nu ̸= 0,\nf(x, u) = cos\n\u0010\n(2x+1)uπ\n2B\n\u0011\n.\nIn practice, we split the input 2D signal into non-overlapping\nblocks of size B × B and treat each channel independently.\nBy analyzing RGB images and latents in the DCT frequency\ndomain, we produce a frequency profile that relates to the\nenergy of the signal at every frequency. A zigzag frequency\nindex is used to map each DCT block D ∈RB×B into a one-\ndimensional sequence following the standard zigzag order-\ning as in JPEG (Wallace, 1991), which indexes the DCT co-\nefficients from lowest frequency D0,0 to highest frequency\nDB−1,B−1. Formally, let zigzag(u, v) ∈{0, . . . , B2 −1}\ndenote the ranks of the coefficient Duv in ascending fre-\nquency order. Given a block, we compute its DCT and\n3\n\nImproving the Diffusability of Autoencoders\n0\n10\n20\n30\n40\n50\n60\nZigzag Frequency Index\n10−2\n10−1\n100\n101\nNormalized Amplitude\nFluxAE; KL β = 0e+00\nFluxAE; KL β = 1e−07\nFluxAE; KL β = 1e−05\nFluxAE; KL β = 1e−04\nFluxAE; KL β = 1e−03\nFluxAE; KL β = 1e−01\nFigure 3. Spectrums for FluxAE autoencoders trained with differ-\nent KL regularization strengths. KL regularization is a double-\nedged sword: it pushes the latents distribution closer to standard\nGaussian (the distribution the reverse diffusion process starts with),\nso that the LDM has less work to do (Vahdat et al., 2021), but it\nalso introduces high-frequency components into the latents due to\nthe random noise addition, which LDM is forced to model as well.\n0\n10\n20\n30\n40\n50\n60\nZigzag Frequency Index\n10−3\n10−2\n10−1\n100\nNormalized Amplitude\nLatents; w/ SE\nLatents\nRGB\nFigure 4. DCT Spectrum of the FluxAE latents with and without\nscale equivariance (SE). Fine-tuning AEs with SE brings the spec-\ntrum closer to the RGB domain.\nproduce normalized amplitudes for each frequency compo-\nnent (u, v) as:\nAuv =\n\f\f\f\f\nDuv\nD0,0\n\f\f\f\f .\n(1)\nWe define the frequency profile as the sequence of normal-\nized amplitudes in the standard zigzag order.\nWhen analyzing the frequency profiles of videos (or latent\ncodes with an additional time dimension), we still rely on\nper-frame 2D DCT since the temporal and spatial domains\npossess different spectral properties.\n3.2. Spectral Analysis of the Latent Space\nWe begin our analysis by observing the frequency profile\nof the latent space in the Flux (Black Forest Labs, 2023)\nfamily of autoencoders to establish a relationship with dif-\nfusability. For the purpose of this study, we train a family of\nFluxAE models with various channel sizes for 100k steps\n(where performance saturates in this setting) and, for each\nof them, compute the averaged frequency profile over 256\nsamples, all channels, and all DCT blocks. Figure 2 presents\nthe frequency profiles of both Flux autoencoders and RGB\nspace, from which we observe: (i) The Flux profile exhibits\nsignificantly larger high-frequency components compared\nto the RGB profile. (ii) As the number of channels in the\nautoencoder’s bottleneck increases, high-frequency com-\nponents become more pronounced. This observation is of\nparticular interest as the number of channels is positively\ncorrelated with autoencoder’s reconstruction quality.\nA common approach to regularizing the latent space in la-\ntent diffusion models (LDMs) is to employ a variational\nautoencoder (VAE) (Kingma & Welling, 2013) framework\nwith a KL divergence term, encouraging the latent distribu-\ntion to align with a Gaussian prior. This regularization can\nhelp to bring the latent distribution closer to the standard\nGaussian (Vahdat et al., 2021), simplifying the job for the\ndiffusion process, as the reverse process starts with the same\ndistribution. However, as we show in Figure 3 which com-\npares FluxAE models trained with different levels of KL\nregularization, higher KL regularization introduces more\nhigh frequencies.\nAs described in (Ning et al., 2024; Dieleman, 2024; Rissa-\nnen et al., 2023), diffusion models can be interpreted in the\nspectral domain as autoregressive processes: when noise\nlevel is high, low frequencies are generated, then, as the\nlevel of noise lowers during sampling, progressively higher\nfrequencies are generated. This property is desirable, as it\nallows the model to leverage the cleaner lower frequencies\nas a conditioning signal for the current prediction. However,\nthe strength of this autoregressive pattern is directly related\nto the shape of the frequency profile for the signal to gen-\nerate. Since the white noise that is applied as part of the\ndiffusion process has a flat frequency profile, it follows that\nthe flatter the frequency profile of the signal, the lower the\ncleanliness of low frequencies that can act as conditioning\nfor the model. For a flat frequency profile, no autoregressive\ngeneration is possible as all frequencies would be erased at\nthe same speed by white noise. We also hypothesize that\nhigher frequencies components are harder to model than\nlower frequency components for the following reasons and\nthus should be avoided: (i) they have higher dimensionality;\n(ii) they are generated only in the final steps of sampling,\nthus must emerge more rapidly; (iii) they are more suscepti-\nble to error accumulation over time.\nMotivated by this analysis, we propose scale equivariance\nregularization for the autoencoder’s latent space.\n3.3. Scale Equivariance Regularization\nEffective regularization should achieve two key objectives:\n(i) to suppress high-frequency components in the latent\n4\n\nImproving the Diffusability of Autoencoders\n0%\n25%\n50%\n75%\nRGB\nFluxAE\nFluxAE + CHF\nFigure 5. RGB and autoencoder reconstructions with progres-\nsively erased DCT high-frequency components. RGB faces min-\nimal degradation (top), as a higher percentage of the latent DCT\nspectrum is removed, but the Flux AE reconstructions (middle)\nquickly degrade when the high-frequency components from the\nlatents are being removed. A high-frequency cutoff regularization\nforces the autoencoder to rely more on the low frequency region\nof the latents and leads to better compression and resilience to\nhigh-frequency error accumulation in diffusion models.\nspace and (ii) to prevent the decoder from amplifying these\ncomponents, as their impact on the final result is what ulti-\nmately matters. This can be accomplished by aligning the\nspectral properties of the latent and RGB spaces at differ-\nent frequencies. A way to achieve this consists in explicitly\nchopping off a portion of the high frequencies in both spaces\nand training the decoder to reconstruct the truncated RGB\nsignal from the truncated latent representation. Our prelim-\ninary experiments demonstrated that an autoencoder can\neasily learn to alter its latent frequency profile to encode the\ninputs in the low-frequency region of the spectrum without\nsacrificing the reconstruction quality much (see Figure 5).\nWhile this regularization, which we name Chopping High\nFrequencies (CHF), improves the spectrum, we develop a\nmuch simpler procedure to achieve the same effect without\nthe need to perform the error-prone DCT transform (the\ndetails of CHF are described in Appendix B). The simplest\nway to achieve high-frequency truncation is through direct\ndownsampling, which we discuss next.\nDownsampling involves resizing both the input x and the\nlatent representation z by a fixed scale, yielding ˜x and ˜z, re-\nspectively. This process effectively removes a portion of the\nhigh-frequency components from both the RGB and latent\nsignals. In practice, we use ×2 −4 bilinear downsampling\nfor all the experiments. Regularization is then enforced\nby ensuring that ˜x and the decoder’s reconstruction of the\ndownsampled latent Dec(˜z) remain consistent through an\nadditional reconstruction loss. The autoencoder is trained\nFigure 6. Denoising trajectories (steps 1, 16, 32, 128 and 256 out\nof 256) for DiT-XL trained with FluxAE (top) and FluxAE+SE.\nDiT-XL with vanilla FluxAE exhibits prominent high-frequency\nartifacts early on in the trajectory.\nusing the following objective:\nL(x) = d(x, Dec(z)) + αd(˜x, Dec(˜z)) + βLKL.\n(2)\nHere, d(·, ·) represents a distance measure for reconstruction\nwhich we instantiate as mean squared error loss and percep-\ntual losses (Zhang et al., 2018) following prior work (Rom-\nbach et al., 2022), α is the regularization strength (we use\nα = 0.25 for the main experiments). The term LKL is\nVAE’s (Kingma & Welling, 2013) KL regularization, if\napplicable (we do not use it when we train with our regu-\nlarization). This regularization effectively enforces scale\nequivariance in the decoder, which is the basis for its name.\nIn Figure 4, we illustrate the effect of scale equivariance on\nthe spectrum of FluxAE. Our proposed regularization effec-\ntively reduces the high-frequency components of the signal,\nbringing it closer to the spectral characteristics of the RGB\nspace. This successfully achieves objective (i) for effec-\ntive regularization. Meanwhile, Figure 8 demonstrates that\nscale equivariance preserves more content compared to the\nbaseline, as more and more high-frequency components are\nsuppressed, thereby fulfilling objective (ii). Finally, Figure 6\nvisualizes intermediate steps in the diffusion trajectory. The\nregularized model exhibits a noticeably smoother and more\nstructured progression, following a healthier coarse-to-fine\ngeneration process.\n4. Experiments\nData. We trained all the autoencoders on in-the-wild data\nwhich do not overlap with ImageNet-1K (Deng et al., 2009)\nor Kinetics-700 (Carreira et al., 2019) to make sure that\nthere is no data leak in the autoencoders, and that they\nremain general-purpose. For this, we used our internal\nimage and video datasets of the 2562 resolution, which\nare similar in distribution of concepts and aesthetics to the\npublicly available in-the-wild datasets like COYO (Byeon\net al., 2022) and Panda-70M (Chen et al., 2024b). To control\nfor the impact of the data (and also the training recipe),\nwe trained a separate autoencoder baseline for each setup\n5\n\nImproving the Diffusability of Autoencoders\nFigure 7. Uncurated samples on ImageNet 256 × 256 from DiT-XL trained on top of FluxAE (top) vs DiT-XL with “FluxAE + FT-SE”\n(bottom). 256 steps with the guidance scale of 3.0. More visualizations are in Appendix C.\nTable 1. Quantitative performance on ImageNet (Deng et al., 2009)\nwithout guidance. The original DiT scores are provided for refer-\nence from Table 4 of (Peebles & Xie, 2022).\nStage II\nStage I\nFID ↓\nFDD ↓\nDiT-B/2\nFluxAE (vanilla)\n25.41\n536.2\nFluxAE + FT\n30.51\n575.4\nFluxAE + FT-SE (ours)\n18.06\n450.6\nDiT-L/2\nFluxAE (vanilla)\n12.42\n306.73\nFluxAE + FT\n14.48\n333.54\nFluxAE + FT-SE (ours)\n9.61\n236.43\nDiT-XL/2\nFluxAE (vanilla)\n12.21\n282.8\nFluxAE + FT\n10.62\n262.2\nFluxAE + FT-SE (ours)\n9.85\n235.8\n+1M steps\n3.27\n85.86\nDiT-B/1\nCMS-AEI (vanilla)\n11.69\n360.83\nCMS-AEI + FT\n13.59\n375.19\nCMS-AEI + FT-SE (ours)\n11.85\n354.22\nDiT-B/2 (orig)\nSD-VAE-ft-MSE\n43.47\n−\nDiT-L/2 (orig)\n23.33\n−\nDiT-XL/2 (orig)\n19.47\n−\n+ 7M steps (orig)\n12.03\n−\nwithout using our proposed regularization. In several cases,\njust fine-tuning on such in-the-wild data already yields better\ndiffusion performance (e.g., see DiT-XL results in Table 1).\nEvaluation.\nWe evaluate image DiT models via FID\nand FDD (Frechet Distance computed on top of DI-\nNOv2 (Oquab et al., 2023) features), where the latter was\nshown to be a more reliable metric (Stein et al., 2024;\nKarras et al., 2024). We evaluate video DiT models with\nFVD10K (Unterthiner et al., 2018), FID, and FDD, except\nfor ablations where we rely on 5,000 samples. For image\nmodels, we use 50,000 samples without any optimization\nfor class balancing. To evaluate autoencoders, we used\nPSNR, SSIM, LPIPS and FID metrics computed on 512\nsamples from ImageNet and Kinetics-700 for image and\nvideo autoencoders, respectively.\nTraining details. All the LDM models are trained for 400k\nsteps with 10k warmup steps following the rectified flow dif-\nfusion parametrization (Albergo & Vanden-Eijnden, 2022;\nDao et al., 2023; Esser et al., 2024). Following Esser et al.\n(2024), we use a logit-normal training noise distribution.\nWe use either 2 × 2 or 1 × 1 patchification in DiT (Peebles\n& Xie, 2022) to match the compute between DiTs trained\non top autoencoders with different compression ratios. Our\nvideo DiT is a direct adaption of the image one where we\nadditionally unroll the temporal axis, following the recent\nworks on video diffusion models (Yang et al., 2024). We do\nnot use patchification for the temporal axis in video DiTs.\nIn contrast to prior work (e.g., Rombach et al. (2022)), we\naverage the KL loss across the latent channels and resolu-\ntions: this has no theoretical impact, but allows to compare\nautoencoders with different bottleneck sizes.\nInference details. We run DiT inference with 256 steps\nwithout classifier-free guidance (Ho & Salimans, 2022) for\nquantitative evaluations since different models are too sensi-\ntive to it and should be tuned separately (Karras et al., 2024;\nKynk¨a¨anniemi et al., 2024).\n4.1. Improving Existing Autoencoders\nWe apply our training pipeline on top of 3 different autoen-\ncoders. For each autoencoder, we trained it while freez-\ning the last output layers to avoid breaking their adversar-\nial fine-tuning, which should have no impact on the latent\nspace (Chen et al., 2025). We emphasize that none of the ex-\nplored modern autoencoders publicly released their training\npipelines. For the pretrained snapshots of autoencoders, we\nused the original snapshots available in the diffusers\nlibrary (von Platen et al., 2022).\nImproving image autoencoders. For the image autoen-\ncoder, we used FluxAE (Black Forest Labs, 2023) with\n8 × 8 compression ratio and 16 latent channels (since it\nis the most popular modern autoencoder in the commu-\n6\n\nImproving the Diffusability of Autoencoders\nnity) and CMS-AEI (Agarwal et al., 2025) with 16 × 16\ncompression ratio and 16 channels as a high-compression\nautoencoder. For all the experiments (unless stated other-\nwise), we fine-tuned it for just 10,000 training steps with\na batch size of 32 (320,000 total seen images) using 2×\nand 4× downsampling ratios, chosen randomly during a\nforward pass of the regularization loss. DiT training on top\nof the unchanged Flux Autoencoder is labeled as “vanilla”.\nAutoencoders fine-tuned for 10,000 steps with our proposed\nSE regularization is denoted via the “+ FT-SE” suffix. To\ncontrol for the fine-tuning data and training pipeline, we fine-\ntuned each autoencoder without adding our regularization\nas an additional loss (denoted via “+ FT”).\nFor the LDM benchmark, we utilized ImageNet (Deng et al.,\n2009) at 256 × 256 resolution. We used the DiT (Peebles\n& Xie, 2022) model as the backbone since it is the most\npopular modern latent diffusion backbone. Compared to\nthe original paper, we incorporated several recent advance-\nments into the DiT architecture to improve the baseline\nperformance, as described in Appendix A. Qualitative sam-\nples from DiT-XL/2 are provided in Figure 7. The results\nare shown in Table 1. One can observe that our proposed\nregularization greatly improves the diffusability of the down-\nstream LDM model, allowing to achieve 19% lower FID\ncompared to the vanilla Flux AE and 8% lower FID com-\npared to the Flux AE, fine-tuned in our training pipeline\nwithout the SE regularization.\nThe improvement for CMS-AEI is reduced with the main\nreason being that our training pipeline hurts its performance\n(we explored over 10 different hyperparameters setups to\ntune the vanilla model): after fine-tuning it for 10,000 steps\nwith only reconstruction losses (it does not use KL regu-\nlarization by default), the downstream FID performance\nincreases by 14% from 11.69 to 13.59.\nImproving video autoencoders. For video autoencoders,\nwe used CogVideoX-AE (Hong et al., 2022) (CV-AE) with\n4 × 8 × 8 compression and 16 latent channels and LTX-\nAE (HaCohen et al., 2024) with 8 × 32 × 32 compression\nand 32 latent channels. The latter serves as a strong high-\ncompression autoencoder baseline. For all the experiments,\nwe fine-tune them for 20,000 training steps on the joint\nimage and video dataset with the batch size of 32. Image\nbatches are treated as single-frame videos which is possible\ndue to the causal structure of the video autoencoders (Yu\net al., 2023).\nSimilar to the image autoencoder experiments, we train a\nDiT model on Kinetics-700 (Carreira et al., 2019) on three\nvariants: 1) a “vanilla” autoencoder snapshot; 2) the vanilla\nautoencoder fine-tuned for 20,000 steps in our training\npipeline (denoted as “FT”); and 3) the autoencoder snapshot,\nfine-tuned with our downsampling regularization (denoted\nas “FT-SE”). For LTX-AE, we used a reduced patchification\nTable 2. Results on Kinetics-700 (Carreira et al., 2019) for DiT\ntrained on top of various autoencoders. See Section 4.1 for details.\nStage II\nStage I\nFVD10K ↓\nFDD ↓\nFID10K ↓\nDiT-B/2\nCV-AE (vanilla)\n650.40\n650.97\n28.85\nCV-AE + FT\n447.26\n593.02\n19.45\nCV-AE + FT-SE (ours)\n252.26\n466.15\n12.19\nDiT-XL/2\nCV-AE (vanilla)\n268.26\n407.23\n12.02\nCV-AE + FT\n270.66\n402.91\n12.78\nCV-AE + FT-SE (ours)\n135.15\n245.27\n8.59\nDiT-B/1\nLTX-AE (vanilla)\n854.47\n814.49\n50.99\nLTX-AE + FT\n876.61\n823.71\n50.18\nLTX-AE + FT-SE (ours)\n389.56\n642.80\n22.88\nTable 3. Ablating KL regularization weight β for FluxAE fine-\ntuning in terms of the reconstruction quality and downstream DiT-\nS/2 and DiT-L/2 (Peebles & Xie, 2022) training. Increasing the KL\nin general improves the LDM’s performance for smaller models,\nbut at the expense of worsened AE reconstruction (and reduced\ntraining stability), which can limits scaling (Esser et al., 2024)\nand results in worse performance of DiT-L/2. Our SE regular-\nization leads to improved LDM performance without hurting the\nreconstruction and scales well to larger models.\nMethod\nDiT-S/2 FDD5K\nDiT-L/2 FDD5K\nAE PSNR512\nFluxAE (vanilla)\n992.05\n415.87\n30.20\n+ KL β = 0\n968.26\n472.08\n29.97\n+ KL β = 10−7\n1018.6\n425.35\n30.29\n+ KL β = 10−6\n1095.2\n612.12\n19.66\n+ KL β = 10−5\n940.13\n403.99\n29.21\n+ KL β = 10−4\n974.67\n404.61\n30.22\n+ KL β = 10−3\n982.91\n425.24\n29.51\n+ KL β = 10−2\n1946.5\n1737.47\n10.82\n+ KL β = 10−1\n929.58\n472.74\n23.72\n+ FT-SE (ours)\n924.28\n369.15\n30.37\nresolution of 1 × 1 to compensate for its extreme compres-\nsion ratio. The results are presented in Table 2: DiT model\non top SE-regularized autoencoders has drastically better\nperformance: 44% and 54% lower FVD10K for CV-AE and\nLTX-AE, respectively. Our training pipeline allowed to\nachieve better DiT-B training for CV-AE (650.4 vs 447.3\nFVD10K), but led to worse scores for LTX-AE (854.4 vs\n876.6), which we found less stable to train. Adding our reg-\nularization strategy greatly improves the LDM performance\nin each case. One can also observe that the boost for video\nautoencoders is larger than in the image domain (at least\n−44% reduced FVD10K for DiT-B for video generation vs\n−7% reduced FID for DiT-XL for image generation). We\nattribute this to two factors. First, improvements in Frechet\nDistances (Heusel et al., 2017) do not scale linearly (i.e.,\ntheir smaller values are progressively harder to improve).\nNext, causal video autoencoders have less regular latent\nstructure: the first frame in a video is encoded into the same\nrepresentation size as the subsequent chunks, which leaves\nmore room to enhance the diffusability of the latent space.\nWe additionally trained a DiT-XL/2 model for the CV-AE\nfamily to explore the scalability of our regularization. For\n7\n\nImproving the Diffusability of Autoencoders\n0.0\n0.2\n0.4\n0.6\nCut Ratio\n20\n22\n24\n26\n28\n30\nPSNR\nFluxAE - Baseline\nFluxAE - Finetune\nFluxAE - Finetune w/ SE\n0.0\n0.2\n0.4\n0.6\nCut Ratio\n0.5\n0.6\n0.7\n0.8\n0.9\nSSIM\nFluxAE - Baseline\nFluxAE - Finetune\nFluxAE - Finetune w/ SE\n0.0\n0.2\n0.4\n0.6\nCut Ratio\n0.1\n0.2\n0.3\n0.4\n0.5\nLPIPS\nFluxAE - Baseline\nFluxAE - Finetune\nFluxAE - Finetune w/ SE\n0.0\n0.2\n0.4\n0.6\nCut Ratio\n75\n100\n125\n150\n175\n200\n225\nFID512\nFluxAE - Baseline\nFluxAE - Finetune\nFluxAE - Finetune w/ SE\nFigure 8. Effect of DCT spectrum cutting. We plot reconstruction metrics on ImageNet for the baseline FluxAE and its finetuned version,\nwith and without scale equivariance. As more high-frequency DCT coefficients are removed from the latents, the AE with regularization\nconsistently achieves the best reconstruction quality.\n-∞\n... 10−2\n10−1\n100\nRegularization Weight\n29.50\n29.75\n30.00\n30.25\n30.50\n30.75\nPSNR\nFluxAE - Finetune\nFluxAE - Base\n-∞\n... 10−2\n10−1\n100\nRegularization Weight\n0.880\n0.885\n0.890\n0.895\nSSIM\nFluxAE - Finetune\nFluxAE - Base\n-∞\n... 10−2\n10−1\n100\nRegularization Weight\n0.050\n0.055\n0.060\n0.065\nLPIPS\nFluxAE - Finetune\nFluxAE - Base\nFigure 9. Reconstruction quality with varying SE regularization\nstrength for fine-tuning. We find the value of 0.25 to be optimal\nin maintaining reconstruction quality compared to the Base while\nalso improving generation quality as shown in Tables 1 and 2.\nTable 4. Influence of spectrum regularization on the reconstruction\nquality for image and video autoencoders. Image autoencoders are\nevaluated on 50,000 images from ImageNet-1K, while the video\nones are on 50,000 videos from Kinetics-700.\nMethod\nPSNR ↑\nSSIM ↑\nLPIPS ↓\nFID ↓\nFVD50K ↓\nFluxAE\n30.243\n0.883\n0.054\n0.183\n−\n+FT-SE (ours)\n30.474\n0.888\n0.055\n0.550\n−\nCMS-AEI\n23.230\n0.638\n0.181\n1.077\n−\n+FT-SE (ours)\n24.558\n0.677\n0.166\n1.570\n−\nCogVideoX-AE\n34.961\n0.947\n0.073\n2.992\n4.614\n+FT-SE (ours)\n35.399\n0.948\n0.067\n2.986\n3.328\nLTX-AE\n30.897\n0.886\n0.152\n5.928\n36.783\n+FT-SE (ours)\n30.386\n0.885\n0.137\n5.303\n34.148\nthis large-scale setup, our SE regularization improved the\nFVD10K score by almost twice.\n4.2. Ablations\nDoes scale equivariance hurt reconstruction? scale equiv-\nariance in VAEs improves downstream generation quality in\nterms of FID (Tables 1 and 2). We now examine its impact\non AE reconstruction quality. Table 4 presents results across\nfour reconstruction metrics — PSNR, SSIM, LPIPS (Zhang\net al., 2018), and FID, on 50,000 samples from ImageNet\nand Kinetics for image and video autoencoders, respectively.\nReconstruction quality remains similar across the models.\nCan LDM performance be improved by tweaking the\nKL weight instead? In Table 3, we show that increasing\nthe KL strength can indeed improve the LDM performance\nfor DiT-S/2, but it inevitably hurts reconstruction, as shown\nby the PSNRs, which bottlenecks the scalability of larger\nLDM models. In contrast, our proposed scale equivariance\nallows to achieve good LDM performance without hurting\nthe reconstruction quality of the autoencoder. For these\nablations, we trained the DiT-S/2 variants for 200K training\nsteps and DiT-L/2 variants for 400K steps, and ran inference\nfor 80 steps without classifier-free guidance. One can see\nthat for a small compute budget, increasing the KL strength\nis beneficial: the best LDM score is obtained with the high-\nest KL β = 0.1. But it severely affected the reconstruction\nquality of the autoencoder, which limited its scaling: the cor-\nresponding DiT-L/2 LDM variant is ranked among the worst.\nAt the same time, our developed regularization performs\nwell for all DiT variants and does limit scaling.\nEffect of SE regularization strength. In Figure 9, we\nshow the effect of varying the loss weight on the FluxAE\nperformance on ImageNet. Increasing the SE regularization\nstrength naturally worsens the total reconstruction quality\nsince the decoder is trained to generalize its performance\nacross both low frequency and high frequency latents (via\ndownsampling) while having the same capacity. We choose\nthe value of 0.25 to maintain reconstruction performance\ncompared to the base AE model while improving the gener-\nation quality as shown in Tables 1 and 2.\nEffect of DCT spectrum cutting. To examine the impact\nof downsampling regularization, we evaluate reconstruc-\ntion quality by progressively removing high-frequency DCT\ncomponents from the latents. Figure 8 presents reconstruc-\ntion metrics on 512 samples from the ImageNet validation\n8\n\nImproving the Diffusability of Autoencoders\nset for the baseline FluxAE and fine-tuning, with and with-\nout regularization. As the cut ratio increases on the x-axis,\nindicating the removal of more high-frequency components,\nthe AE with regularized latents consistently achieves the\nbest reconstruction quality across all metrics. This under-\nscores the regularization role in aligning the spectral proper-\nties of the latent and RGB spaces.\n5. Conclusion\nWe have shown that modern Latent Diffusion Models\n(LDMs) rely just as critically on their autoencoders as on the\nmore frequently investigated diffusion architectures. While\nprior work has largely focused on improving reconstruc-\ntion quality and compression rates for the autoencoders, our\nstudy focuses on diffusability, revealing how latent spec-\ntra with excessive high-frequency components can hamper\nthe downstream diffusion process. Through a systematic\nanalysis of several autoencoders, we uncovered stark dis-\ncrepancies between latent and RGB spectral properties and\ndemonstrated that they lead to worse LDM synthesis quality.\nBuilding on this insight, we developed a regularization strat-\negy that aligns the latent and RGB spaces across different\nfrequencies. Our approach maintains reconstruction fidelity\nand improves diffusion training by suppressing spurious\nhigh-frequency details in the latent code. Potential future di-\nrections include exploring more advanced frequency-based\nregularizations, adaptive compression methods and scale\nequivariance regularization in the temporal axis for video\nautoencoders to further optimize the trade-off between re-\nconstruction quality, compression rate, and diffusability.\nImpact Statement\nThis work focuses on improving the representations in au-\ntoencoders that serve as a backbone for latent diffusion\ntraining, ultimately enhancing generative performance. Our\nimprovements can facilitate beneficial applications such as\nboosting creativity, supporting educational content creation,\nand reducing computational overhead in generative work-\nflows. Beyond these considerations, we do not identify addi-\ntional ethical or societal implications beyond those already\nknown to accompany large-scale generative modeling.\nReferences\nAgarwal, N., Ali, A., Bala, M., Balaji, Y., Barker, E., Cai,\nT., Chattopadhyay, P., Chen, Y., Cui, Y., Ding, Y., et al.\nCosmos world foundation model platform for physical ai.\narXiv preprint arXiv:2501.03575, 2025.\nAhmed, N., Natarajan, T., and Rao, K. R. Discrete cosine\ntransform. IEEE transactions on Computers, 100(1):90–\n93, 2006.\nAlbergo, M. S. and Vanden-Eijnden, E. Building normal-\nizing flows with stochastic interpolants. arXiv preprint\narXiv:2209.15571, 2022.\nBall´e, J., Laparra, V., and Simoncelli, E. P.\nEnd-to-\nend optimized image compression.\narXiv preprint\narXiv:1611.01704, 2016.\nBall´e, J., Minnen, D., Singh, S., Hwang, S. J., and Johnston,\nN. Variational image compression with a scale hyperprior.\narXiv preprint arXiv:1802.01436, 2018.\nBetker, J., Goh, G., Jing, L., Brooks†, T., Wang, J.,\nLi, L., Ouyang, L., Zhuang, J., Lee, J., Guo†, Y.,\nManassra, W., Dhariwal, P., Chu, C., Jiao†, Y., and\nRamesh, A. Improving image generation with better\ncaptions.\nhttps://cdn.openai.com/papers/\ndall-e-3.pdf, 2023. Accessed: 2023-11-14.\nBlack Forest Labs.\nFlux.\nhttps://github.com/\nblack-forest-labs/flux, 2023.\nBlattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D.,\nKilian, M., Lorenz, D., Levi, Y., English, Z., Voleti, V.,\nLetts, A., et al. Stable video diffusion: Scaling latent\nvideo diffusion models to large datasets. arXiv preprint\narXiv:2311.15127, 2023.\nBrooks,\nT.,\nPeebles,\nB.,\nHolmes,\nC.,\nDePue,\nW.,\nGuo, Y., Jing, L., Schnurr, D., Taylor, J., Luhman,\nT., Luhman, E., Ng, C., Wang, R., and Ramesh,\nA.\nVideo generation models as world simulators.\n2024.\nURL https://openai.com/research/\nvideo-generation-models-as-world-simulators.\nByeon,\nM.,\nPark,\nB.,\nKim,\nH.,\nLee,\nS.,\nBaek,\nW., and Kim, S.\nCoyo-700m:\nImage-text pair\ndataset.\nhttps://github.com/kakaobrain/\ncoyo-dataset, 2022.\nCarreira, J., Noland, E., Hillier, C., and Zisserman, A. A\nshort note on the kinetics-700 human action dataset. arXiv\npreprint arXiv:1907.06987, 2019.\nChen, J., Cai, H., Chen, J., Xie, E., Yang, S., Tang, H.,\nLi, M., Lu, Y., and Han, S. Deep compression autoen-\ncoder for efficient high-resolution diffusion models. arXiv\npreprint arXiv:2410.10733, 2024a.\nChen, J., Cai, H., Chen, J., Xie, E., Yang, S., Tang, H.,\nLi, M., Lu, Y., and Han, S. Deep compression autoen-\ncoder for efficient high-resolution diffusion models. In\nThe Thirteenth International Conference on Learning\nRepresentations, 2025. URL https://openreview.\nnet/forum?id=wH8XXUOUZU.\n9\n\nImproving the Diffusability of Autoencoders\nChen, T.-S., Siarohin, A., Menapace, W., Deyneka, E., Chao,\nH.-w., Jeon, B. E., Fang, Y., Lee, H.-Y., Ren, J., Yang, M.-\nH., and Tulyakov, S. Panda-70m: Captioning 70m videos\nwith multiple cross-modality teachers. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2024b.\nCheng, Z., Sun, H., Takeuchi, M., and Katto, J. Learned\nimage compression with discretized gaussian mixture\nlikelihoods and attention modules. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern\nrecognition, pp. 7939–7948, 2020.\nDao, Q., Phung, H., Nguyen, B., and Tran, A. Flow match-\ning in latent space. arXiv preprint arXiv:2307.08698,\n2023.\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,\nL. Imagenet: A large-scale hierarchical image database.\nIn 2009 IEEE conference on computer vision and pattern\nrecognition, 2009.\nDieleman, S.\nDiffusion is spectral autoregression,\n2024. URL https://sander.ai/2024/09/02/\nspectral-autoregression.html.\nEsser, P., Kulal, S., Blattmann, A., Entezari, R., M¨uller, J.,\nSaini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., et al.\nScaling rectified flow transformers for high-resolution\nimage synthesis. In Forty-first International Conference\non Machine Learning, 2024.\nEsteves, C., Suhail, M., and Makadia, A. Spectral image\ntokenizer. arXiv preprint arXiv:2412.09607, 2024.\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,\nWarde-Farley, D., Ozair, S., Courville, A., and Bengio,\nY. Generative adversarial nets. In Advances in Neural\nInformation Processing Systems (NeurIPS), 2014.\nGuo, Y., Yang, C., Rao, A., Wang, Y., Qiao, Y., Lin, D.,\nand Dai, B. Animatediff: Animate your personalized\ntext-to-image diffusion models without specific tuning.\narXiv preprint arXiv:2307.04725, 2023.\nHaCohen, Y., Chiprut, N., Brazowski, B., Shalem, D.,\nMoshe, D., Richardson, E., Levin, E., Shiran, G., Zabari,\nN., Gordon, O., et al. Ltx-video: Realtime video latent\ndiffusion. arXiv preprint arXiv:2501.00103, 2024.\nHansen-Estruch, P., Yan, D., Chung, C.-Y., Zohar, O., Wang,\nJ., Hou, T., Xu, T., Vishwanath, S., Vajda, P., and Chen, X.\nLearnings from scaling visual tokenizers for reconstruc-\ntion and generation. arXiv preprint arXiv:2501.09755,\n2025.\nHeusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and\nHochreiter, S. Gans trained by a two time-scale update\nrule converge to a local nash equilibrium. In Advances in\nNeural Information Processing Systems (NeurIPS), 2017.\nHo, J. and Salimans, T. Classifier-free diffusion guidance.\narXiv preprint arXiv:2207.12598, 2022.\nHo, J., Jain, A., and Abbeel, P. Denoising diffusion prob-\nabilistic models.\nIn Advances in Neural Information\nProcessing Systems (NeurIPS), 2020.\nHo, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko,\nA., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D. J.,\net al. Imagen video: High definition video generation\nwith diffusion models. arXiv preprint arXiv:2210.02303,\n2022.\nHong, W., Ding, M., Zheng, W., Liu, X., and Tang, J.\nCogvideo: Large-scale pretraining for text-to-video gener-\nation via transformers. arXiv preprint arXiv:2205.15868,\n2022.\nJabri, A., Fleet, D. J., and Chen, T.\nScalable adaptive\ncomputation for iterative generation. In Proceedings of\nthe 40th International Conference on Machine Learning,\nICML’23. JMLR.org, 2023.\nKarras, T., Laine, S., and Aila, T. A style-based genera-\ntor architecture for generative adversarial networks. In\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2019.\nKarras, T., Aittala, M., Aila, T., and Laine, S. Elucidating\nthe design space of diffusion-based generative models.\nAdvances in Neural Information Processing Systems, 35:\n26565–26577, 2022.\nKarras, T., Aittala, M., Lehtinen, J., Hellsten, J., Aila, T.,\nand Laine, S.\nAnalyzing and improving the training\ndynamics of diffusion models. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 24174–24184, 2024.\nKingma, D. P. and Gao, R. Understanding the diffusion\nobjective as a weighted integral of elbos. arXiv preprint\narXiv:2303.00848, 2023.\nKingma, D. P. and Welling, M. Auto-encoding variational\nbayes. arXiv preprint arXiv:1312.6114, 2013.\nKong, W., Tian, Q., Zhang, Z., Min, R., Dai, Z., Zhou, J.,\nXiong, J., Li, X., Wu, B., Zhang, J., et al. Hunyuan-\nvideo: A systematic framework for large video generative\nmodels. arXiv preprint arXiv:2412.03603, 2024.\nKouzelis, T., Ioannis, K., Spyros, G., and Nikos, K. Eq-\nvae: Equivariance regularized latent space for improved\ngenerative image modeling. In arxiv, 2025.\n10\n\nImproving the Diffusability of Autoencoders\nKynk¨a¨anniemi, T., Aittala, M., Karras, T., Laine, S., Aila, T.,\nand Lehtinen, J. Applying guidance in a limited interval\nimproves sample and distribution quality in diffusion\nmodels. arXiv preprint arXiv:2404.07724, 2024.\nLi, J., Li, B., and Lu, Y. Deep contextual video compression.\nAdvances in Neural Information Processing Systems, 34:\n18114–18125, 2021.\nLi, J., Li, B., and Lu, Y. Neural video compression with\ndiverse contexts. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pp.\n22616–22626, 2023.\nLin, B., Ge, Y., Cheng, X., Li, Z., Zhu, B., Wang, S.,\nHe, X., Ye, Y., Yuan, S., Chen, L., et al. Open-sora\nplan: Open-source large video generation model. arXiv\npreprint arXiv:2412.00131, 2024.\nLiu, X., Gong, C., and Liu, Q.\nFlow straight and fast:\nLearning to generate and transfer data with rectified flow.\narXiv preprint arXiv:2209.03003, 2022.\nLiu, X., Zhang, X., Ma, J., Peng, J., and Liu, Q. Instaflow:\nOne step is enough for high-quality diffusion-based text-\nto-image generation, 2024. URL https://arxiv.\norg/abs/2309.06380.\nLoshchilov, I. Decoupled weight decay regularization. arXiv\npreprint arXiv:1711.05101, 2017.\nMentzer, F., Toderici, G., Minnen, D., Hwang, S.-J., Caelles,\nS., Lucic, M., and Agustsson, E. Vct: A video com-\npression transformer. arXiv preprint arXiv:2206.07307,\n2022.\nMinnen, D., Ball´e, J., and Toderici, G. D. Joint autoregres-\nsive and hierarchical priors for learned image compres-\nsion. Advances in neural information processing systems,\n31, 2018.\nNichol, A. Q. and Dhariwal, P.\nImproved denois-\ning diffusion probabilistic models.\nIn International\nConference on Machine Learning, pp. 8162–8171,\n2021. URL https://proceedings.mlr.press/\nv139/nichol21a.html.\nNing, M., Li, M., Su, J., Jia, H., Liu, L., Beneˇs, M., Salah,\nA. A., and Ertugrul, I. O. Dctdiff: Intriguing properties\nof image generative modeling in the dct space. arXiv\npreprint arXiv:2412.15032, 2024.\nOquab, M., Darcet, T., Moutakanni, T., Vo, H. V.,\nSzafraniec, M., Khalidov, V., Fernandez, P., Haziza, D.,\nMassa, F., El-Nouby, A., Howes, R., Huang, P.-Y., Xu,\nH., Sharma, V., Li, S.-W., Galuba, W., Rabbat, M., As-\nsran, M., Ballas, N., Synnaeve, G., Misra, I., Jegou, H.,\nMairal, J., Labatut, P., Joulin, A., and Bojanowski, P.\nDinov2: Learning robust visual features without supervi-\nsion, 2023.\nPeebles, W. and Xie, S. Scalable diffusion models with\ntransformers. arXiv preprint arXiv:2212.09748, 2022.\nPodell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn,\nT., M¨uller, J., Penna, J., and Rombach, R. Sdxl: Im-\nproving latent diffusion models for high-resolution image\nsynthesis. arXiv preprint arXiv:2307.01952, 2023.\nPolyak, A., Zohar, A., Brown, A., Tjandra, A., Sinha, A.,\nLee, A., Vyas, A., Shi, B., Ma, C.-Y., Chuang, C.-Y., et al.\nMovie gen: A cast of media foundation models. arXiv\npreprint arXiv:2410.13720, 2024.\nRissanen, S., Heinonen, M., and Solin, A. Generative mod-\nelling with inverse heat dissipation.\nIn The Eleventh\nInternational Conference on Learning Representations,\n2023. URL https://openreview.net/forum?\nid=4PJUBT9f2Ol.\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and\nOmmer, B. High-resolution image synthesis with latent\ndiffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pp.\n10684–10695, 2022.\nRuderman, D. L. Origins of scaling in natural images. Vision\nResearch, 37(23):3385–3398, 1997. ISSN 0042-6989.\ndoi:\nhttps://doi.org/10.1016/S0042-6989(97)00008-4.\nURL\nhttps://www.sciencedirect.com/\nscience/article/pii/S0042698997000084.\nSadat, S., Buhmann, J., Bradley, D., Hilliges, O., and Weber,\nR. M.\nLitevae: Lightweight and efficient variational\nautoencoders for latent diffusion models. arXiv preprint\narXiv:2405.14477, 2024.\nSaharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton,\nE. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan,\nB., Salimans, T., et al. Photorealistic text-to-image dif-\nfusion models with deep language understanding. Ad-\nvances in neural information processing systems, 35:\n36479–36494, 2022.\nSheng, X., Li, J., Li, B., Li, L., Liu, D., and Lu, Y. Temporal\ncontext mining for learned video compression. IEEE\nTransactions on Multimedia, 25:7311–7322, 2022.\nSong, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A.,\nErmon, S., and Poole, B. Score-based generative model-\ning through stochastic differential equations, 2021. URL\nhttps://arxiv.org/abs/2011.13456.\nStein, G., Cresswell, J., Hosseinzadeh, R., Sui, Y., Ross, B.,\nVillecroze, V., Liu, Z., Caterini, A. L., Taylor, E., and\nLoaiza-Ganem, G. Exposing flaws of generative model\n11\n\nImproving the Diffusability of Autoencoders\nevaluation metrics and their unfair treatment of diffusion\nmodels.\nAdvances in Neural Information Processing\nSystems, 36, 2024.\nSu, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y.\nRoformer: Enhanced transformer with rotary position\nembedding. Neurocomputing, 568:127063, 2024.\nTang, A., He, T., Guo, J., Cheng, X., Song, L., and Bian,\nJ. Vidtok: A versatile and open-source video tokenizer.\narXiv preprint arXiv:2412.13061, 2024.\nTian, R., Dai, Q., Bao, J., Qiu, K., Yang, Y., Luo, C., Wu, Z.,\nand Jiang, Y.-G. Reducio! generating 1024x1024 video\nwithin 16 seconds using extremely compressed motion\nlatents. arXiv preprint arXiv:2411.13552, 2024.\nUnterthiner, T., van Steenkiste, S., Kurach, K., Marinier, R.,\nMichalski, M., and Gelly, S. Towards accurate generative\nmodels of video: A new metric & challenges. arXiv\npreprint arXiv:1812.01717, 2018.\nVahdat, A., Kreis, K., and Kautz, J. Score-based gener-\native modeling in latent space. 2021.\narXiv preprint\narXiv:2106.05931, 2021.\nvon Platen, P., Patil, S., Lozhkov, A., Cuenca, P., Lam-\nbert, N., Rasul, K., Davaadorj, M., Nair, D., Paul, S.,\nBerman, W., Xu, Y., Liu, S., and Wolf, T. Diffusers:\nState-of-the-art diffusion models. https://github.\ncom/huggingface/diffusers, 2022.\nWallace, G. K. The jpeg still picture compression standard.\nCommunications of the ACM, 34(4):30–44, 1991.\nWang, J., Jiang, Y., Yuan, Z., Peng, B., Wu, Z., and Jiang,\nY.-G. Omnitokenizer: A joint image-video tokenizer\nfor visual generation. arXiv preprint arXiv:2406.09399,\n2024.\nWu, P., Zhu, K., Liu, Y., Zhao, L., Zhai, W., Cao, Y., and\nZha, Z.-J. Improved video vae for latent video diffusion\nmodel. arXiv preprint arXiv:2411.06449, 2024.\nXie, E., Chen, J., Chen, J., Cai, H., Tang, H., Lin, Y., Zhang,\nZ., Li, M., Zhu, L., Lu, Y., et al. Sana: Efficient high-\nresolution image synthesis with linear diffusion trans-\nformers. arXiv preprint arXiv:2410.10629, 2024.\nXing, Y., Fei, Y., He, Y., Chen, J., Xie, J., Chi, X., and Chen,\nQ. Large motion video autoencoding with cross-modal\nvideo vae. arXiv preprint arXiv:2412.17805, 2024.\nYang, Z., Teng, J., Zheng, W., Ding, M., Huang, S., Xu,\nJ., Yang, Y., Hong, W., Zhang, X., Feng, G., et al.\nCogvideox: Text-to-video diffusion models with an ex-\npert transformer. arXiv preprint arXiv:2408.06072, 2024.\nYu, L., Lezama, J., Gundavarapu, N. B., Versari, L., Sohn,\nK., Minnen, D., Cheng, Y., Gupta, A., Gu, X., Haupt-\nmann, A. G., et al. Language model beats diffusion–\ntokenizer is key to visual generation.\narXiv preprint\narXiv:2310.05737, 2023.\nZhang, R., Isola, P., Efros, A. A., Shechtman, E., and Wang,\nO. The unreasonable effectiveness of deep features as a\nperceptual metric. In CVPR, 2018.\nZhao, S., Zhang, Y., Cun, X., Yang, S., Niu, M., Li, X.,\nHu, W., and Shan, Y.\nCv-vae: A compatible video\nvae for latent generative video models. arXiv preprint\narXiv:2405.20279, 2024.\nZhao, Y., Gu, A., Varma, R., Luo, L., Huang, C.-C., Xu, M.,\nWright, L., Shojanazeri, H., Ott, M., Shleifer, S., et al.\nPytorch fsdp: experiences on scaling fully sharded data\nparallel. arXiv preprint arXiv:2304.11277, 2023.\nZheng, Z., Peng, X., Yang, T., Shen, C., Li, S., Liu, H.,\nZhou, Y., Li, T., and You, Y. Open-sora: Democratizing\nefficient video production for all, 2024. URL https:\n//github.com/hpcaitech/Open-Sora.\nZhou, Y., Wang, Q., Cai, Y., and Yang, H. Allegro: Open the\nblack box of commercial-level video generation model.\narXiv preprint arXiv:2410.15458, 2024.\n12\n\nImproving the Diffusability of Autoencoders\nA. Implementation Details\nDiT model details. To strengthen the baseline DiT performance, we integrated into it the latest advancements from the\ndiffusion model literature. Namely, we used self conditioning (Jabri et al., 2023) and RoPE (Su et al., 2024) positional\nembeddings. Besides, we switched to the rectified flow diffusion parametrization (Albergo & Vanden-Eijnden, 2022; Liu\net al., 2022; Vahdat et al., 2021), which was recently shown to have better scalability with a fewer amount of inference\nsteps (Esser et al., 2024).\nDiT training details. All the DiT models are trained for 400,000 steps with 10,000 warmup steps of the learning rate from\n0 to 0.0003 and then its gradual decay towards 0.00001. We used weight decay of 0.01 and AdamW (Loshchilov, 2017)\noptimizer with beta coefficients of 0.9 and 0.99. We used posterior sampling from the encoder distribution for VAE-based\nautoencoders. In contrast to the original work, we found it helpful to do learning rate decay to 0.00001 using the cosine\nlearning rate schedule. We used the same model sizes for DiT-S (small), DiT-B (base), DiT-L (large) and DiT-XL (extra\nlarge), as the original work (Peebles & Xie, 2022):\n• DiT-S: hidden dimensionality of 384, 12 transformer blocks, and 6 attention heads in the multi-head attention.\n• DiT-B: hidden dimensionality of 768, 12 transformer blocks, and 12 attention heads in the multi-head attention.\n• DiT-L: hidden dimensionality of 1024, 24 transformer blocks, and 16 attention heads in the multi-head attention.\n• DiT-XL: hidden dimensionality of 1152, 28 transformer blocks, and 16 attention heads in the multi-head attention.\nWe used gradient clipping with the norm of 16 for all the DiT models. Our models were trained in the FSDP (Zhao et al.,\n2023) framework with the full sharding strategy on a single node of 8× NVidia A100 80GB GPUs or 8× NVidia H100\n80GB GPUs (depending on their availability in our computational cluster).\nFor CV-AE, since it is considerably slower than other autoencoders, we trained LDMs on pre-extracted latents. For this, we\npre-extracted them on random 17-frames clips. In essence, this reduces\nAutoencoders training details. Since none of the autoencoders had their training pipelines released, we had to develop\nthe training recipes for each of the autoencoder baselines individually which would not be detrimental to neither their\nreconstruction capability nor downstream diffusion performance. To do this, we ablated multiple hyperparameters (the most\nimportant ones being learning rate and KL regularization strength) to arrive to a proper setup. We chose the KL weight in\nsuch a way that the KL penalty maintains approximately the same magnitude as the pre-trained checkpoint.\nEach autoencoder is trained with AdamW (Loshchilov, 2017) optimizer, with betas of 0.9 and 0.99, and weight decay of\n0.01. The learning rate was grid-searched individually for each autoencoder and is provided in Table 5. In all the cases, we\nused mixed precision training with BFloat16.\nDuring training, we maintained an exponential moving average of the weights (Karras et al., 2024), initialized from the\nsame parameters as the starting model, and having a half life of 5,000 steps.\nWe emphasize that, when applying our regularization strategy on top of an autoencoder baseilne, we do not alter other\nhyperparameters (like learning rate), except for KL regularization which we disable for SE-regularized models (even though\nwe found it helpful in some of our explorations).\nFor each autoencoder, we freeze the last output layers of the decoder. The motivation is the following: they were fine-tuned\nwith the adversarial loss, which we want to exclude from the equation without hurting the ability of an autoencoder to\nmodel textural details which FID would be sensitive to (Rombach et al., 2022) and which do not influence the latent space\nproperties. Namely, we freeze the last normalization and output convolution layers. In each case, the amount of frozen\nparameters constitute a negligible amount of total parameters.\nOther hyperparameters for autoencoders training are provided in Table 5.\n13\n\nImproving the Diffusability of Autoencoders\nTable 5. Hyperparameters for the autoencoders explored in the current work. We had to tweak the hyperparameters for various autoencoders\nto prevent the divergence of the baseline training.\nHyperparameter\nFluxAE\nCMS-AEI\nCV-AE\nLTX-AE\nDomain\nimage\nimage\nvideo\nvideo\nCompression rate\n8 × 8\n16 × 16\n4 × 8 × 8\n8 × 32 × 32\nLatent channels\n16\n16\n16\n32\nNumber of fune-tuning steps\n10,000\n10,000\n20,000\n20,000\nImage batch size\n32\n32\n64\n64\nVideo batch size\n0\n0\n32\n32\nDefault KL β weight\n0.001\n0.0\n0.001\n0.0001\nLearning rate\n0.00001\n0.0001\n0.0003\n0.00005\nNumber of parameters\n83.8M\n44M\n211.5M\n419M\nTraining resolution\n256 × 256\n256 × 256\n17 × 256 × 256\n17 × 256 × 256\nMSE loss weight\n1\n1\n1\n5\nLPIPS loss weight\n1.0\n1.0\n1.0\n1.0\nGradient clipping norm\n50\n50\n1\n50\nNum upsampling blocks frozen\n1\n3\n0\n0\nIs output convolution frozen?\nYes\nYes\nYes\nYes\n14\n\nImproving the Diffusability of Autoencoders\nB. Additional Exploration\nIn Section 3, we outlined the base scale equivariance strategy to regularize the spectrum of an autoencoder which has\na strong advantage of being very easy to implement by a practitioner. However, it could be beneficial to possess more\nadvanced tools for a finer-grained control over the latent space spectral properties. This section outlines them and provides\nthe corresponding ablation.\nB.1. Explicitly Chopping off High-Frequency Components\nRather than applying downsampling to produce latents and RGB targets for regualrization, it is possible to replace some\nratio of high-frequency components with zeros. To do so, DCT is applied to the latents and RGB targets where a chosen\nset of frequency components are masked out. The modified components are then translated back to the spatial domain by\ninverse DCT to form the training latents and reconstruction targets.\nLCHF(x) = d(x, Dec(z)) + d(D−1(D(x) ∗M), Dec(D−1(D(z) ∗M)) + Lreg,\n(3)\nwhere D and D−1 represent DCT and its inverse, respectively. M is a B × B binary mask indicating which frequencies to\nzero out defined as follows:\nM(u, v) =\n(\n1,\nif zigzag(u, v) < B2 −N,\n0,\notherwise.\n(4)\nN controls the frequency cutoff. We provide the ablation for this strategy in Table 6.\nTable 6. Ablations for explicit high-frequency chop off for DiT-S/2 trained for 200,000 iterations on top of Flux AE with such a\nregularization. While it can achieve better results for some of the baselines than naive downsampling, we opt out for the latter strategy due\nto its simplicity. For the non-zigzag order ablation, we cut across each x and y axes independently\nStage II\nStage I\nFDD5K\nDiT-S/2\nFluxAE + chop off 90% (non-zigzag order)\n912.4\nDiT-S/2\nFluxAE + chop off 70% (non-zigzag order)\n915.6\nDiT-S/2\nFluxAE + chop off 30% (non-zigzag order)\n929.7\nDiT-S/2\nFluxAE + chop off 10% (non-zigzag order)\n916.5\nDiT-S/2\nFluxAE + chop off 90% (zigzag order)\n935.5\nDiT-S/2\nFluxAE + chop off 70% (zigzag order)\n932.8\nDiT-S/2\nFluxAE + chop off 30% (zigzag order)\n962.9\nDiT-S/2\nFluxAE + chop off 10% (zigzag order)\n930.1\nDiT-S/2\nFluxAE (vanilla)\n992.0\nDiT-S/2\nFluxAE with optimal (out of 8) KL β\n929.6\nIn Figure 5, we provided the visualizations for a FluxAE resiliense with and without such chopping high-frequency\nregularization for 50% HF dropout rate. In Figure 10, we provide an equivalent visualization for SE-fine-tuned FluxAE:\nwhile it is less resilient to frequency dropout than CHF, but is still noticeably better than the vanilla model.\nB.2. Soft Penalty for High-Frequency Components\nInstead of directly removing some of the components, which might become a too strict regularization signal, one can\nconsider penalizing the amplitudes of high-frequency components in a soft manner. Concretely, given a B × B block, we\nconstruct the following weight penalty matrix:\nWuv = (u + v)p/Bp.\n(5)\nNext, the soft regularization loss itself is computed as:\nLsoftreg =\nX\nu,v\nDuv(z) · Wuv.\n(6)\n15\n\nImproving the Diffusability of Autoencoders\n0%\n25%\n50%\n75%\nFluxAE\nFluxAE + SE\nFigure 10. RGB and FluxAE reconstruction with/without scale equivariance regularization for different percentages of chopped-off high\nfrequency components.\nDuring training, when enabled, we add it to the main loss with the weigh γ. We found it beneficial in some of our experiments\nwhen it is added with a small coefficient (e.g., 0.01). While it is possible to achieve higher results with more fine-grained\nregularization, we opt to use the simpler version since we believe it would be easier to employ by the community.\nTo ablate its importance, we trained DiT-B/1 model on top of FluxAE models, fine-tuned with a different strength γ. The\nresults are presented in Table 7.\nTable 7. Ablating the regularization strength α of our proposed scale equivariance regularization.\nStage II\nStage I\nFID 5k\nFDD 5k\nDiT-B/1\nFluxAE + FT-SE γ = 0.001\n26.43\n497.14\nFluxAE + FT-SE γ = 0.025\n25.46\n477.61\nFluxAE + FT-SE γ = 0.01\n26.72\n487.06\nFluxAE + FT-SE γ = 0.05\n24.28\n458.11\nFluxAE + FT-SE γ = 0.1\n25.84\n461.97\nB.3. ImageNet 5122 experiments\nWe trained our DiT-L/2 for class-conditional 5122 ImageNet-1K generation for 400K steps for FluxAE (Black Forest Labs,\n2023), the results are presented in Table 8.\nB.4. Ablating regularization strength α\nTo ablate the importance of the regularization strength α, we train FluxAE for 10,000 steps with a varying strength. The\nresults are presented in Table 9.\n16\n\nImproving the Diffusability of Autoencoders\n0\n1\n2\n3\n4\n5\n6\n7\n0\n1\n2\n3\n4\n5\n6\n7\nFigure 11. Illustration of the zigzag indexing order of DCT.\nTable 8. Class-conditional generation results on ImageNet-1K 5122 without guidance. The original DiT paper reports the results after 3M\ntraining steps, while we use 400K steps for our models.\nStage II\nStage I\nFID\nFDD\nDiT-L/2\nFluxAE (vanilla)\n13.13\n249.4\nFluxAE + FT\n13.69\n267.7\nFluxAE + FT-SE (ours)\n11.63\n203.5\nDiT-XL/2 (orig) + 3M steps\nSD-VAE-ft-MSE\n12.03\n−\nTable 9. Ablating the regularization strength α of our proposed scale equivariance regularization.\nStage II\nStage I\nFID 5k\nFDD 5k\nDiT-B/2\nFluxAE + FT-SE α = 0.01\n33.99\n641.95\nFluxAE + FT-SE α = 0.05\n33.86\n645.94\nFluxAE + FT-SE α = 0.1\n28.62\n586.91\nFluxAE + FT-SE α = 0.25\n26.84\n558.36\nFluxAE + FT-SE α = 0.5\n29.63\n569.92\nFluxAE + FT-SE α = 1\n33.22\n612.45\n17\n\nImproving the Diffusability of Autoencoders\nC. Additional visualizations\nThis section provides additional visualizations for the LDM experiments.\nFigure 12. Uncurated samples from DiT-XL/2 for FluxAE (top), FluxAE + FT (middle) and FluxAE + SE (bottom) on class-conditional\nImageNet 256 × 256 for random classes. During inference, we used 256 steps with the guidance scale of 3.0.\n18\n\nImproving the Diffusability of Autoencoders\nFigure 13. Uncurated samples from DiT-XL/2 trained for 1M steps on top FluxAE + SE (bottom) on class-conditional ImageNet 256×256\nfor random classes. During inference, we used 256 steps with the guidance scale of 3.0.\nFigure 14. Uncurated samples from DiT-XL/2 trained for 1M steps on top FluxAE + SE (bottom) on class-conditional ImageNet 256×256\nfor class 88. During inference, we used 256 steps with the guidance scale of 3.0.\nFigure 15. Uncurated samples from DiT-XL/2 trained for 1M steps on top FluxAE + SE (bottom) on class-conditional ImageNet 256×256\nfor class 130. During inference, we used 256 steps with the guidance scale of 3.0.\n19\n\nImproving the Diffusability of Autoencoders\nFigure 16. Uncurated samples from DiT-XL/2 trained for 1M steps on top FluxAE + SE (bottom) on class-conditional ImageNet 256×256\nfor class 279. During inference, we used 256 steps with the guidance scale of 3.0.\nFigure 17. Uncurated samples from DiT-XL/2 trained for 1M steps on top FluxAE + SE (bottom) on class-conditional ImageNet 256×256\nfor class 555. During inference, we used 256 steps with the guidance scale of 3.0.\n20\n\nImproving the Diffusability of Autoencoders\nFigure 18. Uncurated samples from DiT-XL/2 trained for 400K steps on top FluxAE + SE (bottom) on class-conditional ImageNet\n512 × 512 for random classes. During inference, we used 256 steps with the guidance scale of 3.0.\n21\n\nImproving the Diffusability of Autoencoders\nFigure 19. Uncurated samples from DiT-B/1 for CMS-AEI (top), CMS-AEI + FT (middle) and CMS-AEI + SE (bottom) on class-\nconditional ImageNet 256 × 256. During inference, we used 256 steps with the guidance scale of 1.5.\n22\n\nImproving the Diffusability of Autoencoders\nFigure 20. Uncurated samples from DiT-XL/2 for CogVideoX-AE (top), CogVideoX-AE + FT (middle) and CogVideoX-AE + SE\n(bottom) on class-conditional Kinetics 17 × 256 × 256. During inference, we used 256 steps with the guidance scale of 3.0.\n23\n\nImproving the Diffusability of Autoencoders\nFigure 21. Uncurated samples from DiT-B/2 for CogVideoX-AE (top), CogVideoX-AE + FT (middle) and CogVideoX-AE + SE (bottom)\non class-conditional Kinetics 17 × 256 × 256. During inference, we used 256 steps with the guidance scale of 3.0.\n24\n\nImproving the Diffusability of Autoencoders\nFigure 22. Uncurated samples from DiT-B/1 for LTX-AE (top), LTX-AE + FT (middle) and LTX-AE + SE (bottom) on class-conditional\nKinetics 17 × 256 × 256. During inference, we used 256 steps with the guidance scale of 3.0.\n25\n\nImproving the Diffusability of Autoencoders\nD. Limitations.\nWe identify the following limitations of our work and the proposed regularization:\n1. While we did our best to verify that our framework works in the most general setup possible, testing 4 different\nautoencoders across 2 different domains (image and videos), our study would be more complete when verified across\nother diffusion parametrizations (Karras et al., 2022; Ho et al., 2020; Kingma & Gao, 2023) or architectures (Karras\net al., 2024).\n2. We observed that our regularization still affects the reconstruction slightly: for example, Table 4 shows that FluxAE\nFID increased from 0.183 to 0.55 (though for some AEs, like CogVideoX-AE, it improves). We are convinced that this\nFID increase could be mitigated by training with adversarial losses, which we omitted in this work for simplicity.\n3. There is a mild sensitivity to hyperparameters: for example, we found that varying the SHF regularization weight\nmight improve the results (see Table 9), or adding a small KL regularization (which we disabled in the end for our\nregularization for simplicity).\n4. None of the explored autoencoders released their training pipelines, and it is non-trivial to fine-tune them even without\nany extra regularization. For example, we observed that any fine-tuning of DC-AE (Chen et al., 2025) was leading to\ndivergent reconstructions in our training pipeline (we explored dozens of different hyperparameter setups).\nWe leave the exploration of these limitations for future work.\n26\n",
  "metadata": {
    "source_path": "papers/arxiv/Improving_the_Diffusability_of_Autoencoders_f5ae4ffcb860ef50.pdf",
    "content_hash": "f5ae4ffcb860ef501a5cc0233fcee9873e7ce763fa69a816aea3718e79c572b1",
    "arxiv_id": null,
    "title": "Improving the Diffusability of Autoencoders",
    "author": "Ivan Skorokhodov, Sharath Girish, Benran Hu, Willi Menapace, Yanyu Li, Rameen Abdal, Sergey Tulyakov, Aliaksandr Siarohin",
    "creation_date": "D:20250221020412Z",
    "published": "2025-02-21T02:04:12",
    "pages": 26,
    "size": 29978674,
    "file_mtime": 1740346983.5698595
  }
}