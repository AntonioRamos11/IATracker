{
  "text": "Zero-shot Load Forecasting for Integrated Energy\nSystems: A Large Language Model-based Framework\nwith Multi-task Learning\nJiaheng Lia, Donghe Lia, Ye Yangb, Huan Xic, Yu Xiaod, Li Sune, Dou Ana,\nQingyu Yanga,f\naSchool of Automation Science and Engineering, Xi’an Jiaotong\nUniversity, , Shaanxi, 710049, China\nbSchool of Electronic Information Engineering, Shanghai Dianji University, 300 Shuihua\nRoad, Pudong New District, Shanghai, 201306, China\ncKey Laboratory of Thermo-Fluid Science and Engineering of Ministry of Education,\nSchool of Energy and Power Engineering, Xi’an Jiaotong\nUniversity, , Shaanxi, 710049, China\ndPostdoc in Electrical Engineering, Eindhoven University of\nTechnology, , Eindhoven, 5600 MB, the Netherlands\neSchool of Microelectronics, Xi’an Jiaotong University, , Shaanxi, 710049, China\nfState Key Laboratory for Manufacturing System Engineering Lab, Xi’an Jiaotong\nUniversity, , Shaanxi, 710049, China\nAbstract\nThe growing penetration of renewable energy sources in power systems has\nincreased the complexity and uncertainty of load forecasting, especially for\nintegrated energy systems with multiple energy carriers. Traditional fore-\ncasting methods heavily rely on historical data and exhibit limited trans-\nferability across different scenarios, posing significant challenges for emerg-\ning applications in smart grids and energy internet. This paper proposes\nthe TSLLM-Load Forecasting Mechanism, a novel zero-shot load forecasting\nframework based on large language models (LLMs) to address these chal-\nlenges. The framework consists of three key components: a data preprocess-\ning module that handles multi-source energy load data, a time series prompt\ngeneration module that bridges the semantic gap between energy data and\nLLMs through multi-task learning and similarity alignment, and a predic-\ntion module that leverages pre-trained LLMs for accurate forecasting. The\nframework’s effectiveness was validated on a real-world dataset comprising\nload profiles from 20 Australian solar-powered households, demonstrating\nPreprint submitted to arxiv\nFebruary 25, 2025\narXiv:2502.16896v1  [cs.LG]  24 Feb 2025\n\nsuperior performance in both conventional and zero-shot scenarios. In con-\nventional testing, our method achieved a Mean Squared Error (MSE) of\n0.4163 and a Mean Absolute Error (MAE) of 0.3760, outperforming exist-\ning approaches by at least 8%. In zero-shot prediction experiments across\n19 households, the framework maintained consistent accuracy with a total\nMSE of 11.2712 and MAE of 7.6709, showing at least 12% improvement over\ncurrent methods. The results validate the framework’s potential for accurate\nand transferable load forecasting in integrated energy systems, particularly\nbeneficial for renewable energy integration and smart grid applications.\nKeywords:\nZero-shot forecasting, Large language models (LLMs), Time\nseries prompt generation, Multi-task learning, Similarity alignment\n1. Introduction\nThe growing penetration of renewable energy generation has led to signif-\nicant challenges for power systems, particularly in terms of system dispatch\nand balance. The inherent variability, intermittency, and unpredictability of\nrenewable energy sources make traditional power system management tech-\nniques increasingly inadequate.\nIn the development of integrated energy\nsystems that incorporate multiple energy forms, the effective management\nof multi-energy flexible loads—such as electricity, heating/cooling, and nat-\nural gas at the consumer end—has emerged as a key strategy for mitigating\nrenewable energy instability at the production end[1, 2, 3, 4].\nLoad forecasting plays a crucial role in energy management systems, fa-\ncilitating power system planning and operational management. It helps pre-\ndict future electricity demand, ensuring a balance between power supply\nand demand. This balance is essential for the efficient distribution and uti-\nlization of energy within the energy internet framework. As global energy\nstructures evolve and integrated energy system dispatching advances, load\nforecasting technologies have gained significant attention. These technolo-\ngies have reached a mature stage, encompassing various forecasting mod-\nels and techniques, such as regression models, time series models[5, 6, 7],\ndeep learning models[8, 9, 10, 11, 12, 13, 14], and artificial intelligence (AI)\nmodels[15, 16, 17, 18, 19]. Accurate load forecasting is critical for ensuring\ngrid stability, optimizing resource allocation, promoting energy complemen-\ntarity, and advancing the construction of the energy internet and integrated\n2\n\nFigure 1: Comparison of load forecasting schemes: (a) Traditional machine learning meth-\nods and (b) Large language model-based approach. The traditional method requires ex-\ntensive historical data and exhibits limited transferability, while the LLM-based method\nenables zero-shot prediction across different data features.\nenergy systems. Additionally, it plays a pivotal role in realizing the develop-\nment of the Internet of Things (IoT) across the power industry[20].\nWith the rapid advancements in artificial intelligence, time series fore-\ncasting—a major branch of AI—has been widely applied in load forecasting.\nFor example, several studies have explored the use of convolutional neural\nnetworks (CNN) and long short-term memory (LSTM) networks for short-\nterm load forecasting in cogeneration power systems[21, 22]. Temporal Con-\nvolutional Networks (TCN) combined with LSTM models have also been\nsuccessfully applied for extracting temporal features from raw load data,\nshowing improved forecasting accuracy[23]. Despite the success of these tra-\nditional methods, deep learning models typically require large amounts of\nhistorical data for feature extraction and model training, which limits their\n3\n\neffectiveness in cold-start scenarios where limited historical data is available.\nGiven the data dependency of traditional deep learning algorithms in\nload forecasting, research has increasingly focused on developing forecasting\nmodels that perform well under few-shot or zero-shot conditions. The emer-\ngence of large language models (LLMs) has provided a promising solution to\nthis challenge. LLMs, such as GPT-2 with 1.5 billion parameters, are deep\nlearning models trained on vast amounts of text data and capable of perform-\ning a wide range of natural language processing (NLP) tasks, including text\ngeneration, translation, question answering, and sentiment analysis[24, 25].\nTheir strong performance across general tasks has led to their exploration\nin time series forecasting tasks[26, 27, 28]. For example, experiments have\nshown that pre-trained LLMs can function effectively as zero-shot time se-\nries forecasters[29].\nBeyond simply using LLMs as tools for time series\nforecasting[30, 31], researchers have focused on methods that enable LLMs\nto understand time series data. These include techniques for encoding time\nseries data[32] and aligning it within a semantic space[33, 34, 35, 36, 37, 38].\nFurther studies have employed multimodal large models for time series fore-\ncasting, including methods that transform time windows into images and use\nvisual encoders to map these images into a form understandable by LLMs[39].\nAdditionally, other research efforts have aimed at translating time series data\ninto a text format suitable for LLMs, which has significantly improved the\naccuracy of forecasting models[40].\nIn this context, this paper proposes the TSLLM-Load Forecasting Mecha-\nnism, a novel zero-shot load forecasting framework based on LLMs, aimed at\naddressing the challenges of transferability and the complexities associated\nwith understanding time series data. Specifically, this approach tackles three\nkey issues: the variability in household characteristics, the transformation\nof load data into a textual format that can be processed by LLMs, and the\ninterdependencies among multiple features within the data.\nThe contributions of this paper are as follows:\n• A Transferable Zero-shot Time Series Forecasting Framework\nBased on LLMs: We introduce the TSLLM-Load Forecasting Mech-\nanism, a comprehensive framework for household electricity load fore-\ncasting centered on LLMs. This framework includes a data preprocess-\ning module, a time series prompt generation module, and a forecasting\nmodule based on pre-trained LLMs. The data preprocessing module\nprocesses the data, while the time series prompt generation module\n4\n\nconverts the data into a format understandable by LLMs. The pre-\ntrained LLM is then used to generate accurate forecasts, which are\nreconstructed into time series data through the model’s output layer.\n• Optimization of Time Series Forecasting for LLMs: In the pre-\nprocessing module, we clean, interpolate, and normalize the data from\nintegrated energy households. The time series prompt generation mod-\nule employs a multi-task learning mechanism to capture correlations\nbetween various features of the time series data. The data undergoes\ntrend-seasonal decomposition (SFT) before being segmented to accom-\nmodate the LLM’s context length limitations. Each feature’s data block\nis projected into a text domain understood by the LLM, and similarity\nmatching is employed for text alignment. The aligned text serves as\nthe prefix prompt for the time series data, which is then input into the\nLLM for forecasting. During the pre-training phase, a small subset of\ndata is used to train certain parameters of the LLM. The full set of\nelectricity household data is used to validate the model’s performance.\n• Extensive Experimental Validation: We evaluate the performance\nof the TSLLM-Load Forecasting Mechanism using real-world data, in-\ncluding a dataset of 20 Australian solar-powered households’ electric-\nity load data. In conventional testing, our method achieved a Mean\nSquared Error (MSE) of 0.4163 and a Mean Absolute Error (MAE) of\n0.3760, outperforming other popular methods by at least 8%. In zero-\nshot prediction experiments, the framework maintained consistent ac-\ncuracy across 19 households, with a total MSE of 11.2712 and MAE of\n7.6709, representing at least a 12% improvement over existing methods.\nThese results demonstrate the effectiveness of the proposed framework\nfor accurate and transferable load forecasting, particularly in the con-\ntext of renewable energy integration and smart grid applications.\nThe remainder of this paper is organized as follows: Section 2 presents the\nsystem model and problem formulation. Section 3 introduces the TSLLM-\nLoad Forecasting Mechanism and its components. Section 4 discusses the\nexperimental results, and Section 5 concludes the paper.\n2. System Model of Load Forecasting\nThis section establishes a comprehensive mathematical framework for en-\nergy load forecasting in integrated energy systems. We first formalize the\n5\n\nload forecasting problem for systems with multiple energy sources and then\noutline the key challenges in developing effective prediction models.\nIn modern integrated energy systems, multiple energy sources contribute\nto meeting diverse user demands. The Energy Management System (EMS) is\nresponsible for forecasting energy requirements to optimize resource utiliza-\ntion, minimize waste, and guide rational electricity consumption. Consider\na system with N users, denoted as U = {u1, u2, ..., uN}. For each user ui,\ntheir energy consumption data over a specific time period can be represented\nby a multi-dimensional time series Xf\ni , where f indicates different energy\nfeatures, and t represents the temporal dimension. For a given time interval\n∆t, the data for user ui can be expressed as:\nXi(∆t) = {Xhp\ni , Xap\ni , Xsp\ni } ∈RT×3\n(1)\nwhere:\n• Xhp\ni\n∈RT: Power consumption of high-power appliances\n• Xap\ni\n∈RT: Power consumption of other appliances\n• Xsp\ni ∈RT: Power generation from solar installations\nThe net energy consumption can be formulated as a temporal sequence:\nEi(∆t) = Xhp\ni + Xap\ni −Xsp\ni\n(2)\nFor a time interval ∆t containing T time steps, the temporal evolution of\nenergy consumption is represented as:\nEi(∆t) = {e1\ni , e2\ni , ..., eT\ni } ∈RT\n(3)\nThe complete energy consumption profile for all users during this period\nforms a multi-dimensional tensor:\nE(∆t) = {E1(∆t), E2(∆t), ..., EN(∆t)} ∈RN×T\n(4)\nE(∆t) = {et\ni|i ∈[1, N], t ∈[1, T]}\n(5)\nThe objective of load forecasting is to predict energy consumption ˆE(∆t+k)\nfor the next k time intervals:\n6\n\nˆE(∆t+k) = f(E(∆t)) + ϵ\n(6)\nwhere f(·) represents the forecasting function and ϵ denotes the prediction\nerror term. For individual users, this forecasting task can be expressed as:\nˆEi(∆t+k) = fi(Ei(∆t)) + ϵi\n(7)\nThis formulation presents several significant challenges:\n• Stochastic Nature: Both electrical appliance usage and solar power\ngeneration exhibit inherent randomness, characterized by time-varying\nprobability distributions P(Xt|Xt−1, ..., X1).\n• Heterogeneous Patterns: Energy consumption behaviors vary sig-\nnificantly across users due to local conditions, installed equipment,\nand individual habits, resulting in distinct data distributions Pi(X) ̸=\nPj(X) for i ̸= j.\n• Temporal Dynamics: Usage patterns and solar generation exhibit\nstrong temporal dependencies, while maintaining non-stationarity across\ndifferent time scales, expressed as E[Xt] ̸= E[Xt+τ] for some time lag τ.\nThese challenges necessitate a robust and adaptable forecasting frame-\nwork that can effectively handle diverse data patterns while maintaining\nprediction accuracy across different scenarios.\n3. TSLLM-Load Forecasting Mechanism\nThis section presents the proposed TSLLM-Load Forecasting Mechanism,\nencompassing a comprehensive framework that integrates data preprocess-\ning, prompt generation, and prediction capabilities. The following subsec-\ntions detail the design rationale, operational workflow, and mathematical\nfoundations of each functional module.\n3.1. Design Rationale\nBuilding upon the load prediction analysis presented in Section 2, the\nimplementation of Large Language Models (LLMs) for solar household load\nprediction presents three fundamental technical challenges beyond the inher-\nent stochasticity common to all prediction methods.\n7\n\nThe primary challenge stems from the heterogeneity in household charac-\nteristics, as solar households exhibit diverse electricity consumption patterns\ninfluenced by geographical locations, climatic conditions, and solar panel in-\nstallations. Traditional data-driven methodologies, which heavily rely on his-\ntorical data for feature extraction, demonstrate limited efficacy in cold-start\nscenarios and cross-user transfers under few-shot conditions. Developing a\nmodel capable of accurate predictions across diverse solar households under\nfew-shot conditions remains a fundamental challenge.\nThe second challenge involves the dimensional transformation between\ntime series data and the textual domain. The system input comprises time\nseries electricity load data for individual households over specified periods,\ndenoted as Xt ∈Rn×d, where n represents the sequence length of the time\nseries (i.e., the number of time steps), and d denotes the dimensionality of\nfeatures at each time step. However, LLMs require textual inputs, processing\nthem through Transformer architectures that encode relationships between\ntext segments as high-dimensional vectors. Given that different pre-trained\nLLMs accommodate varying vector dimensions - with BERT and GPT-2 ac-\ncepting 768-dimensional vectors and advanced models like LLAMA3 handling\n1024-dimensional vectors - we propose a mapping function f : Rn×d →Rd′,\nwhere d′ represents the target embedding dimension of the specific LLM, that\ngenerates LLM-compatible input Et.\nThe third challenge arises from the inherent interdependencies in multi-\nvariate time series data. For a system input containing multiple interrelated\nfeatures, we define:\nXt = [x(1)\nt , x(2)\nt , x(3)\nt ]\n(8)\nwhere x(1)\nt\n∈Rn represents the solar power generation sequence, x(2)\nt\n∈Rn\ndenotes the major appliance power consumption sequence, and x(3)\nt\n∈Rn in-\ndicates the regular appliance power consumption sequence. These features\nexhibit complex interdependencies characterized by the functional relation-\nship:\nf(x(1)\nt , x(2)\nt , x(3)\nt ) 7→R\n(9)\nwhere the mapping f captures the intricate interactions between different\ntypes of power consumption and generation patterns across time.\n8\n\nFigure 2: The workflow of TSLLM-Load Forecasting Mechanism, illustrating the system-\natic integration of data preprocessing, prompt generation, and prediction modules.\n3.2. Overview\nAs illustrated in Fig. 2, the TSLLM-Load Forecasting Mechanism im-\nplements a systematic workflow through three primary functional modules.\nEach module performs specific transformations in the sequential process of\nconverting raw load data into accurate forecasting results.\nIn Step 1, the Data Preprocessing Module establishes the foundation of\n9\n\nour framework. This fundamental module transforms input load data into\nusable, normalized time series data through three key operations including\nforward interpolation to address missing data points while maintaining data\ncontinuity, normalization procedures to standardize the data distribution,\nand timestamp standardization to align temporal sequences.\nIn Step 2, the Time Series Prompt Generation Module serves as the criti-\ncal bridge between preprocessed data and forecasting capabilities. This mod-\nule processes the normalized time series load data through two sophisticated\nsubmodules.\nThe Data Decomposition and Segmentation Module applies\nSeasonal and Trend decomposition (SFT) to separate normalized time series\nload data into trend (Tt ∈Rn), periodic (St ∈Rn), and noise (Rt ∈Rn)\ncomponents, and subsequently segments this decomposed data into struc-\ntured blocks of size b to address LLM context length limitations.\nThe Time Series Feature Embedding Module further processes this seg-\nmented data through three primary components:\n• Shared Linear Mapping Layer: Implements transformation with di-\nmensions (b × d, d′) to project time series data into the LLM’s textual\ndomain\n• Task-Independent Text Extraction Layer: Operates as a linear mapping\nlayer to extract representative tokens from the LLM’s textual domain\nfor different features\n• Alignment Module: Processes both TS-Embedding and extracted to-\nkens through similarity function computations to construct the final\nprediction prompt\nIn Step 3, the Time Series LLM Prediction Module culminates the pro-\ncess by utilizing the time series prediction prompt as input and generating\nforecasts. This concluding module integrates three essential components:\n• Pre-trained Large Language Model: Processes time series prediction\nprompts to generate forecasts for specified intervals\n• Reconstruction Output Layer: Transforms text-based predictions into\nstructured time series data ˆXt+1 ∈Rn×d\n• Training and Optimization Component: Adjusts system parameters\nthrough carefully designed loss functions to achieve optimal prediction\naccuracy\n10\n\nThrough this systematic integration of preprocessing, prompt generation,\nand prediction capabilities, the TSLLM-Load Forecasting Mechanism estab-\nlishes a robust framework for zero-shot load forecasting.\nThe structured\nworkflow ensures consistent performance while maintaining adaptability to\ndiverse load patterns and prediction scenarios. In the subsequent sections, we\nprovide a detailed examination of each module’s mathematical foundations,\nalgorithmic implementations, and operational principles, beginning with the\ndata preprocessing techniques in Section 3.3, followed by an in-depth analy-\nsis of the prompt generation mechanism in Section 3.4, and concluding with\na comprehensive discussion of the prediction module in Section 3.5.\n3.3. Data Preprocessing Implementation\nThe TSLLM-Load Forecasting Mechanism utilizes comprehensive load\ndata from 20 selected solar households within the Ausgrid supply area, span-\nning from July 1, 2010, to June 30, 2013. The dataset encompasses house-\nholds equipped with gross metering solar systems, with rigorous data qual-\nity assurance performed by the provider to exclude extreme cases of an-\nnual household electricity consumption and solar power generation efficiency\nduring the initial year. The temporal resolution maintains consistency at\n30-minute intervals, where each timestamp (e.g., 12:00 AM) represents the\naggregated load data for the preceding 30-minute period (e.g., 11:30 PM to\n12:00 AM).\nThe preprocessing pipeline comprises several essential stages to ensure\ndata quality and consistency. For missing data points, we employ linear in-\nterpolation to maintain temporal continuity. To address the non-stationarity\ninherent in real-world data, we implement reversible instance normalization\non the input load data. For a given load input sequence Xt over a time\nwindow of length T, the normalized sequence ˆXt is computed through the\ntransformation:\nˆXt = γ · Xt −µt\np\nσ2\nt + ϵ\n+ β\n(10)\nwhere:\n• µt and σ2\nt represent the mean and variance computed over the temporal\ndimension of the specific instance respectively\n• γ and β are learnable parameters that enable adaptive scaling of the\nnormalized data\n11\n\n• ϵ denotes a small constant (typically 10−5) introduced to ensure nu-\nmerical stability\nThis normalization approach ensures consistent data distribution across\ndifferent time periods while preserving the relative patterns and relationships\nwithin the load sequences. The trainable parameters γ and β enable the\nmodel to learn optimal scaling factors during the training process, enhancing\nits ability to adapt to varying load patterns.\nThe implementation of this preprocessing module adheres to strict quality\ncontrol measures. The linear interpolation for missing values considers both\ntemporal proximity and pattern consistency to maintain data integrity. The\nnormalization procedure is applied independently to each feature dimension,\nensuring that the distinct characteristics of different load components are\npreserved while achieving consistent scale across the dataset.\nFurthermore, the module incorporates robust error handling mechanisms\nand validation checks to ensure the reliability of the preprocessed data. These\nmeasures include:\n• Boundary condition verification for interpolated values\n• Statistical consistency checks for normalized sequences\n• Temporal alignment validation for synchronized data streams\nThe preprocessed data provides a standardized and reliable foundation for\nsubsequent feature extraction and model training processes. The effective-\nness of these preprocessing steps is critical for the overall performance of the\nTSLLM-Load Forecasting Mechanism, particularly in maintaining prediction\naccuracy across diverse household load patterns.\n3.4. Time Series Prompt Generation Module\nThe Time Series Prompt Generation Module incorporates a multi-task\nlearning framework to effectively capture and process the intricate correla-\ntions between various features in time series data.\nThis section presents\na detailed exposition of the data decomposition, segmentation, and feature\nembedding methodologies.\n12\n\n3.4.1. Data Decomposition and Segmentation\nWe implement additive seasonal-trend decomposition to systematically\npartition the normalized time series into its fundamental constituent compo-\nnents. The decomposition process is mathematically formulated as:\nXt = Tt + St + Rt\n(11)\nwhere Tt ∈Rn, St ∈Rn, and Rt ∈Rn represent the long-term trend, sea-\nsonal, and residual components, respectively. The decomposition follows a\nclassical seasonal-trend additive methodology. Initially, the long-term trend\ncomponent is extracted utilizing moving average methods to capture grad-\nual variations in the time series. Subsequently, the detrended time series\nundergoes analysis with predefined seasonal parameters to estimate the peri-\nodic component. Finally, the residual component, which captures stochastic\nfluctuations, is derived by subtracting the estimated trend and seasonal com-\nponents from the normalized time series.\nTo effectively capture the temporal encoding and local context within the\ninput time series, we aggregate consecutive time steps into overlapping patch\ntokens. For the trend component sequence Tt, the patch token representation\nPt is generated as follows:\nPt = {pi}np\ni=1,\npi ∈Rl×d\n(12)\nwhere l denotes the patch length determining the temporal granularity of\nanalysis, np represents the total number of patches dictating the resolution\nof representation, and sh indicates the horizontal sliding stride controlling\nthe overlap between adjacent patches.\n3.4.2. Time Series Feature Embedding\nMulti-Task Learning Framework. The module employs a sophisticated multi-\ntask learning approach to process solar household data characterized by three\ndistinct features: solar power generation, major appliance power consump-\ntion, and regular appliance power consumption. For a time series dataset\nwith these heterogeneous features, we define:\nXt = [x(1)\nt , x(2)\nt , x(3)\nt ]\n(13)\nwhere x(1)\nt , x(2)\nt , x(3)\nt\n∈Rn represent the respective feature sequences. The\ninitial transformation through the input model and subsequent transforma-\n13\n\ntion of LLM outputs are conceptualized as tasks with inherent feature sim-\nilarity. The embeddings of different features are processed as distinct but\ninterconnected sub-tasks:\nEt = [e(1)\nt , e(2)\nt , e(3)\nt ]\n(14)\nFeature Projection and Alignment. To facilitate comprehensive feature rep-\nresentation, we concatenate the component tokens into meta-tokens:\nMt = concat[Tt, St, Rt]\n(15)\nThese meta-tokens undergo transformation through a shared linear pro-\njection layer:\nEt = WsMt + bs\n(16)\nwhere Ws ∈Rd′×(3d) and bs ∈Rd′ are learnable parameters that facilitate\ndimensionality transformation and feature extraction. The resulting embed-\ndings are then separated by features to maintain their distinct identities while\npreserving inter-feature relationships.\nThe token embeddings space of the pre-trained LLM is denoted as RV ×d,\nwhere V represents the vocabulary size (typically in the order of tens of\nthousands) and d is the embedding dimension. To optimize computational\nefficiency and enhance model performance, we employ a task-independent\nlinear mapping layer Wt ∈Rk×V to derive a subset Rk×d for alignment with\ntime series data, where k ≪V represents a significantly reduced vocabulary\nsize.\nThe alignment between semantic space and time series data utilizes cosine\nsimilarity, a metric particularly effective for high-dimensional vector spaces:\ns(e, v) =\ne⊤v\n∥e∥∥v∥\n(17)\nwhere e ∈Rd represents time series embeddings and v ∈Rd denotes token\nembeddings in the reduced vocabulary space. Based on computed similarity\nscores, the most semantically relevant tokens are selected to enhance the\ninput embeddings:\nˆEt = concat[{vi}k\ni=1, Et]\n(18)\n14\n\nwhere k denotes the number of selected tokens and vi represents the top-\nk similar token embeddings that maximize the semantic alignment between\ntextual and time series domains.\n3.5. Time Series LLM Prediction Module\nThis module leverages the time series prediction prompt to generate fore-\ncasts through the pre-trained large language model. It integrates prediction\ngeneration, output reconstruction, and parameter optimization in a cohesive\nframework to achieve accurate load forecasting.\n3.5.1. Reconstruction Output Layer and Pre-trained LLM\nAfter processing through the pre-trained LLM, we strategically discard\nportions of the model’s output Ht ∈RL×d from the m-th layer corresponding\nto the prefix prompt, where L represents the sequence length and d denotes\nthe embedding dimension. The remaining output undergoes flattening and\ntransformation through a linear mapping layer to produce the model’s final\noutput:\nYt = WrHt + br\n(19)\nwhere Wr ∈R(n×d)×d and br ∈Rn×d are learnable parameters of the re-\nconstruction layer that facilitate the transformation from the LLM’s embed-\nding space to the target time series space. Owing to the initial decomposition\nstep, the overall prediction combines individual component predictions in an\nadditive manner. The output Yt is further decomposed into its constituent\ncomponents:\nYt = [y(T)\nt\n, y(S)\nt\n, y(R)\nt\n]\n(20)\nwhere y(T)\nt\n, y(S)\nt\n, and y(R)\nt\nrepresent the predicted trend, seasonal, and\nresidual components, respectively. The final prediction is obtained through\ncomponent recomposition following the inverse operation of the initial de-\ncomposition:\nˆXt+1 = y(T)\nt\n+ y(S)\nt\n+ y(R)\nt\n(21)\nThis additive recombination preserves the distinct characteristics of each\ncomponent while yielding a comprehensive forecasting result.\n15\n\nFigure 3: Training process flowchart of the TSLLM-Load Forecasting Mechanism, illus-\ntrating parameter optimization and data flow through different components.\n3.5.2. Training and Optimization Strategy\nAs illustrated in Fig. 3, the training process adopts a selective optimiza-\ntion approach, focusing on specific model components while preserving the\ngeneral capabilities of the pre-trained LLM. Empirical evidence suggests that\nmaintaining most parameters in a non-trainable state yields superior gener-\nalization performance compared to comprehensive model retraining. There-\nfore, we selectively adjust the parameters of the position embedding layer\nand the layer normalization layer within the large language model, which are\ncrucial for adapting the model to time series data while retaining its linguistic\nknowledge.\nThe optimization process targets five key components: the shared in-\nput layer in the time series prompt generation module, independent token\nextraction layers for each textual domain, position embedding layer, layer\n16\n\nnormalization layer, and the output layer of the large language model. The\nobjective function is meticulously designed to achieve optimal prediction per-\nformance while ensuring effective alignment between time series embeddings\nand textual domain tokens:\nL = Lpred + λLalign\n(22)\nwhere Lpred represents the prediction loss implemented as Mean Squared\nError (MSE) calculated as 1\nn\nPn\ni=1(ˆyi −yi)2, and Lalign denotes the similarity\nscore matching function that aligns selected text tokens with decomposed\nand concatenated time series embeddings. The hyperparameter λ serves as a\nbalancing factor that regulates the contribution of the alignment term to the\noverall loss, with its value determined through systematic sensitivity analysis.\nThis strategically formulated training approach ensures that the model\nmaintains its pre-trained language understanding capabilities while effec-\ntively adapting to the specific requirements of time series forecasting. The se-\nlective parameter optimization methodology promotes efficient learning while\npreserving the model’s generalization ability, enabling robust performance in\nboth conventional and zero-shot prediction scenarios.\n4. Results and Discussion\n4.1. Experimental Setting\nTo validate the effectiveness of the proposed algorithm, we conducted\ncomprehensive experiments using the Australian solar household dataset [41].\nThis dataset comprises load data collected from 300 solar-equipped house-\nholds within the Australian electricity grid jurisdiction from July 1, 2010, to\nJune 30, 2013. The data encompasses three key measurements sampled at\n30-minute intervals: solar panel generation load, major appliance load, and\nother appliance load.\nGiven experimental constraints, we randomly selected 20 households for\nzero-shot load forecasting evaluation using the TSLLM-Load Forecasting\nMechanism.\nFrom these households, we designated one household’s load\ndata as the primary training target, partitioning its data into training, vali-\ndation, and test sets with a ratio of 7:1:2. For the remaining 19 households,\nwe extracted load data segments of equivalent length to the first household’s\ntest set as prediction targets. The implementation utilizes GPT-2 as the\nbackbone pre-trained large language model. All experiments were conducted\n17\n\nFigure 4: Experimental workflow diagram showing the input processing, prediction gen-\neration, and output formation stages of the TSLLM-Load Forecasting Mechanism.\nusing PyTorch on a computing platform equipped with an NVIDIA GeForce\nGTX A100 80GB GPU.\n4.2. Experimental Framework\nThe TSLLM-Load Forecasting Mechanism is designed to predict house-\nhold electricity load for a 48-hour horizon. Fig. 4 illustrates the experimental\nframework of the algorithm.\nThe model accepts three-dimensional input data covering the 10 days pre-\nceding the prediction time point: solar panel generation load, major appli-\nance load, and other appliance load. These data streams are consolidated into\na vector with dimensions (512, 3) for model processing. The system trans-\nforms this input into a TS-Prompt comprising npatch data patch prompts,\nwhere each patch prompt consists of a prefix prompt Pprefix and patch em-\nbedding Pembed.\nAs shown in Fig. 5, the model generates load forecasts for the subsequent\n48 hours without requiring training on historical data from the target house-\nhold. The output maintains the three-dimensional structure, producing a\n(96, 3) vector representing the predicted loads across all three measurement\ncategories.\nTo evaluate prediction performance, we employ two standard metrics:\nMean Squared Error (MSE) and Mean Absolute Error (MAE). These metrics\nquantify the deviation between predicted values and actual measurements,\ndefined as:\n18\n\nFigure 5: Input-output structure and prompt template design of the TSLLM-Load Fore-\ncasting Mechanism, showing the transformation from raw data to prediction output.\nMSE = 1\nn\nn\nX\ni=1\n(yi −ˆyi)2\n(23)\nMAE = 1\nn\nn\nX\ni=1\n|yi −ˆyi|\n(24)\nwhere yi represents the actual values, ˆyi denotes the predicted values,\nand n indicates the number of samples. These metrics provide complemen-\ntary perspectives on prediction accuracy, with MSE being more sensitive to\nlarge errors and MAE offering a direct interpretation of average prediction\ndeviation.\n19\n\n0 . 5 2 6\n0 . 5 0 2\n0 . 9 3 1\n0 . 9 3\n0 . 9 4 2\n0 . 4 1 6\n0 . 4 1 7\n0 . 0\n0 . 2\n0 . 4\n0 . 6\n0 . 8\n1 . 0\nM\nS E\nM\ne t h o d s\n C N N _ L S T M\n_ A t t e n t i o n\n C N N _ L S T M  \n A u t o f o r m e r                 \n i n f o r m e r\n T C N _ L S T M\n_ A t t e n t i o n\n S i n g l e - t a s k - T S L L M\n T S L L M\n- L o a d  F o r e c a s t i n g  M\ne c h a n i s m\n0 . 4 6 3\n0 . 4 2 2\n0 . 7 0 9\n0 . 7 1 5\n0 . 7 3 5\n0 . 3 7 6\n0 . 3 7 6\n0 . 0\n0 . 1\n0 . 2\n0 . 3\n0 . 4\n0 . 5\n0 . 6\n0 . 7\n0 . 8\nM\nA E\nM\ne t h o d s\n C N N _ L S T M\n_ A t t e n t i o n\n C N N _ L S T M  \n A u t o f o r m e r                 \n i n f o r m e r\n T C N _ L S T M\n_ A t t e n t i o n\n S i n g l e - t a s k - T S L L M\n T S L L M\n- L o a d  F o r e c a s t i n g  M\ne c h a n i s m\nFigure 6: Prediction results for training targets: (left) Mean Squared Error and (right)\nMean Absolute Error comparisons across different models.\n4.3. Comparative Analysis of Prediction Capability and Transferability\nTo systematically evaluate the TSLLM-Load Forecasting Mechanism’s\neffectiveness, we conducted a comprehensive comparative analysis against\nstate-of-the-art baseline models. Our evaluation framework encompasses two\nkey dimensions: prediction accuracy on the training dataset and transferabil-\nity performance across multiple households.\nFirst, we established a diverse baseline comprising advanced forecasting\nmodels: Informer [42], Autoformer [43], CNN LSTM [44], CNN LSTM Attention\n[45], and TCN LSTM Attention [46]. Additionally, we included a single-task\nvariant of our TSLLM-Load Forecasting mechanism to isolate the impact of\nthe multi-task learning framework. Each baseline model represents a distinct\napproach to time series forecasting:\nThe CNN LSTM architecture combines convolutional feature extraction\nwith sequential modeling through LSTM networks. The CNN LSTM Attention\nmodel enhances this architecture with an attention mechanism, enabling dy-\nnamic focus on significant sequence components. The TCN LSTM Attention\nmodel further incorporates temporal convolutional networks, leveraging causal\nand dilated convolutions for improved temporal pattern capture.\nIn the Transformer-based category, Informer introduces a probabilistic\nsparse attention mechanism optimized for time-series forecasting, while Aut-\noformer extends this approach with adaptive frequency decomposition for\nseparate modeling of seasonal and trend components.\nOur experimental methodology ensured statistical robustness through\nthree independent evaluation runs for each model, with results averaged\n20\n\n0\n2\n4\n6\n8\n1 0\n1 2\n1 4\n1 6\n1 8\n2 0\n0 . 2\n0 . 4\n0 . 6\n0 . 8\n1 . 0\n1 . 2\n1 . 4\n1 . 6\n1 . 8\n2 . 0\n2 . 2\nM\nS E\nC u s t o m\ne r  I D\n A u t o f o r m e r  \n C N N _ L S T M    \n C N N _ L S T M\n_ A t t e n t i o n         \n i n f o r m e r\n T C N _ L S T M\n_ A t t e n t i o n\nS i n g l e - t a s k -  T S L L M\n ( c o s )\n T S L L M\n- L o a d  F o r e c a s t i n g    M\ne c h a n i s m  ( c o s )  \n0\n2\n4\n6\n8\n1 0\n1 2\n1 4\n1 6\n1 8\n2 0\n0 . 2\n0 . 3\n0 . 4\n0 . 5\n0 . 6\n0 . 7\n0 . 8\n0 . 9\nM\nA E\nC u s t o m e r  I D\n A u t o f o r m e r  \n C N N _ L S T M    \n C N N _ L S T M\n_ A t t e n t i o n         \n i n f o r m e r\n T C N _ L S T M\n_ A t t e n t i o n\nS i n g l e - t a s k -  T S L L M\n ( c o s )\n T S L L M\n- L o a d  F o r e c a s t i n g    M\ne c h a n i s m  ( c o s )  \nFigure 7: Results of household power load transferability experiments: (left) Mean Squared\nError and (right) Mean Absolute Error across all test households.\nacross runs. The hyperparameter λ was empirically optimized to 0.1 based\non preliminary experiments. Detailed performance metrics are documented\nin Appendix A.\nAs illustrated in Fig. 6, our LLM-based approach demonstrates remark-\nable performance superiority on the training household dataset, significantly\noutperforming conventional forecasting methods. Moreover, in zero-shot pre-\ndiction scenarios involving 19 household users, our method maintains consis-\ntent performance advantages. Notably, the MTL-based design consistently\noutperforms its single-task counterpart across most households, validating\nthe effectiveness of our multi-task learning framework.\nCompared to existing approaches, the proposed TSLLM-Load Forecast-\ning Mechanism demonstrates substantial improvements in prediction accu-\nracy across all baseline models. The comprehensive analysis reveals that our\nmethod achieves a 12.2% reduction in total MSE compared to Informer, while\nshowing even more significant improvements of 40.9%, 38.7%, 38.1%, and\n39.6% when compared to Autoformer, CNN LSTM, CNN LSTM Attention,\nand TCN LSTM Attention, respectively.\nThese consistent improvements\nacross different baseline architectures underscore the robust performance ad-\nvantages of our proposed approach.\n4.4. Ablation Analysis\nTo understand the contribution of individual components and validate\nour design choices, we conducted a systematic ablation study focusing on\ntwo critical aspects: the Multi-Task Learning (MTL) framework and the\n21\n\n1 1 . 2 7 1 2 2\n1 1 . 3 5 0 7\n1 1 . 2 9 6 5 8\n1 1 . 3 7 3 2 9\n1 1 . 2 0\n1 1 . 2 5\n1 1 . 3 0\n1 1 . 3 5\n1 1 . 4 0\n1 1 . 4 5\n1 1 . 5 0\n1 1 . 2 0\n1 1 . 2 5\n1 1 . 3 0\n1 1 . 3 5\n1 1 . 4 0\n1 1 . 4 5\n1 1 . 5 0\nS U M\n_ M\nS E\nM\ne t h o d s\n T S L L M\n- L o a d  F o r e c a s t i n g  M\ne c h a n i s m  ( c o s ) \n S i n g l - t a s k  T S L L M\n ( E d )\n T S L L M\n- L o a d  F o r e c a s t i n g  M\ne c h a n i s m  ( E d ) \n S i n g l e - t a s k  T S L L M\n ( c o s )\n7 . 6 7 0 9 3\n7 . 6 8 8 6 2\n7 . 6 8 6 8 9\n7 . 7 1 2 6 8\n7 . 6 5\n7 . 7 0\n7 . 7 5\nS U M\n_ M\nA E\nM\ne t h o d s\n T S L L M\n- L o a d  F o r e c a s t i n g  M\ne c h a n i s m  ( c o s ) \n S i n g l - t a s k  T S L L M\n ( E d )\n T S L L M\n- L o a d  F o r e c a s t i n g  M\ne c h a n i s m  ( E d ) \n S i n g l e - t a s k  T S L L M\n ( c o s )\nFigure 8: Results of ablation experiments: (left) Mean Squared Error and (right) Mean\nAbsolute Error comparisons between MTL and single-task variants with different similarity\nmetrics.\nchoice of similarity metrics. The ablation experiments maintained consistent\nconditions with λ = 0.1 while varying the learning framework (MTL vs.\nSingle-task) and similarity metric (Cosine Similarity vs. Euclidean Distance).\nThe ablation experiments reveal several important insights regarding the\nmodel’s architectural design choices. The cosine similarity metric demon-\nstrates consistently superior performance compared to Euclidean distance in\nboth MTL and single-task implementations, with this advantage being par-\nticularly pronounced in the MTL framework. This observation suggests that\ncosine similarity more effectively captures the semantic relationships within\nthe high-dimensional feature space.\nFurthermore, when employing cosine\nsimilarity, the MTL framework exhibits notably enhanced performance com-\npared to its single-task counterpart, though this advantage becomes less pro-\nnounced when using Euclidean distance. This interaction between similarity\nmetrics and learning frameworks indicates a beneficial synergistic relation-\nship between MTL and cosine similarity. The optimal performance achieved\nthrough their combination demonstrates how these components effectively\ncomplement each other in capturing both feature relationships and task de-\npendencies, validating our architectural design decisions.\nThese comprehensive findings provide strong empirical support for our\narchitectural choices and demonstrate the importance of carefully considering\nboth the learning framework and similarity metrics in multi-task time series\nforecasting applications. The results conclusively validate the effectiveness\nof our proposed approach in achieving robust and accurate load forecasting\nacross diverse scenarios.\n22\n\nTable 1: Sensitivity Analysis Results of Household Load Forecasting\nMetric\nλ = 0.0\nλ = 0.01\nλ = 0.05\nλ = 0.1\nλ = 1.0\nSum MSE\n11.8849\n11.3626\n11.3306\n11.2712\n11.8562\nSum MAE\n8.0830\n7.7422\n7.7336\n7.6710\n7.8257\n0 . 0 1\n0 . 1\n1\n0 . 1 2 4\n0 . 1 2 6\n0 . 1 2 8\n0 . 1 3 0\n0 . 0 8 4\n0 . 0 8 6\n0 . 0 8 8\n0 . 0 9 0\n1 / S U M\n_ M\nS E\n1 / S U M\n_ M\nA E\n 1 / S U M\n_ M\nS E\n 1 / S U M\n_ M\nA E\nFigure 9: Sensitivity analysis results showing the relationship between model accuracy\n(1/SUM MSE and 1/SUM MAE) and hyperparameter λ.\n4.5. Sensitivity Analysis\nTo investigate the influence of hyperparameter λ on model performance,\nwe conducted a comprehensive sensitivity analysis using identical training\nand testing data while varying λ values. This analysis evaluates how dif-\nferent weightings between prediction and alignment losses affect the model’s\nforecasting accuracy.\nThe experimental results, as shown in Table 1 and Fig. 9, demonstrate\nthat increasing λ initially improves time series prediction accuracy up to a\ncertain threshold, beyond which prediction accuracy decreases significantly.\nThe analysis reveals that while optimal λ values differ slightly between MSE\nand MAE metrics, λ = 0.1 achieves consistently strong performance across\nboth evaluation criteria. Figure 9 illustrates this relationship, plotting the\ninverse of SUM MSE and SUM MAE against varying λ values to provide a\n23\n\nclear visualization of the model’s sensitivity to this hyperparameter.\nThis sensitivity analysis provides crucial insights into the model’s behav-\nior and guides the selection of appropriate hyperparameter values for optimal\nperformance. The results indicate that careful tuning of λ is essential for\nbalancing the contributions of prediction and alignment losses in the overall\noptimization objective.\n5. Conclusions\nOur paper introduces a method for load forecasting in power systems\nunder zero-shot scenarios, termed the TSLLM-Load Forecasting Mechanism.\nThis approach leverages large language models (LLMs) to address the limita-\ntions of existing forecasting methods, including poor transferability, inability\nto handle zero-shot scenarios without historical data, and lack of adaptability\nacross diverse data sources.\nTo overcome the limitations of traditional methods in zero-shot forecast-\ning scenarios, our approach combines large language models with load fore-\ncasting. To address the challenge of enabling LLMs to understand time series\ndata, we designed a Time Series Prompt Generation Module. This module\npreprocesses power load data by decomposing and segmenting it into time\nseries format. It employs a shared linear input layer and a task-independent\ntext extraction layer to extract features from both the time series and text\ndomains.\nFinally, a similarity alignment technique is applied to generate\ntime series prompts, enabling the LLM to comprehend the time series data\nand perform load forecasting.\nIn a migration experiment involving load data from 20 Australian solar-\npowered households, the proposed method demonstrated superior perfor-\nmance compared to existing approaches in terms of Mean Squared Error\n(MSE) and Mean Absolute Error (MAE). It excelled both in testing on the\ntraining sample derived from one household’s load data and in zero-shot\nmigration experiments on the remaining 19 households’ data. The method\nachieved a total MSE of 11.2712 and a total MAE of 7.6710 across the entire\ndataset, delivering at least a 12% performance improvement over existing\nmethods, thereby validating the accuracy and transferability of the proposed\nzero-shot forecasting method.\n24\n\nAcknowledgments\nThis work is supported by the National Science Foundation of China\nunder Grants 62203350 and 62373297, in part by Industrial Field Project—\nKey Industrial Innovation Chain (Group) of Shaanxi Province under Grant\n2022ZDLGY06-02. The author gratefully acknowledges the support of K. C.\nWong Education Foundation.\nReferences\n[1] L. Li, Y. Ju, Z. Wang, Quantifying the impact of building load forecasts\non optimizing energy storage systems, Energy and Buildings 307 (2024)\n113913.\n[2] S. An, T.-J. Oh, E. Sohn, D. Kim, Deep learning for precipitation now-\ncasting: A survey from the perspective of time series forecasting, Expert\nSystems with Applications 268 (2025) 126301.\n[3] J. Pei, N. Liu, J. Shi, Y. Ding, Tackling the duck curve in renew-\nable power system: A multi-task learning model with itransformer for\nnet-load forecasting, Energy Conversion and Management 326 (2025)\n119442.\n[4] A. Kaginalkar, S. Kumar, P. Gargava, D. Niyogi, Review of urban com-\nputing in air quality management as smart city service: An integrated\niot, ai, and cloud technology perspective, Urban Climate 39 (2021)\n100972.\n[5] B. Kedem, K. Fokianos, Regression models for time series analysis, John\nWiley & Sons, 2005.\n[6] B. Mallala, A. I. U. Ahmed, S. V. Pamidi, M. O. Faruque, R. Reddy,\nForecasting global sustainable energy from renewable sources using ran-\ndom forest algorithm, Results in Engineering 25 (2025) 103789.\n[7] M. Murat, I. Malinowska, M. Gos, J. Krzyszczak, Forecasting daily me-\nteorological time series using arima and regression models, International\nagrophysics 32 (2) (2018).\n25\n\n[8] S. Tiwari, P. Satpute, S. Ghosh, Time series forecasting of multiphase\nmicrostructure evolution using deep learning, Computational Materials\nScience 247 (2025) 113518.\n[9] H. Wang, Z. Lei, X. Zhang, B. Zhou, J. Peng, A review of deep learning\nfor renewable energy forecasting, Energy Conversion and Management\n198 (2019) 111799.\n[10] G. Singh, J. Bedi, A federated and transfer learning based approach\nfor households load forecasting, Knowledge-Based Systems 299 (2024)\n111967.\n[11] K. Song, Y. Yu, T. Zhang, X. Li, Z. Lei, H. He, Y. Wang, S. Gao, Short-\nterm load forecasting based on ceemdan and dendritic deep learning,\nKnowledge-Based Systems 294 (2024) 111729.\n[12] J. Tian, H. Liu, W. Gan, Y. Zhou, N. Wang, S. Ma, Short-term electric\nvehicle charging load forecasting based on tcn-lstm network with com-\nprehensive similar day identification, Applied Energy 381 (2025) 125174.\n[13] A. A. Alshdadi, A. A. Almazroi, N. Ayub, Iot-driven load forecasting\nwith machine learning for logistics planning, Internet of Things 29 (2025)\n101441.\n[14] I. Priyadarshini, S. Sahu, R. Kumar, D. Taniar, A machine-learning\nensemble model for predicting energy consumption in smart homes, In-\nternet of Things 20 (2022) 100636.\n[15] Z. Chen, D. Chen, X. Zhang, Z. Yuan, X. Cheng, Learning graph struc-\ntures with transformer for multivariate time-series anomaly detection in\niot, IEEE Internet of Things Journal 9 (12) (2021) 9179–9189.\n[16] D. Liang, H. Zhang, D. Yuan, M. Zhang, Periodformer: An efficient\nlong-term time series forecasting method based on periodic attention,\nKnowledge-Based Systems 304 (2024) 112556.\n[17] J. Zhu, D. Liu, H. Chen, J. Liu, Z. Tao, Dtsformer: Decoupled temporal-\nspatial diffusion transformer for enhanced long-term time series forecast-\ning, Knowledge-Based Systems 309 (2025) 112828.\n26\n\n[18] W. Han, T. Zhu, L. Chen, H. Ning, Y. Luo, Y. Wan, Mcformer: mul-\ntivariate time series forecasting with mixed-channels transformer, IEEE\nInternet of Things Journal (2024).\n[19] A. Liang, X. Chai, Y. Sun, M. Guizani, Gtformer:\nGraph-based\ntemporal-order-aware transformer for long-term series forecasting, IEEE\nInternet of Things Journal (2024).\n[20] C. K. Rao, S. K. Sahoo, F. F. Yanine, Iot enabled intelligent energy man-\nagement system employing advanced forecasting algorithms and load\noptimization strategies to enhance renewable energy generation, Uncon-\nventional Resources 4 (2024) 100101.\n[21] A. Wan, Q. Chang, A.-B. Khalil, J. He, Short-term power load forecast-\ning for combined heat and power using cnn-lstm enhanced by attention\nmechanism, Energy 282 (2023) 128274.\n[22] C. Wang, X. Li, Y. Shi, W. Jiang, Q. Song, X. Li, Load forecasting\nmethod based on cnn and extended lstm, Energy Reports 12 (2024)\n2452–2461.\n[23] X. Xu, Z. Zhou, H. Zhao, Short-term power load forecasting based on\ntcn-lstm model, in: 2024 3rd International Conference on Energy, Power\nand Electrical Technology (ICEPET), IEEE, 2024, pp. 1340–1343.\n[24] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al.,\nLanguage models are unsupervised multitask learners, OpenAI blog 1 (8)\n(2019) 9.\n[25] A. Cook, O. Karaku¸s, Llm-commentator: Novel fine-tuning strategies\nof large language models for automatic commentary generation using\nfootball event data, Knowledge-Based Systems 300 (2024) 112219.\n[26] H. Xue, F. D. Salim, Utilizing language models for energy load forecast-\ning, in: Proceedings of the 10th ACM International Conference on Sys-\ntems for Energy-Efficient Buildings, Cities, and Transportation, 2023,\npp. 224–227.\n[27] S. Wu, O. Irsoy, S. Lu, V. Dabravolski, M. Dredze, S. Gehrmann,\nP. Kambadur, D. Rosenberg, G. Mann, Bloomberggpt: A large language\nmodel for finance, arXiv preprint arXiv:2303.17564 (2023).\n27\n\n[28] H. Xue, B. P. Voutharoja, F. D. Salim, Leveraging language foundation\nmodels for human mobility forecasting, in: Proceedings of the 30th In-\nternational Conference on Advances in Geographic Information Systems,\n2022, pp. 1–9.\n[29] N. Gruver, M. Finzi, S. Qiu, A. G. Wilson, Large language models\nare zero-shot time series forecasters, Advances in Neural Information\nProcessing Systems 36 (2023) 19622–19635.\n[30] Z. Zhong, D. Rempe, Y. Chen, B. Ivanovic, Y. Cao, D. Xu, M. Pavone,\nB. Ray, Language-guided traffic simulation via scene-level diffusion, in:\nConference on Robot Learning, PMLR, 2023, pp. 144–177.\n[31] H. Zheng, M. Wang, Z. Wang, X. Huang, Firedm: A weakly-supervised\napproach for massive generation of multi-scale and multi-scene fire seg-\nmentation datasets, Knowledge-Based Systems 290 (2024) 111547.\n[32] B. Cai, S. Yang, L. Gao, Y. Xiang, Hybrid variational autoencoder for\ntime series forecasting, Knowledge-Based Systems 281 (2023) 111079.\n[33] H. Tang, C. Zhang, M. Jin, Q. Yu, Z. Wang, X. Jin, Y. Zhang, M. Du,\nTime series forecasting with llms: Understanding and enhancing model\ncapabilities, ACM SIGKDD Explorations Newsletter 26 (2) (2025) 109–\n118.\n[34] L. Fang, W. Xiang, S. Pan, F. D. Salim, Y.-P. P. Chen, Spatiotemporal\npre-trained large language model for forecasting with missing values,\nIEEE Internet of Things Journal (2025).\n[35] C. Sun, H. Li, Y. Li, S. Hong, Test: Text prototype aligned embedding\nto activate llm’s ability for time series, arXiv preprint arXiv:2308.08241\n(2023).\n[36] X. Liu, J. Hu, Y. Li, S. Diao, Y. Liang, B. Hooi, R. Zimmermann,\nUnitime: A language-empowered unified model for cross-domain time\nseries forecasting, in: Proceedings of the ACM Web Conference 2024,\n2024, pp. 4095–4106.\n[37] T. Zhou, P. Niu, L. Sun, R. Jin, et al., One fits all: Power general\ntime series analysis by pretrained lm, Advances in neural information\nprocessing systems 36 (2023) 43322–43355.\n28\n\n[38] J. Qiu, W. Han, J. Zhu, M. Xu, D. Weber, B. Li, D. Zhao, Can brain\nsignals reveal inner alignment with human languages?, in: Findings of\nthe Association for Computational Linguistics: EMNLP 2023, 2023, pp.\n1789–1804.\n[39] Y. Zhang, Y. Zhang, M. Zheng, K. Chen, C. Gao, R. Ge, S. Teng,\nA. Jelloul, J. Rao, X. Guo, et al., Insight miner: A time series analysis\ndataset for cross-domain alignment with natural language, in: NeurIPS\n2023 AI for Science Workshop, 2023.\n[40] M. Jin, S. Wang, L. Ma, Z. Chu, J. Y. Zhang, X. Shi, P.-Y. Chen,\nY. Liang, Y.-F. Li, S. Pan, et al., Time-llm: Time series forecasting by\nreprogramming large language models, arXiv preprint arXiv:2310.01728\n(2023).\n[41] E. L. Ratnam, S. R. Weller, C. M. Kellett, A. T. Murray, Residential load\nand rooftop pv generation: an australian distribution network dataset,\nInternational Journal of Sustainable Energy 36 (8) (2017) 787–806.\n[42] H. Zhou, S. Zhang, J. Peng, S. Zhang, J. Li, H. Xiong, W. Zhang, In-\nformer: Beyond efficient transformer for long sequence time-series fore-\ncasting, in: Proceedings of the AAAI conference on artificial intelligence,\nVol. 35, 2021, pp. 11106–11115.\n[43] H. Wu, J. Xu, J. Wang, M. Long, Autoformer: Decomposition trans-\nformers with auto-correlation for long-term series forecasting, Advances\nin neural information processing systems 34 (2021) 22419–22430.\n[44] F. Yang, Z. Liu, J. Yan, Y. Geng, Cnn-lstm neural network with atten-\ntion mechanism for fault diagnosis of high voltage circuit breakers, in:\nIET Conference Proceedings CP803, Vol. 2022, IET, 2022, pp. 1102–\n1107.\n[45] X.-Q. Lu, J. Tian, Q. Liao, Z.-W. Xu, L. Gan, Cnn-lstm based incremen-\ntal attention mechanism enabled phase-space reconstruction for chaotic\ntime series prediction, Journal of Electronic Science and Technology\n22 (2) (2024) 100256.\n[46] H. Li, J. Sun, X. Liao, A novel short-term load forecasting model by tcn-\nlstm structure with attention mechanism, in: 2022 4th International\n29\n\nConference on Machine Learning, Big Data and Business Intelligence\n(MLBDBI), IEEE, 2022, pp. 178–182.\n30\n",
  "metadata": {
    "source_path": "papers/arxiv/Zero-shot_Load_Forecasting_for_Integrated_Energy_Systems_A_Large\n__Language_Model-based_Framework_with_Multi-task_Learning_d80891bfd11dec9f.pdf",
    "content_hash": "d80891bfd11dec9fb6774fe0b10254ef8254771bfd27c45ffcc408071cac49ce",
    "arxiv_id": null,
    "title": "Zero-shot Load Forecasting for Integrated Energy Systems: A Large Language Model-based Framework with Multi-task Learning",
    "author": "Jiaheng Li; Donghe Li; Ye Yang; Huan Xi; Yu Xiao; Li Sun; Dou An; Qingyu Yang; ",
    "creation_date": "D:20250225022819Z",
    "published": "2025-02-25T02:28:19",
    "pages": 30,
    "size": 896762,
    "file_mtime": 1740470213.9210403
  }
}