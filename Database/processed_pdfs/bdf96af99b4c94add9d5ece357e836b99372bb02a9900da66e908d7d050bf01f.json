{
  "text": "Improving the Transferability of Adversarial Examples by Inverse Knowledge\nDistillation\nWenyuan Wu1 , Zheng Liu2 , Yong Chen3 , Chao Su1 , Dezhong Peng1 and Xu Wang1∗\n1College of Computer Science, Sichuan University, China\n2Sichuan Newstrong UHD Video Technology Company Ltd., China\n3Institute of Optics and Electronics, Chinese Academy of Sciences, China\nwuwenyuan97@gmail.com, liuzheng@uptcsc.com, cy1415926@gmail.com, suchao.ml@gmail.com,\npengdz@scu.edu.cn, wangxu.scu@gmail.com\nAbstract\nIn recent years, the rapid development of deep neu-\nral networks has brought increased attention to the\nsecurity and robustness of these models. While ex-\nisting adversarial attack algorithms have demon-\nstrated success in improving adversarial transfer-\nability, their performance remains suboptimal due\nto a lack of consideration for the discrepancies\nbetween target and source models.\nTo address\nthis limitation, we propose a novel method, In-\nverse Knowledge Distillation (IKD), designed to\nenhance adversarial transferability effectively. IKD\nintroduces a distillation-inspired loss function that\nseamlessly integrates with gradient-based attack\nmethods, promoting diversity in attack gradients\nand mitigating overfitting to specific model archi-\ntectures. By diversifying gradients, IKD enables\nthe generation of adversarial samples with superior\ngeneralization capabilities across different models,\nsignificantly enhancing their effectiveness in black-\nbox attack scenarios. Extensive experiments on the\nImageNet dataset validate the effectiveness of our\napproach, demonstrating substantial improvements\nin the transferability and attack success rates of ad-\nversarial samples across a wide range of models.\n1\nIntroduction\nThe rapid advancements and significant achievements in deep\nlearning over the past few years have led to an increased fo-\ncus on its security aspects. A key security concern is the sus-\nceptibility of these systems to minimal, nearly undetectable\nadversarial noise [Szegedy et al., 2013].\nThis vulnerabil-\nity suggests a high risk of deliberate attacks, particularly in\ntechnologies such as facial recognition and autonomous driv-\ning.\nAlthough it is crucial to research methods to bolster\ndeep learning models against adversarial attacks, it is of equal\nimportance to investigate strategies for launching attacks on\nthese models.\nCurrent attack strategies create an adversarial sample by\nincorporating elaborate adversarial perturbations into the in-\nput. These perturbations are usually generated by generat-\n∗Corresponding author.\nBenign\nsample\nDecision boundary\nof surrogate model\nInitial adversarial \nsample\nDecision boundary\nof target model\nIKD optimization \ndirection\nOptimized adversarial \nsample after IKD\nOriginal gradient \ndirection\nOptimized gradient \ndirection\nFigure 1: The distinction between Inverse Knowledge Distil-\nlation (IKD) method and existing gradient-based attack ap-\nproaches. The blue arrow indicates the gradient direction of tra-\nditional gradient-based attack methods, which are heavily reliant on\nthe decision boundary of the surrogate model (i.e., closely corre-\nlated with the parameters of the surrogate model). As a result, these\nmethods may fail to effectively attack the target model, as they often\ncannot generate adversarial samples capable of crossing the deci-\nsion boundary of the target model, represented by the orange curve.\nIn contrast, by optimizing the attack gradient direction (indicated\nby the orange dashed line), our IKD method not only ensures that\nthe adversarial sample successfully crosses the surrogate model’s\ndecision boundary, but also increases the likelihood of the adversar-\nial sample overcoming the target model’s decision boundary. Con-\nsequently, our IKD method demonstrates a higher success rate in\ntransfer-based attacks.\ning networks [Zhao et al., 2018; Joshi et al., 2019; Qiu et\nal., 2020; Xiao et al., 2021] or gradient-based optimization\ntechniques [Goodfellow et al., 2014; Madry et al., 2017;\nDong et al., 2018; Xie et al., 2019; Dong et al., 2019;\nLin et al., 2019]. The latter, the gradient-based approach, is\nthe mainstream. The central idea behind these methods is to\ngenerate adversarial perturbations through gradients, which\nare calculated by maximizing the loss function associated\nwith the target task.\nExisting attack methods demonstrate high efficiency in\nwhite-box scenarios but face significant challenges in black-\nbox models, where internal model information is inaccessi-\nble. This limitation significantly increases the difficulty of\nattacks. To address this issue, research efforts are primar-\narXiv:2502.17003v1  [cs.LG]  24 Feb 2025\n\nily categorized into two approaches: query-based attacks and\ntransfer-based attacks. Query-based attacks generate adver-\nsarial samples through extensive queries but incur high com-\nputational costs and time overhead.\nIn contrast, transfer-\nbased attacks leverage the transferability of adversarial sam-\nples, generating them on surrogate models for application to\nblack-box models, offering greater efficiency and practicality.\nConsequently, this paper focuses on transfer-based attacks,\naiming to enhance the transferability of adversarial samples\nand improve their effectiveness in black-box attack scenarios.\nAt present, transfer-based attack methods have covered\na variety of technologies, including input transformation-\nbased attacks [Dong et al., 2019; Liang and Xiao, 2023;\nWang et al., 2021a; Xie et al., 2019], advanced gradient at-\ntacks [Dong et al., 2018; Li et al., 2023; Lin et al., 2019;\nWang and He, 2021], ensemble attacks [Qian et al., 2023;\nTram`er et al., 2017; Xiong et al., 2022], feature-based at-\ntacks [Ganeshan et al., 2019; Wang et al., 2023; Zhang et al.,\n2022], and so on. Although these methods can improve the\ntransferability of attacks to some extent, they usually come\nwith high computational costs. Moreover, existing methods\noften fail to adequately account for the differences between\nsurrogate models and target models. As a result, the gener-\nated adversarial perturbations tend to perform better on surro-\ngate models but may not retain the same effectiveness when\napplied to target models. Specifically, these methods suffer\nfrom a lack of diversity in attack gradients, relying heavily on\na single fixed direction to generate adversarial samples while\nneglecting other potential directions. This fixed direction is\ntypically determined by the computations of the surrogate\nmodel, causing the generated adversarial samples to overfit\nthe surrogate model and limiting their effectiveness in attack-\ning the target model.\nTo address this challenge, we propose Inverse Knowledge\nDistillation (IKD), a novel and effective method aimed at mit-\nigating overfitting by enhancing gradient diversity. IKD inte-\ngrates a distillation-inspired mechanism into the loss compu-\ntation of gradient-based attack methods, optimizing not only\nthe alignment with the specified label but also the divergence\nin output feature distributions between adversarial and benign\nsamples on the surrogate model. This approach introduces\nricher gradient information, breaking the constraints of fixed\ngradient directions and significantly enhancing gradient di-\nversity. As illustrated in Figure 1, by maximizing the dif-\nference in feature distributions, IKD reduces dependence on\nthe specific decision boundaries of surrogate models, com-\npelling the optimization process to prioritize more general-\nized perturbations. This effectively prevents adversarial sam-\nples from overfitting to surrogate models and substantially\nimproves their transferability.\nThe main contributions of our work can be summarized as\nfollows:\n• We propose a simple and effective adversarial attack\nmethod: Inverse Knowledge Distillation (IKD) attack.\nIKD can effectively reduce the overfitting problem and\nenhance the adversarial sample’s transferability.\nOur\nInverse Knowledge distillation (IKD) method is com-\npatible and easy to integrate with gradient-based attack\nmethods to improve attack effectiveness.\n• We reveal that different distillation methods have sig-\nnificant differences on the transferability of adversarial\nperturbations. Compared with mean square error (MSE)\nand cross-entropy (CE) loss, the KL divergence based\ndistillation method shows the best effect in improving\nthe transferability of the adversarial sample.\n• We conducted a large number of experiments on the Im-\nageNet dataset to verify the validity of the proposed IKD\nmethod.\nThe experimental results show that the pro-\nposed method significantly improves the success rate of\nalmost all attack methods, which proves the potential\nand advantage of IKD in practice.\n2\nRelated Works\n2.1\nAdversarial Attack\nThe primary attack techniques comprise of the generative-\nbased [Zhao et al., 2018; Song et al., 2018; Joshi et al., 2019;\nQiu et al., 2020; Xiao et al., 2021], and gradient-based\nstrategies [Goodfellow et al., 2014; Madry et al., 2017;\nDong et al., 2018; Xie et al., 2019; Dong et al., 2019;\nLin et al., 2019]. FGSM (Fast Gradient Sign Method) [Good-\nfellow et al., 2014] is a simple and fast method for generating\nadversarial examples.\nSince then, various methods have been proposed to en-\nhance the attacking capability of adversarial samples.\nIn\nDIM [Xie et al., 2019], randomization operations of random\nresizing and padding of the original image were introduced.\nTIM [Dong et al., 2019] proposes a translation-invariant at-\ntack method by convolving the gradient with a Gaussian ker-\nnel, further augmenting the attacking capability of the sam-\nples. Inspired by Nesterov’s accelerated gradient [Nesterov,\n1983], SIM [Lin et al., 2019] modified the accumulation of\ngradients to effectively predict and enhance the adversariality\nof the samples. VT [Wang and He, 2021] considered the gra-\ndient variance of the previous iteration to adjust the current\ngradient, thereby stabilizing the update direction and avoid-\ning poor local optima. EMI [Wang et al., 2021b] accumu-\nlated the gradients of data points sampled in the direction of\nthe previous iteration’s gradient to find a more stable gradient\ndirection. MagicGAN [Chen et al., 2022] devises a multia-\ngent discriminator capable of adapting to the decision bound-\naries of diverse target models. This provides a more varied\ngradient information spectrum, facilitating the creation of ad-\nversarial perturbations. MTAA [Chen et al., 2023] employs\na representation that preserves relationships to study patterns\nthat are adversarial. APAA [Yuan et al., 2024] directly uti-\nlizes the precise gradient direction with a scale factor to gen-\nerate adversarial perturbations, thereby enhancing the attack\nsuccess rate of adversarial samples, even with lesser pertur-\nbations.\nHowever, none of these methods take into account the ef-\nfect of the lack of diversity of attack gradients on the ad-\nversarial sample’s transferability. Therefore, the adversarial\nsamples generated by these methods are heavily dependent\non the specific decision boundaries of the surrogate model,\nthus reducing the attack effect on the target model.\n\n2.2\nAdversarial Defense\nThe goal of adversarial defense is to enhance the resilience of\nthe target model when adversarial samples serve as inputs.\nDefense approaches can primarily be classified into three\ntypes: adversarial detection, adversarial purification, and ad-\nversarial training. Adversarial detection techniques [Wang\net al., 2019; Meng and Chen, 2017; Liang et al., 2018;\nZheng and Hong, 2018], in most instances, obviate the need\nfor model retraining, thereby substantially reducing the com-\nplexity of the undertaking. The detection of adversarial in-\nstances hinges on the study of the characteristics of adver-\nsarial perturbations and their statistical deviations from nor-\nmal instances. This approach enables the differentiation of\nadversarial instances during the operation of DNN models,\nthereby safeguarding them from potential adversarial attacks.\nAdversarial purification techniques [Liao et al., 2018; Liu et\nal., 2019; Jia et al., 2019], typically aim to eliminate noise\nfrom adversarial samples before they are input into the clas-\nsifier. Adversarial training techniques [Madry et al., 2017;\nTram`er et al., 2017; Pang et al., 2020], on the other hand, uti-\nlize adversarial samples as additional training data to boost\nthe model’s robustness.\n3\nMethod\n3.1\nProblem Definition\nThe task of adversarial attack involves making subtle mod-\nifications to the original image by introducing impercepti-\nble noise, to cause the target model to misclassify the re-\nsulting adversarial samples. For instance, a gradient-based\napproach generates adversarial samples by maximizing the\ncross-entropy loss function, and its optimization objective can\nbe expressed as:\narg max\nxadv\nL(fθ(xadv), y),\ns.t.∥xadv −x∥p ≤ϵ,\n(1)\nwhere xadv denote the adversarial sample, xadv = x + δ,\nwith x representing the benign sample and δ the adversar-\nial perturbation. y represents the ground truth label. fθ is\na well-trained classification model parameterized by θ, and\nL(·, ·) denotes the cross-entropy loss in the classification\ntask. ∥· ∥p represents the lp-norm. In this study, we con-\ntinue the previous work [Dong et al., 2019; Li et al., 2023;\nXie et al., 2019] and use the l∞norm to measure the size of\nthe adversarial perturbation. In this framework, the maximum\nallowable correction of the perturbation δ is controlled by ϵ,\nand the perturbation satisfies a specific constraint, that is, it\nis in the l∞sphere with radius ϵ centered on x. Specifically,\nthe disturbance δ must satisfy ∥δ∥p ≤ϵ, which ensures that\nthe adversarial perturbation does not deviate too far from the\noriginal sample x, thus ensuring that the generated adversar-\nial sample is reasonable in the input space, while maintaining\nthe effectiveness of the attack.\nHowever, the problem in Equation (1) can only be solved if\nthe classification model is directly accessible, i.e., in a white-\nbox setting. In a black-box scenario, where the target model\nfθ is not directly accessible, this method cannot be directly\napplied for attacks. In such cases, directly solving the prob-\nlem becomes infeasible. To address this limitation, a poten-\ntial solution is to generate adversarial samples on a surrogate\nmodel fφ and exploit their transferability to attack the inac-\ncessible target model fθ. Given the inherent differences be-\ntween the surrogate and target models, improving the trans-\nferability of adversarial samples generated by the surrogate\nmodel fφ is crucial. This is because the transferability of ad-\nversarial samples determines their effectiveness across differ-\nent models. In black-box attacks, for instance, highly trans-\nferable adversarial samples can successfully bypass the de-\nfenses of the target model. Therefore, the primary focus of\nthis paper is to enhance the transferability of adversarial sam-\nples generated by surrogate models.\n3.2\nInverse Knowledge Distillation Attack\nKnowledge distillation (KD) is a model compression tech-\nnique that transfers the knowledge of a large, complex model\n(referred to as the “teacher model”) to a smaller, simpler\nmodel (referred to as the “student model”), thereby reduc-\ning the computational complexity of the student model while\nmaintaining performance that is close to that of the teacher\nmodel [Hinton, 2015]. This method guides the training of the\nstudent model by learning from either the output probability\ndistribution or the intermediate representations of the teacher\nmodel.\nInspired by knowledge distillation techniques, we propose\nthe Inverse Knowledge Distillation (IKD) method. Unlike\nknowledge distillation, our IKD attack method does not in-\nvolve the concepts of teacher and student models, nor does it\nrequire the student model to learn from the teacher model. In-\nstead, our approach aims to maximize the disparity between\nthe predictive output fφ(x) of the surrogate model fφ on be-\nnign input data x and the predictive output fφ(xadv) of the\nsurrogate model on adversarial input data xadv, without com-\npromising the performance of the white-box adversarial at-\ntack. In this case, the optimization objective in Equation (1)\nbecomes the following:\narg max\nxadv\n(Lhard(fφ(xadv), y) + γ · Lsoft(fφ(xadv), fφ(x))),\n(2)\nwhere γ is the distillation weight. The objective is to min-\nimize the amount of knowledge contained in the adversarial\nsample xadv that is related to the surrogate model fφ, in or-\nder to avoid overfitting of the adversarial sample xadv to the\nsurrogate model fφ and to enhance the transferability of the\nadversarial sample.\nThe soft label loss Lsoft, defined in Equation (2), can take\nvarious common forms, such as Kullback-Leibler (KL) diver-\ngence, mean square error (MSE), and cross-entropy (CE). In\nIKD, we select KL divergence as the soft label loss, formu-\nlated as:\nLsoft(fφ(xadv), fφ(x)) = KL(fφ(x)∥fφ(xadv))\n=\nX\ni\nfφ(x)(i) log\nfφ(x)(i)\nfφ(xadv)(i),\n(3)\nThe KL divergence measures how much information is lost\nwhen fφ(xadv) is used to approximate fφ(x). It provides\nunique advantages in quantifying the difference between the\n\nprobability distributions of benign samples fφ(x) and adver-\nsarial samples fφ(xadv).\nThe primary reasons for selecting KL divergence over\nMSE or CE as the soft label loss are as follows. First, the\ngradient of KL divergence with respect to fφ(xadv) is given\nby:\n∂KL(fφ(x)∥fφ(xadv))\n∂fφ(xadv)(i)\n= −fφ(x)(i)\nfφ(xadv)(i),\n(4)\nwhich exhibits two key characteristics: asymmetric sensitiv-\nity and probabilistic emphasis. Asymmetric sensitivity refers\nto the fact that KL divergence penalizes larger deviations of\nfφ(xadv) from fφ(x) more heavily. This asymmetry is cru-\ncial for capturing directional differences in the probability\ndistributions, ensuring that adversarial samples move away\nfrom the benign sample distribution and, consequently, the\ndecision boundary of the surrogate model. Probabilistic em-\nphasis, on the other hand, means that the gradient is weighted\nby fφ(x), prioritizing high-probability regions of the benign\ndistribution. This alignment enhances the goal of modifying\nthe adversarial sample’s predictions.\nSecond, the gradient of MSE with respect to fφ(xadv) is\ngiven by:\n∂MSE(fφ(x)∥fφ(xadv))\n∂fφ(xadv)(i)\n= 2\nN (fφ(xadv) −fφ(x)). (5)\nCompared to KL divergence, MSE has two main drawbacks:\nsymmetric penalization and gradient uniformity. Symmet-\nric penalization refers to the fact that MSE treats overesti-\nmation and underestimation of probabilities symmetrically,\nwhich is suboptimal for adversarial attacks. Gradient unifor-\nmity means that MSE does not account for the relative impor-\ntance of the probability fφ(xadv). As a result, MSE applies\nuniform penalties across all categories, diminishing its effec-\ntiveness in optimizing the high-probability regions of the tar-\nget distribution. Consequently, MSE lacks the probabilistic\nemphasis and asymmetric sensitivity needed for generating\neffective adversarial attacks.\nFinally, the gradient of Cross-Entropy (CE) with respect to\nfφ(xadv) is given by:\n∂CE(fφ(x)∥fφ(xadv))\n∂fφ(xadv)(i)\n= −fφ(x)(i)\nfφ(xadv)(i).\n(6)\nAlthough both CE and KL divergence share the same gradi-\nent structure, they differ in their original forms. In CE, the\nrightmost logarithmic term is log fφ(xadv), whereas in KL\ndivergence, it is log\nfφ(x)\nfφ(xadv). This difference introduces two\nkey disadvantages for CE. First, CE cannot capture the rela-\ntionship between fφ(x) and fφ(xadv) when fφ(x) is close\nto zero. In such cases, CE fails to fully account for the de-\nviation of fφ(xadv), potentially leading to suboptimal gradi-\nent updates. Second, CE only focuses on the direction from\nfφ(xadv) to fφ(x). In contrast, the mutual penalization in KL\ndivergence ensures that both fφ(x) and fφ(xadv) are pushed\naway from each other in a balanced manner, a property that\nCE lacks.\nAdditionally, a series of experiments are conducted to val-\nidate the effectiveness of selecting KL divergence as the soft\nlabel loss, as detailed in Section 4.3.\n3.3\nEnhanced Gradient Diversity\nThe introduction of the IKD enhances the diversity of gra-\ndients used during the adversarial example generation. Tra-\nditional gradient-based attack methods, such as FGSM and\nI-FGSM, often rely on the gradient of the loss with respect to\nthe input, which can lead to similar perturbations along spe-\ncific directions of the input space.\nTo address this issue, we introduce IKD into the loss calcu-\nlation. This modification ensures that, in addition to the gra-\ndient of the classification loss with respect to the input, the\noptimization process now incorporates the gradient of the di-\nvergence between the distributions of benign and adversarial\nexamples. This additional term encourages the attack to move\nnot only along the steepest descent direction of the adversar-\nial loss but also in directions that maximize the difference be-\ntween the output distributions of the benign and adversarial\nsamples. As a result, the adversarial example is pushed to ex-\nplore a broader range of directions in the input space, leading\nto greater gradient diversity.\nThis increased gradient diversity prevents the optimization\nfrom getting stuck in local minima and allows the adversar-\nial perturbations to better generalize across different models,\nwhich is essential for improving transferability.\n3.4\nAttack Algorithm\nSimilar to previous work [Xie et al., 2019; Long et al.,\n2022], the proposed method can be seamlessly integrated\nwith any gradient-based attack technique. For instance, us-\ning MIFGSM as an example, we combine inverse knowledge\ndistillation (IKD) with MIFGSM to introduce a new attack\nmethod, named MIFGSM-IKD. In this process, inverse dis-\ntillation is incorporated into the gradient update rules of the\nMIFGSM attack to improve the transferability of adversarial\nsamples. Specifically, by introducing IKD into the loss func-\ntion, inverse distillation adjusts the gradient update direction,\nenabling the generated adversarial samples to not only effec-\ntively attack the surrogate model but also exhibit enhanced\ncross-model transferability. We propose a new adversarial\nsample update formula for the MIFGSM-IKD method, as fol-\nlows:\nxadv\n0\n= x, g0 = 0,\nLtotal = Lhard(fφ(xadv), y) + γLsoft(fφ(xadv), fφ(x)),\ngt+1 = µ · gt +\n∇x(Ltotal)\n∥∇x(Ltotal)∥1\n,\nxadv\nt+1 = xadv\nt\n+ α · sign(gt+1),\n(7)\nwhere gt and xadv\nt\nrepresent the gradient and the adversarial\nsample generated at the t-th iteration, respectively. The pa-\nrameter α denotes the attack step size and controls the mag-\nnitude of the update to the adversarial sample in each itera-\ntion. µ is an attenuation factor used to adjust the influence\nof historical information during gradient updating. ∥· ∥1 rep-\nresents the L1 norm. Thus, the IKD can be applied as an\nenhancement to the gradient-based attack method to improve\nits effectiveness in black-box settings.\n\nTable 1: Comparison results of various attack methods in terms of attack success rate (%). The bold item indicates the best one. Item with\n⋆superscript is white-box attacks, and the others is black-box attacks. AVG column indicates the average attack success rate on black-box\nmodels.\nSource Model\nMethod\nTarget Model\nRN50\nDN121\nRNX50\nVGG19BN\nIncRes-v2\nInc-v3\nInc-v4\nRN101\nRN152\nInc-v3adv\nIncRes-V2adv, ens\nAVG\nRN50\nMIFGSM\n99.9⋆\n65.7\n75.2\n73.2\n38.9\n52.6\n46.0\n68.2\n62.1\n30.6\n16.0\n52.9\nMIFGSM-IKD\n99.9⋆\n68.4\n79.4\n76.0\n39.9\n54.4\n49.6\n73.1\n67.6\n32.1\n16.3\n55.7\nDIFGSM\n99.9⋆\n82.9\n88.8\n84.7\n61.7\n70.3\n69.4\n86.1\n82.1\n35.9\n18.4\n68.0\nDIFGSM-IKD\n99.8⋆\n84.9\n90.6\n84.8\n62.9\n70.8\n69.6\n86.5\n83.3\n38.4\n18.9\n69.1\nTIFGSM\n99.0⋆\n68.4\n65.4\n71.3\n51.1\n63.9\n60.3\n55.6\n49.8\n52.8\n43.0\n58.2\nTIFGSM-IKD\n99.2⋆\n69.1\n67.1\n70.8\n52.1\n64.5\n61.4\n57.9\n49.5\n52.8\n43.8\n58.9\nNIFGSM\n99.9⋆\n71.6\n80.0\n76.5\n43.4\n55.2\n49.2\n75.9\n70.7\n31.6\n16.0\n57.0\nNIFGSM-IKD\n100.0⋆\n76.5\n85.0\n79.3\n44.6\n57.8\n52.5\n81.1\n74.7\n32.0\n15.4\n59.9\nDN121\nMIFGSM\n75.7\n100.0⋆\n71.6\n86.3\n52.2\n65.4\n58.4\n61.4\n55.4\n38.2\n20.9\n58.6\nMIFGSM-IKD\n78.3\n100.0⋆\n76.6\n88.5\n55.7\n70.9\n64.1\n68.1\n62.8\n38.6\n21.0\n62.5\nDIFGSM\n87.2\n100.0⋆\n84.5\n92.0\n74.3\n82.8\n80.2\n77.8\n72.5\n48.5\n29.0\n72.9\nDIFGSM-IKD\n89.0\n100.0⋆\n88.5\n93.9\n77.3\n82.7\n82.9\n81.9\n78.6\n48.3\n29.0\n75.2\nTIFGSM\n58.4\n99.7⋆\n52.8\n82.0\n60.4\n72.8\n66.2\n40.7\n34.1\n64.5\n54.4\n58.6\nTIFGSM-IKD\n60.5\n99.8⋆\n56.1\n81.7\n62.1\n72.4\n67.2\n41.3\n37.8\n65.7\n57.5\n60.2\nNIFGSM\n79.0\n100.0⋆\n77.9\n86.1\n55.3\n68.2\n62.1\n67.3\n60.4\n39.0\n20.4\n61.6\nNIFGSM-IKD\n82.3\n100.0⋆\n80.0\n89.4\n57.9\n70.5\n64.8\n71.9\n66.4\n39.6\n20.6\n64.3\nVGG19BN\nMIFGSM\n47.9\n58.9\n43.4\n100.0⋆\n30.1\n46.8\n42.9\n33.3\n28.9\n26.6\n15.2\n37.4\nMIFGSM-IKD\n55.2\n63.8\n50.2\n100.0⋆\n34.3\n52.5\n49.0\n39.3\n36.4\n27.1\n14.9\n42.3\nDIFGSM\n63.3\n75.0\n58.3\n100.0⋆\n46.7\n64.5\n61.2\n46.1\n43.1\n32.7\n18.6\n51.0\nDIFGSM-IKD\n71.1\n80.3\n67.9\n100.0⋆\n55.8\n69.4\n69.5\n57.4\n52.7\n35.4\n18.6\n57.8\nTIFGSM\n40.2\n59.8\n34.6\n100.0⋆\n40.5\n62.6\n56.8\n25.2\n20.9\n44.1\n33.8\n41.9\nTIFGSM-IKD\n44.8\n66.2\n42.2\n100.0⋆\n48.2\n65.9\n61.6\n30.1\n26.3\n48.8\n36.5\n47.1\nNIFGSM\n54.3\n62.9\n47.7\n100.0⋆\n34.4\n50.6\n45.2\n39.7\n33.9\n25.9\n14.2\n40.9\nNIFGSM-IKD\n57.8\n65.8\n51.5\n100.0⋆\n36.4\n53.5\n50.0\n41.8\n37.0\n26.3\n14.7\n43.5\nIncRes-v2\nMIFGSM\n54.8\n64.4\n55.2\n78.6\n98.9⋆\n74.1\n70.9\n46.4\n44.2\n44.8\n28.0\n56.1\nMIFGSM-IKD\n58.1\n68.7\n57.5\n79.5\n98.9⋆\n77.6\n75.0\n51.4\n47.8\n46.3\n30.4\n59.2\nDIFGSM\n64.8\n75.2\n65.4\n82.5\n98.6⋆\n84.5\n83.9\n58.9\n54.6\n54.7\n39.0\n66.4\nDIFGSM-IKD\n69.7\n79.4\n69.3\n85.3\n98.3⋆\n85.7\n86.5\n64.3\n58.4\n58.1\n39.5\n69.6\nTIFGSM\n40.9\n59.1\n39.4\n68.0\n97.7⋆\n74.3\n70.1\n31.5\n27.4\n67.6\n63.9\n54.2\nTIFGSM-IKD\n45.0\n60.4\n40.9\n68.6\n97.6⋆\n76.1\n73.1\n35.6\n29.3\n70.8\n68.2\n56.8\nNIFGSM\n55.8\n65.9\n56.2\n78.1\n99.3⋆\n74.1\n70.5\n48.6\n45.5\n43.3\n26.6\n56.5\nNIFGSM-IKD\n60.5\n70.3\n59.7\n80.4\n99.4⋆\n76.6\n73.3\n52.4\n50.1\n46.9\n27.0\n59.7\nRN101\nMIFGSM\n72.5\n66.3\n70.4\n74.8\n36.9\n49.3\n44.0\n97.4⋆\n70.1\n29.4\n14.5\n52.8\nMIFGSM-IKD\n75.3\n66.9\n72.4\n75.1\n37.3\n50.3\n44.4\n97.7⋆\n71.7\n30.3\n14.9\n53.9\nDIFGSM\n77.7\n75.4\n77.6\n80.6\n56.6\n63.6\n61.0\n93.2⋆\n76.6\n35.1\n17.9\n62.2\nDIFGSM-IKD\n78.5\n75.0\n80.1\n80.3\n56.0\n64.5\n61.7\n93.3⋆\n78.4\n32.3\n17.7\n62.5\nTIFGSM\n53.6\n56.0\n54.0\n64.6\n44.6\n55.0\n52.9\n77.9⋆\n49.7\n42.9\n36.3\n51.0\nTIFGSM-IKD\n53.2\n54.7\n53.2\n64.3\n44.3\n56.5\n53.5\n79.8⋆\n50.6\n43.5\n35.6\n50.9\nNIFGSM\n79.7\n72.2\n77.0\n78.2\n37.9\n51.5\n45.6\n99.2⋆\n74.1\n30.1\n14.3\n56.1\nNIFGSM-IKD\n80.3\n72.8\n77.7\n79.2\n39.9\n52.4\n47.6\n99.0⋆\n75.1\n29.5\n15.7\n57.0\n4\nExperiments\n4.1\nExperiments Setting\nDataset\nWe use a subset1 of the ImageNet dataset [Russakovsky et\nal., 2015] for the experiment. This subset consists of 1,000\nimages, covering nearly all the major categories in ImageNet,\nand has been widely used in previous related studies. In the\nexperimental setup, we choose a pixel value range of 0-255\nand set the maximum perturbation budget to 16, using the L∞\nnorm for the disturbance. Specifically, the perturbation size is\nconstrained such that the adversarial perturbation must satisfy\n∥δ∥∞≤16, ensuring that the generated adversarial sample\ndoes not deviate significantly from the original sample. Addi-\ntionally, to match the standardized input format, we adjust the\nimage resolution to 3 × 224 × 224, which complies with the\nstandard preprocessing requirements of the ImageNet dataset.\nEvaluation Models\nWe evaluate all attack methods using conventional training\nmodels and defense models. A total of 9 standard models\n1https://drive.google.com/drive/folders/\n1CfobY6i8BfqfWPHL31FKFDipNjqWwAhS\nand 2 defense models, provided by the timm package [Wight-\nman, 2019], are assessed. Specifically, the models used in-\nclude: ResNet50 [He et al., 2016a], DenseNet121 [Huang\net al., 2017], ResNeXt50 [Xie et al., 2017], VGG19BN [Si-\nmonyan, 2014], InceptionResNet-v2 [Szegedy et al., 2017],\nInception-v3 [Szegedy et al., 2016], Inception-v4 [Szegedy\net al., 2017], ResNet101 [He et al., 2016b], ResNet152 [He\net al., 2016b], Inception-v3adv [Tram`er et al., 2017] and\nInceptionResNet-v2adv,ens [Tram`er et al., 2017].\nMetrics\nTo evaluate the effectiveness of different attack methods, we\nuse the attack success rate (ASR) for both white-box and\nblack-box models as measurement indicators.\nBaselines\nTo thoroughly evaluate our proposed approach, we selected\nseveral existing baseline attack methods for comparison, in-\ncluding MIFGSM [Dong et al., 2018], DIFGSM [Xie et al.,\n2019], TIFGSM [Dong et al., 2019], NIFGSM [Lin et al.,\n2019], SINIFGSM [Lin et al., 2019], VMIFGSM [Wang and\nHe, 2021], and VNIFGSM [Wang and He, 2021].\nThese\nmethods encompass various types of gradient-based attacks,\nproviding a broad frame of reference to effectively demon-\n\nTable 2: Evaluation results of the advanced attacks combined with IKD in terms of attack success rate (%). The bold item indicates the best\none. Item with ⋆superscript is white-box attacks, and the others is black-box attacks. AVG column indicates the average attack success rate\non black-box models.\nSource Model\nMethod\nTarget Model\nRN50\nDN121\nRNX50\nVGG19BN\nIncRes-v2\nInc-v3\nInc-v4\nRN101\nRN152\nInc-v3adv\nIncRes-V2adv, ens\nAVG\nRN50\nSINIFGSM\n99.9⋆\n81.7\n85.0\n84.5\n53.2\n67.0\n59.3\n80.1\n75.4\n37.6\n17.1\n64.1\nSINIFGSM-IKD\n100.0⋆\n88.5\n91.3\n88.7\n58.0\n69.9\n64.5\n87.9\n84.1\n38.2\n17.7\n68.9\nVMIFGSM\n99.9⋆\n84.7\n88.6\n86.5\n64.8\n73.6\n73.5\n85.5\n84.9\n43.8\n24.2\n71.0\nVMIFGSM-IKD\n99.9⋆\n86.1\n89.3\n86.1\n65.6\n72.6\n72.6\n87.1\n85.3\n44.7\n23.4\n71.3\nVNIFGSM\n99.9⋆\n86.2\n89.2\n87.0\n68.7\n74.1\n73.1\n87.2\n86.0\n45.0\n23.0\n72.0\nVNIFGSM-IKD\n99.8⋆\n86.6\n91.1\n88.2\n68.7\n75.3\n74.0\n88.8\n88.0\n44.3\n24.2\n72.9\nDN121\nSINIFGSM\n81.2\n100.0⋆\n77.0\n92.7\n63.0\n77.7\n70.6\n69.8\n64.6\n47.8\n27.8\n67.2\nSINIFGSM-IKD\n87.3\n100.0⋆\n85.5\n94.2\n68.2\n79.5\n75.4\n77.0\n73.0\n49.2\n29.3\n71.9\nVMIFGSM\n87.3\n100.0⋆\n84.6\n92.6\n73.3\n82.6\n79.4\n79.5\n76.7\n49.7\n34.0\n74.0\nVMIFGSM-IKD\n88.7\n100.0⋆\n87.3\n93.8\n74.4\n82.4\n81.7\n82.5\n78.9\n50.1\n34.9\n75.5\nVNIFGSM\n87.7\n100.0⋆\n86.2\n93.5\n74.1\n83.7\n80.6\n81.1\n78.7\n49.6\n34.1\n74.9\nVNIFGSM-IKD\n89.2\n100.0⋆\n88.0\n93.9\n75.0\n82.9\n81.3\n82.1\n80.7\n49.8\n34.8\n75.8\nVGG19BN\nSINIFGSM\n55.3\n66.0\n49.5\n100.0⋆\n38.3\n57.4\n50.6\n39.3\n33.9\n31.2\n16.6\n43.8\nSINIFGSM-IKD\n61.6\n71.9\n55.5\n100.0⋆\n39.3\n59.8\n54.2\n44.7\n40.9\n28.9\n15.5\n47.2\nVMIFGSM\n69.9\n80.7\n63.5\n100.0⋆\n53.4\n70.9\n67.5\n52.3\n48.3\n37.1\n20.5\n56.4\nVMIFGSM-IKD\n73.9\n83.2\n67.6\n100.0⋆\n54.2\n71.5\n69.6\n57.4\n53.2\n37.2\n19.7\n58.8\nVNIFGSM\n71.5\n81.2\n63.7\n100.0⋆\n53.6\n71.9\n68.4\n54.0\n49.8\n37.0\n20.7\n57.2\nVNIFGSM-IKD\n76.4\n85.3\n71.5\n100.0⋆\n56.9\n74.0\n71.7\n62.1\n56.0\n39.3\n20.1\n61.3\nIncRes-v2\nSINIFGSM\n64.9\n76.1\n62.6\n85.9\n99.8⋆\n88.0\n79.6\n55.7\n52.4\n61.2\n40.9\n66.7\nSINIFGSM-IKD\n72.9\n82.8\n69.8\n90.5\n99.9⋆\n90.8\n86.4\n64.2\n62.4\n66.0\n44.6\n73.0\nVMIFGSM\n63.1\n72.3\n63.1\n81.6\n98.8⋆\n82.8\n82.4\n57.0\n56.2\n57.2\n42.7\n65.8\nVMIFGSM-IKD\n69.7\n77.0\n69.9\n85.2\n98.9⋆\n86.6\n86.6\n63.5\n63.2\n61.0\n46.9\n71.0\nVNIFGSM\n66.3\n75.1\n65.1\n82.6\n99.5⋆\n83.9\n84.5\n59.1\n56.2\n57.8\n41.9\n67.3\nVNIFGSM-IKD\n71.9\n79.1\n72.2\n85.7\n99.4⋆\n88.9\n86.7\n66.4\n64.7\n63.0\n46.3\n72.5\nRN101\nSINIFGSM\n90.7\n87.5\n90.6\n88.3\n64.4\n71.8\n71.6\n99.9⋆\n89.9\n39.0\n18.8\n71.3\nSINIFGSM-IKD\n91.4\n88.0\n90.9\n88.4\n65.1\n73.2\n71.9\n99.7⋆\n90.4\n38.1\n19.7\n71.7\nVMIFGSM\n78.9\n74.8\n77.9\n80.0\n58.4\n65.3\n64.1\n95.9⋆\n78.3\n38.2\n23.5\n63.9\nVMIFGSM-IKD\n79.2\n74.9\n78.4\n79.5\n59.0\n66.3\n64.1\n96.1⋆\n78.4\n37.9\n23.7\n64.1\nVNIFGSM\n79.9\n76.0\n80.5\n81.8\n59.9\n67.2\n64.3\n96.8⋆\n80.4\n37.6\n24.0\n65.2\nVNIFGSM-IKD\n81.5\n78.1\n81.4\n81.3\n60.7\n67.5\n65.2\n96.8⋆\n80.7\n39.8\n23.3\n66.0\nstrate the performance of our approach against multiple attack\nstrategies.\nImplementation Details\nTo ensure the comparability and consistency of the experi-\nments, we set the maximum permissible perturbation ϵ = 16,\nthe number of iterations T = 10, the attack step size α =\n2\n255,\nand the momentum term attenuation factor µ = 1.0, follow-\ning previous research [Dong et al., 2019; Wang and He, 2021;\nXie et al., 2019]. These parameter settings are consistent\nwith those commonly used in the literature, enabling a fair\ncomparison of different attack methods. To facilitate the im-\nplementation and comparison of attack strategies, we employ\nthe attack toolkit Torchattacks [Kim, 2020] and retain its de-\nfault parameters, with the exception of the custom settings for\nϵ and T. This ensures that all methods are compared within\nthe same attack framework, minimizing the influence of other\nfactors on the experimental outcomes. All experiments are\nimplemented in the PyTorch framework and executed on one\nNVIDIA GeForce RTX 3090 GPU.\n4.2\nExperiments Results\nIn this section, we integrate the proposed Inverse Knowl-\nedge Distillation (IKD) method with existing attack strate-\ngies to investigate potential improvements in attack per-\nformance.\nThe resulting combined methods are denoted\nby the suffix “-IKD”, such as MIFGSM-IKD. The com-\nparison methods include classical attacks (e.g., MIFGSM\nand NIFGSM), attacks based on input transformations (e.g.,\nDIFGSM and TIFGSM), and advanced gradient-based at-\ntacks (e.g., SINIFGSM, VMIFGSM, and VNIFGSM).\nCombination with classical attacks and attacks based on\ninput transformations\nAs shown in Table 1, our method outperforms the comparison\nmethods on the vast majority of black-box models. Specifi-\ncally, we observe a significant improvement in average trans-\nferability across almost all eleven models tested. Notably,\nour method achieves a 6.8% increase in average transferabil-\nity compared to DIFGSM on the VGG19BN model, with\nour DIFGSM-IKD method attaining an average transferabil-\nity of 57.8%, while DIFGSM alone achieves 51.0%. Fur-\nthermore, our approach also outperforms most comparison\nmethods under defense models in terms of average attack suc-\ncess rate, demonstrating the effectiveness of our method in\nattacking defensive mechanisms. Additionally, we find that\nDenseNet121 is the most vulnerable model, exhibiting the\nhighest average transferability, which challenges the conven-\ntional belief that deeper models are inherently more robust\nthan shallower ones. This observation suggests that model\nrobustness is more closely linked to architectural design than\nto depth alone. Finally, we acknowledge that in some cases,\nour method may show slightly lower performance than com-\nparison methods in either white-box or black-box settings.\nThis may be due to the introduction of the IKD, which could\nslightly interfere with the generation of adversarial perturba-\ntions. Additional experimental results can be found in the\nsupplementary material.\nCombination with Advanced gradient Attack\nIn Table 2, we examine the potential enhancement of at-\ntack performance by integrating our approach with advanced\ngradient-based attacks.\nOn one hand, our method in-\n\ncreases the average transferability of the original attack in\nthe vast majority of cases. Specifically, we observe a max-\nimum improvement of 6.3% when using SINIFGSM on the\nInceptionResNet-v2 model.\nOn the other hand, the per-\nformance improvement varies across different attacks and\nmodels. For instance, among the eleven models, ResNet50\nexhibits the largest increase in attack performance, with a\n4.5% improvement.\nSINIFGSM-IKD achieves a 4.8% in-\ncrease, while VMIFGSM-IKD shows a modest 0.3% im-\nprovement.\nIn contrast, the smallest difference in perfor-\nmance improvement is observed with ResNet101, where the\nimprovement is only 0.6%. Additionally, VNIFGSM-IKD\ndemonstrates the largest performance improvement of 3.4%\nagainst DenseNet121 (the best-performing model), and a\nsmaller improvement of 1.6% against VGG19BN (the least\nimproved model) among the four attacks.\nIn comparison,\nVMIFGSM-IKD shows the smallest variation in performance\nimprovement, with a 0.8% difference between VGG19BN\n(2.3%) and DenseNet121 (1.5%). This discrepancy may be\nattributed to the fact that VMIFGSM has less room for im-\nprovement relative to other attacks. Additional experimental\nresults can be found in the supplementary material.\n4.3\nAblation Study\nTo thoroughly investigate the potential factors that may influ-\nence the performance of our method, we conduct two ablation\nexperiments in this section: (1) the effect of IKD’s soft label\nloss selection in inverse knowledge distillation, and (2) the\nimpact of the weight of the IKD.\nMIFGSM\nDIFGSM\nTIFGSM\nNIFGSM\nIKD Method\n0\n10\n20\n30\n40\n50\n60\n70\nAttack Success Rate (%)\nw/o IKD\nMSE\nCE\nKL\n(a) Classical attacks and input\ntransformation-based attacks.\nSINIFGSM\nVMIFGSM\nVNIFGSM\nIKD Method\n0\n10\n20\n30\n40\n50\n60\n70\nAttack Success Rate (%)\nw/o IKD\nMSE\nCE\nKL\n(b) Advanced gradient attacks.\nFigure 2: Effect of different IKD methods on the transfer-based av-\nerage attack success rate.\nInfluence of soft label loss selection on IKD\nTo investigate the effect of different types of IKD’s soft\nlabel loss on adversarial transferability, we employ Mean\nSquared Error (MSE), Cross-Entropy (CE), and Kullback-\nLeibler (KL) Divergence for inverse knowledge distillation.\nSpecifically, we conduct experiments on ResNet50 (RN50)\nusing MSE, CE, and KL divergence as the soft label loss in\nIKD. The average evaluation results are presented in Figure 2.\nFrom the results, it is evident that KL divergence demon-\nstrates superior adversarial transferability on RN50 compared\nto the other two methods. For RN50, the mean transferabil-\nity with MSE, CE, and KL divergence as the IKD’s soft label\n0.001\n0.01\n0.1\n1\n10\n100\n1000\nIKD Weight\n45\n50\n55\n60\n65\n70\nAttack Success Rate (%)\nMIFGSM\nMIFGSM-IKD\nNIFGSM\nNIFGSM-IKD\nTIFGSM\nTIFGSM-IKD\nNIFGSM\nNIFGSM-IKD\n(a) Classical attacks and input\ntransformation-based attacks.\n0.001\n0.01\n0.1\n1\n10\n100\n1000\nIKD Weight\n64\n66\n68\n70\n72\nAttack Success Rate (%)\nSINIFGSM\nSINIFGSM-IKD\nVMIFGSM\nVMIFGSM-IKD\nVNIFGSM\nVNIFGSM-IKD\n(b) Advanced gradient attacks.\nFigure 3: Effect of IKD weight on the transfer-based average attack\nsuccess rate.\nloss were 63.2%, 63.4%, and 65.2%, respectively. The trans-\nferability across different soft label loss varies significantly.\nBased on these findings, we opt to use KL divergence as the\nsoft label loss for IKD. For more detailed experimental re-\nsults, please refer to the supplementary material.\nInfluence of weight of IKD\nThe inverse knowledge distillation method exhibits varying\ninfluences on the gradient direction based on the weight pa-\nrameter, which in turn affects the adversarial transferability.\nTo investigate this relationship, we perform a grid search\nover the weight parameter γ of inverse knowledge distilla-\ntion in the range {0.001, 0.01, 0.1, 1, 10, 100, 1000}, using\nResNet50 (RN50) as the surrogate model. The average eval-\nuation results are presented in Figure 3. As observed, with\nthe increase in the IKD weight, the average transferability of\nRN50 initially remains stable within a certain range, and then\ngradually declines. This suggests that the inverse knowledge\ndistillation method proposed in this paper is not confined to\na specific weight value. Based on these findings, we select\nγ = 0.01 as the weight for the inverse knowledge distillation\nattack. For more detailed experimental results, please refer to\nthe supplementary material.\n5\nConclusion\nIn this paper, we propose an inverse knowledge distillation\n(IKD) method aimed at significantly improving the transfer-\nability of adversarial samples. Specifically, we enhance the\ndiversity of attack gradients by incorporating the IKD’s soft\nlabel loss into the model’s loss function. IKD not only en-\ncourages the model to focus on the classification error of\nthe target label during adversarial sample generation but also\noptimizes the gradient update direction through the inverse\nknowledge distillation process. As a result, our approach ef-\nfectively mitigates overfitting and enhances the transferabil-\nity of adversarial samples. The proposed IKD method in-\ntegrates seamlessly with existing gradient-based attack tech-\nniques, thereby boosting attack performance. Experimental\nresults demonstrate the efficacy of the proposed method, with\nextensive evaluations on the ImageNet dataset confirming sig-\nnificant improvements in attack success rates across several\nbaseline attack strategies.\n\nReferences\n[Chen et al., 2022] Yong Chen, Xu Wang, Peng Hu, and\nDezhong Peng. Magicgan: multiagent attacks generate in-\nterferential category via gan. Knowledge-Based Systems,\n258:110023, 2022.\n[Chen et al., 2023] Yong Chen, Xu Wang, Peng Hu, Zhong\nYuan, Dezhong Peng, and Qilin Li. Learning relationship-\npreserving representation for multi-task adversarial at-\ntacks. Neurocomputing, 554:126580, 2023.\n[Dong et al., 2018] Yinpeng Dong, Fangzhou Liao, Tianyu\nPang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li.\nBoosting adversarial attacks with momentum. In Proceed-\nings of the IEEE conference on computer vision and pat-\ntern recognition, pages 9185–9193, 2018.\n[Dong et al., 2019] Yinpeng Dong, Tianyu Pang, Hang Su,\nand Jun Zhu. Evading defenses to transferable adversarial\nexamples by translation-invariant attacks. In Proceedings\nof the IEEE/CVF conference on computer vision and pat-\ntern recognition, pages 4312–4321, 2019.\n[Ganeshan et al., 2019] Aditya Ganeshan, Vivek BS, and\nR Venkatesh Babu.\nFda: Feature disruptive attack.\nIn\nProceedings of the IEEE/CVF International Conference\non Computer Vision, pages 8069–8079, 2019.\n[Goodfellow et al., 2014] Ian\nJ\nGoodfellow,\nJonathon\nShlens, and Christian Szegedy. Explaining and harnessing\nadversarial examples.\narXiv preprint arXiv:1412.6572,\n2014.\n[He et al., 2016a] Kaiming He, Xiangyu Zhang, Shaoqing\nRen, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 770–778, 2016.\n[He et al., 2016b] Kaiming He, Xiangyu Zhang, Shaoqing\nRen, and Jian Sun.\nIdentity mappings in deep residual\nnetworks. In Computer Vision–ECCV 2016: 14th Euro-\npean Conference, Amsterdam, The Netherlands, October\n11–14, 2016, Proceedings, Part IV 14, pages 630–645.\nSpringer, 2016.\n[Hinton, 2015] Geoffrey Hinton. Distilling the knowledge in\na neural network. arXiv preprint arXiv:1503.02531, 2015.\n[Huang et al., 2017] Gao Huang, Zhuang Liu, Laurens Van\nDer Maaten, and Kilian Q Weinberger. Densely connected\nconvolutional networks. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pages\n4700–4708, 2017.\n[Jia et al., 2019] Xiaojun Jia, Xingxing Wei, Xiaochun Cao,\nand Hassan Foroosh. Comdefend: An efficient image com-\npression model to defend adversarial examples. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 6084–6092, 2019.\n[Joshi et al., 2019] Ameya Joshi, Amitangshu Mukherjee,\nSoumik Sarkar, and Chinmay Hegde.\nSemantic adver-\nsarial attacks: Parametric transformations that fool deep\nclassifiers. In Proceedings of the IEEE/CVF international\nconference on computer vision, pages 4773–4783, 2019.\n[Kim, 2020] Hoki Kim. Torchattacks: A pytorch repository\nfor adversarial attacks. arXiv preprint arXiv:2010.01950,\n2020.\n[Li et al., 2023] Chao Li, Wen Yao, Handing Wang, and\nTingsong Jiang.\nAdaptive momentum variance for\nattention-guided sparse adversarial attacks. Pattern Recog-\nnition, 133:108979, 2023.\n[Liang and Xiao, 2023] Kaisheng Liang and Bin Xiao. Sty-\nless: boosting the transferability of adversarial examples.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 8163–8172, 2023.\n[Liang et al., 2018] Bin Liang, Hongcheng Li, Miaoqiang\nSu, Xirong Li, Wenchang Shi, and Xiaofeng Wang. De-\ntecting adversarial image examples in deep neural net-\nworks with adaptive noise reduction.\nIEEE Transac-\ntions on Dependable and Secure Computing, 18(1):72–85,\n2018.\n[Liao et al., 2018] Fangzhou Liao, Ming Liang, Yinpeng\nDong, Tianyu Pang, Xiaolin Hu, and Jun Zhu. Defense\nagainst adversarial attacks using high-level representation\nguided denoiser. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 1778–\n1787, 2018.\n[Lin et al., 2019] Jiadong Lin, Chuanbiao Song, Kun He, Li-\nwei Wang, and John E Hopcroft. Nesterov accelerated gra-\ndient and scale invariance for adversarial attacks. arXiv\npreprint arXiv:1908.06281, 2019.\n[Liu et al., 2019] Zihao Liu, Qi Liu, Tao Liu, Nuo Xu, Xue\nLin, Yanzhi Wang, and Wujie Wen. Feature distillation:\nDnn-oriented jpeg compression against adversarial exam-\nples. In 2019 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 860–868. IEEE,\n2019.\n[Long et al., 2022] Yuyang Long, Qilong Zhang, Boheng\nZeng, Lianli Gao, Xianglong Liu, Jian Zhang, and\nJingkuan Song. Frequency domain model augmentation\nfor adversarial attack. In European conference on com-\nputer vision, pages 549–566. Springer, 2022.\n[Madry et al., 2017] Aleksander\nMadry,\nAleksandar\nMakelov,\nLudwig\nSchmidt,\nDimitris\nTsipras,\nand\nAdrian Vladu.\nTowards deep learning models resistant\nto adversarial attacks. arXiv preprint arXiv:1706.06083,\n2017.\n[Meng and Chen, 2017] Dongyu Meng and Hao Chen. Mag-\nnet: a two-pronged defense against adversarial examples.\nIn Proceedings of the 2017 ACM SIGSAC conference on\ncomputer and communications security, pages 135–147,\n2017.\n[Nesterov, 1983] Yurii Nesterov.\nA method for uncon-\nstrained convex minimization problem with the rate of\nconvergence o (1/k2). In Dokl. Akad. Nauk. SSSR, volume\n269, page 543, 1983.\n[Pang et al., 2020] Tianyu Pang, Xiao Yang, Yinpeng Dong,\nKun Xu, Jun Zhu, and Hang Su.\nBoosting adversarial\ntraining with hypersphere embedding. Advances in Neural\nInformation Processing Systems, 33:7779–7792, 2020.\n\n[Qian et al., 2023] Yaguan Qian, Shuke He, Chenyu Zhao,\nJiaqiang Sha,\nWei Wang,\nand Bin Wang.\nLea2:\nA lightweight ensemble adversarial attack via non-\noverlapping vulnerable frequency regions. In Proceedings\nof the IEEE/CVF International Conference on Computer\nVision, pages 4510–4521, 2023.\n[Qiu et al., 2020] Haonan Qiu, Chaowei Xiao, Lei Yang,\nXinchen Yan, Honglak Lee, and Bo Li.\nSemanticadv:\nGenerating adversarial examples via attribute-conditioned\nimage editing. In Computer Vision–ECCV 2020: 16th Eu-\nropean Conference, Glasgow, UK, August 23–28, 2020,\nProceedings, Part XIV 16, pages 19–37. Springer, 2020.\n[Russakovsky et al., 2015] Olga Russakovsky,\nJia Deng,\nHao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,\nZhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael\nBernstein, et al.\nImagenet large scale visual recogni-\ntion challenge. International journal of computer vision,\n115:211–252, 2015.\n[Simonyan, 2014] Karen Simonyan.\nVery deep convolu-\ntional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014.\n[Song et al., 2018] Yang Song, Rui Shu, Nate Kushman, and\nStefano Ermon. Constructing unrestricted adversarial ex-\namples with generative models. Advances in neural infor-\nmation processing systems, 31, 2018.\n[Szegedy et al., 2013] Christian\nSzegedy,\nWojciech\nZaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian\nGoodfellow, and Rob Fergus.\nIntriguing properties of\nneural networks. arXiv preprint arXiv:1312.6199, 2013.\n[Szegedy et al., 2016] Christian\nSzegedy,\nVincent\nVan-\nhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 2818–2826, 2016.\n[Szegedy et al., 2017] Christian Szegedy, Sergey Ioffe, Vin-\ncent Vanhoucke, and Alexander Alemi.\nInception-v4,\ninception-resnet and the impact of residual connections on\nlearning. In Proceedings of the AAAI conference on artifi-\ncial intelligence, volume 31, 2017.\n[Tram`er et al., 2017] Florian Tram`er, Alexey Kurakin, Nico-\nlas Papernot, Ian Goodfellow, Dan Boneh, and Patrick Mc-\nDaniel. Ensemble adversarial training: Attacks and de-\nfenses. arXiv preprint arXiv:1705.07204, 2017.\n[Wang and He, 2021] Xiaosen Wang and Kun He. Enhanc-\ning the transferability of adversarial attacks through vari-\nance tuning. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 1924–\n1933, 2021.\n[Wang et al., 2019] Jingyi Wang, Guoliang Dong, Jun Sun,\nXinyu Wang, and Peixin Zhang. Adversarial sample de-\ntection for deep neural network through model mutation\ntesting. In 2019 IEEE/ACM 41st International Conference\non Software Engineering (ICSE), pages 1245–1256. IEEE,\n2019.\n[Wang et al., 2021a] Xiaosen Wang, Xuanran He, Jingdong\nWang, and Kun He. Admix: Enhancing the transferability\nof adversarial attacks. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, pages 16158–\n16167, 2021.\n[Wang et al., 2021b] Xiaosen Wang, Jiadong Lin, Han Hu,\nJingdong Wang, and Kun He. Boosting adversarial trans-\nferability through enhanced momentum.\narXiv preprint\narXiv:2103.10609, 2021.\n[Wang et al., 2023] Donghua Wang, Wen Yao, Tingsong\nJiang, and Xiaoqian Chen. Improving transferability of\nuniversal adversarial perturbation with feature disruption.\nIEEE Transactions on Image Processing, 2023.\n[Wightman, 2019] Ross Wightman.\nPytorch image mod-\nels. https://github.com/rwightman/pytorch-image-models,\n2019.\n[Xiao et al., 2021] Zihao Xiao, Xianfeng Gao, Chilin Fu,\nYinpeng Dong, Wei Gao, Xiaolu Zhang, Jun Zhou, and\nJun Zhu. Improving transferability of adversarial patches\non face recognition with generative models. In Proceed-\nings of the IEEE/CVF conference on computer vision and\npattern recognition, pages 11845–11854, 2021.\n[Xie et al., 2017] Saining Xie, Ross Girshick, Piotr Doll´ar,\nZhuowen Tu, and Kaiming He. Aggregated residual trans-\nformations for deep neural networks. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 1492–1500, 2017.\n[Xie et al., 2019] Cihang Xie, Zhishuai Zhang, Yuyin Zhou,\nSong Bai, Jianyu Wang, Zhou Ren, and Alan L Yuille.\nImproving transferability of adversarial examples with in-\nput diversity. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 2730–\n2739, 2019.\n[Xiong et al., 2022] Yifeng Xiong, Jiadong Lin, Min Zhang,\nJohn E Hopcroft, and Kun He. Stochastic variance reduced\nensemble adversarial attack for boosting the adversarial\ntransferability. In Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition, pages\n14983–14992, 2022.\n[Yuan et al., 2024] Zheng Yuan, Jie Zhang, Zhaoyan Jiang,\nLiangliang Li, and Shiguang Shan. Adaptive perturbation\nfor adversarial attack. IEEE Transactions on Pattern Anal-\nysis and Machine Intelligence, 2024.\n[Zhang et al., 2022] Jianping Zhang, Weibin Wu, Jen-tse\nHuang, Yizhan Huang, Wenxuan Wang, Yuxin Su, and\nMichael R Lyu. Improving adversarial transferability via\nneuron attribution-based attacks.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 14993–15002, 2022.\n[Zhao et al., 2018] Zhengli Zhao, Dheeru Dua, and Sameer\nSingh. Generating natural adversarial examples. In Inter-\nnational Conference on Learning Representations, 2018.\n[Zheng and Hong, 2018] Zhihao Zheng and Pengyu Hong.\nRobust detection of adversarial attacks by modeling the\nintrinsic properties of deep neural networks. Advances in\nneural information processing systems, 31, 2018.\n\nSupplementary Material\nA\nAlgorithm\nTaking MIFGSM as an example, the specific algorithmic\nworkflow of MIFGSM-IKD is detailed in\nAlgorithm 1.\nBy incorporating the momentum term and an appropriate\nIKD strategy, adversarial samples are iteratively updated to\nachieve effective attacks on the target model.\nAlgorithm 1 Example algorithm\nInput: A classifier f with loss function L; a benign sample x\nand ground-truth label y;\nInput: The size of perturbation ϵ; iterations T and decay fac-\ntor µ\nOutput: An adversarial sample xadv with ∥xadv −x∥∞≤\nϵ.\n1: α = ϵ\nT ;\n2: g0 = 0; xadv\n0\n= x;\n3: for t = 0 to T −1 do\n4:\nInput xadv\nt\nto f and obtain the hard label loss\nLhard(fφ(xadv), y);\n5:\nInput xadv\nt\nand x to f and obtain the soft label loss\nLsoft(fφ(xadv), fφ(x));\n6:\nCalculate the total loss\nLtotal = Lhard(fφ(xadv), y)+γLsoft(fφ(xadv), fφ(x));\n7:\nObtain the gradient ∇x(Ltotal);\n8:\nUpdate gt+1 by accumulating the velocity vector in the\ngradient direction as\ngt+1 = µ · gt +\n∇x(Ltotal)\n∥∇x(Ltotal)∥1\n;\n9:\nUpdate xadv\nt+1 by applying the sign gradient as\nxadv\nt+1 = xadv\nt\n+ α · sign(gt+1);\n10: end for\n11: return xadv = xadv\nt\n.\nB\nExperiments\nTable 3 demonstrates the effects of IKD on classical and\ninput transformation-based attacks when using ResNeXt50,\nInception-v3, Inception-v4, and ResNet152 as surrogate\nmodels.\nTable 4 illustrates the impact of IKD on advanced gra-\ndient attacks using ResNeXt50, Inception-v3, Inception-v4,\nand ResNet152 as surrogate models.\nTable 5 presents the impact of different IKD methods on\nthe success rate of transfer-based attacks, with ResNet50\nserving as the surrogate model.\nTables 6 and 7 presents the effect of IKD weight on the\ntransfer-based attack success rate.\n\nTable 3: Comparison results of various attack methods in terms of attack success rate (%). The bold item indicates the best one. Item with\n⋆superscript is white-box attacks, and the others is black-box attacks. AVG column indicates the average attack success rate on black-box\nmodels.\nSource Model\nMethod\nTarget Model\nRN50\nDN121\nRNX50\nVGG19BN\nIncRes-v2\nInc-v3\nInc-v4\nRN101\nRN152\nInc-v3adv\nIncRes-V2adv, ens\nAVG\nRNX50\nMIFGSM\n68.2\n60.6\n98.7⋆\n72.8\n31.3\n46.1\n40.1\n61.8\n55.2\n35.7\n15.5\n48.7\nMIFGSM-IKD\n72.0\n63.3\n98.9⋆\n72.2\n34.0\n45.1\n41.6\n64.8\n57.9\n35.9\n14.9\n50.2\nDIFGSM\n79.0\n75.9\n94.6⋆\n78.5\n52.2\n59.7\n62.1\n75.8\n71.9\n36.3\n16.1\n60.8\nDIFGSM-IKD\n81.3\n76.3\n96.1⋆\n81.9\n55.1\n60.2\n62.0\n77.2\n73.1\n37.4\n17.5\n62.2\nTIFGSM\n54.6\n58.0\n85.6⋆\n67.5\n42.2\n54.6\n52.1\n45.8\n41.2\n42.0\n33.1\n49.1\nTIFGSM-IKD\n54.6\n56.6\n85.5⋆\n68.0\n42.6\n55.0\n52.2\n45.2\n41.1\n41.8\n35.6\n49.3\nNIFGSM\n74.8\n67.8\n99.9⋆\n73.9\n35.5\n47.0\n46.3\n67.4\n61.0\n37.6\n14.8\n52.6\nNIFGSM-IKD\n78.5\n70.2\n99.8⋆\n77.7\n37.1\n49.3\n44.2\n69.6\n63.8\n36.0\n13.6\n54.0\nInc-v3\nMIFGSM\n47.7\n60.9\n44.7\n72.3\n58.3\n99.1⋆\n61.8\n38.3\n35.9\n41.5\n21.4\n48.3\nMIFGSM-IKD\n48.6\n60.0\n45.1\n72.4\n58.1\n99.7⋆\n62.8\n38.6\n36.6\n41.8\n22.6\n48.7\nDIFGSM\n56.6\n68.6\n53.3\n78.7\n70.1\n99.1⋆\n73.8\n47.2\n42.0\n49.5\n26.9\n56.7\nDIFGSM-IKD\n57.8\n69.5\n56.1\n79.8\n74.9\n99.3⋆\n77.2\n50.9\n45.2\n54.0\n29.0\n59.4\nTIFGSM\n32.7\n49.8\n28.6\n64.2\n51.2\n98.8⋆\n60.0\n21.0\n18.8\n56.0\n42.4\n42.5\nTIFGSM-IKD\n33.9\n50.4\n30.9\n66.6\n55.9\n98.7⋆\n62.4\n23.0\n20.3\n60.3\n44.9\n44.9\nNIFGSM\n55.4\n69.4\n52.0\n76.5\n64.9\n99.4⋆\n70.0\n47.7\n40.8\n42.2\n21.6\n54.1\nNIFGSM-IKD\n57.6\n71.5\n56.3\n78.4\n69.5\n99.8⋆\n72.7\n48.1\n45.6\n44.3\n23.0\n56.7\nInc-v4\nMIFGSM\n53.3\n62.4\n52.8\n76.5\n60.4\n69.5\n97.9⋆\n45.0\n42.6\n37.1\n20.4\n52.0\nMIFGSM-IKD\n54.1\n64.2\n53.7\n78.9\n62.2\n71.8\n99.0⋆\n48.3\n44.7\n38.4\n20.1\n53.6\nDIFGSM\n62.9\n70.9\n62.1\n82.4\n76.2\n79.8\n98.0⋆\n55.6\n52.4\n45.2\n25.6\n61.3\nDIFGSM-IKD\n66.2\n74.0\n66.7\n84.4\n76.2\n81.6\n98.1⋆\n59.4\n55.7\n47.7\n26.7\n63.9\nTIFGSM\n39.3\n51.4\n37.2\n67.7\n55.9\n67.5\n97.7⋆\n27.8\n24.1\n53.1\n44.5\n46.9\nTIFGSM-IKD\n40.0\n52.2\n38.0\n69.1\n57.8\n70.1\n97.1⋆\n28.9\n25.7\n55.7\n44.2\n48.2\nNIFGSM\n58.5\n66.4\n58.3\n82.5\n61.6\n72.3\n98.9⋆\n49.6\n45.3\n37.4\n18.9\n55.1\nNIFGSM-IKD\n60.8\n69.1\n60.2\n82.6\n65.7\n74.8\n99.8⋆\n52.7\n48.9\n39.3\n20.7\n57.5\nRN152\nMIFGSM\n69.6\n62.9\n66.3\n74.1\n35.9\n48.7\n42.2\n68.8\n96.2⋆\n26.2\n14.8\n51.0\nMIFGSM-IKD\n71.2\n63.5\n68.8\n74.8\n38.1\n49.7\n44.7\n70.3\n96.4⋆\n27.4\n14.9\n52.3\nDIFGSM\n75.2\n72.3\n75.5\n79.0\n55.4\n62.7\n60.5\n78.0\n91.2⋆\n33.7\n17.6\n61.0\nDIFGSM-IKD\n76.3\n72.0\n77.6\n78.0\n55.2\n62.5\n60.8\n78.7\n91.8⋆\n32.9\n17.8\n61.2\nTIFGSM\n49.4\n52.4\n50.5\n64.1\n42.1\n54.5\n49.0\n49.4\n74.4⋆\n42.6\n36.8\n49.1\nTIFGSM-IKD\n50.8\n51.4\n51.6\n62.8\n42.2\n53.2\n50.0\n51.0\n74.9⋆\n42.4\n35.1\n49.1\nNIFGSM\n77.3\n68.0\n74.5\n77.4\n37.0\n51.1\n44.3\n76.4\n99.1⋆\n27.4\n15.6\n54.9\nNIFGSM-IKD\n78.8\n70.7\n75.0\n79.5\n38.4\n51.0\n45.6\n77.5\n99.1⋆\n26.6\n15.1\n55.8\nTable 4: Evaluation results of the advanced attacks combined with IKD in terms of attack success rate (%). The bold item indicates the best\none. Item with ⋆superscript is white-box attacks, and the others is black-box attacks. AVG column indicates the average attack success rate\non black-box models.\nSource Model\nMethod\nTarget Model\nRN50\nDN121\nRNX50\nVGG19BN\nIncRes-v2\nInc-v3\nInc-v4\nRN101\nRN152\nInc-v3adv\nIncRes-V2adv, ens\nAVG\nRNX50\nSINIFGSM\n91.0\n88.5\n99.9⋆\n86.8\n61.2\n70.0\n70.1\n87.5\n82.9\n39.3\n18.5\n69.6\nSINIFGSM-IKD\n91.7\n87.6\n99.9⋆\n87.0\n61.9\n69.8\n68.9\n87.9\n85.0\n38.2\n18.3\n69.6\nVMIFGSM\n81.5\n77.8\n97.1⋆\n81.1\n59.2\n66.3\n66.9\n77.9\n75.3\n40.0\n23.9\n65.0\nVMIFGSM-IKD\n81.4\n78.1\n98.1⋆\n80.7\n59.2\n67.2\n67.5\n78.8\n76.6\n39.8\n23.0\n65.2\nVNIFGSM\n82.4\n79.8\n97.1⋆\n82.5\n61.9\n68.8\n68.1\n78.8\n77.2\n41.9\n23.5\n66.5\nVNIFGSM-IKD\n82.0\n79.5\n98.0⋆\n80.9\n60.6\n70.3\n66.9\n78.4\n77.1\n40.3\n22.3\n65.8\nInc-v3\nSINIFGSM\n56.3\n71.2\n53.5\n83.4\n67.6\n100.0⋆\n72.5\n45.7\n42.6\n50.2\n26.2\n56.9\nSINIFGSM-IKD\n62.4\n74.4\n57.7\n84.4\n74.7\n100.0⋆\n78.1\n50.9\n47.1\n54.7\n25.3\n61.0\nVMIFGSM\n55.8\n68.8\n53.9\n78.2\n73.0\n99.4⋆\n73.4\n48.7\n43.4\n52.9\n29.5\n57.8\nVMIFGSM-IKD\n58.9\n70.7\n54.4\n79.2\n75.0\n99.4⋆\n78.4\n51.3\n47.5\n56.7\n30.4\n60.3\nVNIFGSM\n61.3\n72.2\n57.5\n80.2\n75.2\n99.7⋆\n77.7\n50.4\n47.4\n53.6\n31.3\n60.7\nVNIFGSM-IKD\n64.7\n76.4\n61.9\n82.9\n79.9\n100.0⋆\n83.2\n56.4\n52.5\n59.0\n33.1\n65.0\nInc-v4\nSINIFGSM\n67.3\n79.6\n66.0\n90.0\n80.2\n87.9\n100.0⋆\n61.3\n57.5\n59.3\n34.9\n68.4\nSINIFGSM-IKD\n73.6\n83.0\n71.3\n92.3\n85.7\n90.2\n99.9⋆\n66.2\n62.7\n61.0\n35.0\n72.1\nVMIFGSM\n60.9\n69.7\n60.0\n81.3\n73.6\n78.9\n98.6⋆\n55.9\n54.5\n50.5\n29.7\n61.5\nVMIFGSM-IKD\n65.7\n73.1\n65.6\n83.4\n79.6\n82.9\n98.5⋆\n60.2\n60.3\n52.0\n31.3\n65.4\nVNIFGSM\n65.3\n73.5\n63.8\n85.0\n77.6\n82.2\n99.5⋆\n56.4\n55.6\n52.1\n29.8\n64.1\nVNIFGSM-IKD\n70.3\n77.0\n70.7\n87.3\n82.1\n85.3\n99.8⋆\n64.6\n62.3\n53.5\n31.9\n68.5\nRN152\nSINIFGSM\n91.1\n87.0\n88.4\n87.4\n65.1\n71.8\n70.7\n91.4\n99.5⋆\n40.4\n20.1\n71.3\nSINIFGSM-IKD\n91.5\n88.1\n89.2\n86.4\n63.6\n71.9\n69.6\n90.7\n99.2⋆\n39.0\n19.3\n70.9\nVMIFGSM\n78.5\n72.0\n76.8\n79.0\n58.1\n63.5\n62.9\n77.0\n95.8⋆\n40.0\n23.4\n63.1\nVMIFGSM-IKD\n78.6\n72.2\n77.7\n78.8\n57.6\n64.7\n62.0\n78.0\n95.7⋆\n39.7\n23.7\n63.3\nVNIFGSM\n81.6\n76.4\n81.2\n82.0\n61.1\n65.7\n66.2\n80.3\n97.4⋆\n40.0\n24.0\n65.9\nVNIFGSM-IKD\n82.1\n78.0\n81.3\n82.1\n61.0\n67.5\n65.9\n81.5\n98.0⋆\n39.9\n24.8\n66.4\n\nTable 5: Comparison results of various attack methods under different IKD methods regarding attack success rate (%). The bold item indicates\nthe best, and the underlined item indicates the suboptimal. An item with ⋆superscript is a white-box attack; the others are black-box attacks.\nThe AVG column indicates the average attack success rate on black-box models. The source model is ResNet50 and the IKD weight is 0.01.\nMethod\nTarget Model\nRN50\nDN121\nRNX50\nVGG19BN\nIncRes-v2\nInc-v3\nInc-v4\nRN101\nRN152\nInc-v3adv\nIncRes-V2adv, ens\nMIFGSM\n99.9*\n65.7\n75.2\n73.2\n38.9\n52.6\n46.0\n68.2\n62.1\n30.6\n16.0\nMIFGSM-IKD(CE)\n99.9*\n66.6\n74.2\n73.0\n38.8\n53.6\n46.0\n69.4\n63.6\n30.7\n16.2\nMIFGSM-IKD(MSE)\n99.9*\n65.4\n74.1\n73.6\n37.2\n53.2\n46.2\n68.0\n62.4\n31.3\n15.9\nMIFGSM-IKD(KL)\n99.9*\n68.4\n79.4\n76.0\n39.9\n54.4\n49.6\n73.1\n67.6\n32.1\n16.3\nDIFGSM\n99.9*\n82.9\n88.8\n84.7\n61.7\n70.3\n69.4\n86.1\n82.1\n35.9\n18.4\nDIFGSM-IKD(CE)\n99.8*\n83.9\n88.9\n83.4\n61.0\n70.5\n68.4\n83.5\n80.2\n38.1\n18.4\nDIFGSM-IKD(MSE)\n99.8*\n83.8\n88.4\n83.4\n60.4\n70.1\n69.0\n83.9\n80.7\n36.2\n19.0\nDIFGSM-IKD(KL)\n99.8*\n84.9\n90.6\n84.8\n62.9\n70.8\n69.6\n86.5\n83.3\n38.4\n18.9\nTIFGSM\n99.0*\n68.4\n65.4\n71.3\n51.1\n63.9\n60.3\n55.6\n49.8\n52.8\n43.0\nTIFGSM-IKD(CE)\n98.9*\n67.2\n64.6\n71.8\n51.6\n64.2\n59.6\n54.5\n47.0\n50.7\n41.7\nTIFGSM-IKD(MSE)\n99.1*\n67.2\n64.9\n70.9\n50.9\n63.3\n59.1\n55.8\n47.8\n51.3\n42.2\nTIFGSM-IKD(KL)\n99.2*\n69.1\n67.1\n70.8\n52.1\n64.5\n61.4\n57.9\n49.5\n52.8\n43.8\nNIFGSM\n99.9*\n71.6\n80.0\n76.5\n43.4\n55.2\n49.2\n75.9\n70.7\n31.6\n16.0\nNIFGSM-IKD(CE)\n100.0*\n71.2\n80.7\n77.4\n42.3\n57.2\n49.1\n76.6\n71.6\n32.0\n16.4\nNIFGSM-IKD(MSE)\n99.9*\n72.7\n81.0\n77.2\n44.2\n55.6\n48.9\n76.8\n71.7\n31.1\n16.4\nNIFGSM-IKD(KL)\n100.0*\n76.5\n85.0\n79.3\n44.6\n57.8\n52.5\n81.1\n74.7\n32.0\n15.4\nSINIFGSM\n99.9*\n81.7\n85.0\n84.5\n53.2\n67.0\n59.3\n80.1\n75.4\n37.6\n17.1\nSINIFGSM-IKD(CE)\n99.9*\n83.4\n85.3\n84.7\n54.3\n67.1\n60.8\n80.1\n74.9\n36.5\n17.8\nSINIFGSM-IKD(MSE)\n99.8*\n82.6\n84.4\n85.4\n53.4\n67.2\n60.5\n79.2\n76.6\n37.9\n17.5\nSINIFGSM-IKD(KL)\n100.0*\n88.5\n91.3\n88.7\n58.0\n69.9\n64.5\n87.9\n84.1\n38.2\n17.7\nVMIFGSM\n99.9*\n84.9\n88.6\n86.5\n64.8\n73.6\n73.5\n85.5\n84.9\n43.8\n24.2\nVMIFGSM-IKD(CE)\n99.9*\n85.9\n88.2\n86.4\n66.4\n73.7\n72.9\n85.5\n85.5\n44.5\n23.8\nVMIFGSM-IKD(MSE)\n99.8*\n85.9\n88.2\n86.1\n65.2\n73.1\n73.0\n85.3\n85.2\n44.0\n23.2\nVMIFGSM-IKD(KL)\n99.9*\n86.1\n89.3\n86.1\n65.6\n72.6\n72.6\n87.1\n85.3\n44.7\n23.4\nVNIFGSM\n99.9*\n86.2\n89.2\n87.0\n68.7\n74.1\n73.1\n87.2\n86.0\n45.0\n23.0\nVNIFGSM-IKD(CE)\n99.9*\n86.2\n89.9\n87.5\n67.5\n75.4\n73.0\n87.6\n86.5\n44.2\n23.4\nVNIFGSM-IKD(MSE)\n99.9*\n86.0\n90.5\n86.7\n68.0\n74.6\n72.6\n87.5\n85.9\n44.1\n23.3\nVNIFGSM-IKD(KL)\n99.8*\n86.6\n91.1\n88.2\n68.7\n75.3\n74.0\n88.8\n88.0\n44.3\n24.2\nTable 6: Comparison results of various attack methods under different IKD weights regarding attack success rate (%). The bold item indicates\nthe best, and the underlined item indicates the suboptimal. An item with ⋆superscript is white-box attack; the others are black-box attacks.\nThe source model is ResNet50 and the distillation method is KL divergence.\nMethod\nWeight\nTarget Model\nRN50\nDN121\nRNX50\nVGG19BN\nIncRes-v2\nInc-v3\nInc-v4\nRN101\nRN152\nInc-v3adv\nIncRes-V2adv, ens\nMIFGSM\n/\n99.9⋆\n65.7\n75.2\n73.2\n38.9\n52.6\n46.0\n68.2\n62.1\n30.6\n16.0\nMIFGSM-IKD\n1000\n81.2⋆\n58.9\n65.6\n65.9\n33.9\n47.3\n41.8\n62.0\n58.4\n30.7\n15.3\n100\n94.2⋆\n66.1\n74.1\n71.7\n39.4\n51.1\n48.1\n69.4\n65.8\n32.1\n15.5\n10\n99.9⋆\n69.5\n78.5\n75.7\n40.2\n54.6\n49.2\n73.3\n68.5\n31.3\n16.7\n1\n99.9⋆\n69.1\n79.3\n75.1\n40.7\n53.9\n48.6\n74.3\n66.9\n32.1\n16.5\n0.1\n99.9⋆\n68.5\n79.3\n76.0\n41.0\n53.8\n49.8\n72.4\n67.4\n31.1\n15.7\n0.01\n99.9⋆\n68.4\n79.4\n76.0\n39.9\n54.4\n49.6\n73.1\n67.6\n32.1\n16.3\n0.001\n99.9⋆\n69.5\n79.8\n74.6\n40.5\n52.6\n49.2\n73.3\n67.4\n31.5\n17.1\nDIFGSM\n/\n99.9⋆\n82.9\n88.8\n84.7\n61.7\n70.3\n69.4\n86.1\n82.1\n35.9\n18.4\nDIFGSM-IKD\n1000\n67.2⋆\n58.2\n61.0\n64.1\n43.2\n51.2\n47.7\n58.2\n57.2\n30.2\n14.7\n100\n74.4⋆\n63.8\n67.6\n68.2\n47.0\n54.3\n52.5\n64.9\n62.8\n31.9\n16.2\n10\n78.8⋆\n68.0\n70.0\n71.5\n50.2\n56.9\n56.2\n67.8\n66.3\n31.6\n17.2\n1\n89.1⋆\n76.7\n81.0\n77.8\n56.0\n63.8\n61.8\n77.2\n75.0\n34.7\n16.8\n0.1\n98.1⋆\n83.9\n88.7\n82.8\n62.2\n71.6\n69.4\n85.8\n82.7\n37.5\n18.7\n0.01\n99.8⋆\n84.9\n90.6\n84.8\n62.9\n70.8\n69.6\n86.5\n83.3\n38.4\n18.9\n0.001\n99.7⋆\n85.0\n89.8\n84.3\n63.7\n72.5\n70.5\n86.5\n83.7\n37.6\n18.5\nTIFGSM\n/\n99.0⋆\n68.4\n65.4\n71.3\n51.1\n63.9\n60.3\n55.6\n49.8\n52.8\n43.0\nTIFGSM-IKD\n1000\n66.6⋆\n47.0\n46.6\n55.1\n37.0\n47.9\n44.1\n38.8\n35.1\n38.4\n31.2\n100\n73.4⋆\n52.1\n50.8\n59.1\n40.5\n49.8\n46.4\n41.5\n37.8\n41.3\n33.1\n10\n78.0⋆\n54.4\n53.3\n62.2\n41.6\n52.3\n49.7\n44.1\n39.1\n43.6\n33.8\n1\n88.3⋆\n61.6\n60.1\n66.0\n46.4\n58.9\n55.0\n50.3\n43.5\n46.9\n38.5\n0.1\n98.1⋆\n68.2\n66.8\n71.4\n52.8\n64.0\n60.2\n55.4\n48.4\n52.3\n42.7\n0.01\n99.2⋆\n69.1\n67.1\n70.8\n52.1\n64.5\n61.4\n57.9\n49.5\n52.8\n43.8\n0.001\n99.1⋆\n68.6\n66.8\n71.6\n53.3\n64.0\n61.7\n57.5\n48.4\n52.6\n42.2\nNIFGSM\n/\n99.9⋆\n71.6\n80.0\n76.5\n43.4\n55.2\n49.2\n75.9\n70.7\n31.6\n16.0\nNIFGSM-IKD\n1000\n87.8⋆\n66.6\n74.1\n71.9\n40.0\n51.7\n47.5\n71.2\n65.3\n30.4\n15.1\n100\n96.2⋆\n72.6\n80.6\n75.0\n43.5\n55.7\n51.8\n77.9\n72.7\n31.6\n15.6\n10\n99.9⋆\n75.8\n83.8\n79.8\n45.0\n59.0\n53.1\n81.8\n76.0\n31.9\n14.9\n1\n99.9⋆\n75.3\n84.8\n78.5\n45.0\n58.1\n53.0\n80.7\n74.2\n32.4\n15.5\n0.1\n99.9⋆\n76.4\n84.6\n79.0\n44.1\n57.9\n53.7\n80.8\n76.0\n31.6\n15.4\n0.01\n100.0⋆\n76.5\n85.0\n79.3\n44.6\n57.8\n52.5\n81.1\n74.7\n32.0\n15.4\n0.001\n100.0⋆\n75.2\n85.0\n79.0\n45.9\n58.8\n54.2\n79.1\n75.7\n31.8\n16.0\n\nTable 7: Evaluation results of the advanced attacks under different IKD weights regarding attack success rate (%). The bold item indicates\nthe best, and the underlined item indicates the suboptimal. An item with ⋆superscript is white-box attack, the others are black-box attacks.\nThe source model is ResNet50 and the IKD method is KL divergence.\nMethod\nWeight\nTarget Model\nRN50\nDN121\nRNX50\nVGG19BN\nIncRes-v2\nInc-v3\nInc-v4\nRN101\nRN152\nInc-v3adv\nIncRes-V2adv, ens\nSINIFGSM\n/\n99.9⋆\n81.7\n85.0\n84.5\n53.2\n67.0\n59.3\n80.1\n75.4\n37.6\n17.1\nSINIFGSM-IKD\n1000\n97.2⋆\n84.1\n84.5\n87.5\n52.8\n66.9\n62.3\n82.3\n78.3\n35.6\n17.6\n100\n97.2⋆\n82.8\n85.3\n87.0\n51.6\n65.4\n62.2\n81.7\n77.2\n34.7\n17.0\n10\n97.7⋆\n84.2\n85.6\n87.2\n53.8\n66.5\n63.5\n81.6\n77.3\n35.2\n16.6\n1\n99.9⋆\n85.5\n88.8\n88.4\n55.1\n69.0\n65.8\n84.8\n81.1\n36.1\n17.0\n0.1\n99.8⋆\n85.9\n90.0\n87.7\n57.6\n70.3\n64.2\n87.0\n82.7\n36.3\n18.4\n0.01\n100.0⋆\n88.5\n91.3\n88.7\n58.0\n69.9\n64.5\n87.9\n84.1\n38.2\n17.7\n0.001\n99.9⋆\n87.2\n91.7\n88.5\n57.9\n70.3\n65.5\n87.9\n84.5\n37.5\n18.0\nVMIFGSM\n/\n99.9⋆\n84.9\n88.6\n86.5\n64.8\n73.6\n73.5\n85.5\n84.9\n43.8\n24.2\nVMIFGSM-IKD\n1000\n91.0⋆\n78.0\n82.7\n79.2\n57.9\n65.4\n64.7\n80.0\n78.0\n38.3\n19.4\n100\n98.2⋆\n82.6\n87.5\n83.8\n59.7\n67.9\n66.5\n84.7\n82.9\n39.3\n20.1\n10\n99.7⋆\n83.4\n88.7\n83.5\n60.3\n68.0\n68.3\n86.7\n84.6\n38.7\n20.4\n1\n99.7⋆\n83.9\n90.5\n84.5\n61.8\n69.0\n68.6\n86.5\n84.5\n40.2\n20.4\n0.1\n99.9⋆\n85.0\n90.6\n85.4\n64.3\n71.0\n70.8\n86.9\n84.9\n42.5\n21.9\n0.01\n99.9⋆\n86.1\n89.3\n86.1\n65.6\n72.6\n72.6\n87.1\n85.3\n44.7\n23.4\n0.001\n99.9⋆\n86.9\n89.1\n86.2\n66.9\n73.3\n73.6\n87.3\n85.9\n44.4\n23.3\nVNIFGSM\n/\n99.9⋆\n86.2\n89.2\n87.0\n68.7\n74.1\n73.1\n87.2\n86.0\n45.0\n23.0\nVNIFGSM-IKD\n1000\n93.8⋆\n83.5\n87.2\n83.4\n62.1\n70.9\n70.4\n85.5\n82.8\n40.1\n18.3\n100\n98.3⋆\n86.3\n92.1\n85.6\n64.3\n72.3\n72.2\n89.1\n86.2\n39.8\n19.4\n10\n99.7⋆\n87.0\n92.6\n86.6\n65.5\n74.0\n72.0\n89.6\n87.8\n40.5\n19.1\n1\n99.8⋆\n87.6\n92.5\n87.4\n66.2\n73.8\n73.6\n90.4\n88.3\n40.2\n20.6\n0.1\n99.9⋆\n87.4\n92.2\n87.7\n68.3\n76.3\n73.9\n90.1\n88.9\n42.2\n21.8\n0.01\n99.8⋆\n86.6\n91.1\n88.2\n68.7\n75.3\n74.0\n88.8\n88.0\n44.3\n24.2\n0.001\n99.7⋆\n87.2\n90.9\n87.7\n68.5\n75.0\n74.5\n88.1\n87.4\n45.4\n23.5\n",
  "metadata": {
    "source_path": "papers/arxiv/Improving_the_Transferability_of_Adversarial_Examples_by_Inverse\n__Knowledge_Distillation_bdf96af99b4c94ad.pdf",
    "content_hash": "bdf96af99b4c94add9d5ece357e836b99372bb02a9900da66e908d7d050bf01f",
    "arxiv_id": null,
    "title": "Improving_the_Transferability_of_Adversarial_Examples_by_Inverse\n__Knowledge_Distillation_bdf96af99b4c94ad",
    "author": "",
    "creation_date": "D:20250225023616Z",
    "published": "2025-02-25T02:36:16",
    "pages": 13,
    "size": 398653,
    "file_mtime": 1740470198.6280193
  }
}