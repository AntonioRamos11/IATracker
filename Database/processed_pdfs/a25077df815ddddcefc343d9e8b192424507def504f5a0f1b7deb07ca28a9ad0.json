{
  "text": "Towards Economical Inference: Enabling\nDeepSeek’s Multi-Head Latent Attention in Any Transformer-based LLMs\nTao Ji♠, Bin Guo♡, Yuanbin Wu♡,\nQipeng Guo♢, Lixing Shen♣, Zhan Chen♣, Xipeng Qiu♠, Qi Zhang♠, Tao Gui♠#\n♠Fudan University ♡East China Normal University ♣Hikvision Inc ♢Shanghai Al Lab\n{taoji, tgui}@fudan.edu.cn\n{binguo@stu, ybwu@cs}.ecnu.edu.cn\nAbstract\nMulti-head Latent Attention (MLA) is an in-\nnovative architecture proposed by DeepSeek,\ndesigned to ensure efficient and economical\ninference by significantly compressing the Key-\nValue (KV) cache into a latent vector. Com-\npared to MLA, standard LLMs employing\nMulti-Head Attention (MHA) and its vari-\nants such as Grouped-Query Attention (GQA)\nexhibit significant cost disadvantages.\nEn-\nabling well-trained LLMs (e.g., Llama) to\nrapidly adapt to MLA without pre-training\nfrom scratch is both meaningful and challeng-\ning. This paper proposes the first data-efficient\nfine-tuning method for transitioning from MHA\nto MLA (MHA2MLA), which includes two\nkey components: for partial-RoPE, we remove\nRoPE from dimensions of queries and keys that\ncontribute less to the attention scores, for low-\nrank approximation, we introduce joint SVD\napproximations based on the pre-trained pa-\nrameters of keys and values. These carefully\ndesigned strategies enable MHA2MLA to re-\ncover performance using only a small fraction\n(3‰ to 6‰) of the data, significantly reducing\ninference costs while seamlessly integrating\nwith compression techniques such as KV cache\nquantization. For example, the KV cache size\nof Llama2-7B is reduced by 92.19%, with only\na 0.5% drop in LongBench performance.1\n1\nIntroduction\nThe rapid advancement of large language mod-\nels (LLMs) has significantly accelerated progress\ntoward artificial general intelligence (AGI), with\nmodel capabilities scaling predictably with param-\neter counts (Kaplan et al., 2020). However, these\ngains come at a steep cost: escalating computa-\ntional demands for training and degraded infer-\nence throughput, resulting in substantial energy\nconsumption and carbon emissions (Strubell et al.,\n2019).\n1Our source code is publicly available at https://github.\ncom/JT-Ushio/MHA2MLA.\nAs downstream tasks grow increasingly com-\nplex, long-context processing and computationally\nintensive inference have become central to LLM ap-\nplications (An et al., 2024). A key bottleneck lies in\nthe memory footprint of the Key-Value (KV) cache\ninherent to the Multi-Head Attention (MHA, 2017)\nmechanism, which scales linearly with sequence\nlength and model size. To mitigate this, variants\nlike Grouped-Query Attention (GQA, 2023) and\nMulti-Query Attention (MQA, 2019) have been ex-\nplored. However, these methods reduce not only\nthe KV cache size but also the number of parame-\nters in the attention, leading to performance degra-\ndation. The DeepSeek introduces Multi-Head La-\ntent Attention (MLA, 2024), an attention mecha-\nnism equipped with low-rank key-value joint com-\npression. Empirically, MLA achieves superior per-\nformance compared with MHA, and meanwhile\nsignificantly reduces the KV cache during infer-\nence, thus boosting the inference efficiency.\nA critical yet unexplored question arises: Can\nLLMs originally well-trained for MHA be\nadapted to enabling MLA for inference? The in-\nherent architectural disparities between MHA and\nMLA render zero-shot transfer impractical, while\nthe prohibitive cost of pretraining from scratch\nmakes this transition both technically challenging\nand underexplored in existing research. To address\nthis gap, we propose the first carefully designed\nMHA2MLA framework that maximizes parame-\nter reuse from pre-trained MHA networks while\naligning the KV cache storage and inference pro-\ncess with MLA’s paradigm (Figure 1). Our frame-\nwork features two pivotal technical innovations:\npartial rotary position embedding (partial RoPE)\nand low-rank approximation. The primary objec-\ntive of MHA2MLA is to achieve data-efficient per-\nformance recovery - restoring architecture-induced\ncapability degradation using minimal fine-tuning\ndata.\nThe inherent incompatibility between MLA’s\n1\narXiv:2502.14837v1  [cs.CL]  20 Feb 2025\n\nSVD Init.\nd\nFKjplKCPow2dKRPFBaWEUrhuTU=\">AC1XicjVHLSsNA\nFD2Nr1pfUZdugkWom5KIqEsfm24EBfuAWsokndZgXkwmg\npS6Erf+gFv9JfEP9C+8M0ZQi+iEJGfOvefM3HvdJPBTa\ndsvBWNicmp6pjhbmptfWFwyl1caZwJj9e9OIhFy2UpD/\nyI16UvA95KBGehG/Cme3mk4s0rLlI/js7kdcI7IRtEft/\n3mCSqa5oVtnlzHjJ5IcLhce1g1DXLdtXWyxoHTg7KyNdJ\nbD7jHD3E8JAhBEcESTgAQ0pPGw5sJMR1MCROEPJ1nGOE\nmkzyuKUwYi9pO+Adu2cjWivPFOt9uiUgF5BSgsbpIkpT\nxBWp1k6nmlnxf7mPdSe6m7X9Hdzr5BYiQti/9J9Zv5Xp2\nqR6GNP1+BTYlmVHVe7pLprqibW1+qkuSQEKdwj+KCsKe\nVn32tCbVtaveMh1/1ZmKVXsvz83wpm5JA3Z+jnMcNLaq\nzk5153S7vH+Yj7qINayjQvPcxT5qOEGdvK/wgEc8GU1jZ\nNwadx+pRiHXrOLbMu7fAUFzlbo=</latexit>(a) MHA\n(b) MHA2MLA\nx\nx\ns\nuqGxcKCtYWbCnJdKrBvJhMCqXUlbj1B9zqL4l/oH/hnTEFtYhOSHLm3HvOzL3XjX0vkb9OmFMTk3PzObm8vMLi0vL5srqRKlgvEai/xINFwn4b4X8pr0pM8bseBO4Pq87t4cqni9x0XiReG57Me8FThXodf1mCOJaptmkW3dNgNHXotgc\nHK8P2ybBbtk62WNg3IGCsjWaWS+oIkOIjCkCMARQhL24SCh5xJl2IiJa2FAnCDk6TjHEHnSpTFKcMh9oa+V7S7zNiQ9soz0WpGp/j0ClJa2CRNRHmCsDrN0vFUOyv2N+B9lR369PfzbwCYiWuif1LN8r8r07VItHFnq7Bo5pizajqWOaS6\nq6om1tfqpLkEBOncIfigjDTylGfLa1JdO2qt46Ov+lMxao9y3JTvKtb0oDLP8c5Di62S+VKqXK2U6geZKPOYR0bKNI8d1HFEU5RI+8eHvGEZ6NuDI074/4z1ZjINGv4toyHD0/XlcA=</latexit>(c) MLA\nA\nC0XicjVHLSsNAFD2N7/qunQTLIKrkMbax05041LRPsBWSdJRQ/NiMhFDEcStP+BWf0r8A/0L74wp6KLohMzcOfecM3PnOrHvJcI03wva1PTM\n7Nz8QnFxaXltbS23k6ilLus5UZ+xLuOnTDfC1lLeMJn3ZgzO3B81nGhzLfuWU8aLwTGQx6wf2dehdea4tCLroOZE/SLKAltHd/WpbBq7u\n3uNZl03DctqWpZFQbVh1c2qXjFMNcrIx3FUekMPA0RwkSIAQwhBsQ8bCX3nqMBETFgfI8I4RZ7KM9yjSNqUWIwYNqFDmq9pd56jIe2lZ6LULp\n3i089JqWObNBHxOMXyNF3lU+Us0UneI+Up75bR6uReAaECN4T+pRsz/6uTtQhcoaFq8KimWCGyOjd3SdWryJvrP6oS5BATJuMB5TnFrlKO31l\nXmkTVLt/WVvkPxZSo3Ls5N8WnvCU1eNxFfXLQtoxKzaidVMv7B3mr57GJLexQP+vYxGO0SJvjme84FU71TLtQXv8pmqFXLOBX0N7+gJhOJXk\n</latexit>x\nRoPE\n8lEKEbd+LWH3CrHyT+gf6Fd8YIuih2QmbunHvOmbl3nNj3EmGa7wVtYXFpeaW4Wlpb39jc0rd3WkmUcpc13ciPeMexE+Z7IWsKT/isE3NmB47P2s74Uubd4wnXhTeiEnMeoE9Cr2h59qCoL6+ZzmRP0gmAS1Ze2od9LMxn/b1crViqmHMDsrIRyPS32BhgAguUgRgCEo9mEjoa+LKkzEhPW\nQEcYp8lSeYoSaVNiMWLYhI5pHtGum6Mh7aVnotQuneLTz0lp4Ig0EfE4xfI0Q+VT5SzRWd6Z8pR3m9Dq5F4BoQK3hP6n+2HOq5O1CAxrmrwqKZYIbI6N3dJVfkzY1fVQlyiAmT8YDynGJXKX/6bChNomqXvbV/kMxJSr3bs5N8SlvOd8Dt04q1Vqldn1arl/kT13EPg5xTO95hjqu0ECTv\nDM84wWvmqXdaw/a4zdVK+SaXfwZ2tMXlFCYnw=</latexit>Wkr\nJ\nhkCmXsyp249Qfc6v+If6B/4U2cgi6KzTDJzbnOTeWJHnclEuf+S0hcWl5ZX8amFtfWNzS9/eafIwiW3WsEMvjNuWyZnBqwhXOGxdhQz07c81rKGlzLfGrGYu2FwI8YR6/rmIHD7rm0Kgnr6XscKPYePfVrS1qRz0Eud4WjS04uVUlkNY3ZQRDbqof6ODhyEsJHAB0MAQbEHE5y+W1RQRkRYF\nylhMUWuyjNMUCBtQixGDJPQIc0D2t1maEB76cmV2qZTPpjUho4Ik1IvJhieZqh8olylugs71R5yruNabUyL59QgTtC/9NmfPqZC0CfZyrGlyqKVKIrM7OXBLVFXlz41dVghwiwmTsUD6m2FbKaZ8NpeGqdtlbU+U/FVOicm9n3ARf8pbzPXDzpFSplqrXp8XaRfbUezjEMf0nmeo4Qp1NMj7\nHi94xZvW1R60R+3ph6rlMs0u/gzt+Ru5z5kR</latexit>Wdkv\nWdq\nckv\nqnope\nRoPE\nkrope\nRoPE\nRoPE\nPartial\nRoPE\nPartial\nRoPE\n4\n9Qfc6v+If6B/4U2cgi6KzTDJzbnOTeWJHnclEuf+S0hcWl5ZX8amFtfWNzS9/eafIwiW3WsEMvjNuWyZnBqwhXOGxdhQz07c81rKGlzLfGrGYu2FwI8YR6/rmIHD7rm0Kgnr6XscKPYePfVrS1qRz0Eud4WjS04uVUlkNY3ZQRDbqof6ODhyEsJHAB0MAQbEHE5y+W1RQRkRYFylhMUWuyjNMUCBtQixGDJPQIc0D2t1\nmaEB76cmV2qZTPpjUho4Ik1IvJhieZqh8olylugs71R5yruNabUyL59QgTtC/9NmfPqZC0CfZyrGlyqKVKIrM7OXBLVFXlz41dVghwiwmTsUD6m2FbKaZ8NpeGqdtlbU+U/FVOicm9n3ARf8pbzPXDzpFSplqrXp8XaRfbUezjEMf0nmeo4Qp1NMj7Hi94xZvW1R60R+3ph6rlMs0u/gzt+Ru5z5kR</latexit>Wdkv\nckv\nLinear\nApply RoPE\nCached\nAligned\nSplit\nq\nk\n3\nueIyIrGzYsavY8gPdlp+p+gfwF9wZHKldRGUsz9w595wzc+eGWRzlyvPuataTp82nm9u1V+83N7ZtV+9HuWikIwPmYiFPA6DnMdRyocqUjE/ziQPkjDm4/Dig86Pv3CZRyL9rBYZP0mCszQ6jVigCJrZb6ehiOf5IqGlvFzOyqniX1UpRcaXy5nd8NxW6DX7zqe6/t93/cpaPf8rtd2mq5nRgPVOBL2LaY\nQ4ChQAKOFIriGAFy+iZowkNG2AlKwiRFkclzLFEnbUEsToyA0Auaz2g3qdCU9tozN2pGp8T0S1I6eEcaQTxJsT7NMfnCOGt0nXdpPXdFrSGlVdCqMI5oY/pVsz/1elaFE7RMzVEVFNmEF0dq1wK8yr65s69qhQ5ZITpeE5STEzytU7O0aTm9r12wYm/9swNar3rOIW+KNvSQ1edFZH4x8t9lxO5/ajcFh1\nepN7GEf76mfXQzwEUcYkvc3XOMGvyxmfbeurB/qFat0rzBg2H9/AsNMJzA</latexit>qrope\nknope\nv\nWuk\nWuv\n3\nCrHyT+gf6Fd8YUdFHshMzcOfecM3Pv2JHnxqJc/shpC4tLyv51cLa+sbmlr6904zDhDus4YReyNu2FTPDVhDuMJj7Ygzy7c91rJHlzLfumc8dsPgRowj1vWtYeAOXMcSBPX0PdMOvX489mlJWxPzoJcmd5OeXqyUymoYs4MislEP9XeY6COEgwQ+GAIij1YiOnroIyIsK6SAnjFLkqzBgbQJsRgxLEJHNA9p18nQgPbS\nM1Zqh07x6OekNHBEmpB4nGJ5mqHyiXKW6CzvVHnKu41ptTMvn1CBW0L/02Z8+pkLQIDnKsaXKopUoiszslcEtUVeXPjV1WCHCLCZNynPKfYUcpnw2liVXtsreWyn8qpkTl3sm4Cb7kLed74OZJqVItVa9Pi7WL7Knz2Mchjuk9z1DFepokHeKF7ziTO1B+1Re/qharlMs4s/Q3v+BqnDmKg=</latexit>Wuq\n3\nCrHyT+gf6Fd8YUdFHshMzcOfecM3Pv2JHnxqJc/shpC4tLyv51cLa+sbmlr6904zDhDus4YReyNu2FTPDVhDuMJj7Ygzy7c91rJHlzLfumc8dsPgRowj1vWtYeAOXMcSBPX0PdMOvX489mlJWxPzoJfe8UlPL1ZKZTWM2UER2aiH+jtM9BHCQIfDAExR4sxPR1UEZEWFdpIRxilyVZ5igQNqEWIwYFqEjmoe062RoQHvpGS\nu1Q6d49HNSGjgiTUg8TrE8zVD5RDlLdJZ3qjzl3ca02pmXT6jALaH/6abMeXWyFoEBzlUNLtUKURW52QuieqKvLnxqypBDhFhMu5TnlPsKOW0z4bSxKp2VtL5T8VU6Jy72TcBF/ylvM9cPOkVKmWqtenxdpF9tR57OMQx/SeZ6jhCnU0yDvFC17xpnag/aoPf1QtVym2cWfoT1/A6KcmKU=</latexit>Wqr\n3\nueIyIrGzYsavY8gPdlp+p+gfwF9wZHKldRGUsz9w595wzc+eGWRzlyvPuataTp82nm9u1V+83N7ZtV+9HuWikIwPmYiFPA6DnMdRyocqUjE/ziQPkjDm4/Dig86Pv3CZRyL9rBYZP0mCszQ6jVigCJrZb6ehiOf5IqGlvFzOyqniX1UpRcaXy5nd8NxW6DX7zqe6/t93/cpaPf8rtd2mq5nRgPVOBL2LaY\nQ4ChQAKOFIriGAFy+iZowkNG2AlKwiRFkclzLFEnbUEsToyA0Auaz2g3qdCU9tozN2pGp8T0S1I6eEcaQTxJsT7NMfnCOGt0nXdpPXdFrSGlVdCqMI5oY/pVsz/1elaFE7RMzVEVFNmEF0dq1wK8yr65s69qhQ5ZITpeE5STEzytU7O0aTm9r12wYm/9swNar3rOIW+KNvSQ1edFZH4x8t9lxO5/ajcFh1\nepN7GEf76mfXQzwEUcYkvc3XOMGvyxmfbeurB/qFat0rzBg2H9/AsNMJzA</latexit>qrope\nWuv\nv\nknope\nqnope krope\nX\nicjVHLbtNAFD0xUNpAWwPLsrAaIbGyHCfNYxeVDcsikaRSUkX2ZNpatT3ueIyIrGzYsavY8gPdlp+p+gfwF9wZHKldRGUsz9w595wzc+eGWRzlyvPu\nataTp82nm9u1V+83N7ZtV+9HuWikIwPmYiFPA6DnMdRyocqUjE/ziQPkjDm4/Dig86Pv3CZRyL9rBYZP0mCszQ6jVigCJrZb6ehiOf5IqGlvFzOyqn\niX1UpRcaXy5nd8NxW6DX7zqe6/t93/cpaPf8rtd2mq5nRgPVOBL2LaYQ4ChQAKOFIriGAFy+iZowkNG2AlKwiRFkclzLFEnbUEsToyA0Auaz2g3q\ndCU9tozN2pGp8T0S1I6eEcaQTxJsT7NMfnCOGt0nXdpPXdFrSGlVdCqMI5oY/pVsz/1elaFE7RMzVEVFNmEF0dq1wK8yr65s69qhQ5ZITpeE5STEz\nytU7O0aTm9r12wYm/9swNar3rOIW+KNvSQ1edFZH4x8t9lxO5/ajcFh1epN7GEf76mfXQzwEUcYkvc3XOMGvyxmfbeurB/qFat0rzBg2H9/AsNMJ\nzA</latexit>qrope\nWuk\nWWk\nWv\nWq\nWv\nWk\nWq\nq\nk\nv\nkrope\nqq\nkrope\nFigure 1: The diagram illustrates the MHA, MLA, and our MHA2MLA. It can be seen that the “cached” part is fully\naligned with MLA after MHA2MLA. The input to the attention module is also completely aligned with MLA (the\naligned region below ). Meanwhile, the parameters in MHA2MLA maximize the use of pre-trained parameters\nfrom MHA (the aligned region above ).\ninference acceleration mechanism and RoPE ne-\ncessitates architectural compromises. DeepSeek’s\nsolution preserves PEs in limited dimensions while\ncompressing others, requiring strategic removal of\nRoPE dimensions (converting them to NoPE) in\nMHA to achieve MLA alignment. While higher\nremoval ratios enhance compression efficiency,\nthey exacerbate performance degradation, creat-\ning an efficiency-capability trade-off.\nThrough\nsystematically exploring RoPE removal strategies,\nwe identify that contribution-aware dimension se-\nlection (retaining top-k dimensions ranked by at-\ntention score impact) optimally balances these\ncompeting objectives. Although previous studies\nhave investigated training partial-RoPE LLMs from\nscratch (Black et al., 2021; Barbero et al., 2024),\nour work pioneers data-efficient fine-tuning for full-\nto-partial RoPE conversion in LLMs.\nMLA reduces memory footprint by projecting\nkeys and values into a low-rank latent representa-\ntion space (stored in the KV cache). MHA2MLA\ncan also apply low-rank approximation to the val-\nues and keys stripped of RoPE (NoPE dimen-\nsions). By performing Singular Value Decompo-\nsition (SVD) on the pre-trained parameter matri-\nces Wv and Wk corresponding to the NoPE sub-\nspaces, we compress these components into a latent\nspace while maximizing the retention of knowledge\nlearned by the original model.\nOur main contributions are:\n• we introduce MHA2MLA, the first parameter-\nefficient fine-tuning framework that adapts pre-\ntrained MHA-based LLMs to the MLA archi-\ntecture using only 3‰ to 6‰ of training data\nwithout training from scratch.\n• we demonstrate that the MHA2MLA architecture\ncan be integrated with KV-cache quantization\nto achieve more economical inference (up to a\n96.87% reduction).\n• we conduct experiments across four model sizes\n(from 135M to 7B, covering both MHA and\nGQA), and detailed ablation studies to provide\nguidance and insights for MHA2MLA.\n2\nPreliminary\n2.1\nMulti-Head Attention (MHA)\nGiven an input sequence {x1, . . . , xl} ∈Rl×d,\nstandard MHA (Vaswani et al., 2017) projects\neach token xi into queries q(h)\ni\n= xiW (h)\nq\n, keys\nk(h)\ni\n= xiW (h)\nk\n, and values v(h)\ni\n= xiW (h)\nv\n,\nwhere W (h)\nq\n, W (h)\nk\n, W (h)\nv\n∈Rd×dh for each head\nh ∈{1, . . . , nh}. The Rotary positional encod-\ning (RoPE, 2024) is applied to queries and keys\n(e.g., q(h)\ni,rope = RoPE(q(h)\ni\n)), followed by scaled\ndot-product attention2:\no(h)\ni\n= Softmax\n\u0010\nq(h)\ni,ropek(h)⊤\n≤i,rope\n\u0011\nv(h)\n≤i ,\nMHA(xi) =\nh\no(1)\ni , . . . , o(nh)\ni\ni\nWo,\n(1)\nwhere Wo ∈R(nhdh)×d and [·, ·] means vec-\ntor concatenate. During autoregressive inference,\nMHA stores the KV cache {k(h)\nrope, v(h)}nh\nh=1 of size\n2We ignore here the\n1\n√\nd scaling factor for ease of notation.\n2\n\nO(2lnhdh), growing linearly with sequence length\nl, posing memory bottlenecks.\nVariants:\nGrouped-Query\nAttention\n(GQA,\n2023) shares keys/values across\nng\ngroups\n(ng ≪nh) to reduce the KV cache. For each head\nh, it maps to group g = ⌊h/ng⌋:\no(h)\ni\n= Softmax\n\u0010\nq(h)\ni,ropek(g)⊤\n≤i,rope\n\u0011\nv(g)\n≤i ,\nGQA(xi) =\nh\no(1)\ni , . . . , o(nh)\ni\ni\nWo.\n(2)\nMulti-Query Attention (MQA, 2019) is a special\ncase of GQA with ng = 1, i.e., all heads share a sin-\ngle global key/value. While reducing the KV cache\nto O(2lngdh), these methods degrade performance\ndue to parameter pruning.\n2.2\nMulti-Head Latent Attention (MLA)\nMLA (DeepSeek-AI et al., 2024) introduces a hy-\nbrid architecture that decouples PE from latent KV\ncompression. For each head h, the input xi is pro-\njected into two complementary components:\nPosition-Aware Component\nA subset of dimen-\nsions retains PE to preserve positional sensitivity:\nq(h)\ni,rope, ki,rope = RoPE\n\u0010\nxiWdqW (h)\nqr , xiWkr\n\u0011\n,\nwhere Wdq ∈Rd×dq, W (h)\nqr\n∈Rdq×dr, Wkr ∈\nRd×dr project queries/keys into a RoPE-preserved\ncomponent of dimension dr.\nPosition-Agnostic Component\nThe remaining\ndimensions dc are stripped of PE (i.e., NoPE),\nk(h)\ni,nope and v(h)\ni\nand compressed into a shared latent\nvector c(h)\ni,kv:\nq(h)\ni,nope = xiWdqW (h)\nqc ,\nci,kv = xiWdkv,\nk(h)\ni,nope, v(h)\ni\n= ci,kvW (h)\nuk , ci,kvW (h)\nuv ,\nwhere W (h)\nqc\n∈Rdq×dc, Wdkv ∈Rd×dkv, W (h)\nuk ∈\nRdkv×dc, W (h)\nuv ∈Rdkv×dh. Note that dr + dc =\ndh. The attention output of MLA combines both\ncomponents:\no(h)\ni\n= Softmax\n\u0010\nq(h)\ni,ropek(h)⊤\n≤i,rope + qi,nopek(h)⊤\n≤i,nope\n\u0011\n· v(h)\n≤i\nMLA(xi) =\nh\no(1)\ni , . . . , o(nh)\ni\ni\n· Wo.\n(3)\nUnlike MHA and its variants, MLA stores the latent\nvector ckv and k(h)\ni,rope (O (ldr + ldkv))) instead of\nfull-rank ki, vi (O(2lnhdh)), where (dr + dkv) ≪\n2nhdh.\nWhy does MLA need to separate RoPE and\nNoPE?\nMLA introduces matrix merging tech-\nniques for the NoPE portion during inference, ef-\nfectively reducing memory usage. For the dot prod-\nuct operation q(h)\ni,nopek(h)⊤\nj,nope, the following identity\ntransformation can be applied 3:\nqi,nopek⊤\nj,nope = (xiWdqWqc) (cj,kvWuk)⊤\n= xi\n\u0010\nWdqWqcW ⊤\nuk\n\u0011\nc⊤\nj,kv\nwhere\n\u0000WdqWqcW ⊤\nuk\n\u0001\ncan be pre-merged into a\nsingle matrix, and cj,kv is already stored in the\nKV cache. As for the RoPE portion, the RoPE(·)\nfunction multiplies the input vector by the rotation\nmatrix (e.g., RoPE(qi) = qiRi, Ri’s specific form\nwill be introduced in Section 3.1). Therefore, the\nidentity transformation becomes as follows:\nqi,ropek⊤\nj,rope = (xiWdqWqrRi) (xjWkrRj)⊤\n= xi\n\u0010\nWdqWqcRj−iW ⊤\nkr\n\u0011\nx⊤\nj\nSince\n\u0000WdqWqcRj−iW ⊤\nkr\n\u0001\nis related to the rela-\ntive position j −i, it cannot be merged into a fixed\nmatrix. Considering that the relative distances in\nLLMs can be very long, such as 128K, the RoPE\nportion is better suited to be computed using the\noriginal form.\n3\nMHA2MLA\n3.1\nPartial-RoPE\nTo enable migration from standard MHA to MLA,\nwe propose partial-RoPE finetuning, a strategy that\nremoves RoPE from a targeted proportion of di-\nmensions and converts them into NoPE. Critically,\nwhile prior work has explored training LLMs with\npartial-RoPE from scratch (achieving marginally\nbetter perplexity than full-RoPE (Black et al., 2021;\nBarbero et al., 2024)), no existing method ad-\ndresses how to efficiently adapt pre-trained full-\nRoPE models (e.g., Llama) to partial-RoPE with-\nout costly retraining. Our work bridges this gap by\nsystematically evaluating partial-RoPE variants to\nidentify the most data-efficient fine-tuning protocol\nfor recovering model performance post-adaptation.\n3To simplify the notation, we omit the superscript (h).\nMatrices Wuv and Wo can also be merged, please refer to\nAppendix C by DeepSeek-AI et al. (2024).\n3\n\n(a)\n(b)\n(c)\n(d)\nRoPE \nFigure 2: Illustration of Shigh, Slow, Suniform, S2-norm.\nWhere dh = 8 and r = 2.\nLayer 3\nHead 4\nLayer 8\nHead 23\nLayer 16\nHead 12\nLayer 23\nHead 14\nLlama2-7B\n      High Frequencies\nLow Frequencies      \n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\nMean 2-norm\nFigure 3: Visualization of Head-wise 2-norm Contri-\nbution for Llama2-7B. We randomly selected 4 heads,\nand the red dashed box highlights the top-4 frequency\nsubspaces chosen when r = 4. It can be seen that differ-\nent heads tend to focus on different frequency subspaces,\nwhich validates the rationality of our S2-norm method.\nMHA’s Full-RoPE\nencodes positional informa-\ntion into queries and keys through frequency-\nspecific rotations. Formally, given a query vector\nqi ∈Rdh and key vector ki ∈Rdh, we partition\nthem into 2D chunks:\nqi, ki =\nh\nq[2k,2k+1]\ni\ni\n0≤k< dh\n2\n,\nh\nk[2k,2k+1]\ni\ni\n0≤k< dh\n2\n,\nwhere q[2k,2k+1]\ni\n∈R2 denotes the k-th 2D sub-\nspace. Each chunk undergoes a rotation by position-\ndependent angles θk = β−2k/dh, forming a spec-\ntrum of wavelengths. High-frequency components,\ne.g., k = 0, rotate rapidly at 1 radian per token.\nLow-frequency components, e.g., k = dh\n2 −1, ro-\ntate slowly at ∼β1/dh radians per token. The base\nwavelength β, typically set to 104 (Su et al., 2024)\nor 5×105.\nFormally, for each 2D chunk q[2k,2k+1]\ni\nand\nk[2k,2k+1]\ni\n, the rotation matrix at position i is de-\nfined as:\nR[2k,2k+1]\ni\n(θk) =\n\u0014cos(iθk)\n−sin(iθk)\nsin(iθk)\ncos(iθk)\n\u0015\n.\nThus, applying RoPE to queries and keys becomes:\nqi,rope =\nh\nR[2k,2k+1]\ni\n(θk)q[2k,2k+1]\ni\ni\n0≤k< dh\n2\n,\nki,rope =\nh\nR[2k,2k+1]\ni\n(θk)k[2k,2k+1]\ni\ni\n0≤k< dh\n2\n.\nFull-RoPE to Partial-RoPE Strategies\nGiven r\nretained rotational subspaces(r = dr\n2 ≪total sub-\nspaces dh\n2 , we propose four strategies (illustrated\nin Figure 2) to select which r subspaces preserve\nRoPE encoding:\nHigh-Frequency Preservation retain the r\nfastest-rotating (high-frequency) subspaces:\nShigh = {k | 0 ≤k < r} .\nIt is consistent with the p-RoPE method proposed\nin Barbero et al. (2024), where they explored set-\ntings in which r constituted 25%, 50%, and 75% of\nthe total subspaces, and observed a slight advantage\nover full-RoPE in LLMs trained from scratch.\nLow-Frequency Preservation retain the r\nslowest-rotating (low-frequency) subspaces:\nSlow =\n\u001a\nk\n\f\f dh\n2 −r ≤k < dh\n2\n\u001b\n.\nIt was chosen as a controlled experiment for the\nhigh-frequency strategy.\nUniform Sampling select r subspaces with\nequidistant intervals:\nSuniform =\n\u001a\u0016\nkdh\n2r\n\u0017 \f\f\f\f 0 ≤k < r\n\u001b\nThis balances high- and low-frequency components\nthrough geometric spacing. In practice, 2r typically\ndivides dh. It is similar to the partial RoPE used in\nGPT-Neo (Black et al., 2021).\nHead-wise 2-norm Contribution Barbero et al.\n(2024) were the first to propose the 2-norm\ncontribution to investigate whether these fre-\nquencies are utilized and how they are help-\nful.\nThis approach is based on the observa-\ntion that, according to the Cauchy-Schwarz in-\nequality, the influence of the k-th frequency sub-\nspace on the attention logits is upper-bounded\nby the 2-norm of the corresponding query and\nkey components, i.e.,\n\f\f\f\nD\nq[2k,2k+1]\ni\n, k[2k,2k+1]\nj\nE\f\f\f ⩽\n\r\r\rq[2k,2k+1]\ni\n\r\r\r\n\r\r\rk[2k,2k+1]\nj\n\r\r\r. For each head h, we\ncompute the mean 2-norm score for each subspace\n4\n\nx\nWdv\nWdk\nck\ncv\nknope\nl\nCxhQW1+j8Qgl1PdHI=\">AC0XicjVHLSsNAFD2Nr1pfVZdugkVwFdJY+9gV3bi\nsaFuhVUnSUPzYjIplFIQt/6AW/0p8Q/0L7wzpqAL0QmZuXPuOWfmznVi30uEab7\nltLn5hcWl/HJhZXVtfaO4udVJopS7rO1GfsQvHDthvheytvCEzy5izuzA8VnXGR\n7LfHfEeOJF4bkYx+wysG9D78ZzbUHQVd+J/EyDmiZjKbXxZJpHBwc1hs13TQsq\n2FZFgWVulUzK3rZMNUoIRutqPiKPgaI4CJFAIYQgmIfNhL6eijDREzYJSaEcYo8l\nWeYokDalFiMGDahQ5pvadfL0JD20jNRapdO8enpNSxR5qIeJxieZqu8qlyluhv\n3hPlKe82ptXJvAJCBe4I/Us3Y/5XJ2sRuEFd1eBRTbFCZHVu5pKqV5E3179VJcg\nhJkzGA8pzil2lnL2zrjSJql2+ra3y74opUbl3M26KD3lLavCsi/rvQcylWjelo\npNY+yVuexg13sUz9raOIELbTJm+MJz3jRzrSxdq89fFG1XKbZxo+hPX4CXHaV4g\n=</latexit>v\nWv\nknope\nv\n>\nAC13icjVHLTsJAFD3UF+Kr4tJNlZi4IsUYdEl04xITeRgpC0DTugr7ZRICHFn3PoDbvWPjH+gf+GdsS6IDJNZ+6ce86ZuXfs0OW\nxM2PjLa0vLK6l3PbWxube/ou/l6HCSRw2pO4AZR07Zi5nKf1QXLmuGEbM82UNe3gp840Ri2Ie+DdiHLKOZw183ueOJQjq6vm2Hb\ni9eOzRMmlM2wfdUVcvlIqmGsb8oIB0VAP9HW30EMBAg8MPgTFLizE9LVQgomQsA4mhEUcZVnmCJH2oRYjBgWoUOaB7RrpahPe+kZK\n7VDp7j0R6Q0cESagHgRxfI0Q+UT5SzRed4T5SnvNqbVTr08QgXuCP1PN2MuqpO1CPRxrmrgVFOoEFmdk7okqivy5savqgQ5hITJuEf5i\nGJHKWd9NpQmVrXL3loq/6mYEpV7J+Um+JK3XOyB6yfFUrlYvj4tVC7Sp85iH4c4pvc8QwVXqKJG3vd4wSvetFvtQXvUn6oWibV7OHP\n0J6/Acl4lyI=</latexit>Wv\nckv\nWdkv\n(a) SVDsplit\n(b) SVDjoint\nU v⌃1/2\nv\n⌃1/2\nv\nV >\nv\n⌃1/2\nk\nV >\nk\n=\n\">ADKXicjVHPb9MwGH0Ng3WFQRlHLtGqSZxKMk2DYwWXHTuN/pCarnISt7OaxCFxJlV/yL+E26cgF25jBvXceKzl0rdqm04Sv\nz8vde/Nl+GolcOc7PivVo4/GTzepW7emz7ecv6i93urksoB3AhnJrO+znEci4R0lVMT7acZ7Ee8508/6nrvnGe5kMknNUv5M\nGaTRIxFwBRo3ro+TIK81lM07yzGE3tVcI7EZOYEXs6d9/uL2zvc8HC+xUrta6mPSXTUb3hNB0z7HXglqCBcrRl/Ts8hJAIUCAG\nRwJFOAJDTs8ALhykxA0xJy4jJEydY4EaeQtScVIwYqf0ndBqULIJrXVmbtwB/SWiNyOnjT3ySNJlhPXfbFMvTLJm78qem0y9txnN\nfpkVE6twRuxDvqXyf326F4Ux3pseBPWUGkZ3F5QphTkVvXN7pStFCSlxGodUzwgHxrk8Z9t4ctO7Pltm6pdGqVm9Dkptgd96l3T\nB7u3rXAfd/aZ72Dw8Pmi0PpRXcVr7OIN3ec7tHCENjqU/RV/cIW/1hfrm/XDuriWpXS8wo3hvXrH9yVuhw=</latexit>\nU k⌃1/2\nk\n7\ndUk4WZTKeSuGRctw=\">AC0XicjVHLSsNAFD2N7/qunQTLIKrkMbax05041LRPs\nBWSdJRQ/NiMhFDEcStP+BWf0r8A/0L74wp6KLohMzcOfecM3PnOrHvJcI03wva1PT\nM7Nz8QnFxaXltbS23k6ilLus5UZ+xLuOnTDfC1lLeMJn3ZgzO3B81nGhzLfuWU8\n8aLwTGQx6wf2dehdea4tCLroOZE/SLKAltHd/WpbBq7u3uNZl03DctqWpZFQbVh1\nc2qXjFMNcrIx3FUekMPA0RwkSIAQwhBsQ8bCX3nqMBETFgfI8I4RZ7KM9yjSNqUWI\nwYNqFDmq9pd56jIe2lZ6LULp3i089JqWObNBHxOMXyNF3lU+Us0UneI+Up75bR6uR\neAaECN4T+pRsz/6uTtQhcoaFq8KimWCGyOjd3SdWryJvrP6oS5BATJuMB5TnFrlK\nO31lXmkTVLt/WVvkPxZSo3Ls5N8WnvCU1eNxFfXLQtoxKzaidVMv7B3mr57GJLexQ\nP+vYxGO0SJvjme84FU71TLtQXv8pmqFXLOBX0N7+gJhOJXk</latexit>x\nA\nADNXicjVFPS8MwFH+tbv7XqUcvxSF4mq3I9Dj04lHRTcHOkXbZDGub2qaDUfa5/CaKB28ievILePAlRpgOmSltXn7/mpd4cBSYduPh\njk1XSjOzM7NLywuLa+UVtcaKc8Sn9Z9HvDk0iMpDVhE64KJgF7GCSWhF9ALr3ck+Ys+TVLGo3MxiGkzJN2IdZhPBEKtUux6PGingxCnvD\n5s5b3+0BrF3DPWDckXcZ07O7tI32akPVE0Qjc04woet0plu2KrY0Xji7KoMcJLz2AC23g4EMGIVCIQGAdAIEUnytwIYsSbkiCVYMc\nVTGMI8ejNUVQRHv47eLqSqMRrmVmqtw+/iXAN0GnBVvo4ahLsJZ/sxSfqWSJ/pWdq0y5twHOns4KERVwg+gk37fyvz7Zi4AOHKgeGPY\nUK0R25+uUTJ2K3Lk10pXAhBgxWbeRT7D2lfP7nC3lSVXv8myJ4l+VUqJy7WtBm9yl3jBzu/rHC8auxWnWqme7pVrh/qZ2EDNmEb73M\nfanAMJ1DH7Hv4MApG0bwzn8xn8+VLahrasw4/hvn+CRSYvk4=</latexit>\nU kv⌃1/2\nkv\n⌃1/2\nkv V >\nkv\nWuk\nWuv\n>\nAC2nicjVHLSsNAFD2Nr1pfUXHlJloEV6UVqS6LblxWsA9oSknSaQ3Ni8ykUEI37sStP+BWP0j8A/0L74wp6KLYCZm5c+45Z+besSP\nP5aJc/shpS8srq2v59cLG5tb2jr671+RhEjus4YReGLdtizPDVhDuMJj7Shmlm97rGWPrmW+NWYxd8PgTkwi1vWtYeAOXMcSBPX0A9\nMOvT6f+LSkral51EuT8bSnFyulshrG/KCIbNRD/R0m+gjhIEPhgCYg8WOH0dVFBGRFgXKWExRa7KM0xRIG1CLEYMi9ARzUPadTI0o\nL305Ert0Cke/TEpDZyQJiReTLE8zVD5RDlLdJ53qjzl3Sa02pmXT6jAPaH/6WbMRXWyFoEBLlUNLtUKURW52QuieqKvLnxqypBDhFhM\nu5TPqbYUcpZnw2l4ap2VtL5T8VU6Jy72TcBF/ylos9cPOsVKmWqrfnxdpV9tR5HOIYp/SeF6jhBnU0yDvFC17xpnag/aoPf1QtVym\n2cefoT1/A7WomK0=</latexit>Wuv\nWuk\nWk,nope\nWk,nope\nFigure 4: Illustration of SVDsplit and SVDjoint. In the\nmulti-head setting, we adhere to the standard MLA ap-\nproach, performing SVD on the merged multi-heads\nrather than on each head individually (e.g., Ukv ∈\nRnhdh×nhdkv.\nin an LLM over long sequences 4. Then, we pro-\npose to rank all subspaces by their 2-norm score\nand select the top-r:\nS2-norm = top-r\n0≤k< dh\n2\n\u0010\r\r\rq[2k,2k+1]\n∗\n\r\r\r\n\r\r\rk[2k,2k+1]\n∗\n\r\r\r\n\u0011\n.\nThis head-specific selection adaptively preserves\nrotation-critical subspaces. Figure 3 visualizes the\n2-norm of Llama2-7B’s four heads.\nWe will analyze the effectiveness of the four\nstrategies in Section 4.3 and conduct an ablation\nstudy on the essential hyperparameter r in Ap-\npendix D. For all strategies, non-selected subspaces\n(k /∈S) become NoPE dimensions, enabling seam-\nless integration with MLA’s latent compression.\n3.2\nLow-rank Approximation\nAfter transitioning from full RoPE to partial\nRoPE, we obtain the first component of the\nKV cache in MLA, represented as: ki,rope =\nh\nR[2k,2k+1]\ni\n(θk)k[2k,2k+1]\ni\ni\nk∈S. Our next goal is to\nderive the second component, ci,kv ∈Rdkv, which\nserves as a low-rank representation of ki,nope and\nvi.\nGiven the keys ki = xiWk and values vi =\nxiWv in MHA, we first extract the subspace of\nWk corresponding to ki,nope, i.e., the dimensions\nnot included in S, yielding: ki,nope = xiWk,nope.\nWe propose two Singular Value Decomposition\n(SVD)-based strategies (Illustrated in Figure 4) to\n4The 2-norm calculation detail is placed in Appendix A.\npreserve pre-trained knowledge while achieving\nrank reduction:\nDecoupled SVD (SVDsplit)\nSeparately decom-\npose Wk,nope and Wv into truncated SVDs, allo-\ncating dkv/2 dimensions to each:\nWk,nope = UkΣkV ⊤\nk ,\nWv = UvΣvV ⊤\nv ,\nwhere Uk, Uv, Vk, Vv\n∈Rdh× dkv\n2 , Σk, Σv\n∈\nR\ndkv\n2 × dkv\n2 . The down-projection matrices Wd∗\nand up-projection matrices Wu∗become:\nWdk = UkΣ1/2\nk\n,\nWuk = Σ1/2\nk\nV ⊤\nk ,\nWdv = UvΣ1/2\nv\n,\nWuv = Σ1/2\nv\nV ⊤\nv .\nThe low-rank representation ci,kv can be con-\nstructed using ci,kv = [xiWdk, xiWdv].\nJoint SVD (SVDjoint)\nTo preserve interactions\nbetween Knope and V , we jointly factorize the con-\ncatenated matrix:\n[Wk,nope, Wv] = UkvΣkvV ⊤\nkv,\nwhere Ukv, Vkv ∈Rdh×dkv, Σkv ∈Rdkv×dkv. The\nlatent projection is then:\nWdkv = UkvΣ1/2\nkv ,\nWuk =Σ1/2\nkv Vkv[:, : −dv], Wuv =Σ1/2\nkv Vkv[:, dv :].\nThis jointly optimizes the latent space for both keys\nand values, i.e., ci,kv = xiWdkv, retaining cross-\nparameter dependencies critical for autoregressive\ngeneration 5. Section 4.3 shows SVDjoint outper-\nforming SVDsplit, validating that joint factorization\nbetter preserves pre-trained knowledge.\n4\nExperiment\nWe evaluate our method on LLMs of varying\nscales (SmolLM-135M/360M/1B7, Llama2-7B)\npre-trained with MHA or GQA. We chose the\nSmolLM-series6 because its pretraining data and\nframework are both open-source, which can mini-\nmize the gap in fine-tuning data and processes. We\nchose Llama2-7B7 because it is one of the widely\nused open-source LLMs (but its pretraining data\nis not open-source, there is a potential gap in fine-\ntuning data).\n5We describe the economical inference process of\nMHA2MLA in Appendix B.\n6https://huggingface.co/collections/\nHuggingFaceTB/smollm-6695016cad7167254ce15966\n7https://huggingface.co/meta-llama/Llama-2-7b\n5\n\nModel\nTokens\nKV Mem.\nAvg.\nMMLU\nARC\nPIQA\nHS\nOBQA\nWG\n135MSmolLM\n600B\n44.50\n29.80\n42.43\n68.06\n41.09\n33.60\n52.01\n- GQA\ndkv =128\n44.25\n29.82\n42.05\n68.34\n41.03\n33.20\n51.07\n- GQA2MLA\ndkv =32\n2.25B\n-68.75%\n43.06 -1.19\n29.50\n40.48\n66.59\n37.99\n33.80\n49.96\ndkv =16\n(3.8‰)\n-81.25%\n41.84 -2.41\n28.66\n39.95\n65.02\n36.04\n31.60\n49.80\ndkv =8\n-87.50%\n40.97 -3.28\n28.37\n38.04\n64.69\n33.58\n30.80\n50.36\n360MSmolLM\n600B\n49.60\n33.70\n49.82\n71.87\n51.65\n37.60\n52.96\n- GQA\ndkv =128\n49.63\n34.01\n50.02\n71.33\n51.43\n38.20\n52.80\n- GQA2MLA\ndkv =32\n2.25B\n-68.75%\n47.91 -1.72\n32.94\n48.36\n70.73\n48.16\n36.00\n51.30\ndkv =16\n(3.8‰)\n-81.25%\n46.94 -2.69\n31.55\n45.73\n70.51\n45.80\n36.60\n51.46\ndkv =8\n-87.50%\n45.04 -4.59\n30.54\n43.33\n68.50\n42.83\n35.00\n50.04\n1B7SmolLM\n1T\n55.90\n39.27\n59.87\n75.73\n62.93\n42.80\n54.85\n- MHA\ndkv =128\n55.93\n39.11\n59.19\n75.95\n62.92\n43.40\n55.09\n- MHA2MLA\ndkv =32\n6B\n-68.75%\n54.76 -1.17\n38.11\n57.13\n76.12\n61.35\n42.00\n53.83\ndkv =16\n(6.0‰)\n-81.25%\n54.65 -1.28\n37.87\n56.81\n75.84\n60.41\n42.60\n54.38\ndkv =8\n-87.50%\n53.61 -2.32\n37.17\n55.50\n74.86\n58.55\n41.20\n54.38\n7BLlama2\n2T\n59.85\n41.43\n59.24\n78.40\n73.29\n41.80\n64.96\n- MHA\ndkv =256\n60.22\n41.63\n60.89\n77.80\n71.98\n45.00\n63.38\n- MHA2MLA\ndkv =64\n6B\n-68.75%\n59.51 -0.71\n41.36\n59.51\n77.37\n71.72\n44.20\n62.90\ndkv =32\n(3.0‰)\n-81.25%\n59.61 -0.61\n40.86\n59.74\n77.75\n70.75\n45.60\n62.98\ndkv =16\n-87.50%\n58.96 -1.26\n40.39\n59.29\n77.75\n69.70\n43.40\n63.22\nTable 1: Commonsense reasoning ability of four LLMs with MHA2MLA or GQA2MLA. The six benchmarks\ninclude MMLU (2021), ARC easy and challenge (ARC, 2018), PIQA (2020), HellaSwag (HS, 2019), OpenBookQA\n(OBQA, 2018), Winogrande (WG, 2021).\nWe denote the architectural migration using\nMHA2MLA and GQA2MLA, respectively.8 Both\nadopt data-efficient full-parameter fine-tuning,\nwith the head-wise 2-norm selection (S2-norm, r =\ndh\n16) for Partial-RoPE and joint SVD factorization\n(SVDjoint) for low-rank approximation as default\nconfigurations. Our experiments address three crit-\nical questions:\n1. How does MHA2MLA minimize accuracy\ndegradation induced by architectural shifts?\n2. What does MHA2MLA achieve in the KV cache\nreduction ratio?\n3. Can MHA2MLA integrate with KV cache quan-\ntization for compound gains?\n4.1\nCommonsense Reasoning Tasks\nMain Results\nAs shown in Table 1, our method\nachieves efficient architectural migration across\nfour model scales (135M to 7B) under varying\nKV cache compression ratios (via latent dimen-\nsion dkv). First, when comparing the performance\nof our fine-tuning approach with the original LLM,\nwe observe only minor changes in performance\nacross the four base models: a -0.25% decrease\non the 135M, +0.03% on the 360M, +0.03% on\nthe 1B7, and +0.37% on the 7B. This suggests that\nthe fine-tuning data does not significantly degrade\nor improve the performance of the original model,\n8The details of the fine-tuning process (including data and\nhyperparameters) are provided in Appendix C.\nproviding an appropriate experimental setting for\nthe MHA2MLA framework.\nNext, as dkv decreases (e.g., from 32 to 16 to\n8), the KV cache reduction increases (i.e., from\n-68.75% to -81.25% to -87.5%), but the perfor-\nmance loss becomes more challenging to recover\nthrough fine-tuning. Figure 5 shows the fine-tuning\nloss curves of 135M (representing GQA) and 7B\n(representing MHA) under different compression\nratios. As the compression ratio increases, the loss\ndifference from the baseline becomes larger. Addi-\ntionally, we observe that the fluctuation trends of\nthe loss curves are almost identical, indicating that\nour architecture migration does not significantly\nharm the model’s internal knowledge.\nWe also find that larger models experience less\nperformance degradation when transitioning to the\nMLA architecture. For example, with compression\ndown to 18.75%, the performance drops by 2.41%\nfor 135M, 2.69% for 360M, 1.28% for 1B7, and\n0.61% for 7B, revealing the potential scaling law\nof MHA2MLA. Finally, from the 135M model to\nthe 7B model, the number of tokens required for\nfine-tuning is only about 0.3% to 0.6% of the pre-\ntraining tokens, demonstrating the data efficiency\nof our method.\nOverall,\nwhether\nusing\nGQA2MLA\nor\nMHA2MLA, the architecture transition is achieved\nwith minimal cost, resulting in efficient and\neconomical inference.\n6\n\n0\n2000\n4000\n6000\n8000\n10000\n12000\n#Step\n1.6\n1.7\n1.8\n1.9\nLoss\nMHA2MLA: Llama2-7B\nMHA2MLA dkv = 16\nMHA2MLA dkv = 32\nMHA2MLA dkv = 64\nMHA\n0\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n#Step\n3\n4\nLoss\nGQA2MLA: SmolLM-135M\nGQA2MLA dkv = 8\nGQA2MLA dkv = 16\nGQA2MLA dkv = 32\nGQA\nFigure 5: The fine-tuning loss curves under different KV\ncache storage ratios (with colors ranging from light to\ndark representing 12.5%, 18.75%, 31.25%, and 100%).\n4.2\nLong Context Tasks\nSettings\nTo evaluate the generative capabilities\nof the model, we adopt LongBench (Bai et al.,\n2024) as the benchmark for generation perfor-\nmance. All models are tested using a greedy de-\ncoding strategy. The context window size is deter-\nmined based on the sequence length used during\nmodel fine-tuning. We use HQQ (Badri and Shaji,\n2023) and Quanto9 to set caches with different lev-\nels of precision to evaluate the performance of the\noriginal model as the baseline. Since our method\nis compatible with KV cache quantization, we also\nconduct additional experiments to assess the com-\nbined effect of both approaches.\nMain\nResults\nAs\nevidenced\nin\nTable\n2,\nMHA2MLA achieves competitive or superior\nefficiency-accuracy profiles compared to post-\ntraining quantization methods on LongBench.\nWhile 4-bit quantization incurs modest degradation\n(-0.2% to -0.4%) at comparable compression ratios,\naggressive 2-bit quantization suffers severe perfor-\nmance collapse (-6.2% to -9%) despite 87.5% KV\ncache reduction. In contrast, MHA2MLA alone\nattains 87.5% compression (at dkv =16) with only\n3% accuracy loss, and further synergizes with 4-bit\nquantization to reach 92.19%/96.87% compression\n(dkv =64/16+Int4HQQ) while limiting degradation\nto -0.5%/-3.2%, outperforming all 2-bit baselines.\nThis highlights that MHA2MLA’s latent space\ndesign remains orthogonal to numerical precision\nreduction, enabling compound efficiency gains\n9https://huggingface.co/blog/\nquanto-introduction\nModel\nPrecision\nKV Mem. Avg@LB\n7BLlama2 BF16\n100.0%\n27.4\nInt4HQQ\n-75.00%\n27.5\nInt4Quanto\n27.3\nInt2HQQ\n-87.50%\n21.2\nInt2Quanto\n18.5\ndkv =64\nBF16\n-68.75%\n27.1\nInt4HQQ\n-92.19%\n26.9\nInt4Quanto\n26.8\ndkv =32\nBF16\n-81.25%\n26.3\nInt4HQQ\n-95.31%\n26.1\nInt4Quanto\n26.1\ndkv =16\nBF16\n-87.50%\n24.4\nInt4HQQ\n-96.87%\n24.2\nInt4Quanto\n23.4\nTable 2:\nEvaluation results of Llama2-7B and\nMHA2MLA on LongBench. Bold indicates compres-\nsion ratios greater than or equal to Int2 quantization\nwhile also achieving performance higher than Int2.\nwithout destructive interference.\n4.3\nAblation Study\nFour Partial-RoPE strategies:\nShigh, Slow,\nSuniform, S2-norm\nTable 3 presents the results\nof four strategies for converting full-RoPE to\npartial-RoPE. First, when comparing the four\nstrategies with full-RoPE, we observed that the\nlow-frequency retention strategy, Slow, incurred\nthe greatest performance loss (a reduction of -\n6.49%@135M and -1.21%@1B7), whereas the\nhigh-frequency retention strategy, Shigh, experi-\nenced significantly less degradation (a reduction of\n-0.85%@135M and -0.76%@1B7), underscoring\nthe importance of high-frequency subspaces. Both\nSuniform and S2-norm yielded better performance, the\nSuniform preserves subspaces across the frequency\nspectrum, while the S2-norm retains subspaces based\non their contribution to the attention scores. We\nchoose S2-norm as the default configuration because\nthe removed subspaces (i.e., NoPE) are more suit-\nable for the (SVD-based) low-rank approximation.\nTwo SVD-based low-rank approximations:\nSVDsplit, SVDjoint\nThe last two rows of each\ngroup in Table 3 compare the effects of the two\nSVD methods. We observe that, on both LLMs, the\nSVDjoint method consistently outperforms SVDsplit,\nyielding an average performance improvement of\n0.92% on the 135M model and 0.74% on the 1B7\nmodel. It indicates that SVDjoint emerges as the\n7\n\nModel\nTokens\nAvg@CS\n135MSmolLM\n600B\n44.50\n- full-rope\n2.25B\n44.25\n- Shigh\n43.40 -0.85\n- Slow\n37.76 -6.49\n- Suniform\n43.76 -0.49\n- S2-norm\n43.77 -0.48\n- Shigh + SVDjoint\n2.25B\n41.04 -3.21\n- Suniform + SVDjoint\n41.77 -2.48\n- S2-norm + SVDjoint\n41.84 -2.41\n- S2-norm + SVDsplit\n40.92 -3.33\n1B7SmolLM\n1T\n55.90\n- full-rope\n6B\n55.93\n- Shigh\n55.17 -0.76\n- Slow\n54.72 -1.21\n- Suniform\n55.31 -0.62\n- S2-norm\n55.10 -0.83\n- Shigh + SVDjoint\n6B\n54.41 -1.52\n- Suniform + SVDjoint\n54.30 -1.63\n- S2-norm + SVDjoint\n54.65 -1.28\n- S2-norm + SVDsplit\n53.91 -2.02\nTable 3: Reasoning ability of ablation studies. The\nresults of other models are provided in Appendix E.\nclear default choice.\n5\nRelated Work\nEfficient Attention Architectures\nThe standard\nMulti-Head Attention (MHA, 2017) mechanism’s\nquadratic complexity in context length has spurred\nnumerous efficiency innovations.\nWhile MHA\nremains foundational, variants like Multi-Query\nAttention (MQA) and Grouped-Query Attention\n(GQA, 2023) reduce memory overhead by shar-\ning keys/values across heads—albeit at the cost of\nparameter pruning and performance degradation.\nParallel efforts, such as Linear Transformers (Guo\net al., 2019; Katharopoulos et al., 2020; Choroman-\nski et al., 2021), RWKV (Peng et al., 2023), and\nMamba (Gu and Dao, 2023), replace softmax atten-\ntion with linear recurrences or state-space models,\nbut struggle to match the expressiveness of stan-\ndard attention in autoregressive generation.\nMulti-Head Latent Attention (MLA, 2024) dis-\ntinguishes itself by compressing KV caches into\nlow-rank latent vectors without pruning atten-\ntion parameters.\nOur work bridges MLA with\nmainstream architectures (MHA/GQA), enabling\nseamless migration via data-efficient fine-tuning.\nNotably, while many linear attention variants\nabandon softmax query-key interactions (e.g.,\nthrough kernel approximations), architectures pre-\nserving a query-key dot product structure—even\nin factorized forms—remain compatible with our\nMHA2MLA framework.\nEconomical Key-Value Cache\nThe memory\nfootprint of KV caches has become a critical bottle-\nneck for long-context inference. Recent advances\nfall into three categories:\nInnovative\nArchitecture\nmethods\nlike\nMLA (DeepSeek-AI et al., 2024), MiniCache (Liu\net al., 2024a), and MLKV (Zuhri et al., 2024) share\nor compress KV representations across layers or\nheads. While effective, cross-layer sharing risks\nconflating distinct attention patterns, potentially\nharming task-specific performance. Only MLA has\nbeen successfully validated in Deepseek’s LLMs.\nQuantization techniques such as GPTQ (Frantar\net al., 2022), FlexGen (Sheng et al., 2023), and\nKIVI (Liu et al., 2024b) store KV caches in low-\nbit formats (e.g., 2-bit), achieving memory savings\nwith precision loss.\nDynamic Pruning approaches like A2SF (Jo\nand Shin, 2024) and SnapKV (Li et al., 2024)\nprune “less important” tokens from the KV cache.\nHowever, token pruning risks discarding critical\nlong-range dependencies, while head pruning (e.g.,\nSliceGPT (Ashkboos et al., 2024), Sheared (Xia\net al., 2024), and Simple Pruning (Sun et al., 2024))\nirreversibly reduces model capacity.\nOur MHA2MLA method achieves the migration\nof standard Transformer-based LLMs to the more\neconomical MLA architecture and has demon-\nstrated its ability to integrate with KV quantization\ntechniques to realize a ~97% cache saving. It is\nalso theoretically compatible with other methods\nlike pruning.\n6\nConclusion\nThis work addresses the critical challenge of adapt-\ning pre-trained MHA-based LLMs (or variants)\nto the KV-cache-efficient MLA architecture. By\nintroducing MHA2MLA with contribution-aware\npartial-RoPE removal and SVD-driven low-rank\nprojection, we achieve near-lossless compression\nof KV cache (up to 96.87% size reduction for\nLlama2-7B) while requiring only 3‰ to 6‰of\ntraining data. The framework demonstrates strong\ncompatibility with existing compression techniques\nand maintains commonsense reasoning and long-\ncontext processing capabilities, offering a practi-\ncal pathway for deploying resource-efficient LLMs\nwithout sacrificing performance. Our results un-\nderscore the feasibility of architectural migration\nfor LLMs through targeted parameter reuse and\ndata-efficient fine-tuning.\n8\n\nLimitations\nVerification on More LLMs\nConsidering that\nMHA2MLA can significantly reduce inference\ncosts, it is worthwhile to validate it on larger and\nmore diverse open-source LLMs. However, con-\nstrained by our computation resources, models\nlike Llama3 require fine-tuning on a 128K con-\ntext length to mitigate performance degradation\nfrom continued training, so we did not perform\nsuch experiments. Furthermore, since Deepseek\nhas not yet open-sourced the tensor-parallel infer-\nence framework for MLA, it is currently challeng-\ning to explore models larger than 7B. This will be\naddressed in our future work.\nParameter-Efficient MHA2MLA Fine-tuning\nThis paper primarily focuses on the data efficiency\nof MHA2MLA. Since the architectural transforma-\ntion does not involve the Feed-Forward (FFN) mod-\nule, future work could explore parameter-efficient\nMHA2MLA fine-tuning, for example by freezing\nthe FFN module and/or freezing the parameters in\nthe queries and keys that correspond to the retained\nRoPE. This could further reduce the cost of the\nMHA2MLA transition.\nReferences\nJoshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury\nZemlyanskiy, Federico Lebrón, and Sumit Sanghai.\n2023. GQA: training generalized multi-query trans-\nformer models from multi-head checkpoints. In Pro-\nceedings of the 2023 Conference on Empirical Meth-\nods in Natural Language Processing, EMNLP 2023,\nSingapore, December 6-10, 2023, pages 4895–4901.\nAssociation for Computational Linguistics.\nChenxin An, Shansan Gong, Ming Zhong, Xingjian\nZhao, Mukai Li, Jun Zhang, Lingpeng Kong, and\nXipeng Qiu. 2024. L-eval: Instituting standardized\nevaluation for long context language models. In Pro-\nceedings of the 62nd Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), ACL 2024, Bangkok, Thailand, August 11-\n16, 2024, pages 14388–14411. Association for Com-\nputational Linguistics.\nSaleh Ashkboos, Maximilian L. Croci, Marcelo Gen-\nnari Do Nascimento, Torsten Hoefler, and James\nHensman. 2024. Slicegpt: Compress large language\nmodels by deleting rows and columns. In The Twelfth\nInternational Conference on Learning Representa-\ntions, ICLR 2024, Vienna, Austria, May 7-11, 2024.\nOpenReview.net.\nHicham Badri and Appu Shaji. 2023. Half-quadratic\nquantization of large machine learning models.\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,\nJiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao\nLiu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang,\nand Juanzi Li. 2024. Longbench: A bilingual, multi-\ntask benchmark for long context understanding. In\nProceedings of the 62nd Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), ACL 2024, Bangkok, Thailand, Au-\ngust 11-16, 2024, pages 3119–3137. Association for\nComputational Linguistics.\nFederico\nBarbero,\nAlex\nVitvitskyi,\nChristos\nPerivolaropoulos,\nRazvan Pascanu,\nand Petar\nVelickovic. 2024. Round and round we go! what\nmakes rotary positional encodings useful?\nCoRR,\nabs/2410.06205.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng\nGao, and Yejin Choi. 2020. PIQA: reasoning about\nphysical commonsense in natural language. In The\nThirty-Fourth AAAI Conference on Artificial Intelli-\ngence, AAAI 2020, The Thirty-Second Innovative Ap-\nplications of Artificial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educational\nAdvances in Artificial Intelligence, EAAI 2020, New\nYork, NY, USA, February 7-12, 2020, pages 7432–\n7439. AAAI Press.\nSid Black, Leo Gao, Phil Wang, Connor Leahy,\nand Stella Biderman. 2021.\nGPT-Neo:\nLarge\nScale Autoregressive Language Modeling with Mesh-\nTensorflow. If you use this software, please cite it\nusing these metadata.\nKrzysztof Marcin Choromanski, Valerii Likhosherstov,\nDavid Dohan, Xingyou Song, Andreea Gane, Tamás\nSarlós, Peter Hawkins, Jared Quincy Davis, Afroz\nMohiuddin, Lukasz Kaiser, David Benjamin Be-\nlanger, Lucy J. Colwell, and Adrian Weller. 2021.\nRethinking attention with performers. In 9th Inter-\nnational Conference on Learning Representations,\nICLR 2021, Virtual Event, Austria, May 3-7, 2021.\nOpenReview.net.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the AI2 reasoning challenge. CoRR,\nabs/1803.05457.\nDeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingx-\nuan Wang, Bo Liu, Chenggang Zhao, Chengqi Deng,\nChong Ruan, Damai Dai, Daya Guo, Dejian Yang,\nDeli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fuli\nLuo, Guangbo Hao, Guanting Chen, Guowei Li,\nHao Zhang, Hanwei Xu, Hao Yang, Haowei Zhang,\nHonghui Ding, Huajian Xin, Huazuo Gao, Hui Li,\nHui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Ji-\naqi Ni, Jiashi Li, Jin Chen, Jingyang Yuan, Junjie\nQiu, Junxiao Song, Kai Dong, Kaige Gao, Kang\nGuan, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia,\nLiang Zhao, Liyue Zhang, Meng Li, Miaojun Wang,\nMingchuan Zhang, Minghua Zhang, Minghui Tang,\nMingming Li, Ning Tian, Panpan Huang, Peiyi Wang,\nPeng Zhang, Qihao Zhu, Qinyu Chen, Qiushi Du,\n9\n\nR. J. Chen, R. L. Jin, Ruiqi Ge, Ruizhe Pan, Runxin\nXu, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan\nZhou, Shanhuang Chen, Shaoqing Wu, Shengfeng\nYe, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuip-\ning Yu, Shunfeng Zhou, Size Zheng, Tao Wang, Tian\nPei, Tian Yuan, Tianyu Sun, W. L. Xiao, Wangding\nZeng, Wei An, Wen Liu, Wenfeng Liang, Wenjun\nGao, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xi-\nanzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang,\nXiaojin Shen, Xiaokang Chen, Xiaosha Chen, Xiao-\ntao Nie, and Xiaowen Sun. 2024. Deepseek-v2: A\nstrong, economical, and efficient mixture-of-experts\nlanguage model. CoRR, abs/2405.04434.\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, and\nDan Alistarh. 2022. GPTQ: accurate post-training\nquantization for generative pre-trained transformers.\nCoRR, abs/2210.17323.\nAlbert Gu and Tri Dao. 2023. Mamba: Linear-time\nsequence modeling with selective state spaces. CoRR,\nabs/2312.00752.\nQipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao,\nXiangyang Xue, and Zheng Zhang. 2019.\nStar-\ntransformer. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 1315–1325. Association for Computational\nLinguistics.\nDan Hendrycks, Collin Burns, Steven Basart, Andy\nZou, Mantas Mazeika, Dawn Song, and Jacob Stein-\nhardt. 2021. Measuring massive multitask language\nunderstanding. In 9th International Conference on\nLearning Representations, ICLR 2021, Virtual Event,\nAustria, May 3-7, 2021. OpenReview.net.\nHyun-rae Jo and Dongkun Shin. 2024.\nA2SF: ac-\ncumulative attention scoring with forgetting factor\nfor token pruning in transformer decoder. CoRR,\nabs/2407.20485.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B.\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. CoRR,\nabs/2001.08361.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-\npas, and François Fleuret. 2020. Transformers are\nrnns: Fast autoregressive transformers with linear\nattention. In Proceedings of the 37th International\nConference on Machine Learning, ICML 2020, 13-18\nJuly 2020, Virtual Event, volume 119 of Proceedings\nof Machine Learning Research, pages 5156–5165.\nPMLR.\nYuhong Li, Yingbing Huang, Bowen Yang, Bharat\nVenkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai,\nPatrick Lewis, and Deming Chen. 2024. Snapkv:\nLLM knows what you are looking for before genera-\ntion. In Advances in Neural Information Processing\nSystems 38: Annual Conference on Neural Informa-\ntion Processing Systems 2024, NeurIPS 2024, Van-\ncouver, BC, Canada, December 10 - 15, 2024.\nAkide Liu, Jing Liu, Zizheng Pan, Yefei He, Reza Haf-\nfari, and Bohan Zhuang. 2024a. Minicache: KV\ncache compression in depth dimension for large lan-\nguage models. In Advances in Neural Information\nProcessing Systems 38: Annual Conference on Neu-\nral Information Processing Systems 2024, NeurIPS\n2024, Vancouver, BC, Canada, December 10 - 15,\n2024.\nZirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong,\nZhaozhuo Xu, Vladimir Braverman, Beidi Chen, and\nXia Hu. 2024b. KIVI: A tuning-free asymmetric 2bit\nquantization for KV cache. In Forty-first Interna-\ntional Conference on Machine Learning, ICML 2024,\nVienna, Austria, July 21-27, 2024. OpenReview.net.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? A new dataset for open book question an-\nswering. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\nBrussels, Belgium, October 31 - November 4, 2018,\npages 2381–2391. Association for Computational\nLinguistics.\nBo Peng, Eric Alcaide, Quentin Anthony, Alon Al-\nbalak, Samuel Arcadinho, Stella Biderman, Huanqi\nCao, Xin Cheng, Michael Chung, Leon Derczyn-\nski, Xingjian Du, Matteo Grella, Kranthi Kiran GV,\nXuzheng He, Haowen Hou, Przemyslaw Kazienko,\nJan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hay-\nden Lau, Jiaju Lin, Krishna Sri Ipsit Mantri, Ferdi-\nnand Mom, Atsushi Saito, Guangyu Song, Xiangru\nTang, Johan S. Wind, Stanislaw Wozniak, Zhenyuan\nZhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu.\n2023. RWKV: reinventing rnns for the transformer\nera. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2023, Singapore, Decem-\nber 6-10, 2023, pages 14048–14077. Association for\nComputational Linguistics.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-\nula, and Yejin Choi. 2021. Winogrande: an adver-\nsarial winograd schema challenge at scale. Commun.\nACM, 64(9):99–106.\nNoam Shazeer. 2019. Fast transformer decoding: One\nwrite-head is all you need. CoRR, abs/1911.02150.\nYing Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan\nLi, Max Ryabinin, Beidi Chen, Percy Liang, Christo-\npher Ré, Ion Stoica, and Ce Zhang. 2023. Flexgen:\nHigh-throughput generative inference of large lan-\nguage models with a single GPU. In International\nConference on Machine Learning, ICML 2023, 23-29\nJuly 2023, Honolulu, Hawaii, USA, volume 202 of\nProceedings of Machine Learning Research, pages\n31094–31116. PMLR.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\n10\n\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 3645–3650, Florence, Italy. Asso-\nciation for Computational Linguistics.\nJianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng\nPan, Wen Bo, and Yunfeng Liu. 2024. Roformer: En-\nhanced transformer with rotary position embedding.\nNeurocomputing, 568:127063.\nMingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter.\n2024. A simple and effective pruning approach for\nlarge language models. In The Twelfth International\nConference on Learning Representations, ICLR 2024,\nVienna, Austria, May 7-11, 2024. OpenReview.net.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pages 5998–6008.\nMengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi\nChen. 2024. Sheared llama: Accelerating language\nmodel pre-training via structured pruning. In The\nTwelfth International Conference on Learning Rep-\nresentations, ICLR 2024, Vienna, Austria, May 7-11,\n2024. OpenReview.net.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. Hellaswag: Can a\nmachine really finish your sentence? In Proceedings\nof the 57th Conference of the Association for Compu-\ntational Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers, pages\n4791–4800. Association for Computational Linguis-\ntics.\nZayd Muhammad Kawakibi Zuhri, Muhammad Farid\nAdilazuarda, Ayu Purwarianti, and Alham Fikri\nAji. 2024.\nMLKV: multi-layer key-value heads\nfor memory efficient transformer decoding. CoRR,\nabs/2406.09297.\nA\nThe Calculation of 2-norm Score\nTo compute the 2-norm scores for each attention\nhead, we selected 1,024 samples from the train-\ning dataset. The proportions of the subsets and\nsequence length used during the 2-norm compu-\ntation are consistent with those used during fine-\ntuning. First, we calculate the query vectors and\nkey vectors for each head. Then, for each rotational\nsubspace of the vectors, we compute the 2-norm\nscores. Finally, the 2-norm scores of the query and\nkey vectors are aggregated within each subspace.\nIf the model employs Grouped-Query Attention\n(GQA), the 2-norm scores are averaged within each\nGQA group, and the scores are shared between the\ngroups.\nB\nInference Process of MHA2MLA\nDuring inference in the MHA2MLA model, our\ninput includes the hidden representation xi of the\ni-th token, as well as the previously stored k(h)\n<i,rope\nand c<i,kv in the KV cache for the first i−1 tokens.\nDuring the inference, our goal is to compute\nthe h-th head’s dot product of these two parts\nq(h)\ni,ropek(h)⊤\n≤i,rope and q(h)\ni,nopek(h)⊤\n≤i,nope. For the RoPE\npart, we can easily extract W (h)\nq,rope and W (h)\nk,rope\nfrom the pre-trained parameter matrices W (h)\nq\nand\nW (h)\nk\n(i.e., the rows corresponding to the sub-\nspace that retains RoPE) and then obtain the result\nthrough a linear transformation:\nq(h)\ni,rope = xiW (h)\nq,rope\nk(h)\ni,rope = xiW (h)\nk,rope\nk(h)\n≤i,rope = [k(h)\n<i,rope, k(h)\ni,rope]\n→q(h)\ni,ropek(h)⊤\n≤i,rope.\nNote that k(h)\n<i,rope is already stored in the KV cache\nand can be directly retrieved.\nFor the NoPE part, q(h)\ni,nope can still be easily\nobtained through a linear transformation W (h)\nq,nope\nwhich extracted from the pre-trained parameter ma-\ntrix W (h)\nq\nby separating the rows corresponding to\nthe subspace with RoPE removed. However, k(h)\ni,nope\nrequires two linear transformations: a dimension-\nality reduction transformation using Wdkv, and\na dimensionality expansion transformation using\nW (h)\nuk . Note that Wdkv is shared across all heads\nin the current layer, and both Wdkv and W (h)\nuk are\n11\n\nconstrained by the SVD decomposition of the pre-\ntrained parameter matrices W (h)\nk,nope and W (h)\nv\n, pre-\nserving most of the pre-trained knowledge:\nq(h)\ni,nope = xiW (h)\nq,nope\nci,kv = xiWdkv,\nk(h)\ni,nope = ci,kvW (h)\nuk\nk(h)\n<i,nope = c<i,kvW (h)\nuk .\nDuring inference, the NoPE part can also leverage\nthe standard MLA matrix merging algorithm to\nreduce memory consumption:\nk(h)\n≤i,nope = [c<i,kv, ci,kv]W (h)\nuk\nq(h)\ni,nopek(h)⊤\n≤i,nope = (xiW (h)\nq,nope)(c≤i,kvW (h)\nuk )⊤\n= xi(W (h)\nq,nopeW (h)⊤\nuk\n)c⊤\n≤i,kv.\nWe\ncan\npre-multiply\nthe\nparameter\nma-\ntrices\n(W (h)\nq,nopeW (h)⊤\nuk\n),\nand\nlet\nc(h)\ni,q\n=\nxi(W (h)\nq,nopeW (h)⊤\nuk\n).\nIn the end, the output\nof MHA2MLA is as follows:\nv(h)\ni\n=\no(h)\ni\n=Softmax\n\u0010\nq(h)\ni,ropek(h)⊤\n≤i,rope+c(h)\ni,q c⊤\n≤i,kv\n\u0011\nc≤i,kv\nMHA2MLA(xi) =\nh\n. . . , o(h)\ni\nW (h)\nuv , . . .\ni\nWo.\nWhere W (h)\nuv\nand Wo can also perform matrix\nmerging to make inference more economical.\nWhy doesn’t MHA2MLA perform low-rank rep-\nresentation on the query as DeepSeek does?\nFirstly, we found that the economical inference of\nMLA is not affected even if W (h)\nq,nope is not decom-\nposed into a dimension-reducing matrix (e.g., Wdq)\nand a dimension-increasing matrix (e.g., W (h)\nuq ).\nSecondly, decomposing W (h)\nq,nope introduces addi-\ntional architectural migration loss (approximation\nloss) and further reduces the number of LLM pa-\nrameters. Therefore, we believe there is no need to\ndecompose W (h)\nq,nope within the MHA2MLA frame-\nwork.\nC\nThe Details of Fine-tuning\nData\nWe fine-tune our model using the pretrain-\ning corpus from SmolLM10. The dataset consists of\nfineweb-edu-dedup, cosmopedia-v2, python-edu,\nopen-web-math, and StackOverflow.\nThe first\n10https://huggingface.co/blog/smollm\nthree datasets are part of the smollm-corpus11 cu-\nrated by HuggingFaceTB. Fineweb-edu-dedup is\na high-quality dataset filtered by HuggingFaceTB\nfrom education-related webpages. Similarly, Hug-\ngingFaceTB filtered Python code snippets from\nThe Stack to construct the python-edu dataset.\nCosmopedia-v2 is a high-quality dataset gener-\nated by a model based on 34,000 topics defined\nby BISAC book classifications. Additionally, open-\nweb-math12 and StackOverflow13 are sourced from\nhigh-quality mathematical texts available online\nand posts from StackOverflow, respectively.\nHyperparameters\nThe fine-tuning hyperparame-\nters for models of all sizes are listed in Table 4. The\ntraining process employs a warmup phase followed\nby a decay strategy. A 1-sqrt decay strategy is\napplied to ensure a smooth and gradual reduction.\nD\nAblation Study on Partial-RoPE\nDimensions\nTo better determine the strategy and dimension-\nality for partial-RoPE, we conducted an ablation\nstudy on the number of RoPE dimensions using\nthe 135MSmolLM model. The experimental results\nare presented in Table 5. By comparing the per-\nformance of four different strategies in varying di-\nmensionalities, we observed that the low-frequency\nstrategy, Slow, suffered significant performance\ndegradation (-14.7%) when the dimensionality was\nrelatively low (≤4). In contrast, both Suniform and\nS2-norm consistently demonstrated superior perfor-\nmance regardless of dimensionality. Furthermore,\nincreasing the dimensionality from 4 to 8 provided\nnegligible performance gains. Based on these find-\nings, we selected a dimensionality of 4 for partial-\nRoPE.\nE\nDetailed Results\nIn this section, we present the detailed results.\nDetailed LongBench evaluation\nis reported in\nTable 6.\nDetailed ablation experiment\nis reported in Ta-\nble 7.\n11https://huggingface.co/datasets/\nHuggingFaceTB/smollm-corpus\n12https://huggingface.co/datasets/\nopen-web-math/open-web-math\n13https://huggingface.co/datasets/bigcode/\nstackoverflow-clean\n12\n\nMetrics\n135MSmolLM 360MSmolLM\n1B7SmolLM\n7BLlama2\nn_batch × n_gpu\n16×4\n16×4\n32×8\n16×16\nLearning Rate\n1e-4\n1e-4\n1e-4\n1e-4\nHardware\nRTX3090\nRTX3090\nNVIDIA L20Y NVIDIA L20Y\nSteps\n18000\n18000\n12000\n12000\nWarmup ratio\n5.0%\n5.0%\n8.3%\n8.3%\nDecay\n10%\n10%\n16.7%\n16.7%\nTime\n6h\n12h\n16h\n28h\nSeqlen\n2048\n2048\n2048\n4096\n#Param.\ndkv =128/256†\n134.52M\n361.82M\n1.71B\n6.61B†\ndkv =32/64†\n130.99M\n351.38M\n1.67B\n6.37B†\ndkv =16/32†\n129.64M\n347.38M\n1.59B\n5.99B†\ndkv =8/16†\n128.97M\n345.39M\n1.56B\n5.79B†\nTable 4: Training detail information across different models.\nModel\nAvg.\nMMLU\nARC\nPIQA\nHS\nOBQA\nWG\n135M\nr=32\n44.25\n29.82\n42.05\n68.34\n41.03\n33.20\n51.07\n- NoPE r=0\n38.99 -5.26\n27.03\n34.23\n62.68\n31.89\n29.40\n48.70\n- Shigh\nr=2\n42.86 -1.39\n29.58\n40.91\n66.54\n38.48\n32.00\n49.64\nr=4\n43.40 -0.85\n29.90\n41.15\n66.92\n39.34\n32.60\n50.51\nr=8\n43.56 -0.69\n29.90\n40.89\n67.63\n40.41\n32.20\n50.36\n- Slow\nr=2\n37.94 -6.31\n26.95\n33.56\n60.28\n31.51\n27.80\n47.51\nr=4\n37.76 -6.49\n27.11\n32.06\n59.79\n30.68\n28.40\n48.54\nr=8\n42.54 -1.71\n29.34\n39.58\n67.36\n37.86\n32.00\n49.09\n- Suniform\nr=2\n43.16 -1.09\n29.89\n41.80\n66.27\n38.78\n32.40\n49.80\nr=4\n43.76 -0.49\n29.87\n41.29\n67.36\n40.22\n32.80\n50.99\nr=8\n43.74 -0.51\n29.95\n40.81\n67.19\n40.47\n32.60\n51.38\n- S2-norm\nr=2\n43.13 -1.12\n29.75\n40.13\n67.25\n39.03\n32.80\n49.80\nr=4\n43.77 -0.48\n30.14\n41.69\n67.57\n39.53\n33.00\n50.67\nr=8\n43.88 -0.37\n29.91\n41.35\n67.74\n40.40\n33.40\n50.51\nTable 5: The impact of positional encoding dimensionality on model performance.\n0\n2000\n4000\n6000\n8000\n10000\n12000\n#Step\n1.8\n2.0\n2.2\n2.4\nLoss\nMHA2MLA: SmolLM-1B7\nMHA2MLA dkv = 8\nMHA2MLA dkv = 16\nMHA2MLA dkv = 32\nMHA\n0\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n#Step\n2.0\n2.5\n3.0\n3.5\nLoss\nGQA2MLA: SmolLM-360M\nGQA2MLA dkv = 8\nGQA2MLA dkv = 16\nGQA2MLA dkv = 32\nGQA\nFigure 6: The fine-tuning loss curves under different KV\ncache storage ratios (with colors ranging from light to\ndark representing 12.5%, 18.75%, 31.25%, and 100%).\nAdditional visualizations of fine-tuning loss\nWe present the loss of the other two models fine-\ntuned, excluding the ones mentioned in the main\ntext, in Figure 6. We observe that as fine-tuning pro-\ngresses, the gap in loss between our approach and\n0\n2000\n4000\n6000\n8000\n10000\n12000\n#Step\n2.0\n2.5\n3.0\n3.5\nLoss\nMHA2MLA: SmolLM-1B7\nlow\nhigh\nuniform\n2-norm\nfull-rope\n0\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n#Step\n2.5\n3.0\n3.5\n4.0\nLoss\nGQA2MLA: SmolLM-135M\nlow\nhigh\nuniform\n2-norm\nfull-rope\nFigure 7: The fine-tuning loss curves under different\npartial-RoPE strategy.\nthe baseline gradually decreases, and both exhibit\nsimilar fluctuations, demonstrating the effective-\nness of our approach. In Figure 7, we show the loss\nunder different partial-RoPE strategies. Except for\nSlow, the other three partial-RoPE schemes show\nlittle difference from the baseline. Additionally,\n13\n\n0\n2000\n4000\n6000\n8000\n10000\n12000\n#Step\n1.8\n2.0\n2.2\n2.4\nLoss\nMHA2MLA: SmolLM-1B7\nfull-rope\n2-norm\nuniform + SVDjoint\nuniform + SVDsplit\n0\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n#Step\n2.5\n3.0\n3.5\n4.0\nLoss\nGQA2MLA: SmolLM-135M\nfull-rope\n2-norm\nuniform + SVDjoint\nuniform + SVDsplit\nFigure 8: The fine-tuning loss curves under the combi-\nnation of S2-norm and different SVD strategies.\nSlow has a higher probability of convergence fail-\nure. In Figure 8, we show the loss under different\nSVD strategies. The loss curves on both 1B7SmolLM\nand 135MSmolLM reveal that SVDjoint outperforms\nSVDsplit.\n14\n\ndkv Precision\nKV Avg.\nS-Doc QA\nM-Doc QA\nSumm.\nFew-shot\nSynth.\nCode\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nM\nN\nO\nP\n7BLlama2 (Length=4K)\nBF16\n100.0% 27.4 15.1 9.6 21.1 7.5\n9.7 3.7 26.7 20.5\n3.2 65.5 87.5 34.1 1.9 6.6 66.5 59.4\nInt4HQQ\n-75.00% 27.5 16.1 9.1 22.0 7.3\n9.9 3.6 26.5 21.1\n3.4 65.5 87.2 34.3 1.5 6.7 66.0 59.9\nInt4Quanto\n27.3 14.4 9.5 20.5 7.5\n9.7 3.5 25.8 20.7\n3.1 65.5 87.7 34.3 1.4 7.3 66.8 59.3\nInt2HQQ\n-87.50% 21.2 18.0 5.5 12.6 7.5\n8.4 3.2 12.6 18.6\n0.9 56.5 73.3 27.0 1.8 6.1 34.5 52.9\nInt2Quanto\n18.5\n9.4 6.2 12.7 6.8\n6.7 3.3\n5.9 17.2\n0.4 61.0 63.9 26.0 1.4 2.7 42.4 30.5\n64\nBF16\n-68.75% 27.1 13.3 9.6 23.2 7.2 10.9 3.5 24.6 20.0 22.1 62.5 83.5 32.4 0.9 8.7 56.9 53.7\nInt4HQQ\n-92.19% 26.9 13.4 9.1 25.6 7.3 10.2 3.4 24.6 20.0 20.9 62.5 83.8 32.3 0.6 9.6 55.3 52.7\nInt4Quanto\n26.8 13.8 9.2 24.6 7.4 10.5 3.5 24.6 19.8 21.4 62.0 84.3 31.8 1.2 7.5 56.1 51.8\n32\nBF16\n-81.25% 26.3 14.9 9.1 27.0 7.3\n9.9 3.1 24.6 19.1 22.5 60.5 81.6 26.9 0.0 8.2 53.4 52.6\nInt4HQQ\n-95.31% 26.1 14.7 9.5 26.6 7.9 10.7 3.4 23.6 19.0 20.5 60.5 80.8 28.3 0.0 7.6 51.9 52.0\nInt4Quanto\n26.1 14.7 9.5 26.6 7.9 10.7 3.4 23.6 19.0 20.5 60.5 80.8 28.3 0.0 7.6 51.9 52.0\n16\nBF16\n-87.50% 24.4 14.7 9.5 24.3 7.8 10.2 3.8 22.8 19.1 24.6 61.0 82.8 20.2 0.2 8.6 39.9 41.4\nInt4HQQ\n-96.87% 24.2 15.2 9.4 25.2 7.4 10.2 3.9 22.9 19.8 20.6 61.0 82.5 21.7 0.1 9.0 38.0 41.2\nInt4Quanto\n23.4 15.6 8.4 22.7 7.3 10.2 3.8 20.2 18.7 18.6 61.0 81.9 21.7 0.5 8.0 36.9 38.3\n1B7SmolLM (Length=2K)\nBF16\n100.0% 18.7\n2.6 6.3 19.9 5.4\n8.6 2.7 23.5 18.4 20.2 46.5 70.2 32.4 2.2 3.2 21.3 16.5\nInt4HQQ\n-75.00% 18.6\n2.5 6.2 19.1 5.5\n8.2 2.7 23.4 18.3 20.0 46.5 69.4 32.1 2.7 3.2 21.5 16.0\nInt4Quanto\n18.6\n2.6 6.2 17.4 5.1\n8.6 2.6 23.0 18.1 20.1 46.0 70.2 31.9 2.9 3.6 21.9 16.7\nInt2HQQ\n-87.50% 16.3\n2.5 5.6 13.0 4.8\n7.5 2.7 14.8 16.3\n9.3 46.0 70.4 26.9 2.6 3.4 18.3 16.8\nInt2Quanto\n13.3\n1.6 3.8 10.3 3.9\n7.3 1.4\n5.9 13.4\n6.3 40.0 64.3 14.6 3.1 3.5 15.6 17.5\n32\nBF16\n-68.75% 16.0\n2.6 6.1 16.9 4.6\n9.3 2.0 22.8 15.1 19.9 50.0 57.1 29.8 1.7 2.4\n9.4\n6.7\nInt4HQQ\n-92.19% 15.9\n2.7 5.7 16.3 5.0\n8.5 1.8 23.0 15.0 18.5 50.0 56.2 30.2 1.8 3.2 10.0\n6.8\nInt4Quanto\n15.4\n2.5 5.7 16.1 5.7\n8.7 2.1 20.9 13.8 17.6 50.0 55.0 29.5 1.7 2.8\n9.6\n5.4\n16\nBF16\n-81.25% 16.5\n2.6 6.2 17.2 4.5\n9.7 2.1 22.0 15.3 21.0 47.5 55.5 31.7 1.2 3.3 15.8\n8.5\nInt4HQQ\n-95.31% 16.2\n2.5 6.1 16.2 4.5\n8.9 2.0 20.6 15.4 19.7 47.5 55.6 30.6 1.2 4.0 16.3\n8.0\nInt4Quanto\n15.6\n2.5 5.7 15.6 4.3\n8.8 1.6 21.2 15.7 17.6 47.0 55.7 27.4 1.7 3.6 15.6\n6.2\n8\nBF16\n-87.50% 15.3\n2.4 5.9 17.9 4.8 10.1 1.8 25.1 15.2 20.6 42.5 49.0 31.4 2.7 3.3\n7.1\n4.4\nInt4HQQ\n-96.87% 15.0\n2.4 5.7 16.9 4.7 10.1 2.0 23.5 14.7 20.3 42.5 47.6 30.6 2.6 3.6\n7.7\n4.5\nInt4Quanto\n14.2\n2.7 5.4 16.9 4.1\n8.8 1.5 22.2 14.4 17.2 42.0 47.9 29.9 1.5 3.3\n7.0\n3.0\n360MSmolLM (Length=2K)\nBF16\n100.0% 13.5\n2.4 6.4 14.3 5.0\n8.8 2.5 18.0 17.5\n7.1 47.5 37.5 24.9 1.5 3.4\n8.1 10.4\nInt4HQQ\n-75.00% 13.4\n2.7 6.1 14.1 5.5\n8.4 3.0 16.2 15.4 11.2 47.5 37.5 23.4 1.3 3.7\n9.0 10.1\nInt4Quanto\n13.3\n2.4 6.2 13.7 5.4\n8.7 2.6 15.4 17.4\n7.3 47.5 37.3 24.4 1.0 3.7\n8.4 11.0\nInt2HQQ\n-87.50% 10.8\n2.7 4.7\n8.3 5.4\n5.9 1.9\n9.9 10.0\n8.4 45.2 27.5 14.2 2.1 4.2 10.0 11.9\nInt2Quanto\n8.6\n2.6 2.2\n4.4 3.9\n4.8 1.4\n5.6\n8.9\n2.9 44.0 26.8\n9.6 1.0 1.9\n7.2\n9.7\n32\nBF16\n-68.75% 13.5\n2.3 5.9 13.4 5.5\n9.8 2.7 20.4 14.5 11.5 41.0 31.2 29.6 1.2 3.5 15.4\n7.9\nInt4HQQ\n-92.19% 12.5\n2.6 5.7 12.1 5.1 10.2 2.7 14.6 12.5\n8.8 41.0 30.3 27.8 1.9 2.7 14.5\n7.6\nInt4Quanto\n12.3\n2.0 5.2 11.9 5.0\n9.1 3.0 15.4 14.9\n8.3 41.0 28.3 27.0 0.9 3.9 13.8\n7.8\n16\nBF16\n-81.25% 11.6\n2.2 5.2 13.0 4.8\n9.5 3.2 13.4 13.4 11.3 32.0 26.1 22.5 1.1 5.0 14.9\n7.7\nInt4HQQ\n-95.31% 11.2\n2.6 5.6 12.0 5.1\n8.8 2.9 13.4 12.4 10.8 32.0 24.8 21.8 2.1 3.7 14.0\n7.2\nInt4Quanto\n10.9\n1.9 4.9 11.5 4.2\n8.8 2.6 12.2 12.2\n9.5 32.5 25.8 18.5 1.4 4.6 15.5\n7.8\n8\nBF16\n-87.50%\n9.9\n1.9 4.7 11.7 4.5\n8.5 2.8 13.0 12.9\n9.4 34.0 17.2 15.3 1.4 3.2 11.4\n6.9\nInt4HQQ\n-96.87% 10.0\n2.2 4.8 11.0 4.2\n8.2 2.6 13.1 12.8 11.7 33.5 17.3 14.8 0.8 4.4 11.6\n7.4\nInt4Quanto\n9.3\n1.8 3.6 11.3 4.0\n8.0 3.0 10.6 12.0\n7.4 31.5 19.8 10.3 0.8 4.8 12.1\n7.6\nTable 6: Evaluation results of all models on LongBench, including Task A: narrativeqa, B: qasper, C: multifieldqa_en,\nD: hotpotqa, E: 2wikimqa, F: musique, G: gov_report, H: qmsum, I: multi_news, J: trec, K: triviaqa, L: samsum, M:\npassage_count, N: passage_retrieval_en, O: lcc, P: repobench-p. Bold indicates compression ratios greater than or\nequal to Int2 quantization while also achieving performance higher than Int2.\n15\n\nModel\nTokens\nAvg@CS\nMMLU\nARC\nPIQA\nHS\nOBQA\nWG\n135MSmolLM\n600B\n44.50\n29.80\n42.43\n68.06\n41.09\n33.60\n52.01\n- full-rope\n2.25B\n44.25\n29.82\n42.05\n68.34\n41.03\n33.20\n51.07\n- Shigh\n43.40 -0.85\n29.90\n41.15\n66.92\n39.34\n32.60\n50.51\n- Slow\n37.76 -6.49\n27.11\n32.06\n59.79\n30.68\n28.40\n48.54\n- Suniform\n43.76 -0.49\n29.87\n41.29\n67.36\n40.22\n32.80\n50.99\n- S2-norm\n43.77 -0.48\n30.14\n41.69\n67.57\n39.53\n33.00\n50.67\n- Shigh + SVDjoint\n2.25B\n41.04 -3.21\n28.16\n37.55\n64.91\n34.91\n32.00\n48.70\n- Suniform + SVDjoint\n41.77 -2.48\n28.58\n38.69\n65.67\n36.17\n32.00\n49.49\n- S2-norm + SVDjoint\n41.84 -2.41\n28.66\n39.95\n65.02\n36.04\n31.60\n49.80\n- S2-norm + SVDsplit\n40.92 -3.33\n28.04\n37.85\n65.56\n34.60\n29.8\n49.65\n1B7SmolLM\n1T\n55.90\n39.27\n59.87\n75.73\n62.93\n42.80\n54.85\n- full-rope\n6B\n55.93\n39.11\n59.19\n75.95\n62.92\n43.40\n55.09\n- Shigh\n55.17 -0.76\n38.56\n57.72\n75.73\n60.93\n44.00\n54.06\n- Slow\n54.72 -1.21\n37.82\n56.47\n75.35\n60.06\n43.20\n55.41\n- Suniform\n55.31 -0.62\n38.93\n57.93\n75.63\n61.97\n42.60\n54.85\n- S2-norm\n55.10 -0.83\n38.60\n57.36\n75.68\n61.77\n43.00\n54.22\n- Shigh + SVDjoint\n6B\n54.41 -1.52\n37.97\n56.74\n75.14\n59.75\n42.00\n54.85\n- Suniform + SVDjoint\n54.30 -1.63\n37.82\n56.30\n75.08\n60.35\n42.40\n53.91\n- S2-norm + SVDjoint\n54.65 -1.28\n37.87\n56.81\n75.84\n60.41\n42.60\n54.38\n- S2-norm + SVDsplit\n53.91 -2.02\n37.64\n55.50\n75.46\n59.52\n42.40\n52.96\nTable 7: The complete results of the ablation experiment.\n16\n",
  "metadata": {
    "source_path": "papers/arxiv/Towards_Economical_Inference_Enabling_DeepSeeks_Multi-Head_Latent\n__Attention_in_Any_Transformer-based_LLMs_a25077df815ddddc.pdf",
    "content_hash": "a25077df815ddddcefc343d9e8b192424507def504f5a0f1b7deb07ca28a9ad0",
    "arxiv_id": null,
    "title": "Towards_Economical_Inference_Enabling_DeepSeeks_Multi-Head_Latent\n__Attention_in_Any_Transformer-based_LLMs_a25077df815ddddc",
    "author": "",
    "creation_date": "D:20250221020420Z",
    "published": "2025-02-21T02:04:20",
    "pages": 16,
    "size": 2472125,
    "file_mtime": 1740346982.0777566
  }
}