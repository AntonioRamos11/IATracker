{
  "text": " \nMaxGlaViT: A novel lightweight vision transformer-based approach \nfor early diagnosis of glaucoma stages from fundus images \n \nMustafa Yurdakul1, K√ºbra Uyar2*, ≈ûakir Ta≈üdemir3 \n \n1 Kƒ±rƒ±kkale University, Computer Engineering Department, Kƒ±rƒ±kkale, Turkey \n2Alanya Alaaddin Keykubat University, Computer Engineering Department, Antalya, Turkey \n3 Selcuk University, Computer Engineering Department, Konya, Turkey \n \nAbstract \nBackground and objective: Glaucoma is a prevalent eye disease that progresses silently without \nsymptoms. If it is not detected and treated at early stages, it can lead to permanent vision loss. Computer-\nassisted diagnosis (CAD) systems are crucial in timely and efficient eye disease identification. Deep \nlearning-based CAD systems have become valuable tools in early detection and treatment.  \nMethods: In this study, a novel lightweight model based on the restructured Multi-Axis Vision Transformer \n(MaxViT), MaxGlaViT, was designed for early detection of glaucoma stages. Firstly, MaxViT was scaled \nto optimize the number of blocks and channels of the model, resulting in a lighter architecture. Secondly, \nthe stem in the MaxViT was improved by adding various attention mechanisms (CBAM, ECA, SE) after \nthe convolution layers. As a result, a model was obtained that learns complex features more efficiently. In \nthe third stage, the MBConv structures in the MaxViT blocks were replaced by advanced DL blocks \n(ConvNeXt, ConvNeXtV2, InceptionNeXt). The model was evaluated using the Harvard Dataverse V1 \n(HDV1) dataset that contains fundus images belonging to different glaucoma stages. In the experimental \nstudies, state-of-the-art 40 Convolutional Neural Networks (CNN) and 40 ViT models were also evaluated \non the HDV1 dataset to prove the efficiency of the proposed MaxGlaViT model.  \nResults: Among CNN models, EfficientB6 outperformed all other CNN models with an accuracy of \n84.91%. On the other hand, among ViT models, MaxViT-Tiny performed the best with an accuracy of \n86.42%. Then, the scaled MaxViT achieved an accuracy of 87.93%. With the addition of ECA to the stem \nblock, the accuracy increased to 89.01%. Another improvement was achieved by replacing the MBConv \nstructure in the MaxViT block with ConvNeXtV2, with an accuracy of 89.87%. In line with all these results, \nscaled MaxViT was reconstructed using ECA in the stem block and ConvNeXtV2 in MaxViT block \nachieved an accuracy of 92.03%. \nConclusions: By testing 80 DL models for diagnosing glaucoma stages from fundus images, the proposed \nstudy is further expanded as the most comprehensive and comparative attempt in the current literature. In \ncomparison with experimental and state-of-the-art methods, MaxGlaViT demonstrates notable \nperformance, achieving 92.03% accuracy, 92.33% precision, 92.03% recall, 92.13% f1-score, and 87.12% \nCohen‚Äôs kappa score.  \nKeywords ConvNeXtV2, ECA, Glaucoma diagnosis, MaxViT, Vision transformer. \n*Corresponding Author: K√ºbra Uyar (kubra.uyar@alanya.edu.tr), Alanya Alaaddin Keykubat University, Computer \nEngineering Department, 07425, Antalya, Turkey. \n \n\n \n1. Introduction \nThe eye is a complex and unique organ that enables humans and many other forms of living beings \nto see. It helps us to understand the world around us by detecting light. The eye forms images by \nrefracting and focusing light, a process that takes place in retina located at the back of the eye. \nThanks to light cells (rod and cone cells), the retina converts images into electrical signals. Then \nthese signals are transmitted via the optic nerve to the brain, where they are interpreted and \ntransformed into an image. Eye health is crucial for keeping this process flowing smoothly, and \nany problems can affect our ability to see. Glaucoma is a chronic eye disease that occurs due to \nincreased intraocular pressure and causes blindness by damaging the optic nerve head. In the early \nstages of the disease, patients do not exhibit symptoms of vision loss, while in its advanced stages, \nvision loss becomes more apparent. Glaucoma known as silent theft of eyesight is an incurable \ndisease; however, with early diagnosis and medication, its progression can be prevented. \nOphthalmologists require a detailed digital image of the eye to diagnose glaucoma. Therefore, the \nstructural changes in the optic disc, nerve loss and atrophy in the peripapillary region are examined \nfrom the fundus image which is a medical imaging technique that displays the structure of the eye \nin color [1-3]. However, the number of ophthalmologists worldwide is insufficient, and the existing \nspecialists are working under heavy workloads. Moreover, the correct interpretation of the details \nand making the correct diagnosis requires experience. For all these reasons, there is a need for a \nCAD system that leverages advanced algorithms to analyze complex medical imaging data and \nsupports specialists. There are numerous studies and approaches aimed to diagnose glaucoma by \nanalyzing fundus images. One of the CAD methods is the classification of medical images based \non feature extraction and machine learning (ML) algorithms. Nayak et al. [4] extracted the cup-to-\ndisc ratio, the ratio of the distance between the center of the optic disc and the optic nerve head to \nthe diameter of the optic disc, and the ratio of the area of blood vessels from fundus images labeled \nas normal and glaucoma. They classified the features using an artificial neural network (ANN) and \nachieved a recall and specificity of 100% and 80%, respectively. Bock et al. [5] extracted a number \nof key features from fundus images using raw intensities, Fourier analysis, and spline interpolation \nto determine the Glaucoma Risk Index (GRI). Principal Component Analysis (PCA) was used to \ndimensionally reduce the features and classify them using support vector machines (SVM) with an \naccuracy of 80%, recall of 73%, and a specificity of 85%. Acharya et al. [6] used a combination of \ntexture and higher order spectral (HOS) features of the fundus image and the Random Forest (RF) \nalgorithm to determine the GRI and achieved a classification accuracy of 91.7%. Acharya et al. [7] \nused Gabor transformation and PCA approaches for feature extraction to classify normal and \nglaucoma images. Naive Bayes (NB) and SVM were tested and the most successful result was \nachieved with SVM with an accuracy of 93.10%, recall of 89.75%, and specificity of 96.20%. In \nparticular, studies mentioned in the literature review above (4-7) work on glaucoma disease \ndatasets as the assessment indicator. Table 1 summarizes the feature extraction and ML \nclassification studies for glaucoma diagnosis. \n \n\n \nTable 1. Summary of feature extraction and ML classification studies in the literature on glaucoma diagnosis. \nReference study \nFeatures and techniques \nClassifier \nPerformance metrics \nNayak et al. [4]  \nCup-to-disc ratio \nRatio of distance between optic disc \ncenter and optic nerve head \nBlood vessel area ratio \nANN \nRecall: 100% \nSpecificity: 80% \nBock et al. [5] \nRaw intensities \nFourier analysis \nSpline interpolation \nSVM \nAccuracy: 80% \nRecall: 73% \nSpecificity: 85% \nAcharya et al. [6] \nTexture features \nHOS features \nRF \nAccuracy: 91.7% \nAcharya et al. [7]  \nGabor transformation \nPCA \nSVM and NB \nBest performance with SVM: \nAccuracy: 93.10% \nRecall: 89.75% \nSpecificity: 96.20% \n \nFeature extraction-based classification techniques require expert knowledge to correctly extract the \nfeatures from the image, however, this process can be both laborious and error-prone. Manual \nextraction of features requires engineering skills and domain expertise. Moreover, the features may \noversimplify the problem and be temporary, as even experts may miss some important hidden \npatterns. In addition, the performance of ML algorithms has been insufficient in the face of \nincreasing data [8]. For these reasons, DL algorithms that can automatically extract important \nfeatures have been proposed. As a deep approach, there has been a wide range of literature studies \nto analyze and diagnose glaucoma-related eye diseases.  \n \nAhn et al. [9] conducted a comparative study comparing the classification performance of Logistic \nRegression (LR) and InceptionV3 for multi-stage glaucoma classification, collecting advanced-\nstage, early-stage, and normal fundus images, referred to as the HDV1 dataset. Using flattened raw \npixel features, LR achieved 82.9% training accuracy, 79.9% validation accuracy, and 77.2% test \naccuracy. InceptionV3 achieved 99.7% accuracy, 87.7% accuracy on validation data, and 84.5% \naccuracy on test data. Juneja et al. [10] proposed G-Net, a modified version of U-Net, to classify \nfundus images as glaucomatous or non-glaucomatous. The G-Net architecture was basically \ninspired by U-Net, scaling U-Net and optimizing the hyperparameters. They obtained 95.8% \naccuracy in an experimental study on the DRISHTI-GS dataset consisting of 101 images. Similarly, \nin another study by Juneja et al. [11], a CNN model called CoG-NET, a modified version of \nXception, was proposed to classify fundus images as normal and abnormal, and 93.5% accuracy, \n95% recall, and 99% specificity were achieved. Chai et al. [12] proposed a multi-branch approach \nfor glaucoma diagnosis from fundus images using a combination of CNN, regional convolutional \nneural network (R-CNN), and fully convolutional network (FCN). In the first branch, the image \nwas taken as input and features were extracted through a CNN. In the second branch, a faster R-\nCNN was used to obtain the optical disc region. In the third branch, an FCN model was used to \nsegment the disc area, dish area, and peripapillary atrophy area and then compute the \nmeasurements. As a result, their approach achieved 91.51% accuracy, 92.33% recall, and 90.90% \n\n \nspecificity on a custom dataset. Haouli et al. [13] compared the classification performances of ViT \n(B16, B32, L16, L32) and CNN (Xception and ResNet152V2) models for binary (normal-\nabnormal) glaucoma classification, inspecting the batch size effect on performance. They \ncombined five datasets which are ACRIMA, RIM-ONE, Drishti-GS1, HRF, and SJCHOI86-HRF, \nachieving the best result with an accuracy of 92.67% using ViT-L32 on the combined dataset. Das \net al. [14] proposed a model named adapter and enhanced self-attention network (AES-Net) to \nimprove the performance of CNN models for glaucoma diagnosis. Firstly, the authors tested \nvarious CNN models on HDV1 and LMG datasets. The DenseNet169 was the most successful \nmodel on the HDV1, achieving 83.83% accuracy, 83.35% precision, 83.87% sensitivity, and \n83.41% f1-score. Similarly, it achieved 82.80% accuracy, 82.58% precision, 82.80% recall, and \n82.61% f1-score on the LMG. The DenseNet169 was used as the backbone and the self-attention \nmechanism enhanced with the proposed adapter was added to the last feature layer of \nDenseNet169. With these modifications, an accuracy of 86.20%, precision of 85.32%, recall of \n85.77%, and f1-score of 85.46% were achieved on the HDV1. On the LMG dataset, 84.48% \naccuracy, 84.27% precision, 84.48% recall, and 84.34% f1-score were achieved. Das and Nayak \n[15] proposed FJA-Net for multi-stage fundus image classification and tested various CNN models \nusing transfer learning to determine the best backbone. Then, they added a fuzzy joint attention \nmodule (FJAM) at the last layer of the best-performing model and compared the classification \nperformance. In experiments on HDV1 and LMG datasets, DenseNet169 achieved 83.83% \naccuracy, 83.35% precision, 83.83% recall, and 83.41% f1-score on HDV1 and 82.80% accuracy, \n82.58% precision, 82.80% recall, and 82.58% f1-score on LMG. When DenseNet169 was used as \nthe backbone and FJAM was added, the accuracy of 87.06%, precision of 87.01%, recall of \n87.06%, and f1-score of 86.90% were achieved on the HDV1. On the LMG dataset, 84.91% \naccuracy, 84.35% precision, 84.91% recall, and 84.55% f1-score were achieved. Das et al. [16] \nproposed a novel cascaded attention-based network model called CA-Net for efficient multi-stage \nglaucoma classification. The authors found that DenseNet121 was the most successful model with \n83.18% accuracy, 83.10% precision, 83.18% recall, and 83.01% f1-score on the HDV1 and 81.55% \naccuracy, 82.10% precision, 81.55% recall, and 81.58% f1-score on LMG. Using DenseNet121 \nwith the proposed cascaded attention, the authors obtained 85.34% accuracy, 85.15% precision, \n85.34% recall, and 84.92% f1-score on HDV1; 83.85% accuracy, 83.69% precision, 83.85% recall, \nand 83.48% f1-score on LMG. Das et al. [17] introduced the GS-Net which DenseNet121 model \nenhanced with a global self-attention module (GSAM) consisting of two parallel attention modules, \na channel attention module (CAM) and a spatial attention module (SAM). The authors tested the \nGS-Net on HDV1 and achieved an accuracy of 84.91% and f1-score of 84.55%.  \nTable 2 lists the detailed summary of the key studies within the categories of DL-based and ViT-\nbased methods in the literature related to glaucoma diagnosis. This table also helps to illustrate the \nprogress and effectiveness of various methodologies in the field of glaucoma stage detection. \nThe analysis of previous studies highlights the significant advancements made in both ML and DL \napproaches for glaucoma detection. Tables 1 and 2 show that there is a continuous trend in the \n\n \nliterature to improve the performance of models for glaucoma-level detection. Despite these \nadvances, challenges remain in generalizability, interpretability, and scalability of existing \nsolutions. Overcoming these challenges requires the development of new approaches that not only \nimprove classification performance but also preserve the efficiency of the model for practical \napplications. In our work, we build on these findings and present innovative improvements to \nprovide higher diagnostic accuracy and reduce model complexity. \nTable 2. Summary of studies in the literature related to diagnosing glaucoma using DL algorithms. \nStudy \nModel \nMethod \nPerformance metrics \nAhn et al. [9] \nLR and InceptionV3 \nPixel-based features and \ntransfer learning-based \nclassification \nLR: Accuracy: 77.2% \nInceptionV3: Accuracy: 84.5% \nJuneja et al. [10] \nG-Net \nScaled U-Net, optimized \nhyperparameters \nAccuracy: 95.8% \nJuneja et al. [11] \nCoG-NET \nModified Xception \narchitecture \nAccuracy: 93.5% \nRecall: 95% \nSpecificity: 99% \nChai et al. [12] \nMulti-Branch Approach \n(CNN, Faster R-CNN, \nFCN) \nMulti-branch architecture \ncombining different models \nAccuracy: 91.51% \nRecall: 92.33% \nSpecificity: 90.90% \nHaouli et al. [13] \nViT and CNN models \nComparative study \nBest result with ViT-L32 \nAccuracy: 92.67% \nDas et al. [14] \nAES-Net \nAdded adapter and enhanced \nself-attention to \nDenseNet169 \nAccuracy: 86.20% (HDV1) \n84.48% (LMG) \nDas and Nayak [15] \nFJA-Net \nFJAM added to \nDenseNet169 backbone \nAccuracy: 87.06% (HDV1) \nDas et al. [16] \nCA-Net \nDenseNet121 with the \ncascaded attention \nAccuracy: 85.34 % (HDV1) \n83.85% (LMG) \nDas et al. [17] \nGS-Net \nGlobal self-attention module \nadded to DenseNet121 \nAccuracy: 84.91% (HDV1) \n \nThe following describe the contributions and novelty of our study: \n‚óè A comprehensive literature review on artificial intelligence techniques for glaucoma stage \ndetection is presented. \n‚óè This paper reports a most extensive comparison covering 40 CNN and 40 ViT models to \ndetect glaucoma stages. \n‚óè The channel and block numbers of MaxViT are rescaled, resulting in a lightweight \narchitecture that provides high and robust accurate results. \n‚óè The stem block of MaxViT is improved with an attention mechanism with efficient channel \nattention (ECA) after a comparison with the convolutional block attention module (CBAM) \nand squeeze-and-excitation (SE). \n‚óè The MBConv block in MaxViT is replaced by the ConvNeXtV2 comparing ConvNeXt and \nInceptionNeXt modules. This change results in a model with improved generalization \ncapabilities on test data. \n\n \n‚óè The proposed MaxViT-based model named MaxGlaViT enhances glaucoma stage \ndetection by improving the stem and MaxViT block with attention modules and advanced \nconvolutional blocks. \n‚óè In addition to a comparative performance analysis between MaxGlaViT series and various \nstate-of-the-art CNN and ViT models, MaxGlaViT‚Äôs performance is compared with other \nliterature studies. \n‚óè Experimental results demonstrate that MaxGlaViT outperforms recent models in literature \nand surpasses a total of 80 DL models. \nThe rest of this paper is organized as follows: Section 2 explains the material and method of the \nstudy. The proposed framework and details are mentioned in Section 3. The experimental results \nand discussion are detailed in Section 4. In addition to this, the comparison of the proposed \napproach with other alternative approaches is reported in this section. Finally, Section 5 \nsummarizes the main findings of the proposed study and gives some future directions. \n2. Material and Methods \nThe basic concepts of CNN and ViT, attention modules, advanced DL blocks, the dataset \ndescription procedure, and various performance measurement metrics are included in the following \nsubsections. \n2.1. CNN \nCNN is a DL algorithm widely used in image analysis studies such as image classification, \ndetection, and segmentation. CNNs are designed to capture spatial hierarchies in data, using \nconvolutional layers to extract local patterns and pooling layers to reduce dimensionality, allowing \nthe network to learn features increasingly through each layer. CNNs are structured with layers that \napply filters to input images, progressively detecting more complex features through deeper layers. \nIn the literature, there are various CNN models that contain different topologies such as dense \nblock, ghost module, inception block, residual block, and separable convolution. Various CNN \narchitectures have been employed utilizing transfer learning to improve model performance for \nglaucoma stage detection. DenseNet models that contain dense blocks; EfficientNet models that \ninclude inverted residual blocks; GhostNet and variants that use ghost modules; Inception models \nwith various versions that include inception modules; ResNet, MobileNet, and NASNetMobile \nmodels that contain residual blocks; VGG models that contain stacked blocks (basic CNN layers); \nand Xception model that contains separable convolution were all assessed as CNN models. \n2.2. ViT \nTransformer is a recent DL algorithm that derives its power from self-attention and was first used \nin natural language processing (NLP). Vaswani et al. [18] used transformers for machine \ntranslation, Devlin et al. [19] proposed the BERT model, a bidirectional language representation \nthat takes context into account. Brown et al. [20] proposed the generative pre-trained transformer \n\n \nGPT, a large language model. And so, transformer-based models kick-started a new era in the field \nof NLP. Following these studies, researchers started to apply the transformer models to image \nanalysis. The first ViT model proposed by Dosovitskiy et al. [21] achieved 88.36% accuracy on \nImageNet by directly applying transformers to image patches. Subsequently, many ViT models \nhave been developed to carry out various image-based tasks.  \nPyramid vision transformers (PVT) [22] process images in a pyramid structure to produce multi-\nfeature maps at different resolution levels to detect both small and large-scale objects. Swin \ntransformer [23] effectively combines local and global context with the shifted window technique. \nThe image is divided into fixed windows and information is shared through shifts between the \nwindows. Dual-axis vision transformer (DaViT) [24] analyzes images in both horizontal and \nvertical axes for more comprehensive feature extraction. FastViT [25] is a speed-oriented ViT \nmodel designed for efficient computer vision tasks, balancing both accuracy and speed. It combines \nthe strengths of transformers and convolutions, leveraging sparse attention and advanced \ncompression techniques to reduce computational load. Global context vision transformer (GCViT) \n[26] is a model developed to capture global context information. It is particularly effective for high-\nresolution vision tasks like image classification, object detection, and segmentation, optimizing \ncomputational resources without sacrificing model accuracy. FlexiViT [27] is a model that \nrandomizes the patch sizes in ViT models, providing high performance for different patch sizes \nwith a single set of weights. The method offers flexibility and computational efficiency by \neliminating the need to train separate models for different patch sizes. GPViT [28] is a non-\nhierarchical ViT model that can efficiently perform global knowledge transfer over high-resolution \nfeatures. GPViT uses an innovative group propagation block that allows information transfer by \ngrouping features and then returns the information back to the initial features. LeViT [29] is a \ndifferent model that is fast and efficient by combining CNN principles with transformer \narchitecture. It uses hybrid architecture elements, combining convolutions with transformers to \nreduce computational complexity and improve processing speed. Finally, MaxViT [30] is another \nViT that integrates local and global context using global and block attention mechanisms. It is a \nmodel that integrates transformers with convolutions and advanced attention mechanisms to \nachieve both high efficiency and accuracy in visual tasks. \n2.3. Attention Modules \nIn recent years, the attention mechanism has gained popularity and has been commonly used to \nimprove the accuracy of DL models [31,32]. The attention mechanism, in simple terms, detects \nwhich areas in the feature map are more important and so the model focuses on these areas. \nInjection of attention into convolution blocks is one of the implementation techniques, showing \ngreat potential for performance improvement in many studies [33,34]. Within the scope of this \nstudy, the popular attention mechanisms (CBAM, ECA, and SE) were used in the stem block of \nthe MaxViT model to improve performance. The general attention mechanisms are explained in \nthe following subsections. \n\n \n 2.3.1. SE \nSENet which uses the SE module is a DL model that aims to improve the performance of models \nby using a customized attention mechanism to determine the importance of channels in feature \nmaps [35]. SE module, transforms the input feature map, compressing its dimensions into a \ncompressed format while maintaining the number of channels. In the process of compressing the \nfeature map, global average pooling (GAP) is applied to obtain a vector representing each channel. \nThe vector is of size 1x1xC and reflects the intensity of each channel. Then, the vector is processed \nwith fully connected layers and activation functions to generate attention scores that represent the \nimportance of the channels. As a result of that process, scaling coefficients of size 1x1xC are \nobtained. In the final stage, these coefficients are applied to the initial feature map on a channel-\nby-channel basis. Each channel is rescaled according to its attention coefficient so that the model \nmakes the important channels more salient and the unimportant ones weaker. The schematic \ndiagram of the SE module is shown in Fig. 1. \n \nFig. 1. SE module. \n2.3.2. ECA \nInspired and in pursuit of improving SENet, Wang et al. found that dimensionality reduction has a \nside effect on channel attention based on empirical studies [36]. They proposed ECA, which avoids \ndimensionality reduction and captures cross-channel interaction in an efficient way. The ECA first \napplies the GAP to the input tensor. With GAP, average values are calculated for each channel to \nreduce the spatial dimensionality and the tensor is transformed into 1ùë•1ùë•ùê∂. Then, ECA uses one-\ndimensional convolution to learn the channel relationships. The resulting channel attention map is \nmultiplied by the input feature map on a channel-by-channel basis to scale the importance of each \nchannel so that the model emphasizes the important channels. The schematic diagram of the ECA \nmodule is shown in Fig. 2. \n\n \n \nFig. 2. ECA module. \n 2.3.3. CBAM \nCBAM [37] generates attention maps in two stages by analyzing the feature maps. In the first stage, \nthe channel attention module compresses the spatial dimension of the input feature map to \ndetermine how important each channel is. An average and maximum pooling method is used to \ncreate two different context descriptors. Then, the descriptors are processed through a shared neural \nnetwork, resulting in a channel attention map.  \n \nFig. 3. CBAM module. \n \n\n \nIn the second stage, the spatial attention module collects cross-channel information to understand \nwhich regions are more important. Then, two two-dimensional maps are generated by averaging \nand maximum pooling. These maps are combined to produce a spatial attention map through a \nconvolution layer. This process improves the quality of the final output by determining which \nfeatures the model should focus on. The schematic diagram of the CBAM is shown in Fig. 3. \n \n2.4. Advanced DL Blocks \nAdvanced DL blocks are specialized architectural components in DL models designed to enhance \nlearning efficiency, scalability, and model performance. Advanced blocks refine feature extraction \nwhile reducing the computational cost. These blocks are modular and can be stacked or combined \nwith other architectures, making them versatile tools for creating novel powerful models. The \ngeneral advanced DL blocks are explained in the following subsections. \n2.4.1. ConvNeXt and ConvNeXtV2  \nConvNeXt [38] is a modern CNN architecture that modernizes classical convolutional designs to \nachieve competitive performance on image classification tasks, competing with ViTs. Developed \nwith insights from both CNNs and ViTs, ConvNeXt refines traditional convolutional layers to \nimprove efficiency, scalability, and accuracy. Simplified convolutional blocks, large kernels for \nthe expanded receptive field, layer normalization and gaussian error linear unit (GELU) activation, \ninverted bottleneck design, and hierarchical feature representation are a breakdown of the key \nfeatures and innovations in ConvNeXt. It uses large kernel sizes of 7x7 to scan a larger area and \nprovide efficient information capture. It uses up-to-date techniques such as layer normalization and \nGELU activation function. It offers a simple yet effective structure with depthwise convolution for \ndownsampling followed by 1x1 convolution layers. In the ConNeXtV2 [39] model, which is an \nevolution of ConvNeXt, the LayerScale layer is removed from the block and the GRN module is \nadded to handle feature changes and avoid feature collapse in the learning process. The GRN layer \nincreases the contrast between channels, effectively improving the performance of the model.  \n2.4.2. InceptionNeXt \nYu et al. [40] proposed InceptionNeXt with the considering baseline as ConvNeXt. Compared to \nConvNeXt, InceptionNeXt is an architecture used to make large-kernel convolutions more \nefficient. In the InceptionNeXt architecture, the feature map is provided as input to an inception \nblock so that comprehensive feature extraction can be performed with filters of different kernel \nsizes. The extracted features are then combined and normalized. Finally, the feature map provided \nas input is merged with the final feature map with the residual block to obtain the resultant features. \nFig. 4 illustrates the general structures of the ConvNeXt, ConvNeXtV2, and InceptionNeXt. \n\n \n \nFig. 4. Advanced DL blocks. \n2.5. MaxViT \nMaxViT is a state-of-the-art DL architecture that effectively merges the strength of CNN and ViT. \nIntroduced by Tu et al. [30], MaxViT employs a hybrid approach that integrates both convolutional \nlayers and self-attention mechanisms. This design allows the model to capture local features \nthrough convolutional operations while also leveraging the global context provided by attention \nlayers. The MaxViT model achieved 86.5% top-1 accuracy on ImageNet-1K and 88.7% top-1 \naccuracy on ImageNet-21K.  \n \nFig. 5. An illustration of the MaxViT architecture. \nMaxViT consists of three main modules: stem block, MaxViT block, and classifier head as shown \nin Fig. 5. The input image is transmitted to the stem block for feature extraction. In the stem block, \nthere are two convolution layers that extract features from the image. The first convolution layer \nuses a 3x3 filter and is implemented with a stride of 2 units. Then, a second convolution layer with \nanother 3x3 filter, using a stride of 1 unit, is applied. It improves the feature extraction by further \n\n \ndetailing the feature maps extracted by the first layer. After the convolution layers, a batch \nnormalization (BN) layer is used to stabilize the learning process and GELU activation function is \napplied to provide a nonlinear structure to the model. In the rest of the model, there are 4 MaxViT \nblocks and each MaxViT block contains a mobile inverted residual bottleneck convolution \n(MBConv), a block attention, and a grid attention module.  \nMBConv first expands the feature map using a 1x1 kernel, which allows the model to learn features \nover more channels. After the expansion, a 3x3 deep convolution (DW) is applied to the feature \nmap. The DW works on each input channel separately, capturing spatial features along the input. \nThe SE mechanism that calculates the importance of the interdependencies between different \nfeature channels is then applied. In this way, important features are highlighted on the feature map. \nThe final 1x1 convolution in the MBConv structure is used to return the expanded channels back \nto their original input size. Thus, the basic features learned in the channels expanded with lower \nparameters are preserved. The feature maps obtained with the MBConv block are given as input to \nblock attention. The block attention module splits the feature map into small windows; a feature \nmap of dimensions HxWxC where H is the height, W is the width and C represents the number of \nchannels, is transformed into a tensor. The feature map is thus divided into C small windows of \nPxP dimensions. Each window represents a region in the feature map that does not intersect with \none another. The self-attention mechanism is then used to understand how the windows are related \nto each other. The features obtained after the self-attention mechanism are given as input to the \nfeedforward network (FFN). The FFN provides the model to learn more complex patterns and \nrelationships by applying nonlinear transformations to the features. In the last stage of the MaxViT \nblock the grid attention is used. The grid attention module focuses on pixels using a grid evenly \ndistributed over the entire feature map and the grid attention mechanism transforms the feature \nmap. Thereby, the feature map is divided into C times GxG partitions. Self-attention is applied on \nthese partitions and the extracted features are transferred to the FFN as in block attention to learn \nmore complex patterns and relationships by applying nonlinear transformations to the features.  \n \nFig. 6. Block and grid attention. \n\n \nThe effects of block and grid attention on feature maps are illustrated in Fig. 6. These attention \nblocks use residual connections to learn the combination of the original input and the output of the \nself-attention and feed-forward processes. In this way, the model can learn better and avoid the \nproblem of gradient loss during training. In the last part of MaxViT, the head part, the GAP is \napplied to the feature map and the resulting feature vector is classified with a fully connected layer. \n2.6. Description of the Dataset \nAll the DL models mentioned in this paper have been evaluated using the HDV1 dataset which is \nproposed in the study [7]. The dataset comprises three distinct classes: advanced glaucoma, early \nglaucoma, and normal (healthy). Early glaucoma class accounts for 289 samples, followed closely \nby advanced glaucoma with 467 samples. The normal class included 786 images. All images are \n224x224 in size and colorful fundus images. The classes of the data were labeled by means of a \nconsensus decision made by two experts and the dataset was divided into training, validation, and \ntest. The distribution of the data is shown in Table 3. Considering the class imbalance, online data \naugmentation techniques were applied during training. Fig. 7 presents randomly selected images \nfrom the classes in the utilized dataset.  \nTable 3. The number of images in the train, validation, and test sets of HDV1. \nClass name (glaucoma stage) \nTrain \nValidation \nTest \nTotal \nAdvanced \n228 \n98 \n141 \n467 \nEarly \n141 \n61 \n87 \n289 \nNormal \n385 \n165 \n236 \n786 \nTotal \n754 \n324 \n464 \n1542 \n \n \nFig. 7. Some fundus images from the HDV1 dataset, showing (top to bottom) advanced, early, and normal classes. \n\n \n2.7. Performance Measurement Metrics \nEvaluating the classification success of DL models objectively is an important task. The confusion \nmatrix provides a tabulation of the predictions of model compared to the actual labels. It gives an \noverview of the performance of the model by distinguishing between correct and incorrect \npredictions for each class. The table contains values for true positive (TP: samples correctly \nclassified as positive), true negative (TN: samples correctly classified as negative), false positive \n(FP: samples incorrectly classified as positive) and false negative (FN: samples incorrectly \nclassified as negative).  \nAccuracy indicates the overall success of the model, precision indicates the effect of false positives, \nand recall indicates the effect of false negatives. The f1-score is the harmonic mean of precision \nand recall and better evaluates performance in imbalanced datasets. Cohen‚Äôs kappa measures the \nclassification success of the model by taking into account the chance factor. With these metrics, it \nis possible to evaluate the model from different perspectives and to better understand its strengths \nand weaknesses. The mathematical formula of the performance metrics is given in Eq. 1-5. \nùê¥ùëêùëêùë¢ùëüùëéùëêùë¶=\nùëáùëÉ+ ùëáùëÅ\nùëáùëÉ+ ùëáùëÅ+ ùêπùëÉ+ ùêπùëÅ \n(1) \nùëÉùëüùëíùëêùëñùë†ùëñùëúùëõ=\nùëáùëÉ\nùëáùëÉ+ ùêπùëÉ \n(2) \nùëÖùëíùëêùëéùëôùëô=\nùëáùëÉ\nùëáùëÉ+ ùêπùëÅ \n(3) \nùêπ1 ‚àíùë†ùëêùëúùëüùëí= 2 ‚àóùëÉùëüùëíùëêùëñùë†ùëñùëúùëõ‚àóùëÖùëíùëêùëéùëôùëô\nùëÉùëüùëíùëêùëñùë†ùëñùëúùëõ+ ùëÖùëíùëêùëéùëôùëô \n(4) \nùê∂ùëú‚Ñéùëíùëõ‚Äôùë† ùëòùëéùëùùëùùëé = ùëù0 ‚àíùëùùëí\n1 ‚àíùëùùëí\n \n(5) \n \nIn Eq. 5, p0 is defined as the observed proportion of agreement, which represents the relative \nfrequency of cases where the raters agree. Pe is the expected proportion of agreement, calculated \non the basis of the frequency of each category, and represents the probability of random agreement \nbetween the raters. \n \n3. Proposed MaxGlaViT Model \n \nFig. 8 shows the graphical representation of the proposed MaxGlaViT architecture with detailed \nlayers. The proposed MaxGlaViT comprises three main phases: scaling blocks and channels in \nMaxViT, improving stem block, and enhancing MaxViT block. The first phase optimizes the \nnumber of blocks and channels in the MaxViT architecture, which has a direct impact on the \ncomputational complexity of the model and the number of parameters. In this way, a lightweight \narchitecture is obtained without compromising the performance of the model and reducing the \ncomputational cost. In the second stage, various attention modules were added after the \n\n \nconvolutional layers in the stem block in the MaxViT architecture. Finally, by replacing the \nMBConv blocks in the MaxViT block with advanced CNN blocks, the proposed model was \nconstructed. The implementation details of the construction phases of the proposed MaxGlaViT \nexplained in the following subsections. \n \nFig 8. The graphical abstract of the designed MaxGlaViT for multiple glaucoma stage detection. \n \n \n\n \n3.1. Scaling MaxViT \nThe scalability of ViTs makes them flexible, enabling them to be used effectively on datasets of \ndifferent sizes and for a variety of visual tasks. Furthermore, the ability to increase or decrease the \nnumber of parameters with scaling helps the model to adapt to different hardware capacities. \nRecent studies show that the performance of MaxViT can be improved by carefully optimizing the \nmodel parameters [41- 43]. In particular, the number of blocks and channels has a direct impact on \nthe classification performance and computational complexity of the model. While the number of \nblocks increases the depth of features that the model can learn at each level, excessive block usage \nincreases the computational load and memory consumption. However, an excessive number of \nblocks and channels not only increases hardware burden but can also lead to model overfitting, \nwhich can negatively impact classification performance. In addition, if there is an imbalance \nbetween data and high-capacity models, there is a tendency for the model to overlearn the data, \nresulting in a less generalized and poorer-performing model.  \nFor the purposes of this study, the number of blocks and channels of the stem block, while \nremaining in its original form, were set to block 2 and channel 32 for stage 1, block 2 and channel \n64 for stage 2, block 2 and channel 128 for stage 3, and block 2 and channel 256 for stage 4. These \nvalues have been chosen as optimal considering the amount and structure of the data. Table 4 lists \nthe block and channel values and the total number of parameters for each stage of the MaxViT \nTiny, Base, Small, and Large models. With the scaled model, a model with 6.2M parameters was \nobtained with 80% fewer parameters than the Tiny version (31M).  \nTable 4. MaxViT architecture variants (B and C denote the number of blocks and number of channels for each stage). \nStage \nScaled MaxViT \nMaxViT-Tiny \nMaxViT-Base \nMaxViT-Small \nMaxViT-Large \nStem Block \n(Stage 0) \nB = 2, C = 64 \nB = 2, C = 64 \nB = 2, C = 64 \nB = 2, C = 64 \nB = 2, C = 128 \nMaxViT Block \n(Stage 1) \nB = 2, C = 32 \nB = 2, C = 64 \n B = 2, C = 96 \nB = 2, C = 96 \nB = 2, C =128 \nMaxViT Block \n(Stage 2) \nB = 2, C = 64 \nB = 2, C =128 \nB = 6, C = 192 \nB = 2, C = 192 \nB = 6, C = 256 \nMaxViT Block \n(Stage 3) \nB = 2, C = 128 \nB = 5, C = 256 \nB = 14, C = 384 \nB = 5, C = 384 \nB = 14, C = 512 \nMaxViT Block \n(Stage 4) \nB = 2, C = 256 \nB = 2, C = 512 \nB = 2, C = 768 \nB = 2, C = 768 \nB = 2, C = 1024 \nParameter Count \n(M: Million) \n6.2 \n31 \n119 \n69 \n212 \n \n3.2.  Improving Stem and MaxViT Block \nIn MaxViT, the stem block is used to extract features from the image for the first time, and the \nextracted features are processed by MaxViT blocks. Since the features extracted in the stem block \nare used as input to other blocks, it is crucial to improve the stem block. To increase the feature \n\n \nrepresentation capacity of the stem block, an attention module was added after each convolutional \nlayer. The first attention module was placed after the first convolution layer, enabling more \nefficient processing of the extracted feature maps, and the second module was implemented after \nthe second convolution layer, enabling the network to learn more complex detailed features. \nCBAM, ECA, and SE modules were used in the experiments to determine the attention that would \nbest improve the performance of the model. Another improvement is conducted in the MaxViT \nblock. The MBConv blocks in the original structure of the MaxViT block perform local feature \nextraction, replacing them with more advanced convolution modules significantly improves the \nperformance and generalization capability of the model. Modern convolution modules enable \ndeeper and more meaningful feature extraction, strengthening the model's learning capacity and \noptimizing its performance. In this study, state-of-the-art ConvNeXt, ConvNeXtV2, and \nInceptionNeXt modules are experimentally used instead of MBConv blocks, respectively. The goal \nof this modification is to provide a more efficient and flexible learning process by improving the \naccuracy, robustness, and overall performance of the model.  \n4. Experimental Results and Discussion \nThis section provides a comprehensive assessment of the advanced DL models for glaucoma stage \ndetection on test data. The performance metrics and outcomes demonstrated in the confusion matrix \nassociated with the proposed model were discussed.  \nThe experiments were performed on a computer with 128 GB of RAM, two Nvidia RTX 3090 \n24GB GPUs combined with an NVLink bridge, and an Intel i9 processor. Python was utilized as \nthe programming language and Keras, a sub-library of Tensorflow, was used to perform CNN, \nViT, and MaxGlaViT models.  \nAll DL models were trained with a transfer learning method based on ImageNet data weights. \nThroughout the experiments, a standardized configuration was consistently applied to all models. \nEach model was trained with categorical cross-entropy loss and the Adam optimizer with a learning \nrate of 1e-3 and a weight reduction value of 0.8. The batch size was set to 16 and the number of \ntraining epochs to 50 in all experiments. Furthermore, data augmentation techniques such as \nscaling, rotation, and vertical-horizontal shifting were used to reduce overfitting. These \nhyperparameters were selected in the same way as in recent work [14] for a fair comparison.  \n4.1. Results of the CNN Models \nThis section discusses the performance metrics of the various CNN models after the test process \nand the outcomes depicted in the confusion matrix. Table 5 presents the experimental results of \nvarious CNN models for glaucoma stage detection on test data. \nAs listed in Table 5, the accuracy values of CNN models vary between 67.24% and 84.91%. \nAmong the GhostNet series models, GhostNet100 showed the highest performance with 81.43% \naccuracy and 81.42% f1-score. GhostNet-130 was the most successful model after GhostNet100 \n\n \nwith 80.80% accuracy and 80.81% f1-score. In contrast, GhostNetV2100 performed the worst with \n78.44% accuracy and 78.11% f1-score. In the DenseNet series, DenseNet201 model achieved the \nbest result with 84.05% accuracy and 83.97% f1-score. DenseNet169 and DenseNet121 were \ncompetitive with 82.33% and 83.41% accuracy. The EfficientNet series models achieved high \naccuracy and f1-score values with different configurations. The EfficientNetB6 performed the best \nwith an accuracy of 84.91% and an f1-score of 85.25%. EfficientNetB5 also performed well with \n84.27% accuracy and 84.10% f1- score. EfficientNetB7 showed high performance with 84.82% \naccuracy and 84.98% f1-score, but it was slightly lower than EfficientNetB6. InceptionResNetV2 \nand InceptionV3 models also stand out with their high accuracy and f1-score values.  \nInceptionResNetV2 outperforms the other model with 82.54% accuracy and 82.85% f1-score. On \nthe other hand, InceptionV3 performs quite well with 81.68% accuracy and 81.51% f1-score. In \nthe MobileNet series, MobileNetV2 had an accuracy of 82.33%, while the NASNet series models \nunderperformed, remaining around 67% accuracy. In the ResNet series, the ResNet50V2 model \nachieved the best results with 81.90% accuracy and 80.39% f1-score. The Xception model was the \nbest-performing model with 84.70% accuracy and 84.97% f1-score. In the VGG series, VGG16 \nachieved the best results with 80.60% accuracy and 80.14% f1-score, while the VGG19 model \nperformed lower with 69.18% accuracy. As a result, it was observed that models such as \nEfficientNetB6, DenseNet201, and Xception showed high performance, while some models such \nas NASNet showed low performance.  \nTable 5.  Results of CNN models. \nModel \nPerformance measurement metrics (%) \nAccuracy \nPrecision \nRecall \nF1-score \nCohen‚Äôs kappa \nGhostNetV2100 \n78.44 \n77.93 \n78.44 \n78.11 \n64.79 \nGhostNetV2130  \n81.25 \n80.20 \n81.25 \n80.32 \n68.89 \nGhostNetV2160 \n80.17 \n79.03 \n80.17 \n78.87 \n66.85 \nGhostNet050 \n80.96 \n81.32 \n80.96 \n81.11 \n69.15 \nGhostNet100 \n81.43 \n81.78 \n81.43 \n81.42 \n69.68 \nGhostNet130 \n80.80 \n81.05 \n80.80 \n80.81 \n68.61 \nDenseNet121 \n83.41  \n82.95  \n83.41  \n83.13  \n72.69  \nDenseNet169 \n82.33  \n81.60  \n82.33  \n81.75  \n71.03  \nDenseNet201 \n84.05  \n83.91  \n84.05  \n83.97  \n73.89  \nEfficientNetB0 \n80.82  \n81.78  \n80.82  \n81.19  \n68.88  \nEfficientNetB1 \n81.47  \n81.32  \n81.47  \n81.37  \n69.59  \nEfficientNetB2 \n81.03  \n81.05  \n81.03  \n81.01  \n68.92  \nEfficientNetB3 \n82.33  \n83.69  \n82.33  \n82.85  \n71.65  \nEfficientNetB4 \n81.47  \n82.09  \n81.47  \n81.71  \n69.87  \n\n \nEfficientNetB5 \n84.27  \n84.00  \n84.27  \n84.10  \n74.12  \nEfficientNetB6 \n84.91 \n85.85 \n84.91 \n85.25 \n75.64 \nEfficientNetB7 \n84.82  \n85.12  \n84.09  \n84.98  \n75.55  \nEfficientNetV2B0 \n80.39  \n80.04  \n80.39  \n80.15  \n68.07  \nEfficientNetV2B1 \n80.82  \n81.11  \n80.82  \n80.84  \n69.12  \nEfficientNetV2B2 \n81.03  \n80.74  \n81.03  \n80.88  \n68.99  \nEfficientNetV2B3 \n81.90  \n81.07  \n81.90  \n81.33  \n70.08  \nEfficientNetV2L \n82.54  \n82.70  \n82.54  \n82.49  \n71.84  \nEfficientNetV2M \n82.11  \n82.13  \n82.11  \n82.06  \n70.61  \nEfficientNetV2S \n82.97 \n83.08 \n82.97  \n82.96  \n72.15 \nInceptionResNetV2 \n82.54  \n83.46  \n82.54  \n82.85  \n71.65  \nInceptionV3 \n81.68  \n81.51  \n81.68  \n81.51  \n69.77  \nMobileNet \n78.66 \n77.45 \n78.66 \n77.63 \n64.78 \nMobileNetV2 \n82.33  \n81.46  \n82.33  \n81.64  \n70.48  \nMobileNetV3 \n81.46 \n80.37 \n81.46 \n80.70 \n69.33 \nNASNetLarge \n67.24  \n67.05  \n67.24  \n66.69  \n45.41  \nNASNetMobile \n67.89  \n66.00  \n67.89  \n65.77  \n44.79  \nResNet101 \n79.53  \n79.24  \n79.53  \n78.85  \n66.79  \nResNet101V2 \n79.09  \n77.96  \n79.09  \n78.05  \n65.78  \nResNet152 \n74.78  \n74.59  \n74.78  \n72.73  \n58.42  \nResNet152V2 \n81.03  \n80.27  \n81.03  \n80.32  \n69.05  \nResNet50 \n80.82  \n79.58  \n80.82  \n79.54  \n68.39  \nResNet50V2 \n81.90  \n80.72  \n81.90  \n80.39  \n70.06  \nVGG13 \n76.72  \n76.57  \n76.72  \n76.49  \n62.28  \nVGG16 \n80.60 \n80.72 \n80.60 \n80.14 \n68.71 \nVGG19 \n69.18  \n58.49  \n69.18  \n62.58  \n47.94  \nXception \n84.70  \n85.44  \n84.70  \n84.97  \n75.21  \n \n4.2. Results of the ViT Models \n \nThe performance of the ViT-based models was also analyzed based on various measurement \nmetrics and results on test data were listed in Table 6. \nIn the DaViT series, the best result was obtained by the DaViT-Base model, with an accuracy of \n83.55% and an f1-score of 83.57%. The DaViT-Tiny model performed similarly, achieving 83.41% \n\n \naccuracy and 83.58% f1-score. However, the DaViT-Huge model lagged behind the other models \nwith 82.76% accuracy and 82.70% f1-score.  \nFastViT-T12 model performed the best among the FastViT models, achieving 84.91% accuracy \nand 83.85% f1-score. The FastViT-T8 model achieved a competitive result with an accuracy of \n83.76% and an f1-score of 83.76%. FastViTSA-12, one of the smaller models in the FastViT series, \nshowed a relatively low accuracy of 81.90% and an f1-score of 82.26%.  \nThe performance analysis of the FlexiViT series models shows that the most successful model is \nFlexiViT-Large. With an accuracy of 82.69% and an f1-score of 82.57%, this model outperformed \nthe other models in the series. FlexiViT-Small came in second place with a balanced performance \nof 82.54% accuracy and 82.50% f1-score. FlexiViT-Base performed the lowest in the series with \nan accuracy of 81.99% and an f1-score of 82.00%. \n \nIn the GCViT series, GCViT-Tiny model showed the highest performance, with an accuracy of \n83.62% and an f1-score of 83.78%. GCViT-Small performed slightly lower with 82.54% accuracy \nand 82.81% f1-score.  \nAccording to the performance analysis of the GPViT series models, the most successful model was \nGPViT-L4. GPViT-L4 is the overall winner of the series with an accuracy of 81.90% and an f1-\nscore of 82.19%. GPViT-L2 ranked second with 81.86% accuracy and 81.98% f1-score, \nperforming satisfactorily in terms of both accuracy and f1-score. GPViT-L1 was the lowest- \nperforming model in the series with 81.22% accuracy and 81.33% f1-score. \n \nRegarding the LeViT series, the LeViT-128 model achieved the best results of the series with an \naccuracy of 82.70% and an f1-score of 83.20%, while the larger versions, the LeViT-256 and \nLeViT-384 models, underperformed with accuracies of 81.01% and 80.80%, respectively.  \nAmong the MaxViT series models, the MaxViT-Tiny was the highest-performing model. MaxViT-\nTiny achieved 86.42% accuracy and 86.53% f1-score, and MaxViT-Small achieved 84.70% \naccuracy and 84.95% f1-score. The larger versions of the MaxViT series, MaxViT-Base and \nMaxViT-Large, have lower accuracy and f1-scores of 82.33% and 81.90% accuracy, respectively. \nThe highest accuracy and f1-score values within the PVTV2 series were achieved by the PVTV2-\nB1 model with 83.84% accuracy and 83.81% f1-score. The PVTV2-B5 model also showed a \nremarkable performance with an accuracy of 82.97% and an f1-score of 83.40%. However, the \nother models of the series, especially PVTV2-B2 and PVTV2-B3, gave lower results, staying \naround 81% accuracy.  \nIn the SwinTransformerV2 series, the SwinTransformerV2-Small model performed the best with \nan accuracy of 84.48% and an f1-score of 84.90%. SwinTransformerV2-Tiny with 84.40% \naccuracy and 84.35% f1-score and SwinTransformerV2-Large with 84.27% accuracy and 84.22% \nf1-score achieved similarly high results.  \n\n \nTable 6.  Results of ViT models. \n \nModel \nPerformance measurement metrics (%) \nAccuracy \nPrecision \nRecall \nF1-score \nCohen‚Äôs kappa \nDaViT-Base \n83.55 \n83.67 \n83.55 \n83.57 \n7321 \nDaViT-Large \n82.97 \n83.51 \n82.97 \n83.15 \n72.57 \nDaViT-Small \n82.97 \n84.39 \n82.97 \n83.38 \n72.69 \nDaViT-Tiny \n83.41 \n83.90 \n83.41 \n83.58 \n73.25 \nDaViT-Huge \n82.76 \n82.78 \n82.76 \n82.70 \n71.70 \nFastViTMA-36 \n82.11 \n82.83 \n82.11 \n82.33 \n71.22 \nFastViTS-12 \n82.33 \n81.48 \n82.33 \n81.66 \n70.94 \nFastViTSA-12 \n81.90 \n82.73 \n81.90 \n82.26 \n70.74 \nFastViTSA-24 \n82.97 \n83.39 \n82.97 \n83.14 \n72.53 \nFastViTSA3-6 \n83.62 \n83.64 \n83.62 \n83.61 \n73.21 \nFastViT-T12 \n84.91 \n84.34 \n84.91 \n84.62 \n74.68 \nFastViT-T8 \n83.76 \n83.84 \n83.76 \n83.76 \n73.53 \nFlexiViT-Base \n81.99 \n82.13 \n81.99 \n82.00 \n70.59 \nFlexiViT-Large \n82.69 \n82.48 \n82.69 \n82.57 \n71.87 \nFlexiViT-Small \n82.54 \n82.60 \n82.54 \n82.50 \n71.38 \nGCViT-Base \n82.97 \n84.22 \n82.97 \n83.26 \n72.63 \nGCViT-Small \n82.54 \n83.85 \n82.54 \n82.81 \n71.77 \nGCViT-Tiny \n83.62 \n85.06 \n83.62 \n83.78 \n73.35 \nGPViT-L1 \n81.22 \n81.71 \n81.22 \n81.33 \n69.25 \nGPViT-L2 \n81.86 \n82.50 \n81.86 \n81.98 \n70.32 \nGPViT-L3 \n80.82 \n82.84 \n80.82 \n81.27 \n69.19 \nGPViT-L4 \n81.90 \n83.45 \n81.90 \n82.19 \n70.76 \nLeViT-128 \n82.70 \n84.51 \n82.70 \n83.20 \n72.32 \nLeViT-192 \n81.65 \n83.13 \n81.65 \n82.11 \n70.47 \nLeViT-256 \n81.01 \n82.08 \n81.01 \n81.32 \n69.27 \nLeViT-384 \n80.80 \n81.78 \n80.80 \n81.06 \n68.77 \nMaxViT-Base \n82.33 \n83.96 \n82.33 \n82.63 \n71.91 \nMaxViT-Large \n81.90 \n83.11 \n81.90 \n82.09 \n71.10 \nMaxViT-Small \n84.70 \n85.71 \n84.70 \n84.95 \n75.59 \nMaxViT-Tiny \n86.42 \n86.83 \n86.42 \n86.53 \n78.16 \nPVTV2-B0 \n81.90 \n81.94 \n81.90 \n81.76 \n70.74 \n\n \nPVTV2-B1 \n83.84 \n83.83 \n83.84 \n83.81 \n73.55 \nPVTV2-B2 \n81.78 \n81.82 \n81.78 \n81.74 \n70.17 \nPVTV2-B3 \n82.20 \n82.30 \n82.20 \n82.20 \n70.93 \nPVTV2-B4 \n82.48 \n82.41 \n82.48 \n82.39 \n71.38 \nPVTV2-B5 \n82.97 \n84.38 \n82.97 \n83.40 \n72.73 \nSwinTransformerV2-Base \n83.69 \n83.77 \n83.69 \n83.73 \n73.50 \nSwinTransformerV2-Large \n84.27 \n84.19 \n84.27 \n84.22 \n74.27 \nSwinTransformerV2-Small \n84.48 \n85.67 \n84.48 \n84.90 \n74.99 \nSwinTransformerV2-Tiny \n84.40 \n8434 \n84.40 \n84.35 \n74.40 \n \nTable 6 indicates that small-sized ViT models such as MaxViT-Tiny, SwinTransformerV2-Small, \nand FastViT-T12 performed the best in terms of all metrics, while some of the larger and more \ncomplex models did not. Due to the extreme complexity of large models, the risk of overfitting \nincreases. The model requires more data as the number of parameters increases. If the model is not \ntrained with a sufficient variety of data, it may overfit the training data and lose the ability to \ngeneralize to the test data. As a result, smaller and optimized models can perform strongly in \nglaucoma stage detection. In this study, the main inspiration for the rescaling and enhancing \nMaxViT model is the impressive performance of MaxViT models on glaucoma stage detection. \nTherefore, MaxViT series was considered as a backbone architecture. \n \n4.3. Results of the Proposed Model \n4.3.1. Scaling The MaxViT \nThe performance results of MaxViT series and the scaled version of the MaxViT are listed in Table \n7. Compared to MaxViT-Base and MaxViT-Large, MaxViT-Small, the MaxViT-Tiny model \nobtains the best result with 86.42% accuracy and 86.53% f1-score despite having fewer parameters \n(31M). It shows that smaller and optimized models can learn efficiently and generalize better when \nthey are of lower complexity.  \nTable 7. The classification performances of MaxViT models with varying scales. \n \nModel \nPerformance measurement metrics (%) \nParameter (M) \nAccuracy \nPrecision \nRecall \nF1-score \nCohen‚Äôs kappa \nMaxViT-Tiny \n31 \n86.42 \n86.83 \n86.42 \n86.53 \n78.16 \nMaxViT-Small \n69 \n84.70 \n85.71 \n84.70 \n84.95 \n75.59 \nMaxViT-Base \n119 \n82.33 \n83.96 \n82.33 \n82.63 \n71.91 \nMaxViT-Large \n212 \n81.90 \n83.11 \n81.90 \n82.09 \n71.10 \nMaxViT-Scaled \n6.2 \n87.93 \n88.10 \n87.93 \n87.96 \n80.51 \n\n \nThe scaled MaxViT model given in Table 4, MaxViT-Scaled, has the highest performance in the \nseries with 87.93% accuracy and 87.96% f1-score, having only 6.2M parameters. There is a need \nto balance model size, complexity, and its interaction with the data. In our case, the dataset contains \na total of 1542 images, which is relatively small. As a result, this is why small models performed \nbetter on the dataset. \nAs illustrated in Fig. 9, the performances of MaxViT models in classifying glaucoma stages show \ndifferent sensitivities and error rates for each model. While all models perform quite strongly in \nthe ‚ÄúN‚Äù class, it is noteworthy that the error rates are higher in the ‚ÄúA‚Äù class. In particular, the \nMaxViT-Tiny and MaxViT-Scaled models had higher accuracy rates in the ‚ÄúA‚Äù class, while the \nMaxViT-Small and MaxViT-Base models misclassified more cases in the ‚ÄúA‚Äù class. This suggests \nthat some models have difficulty classifying advanced glaucoma cases and that the classes may be \nconfused with each other. On the other hand, in the ‚ÄúE‚Äù class, all models performed consistently, \nshowing a balanced success in detecting early stages of glaucoma. These results suggest that \nMaxViT models are promising for glaucoma detection, but additional optimization work is needed \nto improve classification accuracy in advanced cases. \n \nFig. 9. Confusion matrices obtained with MaxViT with various scales for the glaucoma stage classification task  \n(A: Advanced, E: Early, N: Normal). \n4.3.2. Improved Stem \nThe performance results of the MaxViT-Scaled model with the stem block enhanced with different \nattention mechanisms are given in Table 8. The original MaxViT-Scaled model shows a superior \nperformance with an accuracy of 87.93% and an f1-score of 87.96%. Moreover, the performance \nof the model is further improved by adding various attention mechanisms.  \n\n \nTable 8. Classification performance of the MaxViT-Scaled model with the stem block enhanced with different \nattention mechanisms. \nModel \nPerformance measurement metrics (%) \nAccuracy \nPrecision \nRecall \nF1-score \nCohen‚Äôs kappa \nMaxViT-Scaled \n87.93 \n88.10 \n87.93 \n87.96 \n80.51 \nMaxViT-Scaled (ECA) \n89.01 \n89.30 \n89.01 \n89.06 \n82.32 \nMaxViT-Scaled (CBAM) \n88.36 \n88.47 \n88.36 \n88.38 \n81.17 \nMaxViT-Scaled (SE) \n88.15 \n88.66 \n88.15 \n88.27 \n81.00 \nThe MaxViT-Scaled model with ECA achieved the highest accuracy of 89.01% and f1-score of \n89.06%. Since ECA provides channel-based attention, it allows the model to better select \nparticularly important features and neglect unimportant information. The MaxViT-Scaled model \nequipped with CBAM achieved an accuracy of 88.36% and f1-score of 88.38%, lower than ECA \nbut higher than the scaled model. The MaxViT-Scaled model with the addition of the SE block \nperforms similarly to CBAM, with an accuracy of 88.15% and an f1-score of 88.27%. It is obvious \nthat the MaxViT-Scaled model with ECA obtained superior performance compared to other \nattention modules. For visual understanding, Fig. 10 shows the structure of the stem block \nimproved with ECA. \n \nFig. 10. The structure of the improved stem block. \n4.3.2. Improved MaxViT Block \nThe experimental results obtained by replacing MBConv in the MaxViT block with state-of-the-\nart convolution modules are given in Table 9. The ConvNeXt module achieved better results than \nthe MaxViT-Scaled model with an accuracy of 88.15% and an f1-score of 88.16%. The advanced \nstructure of the ConvNeXt block provided a small but significant improvement to the model. The \nConvNeXtV2 module is the highest performing model with an accuracy of 89.87% and f1-score \nof 89.93%. ConvNeXtV2 block significantly improved the performance of the MaxViT-Scaled \nmodel and extracted features from the data more effectively. The optimized structure of the \n\n \nConvNeXtV2 block compared to the previous generation ConvNeXt structure and its GRN \nnormalizes features along the feature map, allowing the network to learn more meaningful and \nimportant features. The MaxViT-Scaled-InceptionNeXt model‚Äôs performance is close to but lower \nthan ConvNeXt with 88.77% accuracy and 88.85% f1-score. The InceptionNeXt block gave the \nmodel a broader perspective, allowing it to extract features at different scales, but it was not as \nsuccessful as ConvNeXtV2, although it achieved better results than MaxViT-Scaled. The results \nshow that MaxViT-Scaled model with ConvNeXtV2 obtained better performance compared to \nother convolutional blocks. For visual understanding, Fig. 11 depicts the structure of the MaxViT \nblock improved with ConvNeXtV2. \nTable 9. Classification performance of the MaxViT-Scaled model replacing MBConv in the MaxViT block with state-\nof-the-art convolution modules. \nModel \nPerformance measurement metrics (%) \nAccuracy \nPrecision \nRecall \nF1-score \nCohen‚Äôs kappa \nMaxViT-Scaled-MBConv \n87.93 \n88.10 \n87.93 \n87.96 \n80.51 \nMaxViT-Scaled-ConvNeXt \n88.15 \n88.27 \n88.15 \n88.16 \n80.77 \nMaxViT-Scaled-ConvNeXtV2 \n89.87 \n90.23 \n89.87 \n89.93 \n83.65 \nMaxViT-Scaled-InceptionNeXt \n88.77 \n89.06 \n88.77 \n88.85 \n81.75 \n \n \nFig. 11. The structure of the improved MaxViT block. \n 4.3.3. Improved Stem and MaxViT Block \nAt this stage of the study, three different experiments were conducted. In the first experiment, \nscaling the MaxViT model reduced its size (block and channel) and improved its performance. In \nthe second experiment, the stem block was enhanced with ECA, which resulted in a more robust \nfeature extraction and improved performance. In the last experiment, the performance was \nimproved by adding different convolutional blocks to the MaxViT block. Considering the results \nof the previous experiments, the power of the ECA attention module and ConvNeXtV2 \nconvolutional block integrated to the scaled MaxViT model as a best combination. We also \nexperimented ECA with other convolutional blocks to prove the consistency of the proposed \napproach. The experimental results show the effects of combinations on the model and the results \nare presented in detail in Table 10. \n\n \nTable 10. Classification performance of the MaxViT-Scaled model replacing MBConv in the MaxViT block with \nconvolution modules and the stem block enhanced with different attention mechanisms. \nModel \nPerformance measurement metrics (%) \nAccuracy \nPrecision \nRecall \nF1-score \nCohen‚Äôs kappa \nMaxViT-Scaled-ECA-MBConv \n89.01 \n89.30 \n89.01 \n89.06 \n82.32 \nMaxViT-Scaled-ECA-ConvNeXt \n90.73 \n90.85 \n90.73 \n90.78 \n84.98 \nMaxViT-Scaled-ECA-ConvNeXtV2 \n92.03 \n92.33 \n92.03 \n92.13 \n87.12 \nMaxViT-Scaled-ECA-InceptionNeXt \n91.16 \n91.58 \n91.16 \n91.30 \n85.76 \n \nFigure 12 details the structure of the proposed MaxViT model (MaxViT-Scaled-ECA-\nConvNeXtV2), named MaxGlaViT. The model starts with a stem block enhanced with ECA \nblocks, followed by MaxViT blocks in four stages. The structure is designed to enhance feature \nextraction and improve the performance of the model. The MaxViT block includes ConvNeXtV2, \nblock and grid attention modules, and the combination of these components increases the learning \ncapacity of the model. In the final stage, the output is generated with a pooling layer and a fully \nconnected layer. \n \nFig. 12. The structure of the proposed MaxGlaViT. \n \n \nFig. 13. Confusion matrices of the models (A: Advanced, E: Early, N: Normal). \n\n \nOverall, all three models perform well with high correct classifications, especially for class ‚ÄúN‚Äù, \nwhere each model achieves over 215 correct predictions shown in Fig. 13. MaxViT-Scaled-ECA-\nConvNeXtV2 has the highest accuracy for class ‚ÄúA‚Äù, with 135 correct classifications, indicating \nits strength in recognizing advanced glaucoma stages. For class ‚ÄúE‚Äù, MaxViT-Scaled-ECA-\nConvNeXtV2 and MaxViT-Scaled-ECA-InceptionNeXtV2 have the fewest misclassifications, \nsuggesting it is more effective at distinguishing class E from the others. These insights suggest that, \nwhile all models are competent, MaxViT-Scaled-ECA-ConvNeXtV2 is preferable for glaucoma \nstage detection. \n \nFig. 14. Bar graph for results of the modified MaxViT deep models with different variations. \n \nAs visually depicted in Fig. 14, overall, MaxViT-Scaled-ECA-ConvNeXtV2 consistently \noutperforms the other variations across all metrics, establishing itself as the most effective model \nin this comparison. The balanced architecture of the model likely contributes to its ability to \ngeneralize well across diverse tasks, ensuring robust performance. MaxGlaViT is not only as the \nbest among the tested models but also as a promising candidate for broader applications in real-\nworld scenarios. \n \n \n \n\n \n4.4. Comparison of the proposed model with other literature studies \nThe performance of the MaxGlaViT model is evaluated by comparing it with other studies in the \nliterature using the same dataset. Based on Table 11, the proposed MaxGlaViT model achieved \nsignificant success in the field of fundus image-based glaucoma detection, reaching 92.03% \naccuracy, 92.33% precision, 92.03% recall, and 92.13% f1-score. Compared to FJA-Net, which \nhas the highest performance in the literature, MaxGlaViT increased its accuracy from 87.06% to \n92.03%, an increase of 5.71%. The precision improved by 6.11%, from 87.01% to 92.33%, and the \nrecall improved by 5.71%, from 87.06% to 92.03%. Also, the f1-score increased from 86.90% to \n92.13%, an increase of 6.02%. For healthcare, even a 1% increase in accuracy is significant enough \nto make a critical difference in accurate diagnosis and treatment. \nTable 11. The comparison of MaxGlaViT with other literature studies that use the same dataset. \nModel \nPerformance measurement metrics (%) \nAccuracy \nPrecision \nRecall \nF1-score \nInceptionV3 [9] \n84.50 \n- \n- \n- \nAES-Net [14] \n86.20 \n85.32 \n85.77 \n85.46 \nFJA-Net [15] \n87.06 \n87.01 \n87.06 \n86.90 \nCA-Net [16] \n85.34 \n85.15 \n85.34 \n84.92 \nGS-Net [17] \n84.91 \n- \n- \n84.55 \nProposed method (MaxGlaViT) \n92.03 \n92.33 \n92.03 \n92.13 \n \nThe results indicate that MaxGlaViT provides a significant performance advantage compared with \nother studies in the literature. In FJA-Net, AES-Net, GS-Net, and CA-Net, a CNN model is used \nas a backbone and an attention mechanism is added to the last layer for classification. However, \none study uses only InceptionV3 and transfer learning. CNNs struggle to capture long-range \ncontextual information, as they focus primarily on local feature extraction, which may limit the \nability to understand global context, leading to reduced robustness in image classification. \nHowever, MaxViT has an innovative architecture that can effectively learn both local and global \ncontextual information. With its grid and block attention mechanisms, MaxViT is capable of \nlearning long-range dependencies where CNNs are limited. In addition to the inherited features in \nMaxViT, MaxGlaViT model has learned many local and global features thanks to the \nimprovements in the stem and MaxViT block, and outperforms recent studies in the literature.  \n4.5. Discussions  \nThe paper introduces an enhanced MaxViT-based model, MaxGlaViT, for the classification of \nglaucoma stages from fundus images (Fig. 7). The model is designed by rescaling the MaxViT \narchitecture, which provides an effective trade-off between performance and model size. The \nScaled MaxViT has 6.2M parameters, 80% fewer than the MaxViT-Tiny model. MaxViT-Scaled \nachieved 87.93% accuracy, 88.10% precision, 87.93% recall, 87.96% f1-score and 80.51% Cohen's \n\n \nkappa (Table 7). The results indicate that fine-tuning model structure complexity can improve \ngeneralizability, especially when working with limited data. Further improvements were achieved \nwith attention mechanisms integrated into the stem block. The addition of ECA to the stem block \nachieved 89.01% accuracy, 89.30% precision, 89.01% recall, 89.06% f1-score, and 82.32% \nCohen's Kappa (Table 8). This enhancement enabled channel-based features to be emphasized in \nearly-stage feature extraction. Replacing the MBConv block in the MaxViT block with the \nConvNeXtV2 module resulted in 89.87% accuracy, 90.23% precision, 89.87% recall, 89.93% f1-\nscore, and 83.65% Cohen's kappa (Table 9). Finally, in the scaled model, using ConvNeXtV2 in \nthe ECA and MaxViT blocks in the stem block and ECA in the MaxViT block, an accuracy of \n92.03%, precision of 92.33%, recall of 92.03%, f1-score of 92.13%, and Cohen‚Äôs Kappa of 87.12% \nwere obtained (Table 10). Comparisons with existing literature demonstrate that the MaxGlaViT \nmodel is superior in terms of classification performance (Table 11). Finally, proposed MaxGlaViT \n(Fig. 12) demonstrates notable performance, achieving 92.03% accuracy, 92.33% precision, \n92.03% recall, 92.13% f1-score, and 87.12% Cohen‚Äôs kappa score compared to existing over 80 \ndeep models and models in mentioned literature studies. \n5. Conclusion \nGlaucoma is a chronic eye disease and it leads to irreversible vision loss if diagnosed at an early \nstage. This paper presents a CAD system to assist ophthalmologists in the diagnostic process of \nglaucoma stages. Experiments were performed on three main DL models, namely CNNs, ViTs and \nMaxGlaViT. Among 40 CNN models, EfficientB6 was the most successful model with an accuracy \nof 84.91%. On the other hand, among the ViT models, MaxViT-Tiny achieved the highest \nperformance with an accuracy rate of 86.42%. We then scaled the number of blocks and channels \nof the MaxViT-Tiny, resulting in a lightweight model with 6.2M parameters and an accuracy rate \nof 87.93%. After adding ECA to the stem block, the accuracy rate increased to 89.01%. Another \nimprovement was made by replacing the MBConv structure in the MaxViT block with \nConvNeXtV2 and an accuracy of 89.87% was obtained. In the last stage, the MaxGlaViT model \nwas obtained by using ECA and ConvNeXtV2 block on the stem and MaxViT block, respectively, \nand the accuracy was increased to 92.03%. Experimental results prove that the proposed \nlightweight MaxGlaViT model is among the most advanced models in this field by showing \nsuperior performance in glaucoma diagnosis. In future work, potential improvements and \nmechanisms that can be applied to the block, grid attention and head parts of the MaxViT model \nwill be investigated and experiments will be conducted.  \nDeclaration of competing interest \nThe authors declare that they have no known competing financial interests or personal relationships \nthat could have appeared to influence the work reported in this paper. \n \nCRediT authorship contribution statement \n\n \nMustafa Yurdakul, KuÃàbra Uyar: Conceptualization, Methodology, Review and editing, \nSoftware, Validation, Visualization, Writing-original draft, SÃßakir TasÃßdemir: Conceptualization, \nMethodology, Supervision.  \n  \nCode Availability The source code used to obtain experimental results is publicly available at \nhttps://github.com/ymyurdakul/MaxGlaViT. \n \nData Availability The data that support the findings of this study are openly available at \nhttps://dataverse.harvard.edu/citation?persistentId=doi:10.7910/DVN/1YRRAC. \n \nReferences \n1. \nAbbas, Q., 2017. Glaucoma-deep: detection of glaucoma eye disease on retinal fundus images using \ndeep learning. International Journal of Advanced Computer Science and Applications, 8 (6). \nhttps://doi.org/10.14569/IJACSA.2017.080606. \n2. \nLotankar, M., Noronha, K. and Koti, J., 2015. Detection of optic disc and cup from color retinal \nimages for automated diagnosis of glaucoma.  2015 IEEE UP Section Conference on Electrical \nComputer and Electronics (UPCON), 1-6. https://doi.org/10.1109/UPCON.2015.7456741. \n3. \nArpitha, S. and Lakshitha, P., 2023. Glaucoma Detection Using Fundus Images of the Retina. \nApplied \nand \nComputational \nEngineering, \n2, \n283-290. \nhttps://doi.org/10.54254/2755-\n2721/2/20220641. \n4. \nNayak, J., Acharya U, R., Bhat, P. S., Shetty, N. and Lim, T.-C., 2008. Automated Diagnosis of \nGlaucoma Using Digital Fundus Images. Journal of Medical Systems, 33 (5), 337. \nhttps://doi.org/10.1007/s10916-008-9195-z. \n5. \nBock, R., Meier, J., Ny√∫l, L. G., Hornegger, J. and Michelson, G., 2010. Glaucoma risk \nindex:Automated glaucoma detection from color fundus images. Medical Image Analysis, 14 (3), \n471-481. https://doi.org/10.1016/j.media.2009.12.006. \n6. \nAcharya, U. R., Dua, S., Du, X., S, V. S. and Chua, C. K., 2011. Automated Diagnosis of Glaucoma \nUsing Texture and Higher Order Spectra Features. IEEE Transactions on Information Technology \nin Biomedicine, 15 (3), 449-455. https://doi.org/10.1109/titb.2011.2119322. \n7. \nAcharya, U. R., Ng, E. Y. K., Eugene, L. W. J., Noronha, K. P., Min, L. C., Nayak, K. P. and \nBhandary, S. V., 2015. Decision support system for the glaucoma using Gabor transformation. \nBiomedical Signal Processing and Control, 15, 18-26. https://doi.org/10.1016/j.bspc.2014.09.004. \n8. \nDong, S., Wang, P. and Abbas, K., 2021. A survey on deep learning and its applications. Computer \nScience Review, 40, 100379. https://doi.org/10.1016/j.cosrev.2021.100379. \n9. \nAhn, J. M., Kim, S., Ahn, K.-S., Cho, S.-H., Lee, K. B. and Kim, U. S., 2018. A deep learning \nmodel for the detection of both advanced and early glaucoma using fundus photography. PLOS \nONE, 13 (11), e0207982. https://doi.org/10.1371/journal.pone.0207982. \n10. \nJuneja, M., Singh, S., Agarwal, N., Bali, S., Gupta, S., Thakur, N. and Jindal, P., 2020. Automated \ndetection of Glaucoma using deep learning convolution network (G-net). Multimedia Tools and \nApplications, 79 (21), 15531-15553. https://doi.org/10.1007/s11042-019-7460-4. \n11. \nJuneja, M., Thakur, S., Uniyal, A., Wani, A., Thakur, N. and Jindal, P., 2022. Deep learning-based \nclassification network for glaucoma in retinal images. Computers and Electrical Engineering, 101, \n108009. https://doi.org/10.1016/j.compeleceng.2022.108009. \n12. \nChai, Y., Liu, H. and Xu, J., 2018. Glaucoma diagnosis based on both hidden features and domain \nknowledge through deep learning models. Knowledge-Based Systems, 161, 147-156. \nhttps://doi.org/10.1016/j.knosys.2018.07.043. \n\n \n13. \nHaouli, I. E., Hariri, W. and Seridi-Bouchelaghem, H., 2023. Exploring Vision Transformers for \nAutomated Glaucoma Disease Diagnosis in Fundus Images.  2023 International Conference on \nDecision \nAid \nSciences \nand \nApplications \n(DASA), \n520-524. \nhttps://doi.org/10.1109/DASA59624.2023.10286714. \n14. \nDas, D., Nayak, D. R. and Pachori, R. B., 2024. AES-Net: An adapter and enhanced self-attention \nguided network for multi-stage glaucoma classification using fundus images. Image and Vision \nComputing, 146, 105042. https://doi.org/10.1016/j.imavis.2024.105042. \n15. \nDas, D. and Nayak, D. R., 2024. FJA-Net: A Fuzzy Joint Attention Guided Network for \nClassification of Glaucoma Stages. IEEE Transactions on Fuzzy Systems, 32 (10), 5438-5448. \nhttps://doi.org/10.1109/TFUZZ.2024.3448231. \n16. \nDas, D., Nayak, D. R. and Pachori, R. B., 2023. CA-Net: A Novel Cascaded Attention-Based \nNetwork for Multistage Glaucoma Classification Using Fundus Images. IEEE Transactions on \nInstrumentation and Measurement, 72, 1-10. https://doi.org/10.1109/TIM.2023.3322499. \n17. \nDas, D. and Nayak, D., 2023. GS-Net: Global Self-Attention Guided CNN for Multi-Stage \nGlaucoma Classification. https://doi.org/10.1109/ICIP49359.2023.10222689. \n18. \nVaswani, A., 2017. Attention is all you need. Advances in Neural Information Processing Systems. \n19. \nDevlin, J., 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. \narXiv preprint arXiv:1810.04805. http://dx.doi.org/10.48550/ARXIV.1810.04805. \n20. \nBrown, T. B., 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165. \n21. \nDosovitskiy, A., 2020. An image is worth 16x16 words: Transformers for image recognition at \nscale. arXiv preprint arXiv:2010.11929. https://doi.org/10.48550/arXiv.2010.11929. \n22. \nWang, W., Xie, E., Li, X., Fan, D. P., Song, K., Liang, D., Lu, T., Luo, P. and Shao, L., 2021. \nPyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions.  \n2021 \nIEEE/CVF \nInternational \nConference \non \nComputer \nVision \n(ICCV), \n548-558. \nhttps://doi.org/10.1109/ICCV48922.2021.00061. \n23. \nLiu, Z., Hu, H., Lin, Y., Yao, Z., Xie, Z., Wei, Y., Ning, J., Cao, Y., Zhang, Z. and Dong, L., 2022. \nSwin transformer v2: Scaling up capacity and resolution.  Proceedings of the IEEE/CVF conference \non computer vision and pattern recognition, 12009-12019. \n24. \nDing, M., Xiao, B., Codella, N., Luo, P., Wang, J. and Yuan, L., 2022. Davit: Dual attention vision \ntransformers.  European conference on computer vision, 74-92. \n25. \nVasu, P. K. A., Gabriel, J., Zhu, J., Tuzel, O. and Ranjan, A., 2023. FastViT: A fast hybrid vision \ntransformer using structural reparameterization.  Proceedings of the IEEE/CVF International \nConference on Computer Vision, 5785-5795. \n26. \nHatamizadeh, A., Yin, H., Heinrich, G., Kautz, J. and Molchanov, P., 2023. Global context vision \ntransformers.  International Conference on Machine Learning, 12633-12646. \n27. \nBeyer, L., Izmailov, P., Kolesnikov, A., Caron, M., Kornblith, S., Zhai, X., Minderer, M., \nTschannen, M., Alabdulmohsin, I. and Pavetic, F., 2023. Flexivit: One model for all patch sizes.  \nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 14496-\n14506. \n28. \nYang, C., Xu, J., De Mello, S., Crowley, E. J. and Wang, X., 2022. Gpvit: a high resolution non-\nhierarchical vision transformer with group propagation. arXiv preprint arXiv:2212.06795. \n29. \nGraham, B., El-Nouby, A., Touvron, H., Stock, P., Joulin, A., J√©gou, H. and Douze, M., 2021. Levit: \na vision transformer in convnet's clothing for faster inference.  Proceedings of the IEEE/CVF \ninternational conference on computer vision, 12259-12269. \n30. \nTu, Z., Talebi, H., Zhang, H., Yang, F., Milanfar, P., Bovik, A. and Li, Y., 2022. MaxViT: Multi-\naxis Vision Transformer.  Computer Vision ‚Äì ECCV 2022, Cham, 459-479. \n31. \nNiu, Z., Zhong, G. and Yu, H., 2021. A review on the attention mechanism of deep learning. \nNeurocomputing, 452, 48-62. https://doi.org/10.1016/j.neucom.2021.03.091. \n32. \nGuo, M.-H., Xu, T.-X., Liu, J.-J., Liu, Z.-N., Jiang, P.-T., Mu, T.-J., Zhang, S.-H., Martin, R. R., \nCheng, M.-M. and Hu, S.-M., 2022. Attention mechanisms in computer vision: A survey. \nComputational Visual Media, 8 (3), 331-368. https://doi.org/10.1007/s41095-022-0271-y. \n\n \n33. \nLiang, Y., Lin, Y., & Lu, Q. (2022). Forecasting gold price using a novel hybrid model with \nICEEMDAN and LSTM-CNN-CBAM. Expert Systems with Applications, 206, 117847.  \n34. \nPraharsha, C. H. and Poulose, A., 2024. CBAM VGG16: An efficient driver distraction \nclassification using CBAM embedded VGG16 architecture. Computers in Biology and Medicine, \n180, 108945. https://doi.org/10.1016/j.compbiomed.2024.108945. \n35. \nAl-Fahdawi, S., Al-Waisy, A. S., Zeebaree, D. Q., Qahwaji, R., Natiq, H., Mohammed, M. A., \nNedoma, J., Martinek, R. and Deveci, M., 2024. Fundus-DeepNet: Multi-label deep learning \nclassification system for enhanced detection of multiple ocular diseases through data fusion of \nfundus images. Information Fusion, 102, 102059. https://doi.org/10.1016/j.inffus.2023.102059. \n36. \nWang, Z. and Wang, L., 2022. An Attention Approach for Dimensionality Reduction That Can \nCorrect \nFeature \nCollection \nSkewness. \nIEEE \nAccess, \n10, \n117273-117280. \nhttps://doi.org/10.1109/ACCESS.2022.3220245. \n37. \nWoo, S., Park, J., Lee, J.-Y. and Kweon, I. S., 2018. Cbam: Convolutional block attention module.  \nProceedings of the European conference on computer vision (ECCV), 3-19. \n38. \nLiu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T. and Xie, S., 2022. A convnet for the \n2020s.  Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, \n11976-11986. \n39. \nWoo, S., Debnath, S., Hu, R., Chen, X., Liu, Z., Kweon, I. S. and Xie, S., 2023. Convnext v2: Co-\ndesigning and scaling convnets with masked autoencoders.  Proceedings of the IEEE/CVF \nConference on Computer Vision and Pattern Recognition, 16133-16142. \n40. \nYu, W., Zhou, P., Yan, S. and Wang, X., 2024. Inceptionnext: When inception meets convnext.  \nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 5672-\n5683. \n41. \nPacal, I., 2024. MaxCerVixT: A novel lightweight vision transformer-based Approach for precise \ncervical \ncancer \ndetection. \nKnowledge-Based \nSystems, \n289, \n111482. \nhttps://doi.org/10.1016/j.knosys.2024.111482. \n42. \nPacal, I., 2024. Enhancing crop productivity and sustainability through disease identification in \nmaize leaves: Exploiting a large dataset with an advanced vision transformer model. Expert Systems \nwith Applications, 238, 122099. https://doi.org/10.1016/j.eswa.2023.122099. \n43.     Gerbasi, A., Dagliati, A., Albi, G., Chiesa, M., Andreini, D., Baggiano, A., Mushtaq, S., Pontone, G., \nBellazzi, R. and Colombo, G., 2024. CAD-RADS scoring of coronary CT angiography with Multi-\nAxis Vision Transformer: A clinically-inspired deep learning pipeline. Computer Methods and \nPrograms in Biomedicine, 244, 107989. https://doi.org/10.1016/j.cmpb.2023.107989. \n",
  "metadata": {
    "source_path": "papers/arxiv/MaxGlaViT_A_novel_lightweight_vision_transformer-based_approach_for\n__early_diagnosis_of_glaucoma_stages_from_fundus_images_9469a378ec093395.pdf",
    "content_hash": "9469a378ec093395f02f8e8588be173286b819ac973745ea9eb3cec0e5f2e7e1",
    "arxiv_id": null,
    "title": "MaxGlaViT_A_novel_lightweight_vision_transformer-based_approach_for\n__early_diagnosis_of_glaucoma_stages_from_fundus_images_9469a378ec093395",
    "author": "Mustafa",
    "creation_date": "D:20250224163048+03'00'",
    "published": "20250224163048+03'00'",
    "pages": 32,
    "size": 1460596,
    "file_mtime": 1740470178.3626676
  }
}